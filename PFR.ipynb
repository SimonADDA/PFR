{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126a7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arxiv\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install nameparser\n",
    "# !pip install pdfminer\n",
    "# !pip install refextract\n",
    "# !pip install pdfx\n",
    "# !pip install -U textblob\n",
    "# !pip install owlready\n",
    "# !pip install boto3\n",
    "# Install a conda package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !conda install -c conda-forge poppler\n",
    "# !pip install pdftotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2ac646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "* Owlready * Creating new ontology owl <http://www.w3.org/2002/07/owl>.\n",
      "* Owlready * Creating new ontology 22-rdf-syntax-ns <http://www.w3.org/1999/02/22-rdf-syntax-ns>.\n",
      "* Owlready * Creating new ontology rdf-schema <http://www.w3.org/2000/01/rdf-schema>.\n",
      "* Owlready * Creating new ontology XMLSchema <http://www.w3.org/2001/XMLSchema>.\n",
      "* Owlready * Creating new ontology anonymous <http://anonymous>.\n",
      "* Owlready * Creating new ontology owlready_ontology <http://www.lesfleursdunormal.fr/static/_downloads/owlready_ontology.owl>.\n",
      "* Owlready *     ...loading ontology owlready_ontology from C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\owlready\\owlready_ontology.owl...\n"
     ]
    }
   ],
   "source": [
    "#Import of librairies\n",
    "\n",
    "#ARVIX\n",
    "import arxiv\n",
    "\n",
    "#PDFTEXT\n",
    "import urllib.request\n",
    "import pdftotext\n",
    "\n",
    "# Prevent future/deprecation warnings from showing in output\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#Lol=cal\n",
    "import os\n",
    "import requests as r\n",
    "\n",
    "#AI\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk.tag.stanford as st\n",
    "from nltk.tag.stanford import StanfordNERTagger \n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "import boto3\n",
    "\n",
    "from owlready import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf6f98",
   "metadata": {},
   "source": [
    "## Download pdf file from Arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63be17ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203.12616v1\n",
      "2203.12614v1\n",
      "2203.12613v1\n",
      "2203.12612v1\n",
      "2203.12610v1\n"
     ]
    }
   ],
   "source": [
    "#You can choose here how many pdf you want. \n",
    "max_results=5\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = \"Computer Science & AI\",\n",
    "    max_results = max_results,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "for result in search.results():\n",
    "    print(result.pdf_url[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b3ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link:  ['http://arxiv.org/pdf/2203.12616v1', 'http://arxiv.org/pdf/2203.12614v1', 'http://arxiv.org/pdf/2203.12613v1', 'http://arxiv.org/pdf/2203.12612v1', 'http://arxiv.org/pdf/2203.12610v1']\n",
      "Authors:  [['Chantal.Pellegrini', 'Anees.Kazi', 'Nassir.Navab'], ['Gyungin.Shin', 'Samuel.Albanie', 'Weidi.Xie'], ['Jiamin.Xu', 'Zihan.Zhu', 'Hujun.Bao', 'Wewei.Xu'], ['Fangjian.Lin', 'Zhanhao.Liang', 'Junjun.He', 'Miao.Zheng', 'Shengwei.Tian', 'Kai.Chen'], ['Ziming.Liu', 'Varun.Madhavan', 'Max.Tegmark']]\n",
      "Title:  ['Unsupervised Pre-Training on Patient Population Graphs for Patient-Level Predictions', 'Unsupervised Salient Object Detection with Spectral Cluster Voting', 'A Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction', 'StructToken : Rethinking Semantic Segmentation with Structural Prior', 'AI Poincaré 2.0: Machine Learning Conservation Laws from Differential Equations']\n"
     ]
    }
   ],
   "source": [
    "#Store in array all informations on pdf (title, authors and links)\n",
    "\n",
    "array_link=[]\n",
    "array_authors=[]\n",
    "array_title=[]\n",
    "\n",
    "for result in search.results():\n",
    "    array_link.append(result.pdf_url)\n",
    "    array_title.append(result.title)\n",
    "    temp=result.authors\n",
    "    array_authors.append([re.sub(\"[^A-Za-z0-9]\",\".\",str(i)) for i in temp])\n",
    "    #Download pdf as link.pdf\n",
    "    result.download_pdf(dirpath=\".\\Download\", filename=f'{result.pdf_url[21:]}.pdf')\n",
    "    \n",
    "print(\"Link: \",array_link)\n",
    "print(\"Authors: \",array_authors)\n",
    "print(\"Title: \",array_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0bf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pdf link to a pdf text to work on it\n",
    "\n",
    "def load_pdf(value):\n",
    "    with open(f'Download/{value}.pdf', \"rb\") as f:\n",
    "        text = pdftotext.PDF(f)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c06a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Unsupervised Pre-Training on Patient Population\\r\\nGraphs for Patient-Level Predictions\\r\\nChantal Pellegrini1 , Anees Kazi1 , and Nassir Navab1,2\\r\\nComputer Aided Medical Procedures, Technical University Munich, Germany\\r\\nComputer Aided Medical Procedures, Johns Hopkins University, Baltimore, USA\\r\\n\\r\\n1\\r\\n\\r\\narXiv:2203.12616v1 [cs.LG] 23 Mar 2022\\r\\n\\r\\n2\\r\\n\\r\\nAbstract. Pre-training has shown success in different areas of machine\\r\\nlearning, such as Computer Vision (CV), Natural Language Processing\\r\\n(NLP) and medical imaging. However, it has not been fully explored\\r\\nfor clinical data analysis. Even though an immense amount of Electronic Health Record (EHR) data is recorded, data and labels can be\\r\\nscarce if the data is collected in small hospitals or deals with rare diseases. In such scenarios, pre-training on a larger set of EHR data could\\r\\nimprove the model performance. In this paper, we apply unsupervised\\r\\npre-training to heterogeneous, multi-modal EHR data for patient outcome prediction. To model this data, we leverage graph deep learning\\r\\nover population graphs. We first design a network architecture based on\\r\\ngraph transformer designed to handle various input feature types occurring in EHR data, like continuous, discrete, and time-series features,\\r\\nallowing better multi-modal data fusion. Further, we design pre-training\\r\\nmethods based on masked imputation to pre-train our network before\\r\\nfine-tuning on different end tasks. Pre-training is done in a fully unsupervised fashion, which lays the groundwork for pre-training on large\\r\\npublic datasets with different tasks and similar modalities in the future.\\r\\nWe test our method on two medical datasets of patient records, TADPOLE and MIMIC-III, including imaging and non-imaging features and\\r\\ndifferent prediction tasks. We find that our proposed graph based pretraining method helps in modeling the data at a population level and\\r\\nfurther improves performance on the fine tuning tasks in terms of AUC\\r\\non average by 4.15% for MIMIC and 7.64% for TADPOLE.\\r\\nKeywords: Outcome/Disease Prediction · Population Graphs · PreTraining.\\r\\n\\r\\n1\\r\\n\\r\\nIntroduction\\r\\n\\r\\nEnormous amounts of data are collected on a daily basis in hospitals. Nevertheless, labeled data can be scarce, as labeling can be tedious, time-consuming,\\r\\nand expensive. Further, in small hospitals and for rare diseases, only little data\\r\\nis accumulated [15]. The ability to leverage the large body of unlabeled medical\\r\\ndata to improve in prediction tasks over such small labeled datasets could increase the confidence of AI models for clinical outcome prediction. Unsupervised\\r\\npre-training was shown to be useful to exploit unlabeled data in NLP [20,4],\\r\\n\\r\\n\\x0c\\n\\n2\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\nCV [18,1] and medical imaging [16,2]. However, for more complex clinical data,\\r\\nit is not explored enough. Some works study how to pre-train BERT [4] over\\r\\nEHR data of medical diagnostic codes. They pre-train with modified masked\\r\\nlanguage modeling and, in one case, a supervised prediction task. The targeted\\r\\ndownstream tasks lie in disease and medication code prediction [23,11,21]. McDermott et al. [14] propose a pre-training approach over heterogeneous EHR\\r\\ndata, including e.g. continuous lab results. They create a benchmark with several downstream tasks over the eICU [19] and MIMIC-III [7] datasets and present\\r\\ntwo baseline pre-training methods. Pre-training and fine-tuning are performed\\r\\nover single EHRs from ICU stays with a Gated Recurrent Unit.\\r\\nOn the other hand, population graphs have been leveraged in the recent literature to help analyze patient data using the relationships among patients, leading\\r\\nto clinically semantic modeling of the data. During unsupervised pre-training,\\r\\ngraphs allow learning representations based on feature similarities between the\\r\\npatients, which can then help to improve patient-level predictions. Several works\\r\\nsuccessfully apply pre-training to graph data in different domains like molecular\\r\\ngraphs and on common graph benchmarks. Proposed pre-training strategies include node level tasks, like attribute reconstruction, graph level tasks like property prediction, or generative tasks such as edge prediction [5,22,27,6,12]. To\\r\\nthe best of our knowledge, no previous work applied pre-training to population\\r\\ngraphs.\\r\\nIn this paper, we propose a model capable of pre-training for understanding\\r\\npatient population data. We choose two medical applications in brain imaging\\r\\nand EHR data analysis on the public datasets TADPOLE [13] and MIMICIII [7] for Alzheimer’s disease prediction [17,9,3] and Length-of-Stay prediction\\r\\n[26,24,14]. The code is available at https://github.com/ChantalMP/UnsupervisedPre-Training-on-Patient-Population-Graphs-for-Patient-Level-Predictions.\\r\\nContribution: We develop an unsupervised pre-training method to learn a\\r\\ngeneral understanding of patient population data modeled as graph, providing a\\r\\nsolution to limited labeled data. We show significant performance gains through\\r\\npre-training when fine-tuning with as little as 1% and up to 100% labels. Further, we propose a (graph) transformer based model suitable for multi-modal\\r\\ndata fusion. It is designed to handle various EHR input types, taking static\\r\\nand time-series data and continuous as well as discrete numerical features into\\r\\naccount.\\r\\n\\r\\n2\\r\\n\\r\\nMethod\\r\\n\\r\\nLet D be a dataset composed of the EHR data of N patients. The ith record is\\r\\nrepresented by ri ⊆ [di , ci , ti ] with static discrete features di ∈ ND , continuous\\r\\nfeatures ci ∈ RC , and time-series features ti ∈ RS×τ , where τ denotes the length\\r\\nof the time-series. For every downstream task T , labels Y ∈ NL are given for L\\r\\nclasses. The task is to predict the classes for the test set patients given all features. Towards this task, we propose to use patient population graphs. Unlike in\\r\\nnon-graph-based methods, the model can exploit similarities between patients to\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n3\\r\\n\\r\\nbetter understand the EHR data at patient and population level. Further, unlike\\r\\nconventional graph neural networks, graph transformers allow flexible attention\\r\\nto all nodes, learning which patients are relevant for a task. This would be most\\r\\napt for learning over population EHR data. Our pipeline consists of two steps.\\r\\n1) Unsupervised pre-training and 2) Fine-tuning. Unsupervised pre-training enables understanding of general EHR data and disorder progression by training\\r\\nthe model for masked imputation task. This understanding can help learn downstream tasks better despite limited labeled data.\\r\\nGraph Construction For each node pair with records ri and rj , we calculate a\\r\\nsimilarity score S(ri , rj ) between the node features. We use L2 distance for continuous and absolute matching for discrete features. As graph construction is not\\r\\nour focus, we choose the most conventional method using k-NN selection rule.\\r\\nWe set k=5 to avoid having many disconnected components and very densely\\r\\nconnected regions (see supplementary material). A detailed description of the\\r\\ngraph construction per dataset follows in the experiment section.\\r\\nModel Architecture Our model consists of an encoder and decoder. The encoder comprises a data embedding module and a graph transformer module\\r\\nexplained later. We design the encoder to handle various input data types. The\\r\\ndecoder is a simple linear layer capable of capturing the essence of features inclined towards a node-level classification task. Figure 1 shows an overview of our\\r\\nmodel architecture.\\r\\nData embedding module: Following the conventional Graphormer [25], we proEncoder\\r\\n\\r\\nDecoder\\r\\nData Embedding Module\\r\\n\\r\\nstatic, discrete\\r\\nfeatures d\\r\\n\\r\\nEmbedding Layer\\r\\n\\r\\nstatic, continuous\\r\\nfeatures c\\r\\n\\r\\nLinear Layer\\r\\n\\r\\nC' x N\\r\\n\\r\\ntime-series\\r\\nfeatures t\\r\\n\\r\\nLinear Layer\\r\\n\\r\\nS' x τ x N\\r\\n\\r\\nsum\\r\\n\\r\\nD' x N\\r\\n\\r\\nconcat\\r\\n\\r\\n2x Transformer\\r\\nLayer\\r\\n\\r\\nQ\\r\\nSequence\\r\\n\\r\\nK\\r\\n\\r\\n+\\r\\n\\r\\nV\\r\\n\\r\\npositional\\r\\nencodings\\r\\n\\r\\nmean\\r\\n\\r\\nScale +\\r\\nNorm\\r\\n\\r\\nx\\r\\n\\r\\nLinear\\r\\nLayer\\r\\n\\r\\nmultiple\\r\\nGraphormer\\r\\nLayer\\r\\n\\r\\nLinear Task\\r\\nLayer\\r\\n\\r\\nLoss\\r\\n\\r\\nS' x N\\r\\n\\r\\nadd\\r\\n\\r\\nadd\\r\\nx\\r\\n\\r\\nFxN\\r\\n\\r\\nNorm\\r\\n\\r\\n2x Linear\\r\\nLayer\\r\\n\\r\\nNorm\\r\\n\\r\\nTransformer Layer\\r\\n\\r\\nFig. 1. Overview of the proposed architecture. All input features are combined into\\r\\none node embedding, applying transformer layers to enhance the time-series features.\\r\\nThe resulting graph is processed by several Graphormer layers and a linear task layer.\\r\\n\\r\\ncess discrete input features by an embedding layer, followed by a summation over\\r\\n0\\r\\nthe feature dimension, resulting in embedded features d0i ∈ RD , where D0 is the\\r\\noutput feature dimension. While Graphormer is limited to static, discrete input\\r\\nfeatures only, we improve upon Graphormer to support also static, continuous\\r\\ninput features, which are processed by a linear layer resulting in the embedding\\r\\n0\\r\\nvector c0i ∈ RC . The third branch of our data embedding module handles timeseries input features ti ∈ RS×τ with a linear layer, followed by two transformer\\r\\nlayers to deal with variable sequence lengths and allow the model to incorporate\\r\\n\\r\\n\\x0c\\n\\n4\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\ntemporal context. The output is given by t0i,h ∈ RE per time-step h. The mean\\r\\n0\\r\\nof these embeddings forms the final time-series embeddings t0i ∈ RS . The fea0\\r\\nture vectors d0i , c0i and\\r\\nP ti are concatenated to form the final node embeddings\\r\\nF\\r\\nni ∈ R , where F = Fk ⊂[D0 ,C 0 ,S 0 ] Fk , for each of the N nodes.\\r\\nGraphormer Module: The backbone of our model comprises multiple graph transformer layers [25]. Graphormer uses attention between all nodes in the graph.\\r\\nTo incorporate the graph structure, structural encodings are used, which encode\\r\\nin and out degrees of the nodes, the distance between nodes, and edge features.\\r\\nPre-training and Fine-Tuning: We propose an unsupervised pre-training\\r\\ntechnique on the same input features as for downstream tasks, but without using labels Y. Instead, we randomly mask a fixed percentage of feature values for\\r\\nevery record ri and optimize the model to predict these values. During masking,\\r\\nstatic features are replaced by a randomly selected fixed value called ‘mask token’. For time-series features, we add a binary column per feature to the input\\r\\nvector, showing masked hours, and replace the value with zero. We optimize the\\r\\nmodel using (binary) cross entropy loss for discrete and mean squared error loss\\r\\nfor continuous features. A model for fine-tuning is initialized using the encoder\\r\\nweights learned during pre-training and random weights for the decoder. Then\\r\\nthe model is fine-tuned for the task T .\\r\\n\\r\\n3\\r\\n\\r\\nExperiments and Results\\r\\n\\r\\nWe use two publicly available medical data sets: TADPOLE [13] and MIMICIII [7]. They differ in size, the targeted prediction task, and the type of input\\r\\nfeatures, allowing comprehensive testing and evaluation of our method.\\r\\n3.1\\r\\n\\r\\nDatasets description:\\r\\n\\r\\nTADPOLE [13] contains 564 patients from the Alzheimer’s Disease Neuroimaging Initiative (ADNI). We use twelve features, which the TADPOLE challenge\\r\\nclaims are informative. They include discrete cognitive test results, demographics, and continuous features extracted from MR and PET imaging, normalized\\r\\nbetween zero and one. The task is to classify the patients into the groups Cognitive Normal (CN), Mild Cognitive Impairment (MCI), or Alzheimer’s Disease\\r\\n(AD). We only use data from patients’ first visits to avoid leakage of information.\\r\\nGraph Construction: We construct a k-NN graph with k=5, dependent on the\\r\\nmean similarity (S) between\\r\\n( the features. For the demographics, age, gender and\\r\\nP 1 if fi = fj else 0\\r\\napoe4, Sdem (ri , rj ) =\\r\\n÷ 3 where, f =(apoe4,\\r\\n1 if |agei − agej | ≤ 2 else 0\\r\\ngender). For the cognitive test results di (ordinal features), and ci (continuous imaging features),\\r\\nwe calculate the respective normalized L2 distances:\\r\\nP\\r\\nP\\r\\nf ∈di ||fri −frj ||\\r\\nand Simg (ri , rj ) = sig( f ∈ci ||fri − frj ||). The\\r\\nScog (ri , rj ) =\\r\\nmax(di )\\r\\noverall similarity S(ri , rj ) is then given as mean of Sdem , Scog and Simg .\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n5\\r\\n\\r\\nPre-Training Configuration: During pre-training on TADPOLE, we randomly\\r\\nmask 30% of the medical features (APOE4, cognitive tests, and imaging features) in each sample. The masking ratio of 30% was chosen experimentally.\\r\\nMIMIC-III [7] is a large EHR dataset of patient records with various static\\r\\nand time-series data collected over the patient’s stay. We use the pre-processed\\r\\ndataset published by McDermott et al. [14]. It includes 19.7 K patients that are\\r\\nat least 15 years old and stayed 24 hours or more in the ICU. The features include\\r\\ndemographics, measurements from bed-side monitoring and lab tests in hourly\\r\\ngranularity (continuous), and binary features stating if different treatments were\\r\\napplied in each hour. In total we have 76 features. We use linear interpolation\\r\\nto impute missing measurements. Fine-tuning is evaluated on Length-of-Stay\\r\\n(LOS) prediction as defined in [14]. The input encompasses the first 24 hours of\\r\\neach patient’s stay, and the goal is to predict if a patient will stay longer than\\r\\nthree days or not.\\r\\nGraph Construction: It is computationally infeasible to process a graph containing all patients. Thus, we create sub-graphs with 500 patients each, which\\r\\nfit into memory, each containing train, validation and test patients. We split\\r\\nrandomly as we do not want to make assumptions on which types of patients\\r\\nthe model should see, but learn this via the attention in the graph transformer.\\r\\nGiven the time-series of the measurement features f, we form feature descriptors\\r\\nfd = (mean(f ), std(f ), min(f ), max(f )) per patient and feature, where d equals\\r\\nthe 56 measurement features. We then compute the averagePsimilarity over all\\r\\n||fr −fr ||\\r\\n\\r\\nfeatures fd between two patients ri and rj : Sim(ri , rj ) = f ∈fd |fd |i j and\\r\\nbuild a k-NN graph with k=5.\\r\\nPre-Training Configuration: On MIMIC-III, we perform masking on the timeseries features from measurement and treatment data. Pre-training is performed\\r\\nover data from the first 24 hours of the patient’s stay. We compute the loss only\\r\\nover measured values, not over interpolated ones. Masking ratios are chosen\\r\\nexperimentally. We compare two types of masking:\\r\\nFeature Masking (FM): We randomly select 30% of the features per patient and mask the full 24 hours of the time-series. The model can not see past or\\r\\nfuture values, only other features and patients, aiming to force an understanding\\r\\nof relations between features and patients to infer masked features.\\r\\nBlock-wise Masking (BM): Instead of the full features, we mask a random\\r\\nblock of 6 hours within the 24-hour time-series in 100% of the features. Here,\\r\\nthe model can access past and future values to make a prediction. Thus, it can\\r\\nlearn to understand temporal context during pre-training.\\r\\n3.2\\r\\n\\r\\nExperimental Setup\\r\\n\\r\\nGiven a pre-trained model, we compare the results of fine-tuning it, with training the same but randomly initialized model from scratch. We manually tuned\\r\\nhyper-parameters per dataset separately for pre-training, from scratch training,\\r\\nand fine-tuning. To simulate scenarios with limited labeled data, we measure the\\r\\nmodel performance at different label ratios, meaning different amounts of labels\\r\\n\\r\\n\\x0c\\n\\n6\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\n(1%, 5%, 10%, 50%, 100%) are used for training or fine-tuning. For pre-training\\r\\nalways the full training data is used.\\r\\nImplementation Details All experiments are implemented in PyTorch, performed on a TITAN Xp GPU with 12GB VRAM, and optimized with the Adam\\r\\noptimizer [10]. For cross-validation, pre-training is performed separately per\\r\\nfold. The model comprises four Graphormer layers for TADPOLE and eight for\\r\\nMIMIC-III. For TADPOLE, we pre-train for 6000 epochs with a LR of 1e-5. We\\r\\ntrain task prediction for 1200 epochs with a polynomial decaying LR (1e-5 to 5e6) to train from scratch and a LR of 5e-6 for fine-tuning. When fine-tuning with\\r\\n1% labels, we reduce the epochs to 200. All results are computed with 10-fold\\r\\ncross-validation. For MIMIC-III, we pre-train for 3000 epochs with a polynomial\\r\\ndecaying LR (1e-3 to 1e-4). We train for 1100 epochs with a LR of 1e-4 from\\r\\nscratch, or fine-tune for 600 epochs with a LR of 1e-5. For a fair comparison with\\r\\nthe state of the art, results are averaged over six folds, each with an 80-10-10\\r\\nsplit into train, validation and test data. The models are selected based on the\\r\\nvalidation sets, and performance is computed over the test sets.\\r\\n3.3\\r\\n\\r\\nResults\\r\\n\\r\\nComparative methods: Table 1 We compare our model to related work without any pre-training. On TADPOLE, we compare to DGM [3], which proposes to\\r\\nlearn an optimal population graph for the given task. Besides, one recent arxiv\\r\\npaper [8] further improves performance on TADPOLE by learning input feature\\r\\nimportance. However it is out of context for this work. We achieve comparable accuracy to DGM and outperform in terms of AUC, which is an important\\r\\nmetric for imbalanced datasets. For MIMIC-III, we compare our method to the\\r\\nEHR pre-training benchmark of McDermott et al. [14], which uses the same LOS\\r\\ndefinition and dataset. We significantly outperform the benchmark model. The\\r\\nresults show that the proposed architecture is a good fit for the task at hand.\\r\\nTable 1. Accuracy and AUC of the proposed method compared with DGM on TADPOLE and McDermott et al. [14] on MIMIC-III.\\r\\nTADPOLE\\r\\nModel\\r\\n\\r\\nACC\\r\\n\\r\\nMIMIC-III\\r\\nAUC\\r\\n\\r\\nModel\\r\\n\\r\\nACC\\r\\n\\r\\nAUC\\r\\n\\r\\nDGM [3] 92.91 ± 02.50 94.49 ± 03.70 McDermott [14] not reported 71.00 ± 1.00\\r\\nProposed 92.59 ± 3.64 96.96 ± 2.32\\r\\nProposed\\r\\n70.29 ± 1.10 76.17 ± 1.02\\r\\n\\r\\nEffect of pre-training: Table 2 The motivation of this experiment is to investigate the smallest amount of labels required during the fine-tuning of the\\r\\ndownstream task. The results emphasize the benefits of our unsupervised pretraining with limited labels. On TADPOLE the main benefit of pre-training\\r\\ncan be seen for settings with limited labels (1%, 5%, 10%), where performance\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n7\\r\\n\\r\\nimproves significantly. Moreover, AUC continues to improve for all ratios. For\\r\\nLOS on MIMIC-III, both metrics significantly improve for all label ratios compared to from scratch training. Further, for MIMIC-III we compare two types of\\r\\nmasking (BM, FM). We see that feature masking consistently outperforms blockwise masking. The performance improvements achieved through pre-training on\\r\\nMIMIC-III are significantly higher than in the benchmark [14]. Moreover, we see\\r\\nimprovements until the full dataset size and not only for limited labels. Further\\r\\nthe pre-trained models have a lower standard deviation, indicating higher stability.\\r\\nAblation experiments: Table 3 We perform several ablation studies to evalTable 2. Performance of the proposed model in accuracy and AUC trained from scratch\\r\\n(SC) or fine-tuned after pre-training (FT) for different label ratios. For MIMIC-III we\\r\\nadditionally compare the block-wise (BM) and feature masking (FM) to each other.\\r\\nTADPOLE\\r\\nSize Metric\\r\\n\\r\\nSC\\r\\n\\r\\nMIMIC-III\\r\\nFT\\r\\n\\r\\nSC\\r\\n\\r\\nFT: BM\\r\\n\\r\\nFT: FM\\r\\n\\r\\n1%\\r\\n\\r\\nACC 59.42 ± 8.40 78.89 ± 2.45 59.86 ± 2.11 63.22 ± 2.39 65.25 ± 1.09\\r\\nAUC 68.72 ± 12.74 93.49 ± 2.07 62.98 ± 2.55 68.07 ± 1.80 69.90 ± 1.26\\r\\n\\r\\n5%\\r\\n\\r\\nACC\\r\\nAUC\\r\\n\\r\\n78.23 ± 6.83 83.37 ± 6.29 64.79 ± 1.16 66.82 ± 0.89 68.66 ± 0.73\\r\\n87.23 ± 4.91 94.99 ± 2.55 68.85 ± 1.53 72.27 ± 1.19 73.97 ± 1.28\\r\\n\\r\\n10% ACC\\r\\nAUC\\r\\n\\r\\n87.00 ± 4.86 87.71 ± 4.65 64.72 ± 0.45 67.71 ± 0.69 69.42 ± 1.23\\r\\n92.03 ± 3.39 95.96 ± 2.51 68.97 ± 0.66 73.55 ± 0.60 75.09 ± 1.29\\r\\n\\r\\n50% ACC 92.41 ± 3.69 91.52 ± 3.76 67.41 ± 1.31 69.98 ± 0.69 70.85 ± 0.92\\r\\nAUC 96.06 ± 2.48 97.23 ± 1.94 72.53 ± 1.08 76.02 ± 0.87 76.86 ± 1.47\\r\\n100% ACC 92.59 ± 3.64 92.24 ± 3.47 70.29 ± 1.10 70.73 ± 0.70 71.44 ± 1.25\\r\\nAUC 96.96 ± 2.23 97.52 ± 1.67 76.17 ± 1.02 76.20 ± 0.54 77.78 ± 1.31\\r\\n\\r\\nuate different parts of our proposed model on pre- and task training.\\r\\nEffect of Graphormer: We replace the Graphormer module with a simple MLP\\r\\nor GCN layer and train the model from scratch on the full dataset (Table 3 a)).\\r\\nWe see a clear benefit from using Graphormer compared to MLP and GCN. For\\r\\nTADPOLE, MLP reaches slightly better performance in terms of AUC as TADPOLE is a relatively small and easy dataset. The effect of the node level attention\\r\\nmechanism to all nodes given by Graphormer is clearly visible when compared\\r\\nto GCN. Further, we perform pre-training followed by fine-tuning for the MLP\\r\\nmodel (Table 3 b)). Our proposed unsupervised pre-training method proves to be\\r\\nbeneficial also for the MLP model, but the effects are less as for our proposed architecture. Table 3 c) shows masked imputation performance during pre-training,\\r\\nmeasured by RMSE for continuous (imaging/ measurements) and accuracy or\\r\\nF1 for discrete features (apoe4+cognitive tests/treatments). Here the proposed\\r\\nmodel outperforms the MLP model, explaining why pre-training has a greater\\r\\n\\r\\n\\x0c\\n\\n8\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\neffect for it. In summary we see a positive effect of using Graphormer over MLP\\r\\nfor solving the pre-training task and improving fine-tuning performance.\\r\\nEffect of Transformer: For MIMIC-III, Transformer is inserted in the encoder\\r\\nto deal with time series data. To test the transformer layers, we remove this\\r\\ncomponent and train the model from scratch on the full dataset, resulting in a\\r\\nreduction of accuracy from 70.29 to 69.39% and AUC from 76.17 to 75.03%. This\\r\\nshows that the transformer layers are helpful for processing time-series inputs.\\r\\nThe model needs to predict time-dependent outputs for pre-training on MIMICIII, for which the transformer layers are important, as they can understand the\\r\\ntemporal context. To investigate the effect of transformer during pre-training,\\r\\nwe remove the transformer layer and replace the Graphormer module with an\\r\\nMLP. We observe a reduction in the performance by 0.45% for ACC and 3.03% in\\r\\nAUC through pre-training. Accordingly, removing the transformer layer results\\r\\nin a 0.049 larger RMSE and a 3.9% lower F1 score in pre-training.\\r\\nTable 3. Ablations to test Graphormer module by replacing it with an MLP/GCN\\r\\nlayer, a) downstream task performance trained from scratch b) results of fine-tuning\\r\\n(FT) on limited labels (TADPOLE 1%, MIMIC-III 10%), compared to training from\\r\\nscratch (SC) c) pre-training task performance, multi-class accuracy for cognitive tests\\r\\nuses feature-dependent error margins in which predictions are considered correct. The\\r\\nsmall number of imaging features might cause the low std of 0.006/0.008.\\r\\nTADPOLE\\r\\n\\r\\na\\r\\n\\r\\nb\\r\\n\\r\\nModel\\r\\n\\r\\nACC\\r\\n\\r\\nAUC\\r\\n\\r\\nMLP\\r\\nGCN\\r\\nProposed\\r\\n\\r\\n91.14 ± 02.62 97.77 ± 01.59 67.25 ± 01.11 72.69 ± 00.97\\r\\n74.27 ± 06.41 89.89 ± 04.12 68.74 ± 01.50 72.64 ± 01.10\\r\\n92.59 ± 03.64 96.96 ± 02.23 70.29 ± 01.10 76.17 ± 01.02\\r\\n\\r\\nMLP SC\\r\\n54.20 ± 08.74\\r\\nMLP FT\\r\\n71.27 ± 09.76\\r\\nProposed SC 59.42 ± 08.40\\r\\nProposed FT 78.89 ± 02.45\\r\\nRMSE\\r\\n\\r\\nc\\r\\n\\r\\n4\\r\\n\\r\\nMIMIC-III\\r\\n\\r\\nMLP\\r\\nProposed\\r\\n\\r\\n00.15 ± 0.008\\r\\n0.14 ± 0.006\\r\\n\\r\\nACC\\r\\n\\r\\nAUC\\r\\n\\r\\n70.41 ± 11.41\\r\\n89.25 ± 06.53\\r\\n68.72 ± 12.74\\r\\n93.49 ± 02.07\\r\\n\\r\\n63.78 ± 00.74\\r\\n64.71 ± 00.84\\r\\n64.72 ± 00.45\\r\\n69.42 ± 01.23\\r\\n\\r\\nACC\\r\\n\\r\\nRMSE\\r\\n\\r\\n62.58 ± 04.87 00.79 ± 0.023\\r\\n63.23 ± 04.25 0.78 ± 0.011\\r\\n\\r\\n67.72 ± 00.68\\r\\n67.94 ± 01.20\\r\\n68.97 ± 00.66\\r\\n75.09 ± 01.29\\r\\nF1\\r\\n81.49 ± 00.35\\r\\n81.58 ± 00.41\\r\\n\\r\\nConclusion\\r\\n\\r\\nIn this paper, we present an unsupervised pre-training method based on masked\\r\\nimputation, significantly improving prediction results. We propose a graph transformer based architecture for learning on population graphs built from heterogeneous EHR data. We show the superiority of our pipeline in both pre-training\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n9\\r\\n\\r\\nand various prediction tasks for two datasets, TADPOLE and MIMIC-III. Pretraining helps for all dataset sizes but especially in scenarios where only a limited\\r\\namount of labeled data is used for fine-tuning. Our pre-training method is unsupervised and therefore independent from the end task, and further it is well\\r\\nsuited for transfer learning. This work opens the path for the community to deals\\r\\nwith small dataset specially with limited labels.\\r\\n\\r\\nReferences\\r\\n1. Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv\\r\\npreprint arXiv:2106.08254 (2021)\\r\\n2. Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., Rueckert, D.: Selfsupervised learning for medical image analysis using image context restoration.\\r\\nMedical image analysis 58, 101539 (2019)\\r\\n3. Cosmo, L., Kazi, A., Ahmadi, S.A., Navab, N., Bronstein, M.: Latent-graph learning for disease prediction. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer (2020)\\r\\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: Burstein, J., Doran, C., Solorio,\\r\\nT. (eds.) NAACL-HLT (1). Association for Computational Linguistics (2019)\\r\\n5. Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., Leskovec, J.: Strategies\\r\\nfor pre-training graph neural networks. arXiv preprint arXiv:1905.12265 (2019)\\r\\n6. Hu, Z., Dong, Y., Wang, K., Chang, K.W., Sun, Y.: Gpt-gnn: Generative pretraining of graph neural networks. In: Proceedings of the 26th ACM SIGKDD\\r\\nInternational Conference on Knowledge Discovery & Data Mining (2020)\\r\\n7. Johnson, A.E., Pollard, T.J., Shen, L., Lehman, L.w.H., Feng, M., Ghassemi, M.,\\r\\nMoody, B., Szolovits, P., Anthony Celi, L., Mark, R.G.: Mimic-iii, a freely accessible\\r\\ncritical care database. Scientific data 3(1) (2016)\\r\\n8. Kazi, A., Farghadani, S., Navab, N.: Ia-gcn: Interpretable attention based graph\\r\\nconvolutional network for disease prediction. arXiv preprint arXiv:2103.15587\\r\\n(2021)\\r\\n9. Kazi, A., Shekarforoush, S., Kortuem, K., Albarqouni, S., Navab, N., et al.: Selfattention equipped graph convolutions for disease prediction. In: 2019 IEEE 16th\\r\\nInternational Symposium on Biomedical Imaging (ISBI 2019). IEEE (2019)\\r\\n10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\\r\\narXiv:1412.6980 (2014)\\r\\n11. Li, Y., Rao, S., Solares, J.R.A., Hassaine, A., Ramakrishnan, R., Canoy, D., Zhu,\\r\\nY., Rahimi, K., Salimi-Khorshidi, G.: Behrt: transformer for electronic health\\r\\nrecords. Scientific reports 10(1) (2020)\\r\\n12. Lu, Y., Jiang, X., Fang, Y., Shi, C.: Learning to pre-train graph neural networks.\\r\\nAAAI (2021)\\r\\n13. Marinescu, R.V., Oxtoby, N.P., Young, A.L., Bron, E.E., Toga, A.W., Weiner,\\r\\nM.W., Barkhof, F., Fox, N.C., Klein, S., Alexander, D.C., et al.: Tadpole challenge: prediction of longitudinal evolution in alzheimer’s disease. arXiv preprint\\r\\narXiv:1805.03909 (2018)\\r\\n14. McDermott, M., Nestor, B., Kim, E., Zhang, W., Goldenberg, A., Szolovits, P.,\\r\\nGhassemi, M.: A comprehensive ehr timeseries pre-training benchmark. In: Proceedings of the Conference on Health, Inference, and Learning (2021)\\r\\n\\r\\n\\x0c\\n\\n10\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\n15. Mitani, A.A., Haneuse, S.: Small data challenges of studying rare diseases. JAMA\\r\\nnetwork open 3(3) (2020)\\r\\n16. Ouyang, C., Biffi, C., Chen, C., Kart, T., Qiu, H., Rueckert, D.: Self-supervision\\r\\nwith superpixels: Training few-shot medical image segmentation without annotation. In: European Conference on Computer Vision. pp. 762–780. Springer (2020)\\r\\n17. Parisot, S., Ktena, S.I., Ferrante, E., Lee, M., Guerrero, R., Glocker, B., Rueckert,\\r\\nD.: Disease prediction using graph convolutional networks: application to autism\\r\\nspectrum disorder and alzheimer’s disease. Medical image analysis 48 (2018)\\r\\n18. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders: Feature learning by inpainting. In: Proceedings of the IEEE conference on\\r\\ncomputer vision and pattern recognition (2016)\\r\\n19. Pollard, T.J., Johnson, A.E., Raffa, J.D., Celi, L.A., Mark, R.G., Badawi, O.: The\\r\\neicu collaborative research database, a freely available multi-center database for\\r\\ncritical care research. Scientific data 5(1) (2018)\\r\\n20. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training (2018)\\r\\n21. Rasmy, L., Xiang, Y., Xie, Z., Tao, C., Zhi, D.: Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease\\r\\nprediction. NPJ digital medicine 4(1) (2021)\\r\\n22. Rong, Y., Bian, Y., Xu, T., Xie, W., Wei, Y., Huang, W., Huang, J.: Grover: Selfsupervised message passing transformer on large-scale molecular data. Advances\\r\\nin Neural Information Processing Systems (2020)\\r\\n23. Shang, J., Ma, T., Xiao, C., Sun, J.: Pre-training of graph augmented transformers\\r\\nfor medication recommendation. arXiv preprint arXiv:1906.00346 (2019)\\r\\n24. Wang, S., McDermott, M.B., Chauhan, G., Ghassemi, M., Hughes, M.C., Naumann, T.: Mimic-extract: A data extraction, preprocessing, and representation\\r\\npipeline for mimic-iii. In: Proceedings of the ACM conference on health, inference,\\r\\nand learning (2020)\\r\\n25. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., Liu, T.Y.: Do\\r\\ntransformers really perform badly for graph representation? Advances in Neural\\r\\nInformation Processing Systems 34 (2021)\\r\\n26. Zebin, T., Rezvy, S., Chaussalet, T.J.: A deep learning approach for length of\\r\\nstay prediction in clinical settings from medical records. In: 2019 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology\\r\\n(CIBCB). IEEE (2019)\\r\\n27. Zhang, J., Zhang, H., Xia, C., Sun, L.: Graph-bert: Only attention is needed for\\r\\nlearning graph representations. arXiv preprint arXiv:2001.05140 (2020)\\r\\n\\r\\n\\x0c\",\n",
       " 'Unsupervised Salient Object Detection with Spectral Cluster Voting\\r\\n\\r\\narXiv:2203.12614v1 [cs.CV] 23 Mar 2022\\r\\n\\r\\nGyungin Shin1\\r\\nSamuel Albanie2\\r\\nWeidi Xie1,3\\r\\n1\\r\\nVisual Geometry Group, University of Oxford, UK\\r\\n2\\r\\nDepartment of Engineering, University of Cambridge, UK\\r\\n3\\r\\nShanghai Jiao Tong University, China\\r\\ngyungin@robots.ox.ac.uk\\r\\n\\r\\nAbstract\\r\\nIn this paper, we tackle the challenging task of unsupervised salient object detection (SOD) by leveraging spectral\\r\\nclustering on self-supervised features. We make the following contributions: (i) We revisit spectral clustering and\\r\\ndemonstrate its potential to group the pixels of salient objects; (ii) Given mask proposals from multiple applications\\r\\nof spectral clustering on image features computed from various self-supervised models, e.g., MoCov2, SwAV, DINO,\\r\\nwe propose a simple but effective winner-takes-all voting\\r\\nmechanism for selecting the salient masks, leveraging object priors based on framing and distinctiveness; (iii) Using the selected object segmentation as pseudo groundtruth\\r\\nmasks, we train a salient object detector, dubbed S ELF M ASK, which outperforms prior approaches on three unsupervised SOD benchmarks. Code is publicly available at\\r\\nhttps://github.com/NoelShin/selfmask\\r\\n\\r\\nFigure 1: In this work, we propose S ELF M ASK, a framework for salient object detection that employs no human\\r\\nannotation. The figure depicts example segmentations produced by our model on DUT-OMRON [70], DUTS-TE [61]\\r\\nand ECSSD [52], with blue and orange regions denoting the\\r\\nintersection and the difference between the predicted mask\\r\\nand ground-truth label, respectively. Despite no supervision, S ELF M ASK reliably segments a close approximation\\r\\nto the full spatial extent of salient regions. Best viewed in\\r\\ncolour.\\r\\n\\r\\n1. Introduction\\r\\nSalient object detection (SOD)1 , which aims to group\\r\\npixels that attract human visual attention, has been extensively studied in the field of computer vision due to its wide\\r\\nrange of applications such as photo cropping [55, 41], retargeting in images [19, 56] and video [50].\\r\\nIn the literature, early work tackled this problem by utilising low-level features (e.g., colour [15]) together with\\r\\npriors on salient regions in an object such as contrast priors [28], boundary priors [64] and centre priors [29]. Recent SOD models have approached this task from the perspective of representation learning, typically training deep\\r\\nneural networks (DNNs) on a large-scale dataset with manual annotations. However, the scalability of such supervised\\r\\nlearning approach is limited because it is a costly process to\\r\\n\\r\\ncollect ground-truth mask annotations.\\r\\nTo overcome the necessity of large-scale human annotation, many unsupervised methods for saliency detection/object segmentation have recently been proposed [7,\\r\\n13, 59, 60, 42, 73, 76, 46]. Despite these efforts, the gap\\r\\nbetween unsupervised and fully supervised SOD methods\\r\\nremains significant.\\r\\nInterestingly, however, it has been noted that recent selfsupervised models such as DINO [11] exhibit significant\\r\\npotential for object segmentation despite the fact that their\\r\\ntraining objective does not explicitly encourage pixel group-\\r\\n\\r\\n1 In contrast to object detection (which aims to localise and recognise\\r\\nobjects with bounding boxes), salient object detection aims to segment\\r\\nforeground objects by predicting pixel-wise masks for them.\\r\\n\\r\\n1\\r\\n\\r\\n\\x0c\\n\\ning. The focus of this work is to leverage this observation\\r\\nto propose a simple yet effective mechanism for extracting\\r\\nobject regions from self-supervised features that can be employed for the task of unsupervised salient object detection.\\r\\nTo this end, we explore the use of spectral clustering [51], a classical graph-theoretic clustering algorithm,\\r\\nand find that it can generate useful segmentation candidates\\r\\nacross a range of self-supervised features (i.e., DINO [11],\\r\\nMoCov2 [25], and SwAV [10]). Motivated by this finding,\\r\\nwe propose a simple winner-takes-all voting strategy to select the salient object masks among a collection of clusters\\r\\nproduced by repeated applications of spectral clustering to\\r\\nself-supervised features.2 In particular, we base our voting\\r\\nstrategy on two priors: The first is a framing prior that a\\r\\nsalient object should not occupy the full image height or\\r\\nwidth; The second is a distinctiveness prior that assumes\\r\\nthat salient regions are sufficiently distinctive that they will\\r\\nappear as clusters among an appropriately constructed collection of redundant re-clusterings of the data. We then\\r\\nshow that the selected salient masks can be employed as\\r\\npseudo-labels to train a saliency estimation network that\\r\\nachieves state-of-the-art results on a variety of benchmarks.\\r\\nIn summary, we make the following contributions:\\r\\n(i) We revisit spectral clustering and highlight its benefits\\r\\nover k-means clustering as a proposal mechanism to identify object regions from self-supervised features on three\\r\\npopular salient object detection (SOD) datasets; (ii) We propose an effective voting strategy to select the most salient\\r\\nobject mask in an image among multiple segmentations\\r\\ngenerated from different self-supervised features by leveraging saliency priors; (iii) Using the salient masks as pseudo\\r\\nground truth masks (pseudo-masks), we train an object segmentation model, S ELF M ASK, that outperforms the previous unsupervised saliency detection approaches on three\\r\\nSOD benchmarks.\\r\\n\\r\\ncent work exploring the use of self-supervised transformers [68, 11], and by the analysis provided by [11, 45]\\r\\nwho noted that the self-attention of Vision Transformers\\r\\n(ViT) [22] are capable of highlighting spatially coherent\\r\\nobject regions in their input. While prior work [72] has\\r\\ndemonstrated that self-supervised pretraining can be effective for semantic segmentation when coupled with end-toend supervised metric learning on the target dataset, we instead seek a simple way to exploit self-supervised features\\r\\nfor object segmentation without annotation.\\r\\n\\r\\n2.2. Unsupervised saliency detection\\r\\nSupervised object segmentation requires pixel-wise annotations which are time-consuming to acquire. Seeking to\\r\\navoid this cost, many attempts have been made to solve the\\r\\ntask in an unsupervised fashion. Prior to the dominance of\\r\\ndeep neural networks, a broad range of handcrafted methods\\r\\nwere proposed [36, 1, 30, 23, 69, 78, 16] based on one or\\r\\nmore priors relating to foreground regions within an image\\r\\nsuch as the contrast prior [28], centre prior [29], and boundary prior [64]. However, these handcrafted approaches suffer from poor performance relative to recent DNN-based\\r\\nmodels, described next.\\r\\nGenerative models. A common approach for DNN-based\\r\\nunsupervised object segmentation is to utilise generative adversarial networks (GANs) [24]. Specifically, given an image, a generator is adversarially trained to produce an object\\r\\nmask which will be used to composite a realistic image by\\r\\ncopying the corresponding object region in the image into\\r\\na background which is either synthesised [7] or taken from\\r\\na different image [2]. An alternative family of approaches\\r\\naims to discover a direction in the latent space of a pretrained GAN that can be used to segment foreground and\\r\\nbackground regions [59, 60, 42]. Then, a saliency detector\\r\\nis trained on a synthetic data set composed of pairs of images together with their foreground masks generated via the\\r\\ndiscovered latent space structure. In contrast, we seek to\\r\\nexploit representations learned via self-supervision by discriminative, rather than generative, models.\\r\\n\\r\\n2. Related work\\r\\nOur work relates to two themes in the literature:\\r\\nself-supervised representation learning and unsupervised\\r\\nsaliency detection. Each of these is discussed next.\\r\\n\\r\\nNoisy supervision with pseudo-labels. More closely related to our work, the use of weak pseudo saliency masks\\r\\nfor training a DNN has been proposed. SBF [73], the first\\r\\nattempt to train saliency detector without human annotation,\\r\\nproposed to train a model with superpixel-level pseudomasks generated by fusing weak saliency maps from multiple unsupervised methods (i.e., [75, 74, 53]). Similarly,\\r\\nUSD [76] aims to learn from diverse noisy pseudo-labels\\r\\nobtained via distinct unsupervised handcrafted methods in\\r\\nsuch a way that a saliency detector trained with the pseudomasks can predict a saliency map free from label noise.\\r\\nDeepUSPS [46] proposed to refine the pseudo-masks for\\r\\nimages produced by handcrafted saliency methods, by train-\\r\\n\\r\\n2.1. Self-supervised representation learning\\r\\nThere has been a great deal of interest in self-supervised\\r\\napproaches to learning visual representations that obviate the requirement for labels. These include techniques\\r\\nfor solving proxy tasks, such as predicting patch locations [20], patch discrimination [22], grouping through cooccurrence [27], colourisation [31], jigsaw puzzles [47],\\r\\ncommon fate principles [40], clustering [9, 4], and instance discrimination [65, 25]. Our work is inspired by re2 In this work, we use the terms cluster and mask interchangeably.\\r\\nSpecifically, by mask, we mean a (one-hot) mask which encodes the spatial\\r\\nextent of a cluster.\\r\\n\\r\\n2\\r\\n\\r\\n\\x0c\\n\\n3.2. Segmentation with spectral clustering\\r\\n\\r\\ning segmentation networks via a self-supervised iterative refinement process. The refined pseudo-masks are then combined from different handcrafted methods to train a final\\r\\nsegmentation network. In contrast to the methods above, we\\r\\nuse neither handcrafted saliency methods nor an iterative\\r\\nrefinement strategy, resulting in a simpler learning framework.\\r\\n\\r\\nConceptually, segmentation is obtained via spectral clustering [51] with pixel-wise image features by projecting the\\r\\nfeatures onto a representation space such that image partitions can be decided by directly comparing similarities between their corresponding features.\\r\\nConcretely, we first extract dense features from the image, i.e., F ∈ Rh×w×D with a pre-trained convolutionalor transformer-based encoder. Then, each feature vector\\r\\nfi ∈ RD in the dense feature map F can be seen as a vertex\\r\\nin an undirected graph with vertex set V = {f1 , . . . , fN } ∈\\r\\nRN ×D , where N = h · w. Each edge between two vertices\\r\\nfi and fj is associated with a non-negative weight wij ≥ 0\\r\\ndefined by feature similarity. In particular, the weighted adjacency matrix W of the graph is computed as\\r\\n\\r\\nObject segmentation properties of self-supervised vision\\r\\ntransformers. Another line of related work has sought to\\r\\ninvestigate the observation that self-supervised ViTs [11]\\r\\nexhibit object segmentation potential. LOST [54] propose\\r\\nto pick a seed patch from such a ViT that is likely to contain\\r\\npart of a foreground object, and then expand the seed patch\\r\\nto different patches sharing a high similarity with the seed\\r\\npatch. Concurrent work, TokenCut [63], proposes to use\\r\\nNormalised Cuts [51] to segment the salient object among\\r\\nthe final layer self-attention key features of ViT. S ELF M ASK differs from the TokenCut approach to salient object detection in two key ways: (1) While we similarly employ spectral clustering as part of our pipeline, we demonstrate the significant additional value of integrating cues\\r\\nfrom diverse re-clusterings via voting to bootstrap a pseudolabelling process; (2) Thanks to the flexibility of our clustering approach, we are able to leverage saliency cues from\\r\\nself-supervised convolutional neural networks (CNNs) as\\r\\nwell as ViT architectures, and show the benefits of doing\\r\\nso. We compare our approach with theirs in section 4.\\r\\n\\r\\nW = (wij ) =\\r\\n\\r\\nVVT\\r\\n∈ RN ×N\\r\\nkVkkVk\\r\\n\\r\\nwhere\\r\\nPN the degree of a vertex fi ∈ V is defined as di =\\r\\nj=1 wij , and the degree matrix D is defined with the degrees d1 , . . . , dn on the diagonal. Given the adjacency matrix W and the degree matrix D, the (un-normalised) graph\\r\\nLaplacian L is defined as:\\r\\nL=D−W\\r\\n\\r\\n(3)\\r\\n\\r\\nGiven L, we can solve the generalised eigenproblem:\\r\\nLu = λDu\\r\\n\\r\\n3. Method\\r\\n\\r\\n(4)\\r\\n\\r\\nwhere u ∈ RN and λ represent an eigenvector and its eigenvalue. We take the k eigenvectors with the lowest eigenvalues and form a matrix U ∈ RN ×k which has the eigenvectors as its columns.\\r\\nFinally, a set of clusters C is obtained by running kmeans algorithm on the row vectors of a matrix U (see supplementary for more detail), producing regions with all pixels from the corresponding cluster. Note that, at this stage,\\r\\nthe resulting clusters are composed of both object and background masks—the object mask itself will be selected by\\r\\nour selection strategy described next.\\r\\n\\r\\nIn this section, we begin by formalising the problem scenario (Sec. 3.1) and briefly summarise spectral clustering\\r\\n(Sec. 3.2). Then, we introduce our approach to address\\r\\nunsupervised salient object detection by selecting pseudo\\r\\nground truth masks via spectral clustering, and train a\\r\\nsaliency prediction network, called S ELF M ASK (Sec. 3.3).\\r\\n\\r\\n3.1. Problem formulation\\r\\nHere, we consider the task of unsupervised salient object detection (SOD), with the goal of training a segmenter (Φseg ) that seeks to partition the image into two disjoint groups, namely foreground and background. That is,\\r\\nΦseg (I; Θ) = Mseg ∈ {0, 1}H×W\\r\\n\\r\\n(2)\\r\\n\\r\\n3.3. Supervision with pseudo-mask from spectral\\r\\nclusters\\r\\n\\r\\n(1)\\r\\n\\r\\nHere, we first introduce our voting strategy for selecting\\r\\na salient mask from a set of spectral clusters from different\\r\\nfeatures and multiple k values (i.e., cluster numbers), which\\r\\nutilises a framing prior and distinctiveness prior. Then, we\\r\\ndescribe our model, S ELF M ASK, which is trained by using\\r\\nthe selected salient masks as pseudo-masks for supervision.\\r\\n\\r\\nwhere I ∈ RH×W ×3 refers to an input image, and Θ represents learnable parameters. Mseg denotes a binary segmentation mask, with 1s denoting a foreground region and 0s\\r\\ndenoting background.\\r\\nTraditionally this has been treated as a clustering problem where the key challenge lies in designing effective features for accurately describing salient regions. In this work,\\r\\nwe instead look for a simple yet effective solution by leveraging self-supervised visual representations.\\r\\n\\r\\n3.3.1\\r\\n\\r\\nSpectral cluster winner-takes-all voting\\r\\n\\r\\nTo choose the salient object among the mixture of foreground and background masks generated by spectral clus3\\r\\n\\r\\n\\x0c\\n\\nFigure 2: Overview of our approach. Given different self-supervised encoders, we first generate a set of pseudo-mask\\r\\ncandidates per image using spectral clustering before the training step. In the figure we show 12 masks from clusterings from\\r\\nthree different encoder features with k=4. We select the most salient mask among them via the proposed voting strategy\\r\\nand use it as a pseudo-mask for the image. Then we train our model to predict nq queries (i.e., predictions), all of which\\r\\nare encouraged to be similar to the salient mask. To make the model aware of the objectness of each prediction, we use the\\r\\nranking loss which encourages the objectness score of a prediction closer to the salient mask to be higher. At inference time,\\r\\nwe select the prediction with the highest objectness score. Please see the text for details.\\r\\ntering, we propose a voting strategy based on two observations: (1) The spatial extent of an object rarely occupies\\r\\nthe entire height and width of an image. (2) Salient object regions are likely to appear in multiple clusters across\\r\\ndifferent self-supervised features as well as with different\\r\\ncluster numbers k. In other words, among k clusters from\\r\\nan application of clustering, we assume that at least one\\r\\ncluster encodes an object region within the image and that\\r\\nthis holds for different features (e.g., DINO, MoCov2, or\\r\\nSwAV). We call these priors the framing prior and distinctiveness prior, respectively. Note that the framing prior\\r\\nbears a resemblance to the centre prior [29], which states\\r\\nthat salient objects are likely to be located near the center\\r\\nof an image, and the boundary prior [64], which presumes\\r\\nthat the foreground object rarely touches the boundary of an\\r\\nimage. However, the framing prior differs from these priors\\r\\nin that it is not related to a location of an object but rather\\r\\nthe scale of an object within an image.\\r\\n\\r\\nremaining masks as the final mask for salient objects (bottom of Fig. 2). There are two edge cases in the background\\r\\nelimination process we handle explicitly: (i) when no masks\\r\\nare left in the candidate set and (ii) when only two masks\\r\\nare left, sharing the same IoU. For the former case, we simply keep all the masks in the candidate set as every mask\\r\\nhighlights regions spanning the spatial extent of the image,\\r\\nbreaking the assumption of the framing prior. For the latter,\\r\\nwe break ties randomly to pick one of the two masks.\\r\\n3.3.2\\r\\n\\r\\nS ELF M ASK\\r\\n\\r\\nHere, we describe our model architecture, training, and inference procedure.\\r\\nArchitecture. We base our salient object detection network, called S ELF M ASK, on a variant of the MaskFormer\\r\\narchitecture [14] which was originally proposed for the semantic/instance segmentation task.\\r\\nS ELF M ASK has two different sets of outputs: mask predictions and objectness scores for each mask. In detail, our\\r\\nmodel comprises an image encoder, a pixel decoder, and a\\r\\ntransformer decoder (upper part of Fig. 2). The image encoder takes an image I ∈ RH×W ×3 as input and outputs\\r\\nfeature maps f (I) ∈ Rh×w×D . Then, the feature maps are\\r\\nfed into the pixel decoder to produce upsampled features\\r\\ng(f (I)) ∈ RH×W ×D . The feature maps are also passed\\r\\n\\r\\nTo use the framing prior and distinctiveness prior in practice, we first form a candidate set of masks by repeatedly\\r\\napplying spectral clustering to different features with multiple k values. Then, we treat masks whose spatial extent is\\r\\nas long as the width or height of the image as background\\r\\nmasks, and eliminate them from the candidate set. Finally,\\r\\nwe employ winner-takes-all voting: we pick the mask with\\r\\nthe highest average pair-wise similarity w.r.t. IoU among all\\r\\n4\\r\\n\\r\\n\\x0c\\n\\n4. Experiments\\r\\n\\r\\nto the transformer decoder which outputs nq per-mask embeddings by using the feature maps as keys and values and\\r\\nthe learnable embeddings as queries. The final mask predictions M are produced via matrix multiplication between the\\r\\nupsampled features and per-mask embeddings, followed by\\r\\nan element-wise sigmoid function σ (·):\\r\\nM = {Mi | Mi = σ (g (f (I)) qi ) , i = 1, ..., nq }\\r\\n\\r\\nIn this section, we first describe the datasets used in our\\r\\nexperiments (Sec. 4.1) and provide implementation details\\r\\n(Sec. 4.2). We then conduct ablation study (Sec. 4.3) and\\r\\nreport our results for salient object detection (Sec. 4.4).\\r\\n\\r\\n4.1. Datasets\\r\\n\\r\\n(5)\\r\\n\\r\\nWe use DUTS-TR [61], which contains 10,553 images,\\r\\nto train our model with the pseudo-masks generated by following Sec. 3.3. We emphasize that only images are used\\r\\nfor generating pseudo-masks and training, without the corresponding labels. For our ablation study and comparison\\r\\nto previous work, we consider five popular saliency datasets\\r\\nincluding DUT-OMRON [70], which comprises 5,168 images of varied content with ground-truth pixel level masks;\\r\\nDUTS-TE [61], containing 5,019 images selected from the\\r\\nSUN dataset [66] and ImageNet test set [18]; ECSSD [52]\\r\\nwhich contains 1,000 images that were selected to represent complex scenes; HKU-IS [32] which consists of 4,447\\r\\nscene images with foreground/background sharing the similar appearances; SOD [44] which contains 300 images with\\r\\nmany images having multiple salient objects.\\r\\n\\r\\nwhere qi ∈ RD denotes the ith query (i.e., per-mask embedding). For each mask Mi , an objectness score oi ∈ [0, 1]\\r\\nis estimated by feeding the corresponding per-mask embedding qi to a simple MLP with two hidden layers followed\\r\\nby a sigmoid function. Note that other encoder and decoder\\r\\narchitectures can also be used here.\\r\\nObjective. For training, we employ two objective functions: a mask loss and a ranking loss, denoted by Lmask and\\r\\nLrank , respectively. Given nq mask predictions for an image\\r\\nfrom the model, we encourage all predictions to be similar\\r\\nto the pseudo-mask. Specifically, following [14], we use the\\r\\nDice coefficient [43], which considers the class-imbalance\\r\\nbetween foreground and background regions within an image, as the mask loss. It is worth noting that, unlike [14],\\r\\nwe do not include the focal loss [34] in the mask loss since\\r\\nwe find that it hinders convergence.\\r\\nTo decide which prediction best highlights the salient\\r\\nregion in the image among the proposed candidates when\\r\\nnq > 1, we rank the predicted masks based on their objectness score. Specifically, we first re-order the indices of the\\r\\npredicted masks by their mask loss in ascending order such\\r\\nthat\\r\\nLmask (Mi , Mpseudo ) ≤ Lmask (Mj , Mpseudo )\\r\\n\\r\\n4.2. Implementation details\\r\\nNetworks. We use the ViT-S/8 architecture [21] for the encoder, a bilinear upsampler with a scale factor of 2 for the\\r\\npixel decoder, and 6 transformer layers [58] for the transformer decoder. For the MLP applied to per-mask queries\\r\\n(with a dimensionality of 384) that outputs a scalar value\\r\\nfor the objectness score, we use three fully-connected layers with a ReLU activation between them. We set the same\\r\\nnumber of units for the hidden nodes as the input (i.e., 384)\\r\\nand output a single value followed by a sigmoid.\\r\\n\\r\\n(6)\\r\\n\\r\\nfor any i < j, where Mpseudo denotes the target pseudomask for the image. Then, we enforce the objectness score\\r\\noi of the mask Mi to be higher than the scores oj for any\\r\\nj > i. As a consequence the model is encouraged to produce a higher score for a predicted mask that more closely\\r\\nresembles the pseudo-mask than other predictions. We instantiate this ranking loss as a hinge loss [17]:\\r\\n\\r\\nTraining details. We train our models for 12 epochs and\\r\\noptimise all parameters including the backbone encoder using AdamW [38] with a learning rate of 6e-6 and the Poly\\r\\nlearning rate policy [48, 67, 12]. For data augmentation,\\r\\nwe use random scaling with a scale range of [0.1, 1.0], random cropping to a size of 224×224 and random horizontal\\r\\nflipping with a probability of 0.5. In addition, photometric\\r\\ntransformations include random color jittering, color dropping, and Gaussian blurring are applied. We run each model\\r\\nwith three different seeds and report the average.\\r\\n\\r\\nnq −1 nq\\r\\n\\r\\nLrank =\\r\\n\\r\\nX X\\r\\n\\r\\nmax (0, oj − oi ) .\\r\\n\\r\\n(7)\\r\\n\\r\\ni=1 j>i\\r\\n\\r\\nOverall, our final objective function is as follows:\\r\\nL = Lmask + λLrank\\r\\n\\r\\n(8)\\r\\n\\r\\nMetrics. In our experiments, we report intersection-overunion (IoU), pixel accuracy (Acc) and maximal Fβ score\\r\\n(max Fβ ) with β 2 set to 0.3 following [63]. Please refer to\\r\\nthe supplementary for more details on these metrics.\\r\\n\\r\\nwhere λ is a weighting factor, which is set to 1.0 across our\\r\\nexperiments. Following [8, 14], we compute the loss for\\r\\noutputs from each layer of the transformer decoder.\\r\\nInference. During inference, given nq predicted masks for\\r\\nan image and their objectness score, we pick the mask with\\r\\nthe highest score as the salient object detection and binarise\\r\\nit with a fixed threshold of 0.5.\\r\\n\\r\\n4.3. Ablation study\\r\\nIn this section, we first conduct experiments to compare\\r\\nthe effectiveness of spectral clustering and k-means when\\r\\n5\\r\\n\\r\\n\\x0c\\n\\nFigure 3: Sample visualisations of the pseudo-masks and predictions from our model on the ECSSD, DUT-OMRON, and\\r\\nDUTS-TE benchmarks. From left to right, the input image, ground-truth mask, a pseudo-mask decided by the proposed\\r\\nvoting-based salient mask selection, and the prediction of our model are shown. Blue and orange coloured regions denote the\\r\\nintersection and difference between a ground-truth and a predicted mask. Best viewed in colour.\\r\\n\\r\\nModel\\r\\n\\r\\nArch.\\r\\n\\r\\nCluster.\\r\\n\\r\\nDUT-OMRON DUTS-TE ECSSD\\r\\nk={2, 3, 4} k={2, 3, 4} k={2, 3, 4}\\r\\n\\r\\nModel\\r\\n\\r\\nConvolutional Nets\\r\\n.375\\r\\n.387\\r\\n\\r\\n.415\\r\\n.454\\r\\n\\r\\n.500\\r\\n.627\\r\\n\\r\\nSwAV [10] ResNet50 k-means\\r\\nSwAV [10] ResNet50 spectral\\r\\n\\r\\n.399\\r\\n.401\\r\\n\\r\\n.444\\r\\n.458\\r\\n\\r\\n.542\\r\\n.590\\r\\n\\r\\nResNet [25] ResNet50 k-means\\r\\nResNet [25] ResNet50 spectral\\r\\n\\r\\nDUT-OMRON DUTS-TE ECSSD\\r\\nk={2, 3, 4} k={2, 3, 4} k={2, 3, 4}\\r\\n\\r\\nDINO [11] ViT-S/16 k-means\\r\\nDINO [11] ViT-S/16 spectral\\r\\n\\r\\n.377\\r\\n.394\\r\\n\\r\\n.392\\r\\n.417\\r\\n\\r\\n.541\\r\\n.577\\r\\n\\r\\nViT-S/8 k-means\\r\\nViT-S/8 spectral\\r\\n\\r\\n.369\\r\\n.398\\r\\n\\r\\n.377\\r\\n.411\\r\\n\\r\\n.551\\r\\n.587\\r\\n\\r\\n.337\\r\\n.310\\r\\n\\r\\n.354\\r\\n.327\\r\\n\\r\\n.444\\r\\n.437\\r\\n\\r\\n.411\\r\\n.400\\r\\n\\r\\n.542\\r\\n.551\\r\\n\\r\\nVision Transformer\\r\\nViT [21]\\r\\nViT [21]\\r\\n\\r\\nVision Transformer\\r\\n\\r\\nViT-S/16 k-means\\r\\nViT-S/16 spectral\\r\\n\\r\\n.394\\r\\n.380\\r\\n\\r\\nTable 2: Spectral clustering and k-means perform comparably for fully-supervised features. We report upper\\r\\nbound IoUs to provide a comparison of mask quality comparison between k-means and spectral clustering applied to\\r\\nfully-supervised features. We report the average of the results from k={2, 3, 4}.\\r\\n\\r\\nTable 1: Spectral clustering dominates k-means for selfsupervised features. We report upper bound IoUs to compare the quality of masks produced by k-means and spectral clustering on self -supervised features with two different\\r\\nencoder architectures (i.e., convolution- and transformerbased encoder). We report the average of the results from\\r\\nk={2, 3, 4}.\\r\\n\\r\\nformance of the clustering algorithms, we consider different\\r\\nk values from 2 to 4 and average the results. For the full results with each k value, please refer to the supplementary.\\r\\nAs shown in Tab. 1, we observe that object masks from\\r\\nspectral clustering consistently outperform k-means masks\\r\\nby a large margin. Interestingly, however, when using fullysupervised image encoders, the performance gain of spectral clustering diminishes (Tab. 2). These findings boil down\\r\\nto a simple summary: while using self-supervised visual\\r\\nrepresentations for grouping, spectral clustering is considerably superior to k-means, regardless of the choice of encoder architecture and self-supervised learning algorithm.\\r\\n\\r\\napplied to self-supervised image encoders. Next, we quantitatively verify the performance of our winner-takes-all voting strategy for foreground mask selection and compare it to\\r\\ndifferent saliency selection methods. Lastly, we investigate\\r\\nthe effect of different number of queries on S ELF M ASK.\\r\\n4.3.1\\r\\n\\r\\nCluster.\\r\\n\\r\\nConvolutional Nets\\r\\n\\r\\nMoCov2 [25] ResNet50 k-means\\r\\nMoCov2 [25] ResNet50 spectral\\r\\n\\r\\nDINO [11]\\r\\nDINO [11]\\r\\n\\r\\nArch.\\r\\n\\r\\nSpectral clustering vs k-means clustering\\r\\n\\r\\nWe compare spectral clustering against a k-means [37]\\r\\nclustering baseline on three salient object detection benchmarks. As the resulting segmentations from each algorithm\\r\\nare agnostic to foreground/background regions, we consider\\r\\na best-case evaluation for both algorithms. In detail, given a\\r\\ngroundtruth mask, we pick the cluster with the highest IoU\\r\\nw.r.t. the groundtruth. Such IoUs act as an upper-bound\\r\\nscore among the clusters (i.e., average best overlap in [3]).\\r\\nTo account for the effect of the cluster number k on per-\\r\\n\\r\\n4.3.2\\r\\n\\r\\nVoting for salient object masks\\r\\n\\r\\nHere, we conduct experiments to assess our voting method\\r\\nfor selecting a foreground mask among the mask candidates. Since these experiments include ablations across hyperparameter choices, we conduct them on the HKU-IS [32]\\r\\nand SOD [44], rather than the benchmarks used to compare\\r\\nto prior work.\\r\\nIn detail, we construct an initial corpus of mask candi6\\r\\n\\r\\n\\x0c\\n\\ndates by clustering different self-supervised features with\\r\\ndifferent number of clusters, as described in Sec. 3.2. For\\r\\nthis, we build the candidate set with spectral clusters from\\r\\ndifferent combinations of self-supervised features, e.g., MoCov2/DINO or SwAV/MoCov2. It is important to do so to\\r\\nallow our voting-based method to leverage the distinctiveness prior across different features. In addition, for each\\r\\ncombination, we experiment with 3 different k value settings: k=2, {2, 3} or {2, 3, 4} to account for various object\\r\\nscales, e.g., a lower k tends to cover large regions, while\\r\\na higher k segments smaller objects. We then evaluate the\\r\\nselected masks on the HKU-IS set. For reference we also\\r\\ncompute an upper bound IoU, which is computed in a similar way as done in the previous section.\\r\\n\\r\\nFeatures\\r\\n\\r\\nEffectiveness of clustering various self-supervised models with different number of clusters.\\r\\nAs shown in\\r\\nTab. 3, we make two observations: (i) IoU of both selected\\r\\nmasks and upper bound masks, denoted by pseudo-mask\\r\\nand UB improves by increasing k across all feature combinations; (ii) using all three features (i.e., DINO, MoCov2\\r\\nand SwAV) results in better pseudo-masks than using two\\r\\nof the three features (e.g., MoCov2 and SwAV). These support the distinctiveness prior, which assumes that at least\\r\\none cluster represents a foreground region, and its application to our voting-based saliency selection method.\\r\\n\\r\\nPseudo-mask UB\\r\\n\\r\\n7\\r\\n\\r\\nX\\r\\n\\r\\nX\\r\\n\\r\\n2\\r\\n2, 3\\r\\n2, 3, 4\\r\\n\\r\\n.508\\r\\n.561\\r\\n.580\\r\\n\\r\\n.562\\r\\n.626\\r\\n.658\\r\\n\\r\\nX\\r\\n\\r\\n7\\r\\n\\r\\nX\\r\\n\\r\\n2\\r\\n2, 3\\r\\n2, 3, 4\\r\\n\\r\\n.473\\r\\n.538\\r\\n.559\\r\\n\\r\\n.553\\r\\n.644\\r\\n.682\\r\\n\\r\\nX\\r\\n\\r\\nX\\r\\n\\r\\n7\\r\\n\\r\\n2\\r\\n2, 3\\r\\n2, 3, 4\\r\\n\\r\\n.459\\r\\n.536\\r\\n.566\\r\\n\\r\\n.546\\r\\n.648\\r\\n.688\\r\\n\\r\\nX\\r\\n\\r\\nX\\r\\n\\r\\nX\\r\\n\\r\\n2\\r\\n2, 3\\r\\n2, 3, 4\\r\\n\\r\\n.511\\r\\n.567\\r\\n.590\\r\\n\\r\\n.584\\r\\n.664\\r\\n.698\\r\\n\\r\\nTable 3: Forming a candidate set with various selfsupervised features and multiple k values improves IoU\\r\\nof both pseudo-masks and upper bound masks (UB).\\r\\nWe compare cases with different combinations of selfsupervised features and cluster numbers of k=2, {2, 3} or\\r\\n{2, 3, 4} on the HKU-IS benchmark.\\r\\nSelection\\r\\n\\r\\nEffectiveness of the proposed voting approaches. We\\r\\nfurther validate the effectiveness of our voting scheme by\\r\\ncomparing to different selection methods, i.e., random selection and a centre prior [29]-based strategy. Specifically,\\r\\nwe form a candidate set using DINO/MoCov2/SwAV features with k = {2, 3, 4}, which amounts to 27 masks in\\r\\ntotal. For the random strategy, we simply pick one of the\\r\\nmasks uniformly from the candidates. For the centre prior\\r\\nselection strategy, we choose a mask whose average Euclidean distance to the image centre from its constituent\\r\\npixel locations is lowest. In addition, we consider each\\r\\nmethod with or without utilising the framing prior to assess\\r\\nthe influence of filtering out background mask. We evaluate\\r\\nthe IoU of each case on the HKU-IS and SOD benchmarks.\\r\\nAs can be seen in Tab. 4, deploying the framing prior\\r\\nboosts IoU in all considered selection methods, with the\\r\\nproposed voting selection method performing best. The\\r\\nframing prior plays a crucial role in the voting process: voting without this prior performs much worse than its counterparts in both random and center-based selections on HKUIS, and performs similarly to the random strategy on SOD.\\r\\nThis is caused in large part by mistakes when selecting\\r\\nbackground masks as salient objects.\\r\\n4.3.3\\r\\n\\r\\nk\\r\\n\\r\\nDINO [11] MoCov2 [25] SwAV [10]\\r\\n\\r\\nFraming prior HKU-IS [32] SOD [44]\\r\\n\\r\\nrandom\\r\\n\\r\\n7\\r\\nX\\r\\n\\r\\n.206\\r\\n.464\\r\\n\\r\\n.197\\r\\n.277\\r\\n\\r\\ncenter\\r\\n\\r\\n7\\r\\nX\\r\\n\\r\\n.362\\r\\n.442\\r\\n\\r\\n.122\\r\\n.392\\r\\n\\r\\nvoting (ours)\\r\\n\\r\\n7\\r\\nX\\r\\n\\r\\n.081\\r\\n.590\\r\\n\\r\\n.200\\r\\n.447\\r\\n\\r\\nTable 4: Winner-takes-all voting and the framing prior\\r\\nboth significantly improve mask quality. We compare our\\r\\nvoting strategy to different selection strategies along with\\r\\nthe effect of framing prior under the IoU metric. Selection is\\r\\nperformed from a candidate set including DINO, MoCov2\\r\\nand SwAV features with k = {2, 3, 4}.\\r\\nthe effect of the number of queries nq in the Transformer decoder. For this, we train our model with nq ={5, 10, 20, 50,\\r\\n100} on DUTS-TR and evaluate performance on the HKUIS benchmark in terms of maxFβ for two settings, (i) using\\r\\nground-truth masks to pick the best mask out of nq mask\\r\\npredictions, denoted as the SelfMask upper bound (UB); (ii)\\r\\ntaking the mask with the highest object score, denoted SelfMask.\\r\\nAs shown in Figure 4, both SelfMask and SelfMask UB\\r\\nare fairly robust to the number of queries, initially increasing slightly with this hyperparameter (i.e., predictions) before degrading after 20 queries. We conjecture that this\\r\\nis because a handful of queries are enough to localise the\\r\\nsalient objects, while further predictions may make it chal-\\r\\n\\r\\nThe influence of the number of queries\\r\\n\\r\\nAs described in Sec. 3.3, we train S ELF M ASK using the selected salient masks as pseudo-masks. Here, we investigate\\r\\n7\\r\\n\\r\\n\\x0c\\n\\nModel\\r\\nHS [69]\\r\\nwCtr [78]\\r\\nWSC [33]\\r\\nDeepUSPS [46]\\r\\nBigBiGAN [60]\\r\\nE-BigBiGAN [60]\\r\\nMelas-Kyriazi et al. [42]\\r\\nLOST [54]\\r\\nLOST [54] + Bilateral solver [6]\\r\\nTokenCut [63]\\r\\nTokenCut [63] + Bilateral solver [6]\\r\\npseudo-masks (Ours)\\r\\nS ELF M ASK (Ours)\\r\\nS ELF M ASK (Ours) + Bilateral solver [6]\\r\\n\\r\\nDUT-OMRON [70]\\r\\nAcc\\r\\nIoU\\r\\nmaxFβ\\r\\n.843\\r\\n.433\\r\\n.561\\r\\n.838\\r\\n.416\\r\\n.541\\r\\n.865\\r\\n.387\\r\\n.523\\r\\n.779\\r\\n.305\\r\\n.414\\r\\n.856\\r\\n.453\\r\\n.549\\r\\n.860\\r\\n.464\\r\\n.563\\r\\n.883\\r\\n.509\\r\\n.797\\r\\n.410\\r\\n.473\\r\\n.818\\r\\n.489\\r\\n.578\\r\\n.880\\r\\n.533\\r\\n.600\\r\\n.897\\r\\n.618\\r\\n.697\\r\\n.811\\r\\n.403\\r\\n.901\\r\\n.582\\r\\n.680\\r\\n.919\\r\\n.655\\r\\n.852\\r\\n\\r\\nDUTS-TE [61]\\r\\nAcc\\r\\n.826\\r\\n.835\\r\\n.862\\r\\n.773\\r\\n.878\\r\\n.882\\r\\n.893\\r\\n.871\\r\\n.887\\r\\n.903\\r\\n.914\\r\\n.845\\r\\n.923\\r\\n.933\\r\\n\\r\\nIoU\\r\\n\\r\\nmaxFβ\\r\\n\\r\\n.369\\r\\n.392\\r\\n.384\\r\\n.305\\r\\n.498\\r\\n.511\\r\\n.528\\r\\n.518\\r\\n.572\\r\\n.576\\r\\n.624\\r\\n.466\\r\\n.626\\r\\n.660\\r\\n\\r\\n.504\\r\\n.522\\r\\n.528\\r\\n.425\\r\\n.608\\r\\n.624\\r\\n.611\\r\\n.697\\r\\n.672\\r\\n.755\\r\\n.750\\r\\n.882\\r\\n\\r\\nECSSD [52]\\r\\nAcc\\r\\n.847\\r\\n.862\\r\\n.852\\r\\n.795\\r\\n.899\\r\\n.906\\r\\n.915\\r\\n.895\\r\\n.916\\r\\n.918\\r\\n.934\\r\\n.893\\r\\n.944\\r\\n.955\\r\\n\\r\\nIoU\\r\\n\\r\\nmaxFβ\\r\\n\\r\\n.508\\r\\n.517\\r\\n.498\\r\\n.440\\r\\n.672\\r\\n.684\\r\\n.713\\r\\n.654\\r\\n.723\\r\\n.712\\r\\n.772\\r\\n.646\\r\\n.781\\r\\n.818\\r\\n\\r\\n.673\\r\\n.684\\r\\n.683\\r\\n.584\\r\\n.782\\r\\n.797\\r\\n.758\\r\\n.837\\r\\n.803\\r\\n.874\\r\\n.889\\r\\n.956\\r\\n\\r\\nTable 5: Comparison to the state-of-the-art unsupervised saliency detection methods on 3 salient object detection\\r\\nbenchmarks. For all metrics, higher number indicates better results. The best score per column is highlighted in bold. We\\r\\nobserve that S ELF M ASK yields improved performance over prior state-of-the-art approaches across all benchmarks.\\r\\n\\r\\nmaximal F\\r\\n\\r\\n0.90\\r\\n\\r\\nthe best salient mask.\\r\\n\\r\\nSelfMask\\r\\nSelfMask (UB)\\r\\n\\r\\n4.5. Broader Impact\\r\\n0.85\\r\\n\\r\\n0.80 5 10\\r\\n\\r\\n20\\r\\n\\r\\n50\\r\\nNumber of queries\\r\\n\\r\\nThis work contributes a new framework to deliver performant unsupervised salient object detection. As such, it\\r\\noffers the potential to underpin a range of societally beneficial applications that are bottlenecked by annotation costs.\\r\\nThese include improved low-cost medical image segmentation, crop measurement from aerial imagery, and wildlife\\r\\nmonitoring. However, low-cost segmentation is a powerful\\r\\ndual-use technology, and we caution against its deployment\\r\\nas a tool for unlawful surveillance and oppression.\\r\\n\\r\\n100\\r\\n\\r\\nFigure 4: Effect of number of queries on the performance of SelfMask on the HKU-IS dataset. The model’s\\r\\nprediction and its upper bound, denoted by SelfMask and\\r\\nSelfMask (UB) each, are shown.\\r\\n\\r\\n5. Conclusion\\r\\n\\r\\nlenging to appropriately rank the objectness of each prediction. For this reason, in the section that follows, we consider S ELF M ASK with 20 queries, and pick the query with\\r\\nthe highest objectness as our prediction during inference.\\r\\n\\r\\nIn this work, we address the challenging problem of unsupervised salient object detection (SOD). For this, we first\\r\\nobserve that self-supervised features exhibit significantly\\r\\ngreater object segmentation potential with spectral clustering than with k-means. Inspired by this observation, we extract foreground regions among multiple masks generated\\r\\nwith multiple types of features, and varying cluster numbers based on winner-takes-all voting. By using the selected\\r\\nmasks as pseudo-masks, we train a saliency detection network and show promising results compared to previous unsupervised methods on various SOD benchmarks.\\r\\n\\r\\n4.4. Comparison to state-of-the-art unsupervised\\r\\nsaliency detection methods\\r\\nTo compare with existing works on unsupervised SOD,\\r\\nwe evaluate on three popular SOD benchmarks in terms\\r\\nof Acc., IoU, and maxFβ . Following [63], we also report results after post-processing predictions with the bilateral solver [6]. As shown in Tab. 5, while the pseudomasks from spectral cluster voting already perform reasonably well compared to previous models, our self-trained\\r\\nmodel outperforms all existing approaches on all benchmarks. This suggests both that the model can learn to generalise effectively from noisy masks, and that the objectness\\r\\nscore trained with the ranking loss is effective for picking\\r\\n\\r\\nAcknowledgements. GS is supported by AI Factory, Inc.\\r\\nin Korea. WX is supported by Visual AI (EP/T028572/1).\\r\\nSA would like to thank Z. Novak and N. Novak for enabling\\r\\nhis contribution. GS would like to thank Jaesung Huh for\\r\\nproof-reading.\\r\\n8\\r\\n\\r\\n\\x0c\\n\\nReferences\\r\\n\\r\\n[20] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In\\r\\nICCV, 2015.\\r\\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\r\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\r\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\r\\nworth 16x16 words: Transformers for image recognition at\\r\\nscale. In ICLR, 2021.\\r\\n[22] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative\\r\\nunsupervised feature learning with exemplar convolutional\\r\\nneural networks. TPAMI, 2015.\\r\\n[23] Stas Goferman, Lihi Zelnik-Manor, and Ayellet Tal.\\r\\nContext-aware saliency detection. TPAMI, 2012.\\r\\n[24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\r\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\r\\nYoshua Bengio. Generative adversarial nets. In Advances in\\r\\nNeural Information Processing Systems, 2014.\\r\\n[25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\r\\nGirshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\\r\\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\r\\nDeep residual learning for image recognition. In CVPR,\\r\\n2016.\\r\\n[27] Phillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H\\r\\nAdelson. Learning visual groups from co-occurrences in\\r\\nspace and time. arXiv:1511.06811, 2015.\\r\\n[28] L. Itti, C. Koch, and E. Niebur. A model of saliency-based\\r\\nvisual attention for rapid scene analysis. TPAMI, 1998.\\r\\n[29] Tilke Judd, Krista Ehinger, Frédo Durand, and Antonio Torralba. Learning to predict where humans look. In ICCV,\\r\\n2009.\\r\\n[30] Dominik A. Klein and Simone Frintrop. Center-surround\\r\\ndivergence of feature statistics for salient object detection.\\r\\nIn ICCV, 2011.\\r\\n[31] Gustav Larsson,\\r\\nMichael Maire,\\r\\nand Gregory\\r\\nShakhnarovich. Colorization as a proxy task for visual\\r\\nunderstanding. In CVPR, 2017.\\r\\n[32] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features. In CVPR, 2015.\\r\\n[33] Nianyi Li, Bilin Sun, and Jingyi Yu. A weighted sparse coding framework for saliency detection. In CVPR, 2015.\\r\\n[34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\\r\\nPiotr Dollar. Focal loss for dense object detection. In ICCV,\\r\\n2017.\\r\\n[35] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng,\\r\\nand Jianmin Jiang. A simple pooling-based design for realtime salient object detection. In CVPR, 2019.\\r\\n[36] Tie Liu, Jian Sun, Nan-Ning Zheng, Xiaoou Tang, and\\r\\nHeung-Yeung Shum. Learning to detect a salient object. In\\r\\nCVPR, 2007.\\r\\n[37] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129–137, 1982.\\r\\n[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\r\\nregularization. In ICLR, 2019.\\r\\n\\r\\n[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada,\\r\\nand Sabine Susstrunk. Frequency-tuned salient region detection. In CVPR, 2009.\\r\\n[2] Relja Arandjelović and Andrew Zisserman. Object discovery\\r\\nwith a copy-pasting gan. arXiv:1905.11369, 2019.\\r\\n[3] Pablo Arbeláez, Jordi Pont-Tuset, Jonathan Barron, Ferran Marques, and Jitendra Malik. Multiscale combinatorial\\r\\ngrouping. In CVPR, 2014.\\r\\n[4] Yuki Markus Asano, Christian Rupprecht, and Andrea\\r\\nVedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020.\\r\\n[5] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:\\r\\nBERT pre-training of image transformers. In ICLR, 2022.\\r\\n[6] Jonathan T. Barron and Ben Poole. The fast bilateral solver.\\r\\nIn ECCV, 2016.\\r\\n[7] Adam Bielski and Paolo Favaro. Emergence of object segmentation in perturbed generative models. In NeurIPS, 2019.\\r\\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\r\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In ECCV, 2020.\\r\\n[9] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\\r\\nMatthijs Douze. Deep clustering for unsupervised learning\\r\\nof visual features. In ECCV, 2018.\\r\\n[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In\\r\\nNeurIPS, 2020.\\r\\n[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou,\\r\\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In\\r\\nICCV, 2021.\\r\\n[12] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\\r\\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\\r\\nseparable convolution for semantic image segmentation. In\\r\\nECCV, 2018.\\r\\n[13] Mickaël Chen, Thierry Artières, and Ludovic Denoyer. Unsupervised object segmentation by redrawing. In NeurIPS,\\r\\n2019.\\r\\n[14] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Perpixel classification is not all you need for semantic segmentation. In NeurIPS, 2021.\\r\\n[15] Ming-Ming Cheng, Niloy J. Mitra, Xiaolei Huang, Philip\\r\\nH. S. Torr, and Shi-Min Hu. Global contrast based salient\\r\\nregion detection. TPAMI, 2015.\\r\\n[16] Ming-Ming Cheng, Niloy J. Mitra, Xiaolei Huang, Philip\\r\\nH. S. Torr, and Shi-Min Hu. Global contrast based salient\\r\\nregion detection. TPAMI, 2015.\\r\\n[17] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 1995.\\r\\n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\r\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\r\\ndatabase. In CVPR, 2009.\\r\\n[19] Yuanyuan Ding, Jing Xiao, and Jingyi Yu. Importance filtering for image retargeting. In CVPR, 2011.\\r\\n\\r\\n9\\r\\n\\r\\n\\x0c\\n\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia\\r\\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\\r\\n[59] Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan latent space. In\\r\\nICML, 2020.\\r\\n[60] Andrey Voynov, Stanislav Morozov, and Artem Babenko.\\r\\nObject segmentation without labels with large-scale generative models. In ICML, 2021.\\r\\n[61] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,\\r\\nDong Wang, Baocai Yin, and Xiang Ruan. Learning to detect salient objects with image-level supervision. In CVPR,\\r\\n2017.\\r\\n[62] Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen,\\r\\nHaibin Ling, and Ruigang Yang. Salient object detection\\r\\nin the deep learning era: An in-depth survey. TPAMI, 2021.\\r\\n[63] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James\\r\\nCrowley, and Dominique Vaufreydaz. Self-supervised transformers for unsupervised object discovery using normalized\\r\\ncut. In CVPR, 2022.\\r\\n[64] Yichen Wei, Fang Wen, Wangjiang Zhu, and Jian Sun.\\r\\nGeodesic saliency using background priors. In Andrew\\r\\nFitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato,\\r\\nand Cordelia Schmid, editors, ECCV, 2012.\\r\\n[65] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\r\\nUnsupervised feature learning via non-parametric instance\\r\\ndiscrimination. In CVPR, 2018.\\r\\n[66] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\\r\\nand Antonio Torralba. Sun database: Large-scale scene\\r\\nrecognition from abbey to zoo. In CVPR, 2010.\\r\\n[67] Shuai Xie, Zunlei Feng, Y. Chen, Songtao Sun, Chao Ma,\\r\\nand Ming-Li Song. Deal: Difficulty-aware active learning\\r\\nfor semantic segmentation. In ACCV, 2020.\\r\\n[68] Kaiming He Xinlei Chen, Saining Xie.\\r\\nAn empirical study of training self-supervised vision transformers.\\r\\narXiv:2104.02057, 2021.\\r\\n[69] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical\\r\\nsaliency detection. In CVPR, 2013.\\r\\n[70] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and\\r\\nMing-Hsuan Yang. Saliency detection via graph-based manifold ranking. In CVPR, 2013.\\r\\n[71] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\r\\nZi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng\\r\\nYan. Tokens-to-token vit: Training vision transformers from\\r\\nscratch on imagenet. In ICCV, 2021.\\r\\n[72] Xiaohang Zhan, Ziwei Liu, Ping Luo, Xiaoou Tang, and\\r\\nChen Loy. Mix-and-match tuning for self-supervised semantic segmentation. In AAAI, 2018.\\r\\n[73] Dingwen Zhang, Junwei Han, and Yu Zhang. Supervision by\\r\\nfusion: Towards unsupervised learning of deep salient object\\r\\ndetector. In ICCV, 2017.\\r\\n[74] Jianming Zhang and Stan Sclaroff. Exploiting surroundedness for saliency detection: A boolean map approach.\\r\\nTPAMI, 2016.\\r\\n[75] Jianming Zhang, Stan Sclaroff, Zhe Lin, Xiaohui Shen,\\r\\nBrian Price, and Radomı́r Mech. Minimum barrier salient\\r\\nobject detection at 80 fps. In ICCV, 2015.\\r\\n\\r\\n[39] Ulrike Luxburg. A tutorial on spectral clustering. Statistics\\r\\nand Computing, 2004.\\r\\n[40] Aravindh Mahendran, James Thewlis, and Andrea Vedaldi.\\r\\nCross pixel optical-flow similarity for self-supervised learning. In ACCV, 2018.\\r\\n[41] Luca Marchesotti, Claudio Cifarelli, and Gabriela Csurka. A\\r\\nframework for visual saliency detection with applications to\\r\\nimage thumbnailing. In ICCV, 2009.\\r\\n[42] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\\r\\nAndrea Vedaldi. Finding an unsupervised image segmenter\\r\\nin each of your deep generative models. arXiv:2105.08127,\\r\\n2021.\\r\\n[43] F. Milletari, N. Navab, and S. Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3DV, 2016.\\r\\n[44] Vida Movahedi and James H. Elder. Design and perceptual\\r\\nvalidation of performance measures for salient object segmentation. In CVPR Workshops, 2010.\\r\\n[45] Muzammal Naseer, Kanchana Ranasinghe, Salman Khan,\\r\\nMunawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\\r\\nYang.\\r\\nIntriguing properties of vision transformers.\\r\\narXiv:2105.10497, 2021.\\r\\n[46] Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu Lou,\\r\\nand Thomas Brox. Deepusps: Deep robust unsupervised\\r\\nsaliency prediction via self-supervision. In NeurIPS, 2019.\\r\\n[47] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\\r\\nvisual representations by solving jigsaw puzzles. In ECCV,\\r\\n2016.\\r\\n[48] Yassine Ouali, Celine Hudelot, and Myriam Tami. Semisupervised semantic segmentation with cross-consistency\\r\\ntraining. In CVPR, 2020.\\r\\n[49] Federico Perazzi, Philipp Krähenbühl, Yael Pritch, and\\r\\nAlexander Hornung. Saliency filters: Contrast based filtering\\r\\nfor salient region detection. In CVPR, 2012.\\r\\n[50] Michael Rubinstein, Ariel Shamir, and Shai Avidan. Improved seam carving for video retargeting. ACM Transactions on Graphics (SIGGRAPH), 2008.\\r\\n[51] Jianbo Shi and Jitendra Malik. Normalized cuts and image\\r\\nsegmentation. TPAMI, 2000.\\r\\n[52] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical\\r\\nimage saliency detection on extended cssd. TPAMI, 2015.\\r\\n[53] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical\\r\\nimage saliency detection on extended cssd. TPAMI, 2016.\\r\\n[54] Oriane Siméoni, Gilles Puy, Huy V. Vo, Simon Roburin,\\r\\nSpyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised\\r\\ntransformers and no labels. In BMVC, 2021.\\r\\n[55] Bongwon Suh, Haibin Ling, Benjamin B. Bederson, and\\r\\nDavid W. Jacobs. Automatic thumbnail cropping and its effectiveness. In ACM Symposium on User Interface Software\\r\\nand Technology, 2003.\\r\\n[56] Jin Sun and Haibin Ling. Scale and object aware image retargeting for thumbnail browsing. In ICCV, 2011.\\r\\n[57] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\\r\\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\\r\\ndata-efficient image transformers &; distillation through attention. In ICML, 2021.\\r\\n\\r\\n10\\r\\n\\r\\n\\x0c\\n\\n[76] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi,\\r\\nand Richard Hartley. Deep unsupervised saliency detection:\\r\\nA multiple noisy labeling perspective. In CVPR, 2018.\\r\\n[77] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao,\\r\\nJufeng Yang, and Ming-Ming Cheng. Egnet: Edge guidance\\r\\nnetwork for salient object detection. In ICCV, 2019.\\r\\n[78] Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun.\\r\\nSaliency optimization from robust background detection. In\\r\\nCVPR, 2014.\\r\\n\\r\\n11\\r\\n\\r\\n\\x0c\\n\\nAppendices\\r\\n\\r\\ndense feature maps FCNN ∈ Rh×w×D where h= Hs and\\r\\nw= W\\r\\ns with s denoting the total stride of the network and\\r\\nD denotes the dimensionality of the features. That is,\\r\\n\\r\\nIn this supplementary material, we first describe the algorithm for spectral clustering (Sec. A). Then, we briefly\\r\\nreview the overall structures of the convolution- and\\r\\ntransformer-based image encoders and how we extract\\r\\ndense features to which the spectral clustering is applied\\r\\nfrom each type of encoder (Sec. B). The evaluation metrics\\r\\nare described in Sec. C, and the full results for the comparison between k-means and spectral clustering with different cluster numbers, i.e., k = {2, 3, 4} on the three main\\r\\nsaliency benchmarks are shown in Sec. D. Lastly, we describe typical failure cases of our model in Sec. E.\\r\\n\\r\\nFCNN = ΦCNN (I) ∈ Rh×w×D\\r\\n\\r\\n(9)\\r\\n\\r\\nwhere the parameters for the CNNs are omitted for simplicity.\\r\\n\\r\\nB.2. Transformer-based visual encoder\\r\\nIn the recent literature, Transformer-based architectures\\r\\nhave shown tremendous success in the computer vision\\r\\ncommunity, including ViT [21], DeiT [57], T2T-ViT [71],\\r\\nand BEiT[5]. Generally speaking, these architectures consist of three components, namely, tokeniser (ΦTK ), linear\\r\\nprojection (ΦLP ), and transformer encoder (ΦTE ):\\r\\n\\r\\nA. Normalised spectral clustering algorithm\\r\\nHere, we describe the normalised spectral clustering algorithm used to generate pseudo-masks for our model in\\r\\nAlg. 1.\\r\\n\\r\\nFTransformer = ΦTE ◦ ΦLP ◦ ΦTK (I) ∈ Rh×w×D\\r\\n\\r\\n(10)\\r\\n\\r\\nwhere FTransformer denotes the dense features from a\\r\\ntransformer-based encoder.\\r\\n\\r\\nALGORITHM 1 Normalised spectral clustering [39, 51]\\r\\n\\r\\nTokeniser. Given an image as input, i.e. I ∈ RH×W ×3 , the\\r\\nimage is first divided by a tokeniser into non-overlapping\\r\\npatches of a fixed size P × P , ending up N patches per\\r\\nframe, i.e. N = HW\\r\\nP2 :\\r\\n\\r\\nInput: An adjacency matrix W ∈ RN ×N , the number of clusters k to be constructed.\\r\\n1: Compute the degree matrix D with W.\\r\\n2: Compute the unnormalised Laplacian L using W and D using Eqn. 3.\\r\\n3: Compute the first k generalised eigenvectors u1 , . . . , uk of\\r\\nthe generalised eigen problem Lu = λDu.\\r\\n4: Let U ∈ RN ×k be the matrix containing the vectors\\r\\nu1 , . . . , uk as the columns.\\r\\n5: For i = 1, . . . , N , let yi ∈ Rk be the vector corresponding\\r\\nto the i-th row of U.\\r\\n6: Cluster the vectors {yi | i = 1, ..., N } ∈ RN ×k with kmeans into clusters C1 , . . . , Ck .\\r\\nOutput: Clusters C = {C1 , . . . , Ck }\\r\\n\\r\\n2\\r\\n\\r\\nΦTK (I) = {xi | xi = ΦTK (I)i ∈ R3P , i = 1, ..., N }\\r\\n(11)\\r\\nwhere xi denotes the ith patch.\\r\\nLinear projection. Once tokenised, each patch from the\\r\\nimage is fed through a linear layer ΦLP and projected into a\\r\\nvector (a.k.a. token):\\r\\nzi = ΦLP (xi ) + PEi ∈ RD\\r\\n\\r\\nNote that the adjacency matrix W is computed using\\r\\nEqn. 2 of the main paper, given the dense features from a\\r\\nvisual encoder described next.\\r\\n\\r\\n2\\r\\n\\r\\nwhere xi ∈ R3P refers to the ith patch, and its corresponding learnable positional embeddings PEi ∈ RD are\\r\\nadded to the patch token ΦLP (xi ) ∈ RD . Then, the N\\r\\naugmented patch tokens are concatenated altogether with a\\r\\nclass token [CLS]∈ RD , producing the final input form of\\r\\nR(N +1)×D to a sequence of transformer layers, described\\r\\nin the following.\\r\\n\\r\\nB. Visual encoder\\r\\nOur approach utilises image representations learned by\\r\\neither convolution-based or transformer-based architectures\\r\\nto which spectral clustering will be applied. Here, we first\\r\\nbriefly review how these feature representations are computed with each model.\\r\\n\\r\\nTransformer encoder. A transformer encoder is composed\\r\\nof multiple transformer layers, each of which is subdivided into a self-attention layer and multi-layer perceptrons\\r\\n(MLPs). The self-attention layer contains three learnable\\r\\nlinear layers, each of which takes the input tokens and outputs either key K, value V , or query Q of the same dimensionality as the input tokens, i.e., R(N +1)×D .3 Then,\\r\\n\\r\\nB.1. Convolution-based visual encoder\\r\\nConvolutional neural networks (CNNs) for image representations, denoted by ΦCNN , consist of a series of 2D convolutional layers and non-linear activation functions which\\r\\noperate on an image in a sliding window fashion. Specifically, given an image I ∈ RH×W ×3 , the CNNs outputs\\r\\n\\r\\n3 In practice, we use a single linear layer which maps the D dimension\\r\\nof the input tokens to 3 × D and equally splits them into Q, K, and V .\\r\\n\\r\\n12\\r\\n\\r\\n\\x0c\\n\\nModel\\r\\n\\r\\nArch.\\r\\n\\r\\nCluster.\\r\\n\\r\\nResNet [26]\\r\\nResNet [26]\\r\\nViT [21]\\r\\nViT [21]\\r\\n\\r\\nResNet50\\r\\nResNet50\\r\\nViT-S/16\\r\\nViT-S/16\\r\\n\\r\\nk-means\\r\\nspectral\\r\\nk-means\\r\\nspectral\\r\\n\\r\\nMoCov2 [25]\\r\\nMoCov2 [25]\\r\\nSwAV [10]\\r\\nSwAV [10]\\r\\nDINO [11]\\r\\nDINO [11]\\r\\nDINO [11]\\r\\nDINO [11]\\r\\n\\r\\nResNet50\\r\\nResNet50\\r\\nResNet50\\r\\nResNet50\\r\\nViT-S/8\\r\\nViT-S/8\\r\\nViT-S/16\\r\\nViT-S/16\\r\\n\\r\\nk-means\\r\\nspectral\\r\\nk-means\\r\\nspectral\\r\\nk-means\\r\\nspectral\\r\\nk-means\\r\\nspectral\\r\\n\\r\\nDUT-OMRON [70]\\r\\nDUTS-TE [61]\\r\\nk=2 k=3 k=4 avg. k=2 k=3 k=4\\r\\nFully-supervised features\\r\\n.311 .346 .355 .337 .345 .358 .360\\r\\n.258 .326 .346 .310 .297 .341 .343\\r\\n.335 .406 .440 .394 .349 .423 .460\\r\\n.268 .392 .481 .380 .260 .428 .511\\r\\nSelf-supervised features\\r\\n.334 .387 .403 .375 .401 .423 .422\\r\\n.311 .399 .453 .387 .403 .464 .496\\r\\n.356 .412 .429 .399 .415 .456 .462\\r\\n.346 .407 .450 .401 .412 .473 .488\\r\\n.299 .381 .427 .369 .299 .385 .447\\r\\n.315 .417 .463 .398 .311 .435 .486\\r\\n.314 .391 .426 .377 .325 .407 .444\\r\\n.310 .413 .459 .394 .324 .445 .483\\r\\n\\r\\navg.\\r\\n\\r\\nk=2\\r\\n\\r\\nECSSD [52]\\r\\nk=3\\r\\nk=4\\r\\n\\r\\navg.\\r\\n\\r\\n.354\\r\\n.327\\r\\n.411\\r\\n.400\\r\\n\\r\\n.461\\r\\n.424\\r\\n.505\\r\\n.402\\r\\n\\r\\n.445\\r\\n.454\\r\\n.560\\r\\n.613\\r\\n\\r\\n.425\\r\\n.432\\r\\n.562\\r\\n.637\\r\\n\\r\\n.444\\r\\n.437\\r\\n.542\\r\\n.551\\r\\n\\r\\n.415\\r\\n.454\\r\\n.444\\r\\n.458\\r\\n.377\\r\\n.411\\r\\n.392\\r\\n.417\\r\\n\\r\\n.507\\r\\n.602\\r\\n.548\\r\\n.594\\r\\n.497\\r\\n.527\\r\\n.507\\r\\n.528\\r\\n\\r\\n.511\\r\\n.642\\r\\n.552\\r\\n.606\\r\\n.566\\r\\n.616\\r\\n.557\\r\\n.609.\\r\\n\\r\\n.481\\r\\n.638\\r\\n.526\\r\\n.569\\r\\n.591\\r\\n.618\\r\\n.560\\r\\n.596\\r\\n\\r\\n.500\\r\\n.627\\r\\n.542\\r\\n.590\\r\\n.551\\r\\n.587\\r\\n.541\\r\\n.577\\r\\n\\r\\nTable 6: Comparison between k-means algorithm and spectral clustering with three different cluster sizes k on the three\\r\\nbenchmarks. IoU between a ground-truth mask and the closest prediction among k predicted masks is considered. On the\\r\\nfourth column of each benchmark, we report the average of the results from the different k. The higher average scores of\\r\\nk-means and spectral clusterings within the same model are in bold.\\r\\nT\\r\\n\\r\\nprevious work [62, 42, 77, 35], we set β 2 to 0.3, putting\\r\\nmore weight on precision. We use Fβ to compute the\\r\\nmaximal-Fβ , described next.\\r\\n\\r\\n√\\r\\n, dim=the self-attention layer outputs softmax( QK\\r\\nD\\r\\n\\r\\n1)V ∈ R(N +1)×D .4 The outputs of the attention layer are\\r\\nfed to the following MLPs, which are composed of two linear layers with a non-linear activation between them and\\r\\noutput tokens with their shape preserved.\\r\\nNote that, as all transformer layers constituting the\\r\\ntransformer encoder share the identical architecture, the\\r\\nfinal outputs from the ViT have the same shape as the input\\r\\ntokens, i.e., R(N +1)×D . For image classification task, only\\r\\nthe [CLS] ∈ RD is taken from the outputs and fed to a\\r\\nlinear classifier. In our work, however, we consider the\\r\\npatch tokens FTransformer ∈ RN ×D which are reshaped to\\r\\nW\\r\\nRh×w×D where h and w equal H\\r\\nP × P . It is worth noting\\r\\nthat, the patch size P plays the role of total stride as in the\\r\\nCNNs.\\r\\n\\r\\n• maximal-Fβ (maxFβ ) is a maximum score of Fβ\\r\\namong multiple masks binarised with different thresholds. Specifically, given a non-binarised mask prediction with its value between [0, 255], it computes Fβ\\r\\nfrom 255 binarised masks, each of which is thresholded by an integer among {0, ..., 254} and takes the\\r\\nmaximum Fβ value for the result.\\r\\n• Intersection-over-union (IoU) is the size of overlapped\\r\\nforeground regions between a ground-truth G and a binarised mask prediction M divided by the total size of\\r\\nforeground regions from G and M .\\r\\n• Accuracy (Acc) is a metric that measures pixel-wise\\r\\naccuracy based on a ground-truth mask G and a binarised mask prediction M :\\r\\n\\r\\nC. Descriptions of evaluation metrics\\r\\nIn the following, we describe the metrics used for evaluation:\\r\\n\\r\\nAcc =\\r\\n\\r\\n• Fβ [49] is the harmonic mean of precision and recall\\r\\nbetween a ground-truth G ∈ {0, 1}H×W and a binarised mask M ∈ {0, 1}H×W :\\r\\n\\r\\nH X\\r\\nW\\r\\nX\\r\\n1\\r\\nδG ,M\\r\\nH × W i=1 j=1 ij ij\\r\\n\\r\\n(13)\\r\\n\\r\\nwhere δ denotes the Kroneker-delta.\\r\\n\\r\\n2\\r\\n\\r\\nFβ =\\r\\n\\r\\n(1 + β )Precision × Recall\\r\\n,\\r\\nβ 2 Precision + Recall\\r\\n\\r\\n(12)\\r\\n\\r\\nD. Comparison between k-means and spectral\\r\\nclustering\\r\\n\\r\\nwhere β 2 denotes a weight of precision.5 Following\\r\\n4 Here, we consider the single head case for simplicity.\\r\\n\\r\\nIn Sec. 4.3 of the main paper, we show the performance\\r\\nof k-means and spectral clustering applied to different architectures (i.e., ResNet50 and ViT-S/{8, 16}) and features (i.e., fully- and self-superivsed features) averaged over\\r\\n\\r\\nFor more details,\\r\\n\\r\\nplease refer to the original paper [58].\\r\\ntp\\r\\ntp\\r\\n5 Precision=\\r\\nand Recall = tp+f\\r\\nwhere tp, f p, and f n reptp+f p\\r\\nn\\r\\nresent true-positive, false-positive, and false-negative, respectively.\\r\\n\\r\\n13\\r\\n\\r\\n\\x0c\\n\\nFigure 5: Sample visualisations for typical prediction failures from our model on the DUT-OMRON [70] and DUTS-TE [61]\\r\\nbenchmarks. From left to right, input image, ground-truth mask, a pseudo-mask, and a predicted mask by our model are\\r\\nshown. The respective salient regions are highlighted in red. Best viewed in colour. Please zoom in for details.\\r\\n\\r\\nk={2, 3, 4} on the three saliency datasets. Here, we show\\r\\nthe full results for each k in Tab. 6. For the description,\\r\\nplease refer to Sec. 4.3 of the main paper.\\r\\n\\r\\nE. Visualisation of failure cases\\r\\nIn Fig. 5, we visualise some failure predictions from\\r\\nour model on the DUT-OMRON [70] and DUTS-TE [61]\\r\\ndatasets.\\r\\nWe notice there are two typical failure cases. First, when\\r\\na salient object is of small scale, the model tends to undersegment it and prefers the large salient object. For instance,\\r\\nas shown by the top left example in Fig 5, the whole bed\\r\\nis segmented, rather than the pillow; Second, when there\\r\\nare more than one salient region in the image, our model\\r\\nmay only segment one of them. For example, as shown by\\r\\nthe middle right example in Fig 5, both screen and seats\\r\\ncan be thought of as a salient region while the model only\\r\\nhighlights only the latter. We conjecture that these cases\\r\\nare caused by a bias of the dataset (i.e., DUTS-TR [61])\\r\\non which the model is trained. That is, the training images\\r\\nlikely to contain large salient regions composed of either an\\r\\nobject or objects sharing a semantic meaning, thus discouraging the model from predicting a small salient region or\\r\\nmore than one object with different semantics even if all the\\r\\nobjects can be regarded salient.\\r\\n\\r\\n14\\r\\n\\r\\n\\x0c',\n",
       " 'A Hybrid Mesh-neural Representation for 3D Transparent Object\\r\\nReconstruction\\r\\n\\r\\narXiv:2203.12613v1 [cs.CV] 23 Mar 2022\\r\\n\\r\\nJIAMIN XU, State Key Lab of CAD&CG, Zhejiang University, China\\r\\nZIHAN ZHU, State Key Lab of CAD&CG, Zhejiang University, China\\r\\nHUJUN BAO, State Key Lab of CAD&CG, Zhejiang University, China\\r\\nWEIWEI XU∗ , State Key Lab of CAD&CG, Zhejiang University, China\\r\\n\\r\\nFig. 1. Our reconstruction results paired with their rendering for three transparent objects. Their fine surface details can be well reconstructed by our method\\r\\nusing hand-held captured images under natural light conditions.\\r\\nWe propose a novel method to reconstruct the 3D shapes of transparent\\r\\nobjects using hand-held captured images under natural light conditions. It\\r\\ncombines the advantage of explicit mesh and multi-layer perceptron (MLP)\\r\\nnetwork, a hybrid representation, to simplify the capture setting used in\\r\\nrecent contributions. After obtaining an initial shape through the multi-view\\r\\nsilhouettes, we introduce surface-based local MLPs to encode the vertex\\r\\ndisplacement field (VDF) for the reconstruction of surface details. The design of local MLPs allows to represent the VDF in a piece-wise manner\\r\\nusing two layer MLP networks, which is beneficial to the optimization algorithm. Defining local MLPs on the surface instead of the volume also\\r\\nreduces the searching space. Such a hybrid representation enables us to\\r\\nrelax the ray-pixel correspondences that represent the light path constraint\\r\\nto our designed ray-cell correspondences, which significantly simplifies\\r\\nthe implementation of single-image based environment matting algorithm.\\r\\nWe evaluate our representation and reconstruction algorithm on several\\r\\n∗ Corresponding\\r\\n\\r\\nauthor\\r\\n\\r\\nAuthors’ addresses: Jiamin Xu, superxjm@yeah.net, State Key Lab of CAD&CG,\\r\\nZhejiang University, China; Zihan Zhu, zihan.zhu@zju.edu.cn, State Key Lab of\\r\\nCAD&CG, Zhejiang University, China; Hujun Bao, bao@cad.zju.edu.cn, State Key\\r\\nLab of CAD&CG, Zhejiang University, China; Weiwei Xu, xww@cad.zju.edu.cn, State\\r\\nKey Lab of CAD&CG, Zhejiang University, China.\\r\\nPermission to make digital or hard copies of all or part of this work for personal or\\r\\nclassroom use is granted without fee provided that copies are not made or distributed\\r\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\r\\non the first page. Copyrights for components of this work owned by others than ACM\\r\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\r\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\r\\nfee. Request permissions from permissions@acm.org.\\r\\n© 2022 Association for Computing Machinery.\\r\\n0730-0301/2022/3-ART $15.00\\r\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\r\\n\\r\\ntransparent objects with ground truth models. Our experiments show that\\r\\nour method can produce high-quality reconstruction results superior to\\r\\nstate-of-the-art methods using a simplified data acquisition setup.\\r\\nCCS Concepts: • Computing methodologies → 3D Reconstruction, Differentiable Rendering, Neural network.\\r\\nAdditional Key Words and Phrases: Transparent Object, 3D Reconstruction,\\r\\nNeural Rendering\\r\\nACM Reference Format:\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu. 2022. A Hybrid Meshneural Representation for 3D Transparent Object Reconstruction. ACM\\r\\nTrans. Graph. 1, 1 (March 2022), 12 pages. https://doi.org/10.1145/nnnnnnn.\\r\\nnnnnnnn\\r\\n\\r\\n1\\r\\n\\r\\nINTRODUCTION\\r\\n\\r\\nThe acquisition of 3D models is a problem that frequently occurs\\r\\nin computer graphics and computer vision. Most existing methods,\\r\\nsuch as laser scanning and multi-view reconstruction, are based\\r\\non surface color observation. Consequently, the surface is assumed\\r\\nas opaque and approximately Lambertian. These methods cannot\\r\\nbe directly applied to transparent objects because the appearance\\r\\nof a transparent object is indirectly observed due to the complex\\r\\nrefraction and reflection light paths at the interface between air and\\r\\ntransparent materials.\\r\\nA core technical challenge in 3D transparent object reconstruction is handling the dramatic appearance change when observing\\r\\nan object in a multi-view setting. Slight changes in an object’s shape\\r\\ncan lead to non-local changes in appearance due to the complexity\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n2 •\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nof light paths. To address this issue, ray-pixel correspondence, (i.e.,\\r\\nthe correspondence between a camera ray and a pixel on a static\\r\\nbackground pattern displayed on a monitor), and ray-ray correspondence, (i.e., the correspondence between a camera ray and the\\r\\nincident ray from the background pattern), are utilized to provide\\r\\nlight path constraints for facilitating 3D transparent object reconstruction [Ihrke et al. 2010; Kutulakos and Steger 2008; Wu et al.\\r\\n2018]. A differentiable refraction-tracing technique can be applied\\r\\nto reduce the complexity of the capture setting, and 3D shape can\\r\\nbe recovered through ray-pixel correspondences as shown in [Lyu\\r\\net al. 2020]. In this method, however, a transparent object should\\r\\nbe placed on a turntable under controlled lighting condition. Li et\\r\\nal. [2020] trained a physical-based neural network to handle complex light paths for 3D transparent objects. The network was trained\\r\\non a synthetic dataset with a differentiable path racing rendering\\r\\ntechnique. This method optimizes surface normals in latent space,\\r\\nand thus, it can reconstruct 3D transparent objects under natural\\r\\nlighting conditions when receiving an environment map and a few\\r\\nimages as input. However, it frequently produces overly smooth\\r\\nreconstruction results.\\r\\nIn this paper, we study how to combine the advantages of explicit\\r\\nmesh and multi-layer perceptron (MLP) network, a hybrid representation, to address the transparent object reconstruction problem\\r\\nunder natural lighting conditions using hand-held captured images.\\r\\nSuch representation can be reconstructed through optimization with\\r\\na differentiable path tracing rendering technique. Its key idea is to\\r\\nuse MLP to encode a vertex displacement field (VDF) defined on\\r\\na base mesh for surface details reconstruction, wherein the base\\r\\nmesh is created using multi-view silhouette images. Our design is\\r\\nmotivated by two observations. First, the MLP network parameterizes VDF with weight parameters. It can constrain the change\\r\\nin VDF in a global manner. Representing functions with MLP has\\r\\nbeen demonstrated to be efficient in optimization and robust to\\r\\nnoise [Mildenhall et al. 2020; Oechsle et al. 2021; Yariv et al. 2020].\\r\\nSecond, defining the MLP-parameterized VDF on the base mesh\\r\\nreduces the search space during optimization [Chen et al. 2018b]. It\\r\\nsignificantly accelerates the optimization process, compared with\\r\\nMLP-based volumetric representation.\\r\\nThe advantage of our hybrid representation is that it allows the\\r\\nrelaxation of the capture setting. The ray-pixel correspondence\\r\\nrequired in the optimization can be significantly relaxed to be a raycell correspondence in our pipeline. Therefore, we can simplify the\\r\\nbackground pattern design and develop a robust single image environment matting (EnvMatt) algorithm for handling images captured\\r\\nunder natural lighting conditions. Moreover, we propose representing VDF with a small number of local MLPs. Each MLP is responsible\\r\\nfor encoding a local VDF. This strategy enables us to design smallscale MLPs for accelerating optimization further. A fusion module is\\r\\ndesigned to disperse the gradient information of displacement vectors of vertices to their neighboring local MLPs. This module helps\\r\\nmaintain the global constraint of VDF and produces high-quality\\r\\nreconstruction results.\\r\\nIn summary, the technical contributions of our work include:\\r\\n\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n• A hybrid representation that employs explicit mesh and localMLP based functions to represent the details surface for transparent objects. It enables us to design small-scale MLPs to accelerate our optimization algorithm’s convergence and achieve\\r\\nhigh-quality 3D reconstruction results for transparent objects.\\r\\n• A ray-cell correspondence as a relaxed representation of the\\r\\nlight path constraint. The ray-cell correspondence is easier to\\r\\ncapture, leading to a simplified capture setting under natural\\r\\nlight conditions. Furthermore, it also eases the implementation\\r\\nof the environment matting algorithm.\\r\\nExperimental results show that our method can produce 3D models with details for a variety of transparent objects, as illustrated\\r\\nin Fig. 1. With our simplified capture setting under natural light\\r\\nconditions, our reconstruction results are superior to those of stateof-the-art 3D reconstruction algorithms for transparent objects.\\r\\n\\r\\n2\\r\\n\\r\\nRELATED WORK\\r\\n\\r\\nOur algorithm is built on the basis of considerable previous research.\\r\\nHere, we review the literature that is most related to our work, including transparent object reconstruction, differentiable rendering,\\r\\nand environment matting.\\r\\n\\r\\n2.1\\r\\n\\r\\nTransparent Object Reconstruction\\r\\n\\r\\nMany transparent object reconstruction techniques utilize special\\r\\nhardware setups, including polarization [Cui et al. 2017; Huynh et al.\\r\\n2010; Miyazaki and Ikeuchi 2005], time-of-flight camera [Tanaka\\r\\net al. 2016], tomography [Trifonov et al. 2006], a moving point light\\r\\nsource [Chen et al. 2006; Morris and Kutulakos 2007] and light field\\r\\nprobes[Wetzstein et al. 2011]. Our algorithm is most closely related\\r\\nto shape-from-distortion and light path triangulation. Kutulakos\\r\\nand Steger [2008] formulated the reconstruction of a refractive or\\r\\nmirror-like surface as a light path triangulation problem. Given a\\r\\nfunction that maps each point in the image onto a 3D “reference\\r\\npoint” that is indirectly projected onto it, the authors characterized\\r\\nthe set of reconstructible cases that depended only on the number\\r\\nof points along a light path. The mapping function can be estimated\\r\\nusing the Environment Matting (EnvMatt) algorithm (Sec. 2.3) with\\r\\na calibrated acquisition setup, denoted as ray-point correspondences.\\r\\nA ray-ray correspondence can be uniquely determined with two\\r\\ndistinct reference points along the same ray.\\r\\nIn accordance with light path triangulation, one reconstructible\\r\\ncase is that of single refraction surfaces [Schwartzburg et al. 2014;\\r\\nShan et al. 2012; Yue et al. 2014], particularly fluid surfaces [Morris\\r\\nand Kutulakos 2011; Qian et al. 2017; Zhang et al. 2014]. Another\\r\\ntractable case is that of transparent objects when the rays undergo\\r\\nrefraction two times. For these transparent objects, Tsai et al. [2015]\\r\\nshowed that ambiguity exists between depth and surface normal,\\r\\ngiving a single ray-ray correspondence. Such ambiguity can be\\r\\nreduced by introducing multi-view ray-ray correspondences or enforcing shape models. [Chari and Sturm 2013] proposed the use of\\r\\nradiometric cues to reduce the number of views. Wu et al. [2018]\\r\\nrecently reconstructed the full shape of a transparent object by first\\r\\nextracting ray-ray correspondences refracted that were only two\\r\\ntimes as in [Qian et al. 2016] and then performing separate optimization and multi-view fusion. Lyu et al. [2020] proposed to extract\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n\\r\\nSurface\\r\\nMLPs\\r\\n\\r\\nClusters\\r\\n\\r\\nInitial\\r\\nshape\\r\\n\\r\\n𝜸(x)\\r\\n\\r\\n𝜹x\\r\\n\\r\\n𝜸(x)\\r\\n\\r\\n𝜹x\\r\\n\\r\\nVSA\\r\\n\\r\\nReconstructed\\r\\nobject\\r\\n\\r\\nx + 𝜹x\\r\\n\\r\\n...\\r\\n\\r\\n...\\r\\n\\r\\nIDR\\r\\n\\r\\nFusion\\r\\nlayer\\r\\n\\r\\n• 3\\r\\n\\r\\nSilhouette loss\\r\\n\\r\\nSurface optimization\\r\\n\\r\\n...\\r\\n\\r\\nRGB loss, correspondence loss\\r\\n\\r\\nFully connection\\r\\n\\r\\nInput captured images\\r\\nScene mesh\\r\\n\\r\\nRandom patterns\\r\\n\\r\\nFig. 2. The pipeline of our approach.\\r\\n\\r\\nper-view ray-point correspondences by using the EnvMatt algorithm in [Zongker et al. 1999], and utilize differentiable rendering\\r\\nfor progressively optimizing an initial mesh.\\r\\nIn addition to optimization-based methods, deep learning techniques can also be incorporated to resolve depth-normal ambiguity.\\r\\nStets et al. [2019] and Sajjan et al. [2020] proposed the use of a\\r\\npre-trained encoder-decoder network to estimate mask, depth, and\\r\\nnormals from a single image. Li et al. [2020] suggested performing optimization in feature space to obtain surface normals. Subsequently, they conducted multi-view feature mapping and 3D point\\r\\ncloud reconstruction to obtain a 3D shape by using the estimated\\r\\nnormals, total reflection masks, and rendering errors as inputs. With\\r\\na pre-trained network as the prior, their method can work on unconstrained natural images without background patterns. However,\\r\\ntheir reconstructed transparent object may lose some details due to\\r\\nthe domain gap between real-world images and synthetic training\\r\\ndata.\\r\\n\\r\\n2.2\\r\\n\\r\\nDifferentiable Rendering\\r\\n\\r\\nIn accordance with the simulation level of light transport, differentiable rendering algorithms in computer graphics can be roughly\\r\\ndivided into three categories: differentiable rasterization [Kato et al.\\r\\n2018; Laine et al. 2020; Li et al. 2018a; Liu et al. 2019; Loper and\\r\\nBlack 2014], differential volumetric rendering [Oechsle et al. 2021;\\r\\nWang et al. 2021; Yariv et al. 2021, 2020], and differentiable raytracing [Bangaru et al. 2020; Li et al. 2018b, 2015; Luan et al. 2020;\\r\\nNimier-David et al. 2019; Zhang et al. 2020, 2019]. Differentiable\\r\\nrasterization can be used to optimize mesh itself, or features and\\r\\nthe neural network parameters defined on the mesh. Differentiable\\r\\nvolumetric rendering can be used to optimize implicit shape representations such as implicit occupancy function [Chen and Zhang\\r\\n2019; Mescheder et al. 2019], signed distance function (SDF) [Park\\r\\net al. 2019; Yariv et al. 2020], and unsigned distance function [Atzmon and Lipman 2020]. Differentiable rendering was also used\\r\\nto optimize for deep surface light fields [Chen et al. 2018b]. This\\r\\nmethod represents per-vertex view-dependent reflections using an\\r\\n\\r\\nMLP. While we also utilize the surface-based MLPs, our focus is\\r\\ndifferent: our method employs local MLPs to represent VDF locally\\r\\nto reconstruct surface details and design a fusion layer to avoid\\r\\ndiscontinuities at the overlapped surface areas.\\r\\nConsidering that a light path with refraction is determined by\\r\\nthe front and back surfaces of a transparent object, the geometry\\r\\ncan be optimized in an iterative way with forward ray-tracing and\\r\\nbackward gradient propagation. To this end, our algorithm exploits\\r\\ndifferential ray-tracing to handle the light path of reflected and\\r\\nrefracted rays on the surface of transparent objects.\\r\\n\\r\\n2.3\\r\\n\\r\\nEnvironment Matting\\r\\n\\r\\nEnvironment matting (EnvMatt) which captures how an object refracts and reflects environment light, can be viewed as an extension\\r\\nof alpha matting [Levin et al. 2007; Porter and Duff 1984]. Imagebased refraction and reflection are represented as pixel-texel (texture\\r\\npixel) correspondences, while environments are represented as texture maps. The seminal work of Zongker et al. [1999] extracted\\r\\nthe EnvMatt from a series of 1D Gray codes, with the assumption\\r\\nthat each pixel was only related to a rectangular background region. Chuang et al. [2000] extended the work to recover a more\\r\\naccurate model at the expense of using additional structured light\\r\\nbackdrops. They also proposed a simplified EnvMatt algorithm by\\r\\nusing only a single backdrop. A pixel-texel correspondence search\\r\\ncan also be performed in the wavelet [Peers and Dutré 2003] and\\r\\nfrequency [Qian et al. 2015] domains. The number of required patterns can be reduced when combining them with a compressive\\r\\nsensing technique [Duan et al. 2015]. Chen et al. [2018a] recently\\r\\npresented a deep learning framework called TOM-Net for estimating\\r\\nthe EnvMatt as a refractive flow field. The aforementioned methods\\r\\nrequire images to be captured under controlled lighting condition,\\r\\ne.g., a dark room, to avoid the influence of ambient light. Wexler\\r\\net al. [2002] developed an EnvMatt algorithm for handling natural\\r\\nscene background. However, their method needs to capture a set of\\r\\nimages by using a fixed camera and a moving background.\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n4\\r\\n\\r\\n•\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nFig. 3. Captured images.\\r\\n\\r\\n3\\r\\n\\r\\nOVERVIEW\\r\\n\\r\\nOur transparent object reconstruction pipeline is illustrated in Fig. 2.\\r\\nThis pipeline starts with reconstructing an object’s rough shape\\r\\n(initial shape) from a collection of multi-view silhouettes. Instead of\\r\\nthe space carving method [Kutulakos and Seitz 2000], we utilize the\\r\\nMLP-based signed distance function (SDF) in IDR [Yariv et al. 2020]\\r\\nto obtain a smooth initial shape as shown in Fig. 2. Afterward, we\\r\\nemploy the MLP network to represent the vertex displacement field\\r\\n(VDF) on that initial shape to reconstruct surface details. This hybrid\\r\\nsurface representation combines explicit mesh and MLP-based neural networks. In the following, we detail the hybrid representation\\r\\nand the optimization algorithm for reconstructing the representation from multi-view images.\\r\\nHybrid Representation: We choose to encode the surface details\\r\\nwith VDF as it is defined on a 2D manifold instead of the entire\\r\\n3D space, reducing the search space of the optimization algorithm\\r\\nand producing high-quality reconstruction results. Moreover, we\\r\\nuse MLPs to represent the displacement field defined on vertices\\r\\nto ease optimization. Such hybrid representation can combine explicit vertex optimization to accelerate convergence and MLP-based\\r\\nneural representation as in IDR [Yariv et al. 2020] to enforce global\\r\\nconstraints among vertices, improving the robustness of the optimization.\\r\\nRather than encoding VDF using a single MLP, we found that\\r\\nrepresenting the field with a couple of small local MLPs can achieve\\r\\nbetter results. As shown in Fig. 2, each local MLP encodes the displacement vectors of vertices within one cluster extracted from the\\r\\nmesh of initial shape using the variational shape approximation\\r\\n(VSA) algorithm [Cohen-Steiner et al. 2004]. To avoid mesh discontinuities across local MLPs, we also add a fusion layer to blend the\\r\\ndisplacement vectors of neighboring vertices based on the geodesic\\r\\ndistances on mesh [Crane et al. 2017].\\r\\nOptimization for VDF: The VDF is optimized based on the multibounce (up to two-bounces) light path constraints and the consistency between the rendering of our representation and the captured\\r\\nRGB images. The rendering procedure is performed via a recursive\\r\\ndifferentiable path tracing algorithm [Li 2019].\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nThe light path constraint due to multi-bounce refraction is approximated by a mapping function that maps each pixel in the input\\r\\nimage onto a pixel of the background pattern image, which can\\r\\nbe obtained using environment matting (EnvMatt) algorithm. We\\r\\nstore the background image as a texture. However, we found that\\r\\nthe traditional EnvMatt algorithms are either restricted to using\\r\\nmultiple images with a fixed camera or sensitive to natural light\\r\\nconditions. Consequently, we design a grid-based background pattern to establish the correspondence between a foreground pixel p\\r\\nthat covers a small part of the object surface and a cell of the grid.\\r\\nSuch a correspondence is recorded as a tuple ⟨p, u⟩, where u is the\\r\\npixel coordinate of the cell center on the background image. In this\\r\\nmanner, the mapping function is simplified but remains efficient\\r\\nin providing information of the light path constraint to facilitate\\r\\noptimization.\\r\\nIn the remainder of this paper, we first describe our data preprocessing steps (Sec. 4.1), including our image acquisition setup\\r\\nand grid-based single image environment matting algorithm. Then,\\r\\nwe present the details of the initial shape reconstruction (Sec. 4.2)\\r\\nand the surface optimization steps (Sec. 4.3).\\r\\n\\r\\n4 METHOD\\r\\n4.1 Pre-processing\\r\\nData Acquisition: We capture images by using a Canon EOS 60D\\r\\ndigital single-lens reflex camera. As shown in Fig. 3, the transparent\\r\\nobject to be captured is placed on a desk with pre-printed AprilTags [Olson 2011] underneath. The AprilTags are used to facilitate\\r\\nimage registration. To capture the ray-cell correspondences for\\r\\nenvironment matting, we place an iPad as a monitor behind the\\r\\ntransparent object to display a grid-based background pattern. The\\r\\ndisplayed pattern P𝑖 for the 𝑖th image is changed after capturing\\r\\nevery four images. We also move the position of the iPad manually\\r\\nafter capturing 60 images in our implementation. This setting is designed to incorporate more valid ray-cell correspondences to cover\\r\\nmore surface areas. All the patterns are pre-generated and used\\r\\nin the EnvMatt step. In addition to the images with a background\\r\\npattern, we capture more images without background patterns to\\r\\nprovide further constraints in RGB space. During capturing, the\\r\\niPad is moved two to four times, and the total number of captured\\r\\nimages ranges from 154 to 302, which is related to the complexity\\r\\nof the object’s surface shape.\\r\\nGrid-based Single Image Environment Matting: Similar to [Chuang\\r\\net al. 2000], we can assume that a transparent object has no intrinsic\\r\\ncolor. Therefore, the correspondences between pixels that cover\\r\\nthe object’s surface and pixels on the background pattern can be\\r\\ncalculated through nearest searching in color space. However, we\\r\\nfound that the color ramp pattern in [Chuang et al. 2000] is sensitive\\r\\nto ambient light under natural light conditions due to the invertible\\r\\nand smoothness properties of the pattern image in RGB space. In\\r\\nlight of this, we design grid-based background patterns. The color\\r\\nof each grid cell is constant, designed to create sharp boundaries\\r\\nbetween cells. As shown in Fig. 4, each pattern consists of a 7 × 7\\r\\ncheckboard with two different colors {cℎ𝑖 |𝑖 ∈ {1, 2}}. Then, we randomly sample five cells on the grid and move each cell with random\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n(1,0)\\r\\n\\r\\n(0,0)\\r\\n\\r\\nnone\\r\\n\\r\\n𝐮4\\r\\n𝐮5\\r\\n\\r\\ninf\\r\\n𝐮5\\r\\n\\r\\n𝐮1\\r\\n\\r\\n(a) Input image\\r\\n(crop)\\r\\n\\r\\n𝐜ℎ1\\r\\n𝐜ℎ2\\r\\n\\r\\n𝐮2\\r\\n\\r\\n(b) Correspondences\\r\\n\\r\\n(0,1)\\r\\n\\r\\n(c) Designed\\r\\npattern\\r\\n\\r\\n𝐜𝑠1\\r\\n𝐜𝑠2\\r\\n𝐜𝑠3\\r\\n𝐜𝑠4\\r\\n𝐜𝑠5\\r\\nSalient\\r\\ncolors\\r\\n\\r\\n𝐮4\\r\\n𝐮3\\r\\n\\r\\n𝐮3\\r\\n\\r\\n(1,1)\\r\\n\\r\\nChessboard\\r\\ncolors\\r\\n\\r\\n(d) Colors\\r\\n\\r\\nFig. 4. Grid-based single image EnvMatt procedure. (a) Input image. (b)\\r\\nEnvMatt results: colored pixels indicate that their traced rays terminate\\r\\ninside the cells with designed salient colors; black pixels indicate that the\\r\\nrays terminate inside cells with checkboard colors, and gray pixels indicates\\r\\nthat rays terminate outside the pattern. (c) The designed pattern. The circles\\r\\nindicate the centers of salient cells. (d) Chosen colors for salient cells and\\r\\ncheckboard cells.\\r\\n\\r\\noffset. These moved cells are assigned with five distinct salient colors {c𝑖𝑠 |𝑖 ∈ {1..5}} as shown in the rightmost column of Fig. 4. We\\r\\nchoose the five salient colors because they have sufficient mutual\\r\\ndistance in RGB space, making them more robust to the influence of\\r\\nenvironment lighting. These salient cells are used to calculate the\\r\\ncorrespondence tuple.\\r\\nOur EnvMatt algorithm calculates ray-cell correspondences for\\r\\neach pixel. Given a captured image, for each pixel p with color cp ,\\r\\nits corresponding cell center u can be calculated as follows:\\r\\n\\x12\\r\\n\\r\\n\\uf8f1\\r\\n\\uf8f4\\r\\n\\uf8f4\\r\\n𝑐𝑟 argmin ∥c𝑖𝑠 − cp ∥\\r\\n\\uf8f4\\r\\n\\uf8f4\\r\\n\\uf8f4\\r\\n𝑖\\r\\n\\uf8f2\\r\\n\\uf8f4\\r\\nu = 𝑖𝑛𝑓\\r\\n\\uf8f4\\r\\n\\uf8f4\\r\\n\\uf8f4\\r\\n\\uf8f4\\r\\n\\uf8f4𝑛𝑜𝑛𝑒\\r\\n\\uf8f4\\r\\n\\uf8f3\\r\\n\\r\\n\\x13\\r\\n\\r\\n• 5\\r\\n\\r\\nFig. 5. Scene mesh and pattern planes. The iPad is moved two times. During\\r\\nimage registration step, they will be registered to the same global coordinates.\\r\\n\\r\\nSpace carving\\r\\n\\r\\nIDR (silhouette loss)\\r\\n\\r\\nFig. 6. Space carving vs. IDR with silhouette loss. The space carving method\\r\\nproduces artifact in the occluded areas (indicated by the red arrow).\\r\\n\\r\\nif min ∥c𝑖𝑠 − cp ∥ < 𝛾 1\\r\\n𝑖 \\x10\\r\\n\\x11\\r\\n𝑗\\r\\nif min ∥cℎ𝑖 − cp ∥, ∥c𝑠 − cp ∥ > 𝛾 2\\r\\n𝑖,𝑗\\r\\n\\r\\notherwise\\r\\n\\r\\n(1)\\r\\n\\r\\nwhere 𝑐𝑟 (·) returns a pre-defined cell center. If cp is similar to any\\r\\nsalient color, then u is a valid correspondence. This u will be marked\\r\\nas 𝑖𝑛𝑓 when no correspondence exists for p on the designed grid\\r\\npattern, indicating that the light path terminates outside the gird.\\r\\nOtherwise, the pixel p corresponds to a pixel on the grid pattern\\r\\nwith checkboard colors. In this case, we can not obtain its precise\\r\\nray-cell correspondence. Therefore, we set the correspondence of p\\r\\nas 𝑛𝑜𝑛𝑒. The parameters 𝛾 1 and 𝛾 2 are set as 0.3 and 0.4 respectively.\\r\\nImage Registration and 3D Reconstruction: After capturing\\r\\ndata, we use the 3D reconstruction software RealityCapture [CapturingReality 2016] to register the captured images enabling us to\\r\\ntrace rays in a unified coordinate frame during differentiable rendering. Given that the position of the iPad is changed after every\\r\\n60 images to provide more ray-cell correspondences, we reconstruct\\r\\nthe 3D scene with textures that contain the iPad every 60 images independently, resulting in a number of components in RealityCapture.\\r\\nEach component records the information of each independently reconstructed 3D scene. All the components are then registered based\\r\\non the AprilTags [Olson 2011] beneath the object as shown in Fig. 5.\\r\\n\\r\\nConsidering that the background pattern displayed on the iPad\\r\\nis changed during the capturing of every 4 images, wrong matching points on the iPad’s surface are produced, resulting in failure\\r\\nin 3D iPad plane reconstruction. To address this issue, we display\\r\\nadditional AprilTags surrounding the background patterns to add\\r\\nextra matching points to guarantee the success of iPad plane reconstruction.\\r\\nEach 3D plane P𝑖 of the background pattern is detected using\\r\\nthe RANSAC shape detection algorithm [Schnabel et al. 2010] as\\r\\nillustracted in Fig. 5. We also generate local coordinates for each\\r\\ngrid pattern to map the 3D points on the plane onto the texture\\r\\ncoordinate.\\r\\n\\r\\n4.2\\r\\n\\r\\nInitial Shape Reconstruction\\r\\n\\r\\nWe utilize IDR [Yariv et al. 2020] with silhouette (mask) loss to\\r\\nobtain the initial shape of a transparent object. The object masks are\\r\\nmanually annotated on a number of selected images. The number\\r\\nof masks is provided in Tab. 1. We only use silhouette loss, and\\r\\nthus, the “neural renderer” MLP in IDR is removed. As shown in\\r\\nFig. 6, IDR with silhouette loss can produce smoother reconstruction\\r\\nresults than the space carving algorithm.\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\n4.3\\r\\n\\r\\nSurface Optimization through Differentiable\\r\\nRendering\\r\\n\\r\\nVertex 𝐱 𝒊 (𝒊 ∈ 𝟏 … 𝑵 )\\r\\nCluster 𝐶𝐱𝒊\\r\\n\\r\\nGiven the initial shape as the base mesh, we first group mesh triangles into a number of clusters and then assign each cluster with a\\r\\nsurface-based MLP. Thus, VDF can be computed as the fusion of the\\r\\noutput of surface-based local MLPs. In particular, for each vertex\\r\\nx𝑖 within cluster 𝐶 x𝑖 , each local MLP MLP𝑘 outputs a displacement\\r\\n^ 𝑖 as follows:\\r\\nvector 𝛿x\\r\\n^ 𝑖 = MLP𝐶 (x𝑖 ) .\\r\\n𝛿x\\r\\nx𝑖\\r\\n\\r\\n𝛿x𝑖 =\\r\\n\\r\\n∑︁\\r\\n\\r\\n\\x01 ^\\r\\n𝑤 x𝑖 , x 𝑗 · 𝛿x\\r\\n𝑗,\\r\\n\\r\\n(3)\\r\\n\\r\\n𝑗\\r\\n\\r\\n\\x01\\r\\n\\x01\\r\\nwhere 𝑤 x𝑖 , x 𝑗 = exp(−𝑑 x𝑖 , x 𝑗 /𝜎); and 𝑑 is the geodesic distance between x𝑖 and x 𝑗 , which is calculated using the heat transportation based method in [Crane et al. 2017]. The parameter 𝜎=0.005\\r\\nin our experiments.\\r\\nThe architecture of each local MLP is shown in Fig. 7 with two\\r\\nfully connected (FC) layers. For each MLP, each input 3D vertex on\\r\\nsurface is firstly mapped to a 99 dimensional feature using positional\\r\\nencoding. Then the first FC layer with weight normalization and\\r\\nReLu activation maps the 99 dimensional feature to a 128 dimensional latent feature. The second FC layer with weight normalization\\r\\nand tanh activation maps the latent feature to a 3D displacement.\\r\\nBefore optimization, all the weights are initialized to produce zero\\r\\ndisplacement field.\\r\\nIn the following, we first describe how to extract clusters from\\r\\nthe base mesh, and then describe the details of designed loss terms\\r\\nand our optimization procedure.\\r\\n4.3.1 Cluster Extraction. We utilize the Variational Shape Approximation (VSA) algorithm [Cohen-Steiner et al. 2004] to segment\\r\\nthe initial shape into several clusters. The VSA algorithm tends to\\r\\nmerge co-planar vertices into the same cluster by minimizing L 2\\r\\ndistortion error which measures the error between the cluster and\\r\\nits linear proxy (plane). To balance the size of each cluster, we add a\\r\\nEuclidean distance error that sums the Euclidean distance between\\r\\neach vertex and its cluster center. This error term is added to L 2\\r\\ndistortion error with a weight of 0.005 to control the clustering\\r\\nprocedure.\\r\\n4.3.2 Loss Terms. We minimize the following loss function to search\\r\\nfor the weight parameters of local MLPs:\\r\\nL𝑡𝑜𝑡𝑎𝑙 =𝜆𝑟𝑔𝑏 L𝑟𝑔𝑏 + 𝜆𝑐𝑜𝑟𝑟 L𝑐𝑜𝑟𝑟 + 𝜆𝑛𝑐𝑜𝑟𝑟 L𝑛𝑐𝑜𝑟𝑟\\r\\n\\r\\n(4)\\r\\n\\r\\n+𝜆𝑠𝑖𝑙 L𝑠𝑖𝑙 + 𝜆𝑟𝑒𝑔 L𝑟𝑒𝑔 ,\\r\\nwhich is the sum of five terms: RGB loss L𝑟𝑔𝑏 , ray-cell correspondence loss L𝑐𝑜𝑟𝑟 , no-correspondence loss L𝑛𝑐𝑜𝑟𝑟 , silhouette loss\\r\\nL𝑠𝑖𝑙 [Ravi et al. 2020], and regularization loss L𝑟𝑒𝑔 . The default\\r\\nvalues of weights, i.e., 𝜆𝑟𝑔𝑏 , 𝜆𝑐𝑜𝑟𝑟 , 𝜆𝑛𝑐𝑜𝑟𝑟 , 𝜆𝑠𝑖𝑙 and 𝜆𝑟𝑒𝑔 are set as\\r\\n0.001, 0.1, 0.03, 50.0, and 1.0, respectively, in our experiments.\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nx\\r\\n\\r\\nFusion layer\\r\\n\\r\\n𝒊\\r\\n\\r\\n𝜹x\\r\\n\\r\\nሶ\\r\\n𝜹x\\r\\n\\r\\n𝜸(x)\\r\\n\\r\\nGeodesic\\r\\ndistances\\r\\n\\r\\n3\\r\\n\\r\\n99\\r\\n\\r\\nPositional\\r\\nencoding\\r\\n\\r\\n(2)\\r\\n\\r\\nTo avoid VDF discontinuity at cluster boundaries, we introduce a\\r\\ndifferentiable fusion layer to obtain the final displacement vector\\r\\n𝛿x𝑖 :\\r\\n\\r\\nMLP𝐶𝐱\\r\\n\\r\\n3\\r\\n\\r\\n128\\r\\n\\r\\nFixed\\r\\nblending\\r\\nweights\\r\\n\\r\\n...\\r\\n\\r\\n6 •\\r\\n\\r\\n3×𝑁\\r\\n\\r\\nSurface MLPs\\r\\n\\r\\nFig. 7. Local MLP representation. Each local MLP is responsible for representing the vertex displacements inside a VSA cluster (shown as the colored\\r\\npatch on the object surface). A fusion layer is used to fuse the vertex displacements output by the local MLPs into a smooth VDF on the surface.\\r\\n\\r\\nRGB Loss: RGB loss measures the difference between the pixel\\r\\ncolor cp and the rendering result c𝑖𝑛\\r\\np of its traced ray on the basis\\r\\nof the recursive ray tracing algorithm. For a pixel p on a captured\\r\\nimage I, we first trace a ray l𝑖𝑛\\r\\np from the viewpoint associated with\\r\\nI. The recursive ray tracing algorithm is then triggered to obtain\\r\\nthe reflection color c𝑟p1 through the first-bounce reflection ray l𝑟p1\\r\\nand the refraction color c𝑡p2 through the second-bounce refraction\\r\\nray l𝑡p2 by intersecting and fetching textures from a scene mesh or a\\r\\ncorresponding pattern. We only consider single-bounce reflection\\r\\nand double-bounce refraction in our algorithm.\\r\\nIn particular, the camera ray l𝑖𝑛\\r\\np first intersects with the front\\r\\nsurface at point x1 , and is refracted by the surface with normal\\r\\nn1 following Snell’s law, as shown in Fig. 8. Then, the first-bounce\\r\\nrefraction ray l𝑡p1 is traced until it hits the back surface at point x2\\r\\nwith normal n2 . The second-bounce refraction ray l𝑡p2 is generated\\r\\nrecursively. Similarly, the reflection ray l𝑟p1 is traced in the mirrorreflection direction. For RGB loss, we only consider the camera rays\\r\\nwith first-bounce reflection ray l𝑟p1 and second-bounce refraction ray\\r\\nl𝑡p2 . We store the valid camera rays whose second-bounce refraction\\r\\nrays can hit the scene mesh or pattern without occlusion and total\\r\\ninternal reflection as a binary mask M𝑡 on image I.\\r\\nFor each refraction ray above, refraction color is attenuated along\\r\\nthe light path according to the Fresnel term F [Born 1999]. Taking\\r\\ntwo successive refraction rays l𝑡p1 and l𝑡p2 as an example, where n2 is\\r\\nthe surface normal, and 𝜂𝑖 and 𝜂𝑜 are the two indexes of refraction\\r\\n(IOR) inside and outside the object, the Fresnel term F ⟨𝑡 1,𝑡 2 ⟩ can\\r\\nbe computed as follows:\\r\\n\\r\\nF\\r\\n\\r\\n⟨𝑡 1,𝑡 2 ⟩\\r\\n\\r\\n𝑖 𝑡1\\r\\n𝑜 𝑡2\\r\\n1 𝜂 rp · n2 − 𝜂 rp · n2\\r\\n=\\r\\n2 𝜂𝑖 r𝑡p1 · n2 + 𝜂𝑜 r𝑡p2 · n2\\r\\n\\r\\n!2\\r\\n\\r\\n𝑜 𝑡1\\r\\n𝑖 𝑡2\\r\\n1 𝜂 rp · n2 − 𝜂 rp · n2\\r\\n+\\r\\n2 𝜂𝑜 r𝑡p1 · n2 + 𝜂𝑖 r𝑡p2 · n2\\r\\n(5)\\r\\n\\r\\nc𝑡p1 = (1 − F ⟨𝑡 1,𝑡 2 ⟩ )c𝑡p2 .\\r\\n\\r\\n(6)\\r\\n\\r\\nIn our experiments, we set the IOR of air as 1.0003 and leave the IOR\\r\\nof the object material as an unknown parameter to be optimized. As\\r\\nshown in Fig. 8, the reflection color c𝑟p1 and refraction color c𝑡p2 are\\r\\n\\r\\n!2\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n\\r\\nL𝑐𝑜𝑟𝑟 =\\r\\n\\r\\n\\x11\\r\\n1 ∑︁ 𝑡 \\x10\\r\\nMp𝑑 vl𝑡p2 , u\\r\\n𝑡\\r\\n|M | p\\r\\n\\r\\n• 7\\r\\n\\r\\n(8)\\r\\n\\r\\nwhere 𝑑 (·, ·) is a clipped L2 distance that reduces the loss to zero if\\r\\nthe projected pixel is within the cell.\\r\\n(\\r\\n∥q1 − q2 ∥ 2 if ∥q1 − q2 ∥ ∞ > 𝑙/2\\r\\n𝑑 (q1, q2 ) =\\r\\n(9)\\r\\n0\\r\\notherwise\\r\\n\\r\\nFig. 8. Recursive ray tracing procedure. During rendering, the refraction\\r\\nand reflection rays fetch colors from the scene texture and background\\r\\npattern. If pixel p has its corresponding cell center u, then the mesh should\\r\\nbe optimized to make the intersection between l𝑡p2 and the 3D pattern plane\\r\\nwithin the corresponding cell (the red point in the illustration).\\r\\n\\r\\nattenuated after ray tracing and composed to obtain the rendered\\r\\ncolor c𝑖𝑛\\r\\np.\\r\\nThe reflection and refraction colors are fetched from the textured\\r\\nscene mesh or corresponding pattern. If one reflection ray does not\\r\\nintersect with the scene mesh or pattern, then we only set c𝑟p1 as\\r\\nzero. RGB loss L𝑟𝑔𝑏 is defined as follows:\\r\\nL𝑟𝑔𝑏 =\\r\\n\\r\\n1 ∑︁ 𝑡 𝑖𝑛\\r\\nMp ∥cp − cp ∥ 1\\r\\n|M𝑡 | p\\r\\n\\r\\n(7)\\r\\n\\r\\nwhere ∥·∥ 1 indicates the L1 norm, c𝑖𝑛\\r\\np is the color rendered from ray\\r\\ntracing, and cp is the color for pixel p. In our implementation, we\\r\\nsample rays based on 200×200 cropped patches instead of individual\\r\\nrays. To match the pixel color and rendered color at the coarse-tofine level, we filter c𝑖𝑛\\r\\np and cp with three differentiable Gaussian\\r\\nfilters (7 × 7, 11 × 11, 13 × 13) and then add the outputs together.\\r\\nThese Gaussian filters have 2.5, 7.5, and 7.5 standard deviations\\r\\nrespectively. Moreover, we filter the textures of the scene mesh and\\r\\npatterns with Gaussian filters to smooth the gradients.\\r\\nCorrespondence Loss and No Correspondence Loss: Correspondence loss L𝑐𝑜𝑟𝑟 and no-correspondence loss L𝑛𝑐𝑜𝑟𝑟 are used to\\r\\nenforce the refraction ray l𝑡p2 to match the ray-cell correspondence\\r\\ndescribed in Sec. 4.1. For each pixel p with its corresponding cell,\\r\\nwhere the cell center is u and the side-length is 𝑙, we obtain the rayplane intersection point between l𝑡p2 and the current pattern plane\\r\\nP. Then the intersection point is projected onto 2D space and transformed into texture coordinate as vl𝑡p2 . Here, the 2D image space of\\r\\nplanar patterns has been pre-rectified to make the texture coordinate align with the grid boundaries. As shown in Fig. 4, the texture\\r\\ncoordinates of four grid corners are set as (0, 0), (1, 0), (1, 1), and\\r\\n(0, 1), with the texture coordinate of the grid center 𝑔 = (0.5, 0.5).\\r\\nCorrespondence loss L𝑐𝑜𝑟𝑟 constrains the projected pixel vl𝑡p2 to its\\r\\ncorresponding grid center u as follows:\\r\\n\\r\\nwhere 𝑙/2 is the half side-length of one square cell. As we clip the\\r\\ndistance function, our correspondence loss only imposes coarse\\r\\nconstraints at the cell level. However, RGB loss can help refine the\\r\\nsurface and correspondences at a fine level.\\r\\nFor pixels with no salient correspondences, where u = 𝑖𝑛𝑓 , we\\r\\ncan also add constraints based on no-correspondence loss L𝑛𝑐𝑜𝑟𝑟 .\\r\\nFor each pixel with u = 𝑖𝑛𝑓 , l𝑡p2 either has no intersection with\\r\\nplane P or the intersection point is out of the grid area. Thus, nocorrespondence loss L𝑛𝑐𝑜𝑟𝑟 is defined as follows:\\r\\nL𝑛𝑐𝑜𝑟𝑟 = −\\r\\n\\r\\n\\x11\\r\\n1 ∑︁ 𝑡 ^ \\x10\\r\\nMp𝑑 vl𝑡p2 , g\\r\\n𝑡\\r\\n|M | p\\r\\n\\r\\n(10)\\r\\n\\r\\nwhere the clipped distance function 𝑑^(·, ·) clips the L2 distance when\\r\\nthe texture coordinate of the hitting point is located outside the grid\\r\\narea:\\r\\n(\\r\\n∥q1 − q2 ∥ 2 if ∥q1 − q2 ∥ ∞ < 0.5\\r\\n^\\r\\n𝑑 (q1, q2 ) =\\r\\n(11)\\r\\n0\\r\\notherwise\\r\\nwhere 0.5 is the half side-length of the entire grid pattern in the\\r\\ntexture coordinate.\\r\\nSilhouette Loss and Regularization Loss: We also add silhouette\\r\\nloss [Ravi et al. 2020] to the annotated object masks similar to the\\r\\ninitial shape reconstruction step (Sec. 4.2). Moreover, to constrain\\r\\nour optimization further, we add a regularization loss as follows:\\r\\n(12)\\r\\n\\r\\nL𝑟𝑒𝑔 = 𝜆𝑙𝑠 L𝑙𝑠 + 𝜆𝑛𝑐 L𝑛𝑐 + 𝜆𝑝𝑐 L𝑝𝑐\\r\\n\\r\\nwhich have three loss terms for the purpose of shape regularization:\\r\\na Laplacian smoothness loss L𝑙𝑠 similar to that in [Ravi et al. 2020], a\\r\\nnormal consistency loss L𝑛𝑐 as in [Lyu et al. 2020], and a point cloud\\r\\nregularization loss L𝑝𝑐 , which minimize Chamfer distance [Ravi\\r\\net al. 2020] between the optimized points on current mesh and the\\r\\ninitial shape. These losses are defined as follows:\\r\\n∑︁\\r\\n\\r\\nL𝑙𝑠 =\\r\\n\\r\\nv 𝑗 ∈N ( v𝑖 )\\r\\n\\r\\nL𝑛𝑐 =\\r\\n\\r\\n∑︁\\r\\n\\r\\n\\x01\\r\\n1\\r\\nv 𝑗 − v𝑖\\r\\n|N (v𝑖 )|\\r\\n\\r\\n(13)\\r\\n\\r\\n\\x01\\x01\\r\\n\\r\\n(14)\\r\\n\\r\\n1 − log 1 + n𝑒1 · n𝑒2\\r\\n\\r\\n𝑒 ∈E\\r\\n\\r\\nL𝑝𝑐 =\\r\\n\\r\\n1\\r\\nS1\\r\\n\\r\\n∑︁\\r\\nx1 ∈S 1\\r\\n\\r\\nmin ∥x1 − x2 ∥ 22 +\\r\\n\\r\\nx2 ∈S 2\\r\\n\\r\\n1\\r\\nS2\\r\\n\\r\\n∑︁\\r\\nx2 ∈S 2\\r\\n\\r\\nmin ∥x1 − x2 ∥ 22\\r\\n\\r\\nx1 ∈S 2\\r\\n\\r\\n(15)\\r\\n\\r\\nwhere N (v𝑖 ) is the neighboring vertices of vertex v𝑖 , E is the set of\\r\\nall edges, and n𝑒1 ·n𝑒2 is the dot product of the normals of two adjacent\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n8 •\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nFig. 9. Captured images for five transparent objects: the cat object, the cow object, the dog object, the trophy object, and the brick object with bumpy front\\r\\nsurface.\\r\\n\\r\\nFig. 10. The reconstruction results and their corresponding rendering results\\r\\nfor a trophy object and a brick object with bumpy front surface.\\r\\n\\r\\ntriangles sharing the same edge 𝑒. We set 𝜆𝑙𝑠 = 0.2, 𝜆𝑛𝑐 = 1.0, and\\r\\n𝜆𝑝𝑐 = 100.0.\\r\\nRemark: To increase stability during optimization, we will remove\\r\\nthree types of camera rays: 1. nearly perpendicular with the surface\\r\\nnormals at its intersection with the surface during recursive raytracing (|l · n| < 0.2), 2. traced from the pixels near the boundary of\\r\\nthe 2D mask created by shape projection (less than 7 pixels), and 3.\\r\\ntraced from the pixels with nearly overexposure color (larger than\\r\\n220 in all RGB channels). After these reflection pruning operations,\\r\\nwe found our algorithm can produce high-quality reconstruction\\r\\nresults without environment map because the remaining rays are\\r\\ndominated by refraction or reflection from the scene mesh.\\r\\nAs the light path inside a transparent object is complex, optimizing the surface shape based on local RGB loss is insufficient, even\\r\\nwith our pyramid loss or perceptual loss (VGG loss). Consequently,\\r\\nwe utilize correspondence-based loss to obtain gradients to move\\r\\nl𝑡p2 inside its corresponding cell. Within a cell, the distance between\\r\\nthe pixel in the rendered image and its corresponding pixel in the\\r\\ncaptured image is short. Then, RGB loss can work to improve surface\\r\\ndetails. We verfiy the preceding observation through the ablation\\r\\nstudy discussed in Sec. 5 (with and without correspondence loss\\r\\nand no-correspondence loss).\\r\\n4.3.3 Optimization. As described earlier, our initial shape reconstruction step is based on IDR [Yariv et al. 2020]. We found that with\\r\\nonly silhouette loss, an insufficient number of rays may cause holes\\r\\non surfaces or sometimes generate another surface beneath the object’s surface. Thus, we increase the number of rays sampled from\\r\\nan image to 20800, and each batch contains rays sampled from three\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nimages. We set the learning rate as 1 × 10−4 in the beginning with\\r\\nthe same decay strategy as in IDR. The initial shape reconstruction\\r\\nstep takes about 1~2 hours after 2000 epochs on NVIDIA GeForce\\r\\nRTX 3090 GPU.\\r\\nIn the surface-based MLP optimization step, we randomly crop\\r\\n200×200 image patches from the RGB and silhouette mask images to\\r\\ncompute the RGB and silhouette loss terms. In our implementation,\\r\\nwe project the initial shape to all the views to generate the projected\\r\\nmask images, denoted as M𝑝 . During optimization, the RGB image\\r\\npatches are sampled if they overlap with M𝑝 to improve the patch\\r\\nsampling efficiency. For silhouette loss, we only sample crops from\\r\\nmasks annotated for initial shape reconstruction at each iteration.\\r\\nThe number of images with annotated masks is much less than the\\r\\ntotal number of captured RGB images in our implementation to ease\\r\\nthe burden of the annotation procedure. The optimizer for surfacebased local MLPs optimization is ADAM [Kingma and Ba 2015], and\\r\\nthe learning rate is set as 1×10−5 with cosine annealing [Loshchilov\\r\\nand Hutter 2016] scheduler. The number of epochs used to train the\\r\\nnetwork is 300. The optimization procedure takes about 5~6 hours\\r\\non one NVIDIA GeForce RTX 3090 GPU, with pure PyTorch [Paszke\\r\\net al. 2017] implementation. Our code can be further optimized\\r\\nin many aspects. The forward procedure can be accelerated using\\r\\nOptiX engine, and the backward procedure can be accelerated by\\r\\nincreasing the degree of parallelism between the ray segments.\\r\\nWe optimize the index of refraction (IOR) coefficient 𝜂 of the\\r\\nobject by using ADAM with a learning rate of 1 × 10−6 . We initialize\\r\\nthe IOR coefficient of the object as 1.52 and add L2 regularization\\r\\nloss ∥𝜂 − 1.52∥ 2 to Eq. 4 with a balancing weight equals 0.001.\\r\\n\\r\\n5\\r\\n\\r\\nEXPERIMENTS\\r\\n\\r\\nWe apply our algorithm to reconstruct the 3D shapes of five different\\r\\ntransparent objects as shown in Fig. 1 and Fig. 10, made from glass\\r\\nor crystal. The captured images for these five objects are illustrated\\r\\nin Fig. 9. The size of each object, the number of input images, the\\r\\nnumber of manually annotated masks, and the number of random\\r\\npatterns with moving frequency are listed in Tab. 1. In the following,\\r\\nwe demonstrate the advantage of our surface-based local MLP representation, perform the ablation studies, and compare our method\\r\\nwith state-of-the-art transparent object reconstruction methods.\\r\\nTo evaluate the accuracy of reconstruction quantitatively, we\\r\\npaint each object with DPT-5 developer as in [Wu et al. 2018] and\\r\\nthen scan it with a scanner to obtain a ground truth mesh in SI\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n\\r\\nVert\\r\\n\\r\\nVert-LS\\r\\n\\r\\nSDF-MLP\\r\\n\\r\\nLi et al.\\r\\n\\r\\nOurs\\r\\n\\r\\n• 9\\r\\n\\r\\nGroud Truth\\r\\n\\r\\nFig. 11. Comparisons with Vert, Vert-LS, SDF-MLP and Li et al. [2020].\\r\\nTable 1. Statistics of data acquisition. #Img denotes the number of captured\\r\\nimages. #Mask, #Pattern, and #Move denote the number of annotated masks,\\r\\nthe number of our grid-based patterns presented on iPad, and the number\\r\\nof iPad movements, respectively.\\r\\n\\r\\nObject\\r\\ncow\\r\\ncat\\r\\ndog\\r\\ntrophy\\r\\nbrick\\r\\n\\r\\nSize (𝑐𝑚 3 )\\r\\n9.5 ∗ 4.5 ∗ 17.5\\r\\n8 ∗ 2.5 ∗ 4.5\\r\\n11 ∗ 3.5 ∗ 11.5\\r\\n12 ∗ 7 ∗ 7\\r\\n10 ∗ 4.5 ∗ 20\\r\\n\\r\\n#Img\\r\\n154\\r\\n302\\r\\n300\\r\\n172\\r\\n178\\r\\n\\r\\n#Mask\\r\\n19\\r\\n29\\r\\n19\\r\\n14\\r\\n15\\r\\n\\r\\n#Pattern\\r\\n30\\r\\n60\\r\\n60\\r\\n30\\r\\n30\\r\\n\\r\\n#Move\\r\\n2\\r\\n4\\r\\n4\\r\\n2\\r\\n2\\r\\n\\r\\nunit (meter) as shown in Fig. 11. We compare the reconstructed results with the ground truth after aligning them using ICP [Zhou et al.\\r\\n2018]. After that, we can evaluate the reconstruction by measuring\\r\\nthe Chamfer distance (Eq. 15) between two point clouds.\\r\\n\\r\\n5.1\\r\\n\\r\\nEvaluations\\r\\n\\r\\n5.1.1 Surface-based Local MLP Representation. We perform a comparison to isolate and highlight the importance of our surface-based\\r\\nlocal MLP representation. With our loss function, we compare our\\r\\nresults with different representations, including: 1. explicit mesh\\r\\n\\r\\nvertices as in [Lyu et al. 2020], denoted as Vert, 2. mesh vertices\\r\\nwith an advanced optimizer in [Nicolet et al. 2021], denoted as\\r\\nVert-LS (large step), and 3. SDF encoded by a single MLP similar\\r\\nto IDR [Yariv et al. 2020], denoted as SDF-MLP. We denote our\\r\\nsurface-based MLPs representation as Surf-MLPs. The implementation details of the representations used in the comparison are as\\r\\nfollows:\\r\\nVert: This representation explicitly optimizes the position of each\\r\\nvertex using the same loss as ours and with an ADAM optimizer. The\\r\\nlearning rate is set as 1 × 10−5 with cosine annealing [Loshchilov\\r\\nand Hutter 2016] scheduler.\\r\\nVert-LS: This representation is similar to Vert but with the gradient\\r\\ncalculation method and the new optimizer proposed in [Nicolet et al.\\r\\n2021]. The gradient steps in [Nicolet et al. 2021] already involve the\\r\\nLaplacian energy. Thus, we remove the explicit Laplacian smoothness loss. We set the regularization weight 𝜆 as 10, and the learning\\r\\nrate is set as 2 × 10−3 as in their paper.\\r\\nSDF-MLP: The SDF-MLP representation is a modified version of\\r\\nIDR [Yariv et al. 2020] that enables to optimize for the SDF-MLP\\r\\nrepresentation with our RGB loss and correspondence loss. The key\\r\\nmodification is propagating the gradients to the SDF MLP based\\r\\nfrom explicit mesh vertices extracted using marching cube, similar\\r\\n\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n10\\r\\n\\r\\n•\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nPE_6/256_9\\r\\n#MLP 1\\r\\n\\r\\nArch\\r\\n\\r\\nPE_16/256_9\\r\\n#MLP 1\\r\\n\\r\\n#MLP 1\\r\\n\\r\\nPE_6\\r\\n256_9\\r\\nPE_16\\r\\n256_9\\r\\n\\r\\n1.287\\r\\n\\r\\nPE_16\\r\\n128_2\\r\\n\\r\\n1.660\\r\\n\\r\\nPE_16/128_2\\r\\n#MLP 1\\r\\n\\r\\n#MLP 50\\r\\n\\r\\nPE_16/128_2\\r\\n#MLP 50\\r\\n\\r\\n#MLP 100\\r\\n\\r\\nPE_16/128_2\\r\\n#MLP 100\\r\\n\\r\\nPE_16/128_2\\r\\n#MLP 150\\r\\n\\r\\n#MLP 150\\r\\n\\r\\n1.303\\r\\n1.287\\r\\n\\r\\n1.297\\r\\n\\r\\n1.163\\r\\n\\r\\nW/o 𝓛corr\\r\\n\\r\\nto MeshSDF [Remelli et al. 2020]. Considering that the spherical ray\\r\\ntracing in IDR is not differentiable, we perform the marching cube\\r\\nalgorithm at each iteration to extract the mesh from SDF, calculate\\r\\nthe loss and gradient, and perform back-propagation. The derivative\\r\\nof each intersection point x with respect to its SDF value 𝑠 is equal\\r\\nto 𝜕x/𝜕𝑠 = −n (x), where n (x) is the surface normal at point x. The\\r\\narchitecture of the SDF-MLP network involves eight hidden layers,\\r\\nwith each layer having a dimension of 256. The activation function,\\r\\nthe level of positional encoding, the optimizer, and the learning rate\\r\\nare set same as in IDR.\\r\\nAs shown in Fig. 11, our Surf-MLP representation outperforms\\r\\nthe other representations. Explicit mesh optimization introduces\\r\\nsome high-frequency artifacts. Although Vert-LS method can reduce\\r\\nthe artifacts, it will introduce some folds in some areas. Simultaneously, the SDF-MLP representation may produce over-smooth\\r\\nresults that lose some details. We also compare our method with\\r\\nlearning based method [Li et al. 2020]. As illustrated in the fourth\\r\\ncolumn of Fig. 11, their method produces smooth surfaces that lose\\r\\ndetails near object boundaries. Compared with the ground truth\\r\\nmesh, our reconstructed model illustrated in the fifth column of\\r\\nFig. 11 can preserve relatively high precision details.\\r\\n5.1.2 Number of MLPs. We first perform ablation studies to evaluate the influence of the MLP number (cluster number). Figure 12\\r\\nshows that our surface-based local MLP representation can obtain\\r\\nbetter result while increasing the number of clusters from 1, 50,\\r\\n100, to 150. We also compare our local MLP representation with a\\r\\nglobal MLP with 9 hidden layers of 256 dimension. For the global\\r\\nMLP, we also test it with two different positional encoding settings,\\r\\nusing number of positional encoding frequencies L=6 and L=16. As\\r\\nillustrated in Fig. 12, our multiple local MLP representation outperforms the single global MLP representation with lower Chamfer\\r\\ndistance, despite the number of positional encoding frequencies.\\r\\nThis experiment also shows that our local MLP representation can\\r\\naccelerate the convergence: it can achieve lower Chamfer distance\\r\\nwith the same number of epochs.\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nW/o 𝓛rgb\\r\\n\\r\\nW/o 𝓛sil\\r\\n\\r\\nW/o 𝓛reg\\r\\n\\r\\nFull\\r\\n\\r\\nFig. 13. Ablation study of loss terms on the cat object.\\r\\n\\r\\nMetric:\\r\\nChamferdistance\\r\\n\\r\\nFig. 12. The influence of MLP number (#MLP) and the MLP architecture\\r\\nmeasured by Chamfer distance between reconstructed mesh and ground\\r\\ntruth mesh. PE_𝑘/𝑑_𝑙: positional encoding L=𝑘, MLP with 𝑙 layer of 𝑑 dimension. All the results are obtained after training networks with 300 epochs.\\r\\nBack slash means the training can not be performed due to computational\\r\\ncost.\\r\\n\\r\\nW/o 𝓛ncorr\\r\\n\\r\\n5.2\\r\\n\\r\\nAblation Study\\r\\n\\r\\nWe remove each loss term individually to evaluate its impact on the\\r\\nreconstruction result of the cat object, as shown in Fig. 13. From\\r\\nthe results, we can see that without correspondence loss L𝑐𝑜𝑟𝑟 ,\\r\\nthe overall shape cannot be reconstructed correctly, e.g. tail of\\r\\nthe cat. As a result, the ray-cell correspondences are essential for\\r\\nour method to obtain surface details. Moreover, when any of the\\r\\nno-correspondence loss L𝑛𝑐𝑜𝑟𝑟 , RGB loss L𝑟𝑔𝑏 , regularization loss\\r\\nL𝑟𝑒𝑔 , and silhouette loss L𝑠𝑖𝑙 is omitted, the reconstructed result\\r\\ncontains artifacts, especially at folded areas. The results verify that\\r\\nall the losses are essential to the quality of the transparent object\\r\\nreconstruction.\\r\\n\\r\\n6\\r\\n\\r\\nLIMITATION AND FUTURE WORK\\r\\n\\r\\nOur environment matting algorithm can only find correspondences\\r\\nassuming the transparent object has no intrinsic color. As a result, our method can not reconstruct colored transparent objects.\\r\\nAnother limitation is that, compared with ground truth mesh, the\\r\\nresults of our method still lack some details, especially for surfaces\\r\\nwith complex occlusions. In the future, it would be interesting to\\r\\ninvestigate how to integrate the variables for the color or other\\r\\nmaterial properties of the transparent object to overcome the “no\\r\\nintrinsic color” limitation. To reduce the capturing efforts, we will\\r\\nalso investigate the relationship between the number of grid cells\\r\\nand the number of images required in optimization. Another direction is to accelerate the training through a carefully tuned CUDA\\r\\nimplementation.\\r\\n\\r\\n7\\r\\n\\r\\nCONCLUSIONS\\r\\n\\r\\nWe have developed a method to reconstruct 3D shapes of transparent objects from hand-held captured images under natural light\\r\\nconditions. Our method has two components: a surface-based MLP\\r\\nrepresentation that encodes the vertex displacement field based\\r\\non an initial shape, the surface optimization through differentiable\\r\\nrendering and environment matting. We use a iPad as background\\r\\nto provide ray-cell correspondences, a simplified capture setting, to\\r\\nfacilitate the optimization. Our method can produce high-quality reconstruction results with fine details under natural light conditions.\\r\\n\\r\\nREFERENCES\\r\\nMatan Atzmon and Yaron Lipman. 2020. SAL: Sign Agnostic Learning of Shapes From\\r\\nRaw Data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition (CVPR).\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n\\r\\nSai Praveen Bangaru, Tzu-Mao Li, and Frédo Durand. 2020. Unbiased warped-area\\r\\nsampling for differentiable rendering. ACM Transactions on Graphics (TOG) 39, 6\\r\\n(2020), 1–18.\\r\\nM. Born. 1999. Principles of optics: electromagnetic theory of propagation, interference\\r\\nand diffraction of light (7. ed.). Principles of optics - electromagnetic theory of\\r\\npropagation, interference and diffraction of light (7. ed.).\\r\\nCapturingReality. 2016. Reality capture, http://capturingreality.com.\\r\\nVisesh Chari and Peter Sturm. 2013. A theory of refractive photo-light-path triangulation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\r\\nRecognition. 1438–1445.\\r\\nAnpei Chen, Minye Wu, Yingliang Zhang, Nianyi Li, Jie Lu, Shenghua Gao, and Jingyi\\r\\nYu. 2018b. Deep surface light fields. Proceedings of the ACM on Computer Graphics\\r\\nand Interactive Techniques 1, 1 (2018), 1–17.\\r\\nGuanying Chen, Kai Han, and Kwan-Yee K Wong. 2018a. Tom-net: Learning transparent\\r\\nobject matting from a single image. In Proceedings of the IEEE Conference on Computer\\r\\nVision and Pattern Recognition. 9233–9241.\\r\\nTongbo Chen, Michael Goesele, and H-P Seidel. 2006. Mesostructure from specularity.\\r\\nIn 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\\r\\n(CVPR’06), Vol. 2. IEEE, 1825–1832.\\r\\nZhiqin Chen and Hao Zhang. 2019. Learning Implicit Fields for Generative Shape\\r\\nModeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition (CVPR).\\r\\nYung-Yu Chuang, Douglas E Zongker, Joel Hindorff, Brian Curless, David H Salesin, and\\r\\nRichard Szeliski. 2000. Environment matting extensions: Towards higher accuracy\\r\\nand real-time capture. In Proceedings of the 27th annual conference on Computer\\r\\ngraphics and interactive techniques. 121–130.\\r\\nDavid Cohen-Steiner, Pierre Alliez, and Mathieu Desbrun. 2004. Variational shape\\r\\napproximation. In ACM SIGGRAPH 2004 Papers. 905–914.\\r\\nKeenan Crane, Clarisse Weischedel, and Max Wardetzky. 2017. The Heat Method for\\r\\nDistance Computation. Commun. ACM 60, 11 (Oct. 2017), 90–99. https://doi.org/\\r\\n10.1145/3131280\\r\\nZhaopeng Cui, Jinwei Gu, Boxin Shi, Ping Tan, and Jan Kautz. 2017. Polarimetric multiview stereo. In Proceedings of the IEEE conference on computer vision and pattern\\r\\nrecognition. 1558–1567.\\r\\nQi Duan, Jianfei Cai, and Jianmin Zheng. 2015. Compressive environment matting. The\\r\\nVisual Computer 31, 12 (2015), 1587–1600.\\r\\nCong Phuoc Huynh, Antonio Robles-Kelly, and Edwin Hancock. 2010. Shape and refractive index recovery from single-view polarisation images. In 2010 IEEE Computer\\r\\nSociety Conference on Computer Vision and Pattern Recognition. IEEE, 1229–1236.\\r\\nIvo Ihrke, Kiriakos N Kutulakos, Hendrik PA Lensch, Marcus Magnor, and Wolfgang Heidrich. 2010. Transparent and specular object reconstruction. In Computer Graphics\\r\\nForum, Vol. 29. Wiley Online Library, 2400–2426.\\r\\nHiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. 2018. Neural 3d mesh renderer.\\r\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition.\\r\\n3907–3916.\\r\\nD. Kingma and J. Ba. 2015. Adam: A Method for Stochastic Optimization. CoRR\\r\\nabs/1412.6980 (2015).\\r\\nKiriakos N Kutulakos and Steven M Seitz. 2000. A theory of shape by space carving.\\r\\nInternational journal of computer vision 38, 3 (2000), 199–218.\\r\\nKiriakos N Kutulakos and Eron Steger. 2008. A theory of refractive and specular 3D\\r\\nshape by light-path triangulation. International Journal of Computer Vision 76, 1\\r\\n(2008), 13–29.\\r\\nSamuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo\\r\\nAila. 2020. Modular primitives for high-performance differentiable rendering. ACM\\r\\nTransactions on Graphics (TOG) 39, 6 (2020), 1–14.\\r\\nAnat Levin, Dani Lischinski, and Yair Weiss. 2007. A closed-form solution to natural\\r\\nimage matting. IEEE transactions on pattern analysis and machine intelligence 30, 2\\r\\n(2007), 228–242.\\r\\nTzu-Mao Li. 2019. Differentiable visual computing. arXiv preprint arXiv:1904.12228\\r\\n(2019).\\r\\nTzu-Mao Li, Miika Aittala, Frédo Durand, and Jaakko Lehtinen. 2018a. Differentiable\\r\\nmonte carlo ray tracing through edge sampling. ACM Transactions on Graphics\\r\\n(TOG) 37, 6 (2018), 1–11.\\r\\nTzu-Mao Li, Miika Aittala, Frédo Durand, and Jaakko Lehtinen. 2018b. Differentiable\\r\\nMonte Carlo Ray Tracing through Edge Sampling. ACM Trans. Graph. (Proc. SIGGRAPH Asia) 37, 6 (2018), 222:1–222:11.\\r\\nTzu-Mao Li, Jaakko Lehtinen, Ravi Ramamoorthi, Wenzel Jakob, and Frédo Durand. 2015.\\r\\nAnisotropic Gaussian mutations for Metropolis light transport through HessianHamiltonian dynamics. ACM Transactions on Graphics (TOG) 34, 6 (2015), 1–13.\\r\\nZhengqin Li, Yu-Ying Yeh, and Manmohan Chandraker. 2020. Through the looking\\r\\nglass: neural 3D reconstruction of transparent shapes. In Proceedings of the IEEE/CVF\\r\\nConference on Computer Vision and Pattern Recognition. 1262–1271.\\r\\nShichen Liu, Tianye Li, Weikai Chen, and Hao Li. 2019. Soft rasterizer: A differentiable\\r\\nrenderer for image-based 3d reasoning. In Proceedings of the IEEE/CVF International\\r\\nConference on Computer Vision. 7708–7717.\\r\\n\\r\\n•\\r\\n\\r\\n11\\r\\n\\r\\nMatthew M Loper and Michael J Black. 2014. OpenDR: An approximate differentiable\\r\\nrenderer. In European Conference on Computer Vision. Springer, 154–169.\\r\\nIlya Loshchilov and Frank Hutter. 2016. Sgdr: Stochastic gradient descent with warm\\r\\nrestarts. arXiv preprint arXiv:1608.03983 (2016).\\r\\nFujun Luan, Shuang Zhao, Kavita Bala, and Ioannis Gkioulekas. 2020. Langevin monte\\r\\ncarlo rendering with gradient-based adaptation. ACM Trans. Graph. 39, 4 (2020),\\r\\n140.\\r\\nJiahui Lyu, Bojian Wu, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. 2020. Differentiable refraction-tracing for mesh reconstruction of transparent objects. ACM\\r\\nTransactions on Graphics (TOG) 39, 6 (2020), 1–13.\\r\\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas\\r\\nGeiger. 2019. Occupancy Networks: Learning 3D Reconstruction in Function Space.\\r\\nIn Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).\\r\\nB. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and N. Ren.\\r\\n2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In\\r\\nECCV, Springer.\\r\\nDaisuke Miyazaki and Katsushi Ikeuchi. 2005. Inverse polarization raytracing: estimating surface shapes of transparent objects. In 2005 IEEE Computer Society Conference\\r\\non Computer Vision and Pattern Recognition (CVPR’05), Vol. 2. IEEE, 910–917.\\r\\nNigel JW Morris and Kiriakos N Kutulakos. 2007. Reconstructing the surface of inhomogeneous transparent scenes by scatter-trace photography. In 2007 IEEE 11th\\r\\nInternational Conference on Computer Vision. IEEE, 1–8.\\r\\nNigel JW Morris and Kiriakos N Kutulakos. 2011. Dynamic refraction stereo. IEEE\\r\\ntransactions on pattern analysis and machine intelligence 33, 8 (2011), 1518–1531.\\r\\nBaptiste Nicolet, Alec Jacobson, and Wenzel Jakob. 2021. Large steps in inverse rendering\\r\\nof geometry. ACM Transactions on Graphics (TOG) 40, 6 (2021), 1–13.\\r\\nMerlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wenzel Jakob. 2019. Mitsuba 2:\\r\\nA retargetable forward and inverse renderer. ACM Transactions on Graphics (TOG)\\r\\n38, 6 (2019), 1–17.\\r\\nMichael Oechsle, Songyou Peng, and Andreas Geiger. 2021. Unisurf: Unifying neural\\r\\nimplicit surfaces and radiance fields for multi-view reconstruction.\\r\\nE. Olson. 2011. AprilTag: A robust and flexible visual fiducial system. In Robotics and\\r\\nAutomation (ICRA), 2011 IEEE International Conference on.\\r\\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape\\r\\nRepresentation. In The IEEE Conference on Computer Vision and Pattern Recognition\\r\\n(CVPR).\\r\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\\r\\nDeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. (2017).\\r\\nPieter Peers and Philip Dutré. 2003. Wavelet environment matting. In Proceedings of\\r\\nthe 14th Eurographics workshop on Rendering. 157–166.\\r\\nThomas Porter and Tom Duff. 1984. Compositing digital images. In Proceedings of the\\r\\n11th annual conference on Computer graphics and interactive techniques. 253–259.\\r\\nYiming Qian, Minglun Gong, and Yee-Hong Yang. 2015. Frequency-based environment\\r\\nmatting by compressive sensing. In Proceedings of the IEEE International Conference\\r\\non Computer Vision. 3532–3540.\\r\\nYiming Qian, Minglun Gong, and Yee Hong Yang. 2016. 3d reconstruction of transparent\\r\\nobjects with position-normal consistency. In Proceedings of the IEEE Conference on\\r\\nComputer Vision and Pattern Recognition. 4369–4377.\\r\\nYiming Qian, Minglun Gong, and Yee-Hong Yang. 2017. Stereo-based 3D reconstruction\\r\\nof dynamic fluid surfaces by global optimization. In Proceedings of the IEEE conference\\r\\non computer vision and pattern recognition. 1269–1278.\\r\\nNikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin\\r\\nJohnson, and Georgia Gkioxari. 2020. Accelerating 3D Deep Learning with PyTorch3D. arXiv:2007.08501 (2020).\\r\\nEdoardo Remelli, Artem Lukoianov, Stephan R Richter, Benoît Guillard, Timur Bagautdinov, Pierre Baque, and Pascal Fua. 2020. Meshsdf: Differentiable iso-surface\\r\\nextraction. arXiv preprint arXiv:2006.03997 (2020).\\r\\nShreeyak Sajjan, Matthew Moore, Mike Pan, Ganesh Nagaraja, Johnny Lee, Andy Zeng,\\r\\nand Shuran Song. 2020. Clear grasp: 3d shape estimation of transparent objects\\r\\nfor manipulation. In 2020 IEEE International Conference on Robotics and Automation\\r\\n(ICRA). IEEE, 3634–3642.\\r\\nR. Schnabel, R. Wahl, and R. Klein. 2010. Efficient RANSAC for Point-Cloud Shape\\r\\nDetection. In John Wiley & Sons, Ltd. 214–226.\\r\\nYuliy Schwartzburg, Romain Testuz, Andrea Tagliasacchi, and Mark Pauly. 2014. Highcontrast computational caustic design. ACM Transactions on Graphics (TOG) 33, 4\\r\\n(2014), 1–11.\\r\\nQi Shan, Sameer Agarwal, and Brian Curless. 2012. Refractive height fields from\\r\\nsingle and multiple images. In 2012 IEEE Conference on Computer Vision and Pattern\\r\\nRecognition. IEEE, 286–293.\\r\\nJonathan Stets, Zhengqin Li, Jeppe Revall Frisvad, and Manmohan Chandraker. 2019.\\r\\nSingle-shot analysis of refractive shape using convolutional neural networks. In 2019\\r\\nIEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 995–1003.\\r\\nKenichiro Tanaka, Yasuhiro Mukaigawa, Hiroyuki Kubo, Yasuyuki Matsushita, and\\r\\nYasushi Yagi. 2016. Recovering transparent shape from time-of-flight distortion.\\r\\n\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n12\\r\\n\\r\\n•\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\\r\\n4387–4395.\\r\\nBorislav Trifonov, Derek Bradley, and Wolfgang Heidrich. 2006. Tomographic reconstruction of transparent objects. In ACM SIGGRAPH 2006 Sketches. 55–es.\\r\\nChia-Yin Tsai, Ashok Veeraraghavan, and Aswin C Sankaranarayanan. 2015. What\\r\\ndoes a single light-ray reveal about a transparent object?. In 2015 IEEE International\\r\\nConference on Image Processing (ICIP). IEEE, 606–610.\\r\\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping\\r\\nWang. 2021. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\\r\\nMulti-view Reconstruction. arXiv preprint arXiv:2106.10689 (2021).\\r\\nGordon Wetzstein, David Roodnick, Wolfgang Heidrich, and Ramesh Raskar. 2011.\\r\\nRefractive shape from light field distortion. In 2011 International Conference on\\r\\nComputer Vision. IEEE, 1180–1186.\\r\\nYdo Wexler, Andrew Fitzgibbon, and Andrew Zisserman. 2002. Image-based environment matting. (2002).\\r\\nBojian Wu, Yang Zhou, Yiming Qian, Minglun Gong, and Hui Huang. 2018. Full 3D\\r\\nreconstruction of transparent objects. arXiv preprint arXiv:1805.03482 (2018).\\r\\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural\\r\\nimplicit surfaces. Advances in Neural Information Processing Systems 34 (2021).\\r\\n\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, and\\r\\nYaron Lipman. 2020. Multiview neural surface reconstruction by disentangling\\r\\ngeometry and appearance. arXiv preprint arXiv:2003.09852 (2020).\\r\\nYonghao Yue, Kei Iwasaki, Bing-Yu Chen, Yoshinori Dobashi, and Tomoyuki Nishita.\\r\\n2014. Poisson-based continuous surface generation for goal-based caustics. ACM\\r\\nTransactions on Graphics (TOG) 33, 3 (2014), 1–7.\\r\\nCheng Zhang, Bailey Miller, Kan Yan, Ioannis Gkioulekas, and Shuang Zhao. 2020.\\r\\nPath-space differentiable rendering. ACM transactions on graphics 39, 4 (2020).\\r\\nCheng Zhang, Lifan Wu, Changxi Zheng, Ioannis Gkioulekas, Ravi Ramamoorthi, and\\r\\nShuang Zhao. 2019. A differential theory of radiative transfer. ACM Transactions on\\r\\nGraphics (TOG) 38, 6 (2019), 1–16.\\r\\nMingjie Zhang, Xing Lin, Mohit Gupta, Jinli Suo, and Qionghai Dai. 2014. Recovering\\r\\nscene geometry under wavy fluid via distortion and defocus analysis. In European\\r\\nConference on Computer Vision. Springer, 234–250.\\r\\nQian-Yi Zhou, Jaesik Park, and Vladlen Koltun. 2018. Open3D: A Modern Library for\\r\\n3D Data Processing. arXiv:1801.09847 (2018).\\r\\nDouglas E Zongker, Dawn M Werner, Brian Curless, and David H Salesin. 1999. Environment matting and compositing. In Proceedings of the 26th annual conference on\\r\\nComputer graphics and interactive techniques. 205–214.\\r\\n\\r\\n\\x0c',\n",
       " \"StructToken : Rethinking Semantic\\r\\nSegmentation with Structural Prior\\r\\nFangjian Lin1, ⋆ , Zhanhao Liang1, 2, ⋆ , Junjun He1 , Miao Zheng3 , Shengwei\\r\\nTian, Kai Chen1, 3,⋆⋆ ,\\r\\n1\\r\\n\\r\\nShanghai AI Laboratory, Shanghai, China\\r\\nBeijing University of Posts and Telecommunications\\r\\n3\\r\\nSenseTime Research\\r\\nlinfangjian@pjlab.org.cn, alexleung@bupt.edu.cn,\\r\\nhejunjun@sjtu.edu.cn, {zhengmiao, chenkai}@sensetime.com\\r\\n2\\r\\n\\r\\nAbstract. In this paper, we present structure token (StructToken), a\\r\\nnew paradigm for semantic segmentation. From a perspective on semantic segmentation as per-pixel classification, the previous deep learningbased methods learn the per-pixel representation first through an encoder\\r\\nand a decoder head and then classify each pixel representation to a specific category to obtain the semantic masks. Differently, we propose a\\r\\nstructure-aware algorithm that takes structural information as prior to\\r\\npredict semantic masks directly without per-pixel classification. Specifically, given an input image, the learnable structure token interacts with\\r\\nthe image representations to reason the final semantic masks. Three interaction approaches are explored and the results not only outperform\\r\\nthe state-of-the-art methods but also contain more structural information. Experiments are conducted on three widely used datasets including\\r\\nADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could serve as an alternative for semantic segmentation and inspire\\r\\nfuture research.\\r\\nKeywords: semantic segmentation, transformer, new paradigm, structural information\\r\\n\\r\\n1\\r\\n\\r\\nIntroduction\\r\\n\\r\\nWith the development of self-driving technology [14], human-computer interaction [16], and augmented reality [1], semantic segmentation has attracted more\\r\\nand more attention. Since Long et al. [29] proposed fully convolutional networks\\r\\n(FCN), per-pixel classification has become the classical paradigm of semantic\\r\\nimage segmentation, which aims to classify each pixel into a semantic category.\\r\\nThe semantic segmentation methods of the classical paradigm classify each\\r\\npixel by applying 1×1 convolution to the feature map. Much work [42,6,38,24,35]\\r\\n⋆\\r\\n⋆⋆\\r\\n\\r\\nEqual contributions\\r\\nCorresponding author\\r\\n\\r\\n\\x0c\\n\\n2\\r\\nFeature learning\\r\\n\\r\\n×\\r\\nK\\r\\n\\r\\nC\\r\\n(C×K)\\r\\nStatic Classifier\\r\\n\\r\\n(a): static per-pixel classification\\r\\n\\r\\nC\\r\\n(C×K)\\r\\n\\r\\nFeature learning\\r\\n1\\r\\n2\\r\\nKernel learning\\r\\n\\r\\nClass kernels\\r\\n\\r\\n×\\r\\n\\r\\nbackbone\\r\\n\\r\\nC\\r\\n\\r\\nK\\r\\n(K×H×W)\\r\\n\\r\\nDynamic Classifier\\r\\n\\r\\n(b): dynamic per-pixel classification\\r\\n\\r\\nKernel learning\\r\\n\\r\\nStructure Token\\r\\n\\r\\nK\\r\\n\\r\\n(c): structural prior paradigm\\r\\n\\r\\nFig. 1. Comparison with three semantic segmentation paradigms. C is the channel\\r\\nnumber, and K is the number of categories in the dataset. (a). means segmentation\\r\\nwith per-pixel classification, using a static classifier, i.e., 1 × 1 convolution kernels. (b).\\r\\nindicates segmentation mask prediction with per-pixel classification, using a dynamic\\r\\nclassifier to perform prediction. The “red line” represents the way of generating the\\r\\ndynamic classifier. “①”: dynamic classifier is generated from the feature map. “②”:\\r\\ndynamic classifier is generated from the class kernels. (c). represents segmentation\\r\\nmask prediction with structural prior, which uses structure token to predict score map\\r\\ndirectly.\\r\\n\\r\\nleveraged the enhancement of context information and the fusion of multi-scale\\r\\nfeatures to acquire excellent results. By exploring the information fusion of pixel\\r\\nrepresentations and expanding the receptive field of each pixel, the convolution\\r\\nin the tail of the network can classify each pixel better and produce more refined\\r\\nscore maps. However, this kind of work focuses on improving the representation\\r\\nability of the per-pixel features and doesn’t consider structural information in\\r\\nimages. Since the weights of convolution have been fixed after the training is\\r\\ncompleted, the per-pixel classification paradigm can be regarded as a static perpixel classification.\\r\\nIn contrast to static per-pixel classification, some recent work [32,9,8,41]\\r\\nstarts to study mask prediction, which uses a set of learnable tokens as a dynamic\\r\\nclassifier and obtains the score map by matrix multiplication of the tokens and\\r\\nthe feature map. Such methods take advantage of the classical paradigm mentioned above and are able to adapt the classifier to different feature maps (i.e.,\\r\\nthe learnable tokens are updated through the interaction with feature maps). As\\r\\nthe learnable tokens are dynamic to input, these methods achieve higher performance than static per-pixel classification. However, although the classifier is\\r\\ndynamic, it still acts on each pixel, and the nature of per-pixel classification does\\r\\nnot change. Thus, the mask prediction paradigm can be regarded as a dynamic\\r\\nper-pixel classification method.\\r\\nAs shown in Fig. 1 (a) and Fig. 1 (b), for the static per-pixel classification\\r\\nparadigm and dynamic per-pixel classification paradigm described above, their\\r\\nfundamental principles are the same. These two paradigms both learn a linear\\r\\ndiscriminant function for each class, which is the last 1 × 1 convolution in the\\r\\nstatic per-pixel classification paradigm and matrix multiplication in the dynamic\\r\\nper-pixel classification paradigm. Then each pixel is assigned a specific class with\\r\\nthe largest output of the linear discriminant function. The formula to describe\\r\\nthis process is:\\r\\n(1)\\r\\n  \\\\begin {aligned} y_k(x) = w_k ^T \\\\cdot x. \\\\end {aligned} \\\\label {eq1} \\r\\n\\r\\n\\x0c\\n\\n3\\r\\n\\r\\nwhere wk is the weights of yk (x), the discriminant function for k th class. Specifically, it assigns a pixel x to class Ck if yk (x) > yj (x) for all j ̸= k.\\r\\nThe difference between them is that the classification weights of the classical\\r\\nper-pixel paradigm is fixed (static) in inference, while the classification weights\\r\\nof the dynamic per-pixel classification paradigm will be dynamically adjusted\\r\\n(dynamic) according to the input feature maps.\\r\\nJumping out of the existing semantic segmentation frameworks, we rethink\\r\\nsemantic segmentation tasks from a more anthropomorphic viewpoint. When\\r\\nobserving an object, people see the general shape and structure of the object first\\r\\nand then focus on the details. That is to say, when we want to identify objects of\\r\\ndifferent classes in a scene, we usually capture the structural features of semantic\\r\\nclasses first and then go to the details of the interior. For example, to label an\\r\\nimage into different classes, humans usually recognize the objects in the image\\r\\nbased on the understanding of the shape (structure) of the objects and roughly\\r\\ndetermine the masks of each semantic class. After that, humans concentrate on\\r\\na semantic class and gradually refine its mask. In summary, when we want to\\r\\nsegment an image into different semantic classes, we usually generate a rough\\r\\nmask according to the structure first and then adjust the details of each mask.\\r\\nHowever, for the two existing paradigms, the classical static per-pixel classification paradigm and dynamic per-pixel classification paradigm, a score map is\\r\\nobtained by classifying each pixel in the feature map of the penultimate layer.\\r\\nThis characteristic of per-pixel classification paradigm encourage the network to\\r\\noptimize the representation of single pixel to facilitate the classification process.\\r\\nWhereas, the most important structural feature is ignored and even destroyed.\\r\\nWe propose a structural prior paradigm to solve this problem, as shown in\\r\\nFig. 1 (c). Unlike the static per-pixel classification paradigm and dynamic perpixel classification paradigm, ours directly construct a score map from structure\\r\\ntoken to segment an image into different semantic classes. As shown in the\\r\\nvisual Fig. 3, our paradigm has splendid capacity for capturing the structural\\r\\nfeatures of the objects. Instead of learning a representation for each pixel then\\r\\nclassify it into one specific semantic class, structural prior paradigm imitates the\\r\\nway humans perform semantic segmentation. It produces a rough mask for each\\r\\nclass according to the structural prior and then refines the mask gradually, for\\r\\nexample, adjusting the edges. We believe that this paradigm is more suitable for\\r\\nsemantic segmentation and that structural features should play a more important\\r\\nrole in semantic segmentation.\\r\\nIn this work, unlike the classical paradigm of per-pixel semantic segmentation, we establish a new paradigm that directly uses structure token to generate\\r\\nscore maps. We mainly study how to extract useful information from the feature\\r\\nmaps according to the priors provided by the structure token. In particular, we\\r\\nexplore three interaction approaches to adapt structure token to different features and demonstrate the effectiveness of our methods by conducting extensive\\r\\nablation studies. Furthermore, we evaluate our StructToken on three challenging\\r\\nsemantic segmentation benchmarks, including ADE20K[44], COCO-Stuff 10K\\r\\n[2], and Cityscapes[10], achieving 54.18%, 49.07%, and 82.07% mIoU, respec-\\r\\n\\r\\n\\x0c\\n\\n4\\r\\n\\r\\ntively, which outperforms the state-of-the-art methods. Our main contributions\\r\\ninclude:\\r\\n– We state that structural information plays a very important role in semantic\\r\\nsegmentation.\\r\\n– We solve the problem that per-pixel classification paradigms destroy structural features.\\r\\n– We are the first to propose a new structural prior paradigm for semantic\\r\\nsegmentation and explore a new development direction of semantic segmentation in the future.\\r\\n\\r\\n2\\r\\n\\r\\nRelated Work\\r\\n\\r\\nThis section briefly reviews the work related to semantic segmentation, including static per-pixel classification paradigm and dynamic per-pixel classification\\r\\nparadigm, and introduces the new structural prior paradigm.\\r\\n2.1\\r\\n\\r\\nStatic Per-pixel Classification\\r\\n\\r\\nSince fully convolutional Networks (FCN) [29] were proposed, per-pixel classification has dominated semantic segmentation. In addition, from 2015 to 2021,\\r\\nthe research of semantic segmentation mainly focuses on the aggregation of context information and multi-scale fusion methods. PSPNet [42] uses a pyramid\\r\\npooling module; the DeepLab series [4,5] introduce the dilated convolution. Furthermore, DANet [15], DSANet [20], CCNet [21], OCRNet [38] use non-local\\r\\nmodules to model context information. Recent work [43,31,32] begin to use ViT\\r\\n[13] as the backbone to capture long-range context information. Inspired by FPN\\r\\n[27], Semantic FPN [24] begins to employ a top-down pathway and lateral connections for progressively fusing multi-scale features. SETR [43] and DPT [31]\\r\\nuse ViT as the backbone to inherit and extend this architecture. The SegFormer\\r\\n[36] uses a transformer-based encoder and designs a lightweight MLP decoder to\\r\\nperform semantic segmentation. FPT [39] starts bringing transformer into the\\r\\ndesign of the decoder, using “Grounding Transformer” to fuse high-level and\\r\\nlow-level features.\\r\\nThe key point of the above work is to strengthen the semantic-level and\\r\\nspatial-level information of the final feature map, so that the pixel-level features\\r\\nof different semantic classes are linear separable in a high dimensional space.\\r\\nFinally, the static convolution kernel is used as the discriminant function to\\r\\nperform pixel-by-pixel classification.\\r\\n2.2\\r\\n\\r\\nDynamic Per-pixel Classification\\r\\n\\r\\nDynamic per-pixel classification updates the learnable class tokens by the interaction between the tokens and the feature map output from the backbone\\r\\nnetwork. To get a score map, the learnable class tokens perform matrix multiplication with the feature map. In this case, learnable class tokens that adjust\\r\\n\\r\\n\\x0c\\n\\n5\\r\\n\\r\\nN×\\r\\n\\r\\nFFN\\r\\n\\r\\n(a): Cross-Slice Extraction\\r\\n\\r\\nN layers\\r\\n\\r\\nC concatenation\\r\\nC×H×W\\r\\n\\r\\nk\\r\\n\\r\\nC v\\r\\n\\r\\nSplit\\r\\n\\r\\nq\\r\\n\\r\\n(b): Self-Slice Extraction\\r\\n\\r\\nC\\r\\n\\r\\nPWE\\r\\n\\r\\nq\\r\\n\\r\\nN×\\r\\n\\r\\nConv\\r\\nBlock\\r\\n\\r\\nK×H×W\\r\\n\\r\\nSSE\\r\\n\\r\\nCSE\\r\\n\\r\\nSplit\\r\\n\\r\\nFFN\\r\\n\\r\\nInteraction\\r\\n\\r\\nFFN\\r\\n\\r\\nInteraction\\r\\nv\\r\\nk\\r\\n\\r\\nbackbone\\r\\n\\r\\nSplit\\r\\n\\r\\n(c): Point-Wise Extraction\\r\\n\\r\\nInteraction\\r\\n\\r\\nFig. 2. Illustrating the pipeline of Structure Token(StructToken). (a), (b), and (c)\\r\\nrepresent three different interaction methods, respectively. See the method section for\\r\\ndetails.\\r\\n\\r\\ndynamically according to the feature maps replace the static weights of the last\\r\\n1 × 1 convolution in static per-pixel classification.\\r\\nLike ViT[13], Segmenter [32] employs the transformer to jointly process the\\r\\npatch and class embeddings (tokens) during the decoding phase and let the\\r\\nclass tokens perform matrix multiplication with the feature map to produce\\r\\nthe final score map. Maskformer [9] unified instance segmentation and semantic\\r\\nsegmentation architecture by performing matrix multiplication between class\\r\\ntokens and feature maps and using a binary matching mechanism. Mask2former\\r\\n[8] and K-Net [41] use learnable semantic tokens, which is equivalent to the class\\r\\ntokens, to replace 1×1 convolution kernel, and use binary matching to unify\\r\\nsemantic, instance, and panoptic segmentation tasks.\\r\\nThe difference between dynamic per-pixel classification and static per-pixel\\r\\nclassification is that the former’s weights of the pixel classifier is dynamic, changing according to the feature map and it is involved in the computation before\\r\\nthe classification process. By contrast, the latter is static and is only involved in\\r\\nthe classification process. The adaptability to different feature maps makes the\\r\\ndynamic per-pixel classification paradigm classify each pixel more precisely.\\r\\n2.3\\r\\n\\r\\nStructural Prior Paradigm\\r\\n\\r\\nFor both static classifier and dynamic classifier, their essence is not divorced from\\r\\nmapping the feature of each pixel to a linear separable space and using the kernel\\r\\nto classify each pixel into a category. This pixel-by-pixel classification destroys\\r\\nthe structural information of the original image. To preserve and utilize the\\r\\nstructural information, we introduce the structural prior paradigm. It leverages\\r\\nstructural priors for each semantic class and extracts useful information from\\r\\nthe feature maps to directly constructs the score maps from the priors.\\r\\n\\r\\n\\x0c\\n\\n6\\r\\n\\r\\n3\\r\\n\\r\\nMethod\\r\\n\\r\\nThis section first describes our framework and then details the three variations of the structural prior paradigm: StructToken-CSE, StructToken-SSE, and\\r\\nStructToken-PWE.\\r\\n\\r\\n3.1\\r\\n\\r\\nFramework\\r\\n\\r\\nThe overall framework of our StructToken is shown in Figure 2. Each basic block\\r\\nconsists of an Interaction module and a Feed Forward Networks (FFN) module.\\r\\nGiven an input image I ∈ R3×H×W , where H and W denote the height and\\r\\nwidth, respectively. We first use a transformer backbone (such as ViT [13]) to\\r\\nH\\r\\nW\\r\\nget the feature map F ∈ RC× 16 × 16 . where C indicates the number of channels.\\r\\nThe learnable structure token S ∈ RK×N are randomly initialized. where K\\r\\nmeans the number of categories in the corresponding dataset. To avoid being\\r\\nconfused with K(key) in the transformer, we use Kclass to represent the number\\r\\nH\\r\\nof categories in the dataset in the following description. N is equal to 16\\r\\n×\\r\\nW\\r\\n,\\r\\nthe\\r\\nresulting\\r\\nnumber\\r\\nof\\r\\npatches.\\r\\nThen,\\r\\nwe\\r\\nuse\\r\\nthe\\r\\nInteraction\\r\\nmodule\\r\\nto\\r\\n16\\r\\ntransform S so that S can capture the structural information from the feature\\r\\nmap and construct a rough mask for each class according to the learned priors.\\r\\nNext, FFN module is used to refine the structure token and transform feature\\r\\nmap. The feature map and structure token pass through several basic blocks to\\r\\nproduce finer masks. Finally, we use a ConvBlock [19] to refine our constructed\\r\\nsegmentation masks and get the segmentation result. The ConvBlock consists of\\r\\ntwo convolution layers and a skip connection.\\r\\n\\r\\n3.2\\r\\n\\r\\nInteraction module\\r\\n\\r\\nMerely having the priors in structure token cannot construct the segmentation\\r\\nmasks. The primary purpose of the Interaction module is to extract the structure information from the feature map guided by the structural priors from S.\\r\\n[13] utilizes a weighted sum mechanism to extract information from all tokens.\\r\\nInspired by that, we use a similar idea to do the extraction. We produce a weight\\r\\nfor each slice of the feature map and use a weighted sum to extract the information from the feature map. Here we propose three methods to produce the\\r\\nweights.\\r\\n\\r\\nCross-Slice Extraction In the NLP transformer decoder [33], output embeddings extract the information from input embeddings using cross attention.\\r\\nInspired by that, we introduce cross attention to extract useful information. The\\r\\nclassical multi-head cross attention aims at the similarity between pixels and\\r\\ngenerates query, key, and value matrices through the linear layer. This process\\r\\n\\r\\n\\x0c\\n\\n7\\r\\n\\r\\ncan be described as follows:\\r\\n Q=\\\\phi (\\\\mathcal {F}_{1})\\\\in \\\\mathbb {R}^{N_1 \\\\times C},\\\\ K=\\\\phi (\\\\mathcal {F}_{2}) \\\\in \\\\mathbb {R}^{N_2 \\\\times C},\\\\ V = \\\\phi (\\\\mathcal {F}_{2}) \\\\in \\\\mathbb {R}^{N_2 \\\\times C}, \\\\\\\\ Attention=SoftMax( \\\\frac { Q \\\\times K^{T} }{\\\\sqrt C} )V \\\\in \\\\mathbb {R}^{N_1 \\\\times C}, \\\\label {eq2}\\r\\n(3)\\r\\nwhere F1 and F2 mean feature maps of different sizes or layers. Ni is the sequence length of features Fi , i=1, 2. The ϕ is a simple fully connected layer.\\r\\nNote that Q × K T ∈ RN1 ×N2 calculates the similarity of each pixel of F1 concerning each pixel of F2 , where the goal is to improve representation at the\\r\\npixel-level. However, for our Cross-Slice Extraction (CSE) module, we need to\\r\\nhave a clear goal to re-model structure token, extract information from feature\\r\\nmap and construct score map. Based on this, we design CSE module, which can\\r\\nbe formulated as follow:\\r\\n Q=\\\\mathscr {L}(\\\\mathcal {S}) \\\\in \\\\mathbb {R}^{K_{class} \\\\times N},\\\\ K=\\\\mathscr {L}(\\\\mathcal {F}) \\\\in \\\\mathbb {R}^{C \\\\times N},\\\\ V = \\\\mathscr {L}(\\\\mathcal {F}) \\\\in \\\\mathbb {R}^{C \\\\times N}, \\\\\\\\ \\\\mathcal {S}^{'}=SoftMax( \\\\frac { Q \\\\times K^{T} }{\\\\sqrt C} )V \\\\in \\\\mathbb {R}^{K_{class} \\\\times N}. \\\\label {eq3}\\r\\n(5)\\r\\nwhere S is the learnable structure token, and F refers to the feature map output\\r\\nby the backbone. Note that Q×K T ∈ RKclass ×C calculates the similarity between\\r\\nthe slices of the feature map and the structure token. To generate query, key\\r\\nand value, the model requires a fully connected layer mapping of dimension N .\\r\\nHowever, we find that this makes it impossible to support multi-scale inference\\r\\nbecause N is fixed after training. To solve this problem, we redesign a mapping\\r\\nstrategy L , which consists of a 1×1 convolution, a 3×3 depth-wise convolution,\\r\\nand a 1×1 convolution.\\r\\n\\r\\nSelf-Slice Extraction Our Self-Slice Extraction (SSE) module is a variant of\\r\\nthe CSE module. Model structure token through multi-head self-attention. The\\r\\nobjectives of SSE and CSE are the same. To be specific, first combine structure\\r\\ntoken with feature map via concatenation at the channel dimension, and then\\r\\nperform self-attention. The process can be described as follows:\\r\\n \\\\widehat {\\\\mathcal {F}}=Concat(\\\\mathcal {S}, \\\\mathcal {F}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ Q,\\\\ K,\\\\ V=\\\\mathscr {L}(\\\\widehat {\\\\mathcal {F}}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ Attention=SoftMax( \\\\frac { Q \\\\times K^{T} }{\\\\sqrt C} )V \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ \\\\mathcal {S}^{'}, \\\\mathcal {F}^{'} = Split(Attention). \\\\label {eq4}\\r\\n\\r\\n(9)\\r\\nwhere Q × K T ∈ R(C+Kclass )×(C+Kclass ) calculates the similarity between each\\r\\nchannel of the concatenated feature map. “Split” operation is used to split “At′\\r\\n′\\r\\ntention” into feature map F and structure token S .\\r\\n\\r\\n\\x0c\\n\\n8\\r\\n\\r\\nPoint-Wise Extraction “Attention” ∈ R(C+Kclass )×(C+Kclass ) matrix is the\\r\\nweights for every slice of the feature map. After the production of attention matrix, weighted sum is computed to extract useful information. Instead of computing the similarity between slices to get the attention matrix, we can produce the\\r\\nattention matrix through predicting directly. To be specific, we use a point-wise\\r\\nconvolution(i.e., the kernel size of the point-wise convolution is also (C + Kclass )\\r\\n× (C + Kclass )) to compute the weighted sum between slices, and the attention\\r\\nmatrix is learned by updating the weights of the point-wise convolution during\\r\\ntraining. Therefore, based on SSE module, we only replace the “Attention” part\\r\\nwith a 1×1 convolution. So for Eq. (6–9), everything else is the same except for\\r\\nEq. (7–8). Here we rewrite it:\\r\\n \\\\widehat {\\\\mathcal {F}}^{'}=\\\\mathscr {L}(\\\\widehat {\\\\mathcal {F}}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ Attention= 1 \\\\times 1 \\\\ Convolution(\\\\widehat {\\\\mathcal {F}}^{'}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}. \\\\label {eq5}\\r\\n(11)\\r\\n3.3\\r\\n\\r\\nFFN module\\r\\n\\r\\nAfter Interaction, the feature map will pass through Feed Forward Networks\\r\\n(FFN). Normal FFN consists of several fully connected layers. However, as we\\r\\nsaid in the introduction, the human visual process often describes the structural\\r\\nfeatures of objects first and then details. The Interaction process can be regarded\\r\\nas extracting structural features from the feature map. We added a lightweight\\r\\n3×3 grouped convolution to refine local features. It is also viewed as an implicit\\r\\nposition encoding. The pipeline of the whole FFN is composed of: a fully connected layer, a 3×3 grouped convolution, and a fully connected layer. Finally,\\r\\n′\\r\\n′\\r\\nFFN is used to enhance the representative ability of S and F .\\r\\n\\r\\n4\\r\\n\\r\\nExperiments\\r\\n\\r\\nWe first introduce the datasets and implementation details. Then, we compare\\r\\nour method with the recent state-of-the-arts on three challenging semantic segmentation benchmarks. Finally, comprehensive ablation studies and visualizations analyses are conducted to evaluate the effectiveness of our approach.\\r\\n4.1\\r\\n\\r\\nDatasets\\r\\n\\r\\nADE20K[44] is a challenging scene parsing dataset, which is split into 20210,\\r\\n2000 images for training and validation, respectively. It has 150 fine-grained\\r\\nobject categories and diverse scenes with 1,038 image-level labels.\\r\\nCityscapes[10] carefully annotates 19 object categories of urban driveway landscape images. It contains 5K finely annotated images and is divided into 2975\\r\\nand 500 images for training and validation. It is a high-quality dataset.\\r\\nCOCO-Stuff-10K [2] is a significant scene parsing benchmark with 9000 training images and 1000 testing images. It has 171 categories. It is referred to as\\r\\nCOCO-Stuff in this paper.\\r\\n\\r\\n\\x0c\\n\\n9\\r\\nTable 1. Comparison with the state-of-the-art methods on the ADE20K dataset. “SS”\\r\\nand “MS” indicate single-scale inference and multi-scale inference, respectively. “† ”\\r\\nmeans the ViT models trained from scratch with SAM optimizer on ImageNet-21k.\\r\\n“∗ ” represents our implementation under the same settings as the official repo.\\r\\nMethod\\r\\nBackbone\\r\\nGFLOPs Params mIoU(SS) mIoU(MS)\\r\\nFCN[29]\\r\\nResNet-101\\r\\n276\\r\\n69M\\r\\n39.91\\r\\n41.40\\r\\nEncNet[40]\\r\\nResNet-101\\r\\n219\\r\\n55M\\r\\n44.65\\r\\nOCRNet[38]\\r\\nHRNet-W48\\r\\n165\\r\\n71M\\r\\n43.25\\r\\n44.88\\r\\nCCNet[21]\\r\\nResNet-101\\r\\n278\\r\\n69M\\r\\n43.71\\r\\n45.04\\r\\nResNet-101\\r\\n263\\r\\n65M\\r\\n45.24\\r\\nANN[46]\\r\\nResNet-101\\r\\n256\\r\\n68M\\r\\n44.39\\r\\n45.35\\r\\nPSPNet[42]\\r\\nFPT[39]\\r\\nResNet-101\\r\\n45.90\\r\\nDeepLabV3+[6]\\r\\nResNet-101\\r\\n255\\r\\n63M\\r\\n45.47\\r\\n46.35\\r\\nDMNet[17]\\r\\nResNet-101\\r\\n274\\r\\n72M\\r\\n45.42\\r\\n46.76\\r\\nISNet[23]\\r\\nResNeSt-101\\r\\n47.55\\r\\nDPT[31]\\r\\nViT-Hybrid\\r\\n49.02\\r\\nViT-L†\\r\\n328\\r\\n338M\\r\\n49.16\\r\\n49.52\\r\\nDPT∗\\r\\nUperNet∗\\r\\nViT-L†\\r\\n710\\r\\n354M\\r\\n48.64\\r\\n50.00\\r\\nSETR[43]\\r\\nViT-L\\r\\n214\\r\\n310M\\r\\n48.64\\r\\n50.28\\r\\nMCIBI[22]\\r\\nViT-L\\r\\n50.80\\r\\nMiT-B5\\r\\n183\\r\\n85M\\r\\n51.80\\r\\nSegFormer[36]\\r\\nSETR-MLA∗\\r\\nViT-L†\\r\\n214\\r\\n310M\\r\\n50.45\\r\\n52.06\\r\\nSwin-L\\r\\n647\\r\\n234M\\r\\n52.10\\r\\n53.50\\r\\nUperNet[28]\\r\\nSegmenter[32]\\r\\nViT-L †\\r\\n380\\r\\n342M\\r\\n51.80\\r\\n53.60\\r\\nStructToken-SSE\\r\\nVIT-L†\\r\\n486\\r\\n395M\\r\\n52.82\\r\\n54.00\\r\\nStructToken-PWE\\r\\nVIT-L†\\r\\n442\\r\\n379M\\r\\n52.95\\r\\n54.03\\r\\nStructToken-CSE\\r\\nVIT-L†\\r\\n398\\r\\n350M\\r\\n52.84\\r\\n54.18\\r\\n\\r\\nTable 2. Comparison with the state-of-the-art methods on the Cityscapes validation\\r\\nset.\\r\\nMethod\\r\\nFCN[29]\\r\\nEncNet[40]\\r\\nPSPNet[42]\\r\\nGCNet[3]\\r\\nDNLNet[37]\\r\\nCCNet[21]\\r\\nSegmenter[32]\\r\\nSegmenter[32]\\r\\nStructToken-CSE\\r\\nStructToken-SSE\\r\\nStructToken-PWE\\r\\n\\r\\nBackbone GFLOPs Params mIoU(SS) mIoU(MS)\\r\\nResNet-101\\r\\n633\\r\\n69M\\r\\n75.52\\r\\n76.61\\r\\nResNet-101\\r\\n502\\r\\n55M\\r\\n76.10\\r\\n76.97\\r\\nResNet-101\\r\\n585\\r\\n68M\\r\\n78.87\\r\\n80.04\\r\\nResNet-101\\r\\n632\\r\\n69M\\r\\n79.18\\r\\n80.71\\r\\nResNet-101\\r\\n637\\r\\n69M\\r\\n79.41\\r\\n80.68\\r\\nResNet-101\\r\\n639\\r\\n69M\\r\\n79.45\\r\\n80.66\\r\\nDeiT-B\\r\\n79.00\\r\\n80.60\\r\\nViT-L\\r\\n553\\r\\n340M\\r\\n79.10\\r\\n81.30\\r\\nVIT-L\\r\\n567\\r\\n349M\\r\\n79.64\\r\\n81.98\\r\\nVIT-L\\r\\n651\\r\\n377M\\r\\n80.01\\r\\n82.02\\r\\nVIT-L\\r\\n600\\r\\n364M\\r\\n80.05\\r\\n82.07\\r\\n\\r\\n\\x0c\\n\\n10\\r\\nTable 3. Comparison with the state-of-the-art methods on the COCO-Stuff 10K\\r\\ndataset.\\r\\nMethod\\r\\nBackbone\\r\\nmIoU(MS)\\r\\nPSPNet[42]\\r\\nResNet-101\\r\\n38.86\\r\\nOCRNet[38]\\r\\nHRNet-W48\\r\\n39.50\\r\\nSVCNet[11]\\r\\nResNet-101\\r\\n39.60\\r\\nDANet[15]\\r\\nResNet-101\\r\\n39.70\\r\\nMaskFormer[9]\\r\\nResNet-101\\r\\n39.80\\r\\nEMANet[26]\\r\\nResNet-101\\r\\n39.90\\r\\nSpyGR[25]\\r\\nResNet-101\\r\\n39.90\\r\\nResNet-101\\r\\n40.10\\r\\nACNet[12]\\r\\nOCRNet[38]\\r\\nHRNetV2-W48\\r\\n40.50\\r\\nGINet[34]\\r\\nResNet-101\\r\\n40.60\\r\\nRecoNet[7]\\r\\nResNet-101\\r\\n41.50\\r\\nResNeSt-101\\r\\n42.08\\r\\nISNet[23]\\r\\nMCIBI[22]\\r\\nViT-L\\r\\n44.89\\r\\nStructToken-PWE\\r\\nVIT-L\\r\\n48.24\\r\\nVIT-L\\r\\n48.71\\r\\nStructToken-CSE\\r\\nStructToken-SSE\\r\\nVIT-L\\r\\n49.07\\r\\n\\r\\n4.2\\r\\n\\r\\nImplementation details\\r\\n\\r\\nWe use ViT[13] as the backbone. Following the default setting (e.g., data augmentation and training schedule) of public codebase mmsegmentation [30]. During training, data augmentation consists of three steps:(i) random horizontal\\r\\nflipping, (ii) random resize with the ratio between 0.5 and 2, (iii) random cropping (512×512 for ADE20K and COCO-Stuff-10K, and 768×768 for Cityscapes.\\r\\nIn addition, 640×640 with ViT-Large for ADE20K). For optimization, we adopt\\r\\na polynomial learning rate decay schedule; following prior works [28], we employ\\r\\nAdamW to optimize our model with 0.9 momenta and 0.01 weight decay; we set\\r\\nthe initial learning rate at 2e-5. The batch size is set to 16 for all datasets. The\\r\\ntotal iterations are 160k, 80k, 80k for ADE20K, Cityscapes and COCO-Stuff10k, respectively. For inference, we follow previous works [28,43] to average the\\r\\nmulti-scale (0.5, 0.75, 1.0, 1.25, 1.5, 1.75) predictions of our model. Interpolation\\r\\noperations are used for multi-scale inference. The slide-window test is applied\\r\\nhere. The performance is measured by the standard mean intersection of union\\r\\n(mIoU) in all experiments. Considering the effectiveness and efficiency, we adopt\\r\\nthe ViT-Tiny [13] as the backbone in the ablation study and also report comprehensive indicators on ADE20K.\\r\\n4.3\\r\\n\\r\\nComparisons with the state-of-the-art methods\\r\\n\\r\\nResults on ADE20K. Table 1 reports the comparison with the state-of-the-art\\r\\nmethods on the ADE20K validation set. From these results, it can be seen that\\r\\nour StructToken is +1.02%, +1.15% and +1.04% mIoU (52.82, 52.95 and 52.84\\r\\nvs. 51.80) higher than Segmenter [32] with the same input size (640× 640),\\r\\n\\r\\n\\x0c\\n\\n11\\r\\nTable 4. Compare the performance of ViT variants on the ADE20K dataset.\\r\\nMethod\\r\\nBackbone GFLOPs Params mIoU(SS) mIoU(MS)\\r\\nSegmenter\\r\\n6\\r\\n7M\\r\\n38.10\\r\\n38.80\\r\\nUpernet\\r\\n35\\r\\n11M\\r\\n38.93\\r\\n39.19\\r\\n10\\r\\n11\\r\\n39.88\\r\\n41.09\\r\\nSETR-MLA\\r\\nDPT\\r\\nViT-Tiny\\r\\n104\\r\\n17M\\r\\n40.82\\r\\n42.13\\r\\nStructToken-CSE\\r\\n7\\r\\n9M\\r\\n39.12\\r\\n40.23\\r\\nStructToken-SSE\\r\\n13\\r\\n14M\\r\\n40.81\\r\\n42.24\\r\\nStructToken-PWE\\r\\n10\\r\\n12M\\r\\n41.87\\r\\n42.99\\r\\nUpernet\\r\\n140\\r\\n42M\\r\\n45.53\\r\\n46.14\\r\\nSETR-MLA\\r\\n21\\r\\n27M\\r\\n44.85\\r\\n46.30\\r\\nSegmenter\\r\\nViT-Small\\r\\n22\\r\\n27M\\r\\n45.00\\r\\n46.90\\r\\nDPT\\r\\n118\\r\\n36M\\r\\n46.37\\r\\n47.45\\r\\nStructToken-CSE\\r\\n23\\r\\n30M\\r\\n45.86\\r\\n47.44\\r\\n37\\r\\n41M\\r\\n47.11\\r\\n49.07\\r\\nStructToken-SSE\\r\\nStructToken-PWE\\r\\n31\\r\\n38M\\r\\n47.36\\r\\n48.89\\r\\nUpernet\\r\\n292\\r\\n128M\\r\\n46.58\\r\\n47.47\\r\\nDPT\\r\\n171\\r\\n110M\\r\\n47.20\\r\\n47.86\\r\\nSETR-MLA\\r\\n65\\r\\n92M\\r\\n48.21\\r\\n49.32\\r\\nSegmenter\\r\\nViT-Base\\r\\n81\\r\\n107M\\r\\n49.00\\r\\n50.00\\r\\nStructToken-CSE\\r\\n86\\r\\n113M\\r\\n49.51\\r\\n50.87\\r\\n123\\r\\n142M\\r\\n50.72\\r\\n51.85\\r\\nStructToken-SSE\\r\\nStructToken-PWE\\r\\n105\\r\\n132M\\r\\n50.92\\r\\n51.82\\r\\n\\r\\nrespectively. When multi-scale testing is adopted, our StructToken is +0.4%,\\r\\n+0.43% and +0.58% mIoU (54.00, 54.03 and 54.18 vs. 53.60) higher than Segmenter, respectively. For ViT-Tiny, as shown in Table 4, our best results is\\r\\n+0.86% mIoU (42.99 vs. 42.13) higher than DPT[31] with the same input size\\r\\n(512 × 512), respectively. For ViT-Small, our best results is +1.44% mIoU (48.89\\r\\nvs. 47.45) higher than DPT. For ViT-Base, our best results is +1.82% mIoU\\r\\n(51.82 vs. 50.00) higher than Segmenter. Furthermore, the larger the model is,\\r\\nthe better the performance of StructToken is.\\r\\nResults on Cityscapes. Table 2 demonstrates the comparison results on the\\r\\nvalidation set of Cityscapes. The previous state-of-the-art method Segmenter\\r\\nwith ViT-Large achieves 79.10% mIoU. Our StructToken is +0.54%, +0.91% and\\r\\n+0.95% mIoU (79.64, 80.01 and 80.05 vs. 79.10) higher than it, respectively. For\\r\\nmulti-scale inference, our method is +0.68%, +0.72% and +0.77% mIou (81.98,\\r\\n82.02, 82.07 vs. 81.30) higher than Segmenter, respectively.\\r\\nResults on COCO-Stuff 10K. Table 3 compares the segmentation results on\\r\\nthe COCO-Stuff 10K testing set. It can be seen that our StructToken-SSE can\\r\\nachieve 49.07%, and our method is +4.18% mIoU higher than MCIBI [22] (49.07\\r\\nvs. 44.89).\\r\\n\\r\\n\\x0c\\n\\n12\\r\\n\\r\\n4.4\\r\\n\\r\\nAblation study\\r\\n\\r\\nIn this section, we conduct extensive ablation studies to illustrate the effectiveness of each module in our method. We also give several design choices and\\r\\nshow their effects on the results. Our baseline model is ViT [13] followed by CSE\\r\\nmodule and FFN module without grouped convolution. Note that we did not\\r\\nperform ablation experiments using a fully connected layer to map q, k, and v\\r\\nmatrices because this approach does not support the multi-scale inference (i.e.,\\r\\nmap the length of the feature map). All the models in the following experiments\\r\\nadopt ViT-Tiny as the backbone and are trained on ADE20K training set for\\r\\n160K iterations.\\r\\nAs shown in Table 5, we experimented with adding a grouped convolution(GPConv) layer to the FFN module and a ConvBlock to the model. In\\r\\naddition, the FLOPs of FFN-DWConv are only 0.002G, which is ignored in Table 5. It is a lightweight convolution layer, and the performance of the model\\r\\nreaches 39.12% mIoU after the FFN module and ConvBlock module are added,\\r\\nwhich is +1.41% mIoU (39.12 vs. 37.71) higher than the base model, and +1.33\\r\\n% mIoU (40.23 vs. 38.90) for multi-scale inference.\\r\\nFurthermore, we use StructToken-CSE to conduct experiments with different numbers of blocks. Each block is composed of an Interaction module and\\r\\na FFN module. As shown in Table 6, as the number of blocks increases, the\\r\\nmodel’s performance also increases. However, considering the trade-off between\\r\\nperformance and computation complexity and the number of parameters, we\\r\\nchose 4 as the number of blocks for all experiments, including StructToken-SSE\\r\\nand StructToken-PWE. It is worth mentioning that as the number of blocks\\r\\nincreases, the performance also increases, so the results listed above are still far\\r\\nfrom the best performance the StrucToken can have.\\r\\nTable 5. Ablation study on the ADE20K dataset. “Base” means using StructTokenCSE as the base module and ViT-Tiny as the backbone. “FFN-GPConv” indicates the\\r\\nFFN module with grouped convolution.\\r\\nBase FFN-GPConv ConvBlock GFLOPs Params mIoU(SS) mIoU(MS)\\r\\n√\\r\\n6.74\\r\\n8.5M\\r\\n37.71\\r\\n38.90\\r\\n√\\r\\n√\\r\\n6.74\\r\\n8.5M\\r\\n38.17\\r\\n38.85\\r\\n√\\r\\n√\\r\\n7.16\\r\\n8.9M\\r\\n38.01\\r\\n39.29\\r\\n√\\r\\n√\\r\\n√\\r\\n7.16\\r\\n8.9M\\r\\n39.12\\r\\n40.23\\r\\n\\r\\n4.5\\r\\n\\r\\nVisual analysis\\r\\n\\r\\nWe design a per-pixel classification paradigm as a counterpart to prove that\\r\\nour structural prior paradigm preserves the structural information better. This\\r\\ncounterpart maps the feature map from the backbone network to the dimension\\r\\nof the number of classes. By doing this, the feature map in the network have the\\r\\n\\r\\n\\x0c\\n\\n13\\r\\nTable 6. Compare the performance of different number of Blocks on the ADE20K\\r\\ndataset. Here, ViT-Tiny is used as the backbone.\\r\\nMethod\\r\\n\\r\\nBlocks GFLOPs Params mIoU(SS) mIoU(MS)\\r\\n1\\r\\n4.73\\r\\n6.7M\\r\\n37.66\\r\\n38.83\\r\\n2\\r\\n5.54\\r\\n7.4M\\r\\n37.08\\r\\n38.05\\r\\nStructToken-CSE\\r\\n4\\r\\n7.16\\r\\n8.9M\\r\\n39.12\\r\\n40.23\\r\\n6\\r\\n8.77\\r\\n10.3M\\r\\n39.29\\r\\n40.61\\r\\n8\\r\\n10.39 11.8M\\r\\n39.61\\r\\n41.04\\r\\n\\r\\nproperty of structure token that each slice of the feature map corresponds to the\\r\\nmask of one class. Then we use four residual blocks [18] to transform the feature\\r\\nmap. Finally, we use a 1 × 1 convolution to generate score map. The feature map\\r\\noutput from each residual block is compared with the structure token output\\r\\nfrom each interaction block. To demonstrate the outstanding structure capturing\\r\\ncapability of our structure token, we visualize the slices of structure token and\\r\\nthat of the feature map from the counterpart in Fig. 3.\\r\\nSpecifically, we get the structure tokens/feature maps before transform blocks\\r\\nand after each transform block. Then we visualize the corresponding slices of\\r\\nclass person, car, airplane, and tree in these structure tokens/feature maps. The\\r\\nslices of these four classes in the output score map are also visualized. We can\\r\\nsee that even though the output score maps are similar, the structure tokens and\\r\\nfeature maps in the counterpart are pretty different in previous blocks. In our\\r\\nstructural prior paradigm, we can explicitly see the structure of the semantic\\r\\nclass.\\r\\nIn the first row, even though many people are in the image, the structure\\r\\ntoken can still distinguish each person. This visualization verifies that our proposed paradigm captures the structure of each semantic class to produce the\\r\\ninitial masks. Then the network refines the initial masks to make the final score\\r\\nmap. The refinement process can be seen in row 9. Since the structure of trees is\\r\\ncomplex, the structure token after the first block highlights the rough tree structure with low confidence. We can see the clear structure in the slice of structure\\r\\ntoken after the second block, and with the structure token going deeper, the\\r\\nstructure is more detailed. In contrast, in the visualization of the slices of the\\r\\ncounterpart, we can only see the blurry structure(row 2 and row 4) or even no\\r\\nstructure(row 6, 8, and 10) of the semantic class until the 1 × 1 convolution\\r\\ntransform the feature maps to score maps.\\r\\nVisualization makes our point that the classical paradigm aims at learning\\r\\nbetter representation for each pixel. Therefore, the structural information in the\\r\\nspatial dimension is destroyed. This visual analysis provides strong evidence of\\r\\nthe strength of our paradigm in retaining structural information.\\r\\n\\r\\n\\x0c\\n\\n14\\r\\nImage\\r\\n\\r\\nStructToken / Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nours\\r\\n\\r\\n88 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n86 %\\r\\n\\r\\nours\\r\\n\\r\\n92 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n91 %\\r\\n\\r\\nours\\r\\n\\r\\n96 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n95 %\\r\\n\\r\\nours\\r\\n\\r\\n87 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n85 %\\r\\n\\r\\nours\\r\\n\\r\\n82 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n86 %\\r\\n\\r\\nFig. 3. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to “person”, “car”, “airplane” and\\r\\n“tree” classes in structure token/feature map from top to bottom with similar performance of the two paradigms. “StructToken” refers to the slices from structure token\\r\\nand “Feature map” refers to the slices from feature maps. We design a four blocks\\r\\ndecoder as a per-pixel classification paradigm counterpart, and “Block 1-4” represents\\r\\nthe slices from the structure token/feature maps after each block. “Output” means the\\r\\nslices from the output score map. “IoU” represents the intersect over union score of\\r\\nthe specific class in this image. Details are at section 4.5.\\r\\n\\r\\n5\\r\\n\\r\\nConclusion\\r\\n\\r\\nIn this paper, we solve the problem that the classical per-pixel classification\\r\\nparadigm destroys the structure information and put forward a new structural\\r\\nprior paradigm different from the classical per-pixel classification paradigm. The\\r\\nsemantic segmentation task is reconsidered and analyzed in a more anthropomorphic way. We state that structural information plays an important role in\\r\\nscene analysis. Furthermore, the structure prior paradigm can perform different\\r\\ntasks by defining the size of the core, so it has universality for image-level tasks.\\r\\nWe hope this work can bring some fundamental enlightenment for semantic segmentation and even other tasks. Let us rethink the essential characteristics of\\r\\neach task more profoundly to analyze images and each task better.\\r\\n\\r\\n\\x0c\\n\\n15\\r\\n\\r\\nA\\r\\n\\r\\nDetails of the Point-Wise Extraction Module\\r\\n\\r\\nSelf-attention generates query, key and value for each token, and computes the\\r\\nweighted sum of values of all tokens to fuse the global information into one token.\\r\\nThe weight of each token is generated by computing the the similarity between\\r\\nthe query of the token into which the global information will be fused and the\\r\\nkeys of all tokens. The computation of similarity scores have quadratic computation complexity to the input image size, and thus the computational overhead\\r\\nis significant. [45] uses a linear layer to predict attention weights directly. We\\r\\nexplore another way to generate the attention weights, that is to “learn” the\\r\\nattention weights.\\r\\nIn our case, we want to compute the weighted sum of all the slices in the\\r\\nfeature map. 1 × 1 convolution can be deemed the calculation of the weighted\\r\\nsum of slices in a feature map, and the weights are the parameters of the convolution. To be specific, let F with k channels be the input feature map, F can\\r\\nbe represented as:\\r\\n \\\\mathcal {F} = [\\\\mathcal {S}_1, \\\\mathcal {S}_2, ..., \\\\mathcal {S}_k], \\\\mathcal {S}_i \\\\in \\\\mathbb {R}^{H \\\\times W}\\r\\n\\r\\n(12)\\r\\n\\r\\nwhere Si is a slice of the feature map.\\r\\n′\\r\\nLet Sj (a,b) be the pixel in the j th slice of the output feature map with coordinate\\r\\n(a, b):\\r\\n \\\\mathcal {S}^{'}_{j\\\\_{}(a, b)} = w_{j, 1} \\\\times \\\\mathcal {S}_{1\\\\_{}(a, b)} + w_{j, 2} \\\\times \\\\mathcal {S}_{2\\\\_{}(a, b)} + ... + w_{j, k} \\\\times \\\\mathcal {S}_{k\\\\_{}(a, b)}\\r\\n\\r\\n(13)\\r\\n\\r\\nwhere the wj,1 , wj,2 , ..., wj,k are the parameters of the 1 × 1 convolution corresponding to the j th slice of the output feature map. Deriving from one pixel to\\r\\n′\\r\\nall pixels in a slice, for slice Sj in the output feature map, it can be represented\\r\\nas:\\r\\n \\\\mathcal {S}^{'}_j = w_{j, 1} \\\\times \\\\mathcal {S}_1 + w_{j, 2} \\\\times \\\\mathcal {S}_2 + ... + w_{j, k} \\\\times \\\\mathcal {S}_k\\r\\n\\r\\n(14)\\r\\n\\r\\nTherefore, each slice of the output feature map is the weighted sum of all\\r\\nslices of the input feature map, and the weights are the parameters of the 1 × 1\\r\\nconvolution. They behave the same as the attention weights in self-attention.\\r\\nBy updating the parameters of the convolution during training, we can “learn”\\r\\nthe attention weights to better fuse the values mapping from the input of the\\r\\nPoint-Wise Extraction module. In summary, the Point-Wise Extraction module\\r\\ncan be formulated as:\\r\\n \\\\widehat {\\\\mathcal {F}}=Concat(\\\\mathcal {S}, \\\\mathcal {F}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ \\\\widehat {\\\\mathcal {F}}^{'}=\\\\mathscr {L}(\\\\widehat {\\\\mathcal {F}}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ Attention= 1 \\\\times 1 \\\\ Convolution(\\\\widehat {\\\\mathcal {F}}^{'}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ \\\\mathcal {S}^{'}, \\\\mathcal {F}^{'} = Split(Attention). \\\\label {eq5}\\r\\n\\r\\n(18)\\r\\nwhere S refers to the input feature map, F refers to the structure token and L\\r\\nis our redesigned mapping strategy used to generate values from the slices of the\\r\\nconcatenated feature map.\\r\\n\\r\\n\\x0c\\n\\n16\\r\\n\\r\\nB\\r\\n\\r\\nVisualization\\r\\n\\r\\nWe add the visualization of slices of structure tokens from StructToken-CSE and\\r\\nStructToken-SSE. It can be seen from the visualization that all of our proposed\\r\\nmodels can capture the structure information well from the feature maps. Details\\r\\nare at section 4.5 of the body of our paper.\\r\\n\\r\\nImage StructToken/Feature map Block 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n96%\\r\\n\\r\\nSSE\\r\\n\\r\\n96%\\r\\n\\r\\nPWE\\r\\n\\r\\n97%\\r\\n\\r\\nperpixel\\r\\n\\r\\n97%\\r\\n\\r\\nFig. 4. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to “bed” class in structure token/feature map from top to bottom with similar performance of the two paradigms.\\r\\n“StructToken” refers to the slices from structure token and “Feature map” refers to the\\r\\nslices from feature maps. We design a four blocks decoder as a per-pixel classification\\r\\nparadigm counterpart. “CSE”, “SSE”, “PWE”, “per-pixel” refers to StructToken-CSE,\\r\\nStructToken-SSE, StructToken-PWE and our per-pixel classification paradigm counterpart respectively. “Block 1-4” represents the slices from the structure token/feature\\r\\nmaps after each block. “Output” means the slices from the output score map. “IoU”\\r\\nrepresents the intersect over union score of the specific class in this image.\\r\\n\\r\\n\\x0c\\n\\n17\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n85%\\r\\n\\r\\nSSE\\r\\n\\r\\n84%\\r\\n\\r\\nPWE\\r\\n\\r\\n84%\\r\\n\\r\\nperpixel\\r\\n\\r\\n85%\\r\\n\\r\\nFig. 5. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to “building” class in structure token/feature map from top to bottom. The meaning of the labels is the same as in Fig.\\r\\n4.\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n95%\\r\\n\\r\\nSSE\\r\\n\\r\\n95%\\r\\n\\r\\nPWE\\r\\n\\r\\n96%\\r\\n\\r\\nperpixel\\r\\n\\r\\n95%\\r\\n\\r\\nFig. 6. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to “car” class in structure token/feature map from top to bottom. The meaning of the labels is the same as in\\r\\nFig. 4.\\r\\n\\r\\n\\x0c\\n\\n18\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n93%\\r\\n\\r\\nSSE\\r\\n\\r\\n94%\\r\\n\\r\\nPWE\\r\\n\\r\\n94%\\r\\n\\r\\nperpixel\\r\\n\\r\\n92%\\r\\n\\r\\nFig. 7. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to “door” class in structure token/feature map from top to bottom. The meaning of the labels is the same as in\\r\\nFig. 4.\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n94%\\r\\n\\r\\nSSE\\r\\n\\r\\n94%\\r\\n\\r\\nPWE\\r\\n\\r\\n94%\\r\\n\\r\\nperpixel\\r\\n\\r\\n94%\\r\\n\\r\\nFig. 8. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to “mountain” class in structure token/feature map from top to bottom. The meaning of the labels is the same as in Fig.\\r\\n4.\\r\\n\\r\\n\\x0c\\n\\n19\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n90%\\r\\n\\r\\nSSE\\r\\n\\r\\n91%\\r\\n\\r\\nPWE\\r\\n\\r\\n90%\\r\\n\\r\\nperpixel\\r\\n\\r\\n90%\\r\\n\\r\\nFig. 9. Another visualization of the slices corresponding to “mountain” class in structure token/feature map from top to bottom. The meaning of the labels is the same as\\r\\nin Fig. 4.\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n91%\\r\\n\\r\\nSSE\\r\\n\\r\\n89%\\r\\n\\r\\nPWE\\r\\n\\r\\n90%\\r\\n\\r\\nperpixel\\r\\n\\r\\n90%\\r\\n\\r\\nFig. 10. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to “tree” class in structure token/feature map from top to bottom. The meaning of the labels is the same as in\\r\\nFig. 4.\\r\\n\\r\\n\\x0c\\n\\n20\\r\\n\\r\\nReferences\\r\\n1. Alhaija, H.A., Mustikovela, S.K., Mescheder, L., Geiger, A., Rother, C.: Augmented reality meets deep learning for car instance segmentation in urban scenes.\\r\\nIn: British machine vision conference. vol. 1, p. 2 (2017)\\r\\n2. Caesar, H., Uijlings, J., Ferrari, V.: Coco-stuff: Thing and stuff classes in context.\\r\\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition.\\r\\npp. 1209–1218 (2018)\\r\\n3. Cao, Y., Xu, J., Lin, S., Wei, F., Hu, H.: Gcnet: Non-local networks meet squeezeexcitation networks and beyond. In: Proceedings of the IEEE/CVF International\\r\\nConference on Computer Vision Workshops. pp. 0–0 (2019)\\r\\n4. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and\\r\\nfully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence 40(4), 834–848 (2018). https://doi.org/10.1109/TPAMI.2017.2699184\\r\\n5. Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution\\r\\nfor semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)\\r\\n6. Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with\\r\\natrous separable convolution for semantic image segmentation. In: Proceedings of\\r\\nthe European conference on computer vision (ECCV). pp. 801–818 (2018)\\r\\n7. Chen, W., Zhu, X., Sun, R., He, J., Li, R., Shen, X., Yu, B.: Tensor low-rank\\r\\nreconstruction for semantic segmentation. In: European Conference on Computer\\r\\nVision. pp. 52–69. Springer (2020)\\r\\n8. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Maskedattention mask transformer for universal image segmentation. arXiv preprint\\r\\narXiv:2112.01527 (2021)\\r\\n9. Cheng, B., Schwing, A.G., Kirillov, A.: Per-pixel classification is not all you need\\r\\nfor semantic segmentation (2021)\\r\\n10. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\\r\\nFranke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\\r\\nunderstanding. In: Proceedings of the IEEE conference on computer vision and\\r\\npattern recognition. pp. 3213–3223 (2016)\\r\\n11. Ding, H., Jiang, X., Shuai, B., Liu, A.Q., Wang, G.: Semantic correlation promoted shape-variant context for segmentation. In: Proceedings of the IEEE/CVF\\r\\nConference on Computer Vision and Pattern Recognition. pp. 8885–8894 (2019)\\r\\n12. Ding, X., Guo, Y., Ding, G., Han, J.: Acnet: Strengthening the kernel skeletons for\\r\\npowerful cnn via asymmetric convolution blocks. In: Proceedings of the IEEE/CVF\\r\\nInternational Conference on Computer Vision. pp. 1911–1920 (2019)\\r\\n13. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\\r\\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\\r\\nAn image is worth 16x16 words: Transformers for image recognition at scale. ICLR\\r\\n(2021)\\r\\n14. Feng, D., Haase-Schütz, C., Rosenbaum, L., Hertlein, H., Glaeser, C., Timm, F.,\\r\\nWiesbeck, W., Dietmayer, K.: Deep multi-modal object detection and semantic\\r\\nsegmentation for autonomous driving: Datasets, methods, and challenges. IEEE\\r\\nTransactions on Intelligent Transportation Systems (2020)\\r\\n15. Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu, H.: Dual attention network for\\r\\nscene segmentation. In: Proceedings of the IEEE/CVF Conference on Computer\\r\\nVision and Pattern Recognition. pp. 3146–3154 (2019)\\r\\n\\r\\n\\x0c\\n\\n21\\r\\n16. Harders, M., Szekely, G.: Enhancing human-computer interaction in medical segmentation. Proceedings of the IEEE 91(9), 1430–1442 (2003)\\r\\n17. He, J., Deng, Z., Qiao, Y.: Dynamic multi-scale filters for semantic segmentation.\\r\\nIn: Proceedings of the IEEE/CVF International Conference on Computer Vision.\\r\\npp. 3562–3572 (2019)\\r\\n18. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\\r\\nProceedings of the IEEE conference on computer vision and pattern recognition.\\r\\npp. 770–778 (2016)\\r\\n19. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.\\r\\nIn: European conference on computer vision. pp. 630–645. Springer (2016)\\r\\n20. Huang, S., Wang, D., Wu, X., Tang, A.: Dsanet: Dual self-attention network for\\r\\nmultivariate time series forecasting. In: Proceedings of the 28th ACM international\\r\\nconference on information and knowledge management. pp. 2129–2132 (2019)\\r\\n21. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: Ccnet: Criss-cross\\r\\nattention for semantic segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 603–612 (2019)\\r\\n22. Jin, Z., Gong, T., Yu, D., Chu, Q., Wang, J., Wang, C., Shao, J.: Mining contextual information beyond image for semantic segmentation. In: Proceedings of the\\r\\nIEEE/CVF International Conference on Computer Vision. pp. 7231–7241 (2021)\\r\\n23. Jin, Z., Liu, B., Chu, Q., Yu, N.: Isnet: Integrate image-level and semantic-level\\r\\ncontext for semantic segmentation. In: Proceedings of the IEEE/CVF International\\r\\nConference on Computer Vision. pp. 7189–7198 (2021)\\r\\n24. Kirillov, A., Girshick, R., He, K., Dollár, P.: Panoptic feature pyramid networks.\\r\\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition. pp. 6399–6408 (2019)\\r\\n25. Li, X., Yang, Y., Zhao, Q., Shen, T., Lin, Z., Liu, H.: Spatial pyramid based graph\\r\\nreasoning for semantic segmentation. In: Proceedings of the IEEE/CVF Conference\\r\\non Computer Vision and Pattern Recognition. pp. 8950–8959 (2020)\\r\\n26. Li, X., Zhong, Z., Wu, J., Yang, Y., Lin, Z., Liu, H.: Expectation-maximization\\r\\nattention networks for semantic segmentation. In: Proceedings of the IEEE/CVF\\r\\nInternational Conference on Computer Vision. pp. 9167–9176 (2019)\\r\\n27. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature\\r\\npyramid networks for object detection. In: Proceedings of the IEEE conference on\\r\\ncomputer vision and pattern recognition. pp. 2117–2125 (2017)\\r\\n28. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\\r\\ntransformer: Hierarchical vision transformer using shifted windows. International\\r\\nConference on Computer Vision (ICCV) (2021)\\r\\n29. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\\r\\nsegmentation. In: Proceedings of the IEEE conference on computer vision and\\r\\npattern recognition. pp. 3431–3440 (2015)\\r\\n30. MMSegmentation Contributors: OpenMMLab Semantic Segmentation Toolbox\\r\\nand Benchmark (7 2020), https://github.com/open-mmlab/mmsegmentation\\r\\n31. Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction.\\r\\nICCV (2021)\\r\\n32. Strudel, R., Garcia, R., Laptev, I., Schmid, C.: Segmenter: Transformer for semantic segmentation. In: Proceedings of the IEEE/CVF International Conference on\\r\\nComputer Vision (ICCV). pp. 7262–7272 (October 2021)\\r\\n33. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\\r\\nL., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017)\\r\\n\\r\\n\\x0c\\n\\n22\\r\\n34. Wu, T., Lu, Y., Zhu, Y., Zhang, C., Wu, M., Ma, Z., Guo, G.: Ginet: Graph interaction network for scene parsing. In: European Conference on Computer Vision.\\r\\npp. 34–51. Springer (2020)\\r\\n35. Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene\\r\\nunderstanding. In: Proceedings of the European Conference on Computer Vision\\r\\n(ECCV). pp. 418–434 (2018)\\r\\n36. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer:\\r\\nSimple and efficient design for semantic segmentation with transformers. arXiv\\r\\npreprint arXiv:2105.15203 (2021)\\r\\n37. Yin, M., Yao, Z., Cao, Y., Li, X., Zhang, Z., Lin, S., Hu, H.: Disentangled nonlocal neural networks. In: European Conference on Computer Vision. pp. 191–207.\\r\\nSpringer (2020)\\r\\n38. Yuan, Y., Chen, X., Wang, J.: Object-contextual representations for semantic segmentation. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow,\\r\\nUK, August 23–28, 2020, Proceedings, Part VI 16. pp. 173–190. Springer (2020)\\r\\n39. Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., Sun, Q.: Feature pyramid\\r\\ntransformer. In: European Conference on Computer Vision. pp. 323–339. Springer\\r\\n(2020)\\r\\n40. Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A.: Context\\r\\nencoding for semantic segmentation. In: Proceedings of the IEEE conference on\\r\\nComputer Vision and Pattern Recognition. pp. 7151–7160 (2018)\\r\\n41. Zhang, W., Pang, J., Chen, K., Loy, C.C.: K-Net: Towards unified image segmentation. In: NeurIPS (2021)\\r\\n42. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\\r\\nProceedings of the IEEE conference on computer vision and pattern recognition.\\r\\npp. 2881–2890 (2017)\\r\\n43. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang,\\r\\nT., Torr, P.H., Zhang, L.: Rethinking semantic segmentation from a sequence-tosequence perspective with transformers. In: CVPR (2021)\\r\\n44. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.: Semantic understanding of scenes through the ade20k dataset. International Journal\\r\\nof Computer Vision 127(3), 302–321 (2019)\\r\\n45. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable\\r\\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159\\r\\n(2020)\\r\\n46. Zhu, Z., Xu, M., Bai, S., Huang, T., Bai, X.: Asymmetric non-local neural networks for semantic segmentation. In: Proceedings of the IEEE/CVF International\\r\\nConference on Computer Vision. pp. 593–602 (2019)\\r\\n\\r\\n\\x0c\",\n",
       " 'AI Poincaré 2.0: Machine Learning Conservation Laws from Differential Equations\\r\\nZiming Liu,1 Varun Madhavan,2 and Max Tegmark1\\r\\n1\\r\\n\\r\\nDepartment of Physics, Massachusetts Institute of Technology, Cambridge, USA\\r\\n2\\r\\nIndian Institute of Technology Kharagpur, India\\r\\n(Dated: March 24, 2022)\\r\\n\\r\\narXiv:2203.12610v1 [cs.LG] 23 Mar 2022\\r\\n\\r\\nWe present a machine learning algorithm that discovers conservation laws from differential equations, both numerically (parametrized as neural networks) and symbolically, ensuring their functional independence (a non-linear generalization of linear independence). Our independence module\\r\\ncan be viewed as a nonlinear generalization of singular value decomposition. Our method can readily\\r\\nhandle inductive biases for conservation laws. We validate it with examples including the 3-body\\r\\nproblem, the KdV equation and nonlinear Schrödinger equation.\\r\\n\\r\\nI.\\r\\n\\r\\nINTRODUCTION\\r\\n\\r\\nThe importance of conservation laws (CLs) in physics\\r\\ncan hardly be overstated [1]. Physicists usually derive\\r\\nconservation laws with time-consuming pencil and paper methods, using different hand-crafted strategies for\\r\\neach specific specific problem. This motivates searching\\r\\nfor a general-purpose problem-agnostic approach. A few\\r\\nrecent papers have exploited machine learning to autodiscover conservation laws [2–5], but despite of promising\\r\\npreliminary results, these techniques are not guaranteed\\r\\nto discover all conservation laws. In this paper, we start\\r\\nwith differential equations defining a dynamical system\\r\\nand aim to discover all its conservations laws, either in in\\r\\nnumerical form (parameterized as neural networks) or in\\r\\nsymbolic form. The new method, named AI Poincaré 2.0\\r\\nsince it builds on [2], involves three major contributions:\\r\\nFirst, we present an algorithm based on manifold learning for finding a complete set of algebraically independent functions spanning a manifold. This is a non-linear\\r\\ngeneralization of singular value decomposition, and undoubtedly has applications far beyond the current paper.\\r\\nSecond, the problem setting in this paper is more abinitio than prior work in the sense that we deal with\\r\\ndifferential equations directly as opposed to trajectories.\\r\\nThe new setting is more analogous to Henry Poincaré’s\\r\\nsituation, so it should appeal to theoretical physicists and\\r\\nmathematicians who have symbolic equations for which\\r\\nnumerical simulations might be expensive or problematic.\\r\\nThird, selecting conservation laws with certain simple\\r\\nproperties is often desirable in physics. To the best of\\r\\nour knowledge, this paper is the first attempt to allow\\r\\nphysical inductive biases when searching for conservation\\r\\nlaws. Our framework can benefit from the ever-growing\\r\\nliterature of physics-informed learning [6] and physicsaugmented learning [7] to make the results more interpretable.\\r\\nIn the Methods section, we introduce our notation and\\r\\nAI Poincaré 2.0 algorithm. In the Results section, we\\r\\napply AI Poincaré 2.0 to ordinary and partial differential\\r\\nequations (illustrated in FIG. 2) to test its ability to autodiscover conservation laws.\\r\\n\\r\\n(a) Pipeline\\r\\n\\r\\n(b) Train CL networks\\r\\n\\r\\nf\\r\\n\\r\\nDifferential\\r\\nEquations\\r\\nNN front\\r\\n(1) Training\\r\\nCL\\r\\nnetworks\\r\\n\\r\\n𝛻H1\\r\\n\\r\\n𝛻H3\\r\\n\\r\\nSymbolic front\\r\\n𝛻H2\\r\\n\\r\\n(3) Symbolic\\r\\nSearch with\\r\\nfast rejection\\r\\n\\r\\n(2) Manifold\\r\\nLearning\\r\\nwhen to\\r\\nhalt\\r\\nNumber\\r\\nCL\\r\\nof CL\\r\\nformulas\\r\\n\\r\\nH1\\r\\n\\r\\nH2\\r\\n\\r\\n𝐳\\r\\n\\r\\nConservation Loss\\r\\n\\r\\nRegularization Loss\\r\\n\\r\\nH3\\r\\n\\r\\n𝐳\\r\\n\\r\\n𝐳\\r\\n\\r\\nFIG. 1: (a) The AI Poincaré 2.0 pipeline: The NN front\\r\\nleverages neural networks to learn conservation laws,\\r\\nwhile the symbolic front uses a fast rejection method to\\r\\nsearch for formulas. (b) Training multiple networks\\r\\nrequires minimizing each network’s conservation loss\\r\\ncombined with a function correlation penalty.\\r\\n\\r\\nII.\\r\\n\\r\\nMETHOD\\r\\n\\r\\nProblem and Notation We consider a first-order\\r\\nordinary differential equation (ODE) dz\\r\\ndt = f (z) where\\r\\nz ∈ Rs is the state vector and f : Rs → Rs is a vector\\r\\nfield. Hamiltonian systems\\r\\n\\x10 correspond\\r\\n\\x11 to the special case\\r\\n∂H0\\r\\n0\\r\\nwhere s is even and f = ∂H\\r\\n,\\r\\n−\\r\\nfor a Hamiltonian\\r\\n∂p\\r\\n∂x\\r\\nfunction H0 . A conserved quantity is a scalar function\\r\\nH(z) whose value remains constant along a trajectory\\r\\nz(t) determined by dz\\r\\ndt = f (z) with any initial condition\\r\\nz(t = 0) = z0 . A necessary and sufficient condition\\r\\nfor a scalar function H(z) being a conservation law is\\r\\nd\\r\\n∇H · f = 0, because dt\\r\\nH (z(t)) = ∇H · dz\\r\\ndt = ∇H · f .\\r\\nb\\r\\nWe use hats to denote unit vectors, e.g., f ≡ f /|f |. Our\\r\\ngoal is to discover all nc independent conserved quantities {H1 (z), H2 (z), · · · , Hnc (z)} numerically and symbolically, optionally with user-specified properties.\\r\\nDynamical systems of the form dz\\r\\ndt = f (z) are very\\r\\ngeneral because (1) higher-order ODEs, e.g. Newtonian mechanics, can always be transformed to first-order\\r\\nODEs by including derivatives as new variables in z, and\\r\\n\\r\\n\\x0c\\n\\nModel\\r\\n\\r\\n2\\r\\n\\r\\nIllustration\\r\\n\\r\\n2\\r\\n\\r\\n𝑖\\r\\n\\r\\n𝜕𝜓\\r\\n1 𝜕 2𝜓\\r\\n=−\\r\\n+ 𝑘 𝜓 2𝜓\\r\\n𝜕𝑡\\r\\n2 𝜕𝑥 2\\r\\n\\r\\n𝑥\\r\\n\\r\\n𝑥, 𝑘1\\r\\n\\r\\n𝑥, 𝑘1\\r\\n\\r\\n𝜕𝜙\\r\\n𝜕𝜙 𝜕 3 𝜙\\r\\n− 6𝜙\\r\\n+\\r\\n=0\\r\\n𝜕𝑡\\r\\n𝜕𝑥 𝜕𝑥 3\\r\\n\\r\\nNonlinear\\r\\nSchrÖdinger Equation\\r\\n\\r\\n3\\r\\n\\r\\n𝑘2 = 𝑘1\\r\\n\\r\\n𝑘1 = 4𝑘2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2 (𝜙), 3 (𝜙, 𝜙𝑥 ),\\r\\n4 (𝜙, 𝜙𝑥 , 𝜙𝑥𝑥 )\\r\\n\\r\\n4\\r\\n\\r\\n1 (𝜓), 2 (𝜓, 𝜓𝑥 ),\\r\\n3 (𝜓, 𝜓𝑥 , 𝜓𝑥𝑥 )\\r\\n\\r\\nDifferential Rank\\r\\n\\r\\nRank\\r\\n\\r\\nCLs\\r\\n\\r\\n1\\r\\n\\r\\n𝑥\\r\\n\\r\\n3\\r\\n\\r\\n𝑦\\r\\n\\r\\n𝑦, 𝑘2\\r\\n\\r\\n𝑦, 𝑘2\\r\\n\\r\\n𝑦\\r\\n\\r\\nKdV wave\\r\\nEquation\\r\\n\\r\\nThree-body\\r\\nProblem\\r\\n\\r\\n2D Anisotropic\\r\\n2D isotropic\\r\\nHarmonic Oscillator Harmonic Oscillator\\r\\n\\r\\nKepler\\r\\n\\r\\nFIG. 2: Tested ordinary and partial differential equation examples. AI Poincaré 2.0 is seen to find the correct\\r\\nnumber of CLs by computing rank (read off as the low flat region of the thick nef f curve as defined in [2]) or\\r\\ndifferential rank.\\r\\n\\r\\n(2) partial differential equations (PDEs) can be approximated by ODEs by discretizing space.\\r\\nAI Poincaré 2.0 consists of three steps: (a) learn\\r\\nconservation laws parameterized by neural networks, (b)\\r\\ncount the number of independent conservation laws and\\r\\n(c) find symbolic formulas for conservation laws. The\\r\\npipeline is illustrated in FIG. 1.\\r\\n(a) Parameterizing conservation laws by neural\\r\\nnetworks: We parameterize a conserved quantity as a\\r\\nneural network H(z; θ) where θ are model parameters.\\r\\nOur loss function is defined as\\r\\n`(θ) ≡\\r\\n\\r\\nP\\r\\n2\\r\\n1 X b (i) d (i)\\r\\nf (z ) · ∇H(z ; θ) ,\\r\\nP i=1\\r\\n\\r\\nR(θ1 , θ2 ) ≡\\r\\n\\r\\nwhere z(i) denotes the ith sample in phase space. ∇H(z)\\r\\ncan be easily computed with automatic differentiation [8].\\r\\nd are normalized unit vectors, to make\\r\\nNote that b\\r\\nf and ∇H\\r\\nthe loss function dimensionless and invariant under uninteresting re-scaling of H. We update θ by trying to\\r\\nminimize the loss function until it drops below a small\\r\\nthreshold \\x0f.\\r\\nTo obtain multiple conserved quantities, one can repeat the above method with different random seeds and\\r\\nhope to discover algebraically independent ones. In practice, however, we find that learned conservation laws are\\r\\noften highly correlated for different initializations [9]. To\\r\\nencourage linear independence between two neural networks, say, H1 and H2 , we add a regularization term\\r\\n\\r\\n2\\r\\n\\r\\n(2)\\r\\n\\r\\nto the loss function. Since we know that there cannot\\r\\nbe more conservation laws than degrees of freedom s,\\r\\nwe train n = s models together by minimizing the loss\\r\\nfunction `1 + λ`2 defined by\\r\\n`=\\r\\n\\r\\nn\\r\\nn\\r\\nn\\r\\nX\\r\\nX\\r\\n1X\\r\\n2\\r\\nR(θi , θj ), (3)\\r\\n`(θi ) +λ ×\\r\\nn i=1\\r\\nn(n − 1) i=1 j=i+1\\r\\n| {z }\\r\\n|\\r\\n{z\\r\\n}\\r\\n`1\\r\\n\\r\\n(1)\\r\\n\\r\\nP\\r\\n1 X [ (i)\\r\\n[2 (z(i) ; θ2 )\\r\\n∇H1 (z ; θ1 ) · ∇H\\r\\nP i=1\\r\\n\\r\\n`2\\r\\n\\r\\nwhere λ is a penalty coefficient. We refer to `1 and `2 as\\r\\nconservation loss and independence loss, respectively.\\r\\n(b) Counting the number of independent conserved quantities: After training, we aim to determine\\r\\n(in)dependence among these neural networks. Specifically, we are interested in functional independence, a direct generalization linear independence that we define\\r\\nand compute as described below.\\r\\nDefinition II.1. Functional independence. A set of\\r\\nnon-zero functions H1 (z), H2 (z), · · · , Hn (z) is independent if\\r\\nf (H1 (z), H2 (z), · · · , Hn (z)) = 0 =⇒ f = 0\\r\\n\\r\\n(4)\\r\\n\\r\\nor, equivalently, if no function Hi (z) can be constructed\\r\\nfrom (possibly nonlinear and multivalued) combinations\\r\\nof the other functions.\\r\\n\\r\\n\\x0c\\n\\n3\\r\\nDefinition II.2. Function set rank. The function set\\r\\nH = {H1 (z), H2 (z), · · · Hn (z)} has rank k ≤ n if it contains k but not k + 1 functions that are independent.\\r\\nComputing the function set rank We determine\\r\\nthe rank k with a nonlinear manifold learning method.\\r\\nWe define the matrix A such that Aij is the value of the\\r\\nj th neural network evaluated at the ith sample point:\\r\\n\\uf8eb\\r\\n\\uf8f6\\r\\nH1 (z(1) ) H2 (z(1) ) · · · Hn (z(1) )\\r\\n\\uf8ec H1 (z(2) ) H2 (z(2) ) · · · Hn (z(2) ) \\uf8f7\\r\\n\\uf8f7,\\r\\nA=\\uf8ec\\r\\n(5)\\r\\n\\uf8ed ···\\r\\n···\\r\\n···\\r\\n··· \\uf8f8\\r\\n(P )\\r\\n(P )\\r\\n(P )\\r\\nH1 (z ) H2 (z ) · · · Hn (z )\\r\\nwhere P \\x1d n is the number of data points z(i) . If we interpret each row of A as a point in Rn , then the matrix\\r\\ncorresponds to a point cloud in Rn located on a a manifold, whose dimensionality k is equal to the function set\\r\\nrank. If there are k independent linear conserved quantities (where Hi (z) are linear functions), then the point\\r\\ncloud will lie on a k-dimensional hyperplane that can\\r\\nreadily be discovered using singular value decomposition\\r\\n(SVD): k is then then the number of non-zero singular\\r\\nvalues, i.e., the rank of the matrix A. For our more\\r\\ngeneral nonlinear case, we wish to discover the manifold that the point cloud lies on even if it is curved. For\\r\\nthis, we exploit the manifold learning algorithm proposed\\r\\nin Poincare 1.0 [2] to measure the manifold dimensionality [10], which performs local Monte Carlo sampling\\r\\nfollowed by a linear dimensionality estimation method.\\r\\nTaking the derivative of f (H1 (z), H2 (z) · · · , Hn (z)) =\\r\\n0 from equation (4) with respect to zi gives.\\r\\n\\uf8eb\\r\\n\\uf8f6\\uf8eb \\uf8f6\\r\\nH1,1 H2,1 · · · Hn,1\\r\\nf,1\\r\\n\\uf8ecH1,2 H2,2 · · · Hn,2 \\uf8f7 \\uf8ec f,2 \\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8f7\\uf8ec \\uf8f7\\r\\n\\uf8ec ..\\r\\n..\\r\\n.. \\uf8f7 \\uf8ec .. \\uf8f7 = 0.\\r\\n\\uf8ed .\\r\\n(6)\\r\\n.\\r\\n. \\uf8f8\\uf8ed . \\uf8f8\\r\\n|\\r\\n\\r\\nH1,s H2,s · · · Hn,s\\r\\nf,n\\r\\n{z\\r\\n} | {z }\\r\\nB\\r\\n\\r\\n∇f\\r\\n\\r\\nThis means that, if {H1 , · · · , Hn } and f are differentiable\\r\\nfunctions and B has full rank, then ∇f (z) and therefore\\r\\nf (z) itself must vanish identically, so the functions Hi\\r\\nmust be independent. We exploit this to define differentiable independence and differentiable rank as follows:\\r\\nDefinition II.3. Differential functional independence. A set of n non-zero differentiable functions H is\\r\\ndifferentially independent if their gradients are linearly\\r\\nindependent, i.e., if rank B(z) = n almost everywhere\\r\\n(for all z except for a set of measure zero).\\r\\nDefinition\\r\\nII.4. differential\\r\\nfunction\\r\\nset\\r\\nrank.\\r\\nThe differential rank of the function\\r\\nset H = {H1 (z), H2 (z), · · · Hn (z)} is defined as\\r\\nkD = max rank B(z).\\r\\nz\\r\\n\\r\\nIn practice, it suffices to compute the maximum over\\r\\na finite number of points P \\x1d n: it is exponentially\\r\\nunlikely that such sampling will underestimate the true\\r\\n\\r\\nmanifold dimensionality, just as it is exponentially unlikely that P random points in 3-dimensional space will\\r\\nhappen to lie on a plane.\\r\\nNumerically, one can apply singular value decomposition to B to obtain singular values {σ1 , σ2 , · · · , σn }, and\\r\\ndefine the rank as the number of non-zero singular values.\\r\\nIn practice, we treat components as vanishing\\r\\nif the exP\\r\\nplained fraction of the total variance, σi2 / j σj2 , is below\\r\\n\\x0f = 10−2 .\\r\\nEquipped with the tools above, we can obtain the number of independent conserved quantities and determine\\r\\nthem numerically.\\r\\n(c) Discovering symbolic formulas When no domain knowledge is available for a physical system, we\\r\\nperform a brute-force search over symbolic formulas ordered by increasing complexity as in [11, 12]. We leverd = 0 to determine if a candidate\\r\\nage the criterion f̂ · ∇H\\r\\nfunction H(z) is a conserved quantity or not. We implement a brute force algorithm in C++ for speed and\\r\\nemploy a fast rejection strategy for further speedup: we\\r\\nprepare np = 10 test points in advance, and reject H\\r\\nd\\r\\nimmediately if b\\r\\nf (z) · ∇H(z)\\r\\n> \\x0fs = 10−4 for any test\\r\\npoint z. If a formula survives at the np test points, we\\r\\ntest thoroughly by checking the condition numerically\\r\\non the whole dataset, or test the condition symbolically.\\r\\nWe determine whether the new conserved quantity is independent of already discovered ones by checking if the\\r\\ndifferential function set rank increases by 1 when adding\\r\\nthe new conserved quantity. Appendix B provides further technical details.\\r\\nIncluding inductive biases in conservation law\\r\\nlearning Above we did not distinguish between integrals\\r\\nof motion and conserved quantities. As clarified in [13]\\r\\nand Appendix C , conserved quantities are special cases\\r\\nof Integrals of Motion. Conservation laws are usually derived from homogeneity and isotropy of time and space,\\r\\nand have the feature of being additive, i.e., expressible\\r\\nas a sum of simple terms involving only a small subset\\r\\nof the degrees of freedom. Conserved quantities of PDEs\\r\\nusually take the form of integral over space. We incorporate any such desired inductive biases into our method\\r\\nby restricting the neural networks parametrizing Hi (z)\\r\\nto have the corresponding properties.\\r\\n\\r\\nIII.\\r\\n\\r\\nRESULTS\\r\\n\\r\\nNumerical experiments We test AI Poincaré 2.0 on\\r\\nsix systems: the Kepler problem, isotropic/anisotropic\\r\\nharmonic oscillators, the gravitational three-body problem, the KdV wave equation and the nonlinear\\r\\nSchrödinger equation. The details of these systems are\\r\\nincluded in Appendix A. We obtain discrete samples\\r\\n(z(i) , f (i) ) , i = 1, · · · , P = 104 , by first sampling z uniformly from a box, z ∼ U [−2, 2]s , and then computing\\r\\nf (i) = f (z(i) ). For the Kepler problem and three-body\\r\\nproblem, we drop samples whenever a two-body distance\\r\\n\\r\\n\\x0c\\n\\n4\\r\\nSystem\\r\\nKepler Problem\\r\\n\\r\\nIntegrals of Motion or Conservation Laws\\r\\nH1 = 12 (p2x + p2y ) − √ 21 2\\r\\nx +y\\r\\n\\r\\nReverse Polish Notation Discovered\\r\\npx Qpy Q+rIoYes\\r\\n\\r\\nH2 = xpy − ypx\\r\\nxpy *ypx *H3 = (xpy − ypx )py + r̂x\\r\\nxpy *ypx *-py *xr/+\\r\\nH1 = 12 (x2 + p2x )\\r\\nxQ*px Q+\\r\\nIsotropic Oscillator\\r\\nH2 = 21 (y 2 + p2y )\\r\\nyQpy Q+\\r\\nH3 = xpy − ypx\\r\\nxpy *ypx H1 = 12 (x2 + p2x )\\r\\nxQ*px Q+\\r\\n2\\r\\n2\\r\\n1\\r\\nAnisotropic Oscillator\\r\\nH\\r\\nyQOOpy Q+\\r\\n2 = 2 (4y + py )\\r\\n√\\r\\nH3 = x H1 H2 − l2 − lpx (l = xpy − 2ypx )\\r\\nH H *lQ-Rx*lpx *P\\r\\nP 1 2\\r\\n1\\r\\n1\\r\\n1\\r\\nH1 = 3i=1 21 (p2i,x + p2i,y ) − ( r12\\r\\n+ r13\\r\\n+ r23\\r\\n)\\r\\npi,x Qpi,y Q+ri(i+1) IOiP\\r\\nP3\\r\\nx\\r\\np\\r\\n−\\r\\ny\\r\\np\\r\\nH\\r\\n=\\r\\npi,y *yi pi,x *i\\r\\ni,y\\r\\ni\\r\\ni,x\\r\\n2\\r\\ni=1P\\r\\ni xiP\\r\\nThree Body Problem\\r\\nH3 = 3i=1 pi,x\\r\\np\\r\\nP\\r\\nPi i,x\\r\\nH4 = R3i=1 pi,y\\r\\ni pi,y\\r\\nH1 = R φ dx\\r\\nφ\\r\\nKdV\\r\\nH2R = φ2 dx\\r\\nφQ\\r\\n3\\r\\n2\\r\\nH3 = (2φ\\r\\nφQφ*O|ψx |QR −2φx ) dx\\r\\nHR1 = |ψ| dx\\r\\n|psi|Q\\r\\nNonlinear Schrödinger\\r\\nH2 = (|ψx |2 + |ψ|4 ) dx\\r\\n|ψx |Q|ψ|QQ+\\r\\n\\r\\nYes\\r\\nNo\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nNo\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\n\\r\\nTABLE I: 16 of the 18 conservation laws were discovered not only numerically, but also symbolically using our\\r\\nfast-rejection brute force search limited to 9 distinct symbols.\\r\\n\\r\\nis smaller than rc = 0.5. For the two PDEs, we we\\r\\nrepresent each function by a 240-dimensional vector z\\r\\nparametrizing the first 6 terms of its Taylor expansion at\\r\\n40 uniformly spaced points on the interval [−10, 10], and\\r\\nwe generate random sample points z(i) for random linear combination of Gaussians as detailed in Appendix D.\\r\\nThe neural network has 2 hidden layers, each containing\\r\\n256 neurons with SiLU activation, and is trained with\\r\\nthe Adam optimizer [14] for 100 epochs. When training\\r\\nmultiple networks simultaneously, we choose the regularization coefficient λ = 0.02 as detailed in Appendix D.\\r\\nIntegrals of Motion (IOMs): Any first-order differential equation with s degrees of freedom has s − 1\\r\\nIOMs, simply because trajectories are one-dimensional\\r\\n[13]. For example, the 2D Kepler problem (s = 4) has\\r\\ns − 1 = 3 IOMs, which are all recognized by physicists as\\r\\nconservation laws: energy, angular momentum and the\\r\\nLaplace-Runge-Lenz vector specifying the ellipse orientation. In FIG. 2, we test two other systems with s = 4\\r\\ndegrees of freedom: a 2D isotropic oscillator and a 2D\\r\\nanisotropic oscillator. We see that our NN front (using\\r\\neither rank definition) indeed obtains nc = 3 independent\\r\\nIOMs.\\r\\nSomething amusing happened for the anisotropic oscillator example. The first author, despite passing his\\r\\nclassical mechanics exam with full score, expected two\\r\\nIOMs rather than three because the angular momentum\\r\\nis not conserved for the anisotropic oscillator. However,\\r\\nAI Poincaré insisted there were three IOMs. The authors\\r\\neventually realized that AI Poincaré was right: a third\\r\\nIOM is indeed present, although poorly known among\\r\\nphysicists [15].\\r\\nConservation laws For the famous 2D gravitational\\r\\nthree-body problem, the 12-dimensional state vector z =\\r\\n{xi , yi , vi,x , vi,y }i=1,2,3 specifies the positions and veloci-\\r\\n\\r\\nties of three bodies. Although there are 12-1=11 IOMs,\\r\\nonly 4 are recognized as conservation laws by physicists:\\r\\nenergy, momentum (along the x- and y-axes) and angular momentum (along the z-axis). To incorporate the\\r\\nabove-mentioned inductive biases, we assume that a conserved quantity decomposes into 1-body terms and 2body terms. We assume nothing about the 1-body terms,\\r\\nbut assume translational and rotational invariance for the\\r\\n2-body terms. As a result, a candidate conservation law\\r\\nmust have the form:\\r\\n\\r\\nH=\\r\\n\\r\\n3\\r\\nX\\r\\ni=1\\r\\n\\r\\ng(xi , yi , vi,x , vi,y ) +\\r\\n\\r\\n3\\r\\n3\\r\\nX\\r\\nX\\r\\n\\r\\nh(rij )\\r\\n\\r\\n(7)\\r\\n\\r\\ni=1 j=i+1\\r\\n\\r\\np\\r\\nwhere rij ≡\\r\\n(xj − xi )2 + (yj − yi )2 .\\r\\nBy parameterizing g and h as two separate neural networks,\\r\\nthe learned conservation laws automatically satisfy the\\r\\nabove-mentioned desired physical properties. Our algorithm now discovers precisely 4 independent conservation\\r\\nlaws is indeed 4, agreeing with physicists’ intuition.\\r\\nAnother set of interesting systems are partial differential equations (PDE) in the form ut = f (u, ux , uxx , · · · ).\\r\\nSince a field has infinite number of degrees of freedom\\r\\n(hence infinitely many IOMs), it is crucial to constrain\\r\\nthe form of conservation laws to exclude trivial ones. In\\r\\nquantum mechanics, for example, any projector onto an\\r\\neigenstate is an IOM, but these less profound than probability conservation (known as unitarity), energy conservation etc. Thus we focus on conservation laws with an\\r\\nintegral form obeying translational invariance:\\r\\nZ\\r\\nH = h(u, |ux |, |uxx |, · · · ) dx\\r\\n(8)\\r\\nIn practice, we replace the integral by a sum over the\\r\\n\\r\\n\\x0c\\n\\n5\\r\\n40 aforementioned grid points. Moreover, we take the\\r\\nabsolute value of derivatives, e.g., |ux | and |uxx |, to avoid\\r\\ntrivial “conserved quantities” of the total derivative form\\r\\nd\\r\\nF (u, ux , uxx , ...), e.g., ux , uux , or uxx , which are\\r\\nh = dx\\r\\nconserved simply due to zero boundary conditions.\\r\\nThe Korteweg–De Vries (KdV) equation φt + φxxx −\\r\\n6φφx = 0 describes shallow water waves and is of particular interest in mathematical physics. It is known to\\r\\nhave infinitely many conservation laws [16]. Of course\\r\\nour framework cannot handle infinity, but conservation\\r\\nlaws involving low-order derivatives are finite and important in physics. For example, the first three conservation laws involve no more than the first derivative, corresponding to mass, momentum and energy conservation\\r\\nrespectively. We feed our neural network with (1) φ only;\\r\\n(2) φ and |φx |; (3) φ, |φx | and |φxx |, and our method\\r\\npredicts 2, 3 and 4 conservation laws respectively, which\\r\\nagree exactly with the ground truth.\\r\\nThe Nonlinear Schrödinger equation (NLS) iψt =\\r\\n− 12 ψxx + κ|ψ|2 ψ can describe interesting phenomena including Bose-Einstein condensation. It also has infinitely\\r\\nmany conservation laws, but as in the last example, conservation laws of interest to physicists involve only loworder derivatives. For example, probability, momentum\\r\\nand energy conservation include no more than the first\\r\\nderivative. We feed the neural network with (1) ψ only;\\r\\n(2) ψ and |ψx |; (3) ψ, |ψx | and |ψxx |, and our method predicts 1, 2 and 3 conservation laws respectively, which ba-\\r\\n\\r\\nsically agree with the ground truth, although our method\\r\\nis unable to discover the momentum which involves ψx\\r\\nbecause the input |ψx | lacks the phase information. We\\r\\nwould like to investigate how to include more information to facilitate learning the momentum while avoiding\\r\\ntrivial solutions.\\r\\n\\r\\nIV.\\r\\n\\r\\nCONCLUSIONS\\r\\n\\r\\nWe have presented a method that, given some differential equations, can determine not only the number of\\r\\nindependent conserved quantities, but also obtain neural\\r\\n(or even symbolic) representations of them. Conservation laws and integrability have many competing definitions listed in Appendix C, and AI Poincaré 2.0 is able to\\r\\nadapt to all of them. This new tool may accelerate future\\r\\nprogress on exciting open physics problems, for example\\r\\nintegrability of quantum many-body systems and manybody localization.\\r\\nAcknowledgements We thank Bohan Wang, Di Luo\\r\\nand Sijing Du for helpful discussions and the Center\\r\\nfor Brains, Minds, and Machines (CBMM) for hospitality. This work was supported by The Casey and Family\\r\\nFoundation, the Foundational Questions Institute, the\\r\\nRothberg Family Fund for Cognitive Science and IAIFI\\r\\nthrough NSF grant PHY-2019786.\\r\\n\\r\\n[1] P.\\r\\nW.\\r\\nAnderson,\\r\\nMore\\r\\nis\\r\\ndif[9]\\r\\nferent,\\r\\nScience\\r\\n177,\\r\\n393\\r\\n(1972),\\r\\nhttps://science.sciencemag.org/content/177/4047/393.full.pdf.\\r\\n[10]\\r\\n[2] Z. Liu and M. Tegmark, Machine learning conservation\\r\\nlaws from trajectories, Phys. Rev. Lett. 126, 180604\\r\\n(2021).\\r\\n[3] Y. ichi Mototake, Interpretable conservation law estimation by deriving the symmetries of dynamics from trained\\r\\ndeep neural networks, in Machine Learning and the Physical Sciences Workshop at the 33rd Conference on Neural Information Processing Systems (NeurIPS) (2019)\\r\\narXiv:2001.00111 [physics.data-an].\\r\\n[4] S. J. Wetzel, R. G. Melko, J. Scott, M. Panju, and\\r\\n[11]\\r\\nV. Ganesh, Discovering symmetry invariants and conserved quantities by interpreting siamese neural networks, Phys. Rev. Research 2, 033499 (2020).\\r\\n[5] S. Ha and H. Jeong, Discovering conservation laws\\r\\n[12]\\r\\nfrom trajectories via machine learning, arXiv preprint\\r\\narXiv:2102.04008 (2021).\\r\\n[6] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris,\\r\\nS. Wang, and L. Yang, Physics-informed machine learn[13]\\r\\ning, Nature Reviews Physics 3, 422 (2021).\\r\\n[7] Z. Liu, Y. Chen, Y. Du, and M. Tegmark, Physicsaugmented learning: A new paradigm beyond physics[14]\\r\\ninformed learning, arXiv preprint arXiv:2109.13901\\r\\n(2021).\\r\\n[15]\\r\\n[8] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning\\r\\n(MIT press, 2016).\\r\\n\\r\\nThis seems to imply some ‘simpler’ conservation laws are\\r\\npreferred by neural networks over others.\\r\\nAlthough the nonlinear manifold learning method introduced in AI Poincaré 1.0 also applies here, the ways to\\r\\ncompute the number of conserved quantities nc is different and actually dual. In Poincaré 1.0, nc is the phase\\r\\nspace dimension minus the dimension of the trajectory\\r\\nmanifold. While in this paper, nc is equal to the dimension of the manifold. Because of this duality, the\\r\\nexplained ratio diagram (ERD) in Poincare 1.0 resembles a hill while in FIG. 2 the ERD is upside down and\\r\\nresembles a valley.\\r\\nS.-M. Udrescu and M. Tegmark, Ai feynman:\\r\\nA physics-inspired method for symbolic regression,\\r\\nScience\\r\\nAdvances\\r\\n6,\\r\\neaay2631\\r\\n(2020),\\r\\nhttps://www.science.org/doi/pdf/10.1126/sciadv.aay2631.\\r\\nS.-M. Udrescu, A. Tan, J. Feng, O. Neto, T. Wu, and\\r\\nM. Tegmark, Ai feynman 2.0: Pareto-optimal symbolic\\r\\nregression exploiting graph modularity, Advances in Neural Information Processing Systems 33, 4860 (2020).\\r\\nL. Landau and E. Lifshitz, Mechanics third edition: Volume 1 of course of theoretical physics, Elsevier Science\\r\\n(1976).\\r\\nD. P. Kingma and J. Ba, Adam: A method for stochastic\\r\\noptimization, arXiv preprint arXiv:1412.6980 (2014).\\r\\nG. Arutyunov, Liouville integrability, in Elements of\\r\\nClassical and Quantum Integrable Systems (Springer International Publishing, Cham, 2019) pp. 1–68.\\r\\n\\r\\n\\x0c\\n\\n6\\r\\n[16] R. M. Miura, C. S. Gardner, and M. D. Kruskal,\\r\\nKorteweg-de vries equation and generalizations. ii. existence of conservation laws and constants of motion, Journal of Mathematical Physics 9, 1204 (1968),\\r\\nhttps://doi.org/10.1063/1.1664701.\\r\\n[17] V. A. Dulock and H. V. McIntosh, On the degeneracy of the two-dimensional harmonic oscillator, American Journal of Physics 33, 109 (1965),\\r\\nhttps://doi.org/10.1119/1.1971258.\\r\\n[18] R. M. Miura, C. S. Gardner, and M. D. Kruskal,\\r\\nKorteweg-de vries equation and generalizations. ii. existence of conservation laws and constants of motion, Journal of Mathematical Physics 9, 1204 (1968),\\r\\nhttps://doi.org/10.1063/1.1664701.\\r\\n[19] J. Barrett, Title : The local conservation laws of the\\r\\nnonlinear schrodinger equation (2013).\\r\\n\\r\\n[20] Informally speaking, an integrable system is a dynamical\\r\\nsystem with sufficiently many conserved quantities.\\r\\n[21] Wikipedia contributors, Integrable system — Wikipedia,\\r\\nthe free encyclopedia, https://en.wikipedia.org/\\r\\nw/index.php?title=Integrable_system&oldid=\\r\\n1058752403 (2021), [Online; accessed 5-February-2022].\\r\\n[22] J. VICKERS, Integrable systems: Twistors, loop groups,\\r\\nand riemann surfaces (oxford graduate texts in mathematics 4) by n. j. hitchin, g. b. segal and r. s. ward: 136\\r\\npp., £25.00, isbn 0-19-850421-7 (clarendon press, oxford,\\r\\n1999)., Bulletin of the London Mathematical Society 33,\\r\\n116–127 (2001).\\r\\n[23] Z. Liu and M. Tegmark, Machine-learning hidden symmetries, arXiv preprint arXiv:2109.09721 (2021).\\r\\n[24] Wikipedia contributors, Frobenius theorem (differential\\r\\ntopology) — Wikipedia, the free encyclopedia, https:\\r\\n//en.wikipedia.org/w/index.php?title=Frobenius_\\r\\ntheorem_(differential_topology)&oldid=1049676730\\r\\n(2021), [Online; accessed 5-February-2022].\\r\\n\\r\\n\\x0c\\n\\n7\\r\\nAppendix A: Dynamical Systems\\r\\n\\r\\nOur method can be applied to dynamical systems of the form\\r\\nphysical systems used to test it.\\r\\n\\r\\n1.\\r\\n\\r\\ndz\\r\\ndt\\r\\n\\r\\n= f (z). Here we include technical details of the\\r\\n\\r\\nHarmonic Oscillator (2D)\\r\\n\\r\\nThe Harmonic Oscillator (2D) is described by two coordinates (x, y) and two momenta (px , py ).\\r\\n\\uf8eb\\r\\n\\uf8f6\\r\\n\\uf8f6\\r\\npx /m\\r\\nx\\r\\n\\uf8ec −k x \\uf8f7\\r\\n\\uf8ecp \\uf8f7\\r\\nz = \\uf8ed x \\uf8f8 , f (z) = \\uf8ed 1 \\uf8f8 ,\\r\\npy /m\\r\\ny\\r\\n−k2 y\\r\\npy\\r\\n\\uf8eb\\r\\n\\r\\n(A1)\\r\\n\\r\\nwhere m is the mass, and k1 and k2 are spring constants. When k1 6= k2 , the system is anisotropic and has two obvious\\r\\n1 2\\r\\n1 2\\r\\nconserved quantities: (1) x-energy H1 = 21 k1 x2 + 2m\\r\\npx and (2) y-energy H2 = 12 k2 y 2 + 2m\\r\\npy . The third conserved\\r\\np\\r\\nquantity is less studied by physicists but still exists if k1 /k2 is a rational number [15]. When k1 = k2 , the system is\\r\\nisotropic and has three conserved quantities. Besides H1 and H2 , angular momentum H3 = xpy −ypx is also conserved.\\r\\nFor the isotropic case, we choose m = k1 = k2 = 1; for the anisotropic case, we choose m = k1 = 1, k2 = 4. Samples\\r\\nare drawn from the uniform distribution z ∼ U [−2, 2]4 . We include more physics discussion below for completeness:\\r\\nisotropic case In the isotropic case k1 = k2 = m = 1, there are four conservation laws [17]:\\r\\n\\r\\n2Ex = x2 + p2x ,\\r\\n\\r\\n2Ey = y 2 + p2y ,\\r\\n\\r\\nL = ypx − xpy ,\\r\\n\\r\\nK = xy + px py\\r\\n\\r\\n(A2)\\r\\n\\r\\nbut they are dependent because L2 + K 2 = 4Ex Ey . Ex , Ey and L are more common in physics, while K is less\\r\\ncommon. However, there is no need to prefer L over K. In fact, our symbolic module discovers the three conserved\\r\\nquantities 2Ex , 2Ey , K and then ignores L because of its dependence on the other three quantities.\\r\\nAnisotropic case Although conservation laws are explicitly constructed in [15], let us consider the specific case\\r\\nm = k1 = 1, k2 = 4. The equations of motion are:\\r\\n\\uf8eb \\uf8f6 \\uf8eb\\r\\n\\uf8f6\\r\\nx\\r\\nvx\\r\\nd \\uf8ecvx \\uf8f7 \\uf8ec −x \\uf8f7\\r\\n\\uf8ed \\uf8f8 = \\uf8ed v \\uf8f8.\\r\\ndt y\\r\\ny\\r\\nvy\\r\\n−4y\\r\\n\\r\\n(A3)\\r\\n\\r\\n\\uf8eb \\uf8f6 \\uf8eb\\r\\n\\uf8f6\\r\\nx\\r\\nAx sin(t + ϕx )\\r\\n\\uf8ecvx \\uf8f7 \\uf8ec Ax cos(t + ϕx ) \\uf8f7\\r\\n\\uf8ed y \\uf8f8 = \\uf8ed A sin(2t + ϕ ) \\uf8f8\\r\\ny\\r\\ny\\r\\nvy\\r\\n2Ay cos(2t + ϕy )\\r\\n\\r\\n(A4)\\r\\n\\r\\nSolving the equation yields the trajectory\\r\\n\\r\\nwith arbitraty constants Ax , Ay , ϕx and ϕy .\\r\\nWe define angular momentum\\r\\nL(1) ≡ xpy − ypx = 2Ax Ay (sin(t + ϕ1 − ϕ2 )).\\r\\n\\r\\n(A5)\\r\\n\\r\\np\\r\\nAlthough L(1) is not conserved, nor is K (1) ≡ (2Ax Ay )2 − L(1)2 = 2Ax Ay cos(t + ϕ1 − ϕ2 ). The trajectory of\\r\\nz0 ≡ (x, vx , L(1) , K (1) ) can be generated from an isotropic harmonic oscillator, because all components have the same\\r\\nangular frequency. Hence the ‘angular momentum’ is conserved:\\r\\nL(2) ≡ xK (1) − yL(1) = x(xpy − ypx ) − y\\r\\n\\r\\nq\\r\\n(x2 + p2x )(y 2 + p2y ) − (xpy − ypx )2\\r\\n\\r\\n(A6)\\r\\n\\r\\n\\x0c\\n\\n8\\r\\n2.\\r\\n\\r\\n2D Kepler Problem\\r\\n\\r\\nThe 2D Kepler Problem is described by two coordinates (x, y) and two velocity components (vx , vy ),\\r\\n\\uf8eb\\r\\n\\uf8f6\\r\\n\\uf8eb \\uf8f6\\r\\nvx\\r\\nx\\r\\n\\uf8ec−GM x/(x2 + y 2 )3/2 \\uf8f7\\r\\n\\uf8ecv \\uf8f7\\r\\n\\uf8f7\\r\\nz = \\uf8ed x \\uf8f8 , f (z) = \\uf8ec\\r\\n\\uf8ed\\r\\n\\uf8f8\\r\\nvy\\r\\ny\\r\\nvy\\r\\n−GM y/(x2 + y 2 )3/2\\r\\n\\r\\n(A7)\\r\\n\\r\\nwhere G is the gravitational constant, M and m are the\\r\\nsun and the planet, respectively. The system has\\r\\np mass of the\\r\\nm 2\\r\\n2\\r\\n2\\r\\nthree conserved quantities: (1) energy H1 = −GM m/ x + y + 2 (vx + vy2 ); (2) angular momentum H2 = m(xvy −\\r\\nv H +GM r̂\\r\\nyvx ); (3) The direction of the Runge-lenz vector H3 = arctan( −vxy H22 +GM r̂yx ) where r̂ ≡ (r̂x , r̂y ) = ( √ 2x 2 , √ 2y 2 ).\\r\\nx +y\\r\\n\\r\\nx +y\\r\\n\\r\\nWithout loss of generality, GM = 1.\\r\\np\\r\\nsymbols) which is quite expensive. To facilitate symbolic\\r\\nThe reverse Polish notation for x2 + y 2 is xQyQ+R (6 p\\r\\nlearning, one may wish to add in the radius variable r = x2 + y 2 to exploit the symmetry of the problem. To do\\r\\nso, we augment the original system with the extra variable r into an augmented system:\\r\\n\\uf8eb\\r\\n\\uf8f6\\r\\n\\uf8eb \\uf8f6\\r\\nvx\\r\\nx\\r\\n\\uf8ec−GM x/(x2 + y 2 )3/2 \\uf8f7\\r\\n\\uf8ec vx \\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8ec \\uf8f7 0 0\\r\\n0\\r\\n\\uf8f7\\r\\nvy\\r\\nz = \\uf8ec y \\uf8f7 , f (z ) = \\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8edv \\uf8f8\\r\\n2\\r\\n2 3/2 \\uf8f8\\r\\n\\uf8ed\\r\\n−GM y/(x + y )\\r\\ny\\r\\nr\\r\\n(xvx + yvy )/r\\r\\n3.\\r\\n\\r\\n(A8)\\r\\n\\r\\n2D gravitational Three-body Problem\\r\\n\\r\\nThe three-body problem has 12 degrees of freedom: 6 positions (xi , yi )(i = 1, 2, 3) and 6 velocities (vx,i , vy,i )(i =\\r\\n1, 2, 3),\\r\\n\\uf8f6\\r\\n\\r\\n\\uf8eb\\r\\n\\r\\nv1,x\\r\\n\\uf8ec\\r\\nv1,y\\r\\nx1\\r\\n\\uf8ec\\r\\nGm2 (x2 −x1 )\\r\\n\\uf8ec y1 \\uf8f7\\r\\n\\uf8ec−\\r\\n\\uf8ec\\r\\n\\uf8ec ((x2 −x1 )2 +(y2 −y1 )2 )3/2 −\\r\\n\\uf8f7\\r\\n\\uf8ecv1,x \\uf8f7\\r\\n\\uf8ec\\r\\n2 (y2 −y1 )\\r\\n\\uf8ec\\r\\n\\uf8ec− ((x −xGm\\r\\n\\uf8f7\\r\\n2\\r\\n2 3/2 −\\r\\n2\\r\\n1 ) +(y2 −y1 ) )\\r\\n\\uf8ecv1,y \\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\nv2,x\\r\\n\\uf8ec x2 \\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\nv2,y\\r\\n\\uf8ecy \\uf8f7\\r\\n\\uf8ec\\r\\nGm1 (x1 −x2 )\\r\\nz = \\uf8ec 2 \\uf8f7 , f = \\uf8ec−\\r\\n\\uf8ecv2,x \\uf8f7\\r\\n\\uf8ec ((x1 −x2 )2 +(y1 −y2 )2 )3/2 −\\r\\n\\uf8ecv \\uf8f7\\r\\n\\uf8ec\\r\\n1 (y1 −y2 )\\r\\n\\uf8ec 2,y \\uf8f7\\r\\n\\uf8ec− ((x −xGm\\r\\n2\\r\\n2 3/2 −\\r\\n1\\r\\n2 ) +(y1 −y2 ) )\\r\\n\\uf8ecx \\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec 3\\uf8f7\\r\\n\\uf8ec\\r\\nv3,x\\r\\n\\uf8ecy \\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec 3\\uf8f7\\r\\n\\uf8ec\\r\\nv3,y\\r\\n\\uf8ec\\r\\n\\uf8edv3,x \\uf8f8\\r\\nGm1 (x1 −x3 )\\r\\n\\uf8ec−\\r\\n\\uf8ed ((x1 −x3 )2 +(y1 −y3 )2 )3/2 −\\r\\nv3,y\\r\\n1 (y1 −y3 )\\r\\n− ((x1 −xGm\\r\\n2\\r\\n2 3/2 −\\r\\n3 ) +(y1 −y3 ) )\\r\\n\\uf8eb\\r\\n\\r\\n\\uf8f6\\r\\n\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\r\\nGm3 (x3 −x1 )\\r\\n\\uf8f7\\r\\n((x3 −x1 )2 +(y3 −y1 )2 )3/2 \\uf8f7\\r\\n\\uf8f7\\r\\nGm3 (y3 −y1 )\\r\\n\\uf8f7\\r\\n((x3 −x1 )2 +(y3 −y1 )2 )3/2 \\uf8f7\\r\\n\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\nGm3 (x3 −x2 )\\r\\n\\uf8f7,\\r\\n((x3 −x2 )2 +(y3 −y2 )2 )3/2 \\uf8f7\\r\\n\\uf8f7\\r\\nGm3 (y3 −y2 )\\r\\n\\uf8f7\\r\\n((x3 −x2 )2 +(y3 −y2 )2 )3/2 \\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\nGm2 (x2 −x3 )\\r\\n\\uf8f7\\r\\n((x2 −x3 )2 +(y2 −y3 )2 )3/2 \\uf8f8\\r\\n\\r\\n(A9)\\r\\n\\r\\nGm2 (y2 −y3 )\\r\\n((x2 −x3 )2 +(y2 −y3 )2 )3/2\\r\\n\\r\\nwhere G is the gravitational constant, and mi (i = 1, 2, 3) are three masses. The system has 4 conserved quanP3\\r\\nP3\\r\\ntities: (1) x-momentum: H1 =\\r\\n; (2) y-momentum: H2 =\\r\\ni=1 mi vi,x\\r\\ni=1 mi vi,y ; (3) angular momentum:\\r\\nP3 1\\r\\nP3\\r\\nGm1 m3\\r\\n2\\r\\n2\\r\\n1 m2\\r\\nH3 = i=1 mi (xi vi,y −yi vi,x ); (4) energy H = i=1 2 mi (vi,x\\r\\n+vi,y\\r\\n)+( ((x1 −x2 )Gm\\r\\n2 +(y −y )2 )1/2 + ((x −x )2 +(y −y )2 )1/2 +\\r\\n1\\r\\n2\\r\\n1\\r\\n3\\r\\n1\\r\\n3\\r\\nGm2 m3\\r\\n).\\r\\n((x2 −x3 )2 +(y2 −y3 )2 )1/2\\r\\n\\r\\nIn numerical experiments, we set G = m1 = m2 = m3 = 1. Similar to the Kepler problem, we\\r\\ncan simplify symbolic search by adding three distance variables:\\r\\np\\r\\nr12 = (x1 − x2 )2 + (y1 − y2 )2\\r\\np\\r\\n(A10)\\r\\nr13 = (x1 − x3 )2 + (y1 − y3 )2\\r\\np\\r\\n2\\r\\n2\\r\\nr23 = (x2 − x3 ) + (y2 − y3 )\\r\\nThe equations are augmented correspondingly:\\r\\n\\r\\n\\x0c\\n\\n9\\r\\n\\r\\n\\uf8f6\\r\\n\\uf8eb\\r\\n\\uf8f6\\r\\n..\\r\\n..\\r\\n.\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec . \\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec \\uf8f7\\r\\nz = \\uf8ecr12 \\uf8f7 , f = \\uf8ec((x1 − x2 )(v1,x − v2,x ) + (y1 − y2 )(v1,y − v2,y ))/r12 \\uf8f7\\r\\n\\uf8ed((x1 − x3 )(v1,x − v3,x ) + (y1 − y3 )(v1,y − v3,y ))/r13 \\uf8f8\\r\\n\\uf8edr13 \\uf8f8\\r\\n((x2 − x3 )(v2,x − v3,x ) + (y2 − y3 )(v2,y − v3,y ))/r23\\r\\nr23\\r\\n\\uf8eb\\r\\n\\r\\n4.\\r\\n\\r\\n(A11)\\r\\n\\r\\nKorteweg–De Vries equation (KdV) wave equation\\r\\n\\r\\nThe Korteweg–De Vries (KdV) equation is a mathematical model for shallow water surfaces. It is a nonlinear\\r\\npartial differential equation for a function φ with two real variables, x (space) and t (time):\\r\\n\\r\\nφt + φxxx − 6φφx = 0\\r\\n\\r\\n(A12)\\r\\n\\r\\nZero boundary conditions are imposed at the ends of the interval [a, b]. The KdV equation is known to have infinitely\\r\\nmany conserved quantities [18], which can be written explicitly as\\r\\nZ\\r\\n\\r\\nb\\r\\n\\r\\nP2n−1 (φ, φx , φxx , · · · )dx,\\r\\n\\r\\n(A13)\\r\\n\\r\\na\\r\\n\\r\\nwhich follows from locality and translational symmetry. The polynomials Pn are defined recursively by\\r\\nP1 = φ\\r\\nn−2\\r\\n\\r\\ndPn−1 X\\r\\nPn = −\\r\\n+\\r\\nPi Pn−1−i\\r\\ndx\\r\\ni=1\\r\\n\\r\\n(A14)\\r\\n\\r\\nThe first few conservation laws are\\r\\nZ\\r\\nZ\\r\\nZ\\r\\n\\r\\nφdx\\r\\n\\r\\n(mass)\\r\\n\\r\\nφ2 dx\\r\\n\\r\\n(momentum)\\r\\n\\r\\n(2φ3 − φ2x )dx\\r\\n\\r\\n(energy)\\r\\n\\r\\n(A15)\\r\\n\\r\\nDespite infinite many conservation laws, useful ones in physics are usually constrained to contain only φ and low-order\\r\\nderivatives (φx , φxx , · · · ). On the numerical front, our algorithm successfully discovers 2, 3, 4 conserved quantities\\r\\nwhich are dependent on φ, (φ, φx ) and (φ, φx , φxx ) respectively. On the symbolic front, we constrain the input variables\\r\\nto be (φ, φx ), and all three conservation laws (mass, momentum and energy) can be discovered.\\r\\nConverting to the canonical form ż = f (z) Since our framework can only deal with systems with finite degrees\\r\\nof freedom, we need to discretize space. We discretize the interval x ∈ [−10, 10] uniformly into Np = 40 points,\\r\\ndenoted x1 , · · · , xNp and only store derivatives up to fifth order on each grid point, using them to parametrize our\\r\\n(i)\\r\\nφ(x). This transforms our PDE into an ordinary differential equation with 3Np degrees of freedom (φ(i) = φ(xi ), φx =\\r\\nφx (xi ), · · · ): Eq. (A12) implies that\\r\\n\\uf8eb\\r\\n\\r\\n\\uf8f6 \\uf8eb\\r\\n\\uf8f6\\r\\n−φxxx + 6φφx\\r\\nφ\\r\\n\\uf8ec φx \\uf8f7 \\uf8ec −φxxxx + 6(φ2x + φφxx ) \\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8f7 \\uf8ec\\r\\n\\uf8f7\\r\\n∂t \\uf8ecφxx \\uf8f7 = \\uf8ec−φxxxxx + 6(3φx φxx + φφxxx )\\uf8f7\\r\\n\\uf8ed\\r\\n\\uf8f8 \\uf8ed\\r\\n\\uf8f8\\r\\n..\\r\\n..\\r\\n.\\r\\n.\\r\\n\\r\\n(A16)\\r\\n\\r\\n\\x0c\\n\\n10\\r\\nso our discretized PDE problem becomes\\r\\n\\uf8eb\\r\\n\\r\\nφ(1)\\r\\n(1)\\r\\nφx\\r\\n(1)\\r\\nφxx\\r\\n..\\r\\n.\\r\\n\\r\\n\\uf8eb\\r\\n\\r\\n\\uf8f6\\r\\n\\r\\n(1)\\r\\n\\r\\n(1)\\r\\n\\r\\n−φxxx + 6φ(1) φx\\r\\n(1)\\r\\n(1)2\\r\\n(1)\\r\\n−φxxxx + 6(φx + φ(1) φxx )\\r\\n(1)\\r\\n(1) (1)\\r\\n(1)\\r\\n−φxxxxx + 6(3φx φxx + φ(1) φxxx )\\r\\n..\\r\\n.\\r\\n\\r\\n\\uf8f6\\r\\n\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8f7 , f (z) ≡ ∂t z = \\uf8ec\\r\\nz≡\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8ec (Np ) \\uf8f7\\r\\n(Np )\\r\\n(N\\r\\n)\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\np\\r\\n(Np )\\r\\n\\uf8f7\\r\\n\\uf8ecφ\\r\\n−φ\\r\\n+\\r\\n6φ\\r\\nφ\\r\\nxxx\\r\\nx\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8ec (Np ) \\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n(Np )\\r\\n(N )2\\r\\n(N )\\r\\n\\uf8edφx \\uf8f8\\r\\n\\uf8ed −φxxxx\\r\\n+ 6(φx p + φ(Np ) φxx p ) \\uf8f8\\r\\n(N )\\r\\n(Np )\\r\\n(N ) (N )\\r\\n(N )\\r\\nφxx p\\r\\n−φxxxxx\\r\\n+ 6(3φx p φxx p + φ(Np ) φxxxp )\\r\\n\\r\\n(A17)\\r\\n\\r\\nSample generation We represent φ as a Gaussian mixture, so all derivatives can be computed analytically. In\\r\\nparticular,\\r\\n\\r\\nφ(x) =\\r\\n\\r\\nNg\\r\\nX\\r\\n\\r\\nAi ( √\\r\\n\\r\\ni=1\\r\\n\\r\\n1\\r\\nexp(−(x − µi )2 )/2σi2 ),\\r\\n2πσi\\r\\n\\r\\n−10 ≤ x ≤ 10\\r\\n\\r\\n(A18)\\r\\n\\r\\nwhere coefficients are set or drawn randomly accordingly to Ai ∼ U [−5, 5], µi ∼ U [−3, 3], σi = 1.5. These distributions\\r\\nare chosen such that (1) φ(x) is (almost) zero at two boundary points x = −10, 10; and (2) every single term in f (z)\\r\\nhave similar magnitudes. We choose Ng = 5 and generate P = 104 profiles of φ.\\r\\nConstraining\\r\\nR conservation laws The conservation laws of partial differential equations usually have the integral\\r\\nform, i.e., H = h(x0 )dx where x0 = (φ, φx , φxx , · · · ). When space is discretized, we constrain the conservation law\\r\\nPNp\\r\\nto the form H = i=1\\r\\nh(x0 ). On the numerical front, we parameterize h(x0 ) (as opposed to H) by a neural network;\\r\\nOn the symbolic front, we search the symbolic formula of h(x0 ) (as opposed to H). The summation operation is hard\\r\\ncoded for both fronts.\\r\\nAvoiding trivial conservation laws Due to zero boundary conditions, if h(x0 ) is an x-derivative of another\\r\\nRb\\r\\nfunction g(x0 ), then it is obvious that a h(x0 )dx = g(x0 )|b − g(x0 )|a = 0 which is a trivial conserved quantity. For\\r\\nexample, h(x0 ) = φx , φφx , φxx , φ2x + φφxx are all trivial. We observe that each of them has at least one term that is\\r\\nan odd function of a derivative. Consequently a simple solution is to use absolute values (|φx |, |φxx |, · · · ) instead of\\r\\n(φx , φxx , · · · ) so that these trivial conservation laws are avoided in the first place.\\r\\n\\r\\n5.\\r\\n\\r\\nNonlinear Schrödinger Equation\\r\\n\\r\\nThe (1D nonlinear Schrödinger equation (NLS) is a nonlinear generalization of the Schrödinger equation. Its\\r\\nprincipal applications are to the propagation of light in nonlinear optical fibres and planar waveguides and to BoseEinstein condensates. The classical field equation (in dimensionless form) is\\r\\n1\\r\\niψt = − ψxx + κ|ψ|2 ψ.\\r\\n2\\r\\n\\r\\n(A19)\\r\\n\\r\\nZero boundary conditions are imposed at infinity [19]. Like the KdV equation, the NLS has infinite many conserved\\r\\nquantities of the integral form\\r\\nZ\\r\\n\\r\\n∞\\r\\n\\r\\nh(ψ, ψx , ψxx , · · · )dx.\\r\\n\\r\\nH(x) =\\r\\n\\r\\n(A20)\\r\\n\\r\\n−∞\\r\\n\\r\\nUseful conservation laws in physics usually contain only low-order derivatives, e.g.,\\r\\nZ\\r\\nunitarity : |ψ|2 dx\\r\\nZ\\r\\n\\x01\\r\\n1\\r\\nenergy :\\r\\n|ψx |2 + κ|ψ|4 dx\\r\\n2\\r\\n\\r\\n(A21)\\r\\n\\r\\n\\x0c\\n\\n11\\r\\nConverting to the canonical form ż = f (z) Similar to the KdV equation, we treat (ψ, ψx , ψxx , · · · ) as different\\r\\nvariables. We denote ψr ≡ Re(ψ), ψi ≡ Im(ψ), Re(ψx ) = ψx,r , Im(ψx ) = ψx,i , etc.\\r\\n\\uf8f6 \\uf8eb\\r\\n\\uf8eb\\r\\n\\uf8f6\\r\\n1\\r\\n2\\r\\nψ\\r\\n2 iψxx − iκ|ψ| ψ\\r\\n1\\r\\n2\\r\\n\\uf8ec ψx \\uf8f7 \\uf8ec\\r\\n\\uf8f7\\r\\n2 iψxxx − iκ(|ψ| ψx + (ψr ψx,r + ψi ψx,i )ψ)\\r\\n\\uf8f7 \\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n2\\r\\n2\\r\\n(A22)\\r\\n∂t \\uf8ecψxx \\uf8f7 = \\uf8ec 1 iψxxxx − iκ(|ψ|2 ψxx + 2(ψr ψx,r + ψi ψx,i )ψx + (ψx,r\\r\\n+ ψr ψxx,r + ψx,i + ψi ψxx,i )ψ)\\uf8f7\\r\\n\\uf8f8 \\uf8ed2\\r\\n\\uf8ed\\r\\n\\uf8f8\\r\\n..\\r\\n..\\r\\n.\\r\\n.\\r\\nSince ψ is a complex number, we should treat real and imaginary parts separately.\\r\\n\\uf8f6\\r\\n\\uf8f6 \\uf8eb\\r\\n\\uf8eb\\r\\n− 21 ψxx,i + κ|ψ|2 ψi\\r\\nψr\\r\\n1\\r\\n2\\r\\n\\uf8f7\\r\\n\\uf8ec ψi \\uf8f7 \\uf8ec\\r\\n2 ψxx,r − κ|ψ| ψr\\r\\n\\uf8f7\\r\\n\\uf8f7 \\uf8ec\\r\\n\\uf8ec\\r\\n1\\r\\n2\\r\\n\\uf8f7\\r\\nψ\\r\\n+\\r\\nκ(|ψ|\\r\\nψ\\r\\n+\\r\\n(ψ\\r\\nψ\\r\\n+\\r\\nψ\\r\\nψ\\r\\n)ψ\\r\\n)\\r\\n−\\r\\n\\uf8ec ψx,r \\uf8f7 \\uf8ec\\r\\nxxx,i\\r\\nx\\r\\nr\\r\\nx,r\\r\\ni\\r\\nx,i\\r\\ni\\r\\ni\\r\\n2\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8ecψ \\uf8f7 \\uf8ec\\r\\n1\\r\\n2\\r\\n\\uf8f7\\r\\nψ\\r\\n−\\r\\nκ(|ψ|\\r\\nψ\\r\\n+\\r\\n(ψ\\r\\nψ\\r\\n+\\r\\nψ\\r\\nψ\\r\\n)ψ\\r\\n)\\r\\n\\uf8f7\\r\\n\\uf8ec\\r\\nxxx,r\\r\\nx,r\\r\\nr\\r\\nx,r\\r\\ni\\r\\nx,i\\r\\nr\\r\\nx,i\\r\\n2\\r\\n=\\r\\n∂t \\uf8ec\\r\\n\\uf8ec\\r\\n\\uf8f7 (A23)\\r\\n\\uf8f7\\r\\n1\\r\\n2\\r\\n2\\r\\n2\\r\\n\\uf8f7\\r\\n+\\r\\nψ\\r\\nψ\\r\\n)ψ\\r\\n)\\r\\n−\\r\\nψ\\r\\n+\\r\\nκ(|ψ|\\r\\nψ\\r\\n+\\r\\n2(ψ\\r\\nψ\\r\\n+\\r\\nψ\\r\\nψ\\r\\n)ψ\\r\\n+\\r\\n(ψ\\r\\n+\\r\\nψ\\r\\nψ\\r\\n+\\r\\nψ\\r\\n\\uf8ecψxx,r \\uf8f7 \\uf8ec\\r\\ni\\r\\nxx,i\\r\\ni\\r\\nxxxx,i\\r\\nxx,i\\r\\nr\\r\\nx,r\\r\\ni\\r\\nx,i\\r\\nx,i\\r\\nr,i\\r\\nxx,r\\r\\nx,r\\r\\nx,i\\r\\n2\\r\\n\\uf8ec\\r\\n\\uf8f7\\r\\n\\uf8f7\\r\\n\\uf8ecψ\\r\\n1\\r\\n2\\r\\n2\\r\\n2\\r\\nψ\\r\\n−\\r\\nκ(|ψ|\\r\\nψ\\r\\n+\\r\\n2(ψ\\r\\nψ\\r\\n+\\r\\nψ\\r\\nψ\\r\\n)ψ\\r\\n+\\r\\n(ψ\\r\\n+\\r\\nψ\\r\\nψ\\r\\n+\\r\\nψ\\r\\n+\\r\\nψ\\r\\nψ\\r\\n)ψ\\r\\n)\\r\\n\\uf8ed xx,i \\uf8f8 \\uf8ec\\r\\nxx,r\\r\\nr x,r\\r\\ni x,i\\r\\nx,r\\r\\nr xx,r\\r\\ni xx,i\\r\\nr \\uf8f7\\r\\nx,r\\r\\nx,i\\r\\n2 xxxx,r\\r\\n\\uf8ed\\r\\n\\uf8f8\\r\\n..\\r\\n..\\r\\n.\\r\\n.\\r\\nJust as in the KdV example, to avoid trivial solutions, we consider only the equations for magnitude\\r\\n(|ψ|, |ψx |, |ψxx |, · · · ).\\r\\n\\uf8eb\\r\\n\\uf8f6 \\uf8eb\\r\\n\\uf8f6\\r\\n|ψ|\\r\\n(ψr ∂t ψr + ψi ∂t ψi )/|ψ|\\r\\n\\uf8ec |ψx | \\uf8f7 \\uf8ec (ψx,r ∂t ψx,r + ψx,i ∂t ψx,i )/|ψx | \\uf8f7\\r\\n\\uf8ec\\r\\n\\uf8f7 \\uf8ec\\r\\n\\uf8f7\\r\\n(A24)\\r\\n∂t \\uf8ec|ψxx |\\uf8f7 = \\uf8ec(ψxx,r ∂t ψxx,r + ψxx,i ∂t ψxx,i )/|ψxx |\\uf8f7\\r\\n\\uf8ed\\r\\n\\uf8f8 \\uf8ed\\r\\n\\uf8f8\\r\\n..\\r\\n..\\r\\n.\\r\\n.\\r\\n| {z } |\\r\\n{z\\r\\n}\\r\\nz\\r\\n\\r\\nf\\r\\n\\r\\nSample generation is similar to the KdV equations, with the only difference that real and imaginary parts are\\r\\nboth treated as (independent) Gaussian mixtures.\\r\\nAppendix B: How to determine (in)dependence of multiple conserved quantities\\r\\n\\r\\nSuppose we already have n independent conserved quantities H = {H1 (z), · · · , Hn (z), z ∈ Rs }, which are parameterized as neural networks or symbolic formulas. How do we determine a new conserved quantity Hn+1 (z) is dependent\\r\\non or independent of H?\\r\\nMethod S\\r\\nA: differential rank. We know kD (H) = n due to the function independence of H. We then compute\\r\\nk 0 ≡ kD (H Hn+1 ). If k 0 = n + 1, then Hn+1 is independent of Hn ; otherwise k 0 = n, Hn+1 is depedent on\\r\\nH. In practice, we compute the singular value decomposition of B (defined in Eq. (6)). If the least singular value\\r\\nσn+1 < \\x0fσ = 10−3 , we consider it vanishing hence k 0 = n; otherwise k 0 = n + 1. However the complexity of SVD is\\r\\nO(sn2 ), which is more expensive to compute than the method B.\\r\\nMethod B: orthogonality test. Because H is an independent set of functions, their gradients at all z (except\\r\\nzero measure) should span a linear subspace S(z) ≡ span(∇H1 (z), · · · , ∇Hn (z)) with dimensionality n. We construct\\r\\na random unit vector b\\r\\nt(z) that is orthogonal to S, which can be implemented as a Gram-Schmidt process of a\\r\\nrandom vector and n gradient vectors. If Hn+1 (z) is not independent of H, then the gradient ∇Hn+1 (z) ∈ S(z), so\\r\\n−3\\r\\n\\\\\\r\\n\\\\\\r\\nb\\r\\nb\\r\\nt · ∇H\\r\\nand reject it. If Hn+1 (z)\\r\\nn+1 (z) = 0. We consider Hn+1 to be not independent if t(z) · ∇Hn+1 (z) < \\x0fi = 10\\r\\n\\\\\\r\\nis independent of H, then b\\r\\nt(z) · ∇H\\r\\nn+1 (z) > \\x0fi is true with high probability. To further reduce probability of errors,\\r\\none may test on nt points, which incurs an O(nt s) complexity.\\r\\nOnce Hn+1 is verified as being independent of H, we append Hn+1 to H. This process is repeated until |H| (the\\r\\nnumber of functions) is equal the number of conserved quantities (obtained from the neural network front) or the\\r\\nbrute force search reaches the maximum computation budget.\\r\\nAppendix C: Various definitions of integrability and relations to AI Poincaré 1.0/2.0\\r\\n\\r\\nConservation laws are closed related to the notion of integrability [20], which in turn has various definitions from\\r\\ndifferent perspectives [21, 22]. In the following, we list five definitions of integrability and corresponding definitions\\r\\n\\r\\n\\x0c\\n\\n12\\r\\n\\r\\nAI Poincaré 1.0\\r\\nAI Poincaré 2.0\\r\\na\\r\\n\\r\\nGeneral Frobenius Liouville Landau solvable\\r\\nYes\\r\\nNo\\r\\nNo\\r\\nNo\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes a\\r\\nYes\\r\\nYes\\r\\n\\r\\nThis case is not included in paper, but is doable when we combine the techniques of searching for hidden symmetries in [23].\\r\\n\\r\\nTABLE II: Five integrability definitions and whether AI Poincaré 1.0/2.0 can deal with them.\\r\\n\\r\\nof conserved quantities.\\r\\n(1) General integrability [global geometry/topology]. In the context of differential dynamical systems, the\\r\\nnotion of integrability refers to the existence of an invariant regular foliation of phase space [21]. Consequently, a\\r\\nconserved quantity should be a well-behaved function globally, not demonstrating any fractal or other pathological\\r\\nbehavior.\\r\\n(2) Frobenius integrability [local geometry/topology]. A dynamical system is said to be Frobenius integrable\\r\\nif, locally, the phase space has a foliation of invariant manifolds [21]. One major corollary of the Frobenius theorem\\r\\nis that a first-order dynamical system with s degrees of freedom has s − 1 (local) integrals of motion. Consequently,\\r\\na conserved quantity in the sense of Frobenius integrability does not require the foliation to be regular in the global\\r\\nsense. The visual differences between local and global conserved quantities are shown in FIG. (3) and FIG. (4).\\r\\n(3) Liouville integrability [algebra]. In the special setting of Hamiltonian systems, we have Liouville integrability, which focuses on algebraic properties of a Hamiltonian system [15]. Liouville integrability states that there exists\\r\\na maximal set of Poisson commuting invariants. Consequently, conserved quantities corresponds to Poisson commuting quantities. A system in the 2n-dimensional phase space is Liouville integrable if it has n independent conserved\\r\\nquantities which commutate with each other, i.e., {Hi , Hj } = 0. According to the Liouville-Arnold theorem [15], such\\r\\nsystems can be solved exactly by quadrature, which is a special case of solvability integrability (the fifth criterion\\r\\nbelow).\\r\\n(4) Landau integrability [concept simplicity] Landau stated in his textbook [13] that physicists prefer symmetric and additive IOMs and promote them as fundamental “conservation laws”.\\r\\n(5) Solvable integrability [symbolic simplicity]. Solvable integrability requires the determination of solutions\\r\\nin an explicit functional form [22]. This property is intrinsic, but can be very useful to simplify and theoretically\\r\\nunderstand problems.\\r\\n(6) Experimental integrability [robustness]. In physics, we consider a conserved quantity useful if a measurement of it at some time t can constrain the state at some later time. In experimental physics, a measurement\\r\\nof a physical quantity always contains some finite error. Hence a useful conserved quantity must not be infinitely\\r\\nsensitive to measurement error. In contrast, FIG. 4 (top row) shows that, although a conserved quantity exists for\\r\\nall possible frequency pairs (ωx , ωy ), their robustness to noise differ widely. Once the noise scale significantly exceeds\\r\\nthe width of stripe pattern, an accurate measurement of the conserved quantity is impossible, and a measurement\\r\\nof the “conserved quantity” provides essentially zero useful information for predicting the future state. When the\\r\\nfrequency ratio is an irrational number, the “conserved quantity” becomes discontinuous and pathological throughout\\r\\nphase space and completely useless for making physics predictions. This experimental integrability criterion is thus\\r\\ncompatible with general integrability, but does not follow from Frobenius integrability.\\r\\nIn summary, the various notions of integrability are used to study dynamical systems, but have different motivations\\r\\nand scopes. General and the Frobenius integrability characterize global and local geometry; Liouville integrability\\r\\ntakes an algebraic perspective and applies only to Hamiltonian systems; Landau and solvable integrability instead\\r\\nfocus on simplicity based on concepts and symbolic equations, respectively. To the best of our knowledge, there is no\\r\\nagreement on whether one particular definition outperform others in all senses. We believe they are complementary\\r\\nto each other, rather than being contradictory or redundant. In AI Poincaré 1.0 and 2.0 (the current paper), we\\r\\nmostly did not mentioned explicitly which sense of integrability/conserved quantities we referred to. Fortunately, AI\\r\\nPoincaré 2.0 is quite flexible to adapt to all definitions, as summarized in Table II.\\r\\nAI Poincaré 1.0 defines a trajectory manifold, which is orthogonal to the invariant manifold. The trajectory manifold\\r\\nis globally defined, and its dimensionality is a topological invariant. As a consequence, in AI Poincaré 1.0, conserved\\r\\nquantities satisfy general integrability. The symbolic part of AI Poincaré 1.0 looks for formulas with simple symbolic\\r\\nforms, in the spirit of solvable integrability.\\r\\nAI Poincaré 2.0 addresses the problem of finding a maximal set of independent conserved quantities, in analogy to\\r\\nthe goal of the Frobenius theorem [24] which searches for a maximal set of solutions of a regular system of first-order\\r\\nlinear homogeneous partial differential equations. The loss formulation in Eq. (3) can be viewed as a variational\\r\\nformulation of the system of PDEs to be satisfied for conserved quantities. Consequently, AI Poincaré 2.0 (neural\\r\\nnetwork front) is aligned with the Frobenius integrability if there is only one training sample z. In the presence of many\\r\\ntraining samples over the phase space, our algorithm becomes aligned with the notion of the general integrability,\\r\\n\\r\\n\\x0c\\n\\n13\\r\\n=0\\r\\n\\r\\n3\\r\\n\\r\\n= 0.01\\r\\n\\r\\n3\\r\\n\\r\\n= 0.1\\r\\n\\r\\n3\\r\\n\\r\\n=1\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n= 10\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\nFIG. 3: The conserved quantity of the 1D damped harmonic oscillator with different γ. Neural networks cannot\\r\\nperfectly learn the singular behavior near at the origin, and also struggle when the stripes get too narrow.\\r\\n\\r\\nbecause the conserved quantity is parameterized as a neural network which has an implicit bias towards smooth\\r\\nand regular functions globally. Although we did not explicitly deal with Liouville integrability in this paper, the\\r\\nalgebraic nature of Liouville integrability makes it simply a “hidden symmetry problem” that is defined and solved\\r\\nby [23], and the techniques in the current paper can further polish the story by determining functional dependence\\r\\namong invariants learned by neural networks. The symmetry and additivity in Landau integrability is known in the\\r\\nmachine learning literature as physical inductive biases, which can be elegantly handled by adding constraints to the\\r\\narchitectures or loss functions [6, 7]. Finally, the symbolic front of AI Poincaré 2.0 addresses the problem of finding\\r\\nconserved quantities with simple symbolic formulas.\\r\\nTo gain more intuition about the difference between local and global conserved quantities in the sense of Frobenius\\r\\nand general integrability respectively, we visualize conserved quantities for two simple yet illustrative examples. The\\r\\nmessage we want to deliver here is that local conserved quantities may be ill-behaved globally, thus just being more\\r\\nof a mathematical curiosity than something physically useful.\\r\\n1D damped harmonic oscillator is described by the equation\\r\\n\\x12 \\x13 \\x12\\r\\n\\x13\\r\\nd x\\r\\np\\r\\n=\\r\\n(C1)\\r\\n−x − γp\\r\\ndt p\\r\\nwhere γ is the damping coefficient. In the sense of Frobenius integrability, the system has 1 conserved quantity. We\\r\\nfirst attempt to construct the quantity analytically. The family of solutions is\\r\\n\\x12\\r\\n\\x13 \\x12 −γt\\r\\n\\x13\\r\\nx(t)\\r\\ne cos(t + ϕ)\\r\\n=\\r\\n, ϕ ∈ [0, 2π)\\r\\n(C2)\\r\\np(t)\\r\\ne−γt sin(t + ϕ)\\r\\nDefine the complex variable z(t) ≡ x(t) + ip(t) = e(−γ+i)t+iϕ and its complex conjugate z̄ = e(−γ−i)t−iϕ . Then\\r\\n\\r\\nH ≡ z (−γ−i) /z̄ (−γ+i) =\\r\\n\\r\\n\\x10 z \\x11−γ\\r\\nz̄\\r\\n\\r\\n(z z̄)−i = e−2iγϕ\\r\\n\\r\\n(C3)\\r\\n\\r\\nis a conserved quantity. When γ = 0, H ∼ (z z̄) = |z|2 = x2 + p2 which is the energy; when γ → ∞, H ∼ (z/z̄) ∼\\r\\ni\\r\\narg(z) = arctan(p/x) which is the polar angle. For visualization purposes, we define H 0 ≡ 2γ\\r\\nlnH = θ + lnr\\r\\nγ , where\\r\\np\\r\\np\\r\\n0\\r\\n2\\r\\n2\\r\\nθ = arctan x and r = x + p . We visualize cosH in FIG. 3 for different γ. The function looks regular for γ = 0\\r\\nand large γ = 10, while looks ill-behaved for γ → 0+ , say γ = 0.01 and 0.1.\\r\\n2D anisotropic harmonic oscillator is described by the equation\\r\\n\\uf8f6 \\uf8eb\\r\\n\\uf8f6\\r\\npx\\r\\nx\\r\\nd \\uf8ecpx \\uf8f7 \\uf8ec−ωx2 x\\uf8f7\\r\\n\\uf8ed \\uf8f8=\\uf8ed p \\uf8f8\\r\\ny\\r\\ndt y\\r\\n−ωy2 y\\r\\npy\\r\\n\\uf8eb\\r\\n\\r\\n(C4)\\r\\n\\r\\nwhere ωx and ωy are angular frequencies along the x and y direction respectively. Two apparent conserved quantities\\r\\nare energies along the x and y direction, i.e., H1 = 21 (p2x + ωx2 x2 ) and H2 = 21 (p2y + ωy2 y 2 ). There exists a third\\r\\nconserved quantity in the sense of Frobenius integrability, as we construct below (also in [17]). The family of solutions\\r\\n\\r\\n\\x0c\\n\\n14\\r\\n\\r\\n2\\r\\n\\r\\n( x, y) = (1, 1)\\r\\n\\r\\n( x, y) = (1, 2)\\r\\n\\r\\n( x, y) = (2, 3)\\r\\n\\r\\n( x, y) = (17, 23)\\r\\n\\r\\n( x, y) = (67, 97)\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n( x, y) = (1, 1)\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n( x, y) = (1, 2)\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n4\\r\\n\\r\\n1\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n( x, y) = (2, 3)\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n( x, y) = (17, 23)\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n4\\r\\n\\r\\n1\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n( x, y) = (67, 97)\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\nFIG. 4: The conserved quantity of the 2D harmonic oscillator with different frequency pairs (ωx , ωy ). Top: ground\\r\\nTruth; bottom: learned results by neural networks. A neural network can only learn this conserved quantity if the\\r\\nfrequency ratio q ≡ ωx /ωy is a ratio of small integers; if q is irrational, the conserved quantity is an everywhere\\r\\ndiscontinuous function that is completely useless to physicists.\\r\\nis\\r\\n\\uf8f6 \\uf8eb\\r\\n\\uf8f6\\r\\nx\\r\\nAx cos(ωx t + ϕx )\\r\\n\\uf8ecpx \\uf8f7 \\uf8ec−ωx Ax sin(ωx t + ϕx )\\uf8f7\\r\\n\\uf8ed y \\uf8f8 = \\uf8ed A cos(ω t + ϕ ) \\uf8f8\\r\\ny\\r\\ny\\r\\ny\\r\\npy\\r\\n−ωy Ay sin(ωy t + ϕy )\\r\\n\\uf8eb\\r\\n\\r\\nWe define z1 ≡\\r\\n\\r\\n1\\r\\nAx (x\\r\\n\\r\\n+ i ωpxx ) = ei(ωx t+ϕx ) , and z2 ≡\\r\\n\\r\\n1\\r\\nAy (y\\r\\n\\r\\n(C5)\\r\\n\\r\\np\\r\\n\\r\\n+ i ωyy ) = ei(ωy t+ϕy ) . Hence\\r\\n\\r\\nω\\r\\n\\r\\nH3 ≡ z1 y /z2ωx = ei(ωy ϕx −ωx ϕy )\\r\\n\\r\\n(C6)\\r\\n\\r\\nis a conserved quantity. In the isotropic case when ωx = ωy = ω, H3 simplifies to\\r\\nH3 = (ω 2 xy + px py + iω(xpy − ypx ))/H2\\r\\n\\r\\n(C7)\\r\\n\\r\\nwhose imaginary part is the well-known angular momentum. Since the norm of H3 is 1, the real and imaginary part\\r\\nare not independent. We plot −ilnH3 in FIG. 4 with different (ωx , ωy ). We set Ax = Ay = 1. In the cases when ωy /ωx\\r\\nis an integer or simple fractional number, H3 is regular; however when ωy /ωx is a complicated fractional number, H3\\r\\nis ill-behaved, demonstrating fractal behavior.\\r\\nNeural networks cannot learn ill-behaved conserved quantities well Neural networks have an implicit bias\\r\\ntowards smooth functions, so they are unable to learn ill-behaved conserved quantities. To verify the argument, we\\r\\nrun AI Poincaré 2.0 (only n = 1 model, hence no regularization) on the 1D damped harmonic oscillator with different\\r\\ndamping coefficient γ, and plot `1 as a function of γ in FIG. 5. We found that: (1) the conservation loss `1 is almost\\r\\nvanishing at small γ = 0.01 and large γ = 100; (2) `1 peaks around γ = 1, which agrees with the visualization in\\r\\nFIG. 3. We visualize functions learned by neural p\\r\\nnetworks in FIG. 6 top row, each column displaying results of a\\r\\nspecific γ. We also compare the function with r = x2 + p2 and x in the bottom row. When γ = 0.01, the conserved\\r\\nquantity is equivalent to r up to a nonlinear re-parameterization; When γ = 100, the conserved quantity is equivalent\\r\\nto x up to a nonlinear re-parameterization. We also run AI Poincaré 2.0 (n = 4 models are trained) on the 2D\\r\\nharmonic oscillator example with different frequency ratios ωy /ωx . In FIG. ??, we visualize the worst conserved\\r\\nquantity, i.e., the one with the highest conservation loss, out of 4 neural networks. To map the four-dimensional\\r\\nfunction to a 2D plot, we constrain x = cosϕ1 , px = sinϕ1 , y = cosϕ2 , py = sinϕ2 . When (ωx , ωy ) = (1, 1) or (1, 2),\\r\\nthe neural network prediction of the third conserved quantity aligns well with our expectation (visualized in FIG. 4).\\r\\nFor more complicated ωy /ωx ratios, the prediction looks similar to the (ωx , ωy ) = (1, 1) case, but they have high\\r\\nconservation loss, as shown in TABLE III.\\r\\n\\r\\n\\x0c\\n\\n15\\r\\n(ωx , ωy )\\r\\n(1, 1)\\r\\n(1, 2)\\r\\n(2, 3)\\r\\n(17, 23)\\r\\n(67, 97)\\r\\nWorst conservation loss 1.1 × 10−4 5.1 × 10−4 7.9 × 10−4 1.2 × 10−3 1.4 × 10−3\\r\\nAverage conservation loss 7.7 × 10−5 4.6 × 10−4 4.7 × 10−4 1.0 × 10−3 1.1 × 10−3\\r\\n\\r\\n1\\r\\n\\r\\nTABLE III: 2D harmonic oscillator: worst and average conservation loss for different ratios ωy /ωx .\\r\\n\\r\\n10\\r\\n\\r\\n3\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n5\\r\\n\\r\\n10\\r\\n\\r\\n6\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n1\\r\\n\\r\\n100\\r\\n\\r\\n101\\r\\n\\r\\n102\\r\\n\\r\\nFIG. 5: 1D damped harmonic oscillator: conservation loss `1 as a function of γ.\\r\\n\\r\\nAppendix D: phase transitions and how to choose λ\\r\\n\\r\\nIn Eq. (3), a hyperparameter is the regularization coefficient λ. If λ is too small, then multiple networks may learn\\r\\ndependent conserved quantities. If λ is too large, then the regularization loss dominates the conservation loss, making\\r\\nthe conservation laws inaccurate. As we argue below, the proper choice of λ has a lower bound which is determined\\r\\nby the tolerance of approximation error \\x0f, and an upper bound O(1).\\r\\nWe first use two analytical toy examples to provide some insights. In both cases, the number of neural networks\\r\\nn is equal to the dimension of the problem s, just to demonstrate all possible phase transitions. In practice, it is\\r\\nsufficient to choose n = s − 1. The geometric intuition of minimizing the loss function Eq. (3) is that: `1 encourages\\r\\n\\r\\nFIG. 6: 1D damped harmonic oscillator: Each column corresponds to a damping coefficient γ. Top: visualizations of\\r\\nneural network predictions of the conserved quantity. Bottom:\\r\\nComparison of neural network predictions with x and\\r\\np\\r\\n2\\r\\nr = x + p2 .\\r\\n\\r\\n\\x0c\\n\\n16\\r\\n\\r\\nFIG. 7: 2D Toy example: With different λ, the global minima may have different geometric configurations. Assume\\r\\nthe single conserved quantity can be approximated by a neural network with error \\x0f.\\r\\n\\r\\n∇Hi to be orthogonal to f , while the regularization loss `2 encourages ∇Hi and ∇Hj (j 6= i) to be orthogonal.\\r\\nToy example 1: The first toy example is inspired by the 1D damped harmonic oscillator. We consider a 2D phase\\r\\nspace. There is only one conserved quantity in the sense of Frobenius integrability, and the approximation error of\\r\\na neural network is \\x0f. We train 2 networks to learn the conserved quantities. At the global minima, two possible\\r\\ngeometric configurations (gradients of neural conserved quantities) are shown in FIG. 7. It is easy to check that any\\r\\nother configuration has higher loss than at least one of the two configurations. Which configuration has lower loss\\r\\ndepends on λ: when λ < 1−\\x0f\\r\\n2 , two networks represent the same function (i.e., the only conserved quantity); when\\r\\n,\\r\\ntwo\\r\\nnetworks\\r\\nrepresent\\r\\ntwo independent functions, one of which is not a conserved quantity even in the\\r\\nλ > 1−\\x0f\\r\\n2\\r\\nsense of Frobenius integrability. Since only the first phase is desirable, we need to set λ < 1−\\x0f\\r\\n2 . The condition can be\\r\\neasily satisfied if we assume \\x0f \\x1c 1.\\r\\nToy example 2: The second toy example is inspired by the 2D anisotropic harmonic oscillator. To better visualize\\r\\nthe example, we consider a 3D phase space (rather than 4D), but the intrinsic nature of the problem does not change.\\r\\nThere are two conserved quantities in the sense of Frobenius integrability. One conserved quanitity is easy for neural\\r\\nnetworks to fit, hence the approximation error can be minimized to zero; another conserved quantity is hard, so a\\r\\nneural network can at best approximate the function up to error \\x0f. Similar to the analysis above, there are three\\r\\npossible configurations corresponding to the global minima. We train three neural networks to learn the conserved\\r\\nquantities. When λ < 2\\x0f , three models represent only one conserved quantity (the easy one); when 2\\x0f < λ < 1,\\r\\nthree models represent two independent conserved quantities (both the easy and the hard one); when λ > 1, a third\\r\\nfalse conserved quantity is learned. Both the first phase and the second phase are acceptable, depending on different\\r\\nnotions of integrability, since a hard conserved quantity may be locally well-behaved but globally ill-behaved. If we\\r\\nsearch for global conserved quantities, the first phase is desired. However, if we allow local conserved quantities, the\\r\\nsecond phase is desired. All the experiments in the main text are conducted with λ = 0.02, which is equivalent to\\r\\nsaying we only care about conserved quantities whose approximation errors are less than 0.02 × c, c = 2 in the current\\r\\ntoy example, but we expect c ∼ O(1) in general.\\r\\nBased on the analysis of two toy examples above, we can have a basic picture of phase transitions for more\\r\\ncomplicated systems: for n conserved quantities with different difficulty (approximation error \\x0f1 < \\x0f2 < · · · < \\x0fn ), we\\r\\nexpect there are n + 1 phases. At each phase transition, only one conserved quantity is learned or un-learned, and the\\r\\norder of phase transitions depends on the order of \\x0f. From the picture of phase transitions, one not only knows the\\r\\nnumber of conserved quantities, but also knows their difficulty hierarchy. In practice, the phase transition diagram\\r\\nmay not be as clean as these toy examples due to inefficiency of neural network training. We would like to investigate\\r\\nthis more in future works. In the following, we show that the phase transition diagram agrees creditably well with\\r\\nour theory above for the 1D damped harmonic oscillator and 2D harmonic oscillator.\\r\\n1D damped harmonic oscillator Toy example 1 can apply to the 1D damped harmonic oscillator without any\\r\\nmodification. FIG. 9 shows that we find a phase transition of `1 /`2 at λ ≈ 12 for both γ = 0 and γ = 1. When γ = 1,\\r\\nthe non-zero `1 in the first phase implies the irregularity of the conserved quantity.\\r\\n\\r\\n\\x0c\\n\\n17\\r\\n\\r\\nFIG. 8: 3D Toy example: With different λ, the global minima may have different geometric configurations. Assume\\r\\nthe first and second conserved quantity can be approximated by a neural network with zero error (easy) and \\x0f > 0\\r\\nerror (hard), respectively.\\r\\n\\r\\n=0\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n6\\r\\n\\r\\n10\\r\\n\\r\\n8\\r\\n\\r\\n1\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n101\\r\\n\\r\\n100\\r\\n\\r\\n1\\r\\n\\r\\n=1\\r\\n\\r\\n100\\r\\n\\r\\nLoss\\r\\n\\r\\nLoss\\r\\n\\r\\n100\\r\\n\\r\\n102\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n6\\r\\n\\r\\n10\\r\\n\\r\\n8\\r\\n\\r\\n10\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n101\\r\\n\\r\\n100\\r\\n\\r\\n1\\r\\n\\r\\n102\\r\\n\\r\\nFIG. 9: 1D damped harmonic oscillator: `1 /`2 as functions of λ demonstrate phase transition behavior.\\r\\n\\r\\n2D harmonic oscillator Toy example 2 is a good abstract of the 2D harmonic oscillator, but should not be\\r\\nconsidered to be exact in the quantitative sense. The two energies are easy conserved quantities, while the third\\r\\nconserved quantity regarding phases are harder to learn due to its irregularity when ωy /ωx is not a fractional number.\\r\\nFIG.\\r\\n√ 10 shows that: when (ωx , ωy ) = (1, 1), only one clear phase transition happens around λ = 1. When (ωx , ωy ) =\\r\\n(1, 2), two phase transitions are present, one around λ = 1, another around 10−3 < λ < 10−2 .\\r\\n\\r\\n( x, y) = (1, 1)\\r\\n\\r\\n0.40\\r\\n\\r\\n1\\r\\n2\\r\\n\\r\\n0.35\\r\\n0.30\\r\\n\\r\\n0.35\\r\\n0.30\\r\\n0.25\\r\\n\\r\\nLoss\\r\\n\\r\\nLoss\\r\\n\\r\\n0.25\\r\\n0.20\\r\\n0.15\\r\\n\\r\\n0.20\\r\\n0.15\\r\\n\\r\\n0.10\\r\\n\\r\\n0.10\\r\\n\\r\\n0.05\\r\\n\\r\\n0.05\\r\\n\\r\\n0.00\\r\\n\\r\\n( x, y) = (1, 2)\\r\\n\\r\\n0.40\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n3\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n1\\r\\n\\r\\n100\\r\\n\\r\\n101\\r\\n\\r\\n102\\r\\n\\r\\n103\\r\\n\\r\\n0.00\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n3\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n1\\r\\n\\r\\n100\\r\\n\\r\\n101\\r\\n\\r\\n102\\r\\n\\r\\n103\\r\\n\\r\\nFIG. 10: 2D isotropic/anisotropic harmonic oscillator: `1 /`2 as functions of λ demonstrate phase transition behavior.\\r\\n\\r\\n\\x0c']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the text into one string\n",
    "\n",
    "array_pdf_text=[]\n",
    "def pdf_to_text():\n",
    "    for i in array_link:\n",
    "        array_pdf_text.append(\"\\n\\n\".join(load_pdf(i[21:])))\n",
    "    return array_pdf_text\n",
    "pdf_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc82c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chantal.Pellegrini', 'Anees.Kazi', 'Nassir.Navab']\n",
      "Unsupervised Pre-Training on Patient Population\r\n",
      "Graphs for Patient-Level Predictions\r\n",
      "Chantal Pellegrini1 , Anees Kazi1 , and Nassir Navab1,2\r\n",
      "Computer Aided Medical Procedures, Technical University Munich, Germany\r\n",
      "Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, USA\r\n",
      "\r\n",
      "1\r\n",
      "\r\n",
      "arXiv:2203.12616v1 [cs.LG] 23 Mar 2022\r\n",
      "\r\n",
      "2\r\n",
      "\r\n",
      "Abstract. Pre-training has shown success in different areas of machine\r\n",
      "learning, such as Computer Vision (CV), Natural Language Processing\r\n",
      "(NLP) and medical imaging. However, it has not been fully explored\r\n",
      "for clinical data analysis. Even though an immense amount of Electronic Health Record (EHR) data is recorded, data and labels can be\r\n",
      "scarce if the data is collected in small hospitals or deals with rare diseases. In such scenarios, pre-training on a larger set of EHR data could\r\n",
      "improve the model performance. In this paper, we apply unsupervised\r\n",
      "pre-training to heterogeneous, multi-modal EHR data for patient outcome prediction. To model this data, we leverage graph deep learning\r\n",
      "over population graphs. We first design a network architecture based on\r\n",
      "graph transformer designed to handle various input feature types occurring in EHR data, like continuous, discrete, and time-series features,\r\n",
      "allowing better multi-modal data fusion. Further, we design pre-training\r\n",
      "methods based on masked imputation to pre-train our network before\r\n",
      "fine-tuning on different end tasks. Pre-training is done in a fully unsupervised fashion, which lays the groundwork for pre-training on large\r\n",
      "public datasets with different tasks and similar modalities in the future.\r\n",
      "We test our method on two medical datasets of patient records, TADPOLE and MIMIC-III, including imaging and non-imaging features and\r\n",
      "different prediction tasks. We find that our proposed graph based pretraining method helps in modeling the data at a population level and\r\n",
      "further improves performance on the fine tuning tasks in terms of AUC\r\n",
      "on average by 4.15% for MIMIC and 7.64% for TADPOLE.\r\n",
      "Keywords: Outcome/Disease Prediction · Population Graphs · PreTraining.\r\n",
      "\r\n",
      "1\r\n",
      "\r\n",
      "Introduction\r\n",
      "\r\n",
      "Enormous amounts of data are collected on a daily basis in hospitals. Nevertheless, labeled data can be scarce, as labeling can be tedious, time-consuming,\r\n",
      "and expensive. Further, in small hospitals and for rare diseases, only little data\r\n",
      "is accumulated [15]. The ability to leverage the large body of unlabeled medical\r\n",
      "data to improve in prediction tasks over such small labeled datasets could increase the confidence of AI models for clinical outcome prediction. Unsupervised\r\n",
      "pre-training was shown to be useful to exploit unlabeled data in NLP [20,4],\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "2\r\n",
      "\r\n",
      "Chantal Pellegrini, Anees Kazi, Nassir Navab\r\n",
      "\r\n",
      "CV [18,1] and medical imaging [16,2]. However, for more complex clinical data,\r\n",
      "it is not explored enough. Some works study how to pre-train BERT [4] over\r\n",
      "EHR data of medical diagnostic codes. They pre-train with modified masked\r\n",
      "language modeling and, in one case, a supervised prediction task. The targeted\r\n",
      "downstream tasks lie in disease and medication code prediction [23,11,21]. McDermott et al. [14] propose a pre-training approach over heterogeneous EHR\r\n",
      "data, including e.g. continuous lab results. They create a benchmark with several downstream tasks over the eICU [19] and MIMIC-III [7] datasets and present\r\n",
      "two baseline pre-training methods. Pre-training and fine-tuning are performed\r\n",
      "over single EHRs from ICU stays with a Gated Recurrent Unit.\r\n",
      "On the other hand, population graphs have been leveraged in the recent literature to help analyze patient data using the relationships among patients, leading\r\n",
      "to clinically semantic modeling of the data. During unsupervised pre-training,\r\n",
      "graphs allow learning representations based on feature similarities between the\r\n",
      "patients, which can then help to improve patient-level predictions. Several works\r\n",
      "successfully apply pre-training to graph data in different domains like molecular\r\n",
      "graphs and on common graph benchmarks. Proposed pre-training strategies include node level tasks, like attribute reconstruction, graph level tasks like property prediction, or generative tasks such as edge prediction [5,22,27,6,12]. To\r\n",
      "the best of our knowledge, no previous work applied pre-training to population\r\n",
      "graphs.\r\n",
      "In this paper, we propose a model capable of pre-training for understanding\r\n",
      "patient population data. We choose two medical applications in brain imaging\r\n",
      "and EHR data analysis on the public datasets TADPOLE [13] and MIMICIII [7] for Alzheimer’s disease prediction [17,9,3] and Length-of-Stay prediction\r\n",
      "[26,24,14]. The code is available at https://github.com/ChantalMP/UnsupervisedPre-Training-on-Patient-Population-Graphs-for-Patient-Level-Predictions.\r\n",
      "Contribution: We develop an unsupervised pre-training method to learn a\r\n",
      "general understanding of patient population data modeled as graph, providing a\r\n",
      "solution to limited labeled data. We show significant performance gains through\r\n",
      "pre-training when fine-tuning with as little as 1% and up to 100% labels. Further, we propose a (graph) transformer based model suitable for multi-modal\r\n",
      "data fusion. It is designed to handle various EHR input types, taking static\r\n",
      "and time-series data and continuous as well as discrete numerical features into\r\n",
      "account.\r\n",
      "\r\n",
      "2\r\n",
      "\r\n",
      "Method\r\n",
      "\r\n",
      "Let D be a dataset composed of the EHR data of N patients. The ith record is\r\n",
      "represented by ri ⊆ [di , ci , ti ] with static discrete features di ∈ ND , continuous\r\n",
      "features ci ∈ RC , and time-series features ti ∈ RS×τ , where τ denotes the length\r\n",
      "of the time-series. For every downstream task T , labels Y ∈ NL are given for L\r\n",
      "classes. The task is to predict the classes for the test set patients given all features. Towards this task, we propose to use patient population graphs. Unlike in\r\n",
      "non-graph-based methods, the model can exploit similarities between patients to\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Unsupervised Pre-Training on Patient Population Graphs\r\n",
      "\r\n",
      "3\r\n",
      "\r\n",
      "better understand the EHR data at patient and population level. Further, unlike\r\n",
      "conventional graph neural networks, graph transformers allow flexible attention\r\n",
      "to all nodes, learning which patients are relevant for a task. This would be most\r\n",
      "apt for learning over population EHR data. Our pipeline consists of two steps.\r\n",
      "1) Unsupervised pre-training and 2) Fine-tuning. Unsupervised pre-training enables understanding of general EHR data and disorder progression by training\r\n",
      "the model for masked imputation task. This understanding can help learn downstream tasks better despite limited labeled data.\r\n",
      "Graph Construction For each node pair with records ri and rj , we calculate a\r\n",
      "similarity score S(ri , rj ) between the node features. We use L2 distance for continuous and absolute matching for discrete features. As graph construction is not\r\n",
      "our focus, we choose the most conventional method using k-NN selection rule.\r\n",
      "We set k=5 to avoid having many disconnected components and very densely\r\n",
      "connected regions (see supplementary material). A detailed description of the\r\n",
      "graph construction per dataset follows in the experiment section.\r\n",
      "Model Architecture Our model consists of an encoder and decoder. The encoder comprises a data embedding module and a graph transformer module\r\n",
      "explained later. We design the encoder to handle various input data types. The\r\n",
      "decoder is a simple linear layer capable of capturing the essence of features inclined towards a node-level classification task. Figure 1 shows an overview of our\r\n",
      "model architecture.\r\n",
      "Data embedding module: Following the conventional Graphormer [25], we proEncoder\r\n",
      "\r\n",
      "Decoder\r\n",
      "Data Embedding Module\r\n",
      "\r\n",
      "static, discrete\r\n",
      "features d\r\n",
      "\r\n",
      "Embedding Layer\r\n",
      "\r\n",
      "static, continuous\r\n",
      "features c\r\n",
      "\r\n",
      "Linear Layer\r\n",
      "\r\n",
      "C' x N\r\n",
      "\r\n",
      "time-series\r\n",
      "features t\r\n",
      "\r\n",
      "Linear Layer\r\n",
      "\r\n",
      "S' x τ x N\r\n",
      "\r\n",
      "sum\r\n",
      "\r\n",
      "D' x N\r\n",
      "\r\n",
      "concat\r\n",
      "\r\n",
      "2x Transformer\r\n",
      "Layer\r\n",
      "\r\n",
      "Q\r\n",
      "Sequence\r\n",
      "\r\n",
      "K\r\n",
      "\r\n",
      "+\r\n",
      "\r\n",
      "V\r\n",
      "\r\n",
      "positional\r\n",
      "encodings\r\n",
      "\r\n",
      "mean\r\n",
      "\r\n",
      "Scale +\r\n",
      "Norm\r\n",
      "\r\n",
      "x\r\n",
      "\r\n",
      "Linear\r\n",
      "Layer\r\n",
      "\r\n",
      "multiple\r\n",
      "Graphormer\r\n",
      "Layer\r\n",
      "\r\n",
      "Linear Task\r\n",
      "Layer\r\n",
      "\r\n",
      "Loss\r\n",
      "\r\n",
      "S' x N\r\n",
      "\r\n",
      "add\r\n",
      "\r\n",
      "add\r\n",
      "x\r\n",
      "\r\n",
      "FxN\r\n",
      "\r\n",
      "Norm\r\n",
      "\r\n",
      "2x Linear\r\n",
      "Layer\r\n",
      "\r\n",
      "Norm\r\n",
      "\r\n",
      "Transformer Layer\r\n",
      "\r\n",
      "Fig. 1. Overview of the proposed architecture. All input features are combined into\r\n",
      "one node embedding, applying transformer layers to enhance the time-series features.\r\n",
      "The resulting graph is processed by several Graphormer layers and a linear task layer.\r\n",
      "\r\n",
      "cess discrete input features by an embedding layer, followed by a summation over\r\n",
      "0\r\n",
      "the feature dimension, resulting in embedded features d0i ∈ RD , where D0 is the\r\n",
      "output feature dimension. While Graphormer is limited to static, discrete input\r\n",
      "features only, we improve upon Graphormer to support also static, continuous\r\n",
      "input features, which are processed by a linear layer resulting in the embedding\r\n",
      "0\r\n",
      "vector c0i ∈ RC . The third branch of our data embedding module handles timeseries input features ti ∈ RS×τ with a linear layer, followed by two transformer\r\n",
      "layers to deal with variable sequence lengths and allow the model to incorporate\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "4\r\n",
      "\r\n",
      "Chantal Pellegrini, Anees Kazi, Nassir Navab\r\n",
      "\r\n",
      "temporal context. The output is given by t0i,h ∈ RE per time-step h. The mean\r\n",
      "0\r\n",
      "of these embeddings forms the final time-series embeddings t0i ∈ RS . The fea0\r\n",
      "ture vectors d0i , c0i and\r\n",
      "P ti are concatenated to form the final node embeddings\r\n",
      "F\r\n",
      "ni ∈ R , where F = Fk ⊂[D0 ,C 0 ,S 0 ] Fk , for each of the N nodes.\r\n",
      "Graphormer Module: The backbone of our model comprises multiple graph transformer layers [25]. Graphormer uses attention between all nodes in the graph.\r\n",
      "To incorporate the graph structure, structural encodings are used, which encode\r\n",
      "in and out degrees of the nodes, the distance between nodes, and edge features.\r\n",
      "Pre-training and Fine-Tuning: We propose an unsupervised pre-training\r\n",
      "technique on the same input features as for downstream tasks, but without using labels Y. Instead, we randomly mask a fixed percentage of feature values for\r\n",
      "every record ri and optimize the model to predict these values. During masking,\r\n",
      "static features are replaced by a randomly selected fixed value called ‘mask token’. For time-series features, we add a binary column per feature to the input\r\n",
      "vector, showing masked hours, and replace the value with zero. We optimize the\r\n",
      "model using (binary) cross entropy loss for discrete and mean squared error loss\r\n",
      "for continuous features. A model for fine-tuning is initialized using the encoder\r\n",
      "weights learned during pre-training and random weights for the decoder. Then\r\n",
      "the model is fine-tuned for the task T .\r\n",
      "\r\n",
      "3\r\n",
      "\r\n",
      "Experiments and Results\r\n",
      "\r\n",
      "We use two publicly available medical data sets: TADPOLE [13] and MIMICIII [7]. They differ in size, the targeted prediction task, and the type of input\r\n",
      "features, allowing comprehensive testing and evaluation of our method.\r\n",
      "3.1\r\n",
      "\r\n",
      "Datasets description:\r\n",
      "\r\n",
      "TADPOLE [13] contains 564 patients from the Alzheimer’s Disease Neuroimaging Initiative (ADNI). We use twelve features, which the TADPOLE challenge\r\n",
      "claims are informative. They include discrete cognitive test results, demographics, and continuous features extracted from MR and PET imaging, normalized\r\n",
      "between zero and one. The task is to classify the patients into the groups Cognitive Normal (CN), Mild Cognitive Impairment (MCI), or Alzheimer’s Disease\r\n",
      "(AD). We only use data from patients’ first visits to avoid leakage of information.\r\n",
      "Graph Construction: We construct a k-NN graph with k=5, dependent on the\r\n",
      "mean similarity (S) between\r\n",
      "( the features. For the demographics, age, gender and\r\n",
      "P 1 if fi = fj else 0\r\n",
      "apoe4, Sdem (ri , rj ) =\r\n",
      "÷ 3 where, f =(apoe4,\r\n",
      "1 if |agei − agej | ≤ 2 else 0\r\n",
      "gender). For the cognitive test results di (ordinal features), and ci (continuous imaging features),\r\n",
      "we calculate the respective normalized L2 distances:\r\n",
      "P\r\n",
      "P\r\n",
      "f ∈di ||fri −frj ||\r\n",
      "and Simg (ri , rj ) = sig( f ∈ci ||fri − frj ||). The\r\n",
      "Scog (ri , rj ) =\r\n",
      "max(di )\r\n",
      "overall similarity S(ri , rj ) is then given as mean of Sdem , Scog and Simg .\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Unsupervised Pre-Training on Patient Population Graphs\r\n",
      "\r\n",
      "5\r\n",
      "\r\n",
      "Pre-Training Configuration: During pre-training on TADPOLE, we randomly\r\n",
      "mask 30% of the medical features (APOE4, cognitive tests, and imaging features) in each sample. The masking ratio of 30% was chosen experimentally.\r\n",
      "MIMIC-III [7] is a large EHR dataset of patient records with various static\r\n",
      "and time-series data collected over the patient’s stay. We use the pre-processed\r\n",
      "dataset published by McDermott et al. [14]. It includes 19.7 K patients that are\r\n",
      "at least 15 years old and stayed 24 hours or more in the ICU. The features include\r\n",
      "demographics, measurements from bed-side monitoring and lab tests in hourly\r\n",
      "granularity (continuous), and binary features stating if different treatments were\r\n",
      "applied in each hour. In total we have 76 features. We use linear interpolation\r\n",
      "to impute missing measurements. Fine-tuning is evaluated on Length-of-Stay\r\n",
      "(LOS) prediction as defined in [14]. The input encompasses the first 24 hours of\r\n",
      "each patient’s stay, and the goal is to predict if a patient will stay longer than\r\n",
      "three days or not.\r\n",
      "Graph Construction: It is computationally infeasible to process a graph containing all patients. Thus, we create sub-graphs with 500 patients each, which\r\n",
      "fit into memory, each containing train, validation and test patients. We split\r\n",
      "randomly as we do not want to make assumptions on which types of patients\r\n",
      "the model should see, but learn this via the attention in the graph transformer.\r\n",
      "Given the time-series of the measurement features f, we form feature descriptors\r\n",
      "fd = (mean(f ), std(f ), min(f ), max(f )) per patient and feature, where d equals\r\n",
      "the 56 measurement features. We then compute the averagePsimilarity over all\r\n",
      "||fr −fr ||\r\n",
      "\r\n",
      "features fd between two patients ri and rj : Sim(ri , rj ) = f ∈fd |fd |i j and\r\n",
      "build a k-NN graph with k=5.\r\n",
      "Pre-Training Configuration: On MIMIC-III, we perform masking on the timeseries features from measurement and treatment data. Pre-training is performed\r\n",
      "over data from the first 24 hours of the patient’s stay. We compute the loss only\r\n",
      "over measured values, not over interpolated ones. Masking ratios are chosen\r\n",
      "experimentally. We compare two types of masking:\r\n",
      "Feature Masking (FM): We randomly select 30% of the features per patient and mask the full 24 hours of the time-series. The model can not see past or\r\n",
      "future values, only other features and patients, aiming to force an understanding\r\n",
      "of relations between features and patients to infer masked features.\r\n",
      "Block-wise Masking (BM): Instead of the full features, we mask a random\r\n",
      "block of 6 hours within the 24-hour time-series in 100% of the features. Here,\r\n",
      "the model can access past and future values to make a prediction. Thus, it can\r\n",
      "learn to understand temporal context during pre-training.\r\n",
      "3.2\r\n",
      "\r\n",
      "Experimental Setup\r\n",
      "\r\n",
      "Given a pre-trained model, we compare the results of fine-tuning it, with training the same but randomly initialized model from scratch. We manually tuned\r\n",
      "hyper-parameters per dataset separately for pre-training, from scratch training,\r\n",
      "and fine-tuning. To simulate scenarios with limited labeled data, we measure the\r\n",
      "model performance at different label ratios, meaning different amounts of labels\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "6\r\n",
      "\r\n",
      "Chantal Pellegrini, Anees Kazi, Nassir Navab\r\n",
      "\r\n",
      "(1%, 5%, 10%, 50%, 100%) are used for training or fine-tuning. For pre-training\r\n",
      "always the full training data is used.\r\n",
      "Implementation Details All experiments are implemented in PyTorch, performed on a TITAN Xp GPU with 12GB VRAM, and optimized with the Adam\r\n",
      "optimizer [10]. For cross-validation, pre-training is performed separately per\r\n",
      "fold. The model comprises four Graphormer layers for TADPOLE and eight for\r\n",
      "MIMIC-III. For TADPOLE, we pre-train for 6000 epochs with a LR of 1e-5. We\r\n",
      "train task prediction for 1200 epochs with a polynomial decaying LR (1e-5 to 5e6) to train from scratch and a LR of 5e-6 for fine-tuning. When fine-tuning with\r\n",
      "1% labels, we reduce the epochs to 200. All results are computed with 10-fold\r\n",
      "cross-validation. For MIMIC-III, we pre-train for 3000 epochs with a polynomial\r\n",
      "decaying LR (1e-3 to 1e-4). We train for 1100 epochs with a LR of 1e-4 from\r\n",
      "scratch, or fine-tune for 600 epochs with a LR of 1e-5. For a fair comparison with\r\n",
      "the state of the art, results are averaged over six folds, each with an 80-10-10\r\n",
      "split into train, validation and test data. The models are selected based on the\r\n",
      "validation sets, and performance is computed over the test sets.\r\n",
      "3.3\r\n",
      "\r\n",
      "Results\r\n",
      "\r\n",
      "Comparative methods: Table 1 We compare our model to related work without any pre-training. On TADPOLE, we compare to DGM [3], which proposes to\r\n",
      "learn an optimal population graph for the given task. Besides, one recent arxiv\r\n",
      "paper [8] further improves performance on TADPOLE by learning input feature\r\n",
      "importance. However it is out of context for this work. We achieve comparable accuracy to DGM and outperform in terms of AUC, which is an important\r\n",
      "metric for imbalanced datasets. For MIMIC-III, we compare our method to the\r\n",
      "EHR pre-training benchmark of McDermott et al. [14], which uses the same LOS\r\n",
      "definition and dataset. We significantly outperform the benchmark model. The\r\n",
      "results show that the proposed architecture is a good fit for the task at hand.\r\n",
      "Table 1. Accuracy and AUC of the proposed method compared with DGM on TADPOLE and McDermott et al. [14] on MIMIC-III.\r\n",
      "TADPOLE\r\n",
      "Model\r\n",
      "\r\n",
      "ACC\r\n",
      "\r\n",
      "MIMIC-III\r\n",
      "AUC\r\n",
      "\r\n",
      "Model\r\n",
      "\r\n",
      "ACC\r\n",
      "\r\n",
      "AUC\r\n",
      "\r\n",
      "DGM [3] 92.91 ± 02.50 94.49 ± 03.70 McDermott [14] not reported 71.00 ± 1.00\r\n",
      "Proposed 92.59 ± 3.64 96.96 ± 2.32\r\n",
      "Proposed\r\n",
      "70.29 ± 1.10 76.17 ± 1.02\r\n",
      "\r\n",
      "Effect of pre-training: Table 2 The motivation of this experiment is to investigate the smallest amount of labels required during the fine-tuning of the\r\n",
      "downstream task. The results emphasize the benefits of our unsupervised pretraining with limited labels. On TADPOLE the main benefit of pre-training\r\n",
      "can be seen for settings with limited labels (1%, 5%, 10%), where performance\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Unsupervised Pre-Training on Patient Population Graphs\r\n",
      "\r\n",
      "7\r\n",
      "\r\n",
      "improves significantly. Moreover, AUC continues to improve for all ratios. For\r\n",
      "LOS on MIMIC-III, both metrics significantly improve for all label ratios compared to from scratch training. Further, for MIMIC-III we compare two types of\r\n",
      "masking (BM, FM). We see that feature masking consistently outperforms blockwise masking. The performance improvements achieved through pre-training on\r\n",
      "MIMIC-III are significantly higher than in the benchmark [14]. Moreover, we see\r\n",
      "improvements until the full dataset size and not only for limited labels. Further\r\n",
      "the pre-trained models have a lower standard deviation, indicating higher stability.\r\n",
      "Ablation experiments: Table 3 We perform several ablation studies to evalTable 2. Performance of the proposed model in accuracy and AUC trained from scratch\r\n",
      "(SC) or fine-tuned after pre-training (FT) for different label ratios. For MIMIC-III we\r\n",
      "additionally compare the block-wise (BM) and feature masking (FM) to each other.\r\n",
      "TADPOLE\r\n",
      "Size Metric\r\n",
      "\r\n",
      "SC\r\n",
      "\r\n",
      "MIMIC-III\r\n",
      "FT\r\n",
      "\r\n",
      "SC\r\n",
      "\r\n",
      "FT: BM\r\n",
      "\r\n",
      "FT: FM\r\n",
      "\r\n",
      "1%\r\n",
      "\r\n",
      "ACC 59.42 ± 8.40 78.89 ± 2.45 59.86 ± 2.11 63.22 ± 2.39 65.25 ± 1.09\r\n",
      "AUC 68.72 ± 12.74 93.49 ± 2.07 62.98 ± 2.55 68.07 ± 1.80 69.90 ± 1.26\r\n",
      "\r\n",
      "5%\r\n",
      "\r\n",
      "ACC\r\n",
      "AUC\r\n",
      "\r\n",
      "78.23 ± 6.83 83.37 ± 6.29 64.79 ± 1.16 66.82 ± 0.89 68.66 ± 0.73\r\n",
      "87.23 ± 4.91 94.99 ± 2.55 68.85 ± 1.53 72.27 ± 1.19 73.97 ± 1.28\r\n",
      "\r\n",
      "10% ACC\r\n",
      "AUC\r\n",
      "\r\n",
      "87.00 ± 4.86 87.71 ± 4.65 64.72 ± 0.45 67.71 ± 0.69 69.42 ± 1.23\r\n",
      "92.03 ± 3.39 95.96 ± 2.51 68.97 ± 0.66 73.55 ± 0.60 75.09 ± 1.29\r\n",
      "\r\n",
      "50% ACC 92.41 ± 3.69 91.52 ± 3.76 67.41 ± 1.31 69.98 ± 0.69 70.85 ± 0.92\r\n",
      "AUC 96.06 ± 2.48 97.23 ± 1.94 72.53 ± 1.08 76.02 ± 0.87 76.86 ± 1.47\r\n",
      "100% ACC 92.59 ± 3.64 92.24 ± 3.47 70.29 ± 1.10 70.73 ± 0.70 71.44 ± 1.25\r\n",
      "AUC 96.96 ± 2.23 97.52 ± 1.67 76.17 ± 1.02 76.20 ± 0.54 77.78 ± 1.31\r\n",
      "\r\n",
      "uate different parts of our proposed model on pre- and task training.\r\n",
      "Effect of Graphormer: We replace the Graphormer module with a simple MLP\r\n",
      "or GCN layer and train the model from scratch on the full dataset (Table 3 a)).\r\n",
      "We see a clear benefit from using Graphormer compared to MLP and GCN. For\r\n",
      "TADPOLE, MLP reaches slightly better performance in terms of AUC as TADPOLE is a relatively small and easy dataset. The effect of the node level attention\r\n",
      "mechanism to all nodes given by Graphormer is clearly visible when compared\r\n",
      "to GCN. Further, we perform pre-training followed by fine-tuning for the MLP\r\n",
      "model (Table 3 b)). Our proposed unsupervised pre-training method proves to be\r\n",
      "beneficial also for the MLP model, but the effects are less as for our proposed architecture. Table 3 c) shows masked imputation performance during pre-training,\r\n",
      "measured by RMSE for continuous (imaging/ measurements) and accuracy or\r\n",
      "F1 for discrete features (apoe4+cognitive tests/treatments). Here the proposed\r\n",
      "model outperforms the MLP model, explaining why pre-training has a greater\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "8\r\n",
      "\r\n",
      "Chantal Pellegrini, Anees Kazi, Nassir Navab\r\n",
      "\r\n",
      "effect for it. In summary we see a positive effect of using Graphormer over MLP\r\n",
      "for solving the pre-training task and improving fine-tuning performance.\r\n",
      "Effect of Transformer: For MIMIC-III, Transformer is inserted in the encoder\r\n",
      "to deal with time series data. To test the transformer layers, we remove this\r\n",
      "component and train the model from scratch on the full dataset, resulting in a\r\n",
      "reduction of accuracy from 70.29 to 69.39% and AUC from 76.17 to 75.03%. This\r\n",
      "shows that the transformer layers are helpful for processing time-series inputs.\r\n",
      "The model needs to predict time-dependent outputs for pre-training on MIMICIII, for which the transformer layers are important, as they can understand the\r\n",
      "temporal context. To investigate the effect of transformer during pre-training,\r\n",
      "we remove the transformer layer and replace the Graphormer module with an\r\n",
      "MLP. We observe a reduction in the performance by 0.45% for ACC and 3.03% in\r\n",
      "AUC through pre-training. Accordingly, removing the transformer layer results\r\n",
      "in a 0.049 larger RMSE and a 3.9% lower F1 score in pre-training.\r\n",
      "Table 3. Ablations to test Graphormer module by replacing it with an MLP/GCN\r\n",
      "layer, a) downstream task performance trained from scratch b) results of fine-tuning\r\n",
      "(FT) on limited labels (TADPOLE 1%, MIMIC-III 10%), compared to training from\r\n",
      "scratch (SC) c) pre-training task performance, multi-class accuracy for cognitive tests\r\n",
      "uses feature-dependent error margins in which predictions are considered correct. The\r\n",
      "small number of imaging features might cause the low std of 0.006/0.008.\r\n",
      "TADPOLE\r\n",
      "\r\n",
      "a\r\n",
      "\r\n",
      "b\r\n",
      "\r\n",
      "Model\r\n",
      "\r\n",
      "ACC\r\n",
      "\r\n",
      "AUC\r\n",
      "\r\n",
      "MLP\r\n",
      "GCN\r\n",
      "Proposed\r\n",
      "\r\n",
      "91.14 ± 02.62 97.77 ± 01.59 67.25 ± 01.11 72.69 ± 00.97\r\n",
      "74.27 ± 06.41 89.89 ± 04.12 68.74 ± 01.50 72.64 ± 01.10\r\n",
      "92.59 ± 03.64 96.96 ± 02.23 70.29 ± 01.10 76.17 ± 01.02\r\n",
      "\r\n",
      "MLP SC\r\n",
      "54.20 ± 08.74\r\n",
      "MLP FT\r\n",
      "71.27 ± 09.76\r\n",
      "Proposed SC 59.42 ± 08.40\r\n",
      "Proposed FT 78.89 ± 02.45\r\n",
      "RMSE\r\n",
      "\r\n",
      "c\r\n",
      "\r\n",
      "4\r\n",
      "\r\n",
      "MIMIC-III\r\n",
      "\r\n",
      "MLP\r\n",
      "Proposed\r\n",
      "\r\n",
      "00.15 ± 0.008\r\n",
      "0.14 ± 0.006\r\n",
      "\r\n",
      "ACC\r\n",
      "\r\n",
      "AUC\r\n",
      "\r\n",
      "70.41 ± 11.41\r\n",
      "89.25 ± 06.53\r\n",
      "68.72 ± 12.74\r\n",
      "93.49 ± 02.07\r\n",
      "\r\n",
      "63.78 ± 00.74\r\n",
      "64.71 ± 00.84\r\n",
      "64.72 ± 00.45\r\n",
      "69.42 ± 01.23\r\n",
      "\r\n",
      "ACC\r\n",
      "\r\n",
      "RMSE\r\n",
      "\r\n",
      "62.58 ± 04.87 00.79 ± 0.023\r\n",
      "63.23 ± 04.25 0.78 ± 0.011\r\n",
      "\r\n",
      "67.72 ± 00.68\r\n",
      "67.94 ± 01.20\r\n",
      "68.97 ± 00.66\r\n",
      "75.09 ± 01.29\r\n",
      "F1\r\n",
      "81.49 ± 00.35\r\n",
      "81.58 ± 00.41\r\n",
      "\r\n",
      "Conclusion\r\n",
      "\r\n",
      "In this paper, we present an unsupervised pre-training method based on masked\r\n",
      "imputation, significantly improving prediction results. We propose a graph transformer based architecture for learning on population graphs built from heterogeneous EHR data. We show the superiority of our pipeline in both pre-training\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Unsupervised Pre-Training on Patient Population Graphs\r\n",
      "\r\n",
      "9\r\n",
      "\r\n",
      "and various prediction tasks for two datasets, TADPOLE and MIMIC-III. Pretraining helps for all dataset sizes but especially in scenarios where only a limited\r\n",
      "amount of labeled data is used for fine-tuning. Our pre-training method is unsupervised and therefore independent from the end task, and further it is well\r\n",
      "suited for transfer learning. This work opens the path for the community to deals\r\n",
      "with small dataset specially with limited labels.\r\n",
      "\r\n",
      "References\r\n",
      "1. Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv\r\n",
      "preprint arXiv:2106.08254 (2021)\r\n",
      "2. Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., Rueckert, D.: Selfsupervised learning for medical image analysis using image context restoration.\r\n",
      "Medical image analysis 58, 101539 (2019)\r\n",
      "3. Cosmo, L., Kazi, A., Ahmadi, S.A., Navab, N., Bronstein, M.: Latent-graph learning for disease prediction. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer (2020)\r\n",
      "4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: Burstein, J., Doran, C., Solorio,\r\n",
      "T. (eds.) NAACL-HLT (1). Association for Computational Linguistics (2019)\r\n",
      "5. Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., Leskovec, J.: Strategies\r\n",
      "for pre-training graph neural networks. arXiv preprint arXiv:1905.12265 (2019)\r\n",
      "6. Hu, Z., Dong, Y., Wang, K., Chang, K.W., Sun, Y.: Gpt-gnn: Generative pretraining of graph neural networks. In: Proceedings of the 26th ACM SIGKDD\r\n",
      "International Conference on Knowledge Discovery & Data Mining (2020)\r\n",
      "7. Johnson, A.E., Pollard, T.J., Shen, L., Lehman, L.w.H., Feng, M., Ghassemi, M.,\r\n",
      "Moody, B., Szolovits, P., Anthony Celi, L., Mark, R.G.: Mimic-iii, a freely accessible\r\n",
      "critical care database. Scientific data 3(1) (2016)\r\n",
      "8. Kazi, A., Farghadani, S., Navab, N.: Ia-gcn: Interpretable attention based graph\r\n",
      "convolutional network for disease prediction. arXiv preprint arXiv:2103.15587\r\n",
      "(2021)\r\n",
      "9. Kazi, A., Shekarforoush, S., Kortuem, K., Albarqouni, S., Navab, N., et al.: Selfattention equipped graph convolutions for disease prediction. In: 2019 IEEE 16th\r\n",
      "International Symposium on Biomedical Imaging (ISBI 2019). IEEE (2019)\r\n",
      "10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\r\n",
      "arXiv:1412.6980 (2014)\r\n",
      "11. Li, Y., Rao, S., Solares, J.R.A., Hassaine, A., Ramakrishnan, R., Canoy, D., Zhu,\r\n",
      "Y., Rahimi, K., Salimi-Khorshidi, G.: Behrt: transformer for electronic health\r\n",
      "records. Scientific reports 10(1) (2020)\r\n",
      "12. Lu, Y., Jiang, X., Fang, Y., Shi, C.: Learning to pre-train graph neural networks.\r\n",
      "AAAI (2021)\r\n",
      "13. Marinescu, R.V., Oxtoby, N.P., Young, A.L., Bron, E.E., Toga, A.W., Weiner,\r\n",
      "M.W., Barkhof, F., Fox, N.C., Klein, S., Alexander, D.C., et al.: Tadpole challenge: prediction of longitudinal evolution in alzheimer’s disease. arXiv preprint\r\n",
      "arXiv:1805.03909 (2018)\r\n",
      "14. McDermott, M., Nestor, B., Kim, E., Zhang, W., Goldenberg, A., Szolovits, P.,\r\n",
      "Ghassemi, M.: A comprehensive ehr timeseries pre-training benchmark. In: Proceedings of the Conference on Health, Inference, and Learning (2021)\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "10\r\n",
      "\r\n",
      "Chantal Pellegrini, Anees Kazi, Nassir Navab\r\n",
      "\r\n",
      "15. Mitani, A.A., Haneuse, S.: Small data challenges of studying rare diseases. JAMA\r\n",
      "network open 3(3) (2020)\r\n",
      "16. Ouyang, C., Biffi, C., Chen, C., Kart, T., Qiu, H., Rueckert, D.: Self-supervision\r\n",
      "with superpixels: Training few-shot medical image segmentation without annotation. In: European Conference on Computer Vision. pp. 762–780. Springer (2020)\r\n",
      "17. Parisot, S., Ktena, S.I., Ferrante, E., Lee, M., Guerrero, R., Glocker, B., Rueckert,\r\n",
      "D.: Disease prediction using graph convolutional networks: application to autism\r\n",
      "spectrum disorder and alzheimer’s disease. Medical image analysis 48 (2018)\r\n",
      "18. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders: Feature learning by inpainting. In: Proceedings of the IEEE conference on\r\n",
      "computer vision and pattern recognition (2016)\r\n",
      "19. Pollard, T.J., Johnson, A.E., Raffa, J.D., Celi, L.A., Mark, R.G., Badawi, O.: The\r\n",
      "eicu collaborative research database, a freely available multi-center database for\r\n",
      "critical care research. Scientific data 5(1) (2018)\r\n",
      "20. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training (2018)\r\n",
      "21. Rasmy, L., Xiang, Y., Xie, Z., Tao, C., Zhi, D.: Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease\r\n",
      "prediction. NPJ digital medicine 4(1) (2021)\r\n",
      "22. Rong, Y., Bian, Y., Xu, T., Xie, W., Wei, Y., Huang, W., Huang, J.: Grover: Selfsupervised message passing transformer on large-scale molecular data. Advances\r\n",
      "in Neural Information Processing Systems (2020)\r\n",
      "23. Shang, J., Ma, T., Xiao, C., Sun, J.: Pre-training of graph augmented transformers\r\n",
      "for medication recommendation. arXiv preprint arXiv:1906.00346 (2019)\r\n",
      "24. Wang, S., McDermott, M.B., Chauhan, G., Ghassemi, M., Hughes, M.C., Naumann, T.: Mimic-extract: A data extraction, preprocessing, and representation\r\n",
      "pipeline for mimic-iii. In: Proceedings of the ACM conference on health, inference,\r\n",
      "and learning (2020)\r\n",
      "25. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., Liu, T.Y.: Do\r\n",
      "transformers really perform badly for graph representation? Advances in Neural\r\n",
      "Information Processing Systems 34 (2021)\r\n",
      "26. Zebin, T., Rezvy, S., Chaussalet, T.J.: A deep learning approach for length of\r\n",
      "stay prediction in clinical settings from medical records. In: 2019 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology\r\n",
      "(CIBCB). IEEE (2019)\r\n",
      "27. Zhang, J., Zhang, H., Xia, C., Sun, L.: Graph-bert: Only attention is needed for\r\n",
      "learning graph representations. arXiv preprint arXiv:2001.05140 (2020)\r\n",
      "\r\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "print(array_authors[0])\n",
    "print(array_pdf_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1220700",
   "metadata": {},
   "source": [
    "### Extract all word after the term: \"References\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e6468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unsupervised Pre-Training on Patient Population\\r\\nGraphs for Patient-Level Predictions\\r\\nChantal Pellegrini1 , Anees Kazi1 , and Nassir Navab1,2\\r\\nComputer Aided Medical Procedures, Technical University Munich, Germany\\r\\nComputer Aided Medical Procedures, Johns Hopkins University, Baltimore, USA\\r\\n\\r\\n1\\r\\n\\r\\narXiv:2203.12616v1 [cs.LG] 23 Mar 2022\\r\\n\\r\\n2\\r\\n\\r\\nAbstract. Pre-training has shown success in different areas of machine\\r\\nlearning, such as Computer Vision (CV), Natural Language Processing\\r\\n(NLP) and medical imaging. However, it has not been fully explored\\r\\nfor clinical data analysis. Even though an immense amount of Electronic Health Record (EHR) data is recorded, data and labels can be\\r\\nscarce if the data is collected in small hospitals or deals with rare diseases. In such scenarios, pre-training on a larger set of EHR data could\\r\\nimprove the model performance. In this paper, we apply unsupervised\\r\\npre-training to heterogeneous, multi-modal EHR data for patient outcome prediction. To model this data, we leverage graph deep learning\\r\\nover population graphs. We first design a network architecture based on\\r\\ngraph transformer designed to handle various input feature types occurring in EHR data, like continuous, discrete, and time-series features,\\r\\nallowing better multi-modal data fusion. Further, we design pre-training\\r\\nmethods based on masked imputation to pre-train our network before\\r\\nfine-tuning on different end tasks. Pre-training is done in a fully unsupervised fashion, which lays the groundwork for pre-training on large\\r\\npublic datasets with different tasks and similar modalities in the future.\\r\\nWe test our method on two medical datasets of patient records, TADPOLE and MIMIC-III, including imaging and non-imaging features and\\r\\ndifferent prediction tasks. We find that our proposed graph based pretraining method helps in modeling the data at a population level and\\r\\nfurther improves performance on the fine tuning tasks in terms of AUC\\r\\non average by 4.15% for MIMIC and 7.64% for TADPOLE.\\r\\nKeywords: Outcome/Disease Prediction · Population Graphs · PreTraining.\\r\\n\\r\\n1\\r\\n\\r\\nIntroduction\\r\\n\\r\\nEnormous amounts of data are collected on a daily basis in hospitals. Nevertheless, labeled data can be scarce, as labeling can be tedious, time-consuming,\\r\\nand expensive. Further, in small hospitals and for rare diseases, only little data\\r\\nis accumulated [15]. The ability to leverage the large body of unlabeled medical\\r\\ndata to improve in prediction tasks over such small labeled datasets could increase the confidence of AI models for clinical outcome prediction. Unsupervised\\r\\npre-training was shown to be useful to exploit unlabeled data in NLP [20,4],\\r\\n\\r\\n\\x0c\\n\\n2\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\nCV [18,1] and medical imaging [16,2]. However, for more complex clinical data,\\r\\nit is not explored enough. Some works study how to pre-train BERT [4] over\\r\\nEHR data of medical diagnostic codes. They pre-train with modified masked\\r\\nlanguage modeling and, in one case, a supervised prediction task. The targeted\\r\\ndownstream tasks lie in disease and medication code prediction [23,11,21]. McDermott et al. [14] propose a pre-training approach over heterogeneous EHR\\r\\ndata, including e.g. continuous lab results. They create a benchmark with several downstream tasks over the eICU [19] and MIMIC-III [7] datasets and present\\r\\ntwo baseline pre-training methods. Pre-training and fine-tuning are performed\\r\\nover single EHRs from ICU stays with a Gated Recurrent Unit.\\r\\nOn the other hand, population graphs have been leveraged in the recent literature to help analyze patient data using the relationships among patients, leading\\r\\nto clinically semantic modeling of the data. During unsupervised pre-training,\\r\\ngraphs allow learning representations based on feature similarities between the\\r\\npatients, which can then help to improve patient-level predictions. Several works\\r\\nsuccessfully apply pre-training to graph data in different domains like molecular\\r\\ngraphs and on common graph benchmarks. Proposed pre-training strategies include node level tasks, like attribute reconstruction, graph level tasks like property prediction, or generative tasks such as edge prediction [5,22,27,6,12]. To\\r\\nthe best of our knowledge, no previous work applied pre-training to population\\r\\ngraphs.\\r\\nIn this paper, we propose a model capable of pre-training for understanding\\r\\npatient population data. We choose two medical applications in brain imaging\\r\\nand EHR data analysis on the public datasets TADPOLE [13] and MIMICIII [7] for Alzheimer’s disease prediction [17,9,3] and Length-of-Stay prediction\\r\\n[26,24,14]. The code is available at https://github.com/ChantalMP/UnsupervisedPre-Training-on-Patient-Population-Graphs-for-Patient-Level-Predictions.\\r\\nContribution: We develop an unsupervised pre-training method to learn a\\r\\ngeneral understanding of patient population data modeled as graph, providing a\\r\\nsolution to limited labeled data. We show significant performance gains through\\r\\npre-training when fine-tuning with as little as 1% and up to 100% labels. Further, we propose a (graph) transformer based model suitable for multi-modal\\r\\ndata fusion. It is designed to handle various EHR input types, taking static\\r\\nand time-series data and continuous as well as discrete numerical features into\\r\\naccount.\\r\\n\\r\\n2\\r\\n\\r\\nMethod\\r\\n\\r\\nLet D be a dataset composed of the EHR data of N patients. The ith record is\\r\\nrepresented by ri ⊆ [di , ci , ti ] with static discrete features di ∈ ND , continuous\\r\\nfeatures ci ∈ RC , and time-series features ti ∈ RS×τ , where τ denotes the length\\r\\nof the time-series. For every downstream task T , labels Y ∈ NL are given for L\\r\\nclasses. The task is to predict the classes for the test set patients given all features. Towards this task, we propose to use patient population graphs. Unlike in\\r\\nnon-graph-based methods, the model can exploit similarities between patients to\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n3\\r\\n\\r\\nbetter understand the EHR data at patient and population level. Further, unlike\\r\\nconventional graph neural networks, graph transformers allow flexible attention\\r\\nto all nodes, learning which patients are relevant for a task. This would be most\\r\\napt for learning over population EHR data. Our pipeline consists of two steps.\\r\\n1) Unsupervised pre-training and 2) Fine-tuning. Unsupervised pre-training enables understanding of general EHR data and disorder progression by training\\r\\nthe model for masked imputation task. This understanding can help learn downstream tasks better despite limited labeled data.\\r\\nGraph Construction For each node pair with records ri and rj , we calculate a\\r\\nsimilarity score S(ri , rj ) between the node features. We use L2 distance for continuous and absolute matching for discrete features. As graph construction is not\\r\\nour focus, we choose the most conventional method using k-NN selection rule.\\r\\nWe set k=5 to avoid having many disconnected components and very densely\\r\\nconnected regions (see supplementary material). A detailed description of the\\r\\ngraph construction per dataset follows in the experiment section.\\r\\nModel Architecture Our model consists of an encoder and decoder. The encoder comprises a data embedding module and a graph transformer module\\r\\nexplained later. We design the encoder to handle various input data types. The\\r\\ndecoder is a simple linear layer capable of capturing the essence of features inclined towards a node-level classification task. Figure 1 shows an overview of our\\r\\nmodel architecture.\\r\\nData embedding module: Following the conventional Graphormer [25], we proEncoder\\r\\n\\r\\nDecoder\\r\\nData Embedding Module\\r\\n\\r\\nstatic, discrete\\r\\nfeatures d\\r\\n\\r\\nEmbedding Layer\\r\\n\\r\\nstatic, continuous\\r\\nfeatures c\\r\\n\\r\\nLinear Layer\\r\\n\\r\\nC' x N\\r\\n\\r\\ntime-series\\r\\nfeatures t\\r\\n\\r\\nLinear Layer\\r\\n\\r\\nS' x τ x N\\r\\n\\r\\nsum\\r\\n\\r\\nD' x N\\r\\n\\r\\nconcat\\r\\n\\r\\n2x Transformer\\r\\nLayer\\r\\n\\r\\nQ\\r\\nSequence\\r\\n\\r\\nK\\r\\n\\r\\n+\\r\\n\\r\\nV\\r\\n\\r\\npositional\\r\\nencodings\\r\\n\\r\\nmean\\r\\n\\r\\nScale +\\r\\nNorm\\r\\n\\r\\nx\\r\\n\\r\\nLinear\\r\\nLayer\\r\\n\\r\\nmultiple\\r\\nGraphormer\\r\\nLayer\\r\\n\\r\\nLinear Task\\r\\nLayer\\r\\n\\r\\nLoss\\r\\n\\r\\nS' x N\\r\\n\\r\\nadd\\r\\n\\r\\nadd\\r\\nx\\r\\n\\r\\nFxN\\r\\n\\r\\nNorm\\r\\n\\r\\n2x Linear\\r\\nLayer\\r\\n\\r\\nNorm\\r\\n\\r\\nTransformer Layer\\r\\n\\r\\nFig. 1. Overview of the proposed architecture. All input features are combined into\\r\\none node embedding, applying transformer layers to enhance the time-series features.\\r\\nThe resulting graph is processed by several Graphormer layers and a linear task layer.\\r\\n\\r\\ncess discrete input features by an embedding layer, followed by a summation over\\r\\n0\\r\\nthe feature dimension, resulting in embedded features d0i ∈ RD , where D0 is the\\r\\noutput feature dimension. While Graphormer is limited to static, discrete input\\r\\nfeatures only, we improve upon Graphormer to support also static, continuous\\r\\ninput features, which are processed by a linear layer resulting in the embedding\\r\\n0\\r\\nvector c0i ∈ RC . The third branch of our data embedding module handles timeseries input features ti ∈ RS×τ with a linear layer, followed by two transformer\\r\\nlayers to deal with variable sequence lengths and allow the model to incorporate\\r\\n\\r\\n\\x0c\\n\\n4\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\ntemporal context. The output is given by t0i,h ∈ RE per time-step h. The mean\\r\\n0\\r\\nof these embeddings forms the final time-series embeddings t0i ∈ RS . The fea0\\r\\nture vectors d0i , c0i and\\r\\nP ti are concatenated to form the final node embeddings\\r\\nF\\r\\nni ∈ R , where F = Fk ⊂[D0 ,C 0 ,S 0 ] Fk , for each of the N nodes.\\r\\nGraphormer Module: The backbone of our model comprises multiple graph transformer layers [25]. Graphormer uses attention between all nodes in the graph.\\r\\nTo incorporate the graph structure, structural encodings are used, which encode\\r\\nin and out degrees of the nodes, the distance between nodes, and edge features.\\r\\nPre-training and Fine-Tuning: We propose an unsupervised pre-training\\r\\ntechnique on the same input features as for downstream tasks, but without using labels Y. Instead, we randomly mask a fixed percentage of feature values for\\r\\nevery record ri and optimize the model to predict these values. During masking,\\r\\nstatic features are replaced by a randomly selected fixed value called ‘mask token’. For time-series features, we add a binary column per feature to the input\\r\\nvector, showing masked hours, and replace the value with zero. We optimize the\\r\\nmodel using (binary) cross entropy loss for discrete and mean squared error loss\\r\\nfor continuous features. A model for fine-tuning is initialized using the encoder\\r\\nweights learned during pre-training and random weights for the decoder. Then\\r\\nthe model is fine-tuned for the task T .\\r\\n\\r\\n3\\r\\n\\r\\nExperiments and Results\\r\\n\\r\\nWe use two publicly available medical data sets: TADPOLE [13] and MIMICIII [7]. They differ in size, the targeted prediction task, and the type of input\\r\\nfeatures, allowing comprehensive testing and evaluation of our method.\\r\\n3.1\\r\\n\\r\\nDatasets description:\\r\\n\\r\\nTADPOLE [13] contains 564 patients from the Alzheimer’s Disease Neuroimaging Initiative (ADNI). We use twelve features, which the TADPOLE challenge\\r\\nclaims are informative. They include discrete cognitive test results, demographics, and continuous features extracted from MR and PET imaging, normalized\\r\\nbetween zero and one. The task is to classify the patients into the groups Cognitive Normal (CN), Mild Cognitive Impairment (MCI), or Alzheimer’s Disease\\r\\n(AD). We only use data from patients’ first visits to avoid leakage of information.\\r\\nGraph Construction: We construct a k-NN graph with k=5, dependent on the\\r\\nmean similarity (S) between\\r\\n( the features. For the demographics, age, gender and\\r\\nP 1 if fi = fj else 0\\r\\napoe4, Sdem (ri , rj ) =\\r\\n÷ 3 where, f =(apoe4,\\r\\n1 if |agei − agej | ≤ 2 else 0\\r\\ngender). For the cognitive test results di (ordinal features), and ci (continuous imaging features),\\r\\nwe calculate the respective normalized L2 distances:\\r\\nP\\r\\nP\\r\\nf ∈di ||fri −frj ||\\r\\nand Simg (ri , rj ) = sig( f ∈ci ||fri − frj ||). The\\r\\nScog (ri , rj ) =\\r\\nmax(di )\\r\\noverall similarity S(ri , rj ) is then given as mean of Sdem , Scog and Simg .\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n5\\r\\n\\r\\nPre-Training Configuration: During pre-training on TADPOLE, we randomly\\r\\nmask 30% of the medical features (APOE4, cognitive tests, and imaging features) in each sample. The masking ratio of 30% was chosen experimentally.\\r\\nMIMIC-III [7] is a large EHR dataset of patient records with various static\\r\\nand time-series data collected over the patient’s stay. We use the pre-processed\\r\\ndataset published by McDermott et al. [14]. It includes 19.7 K patients that are\\r\\nat least 15 years old and stayed 24 hours or more in the ICU. The features include\\r\\ndemographics, measurements from bed-side monitoring and lab tests in hourly\\r\\ngranularity (continuous), and binary features stating if different treatments were\\r\\napplied in each hour. In total we have 76 features. We use linear interpolation\\r\\nto impute missing measurements. Fine-tuning is evaluated on Length-of-Stay\\r\\n(LOS) prediction as defined in [14]. The input encompasses the first 24 hours of\\r\\neach patient’s stay, and the goal is to predict if a patient will stay longer than\\r\\nthree days or not.\\r\\nGraph Construction: It is computationally infeasible to process a graph containing all patients. Thus, we create sub-graphs with 500 patients each, which\\r\\nfit into memory, each containing train, validation and test patients. We split\\r\\nrandomly as we do not want to make assumptions on which types of patients\\r\\nthe model should see, but learn this via the attention in the graph transformer.\\r\\nGiven the time-series of the measurement features f, we form feature descriptors\\r\\nfd = (mean(f ), std(f ), min(f ), max(f )) per patient and feature, where d equals\\r\\nthe 56 measurement features. We then compute the averagePsimilarity over all\\r\\n||fr −fr ||\\r\\n\\r\\nfeatures fd between two patients ri and rj : Sim(ri , rj ) = f ∈fd |fd |i j and\\r\\nbuild a k-NN graph with k=5.\\r\\nPre-Training Configuration: On MIMIC-III, we perform masking on the timeseries features from measurement and treatment data. Pre-training is performed\\r\\nover data from the first 24 hours of the patient’s stay. We compute the loss only\\r\\nover measured values, not over interpolated ones. Masking ratios are chosen\\r\\nexperimentally. We compare two types of masking:\\r\\nFeature Masking (FM): We randomly select 30% of the features per patient and mask the full 24 hours of the time-series. The model can not see past or\\r\\nfuture values, only other features and patients, aiming to force an understanding\\r\\nof relations between features and patients to infer masked features.\\r\\nBlock-wise Masking (BM): Instead of the full features, we mask a random\\r\\nblock of 6 hours within the 24-hour time-series in 100% of the features. Here,\\r\\nthe model can access past and future values to make a prediction. Thus, it can\\r\\nlearn to understand temporal context during pre-training.\\r\\n3.2\\r\\n\\r\\nExperimental Setup\\r\\n\\r\\nGiven a pre-trained model, we compare the results of fine-tuning it, with training the same but randomly initialized model from scratch. We manually tuned\\r\\nhyper-parameters per dataset separately for pre-training, from scratch training,\\r\\nand fine-tuning. To simulate scenarios with limited labeled data, we measure the\\r\\nmodel performance at different label ratios, meaning different amounts of labels\\r\\n\\r\\n\\x0c\\n\\n6\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\n(1%, 5%, 10%, 50%, 100%) are used for training or fine-tuning. For pre-training\\r\\nalways the full training data is used.\\r\\nImplementation Details All experiments are implemented in PyTorch, performed on a TITAN Xp GPU with 12GB VRAM, and optimized with the Adam\\r\\noptimizer [10]. For cross-validation, pre-training is performed separately per\\r\\nfold. The model comprises four Graphormer layers for TADPOLE and eight for\\r\\nMIMIC-III. For TADPOLE, we pre-train for 6000 epochs with a LR of 1e-5. We\\r\\ntrain task prediction for 1200 epochs with a polynomial decaying LR (1e-5 to 5e6) to train from scratch and a LR of 5e-6 for fine-tuning. When fine-tuning with\\r\\n1% labels, we reduce the epochs to 200. All results are computed with 10-fold\\r\\ncross-validation. For MIMIC-III, we pre-train for 3000 epochs with a polynomial\\r\\ndecaying LR (1e-3 to 1e-4). We train for 1100 epochs with a LR of 1e-4 from\\r\\nscratch, or fine-tune for 600 epochs with a LR of 1e-5. For a fair comparison with\\r\\nthe state of the art, results are averaged over six folds, each with an 80-10-10\\r\\nsplit into train, validation and test data. The models are selected based on the\\r\\nvalidation sets, and performance is computed over the test sets.\\r\\n3.3\\r\\n\\r\\nResults\\r\\n\\r\\nComparative methods: Table 1 We compare our model to related work without any pre-training. On TADPOLE, we compare to DGM [3], which proposes to\\r\\nlearn an optimal population graph for the given task. Besides, one recent arxiv\\r\\npaper [8] further improves performance on TADPOLE by learning input feature\\r\\nimportance. However it is out of context for this work. We achieve comparable accuracy to DGM and outperform in terms of AUC, which is an important\\r\\nmetric for imbalanced datasets. For MIMIC-III, we compare our method to the\\r\\nEHR pre-training benchmark of McDermott et al. [14], which uses the same LOS\\r\\ndefinition and dataset. We significantly outperform the benchmark model. The\\r\\nresults show that the proposed architecture is a good fit for the task at hand.\\r\\nTable 1. Accuracy and AUC of the proposed method compared with DGM on TADPOLE and McDermott et al. [14] on MIMIC-III.\\r\\nTADPOLE\\r\\nModel\\r\\n\\r\\nACC\\r\\n\\r\\nMIMIC-III\\r\\nAUC\\r\\n\\r\\nModel\\r\\n\\r\\nACC\\r\\n\\r\\nAUC\\r\\n\\r\\nDGM [3] 92.91 ± 02.50 94.49 ± 03.70 McDermott [14] not reported 71.00 ± 1.00\\r\\nProposed 92.59 ± 3.64 96.96 ± 2.32\\r\\nProposed\\r\\n70.29 ± 1.10 76.17 ± 1.02\\r\\n\\r\\nEffect of pre-training: Table 2 The motivation of this experiment is to investigate the smallest amount of labels required during the fine-tuning of the\\r\\ndownstream task. The results emphasize the benefits of our unsupervised pretraining with limited labels. On TADPOLE the main benefit of pre-training\\r\\ncan be seen for settings with limited labels (1%, 5%, 10%), where performance\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n7\\r\\n\\r\\nimproves significantly. Moreover, AUC continues to improve for all ratios. For\\r\\nLOS on MIMIC-III, both metrics significantly improve for all label ratios compared to from scratch training. Further, for MIMIC-III we compare two types of\\r\\nmasking (BM, FM). We see that feature masking consistently outperforms blockwise masking. The performance improvements achieved through pre-training on\\r\\nMIMIC-III are significantly higher than in the benchmark [14]. Moreover, we see\\r\\nimprovements until the full dataset size and not only for limited labels. Further\\r\\nthe pre-trained models have a lower standard deviation, indicating higher stability.\\r\\nAblation experiments: Table 3 We perform several ablation studies to evalTable 2. Performance of the proposed model in accuracy and AUC trained from scratch\\r\\n(SC) or fine-tuned after pre-training (FT) for different label ratios. For MIMIC-III we\\r\\nadditionally compare the block-wise (BM) and feature masking (FM) to each other.\\r\\nTADPOLE\\r\\nSize Metric\\r\\n\\r\\nSC\\r\\n\\r\\nMIMIC-III\\r\\nFT\\r\\n\\r\\nSC\\r\\n\\r\\nFT: BM\\r\\n\\r\\nFT: FM\\r\\n\\r\\n1%\\r\\n\\r\\nACC 59.42 ± 8.40 78.89 ± 2.45 59.86 ± 2.11 63.22 ± 2.39 65.25 ± 1.09\\r\\nAUC 68.72 ± 12.74 93.49 ± 2.07 62.98 ± 2.55 68.07 ± 1.80 69.90 ± 1.26\\r\\n\\r\\n5%\\r\\n\\r\\nACC\\r\\nAUC\\r\\n\\r\\n78.23 ± 6.83 83.37 ± 6.29 64.79 ± 1.16 66.82 ± 0.89 68.66 ± 0.73\\r\\n87.23 ± 4.91 94.99 ± 2.55 68.85 ± 1.53 72.27 ± 1.19 73.97 ± 1.28\\r\\n\\r\\n10% ACC\\r\\nAUC\\r\\n\\r\\n87.00 ± 4.86 87.71 ± 4.65 64.72 ± 0.45 67.71 ± 0.69 69.42 ± 1.23\\r\\n92.03 ± 3.39 95.96 ± 2.51 68.97 ± 0.66 73.55 ± 0.60 75.09 ± 1.29\\r\\n\\r\\n50% ACC 92.41 ± 3.69 91.52 ± 3.76 67.41 ± 1.31 69.98 ± 0.69 70.85 ± 0.92\\r\\nAUC 96.06 ± 2.48 97.23 ± 1.94 72.53 ± 1.08 76.02 ± 0.87 76.86 ± 1.47\\r\\n100% ACC 92.59 ± 3.64 92.24 ± 3.47 70.29 ± 1.10 70.73 ± 0.70 71.44 ± 1.25\\r\\nAUC 96.96 ± 2.23 97.52 ± 1.67 76.17 ± 1.02 76.20 ± 0.54 77.78 ± 1.31\\r\\n\\r\\nuate different parts of our proposed model on pre- and task training.\\r\\nEffect of Graphormer: We replace the Graphormer module with a simple MLP\\r\\nor GCN layer and train the model from scratch on the full dataset (Table 3 a)).\\r\\nWe see a clear benefit from using Graphormer compared to MLP and GCN. For\\r\\nTADPOLE, MLP reaches slightly better performance in terms of AUC as TADPOLE is a relatively small and easy dataset. The effect of the node level attention\\r\\nmechanism to all nodes given by Graphormer is clearly visible when compared\\r\\nto GCN. Further, we perform pre-training followed by fine-tuning for the MLP\\r\\nmodel (Table 3 b)). Our proposed unsupervised pre-training method proves to be\\r\\nbeneficial also for the MLP model, but the effects are less as for our proposed architecture. Table 3 c) shows masked imputation performance during pre-training,\\r\\nmeasured by RMSE for continuous (imaging/ measurements) and accuracy or\\r\\nF1 for discrete features (apoe4+cognitive tests/treatments). Here the proposed\\r\\nmodel outperforms the MLP model, explaining why pre-training has a greater\\r\\n\\r\\n\\x0c\\n\\n8\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\neffect for it. In summary we see a positive effect of using Graphormer over MLP\\r\\nfor solving the pre-training task and improving fine-tuning performance.\\r\\nEffect of Transformer: For MIMIC-III, Transformer is inserted in the encoder\\r\\nto deal with time series data. To test the transformer layers, we remove this\\r\\ncomponent and train the model from scratch on the full dataset, resulting in a\\r\\nreduction of accuracy from 70.29 to 69.39% and AUC from 76.17 to 75.03%. This\\r\\nshows that the transformer layers are helpful for processing time-series inputs.\\r\\nThe model needs to predict time-dependent outputs for pre-training on MIMICIII, for which the transformer layers are important, as they can understand the\\r\\ntemporal context. To investigate the effect of transformer during pre-training,\\r\\nwe remove the transformer layer and replace the Graphormer module with an\\r\\nMLP. We observe a reduction in the performance by 0.45% for ACC and 3.03% in\\r\\nAUC through pre-training. Accordingly, removing the transformer layer results\\r\\nin a 0.049 larger RMSE and a 3.9% lower F1 score in pre-training.\\r\\nTable 3. Ablations to test Graphormer module by replacing it with an MLP/GCN\\r\\nlayer, a) downstream task performance trained from scratch b) results of fine-tuning\\r\\n(FT) on limited labels (TADPOLE 1%, MIMIC-III 10%), compared to training from\\r\\nscratch (SC) c) pre-training task performance, multi-class accuracy for cognitive tests\\r\\nuses feature-dependent error margins in which predictions are considered correct. The\\r\\nsmall number of imaging features might cause the low std of 0.006/0.008.\\r\\nTADPOLE\\r\\n\\r\\na\\r\\n\\r\\nb\\r\\n\\r\\nModel\\r\\n\\r\\nACC\\r\\n\\r\\nAUC\\r\\n\\r\\nMLP\\r\\nGCN\\r\\nProposed\\r\\n\\r\\n91.14 ± 02.62 97.77 ± 01.59 67.25 ± 01.11 72.69 ± 00.97\\r\\n74.27 ± 06.41 89.89 ± 04.12 68.74 ± 01.50 72.64 ± 01.10\\r\\n92.59 ± 03.64 96.96 ± 02.23 70.29 ± 01.10 76.17 ± 01.02\\r\\n\\r\\nMLP SC\\r\\n54.20 ± 08.74\\r\\nMLP FT\\r\\n71.27 ± 09.76\\r\\nProposed SC 59.42 ± 08.40\\r\\nProposed FT 78.89 ± 02.45\\r\\nRMSE\\r\\n\\r\\nc\\r\\n\\r\\n4\\r\\n\\r\\nMIMIC-III\\r\\n\\r\\nMLP\\r\\nProposed\\r\\n\\r\\n00.15 ± 0.008\\r\\n0.14 ± 0.006\\r\\n\\r\\nACC\\r\\n\\r\\nAUC\\r\\n\\r\\n70.41 ± 11.41\\r\\n89.25 ± 06.53\\r\\n68.72 ± 12.74\\r\\n93.49 ± 02.07\\r\\n\\r\\n63.78 ± 00.74\\r\\n64.71 ± 00.84\\r\\n64.72 ± 00.45\\r\\n69.42 ± 01.23\\r\\n\\r\\nACC\\r\\n\\r\\nRMSE\\r\\n\\r\\n62.58 ± 04.87 00.79 ± 0.023\\r\\n63.23 ± 04.25 0.78 ± 0.011\\r\\n\\r\\n67.72 ± 00.68\\r\\n67.94 ± 01.20\\r\\n68.97 ± 00.66\\r\\n75.09 ± 01.29\\r\\nF1\\r\\n81.49 ± 00.35\\r\\n81.58 ± 00.41\\r\\n\\r\\nConclusion\\r\\n\\r\\nIn this paper, we present an unsupervised pre-training method based on masked\\r\\nimputation, significantly improving prediction results. We propose a graph transformer based architecture for learning on population graphs built from heterogeneous EHR data. We show the superiority of our pipeline in both pre-training\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n9\\r\\n\\r\\nand various prediction tasks for two datasets, TADPOLE and MIMIC-III. Pretraining helps for all dataset sizes but especially in scenarios where only a limited\\r\\namount of labeled data is used for fine-tuning. Our pre-training method is unsupervised and therefore independent from the end task, and further it is well\\r\\nsuited for transfer learning. This work opens the path for the community to deals\\r\\nwith small dataset specially with limited labels.\\r\\n\\r\\nReferences\\r\\n1. Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv\\r\\npreprint arXiv:2106.08254 (2021)\\r\\n2. Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., Rueckert, D.: Selfsupervised learning for medical image analysis using image context restoration.\\r\\nMedical image analysis 58, 101539 (2019)\\r\\n3. Cosmo, L., Kazi, A., Ahmadi, S.A., Navab, N., Bronstein, M.: Latent-graph learning for disease prediction. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer (2020)\\r\\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: Burstein, J., Doran, C., Solorio,\\r\\nT. (eds.) NAACL-HLT (1). Association for Computational Linguistics (2019)\\r\\n5. Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., Leskovec, J.: Strategies\\r\\nfor pre-training graph neural networks. arXiv preprint arXiv:1905.12265 (2019)\\r\\n6. Hu, Z., Dong, Y., Wang, K., Chang, K.W., Sun, Y.: Gpt-gnn: Generative pretraining of graph neural networks. In: Proceedings of the 26th ACM SIGKDD\\r\\nInternational Conference on Knowledge Discovery & Data Mining (2020)\\r\\n7. Johnson, A.E., Pollard, T.J., Shen, L., Lehman, L.w.H., Feng, M., Ghassemi, M.,\\r\\nMoody, B., Szolovits, P., Anthony Celi, L., Mark, R.G.: Mimic-iii, a freely accessible\\r\\ncritical care database. Scientific data 3(1) (2016)\\r\\n8. Kazi, A., Farghadani, S., Navab, N.: Ia-gcn: Interpretable attention based graph\\r\\nconvolutional network for disease prediction. arXiv preprint arXiv:2103.15587\\r\\n(2021)\\r\\n9. Kazi, A., Shekarforoush, S., Kortuem, K., Albarqouni, S., Navab, N., et al.: Selfattention equipped graph convolutions for disease prediction. In: 2019 IEEE 16th\\r\\nInternational Symposium on Biomedical Imaging (ISBI 2019). IEEE (2019)\\r\\n10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\\r\\narXiv:1412.6980 (2014)\\r\\n11. Li, Y., Rao, S., Solares, J.R.A., Hassaine, A., Ramakrishnan, R., Canoy, D., Zhu,\\r\\nY., Rahimi, K., Salimi-Khorshidi, G.: Behrt: transformer for electronic health\\r\\nrecords. Scientific reports 10(1) (2020)\\r\\n12. Lu, Y., Jiang, X., Fang, Y., Shi, C.: Learning to pre-train graph neural networks.\\r\\nAAAI (2021)\\r\\n13. Marinescu, R.V., Oxtoby, N.P., Young, A.L., Bron, E.E., Toga, A.W., Weiner,\\r\\nM.W., Barkhof, F., Fox, N.C., Klein, S., Alexander, D.C., et al.: Tadpole challenge: prediction of longitudinal evolution in alzheimer’s disease. arXiv preprint\\r\\narXiv:1805.03909 (2018)\\r\\n14. McDermott, M., Nestor, B., Kim, E., Zhang, W., Goldenberg, A., Szolovits, P.,\\r\\nGhassemi, M.: A comprehensive ehr timeseries pre-training benchmark. In: Proceedings of the Conference on Health, Inference, and Learning (2021)\\r\\n\\r\\n\\x0c\\n\\n10\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\n15. Mitani, A.A., Haneuse, S.: Small data challenges of studying rare diseases. JAMA\\r\\nnetwork open 3(3) (2020)\\r\\n16. Ouyang, C., Biffi, C., Chen, C., Kart, T., Qiu, H., Rueckert, D.: Self-supervision\\r\\nwith superpixels: Training few-shot medical image segmentation without annotation. In: European Conference on Computer Vision. pp. 762–780. Springer (2020)\\r\\n17. Parisot, S., Ktena, S.I., Ferrante, E., Lee, M., Guerrero, R., Glocker, B., Rueckert,\\r\\nD.: Disease prediction using graph convolutional networks: application to autism\\r\\nspectrum disorder and alzheimer’s disease. Medical image analysis 48 (2018)\\r\\n18. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders: Feature learning by inpainting. In: Proceedings of the IEEE conference on\\r\\ncomputer vision and pattern recognition (2016)\\r\\n19. Pollard, T.J., Johnson, A.E., Raffa, J.D., Celi, L.A., Mark, R.G., Badawi, O.: The\\r\\neicu collaborative research database, a freely available multi-center database for\\r\\ncritical care research. Scientific data 5(1) (2018)\\r\\n20. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training (2018)\\r\\n21. Rasmy, L., Xiang, Y., Xie, Z., Tao, C., Zhi, D.: Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease\\r\\nprediction. NPJ digital medicine 4(1) (2021)\\r\\n22. Rong, Y., Bian, Y., Xu, T., Xie, W., Wei, Y., Huang, W., Huang, J.: Grover: Selfsupervised message passing transformer on large-scale molecular data. Advances\\r\\nin Neural Information Processing Systems (2020)\\r\\n23. Shang, J., Ma, T., Xiao, C., Sun, J.: Pre-training of graph augmented transformers\\r\\nfor medication recommendation. arXiv preprint arXiv:1906.00346 (2019)\\r\\n24. Wang, S., McDermott, M.B., Chauhan, G., Ghassemi, M., Hughes, M.C., Naumann, T.: Mimic-extract: A data extraction, preprocessing, and representation\\r\\npipeline for mimic-iii. In: Proceedings of the ACM conference on health, inference,\\r\\nand learning (2020)\\r\\n25. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., Liu, T.Y.: Do\\r\\ntransformers really perform badly for graph representation? Advances in Neural\\r\\nInformation Processing Systems 34 (2021)\\r\\n26. Zebin, T., Rezvy, S., Chaussalet, T.J.: A deep learning approach for length of\\r\\nstay prediction in clinical settings from medical records. In: 2019 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology\\r\\n(CIBCB). IEEE (2019)\\r\\n27. Zhang, J., Zhang, H., Xia, C., Sun, L.: Graph-bert: Only attention is needed for\\r\\nlearning graph representations. arXiv preprint arXiv:2001.05140 (2020)\\r\\n\\r\\n\\x0c\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with one pdf first\n",
    "mypdftext=array_pdf_text[0]\n",
    "mypdftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a99fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "1. Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv\r\n",
      "preprint arXiv:2106.08254 (2021)\r\n",
      "2. Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., Rueckert, D.: Selfsupervised learning for medical image analysis using image context restoration.\r\n",
      "Medical image analysis 58, 101539 (2019)\r\n",
      "3. Cosmo, L., Kazi, A., Ahmadi, S.A., Navab, N., Bronstein, M.: Latent-graph learning for disease prediction. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer (2020)\r\n",
      "4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: Burstein, J., Doran, C., Solorio,\r\n",
      "T. (eds.) NAACL-HLT (1). Association for Computational Linguistics (2019)\r\n",
      "5. Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., Leskovec, J.: Strategies\r\n",
      "for pre-training graph neural networks. arXiv preprint arXiv:1905.12265 (2019)\r\n",
      "6. Hu, Z., Dong, Y., Wang, K., Chang, K.W., Sun, Y.: Gpt-gnn: Generative pretraining of graph neural networks. In: Proceedings of the 26th ACM SIGKDD\r\n",
      "International Conference on Knowledge Discovery & Data Mining (2020)\r\n",
      "7. Johnson, A.E., Pollard, T.J., Shen, L., Lehman, L.w.H., Feng, M., Ghassemi, M.,\r\n",
      "Moody, B., Szolovits, P., Anthony Celi, L., Mark, R.G.: Mimic-iii, a freely accessible\r\n",
      "critical care database. Scientific data 3(1) (2016)\r\n",
      "8. Kazi, A., Farghadani, S., Navab, N.: Ia-gcn: Interpretable attention based graph\r\n",
      "convolutional network for disease prediction. arXiv preprint arXiv:2103.15587\r\n",
      "(2021)\r\n",
      "9. Kazi, A., Shekarforoush, S., Kortuem, K., Albarqouni, S., Navab, N., et al.: Selfattention equipped graph convolutions for disease prediction. In: 2019 IEEE 16th\r\n",
      "International Symposium on Biomedical Imaging (ISBI 2019). IEEE (2019)\r\n",
      "10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\r\n",
      "arXiv:1412.6980 (2014)\r\n",
      "11. Li, Y., Rao, S., Solares, J.R.A., Hassaine, A., Ramakrishnan, R., Canoy, D., Zhu,\r\n",
      "Y., Rahimi, K., Salimi-Khorshidi, G.: Behrt: transformer for electronic health\r\n",
      "records. Scientific reports 10(1) (2020)\r\n",
      "12. Lu, Y., Jiang, X., Fang, Y., Shi, C.: Learning to pre-train graph neural networks.\r\n",
      "AAAI (2021)\r\n",
      "13. Marinescu, R.V., Oxtoby, N.P., Young, A.L., Bron, E.E., Toga, A.W., Weiner,\r\n",
      "M.W., Barkhof, F., Fox, N.C., Klein, S., Alexander, D.C., et al.: Tadpole challenge: prediction of longitudinal evolution in alzheimer’s disease. arXiv preprint\r\n",
      "arXiv:1805.03909 (2018)\r\n",
      "14. McDermott, M., Nestor, B., Kim, E., Zhang, W., Goldenberg, A., Szolovits, P.,\r\n",
      "Ghassemi, M.: A comprehensive ehr timeseries pre-training benchmark. In: Proceedings of the Conference on Health, Inference, and Learning (2021)\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "10\r\n",
      "\r\n",
      "Chantal Pellegrini, Anees Kazi, Nassir Navab\r\n",
      "\r\n",
      "15. Mitani, A.A., Haneuse, S.: Small data challenges of studying rare diseases. JAMA\r\n",
      "network open 3(3) (2020)\r\n",
      "16. Ouyang, C., Biffi, C., Chen, C., Kart, T., Qiu, H., Rueckert, D.: Self-supervision\r\n",
      "with superpixels: Training few-shot medical image segmentation without annotation. In: European Conference on Computer Vision. pp. 762–780. Springer (2020)\r\n",
      "17. Parisot, S., Ktena, S.I., Ferrante, E., Lee, M., Guerrero, R., Glocker, B., Rueckert,\r\n",
      "D.: Disease prediction using graph convolutional networks: application to autism\r\n",
      "spectrum disorder and alzheimer’s disease. Medical image analysis 48 (2018)\r\n",
      "18. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders: Feature learning by inpainting. In: Proceedings of the IEEE conference on\r\n",
      "computer vision and pattern recognition (2016)\r\n",
      "19. Pollard, T.J., Johnson, A.E., Raffa, J.D., Celi, L.A., Mark, R.G., Badawi, O.: The\r\n",
      "eicu collaborative research database, a freely available multi-center database for\r\n",
      "critical care research. Scientific data 5(1) (2018)\r\n",
      "20. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training (2018)\r\n",
      "21. Rasmy, L., Xiang, Y., Xie, Z., Tao, C., Zhi, D.: Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease\r\n",
      "prediction. NPJ digital medicine 4(1) (2021)\r\n",
      "22. Rong, Y., Bian, Y., Xu, T., Xie, W., Wei, Y., Huang, W., Huang, J.: Grover: Selfsupervised message passing transformer on large-scale molecular data. Advances\r\n",
      "in Neural Information Processing Systems (2020)\r\n",
      "23. Shang, J., Ma, T., Xiao, C., Sun, J.: Pre-training of graph augmented transformers\r\n",
      "for medication recommendation. arXiv preprint arXiv:1906.00346 (2019)\r\n",
      "24. Wang, S., McDermott, M.B., Chauhan, G., Ghassemi, M., Hughes, M.C., Naumann, T.: Mimic-extract: A data extraction, preprocessing, and representation\r\n",
      "pipeline for mimic-iii. In: Proceedings of the ACM conference on health, inference,\r\n",
      "and learning (2020)\r\n",
      "25. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., Liu, T.Y.: Do\r\n",
      "transformers really perform badly for graph representation? Advances in Neural\r\n",
      "Information Processing Systems 34 (2021)\r\n",
      "26. Zebin, T., Rezvy, S., Chaussalet, T.J.: A deep learning approach for length of\r\n",
      "stay prediction in clinical settings from medical records. In: 2019 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology\r\n",
      "(CIBCB). IEEE (2019)\r\n",
      "27. Zhang, J., Zhang, H., Xia, C., Sun, L.: Graph-bert: Only attention is needed for\r\n",
      "learning graph representations. arXiv preprint arXiv:2001.05140 (2020)\r\n",
      "\r\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#First filter to have only the References from a pdf\n",
    "\n",
    "def after_references(mypdftext): \n",
    "    keyword1 = 'References'\n",
    "    keyword2 = 'REFERENCES'\n",
    "    keyword3 = 'R EFERENCES'\n",
    "    keyword4 = 'Reference'\n",
    "    keyword5='[1]' \n",
    "\n",
    "    if keyword1 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword1)\n",
    "    elif keyword2 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword2)\n",
    "    elif keyword3 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword3)\n",
    "    elif keyword4 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword4)\n",
    "    elif keyword5 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword5)\n",
    "    else:\n",
    "        after_keyword = mypdftext[:10000]\n",
    "    return after_keyword\n",
    "\n",
    "#All references in a variable\n",
    "\n",
    "references=after_references(mypdftext)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439d489",
   "metadata": {},
   "source": [
    "## Preprocess to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e325730d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First cleaning\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns): \n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s) \n",
    "        return s\n",
    "\n",
    "replacer=RegexpReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e46e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess a text before using IA\n",
    "\n",
    "def preprocess_text(test):\n",
    "\n",
    "    #Removing Numbers\n",
    "    test=re.sub(r'\\d+','',test)\n",
    "\n",
    "    #Removing Letter alone\n",
    "    test = re.sub(r'\\b\\w\\b', ' ', test)\n",
    "    \n",
    "#     #Keep only letter and number\n",
    "#     test=re.sub(\"[^A-Za-z0-9]\",\" \",test)\n",
    "\n",
    "    #Removing white spaces\n",
    "    test=test.strip()\n",
    "    \n",
    "    #Replacer replace\n",
    "    text_replaced = replacer.replace(test)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text_replaced)\n",
    "\n",
    "    #Tokenize words\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = tokenizer.tokenize(sentences[i])\n",
    "\n",
    "    #Remove stop words\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stops=set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [word for word in sentences[i] if word not in stops]\n",
    "\n",
    "    #Lemmatize\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer_output=WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            sentences[i][j] = lemmatizer_output.lemmatize(sentences[i][j])\n",
    "\n",
    "\n",
    "    #Join the words back into a sentence.\n",
    "    a=[' '.join(s) for s in sentences]\n",
    "    b=', '.join(a)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c077ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Bao Dong Wei, Beit Bert pre training image transformer, arXiv preprint arXiv, , Chen Bentley Mori Misawa Fujiwara Rueckert, Selfsupervised learning medical image analysis using image context restoration, Medical image analysis, Cosmo Kazi Ahmadi, Navab Bronstein, Latent graph learning disease prediction, In International Conference Medical Image Computing Computer Assisted Intervention, Springer, Devlin Chang, Lee Toutanova, Bert Pre training deep bidirectional transformer language understanding, In Burstein Doran Solorio, ed, NAACL HLT, Association Computational Linguistics, Hu Liu Gomes Zitnik Liang Pande Leskovec, Strategies pre training graph neural network, arXiv preprint arXiv, , Hu Dong Wang Chang, Sun, Gpt gnn Generative pretraining graph neural network, In Proceedings th ACM SIGKDD International Conference Knowledge Discovery Data Mining, Johnson, Pollard, Shen Lehman, , Feng Ghassemi Moody Szolovits Anthony Celi Mark, , Mimic iii freely accessible critical care database, Scientific data, Kazi Farghadani Navab, Ia gcn Interpretable attention based graph convolutional network disease prediction, arXiv preprint arXiv, , Kazi Shekarforoush Kortuem Albarqouni Navab et al, Selfattention equipped graph convolution disease prediction, In IEEE th International Symposium Biomedical Imaging ISBI, IEEE, Kingma, Ba, Adam method stochastic optimization, arXiv preprint arXiv, , Li Rao Solares, , Hassaine Ramakrishnan Canoy Zhu Rahimi Salimi Khorshidi, Behrt transformer electronic health record, Scientific report, Lu Jiang Fang Shi, Learning pre train graph neural network, AAAI, Marinescu, Oxtoby, Young, Bron, Toga, Weiner, Barkhof Fox, Klein Alexander, et al, Tadpole challenge prediction longitudinal evolution alzheimer disease, arXiv preprint arXiv, , McDermott Nestor Kim Zhang Goldenberg Szolovits Ghassemi, comprehensive ehr timeseries pre training benchmark, In Proceedings Conference Health Inference Learning Chantal Pellegrini Anees Kazi Nassir Navab, Mitani, Haneuse, Small data challenge studying rare disease, JAMA network open, Ouyang Biffi Chen Kart Qiu Rueckert, Self supervision superpixels Training shot medical image segmentation without annotation, In European Conference Computer Vision, pp, , Springer, Parisot Ktena, Ferrante Lee Guerrero Glocker Rueckert, Disease prediction using graph convolutional network application autism spectrum disorder alzheimer disease, Medical image analysis, Pathak Krahenbuhl Donahue Darrell Efros, , Context encoders Feature learning inpainting, In Proceedings IEEE conference computer vision pattern recognition, Pollard, Johnson, Raffa, Celi, Mark, Badawi, The eicu collaborative research database freely available multi center database critical care research, Scientific data, Radford Narasimhan Salimans Sutskever, Improving language understanding generative pre training, Rasmy Xiang Xie Tao Zhi, Med bert pretrained contextualized embeddings large scale structured electronic health record disease prediction, NPJ digital medicine, Rong Bian Xu Xie Wei Huang Huang, Grover Selfsupervised message passing transformer large scale molecular data, Advances Neural Information Processing Systems, Shang Ma Xiao Sun, Pre training graph augmented transformer medication recommendation, arXiv preprint arXiv, , Wang McDermott, Chauhan Ghassemi Hughes, Naumann, Mimic extract data extraction preprocessing representation pipeline mimic iii, In Proceedings ACM conference health inference learning, Ying Cai Luo Zheng Ke He Shen Liu, , Do transformer really perform badly graph representation, Advances Neural Information Processing Systems, Zebin Rezvy Chaussalet, , deep learning approach length stay prediction clinical setting medical record, In IEEE Conference Computational Intelligence Bioinformatics Computational Biology CIBCB, IEEE, Zhang Zhang Xia Sun, Graph bert Only attention needed learning graph representation, arXiv preprint arXiv, \n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "\n",
    "references_clean= preprocess_text(references)\n",
    "print(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5af5e9",
   "metadata": {},
   "source": [
    "## Get_human_names Algorithme using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa296d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    \n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    for person in person_list:\n",
    "        person_split = person.split(\" \")\n",
    "        for name in person_split:\n",
    "            if wordnet.synsets(name):\n",
    "                if(name in person):\n",
    "                    person_names.remove(person)\n",
    "                    break\n",
    "    return person_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864d30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 23 and without 22\n",
      "Ex: Beit Bert\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "print(\"Len with preprocess\",len(get_human_names(references_clean)), \"and without\",len(get_human_names(references)))\n",
    "print(\"Ex:\",get_human_names(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8dd9ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beit Bert',\n",
       " 'Cosmo Kazi Ahmadi',\n",
       " 'Navab Bronstein',\n",
       " 'Bert Pre',\n",
       " 'Hu Liu Gomes Zitnik Liang Pande Leskovec',\n",
       " 'Shen Lehman',\n",
       " 'Kazi Farghadani Navab',\n",
       " 'Kazi Shekarforoush Kortuem Albarqouni Navab',\n",
       " 'Hassaine Ramakrishnan Canoy',\n",
       " 'Zhu Rahimi Salimi Khorshidi',\n",
       " 'Kim Zhang Goldenberg Szolovits Ghassemi',\n",
       " 'Anees Kazi Nassir Navab',\n",
       " 'Parisot Ktena',\n",
       " 'Pathak Krahenbuhl Donahue Darrell Efros',\n",
       " 'Radford Narasimhan Salimans Sutskever',\n",
       " 'Wang McDermott',\n",
       " 'Shen Liu',\n",
       " 'Zebin Rezvy Chaussalet',\n",
       " 'Anees Kazi',\n",
       " 'Nassir Navab',\n",
       " 'Lee Toutanova',\n",
       " 'Lu Jiang Fang Shi',\n",
       " 'Ferrante Lee Guerrero Glocker Rueckert',\n",
       " 'Chauhan Ghassemi Hughes',\n",
       " 'Bao Dong Wei',\n",
       " 'Hu Dong Wang Chang',\n",
       " 'Barkhof Fox',\n",
       " 'Rasmy Xiang Xie Tao Zhi',\n",
       " 'Shang Ma Xiao Sun']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_human_names(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fcb76",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "656d73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "def nlp_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    PROPN=[token.lemma_ for token in doc if token.pos_ == \"PROPN\"]\n",
    "    \n",
    "#     Remove duplicate\n",
    "    PROPN = list(dict.fromkeys(PROPN))\n",
    "#     Remove word with first letter as lowercase \n",
    "    for word in PROPN:\n",
    "        if word[0].islower():\n",
    "            PROPN.remove(word)\n",
    "    return PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4230cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n"
     ]
    }
   ],
   "source": [
    "final_names_prep_spacy=nlp_entities(references_clean)\n",
    "final_names_spacy=nlp_entities(references)\n",
    "print(len(final_names_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d123b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An other function more accurate\n",
    "\n",
    "def extract(text:str) :\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    doc = spacy_nlp(text.strip())\n",
    "    named_entities = []\n",
    "    \n",
    "    for i in doc.ents:\n",
    "        entry = str(i.lemma_).lower()\n",
    "        text = text.replace(str(i).lower(), \"\")\n",
    "        if i.label_ in [\"ART\", \"EVE\", \"NAT\", \"PERSON\"]:\n",
    "            named_entities.append(entry.title().replace(\" \", \"_\").replace(\"\\n\",\"_\"))\n",
    "        named_entities = list(dict.fromkeys(named_entities))\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f8b14fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 47 and without 29\n",
      "Ex: Bao_Dong_Wei\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "print(\"Len with preprocess\",len(extract(references_clean)), \"and without\",len(extract(references)))\n",
    "print(\"Ex:\",extract(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "167b9906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bao_Dong_Wei',\n",
       " 'Chen_Bentley_Mori',\n",
       " 'Fujiwara_Rueckert',\n",
       " 'Cosmo_Kazi_Ahmadi',\n",
       " 'Navab_Bronstein',\n",
       " 'Devlin_Chang',\n",
       " 'Lee_Toutanova',\n",
       " 'Bert_Pre',\n",
       " 'Doran_Solorio',\n",
       " 'Hu_Liu',\n",
       " 'Hu_Dong_Wang_Chang',\n",
       " 'Shen_Lehman',\n",
       " 'Feng_Ghassemi_Moody',\n",
       " 'Anthony_Celi_Mark',\n",
       " 'Kazi_Farghadani_Navab',\n",
       " 'Kazi',\n",
       " 'Kortuem_Albarqouni',\n",
       " 'Ba',\n",
       " 'Li_Rao_Solares',\n",
       " 'Ramakrishnan_Canoy',\n",
       " 'Zhu_Rahimi_Salimi_Khorshidi',\n",
       " 'Lu_Jiang_Fang_Shi',\n",
       " 'Marinescu',\n",
       " 'Barkhof_Fox',\n",
       " 'Klein_Alexander',\n",
       " 'Kim_Zhang_Goldenberg',\n",
       " 'Nassir_Navab',\n",
       " 'Ouyang_Biffi_Chen_Kart',\n",
       " 'Qiu_Rueckert',\n",
       " 'Springer',\n",
       " 'Parisot_Ktena',\n",
       " 'Ferrante_Lee_Guerrero_Glocker_Rueckert',\n",
       " 'Pathak_Krahenbuhl',\n",
       " 'Darrell_Efros',\n",
       " 'Badawi',\n",
       " 'Rasmy_Xiang_Xie_Tao_Zhi',\n",
       " 'Rong_Bian',\n",
       " 'Wei_Huang_Huang',\n",
       " 'Grover_Selfsupervised',\n",
       " 'Shang_Ma_Xiao_Sun',\n",
       " 'Wang_Mcdermott',\n",
       " 'Ghassemi_Hughes',\n",
       " 'Ying_Cai_Luo_Zheng_Ke',\n",
       " 'Shen_Liu',\n",
       " 'Zebin_Rezvy_Chaussalet',\n",
       " 'Zhang_Zhang_Xia_Sun',\n",
       " 'Graph']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7a418",
   "metadata": {},
   "source": [
    "### NLTK - StanfordNERTagger (quite long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15b08f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('You', 'O'), ('can', 'O'), ('call', 'O'), ('me', 'O'), ('Billiy', 'PERSON'), ('Bubu', 'PERSON'), ('and', 'O'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('Amsterdam.', 'LOCATION')]\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "java_path = \"C:/Program Files/Java/jdk-17.0.1/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "st = StanfordNERTagger('stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner-2020-11-17/stanford-ner.jar')\n",
    "\n",
    "st = StanfordNERTagger('stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner-2020-11-17/stanford-ner.jar')\n",
    "print (st.tag('You can call me Billiy Bubu and I live in Amsterdam.'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee89f9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Stanford_array=[]\n",
    "def StanfordNER(text):\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        tokens = nltk.tokenize.word_tokenize(sent)\n",
    "        tags = st.tag(tokens)\n",
    "        for tag in tags:\n",
    "            if tag[1]=='PERSON': \n",
    "                Stanford_array.append(tag[0])\n",
    "    return Stanford_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82c14844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 157\n",
      "Ex: Bao\n"
     ]
    }
   ],
   "source": [
    "#With preprocess\n",
    "final_names_prep_Stanford=StanfordNER(references_clean)\n",
    "print(\"Len with preprocess\",len(final_names_prep_Stanford))\n",
    "print(\"Ex:\",final_names_prep_Stanford[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dbd17",
   "metadata": {},
   "source": [
    "## TextBlob : TextBlob est une bibliothèque python et propose une API simple pour accéder à ses méthodes et effectuer des tâches NLP de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86155dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_names_TextBlob= TextBlob(references_clean)\n",
    "PROPN=[words for words, tag in final_names_TextBlob.tags if tag == \"NNP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cc696d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 251\n",
      "Ex: Bao\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess\n",
    "final_names_TextBlob=PROPN\n",
    "#With preprocess\n",
    "print(\"Len with preprocess\",len(final_names_TextBlob))\n",
    "print(\"Ex:\",final_names_TextBlob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60bbf0",
   "metadata": {},
   "source": [
    "Also efficence but not useful here because spacy hasa better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb67165",
   "metadata": {},
   "source": [
    "## Apply Spacy functions on all pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b585bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b\"Unsupervised Pre-Training on Patient Population\\r\\nGraphs for Patient-Level Predictions\\r\\nChantal Pellegrini1 , Anees Kazi1 , and Nassir Navab1,2\\r\\nComputer Aided Medical Procedures, Technical University Munich, Germany\\r\\nComputer Aided Medical Procedures, Johns Hopkins University, Baltimore, USA\\r\\n\\r\\n1\\r\\n\\r\\narXiv:2203.12616v1 [cs.LG] 23 Mar 2022\\r\\n\\r\\n2\\r\\n\\r\\nAbstract. Pre-training has shown success in different areas of machine\\r\\nlearning, such as Computer Vision (CV), Natural Language Processing\\r\\n(NLP) and medical imaging. However, it has not been fully explored\\r\\nfor clinical data analysis. Even though an immense amount of Electronic Health Record (EHR) data is recorded, data and labels can be\\r\\nscarce if the data is collected in small hospitals or deals with rare diseases. In such scenarios, pre-training on a larger set of EHR data could\\r\\nimprove the model performance. In this paper, we apply unsupervised\\r\\npre-training to heterogeneous, multi-modal EHR data for patient outcome prediction. To model this data, we leverage graph deep learning\\r\\nover population graphs. We first design a network architecture based on\\r\\ngraph transformer designed to handle various input feature types occurring in EHR data, like continuous, discrete, and time-series features,\\r\\nallowing better multi-modal data fusion. Further, we design pre-training\\r\\nmethods based on masked imputation to pre-train our network before\\r\\nfine-tuning on different end tasks. Pre-training is done in a fully unsupervised fashion, which lays the groundwork for pre-training on large\\r\\npublic datasets with different tasks and similar modalities in the future.\\r\\nWe test our method on two medical datasets of patient records, TADPOLE and MIMIC-III, including imaging and non-imaging features and\\r\\ndifferent prediction tasks. We find that our proposed graph based pretraining method helps in modeling the data at a population level and\\r\\nfurther improves performance on the fine tuning tasks in terms of AUC\\r\\non average by 4.15% for MIMIC and 7.64% for TADPOLE.\\r\\nKeywords: Outcome/Disease Prediction \\xc2\\xb7 Population Graphs \\xc2\\xb7 PreTraining.\\r\\n\\r\\n1\\r\\n\\r\\nIntroduction\\r\\n\\r\\nEnormous amounts of data are collected on a daily basis in hospitals. Nevertheless, labeled data can be scarce, as labeling can be tedious, time-consuming,\\r\\nand expensive. Further, in small hospitals and for rare diseases, only little data\\r\\nis accumulated [15]. The ability to leverage the large body of unlabeled medical\\r\\ndata to improve in prediction tasks over such small labeled datasets could increase the confidence of AI models for clinical outcome prediction. Unsupervised\\r\\npre-training was shown to be useful to exploit unlabeled data in NLP [20,4],\\r\\n\\r\\n\\x0c\\n\\n2\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\nCV [18,1] and medical imaging [16,2]. However, for more complex clinical data,\\r\\nit is not explored enough. Some works study how to pre-train BERT [4] over\\r\\nEHR data of medical diagnostic codes. They pre-train with modified masked\\r\\nlanguage modeling and, in one case, a supervised prediction task. The targeted\\r\\ndownstream tasks lie in disease and medication code prediction [23,11,21]. McDermott et al. [14] propose a pre-training approach over heterogeneous EHR\\r\\ndata, including e.g. continuous lab results. They create a benchmark with several downstream tasks over the eICU [19] and MIMIC-III [7] datasets and present\\r\\ntwo baseline pre-training methods. Pre-training and fine-tuning are performed\\r\\nover single EHRs from ICU stays with a Gated Recurrent Unit.\\r\\nOn the other hand, population graphs have been leveraged in the recent literature to help analyze patient data using the relationships among patients, leading\\r\\nto clinically semantic modeling of the data. During unsupervised pre-training,\\r\\ngraphs allow learning representations based on feature similarities between the\\r\\npatients, which can then help to improve patient-level predictions. Several works\\r\\nsuccessfully apply pre-training to graph data in different domains like molecular\\r\\ngraphs and on common graph benchmarks. Proposed pre-training strategies include node level tasks, like attribute reconstruction, graph level tasks like property prediction, or generative tasks such as edge prediction [5,22,27,6,12]. To\\r\\nthe best of our knowledge, no previous work applied pre-training to population\\r\\ngraphs.\\r\\nIn this paper, we propose a model capable of pre-training for understanding\\r\\npatient population data. We choose two medical applications in brain imaging\\r\\nand EHR data analysis on the public datasets TADPOLE [13] and MIMICIII [7] for Alzheimer\\xe2\\x80\\x99s disease prediction [17,9,3] and Length-of-Stay prediction\\r\\n[26,24,14]. The code is available at https://github.com/ChantalMP/UnsupervisedPre-Training-on-Patient-Population-Graphs-for-Patient-Level-Predictions.\\r\\nContribution: We develop an unsupervised pre-training method to learn a\\r\\ngeneral understanding of patient population data modeled as graph, providing a\\r\\nsolution to limited labeled data. We show significant performance gains through\\r\\npre-training when fine-tuning with as little as 1% and up to 100% labels. Further, we propose a (graph) transformer based model suitable for multi-modal\\r\\ndata fusion. It is designed to handle various EHR input types, taking static\\r\\nand time-series data and continuous as well as discrete numerical features into\\r\\naccount.\\r\\n\\r\\n2\\r\\n\\r\\nMethod\\r\\n\\r\\nLet D be a dataset composed of the EHR data of N patients. The ith record is\\r\\nrepresented by ri \\xe2\\x8a\\x86 [di , ci , ti ] with static discrete features di \\xe2\\x88\\x88 ND , continuous\\r\\nfeatures ci \\xe2\\x88\\x88 RC , and time-series features ti \\xe2\\x88\\x88 RS\\xc3\\x97\\xcf\\x84 , where \\xcf\\x84 denotes the length\\r\\nof the time-series. For every downstream task T , labels Y \\xe2\\x88\\x88 NL are given for L\\r\\nclasses. The task is to predict the classes for the test set patients given all features. Towards this task, we propose to use patient population graphs. Unlike in\\r\\nnon-graph-based methods, the model can exploit similarities between patients to\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n3\\r\\n\\r\\nbetter understand the EHR data at patient and population level. Further, unlike\\r\\nconventional graph neural networks, graph transformers allow flexible attention\\r\\nto all nodes, learning which patients are relevant for a task. This would be most\\r\\napt for learning over population EHR data. Our pipeline consists of two steps.\\r\\n1) Unsupervised pre-training and 2) Fine-tuning. Unsupervised pre-training enables understanding of general EHR data and disorder progression by training\\r\\nthe model for masked imputation task. This understanding can help learn downstream tasks better despite limited labeled data.\\r\\nGraph Construction For each node pair with records ri and rj , we calculate a\\r\\nsimilarity score S(ri , rj ) between the node features. We use L2 distance for continuous and absolute matching for discrete features. As graph construction is not\\r\\nour focus, we choose the most conventional method using k-NN selection rule.\\r\\nWe set k=5 to avoid having many disconnected components and very densely\\r\\nconnected regions (see supplementary material). A detailed description of the\\r\\ngraph construction per dataset follows in the experiment section.\\r\\nModel Architecture Our model consists of an encoder and decoder. The encoder comprises a data embedding module and a graph transformer module\\r\\nexplained later. We design the encoder to handle various input data types. The\\r\\ndecoder is a simple linear layer capable of capturing the essence of features inclined towards a node-level classification task. Figure 1 shows an overview of our\\r\\nmodel architecture.\\r\\nData embedding module: Following the conventional Graphormer [25], we proEncoder\\r\\n\\r\\nDecoder\\r\\nData Embedding Module\\r\\n\\r\\nstatic, discrete\\r\\nfeatures d\\r\\n\\r\\nEmbedding Layer\\r\\n\\r\\nstatic, continuous\\r\\nfeatures c\\r\\n\\r\\nLinear Layer\\r\\n\\r\\nC' x N\\r\\n\\r\\ntime-series\\r\\nfeatures t\\r\\n\\r\\nLinear Layer\\r\\n\\r\\nS' x \\xcf\\x84 x N\\r\\n\\r\\nsum\\r\\n\\r\\nD' x N\\r\\n\\r\\nconcat\\r\\n\\r\\n2x Transformer\\r\\nLayer\\r\\n\\r\\nQ\\r\\nSequence\\r\\n\\r\\nK\\r\\n\\r\\n+\\r\\n\\r\\nV\\r\\n\\r\\npositional\\r\\nencodings\\r\\n\\r\\nmean\\r\\n\\r\\nScale +\\r\\nNorm\\r\\n\\r\\nx\\r\\n\\r\\nLinear\\r\\nLayer\\r\\n\\r\\nmultiple\\r\\nGraphormer\\r\\nLayer\\r\\n\\r\\nLinear Task\\r\\nLayer\\r\\n\\r\\nLoss\\r\\n\\r\\nS' x N\\r\\n\\r\\nadd\\r\\n\\r\\nadd\\r\\nx\\r\\n\\r\\nFxN\\r\\n\\r\\nNorm\\r\\n\\r\\n2x Linear\\r\\nLayer\\r\\n\\r\\nNorm\\r\\n\\r\\nTransformer Layer\\r\\n\\r\\nFig. 1. Overview of the proposed architecture. All input features are combined into\\r\\none node embedding, applying transformer layers to enhance the time-series features.\\r\\nThe resulting graph is processed by several Graphormer layers and a linear task layer.\\r\\n\\r\\ncess discrete input features by an embedding layer, followed by a summation over\\r\\n0\\r\\nthe feature dimension, resulting in embedded features d0i \\xe2\\x88\\x88 RD , where D0 is the\\r\\noutput feature dimension. While Graphormer is limited to static, discrete input\\r\\nfeatures only, we improve upon Graphormer to support also static, continuous\\r\\ninput features, which are processed by a linear layer resulting in the embedding\\r\\n0\\r\\nvector c0i \\xe2\\x88\\x88 RC . The third branch of our data embedding module handles timeseries input features ti \\xe2\\x88\\x88 RS\\xc3\\x97\\xcf\\x84 with a linear layer, followed by two transformer\\r\\nlayers to deal with variable sequence lengths and allow the model to incorporate\\r\\n\\r\\n\\x0c\\n\\n4\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\ntemporal context. The output is given by t0i,h \\xe2\\x88\\x88 RE per time-step h. The mean\\r\\n0\\r\\nof these embeddings forms the final time-series embeddings t0i \\xe2\\x88\\x88 RS . The fea0\\r\\nture vectors d0i , c0i and\\r\\nP ti are concatenated to form the final node embeddings\\r\\nF\\r\\nni \\xe2\\x88\\x88 R , where F = Fk \\xe2\\x8a\\x82[D0 ,C 0 ,S 0 ] Fk , for each of the N nodes.\\r\\nGraphormer Module: The backbone of our model comprises multiple graph transformer layers [25]. Graphormer uses attention between all nodes in the graph.\\r\\nTo incorporate the graph structure, structural encodings are used, which encode\\r\\nin and out degrees of the nodes, the distance between nodes, and edge features.\\r\\nPre-training and Fine-Tuning: We propose an unsupervised pre-training\\r\\ntechnique on the same input features as for downstream tasks, but without using labels Y. Instead, we randomly mask a fixed percentage of feature values for\\r\\nevery record ri and optimize the model to predict these values. During masking,\\r\\nstatic features are replaced by a randomly selected fixed value called \\xe2\\x80\\x98mask token\\xe2\\x80\\x99. For time-series features, we add a binary column per feature to the input\\r\\nvector, showing masked hours, and replace the value with zero. We optimize the\\r\\nmodel using (binary) cross entropy loss for discrete and mean squared error loss\\r\\nfor continuous features. A model for fine-tuning is initialized using the encoder\\r\\nweights learned during pre-training and random weights for the decoder. Then\\r\\nthe model is fine-tuned for the task T .\\r\\n\\r\\n3\\r\\n\\r\\nExperiments and Results\\r\\n\\r\\nWe use two publicly available medical data sets: TADPOLE [13] and MIMICIII [7]. They differ in size, the targeted prediction task, and the type of input\\r\\nfeatures, allowing comprehensive testing and evaluation of our method.\\r\\n3.1\\r\\n\\r\\nDatasets description:\\r\\n\\r\\nTADPOLE [13] contains 564 patients from the Alzheimer\\xe2\\x80\\x99s Disease Neuroimaging Initiative (ADNI). We use twelve features, which the TADPOLE challenge\\r\\nclaims are informative. They include discrete cognitive test results, demographics, and continuous features extracted from MR and PET imaging, normalized\\r\\nbetween zero and one. The task is to classify the patients into the groups Cognitive Normal (CN), Mild Cognitive Impairment (MCI), or Alzheimer\\xe2\\x80\\x99s Disease\\r\\n(AD). We only use data from patients\\xe2\\x80\\x99 first visits to avoid leakage of information.\\r\\nGraph Construction: We construct a k-NN graph with k=5, dependent on the\\r\\nmean similarity (S) between\\r\\n( the features. For the demographics, age, gender and\\r\\nP 1 if fi = fj else 0\\r\\napoe4, Sdem (ri , rj ) =\\r\\n\\xc3\\xb7 3 where, f =(apoe4,\\r\\n1 if |agei \\xe2\\x88\\x92 agej | \\xe2\\x89\\xa4 2 else 0\\r\\ngender). For the cognitive test results di (ordinal features), and ci (continuous imaging features),\\r\\nwe calculate the respective normalized L2 distances:\\r\\nP\\r\\nP\\r\\nf \\xe2\\x88\\x88di ||fri \\xe2\\x88\\x92frj ||\\r\\nand Simg (ri , rj ) = sig( f \\xe2\\x88\\x88ci ||fri \\xe2\\x88\\x92 frj ||). The\\r\\nScog (ri , rj ) =\\r\\nmax(di )\\r\\noverall similarity S(ri , rj ) is then given as mean of Sdem , Scog and Simg .\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n5\\r\\n\\r\\nPre-Training Configuration: During pre-training on TADPOLE, we randomly\\r\\nmask 30% of the medical features (APOE4, cognitive tests, and imaging features) in each sample. The masking ratio of 30% was chosen experimentally.\\r\\nMIMIC-III [7] is a large EHR dataset of patient records with various static\\r\\nand time-series data collected over the patient\\xe2\\x80\\x99s stay. We use the pre-processed\\r\\ndataset published by McDermott et al. [14]. It includes 19.7 K patients that are\\r\\nat least 15 years old and stayed 24 hours or more in the ICU. The features include\\r\\ndemographics, measurements from bed-side monitoring and lab tests in hourly\\r\\ngranularity (continuous), and binary features stating if different treatments were\\r\\napplied in each hour. In total we have 76 features. We use linear interpolation\\r\\nto impute missing measurements. Fine-tuning is evaluated on Length-of-Stay\\r\\n(LOS) prediction as defined in [14]. The input encompasses the first 24 hours of\\r\\neach patient\\xe2\\x80\\x99s stay, and the goal is to predict if a patient will stay longer than\\r\\nthree days or not.\\r\\nGraph Construction: It is computationally infeasible to process a graph containing all patients. Thus, we create sub-graphs with 500 patients each, which\\r\\nfit into memory, each containing train, validation and test patients. We split\\r\\nrandomly as we do not want to make assumptions on which types of patients\\r\\nthe model should see, but learn this via the attention in the graph transformer.\\r\\nGiven the time-series of the measurement features f, we form feature descriptors\\r\\nfd = (mean(f ), std(f ), min(f ), max(f )) per patient and feature, where d equals\\r\\nthe 56 measurement features. We then compute the averagePsimilarity over all\\r\\n||fr \\xe2\\x88\\x92fr ||\\r\\n\\r\\nfeatures fd between two patients ri and rj : Sim(ri , rj ) = f \\xe2\\x88\\x88fd |fd |i j and\\r\\nbuild a k-NN graph with k=5.\\r\\nPre-Training Configuration: On MIMIC-III, we perform masking on the timeseries features from measurement and treatment data. Pre-training is performed\\r\\nover data from the first 24 hours of the patient\\xe2\\x80\\x99s stay. We compute the loss only\\r\\nover measured values, not over interpolated ones. Masking ratios are chosen\\r\\nexperimentally. We compare two types of masking:\\r\\nFeature Masking (FM): We randomly select 30% of the features per patient and mask the full 24 hours of the time-series. The model can not see past or\\r\\nfuture values, only other features and patients, aiming to force an understanding\\r\\nof relations between features and patients to infer masked features.\\r\\nBlock-wise Masking (BM): Instead of the full features, we mask a random\\r\\nblock of 6 hours within the 24-hour time-series in 100% of the features. Here,\\r\\nthe model can access past and future values to make a prediction. Thus, it can\\r\\nlearn to understand temporal context during pre-training.\\r\\n3.2\\r\\n\\r\\nExperimental Setup\\r\\n\\r\\nGiven a pre-trained model, we compare the results of fine-tuning it, with training the same but randomly initialized model from scratch. We manually tuned\\r\\nhyper-parameters per dataset separately for pre-training, from scratch training,\\r\\nand fine-tuning. To simulate scenarios with limited labeled data, we measure the\\r\\nmodel performance at different label ratios, meaning different amounts of labels\\r\\n\\r\\n\\x0c\\n\\n6\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\n(1%, 5%, 10%, 50%, 100%) are used for training or fine-tuning. For pre-training\\r\\nalways the full training data is used.\\r\\nImplementation Details All experiments are implemented in PyTorch, performed on a TITAN Xp GPU with 12GB VRAM, and optimized with the Adam\\r\\noptimizer [10]. For cross-validation, pre-training is performed separately per\\r\\nfold. The model comprises four Graphormer layers for TADPOLE and eight for\\r\\nMIMIC-III. For TADPOLE, we pre-train for 6000 epochs with a LR of 1e-5. We\\r\\ntrain task prediction for 1200 epochs with a polynomial decaying LR (1e-5 to 5e6) to train from scratch and a LR of 5e-6 for fine-tuning. When fine-tuning with\\r\\n1% labels, we reduce the epochs to 200. All results are computed with 10-fold\\r\\ncross-validation. For MIMIC-III, we pre-train for 3000 epochs with a polynomial\\r\\ndecaying LR (1e-3 to 1e-4). We train for 1100 epochs with a LR of 1e-4 from\\r\\nscratch, or fine-tune for 600 epochs with a LR of 1e-5. For a fair comparison with\\r\\nthe state of the art, results are averaged over six folds, each with an 80-10-10\\r\\nsplit into train, validation and test data. The models are selected based on the\\r\\nvalidation sets, and performance is computed over the test sets.\\r\\n3.3\\r\\n\\r\\nResults\\r\\n\\r\\nComparative methods: Table 1 We compare our model to related work without any pre-training. On TADPOLE, we compare to DGM [3], which proposes to\\r\\nlearn an optimal population graph for the given task. Besides, one recent arxiv\\r\\npaper [8] further improves performance on TADPOLE by learning input feature\\r\\nimportance. However it is out of context for this work. We achieve comparable accuracy to DGM and outperform in terms of AUC, which is an important\\r\\nmetric for imbalanced datasets. For MIMIC-III, we compare our method to the\\r\\nEHR pre-training benchmark of McDermott et al. [14], which uses the same LOS\\r\\ndefinition and dataset. We significantly outperform the benchmark model. The\\r\\nresults show that the proposed architecture is a good fit for the task at hand.\\r\\nTable 1. Accuracy and AUC of the proposed method compared with DGM on TADPOLE and McDermott et al. [14] on MIMIC-III.\\r\\nTADPOLE\\r\\nModel\\r\\n\\r\\nACC\\r\\n\\r\\nMIMIC-III\\r\\nAUC\\r\\n\\r\\nModel\\r\\n\\r\\nACC\\r\\n\\r\\nAUC\\r\\n\\r\\nDGM [3] 92.91 \\xc2\\xb1 02.50 94.49 \\xc2\\xb1 03.70 McDermott [14] not reported 71.00 \\xc2\\xb1 1.00\\r\\nProposed 92.59 \\xc2\\xb1 3.64 96.96 \\xc2\\xb1 2.32\\r\\nProposed\\r\\n70.29 \\xc2\\xb1 1.10 76.17 \\xc2\\xb1 1.02\\r\\n\\r\\nEffect of pre-training: Table 2 The motivation of this experiment is to investigate the smallest amount of labels required during the fine-tuning of the\\r\\ndownstream task. The results emphasize the benefits of our unsupervised pretraining with limited labels. On TADPOLE the main benefit of pre-training\\r\\ncan be seen for settings with limited labels (1%, 5%, 10%), where performance\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n7\\r\\n\\r\\nimproves significantly. Moreover, AUC continues to improve for all ratios. For\\r\\nLOS on MIMIC-III, both metrics significantly improve for all label ratios compared to from scratch training. Further, for MIMIC-III we compare two types of\\r\\nmasking (BM, FM). We see that feature masking consistently outperforms blockwise masking. The performance improvements achieved through pre-training on\\r\\nMIMIC-III are significantly higher than in the benchmark [14]. Moreover, we see\\r\\nimprovements until the full dataset size and not only for limited labels. Further\\r\\nthe pre-trained models have a lower standard deviation, indicating higher stability.\\r\\nAblation experiments: Table 3 We perform several ablation studies to evalTable 2. Performance of the proposed model in accuracy and AUC trained from scratch\\r\\n(SC) or fine-tuned after pre-training (FT) for different label ratios. For MIMIC-III we\\r\\nadditionally compare the block-wise (BM) and feature masking (FM) to each other.\\r\\nTADPOLE\\r\\nSize Metric\\r\\n\\r\\nSC\\r\\n\\r\\nMIMIC-III\\r\\nFT\\r\\n\\r\\nSC\\r\\n\\r\\nFT: BM\\r\\n\\r\\nFT: FM\\r\\n\\r\\n1%\\r\\n\\r\\nACC 59.42 \\xc2\\xb1 8.40 78.89 \\xc2\\xb1 2.45 59.86 \\xc2\\xb1 2.11 63.22 \\xc2\\xb1 2.39 65.25 \\xc2\\xb1 1.09\\r\\nAUC 68.72 \\xc2\\xb1 12.74 93.49 \\xc2\\xb1 2.07 62.98 \\xc2\\xb1 2.55 68.07 \\xc2\\xb1 1.80 69.90 \\xc2\\xb1 1.26\\r\\n\\r\\n5%\\r\\n\\r\\nACC\\r\\nAUC\\r\\n\\r\\n78.23 \\xc2\\xb1 6.83 83.37 \\xc2\\xb1 6.29 64.79 \\xc2\\xb1 1.16 66.82 \\xc2\\xb1 0.89 68.66 \\xc2\\xb1 0.73\\r\\n87.23 \\xc2\\xb1 4.91 94.99 \\xc2\\xb1 2.55 68.85 \\xc2\\xb1 1.53 72.27 \\xc2\\xb1 1.19 73.97 \\xc2\\xb1 1.28\\r\\n\\r\\n10% ACC\\r\\nAUC\\r\\n\\r\\n87.00 \\xc2\\xb1 4.86 87.71 \\xc2\\xb1 4.65 64.72 \\xc2\\xb1 0.45 67.71 \\xc2\\xb1 0.69 69.42 \\xc2\\xb1 1.23\\r\\n92.03 \\xc2\\xb1 3.39 95.96 \\xc2\\xb1 2.51 68.97 \\xc2\\xb1 0.66 73.55 \\xc2\\xb1 0.60 75.09 \\xc2\\xb1 1.29\\r\\n\\r\\n50% ACC 92.41 \\xc2\\xb1 3.69 91.52 \\xc2\\xb1 3.76 67.41 \\xc2\\xb1 1.31 69.98 \\xc2\\xb1 0.69 70.85 \\xc2\\xb1 0.92\\r\\nAUC 96.06 \\xc2\\xb1 2.48 97.23 \\xc2\\xb1 1.94 72.53 \\xc2\\xb1 1.08 76.02 \\xc2\\xb1 0.87 76.86 \\xc2\\xb1 1.47\\r\\n100% ACC 92.59 \\xc2\\xb1 3.64 92.24 \\xc2\\xb1 3.47 70.29 \\xc2\\xb1 1.10 70.73 \\xc2\\xb1 0.70 71.44 \\xc2\\xb1 1.25\\r\\nAUC 96.96 \\xc2\\xb1 2.23 97.52 \\xc2\\xb1 1.67 76.17 \\xc2\\xb1 1.02 76.20 \\xc2\\xb1 0.54 77.78 \\xc2\\xb1 1.31\\r\\n\\r\\nuate different parts of our proposed model on pre- and task training.\\r\\nEffect of Graphormer: We replace the Graphormer module with a simple MLP\\r\\nor GCN layer and train the model from scratch on the full dataset (Table 3 a)).\\r\\nWe see a clear benefit from using Graphormer compared to MLP and GCN. For\\r\\nTADPOLE, MLP reaches slightly better performance in terms of AUC as TADPOLE is a relatively small and easy dataset. The effect of the node level attention\\r\\nmechanism to all nodes given by Graphormer is clearly visible when compared\\r\\nto GCN. Further, we perform pre-training followed by fine-tuning for the MLP\\r\\nmodel (Table 3 b)). Our proposed unsupervised pre-training method proves to be\\r\\nbeneficial also for the MLP model, but the effects are less as for our proposed architecture. Table 3 c) shows masked imputation performance during pre-training,\\r\\nmeasured by RMSE for continuous (imaging/ measurements) and accuracy or\\r\\nF1 for discrete features (apoe4+cognitive tests/treatments). Here the proposed\\r\\nmodel outperforms the MLP model, explaining why pre-training has a greater\\r\\n\\r\\n\\x0c\\n\\n8\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\neffect for it. In summary we see a positive effect of using Graphormer over MLP\\r\\nfor solving the pre-training task and improving fine-tuning performance.\\r\\nEffect of Transformer: For MIMIC-III, Transformer is inserted in the encoder\\r\\nto deal with time series data. To test the transformer layers, we remove this\\r\\ncomponent and train the model from scratch on the full dataset, resulting in a\\r\\nreduction of accuracy from 70.29 to 69.39% and AUC from 76.17 to 75.03%. This\\r\\nshows that the transformer layers are helpful for processing time-series inputs.\\r\\nThe model needs to predict time-dependent outputs for pre-training on MIMICIII, for which the transformer layers are important, as they can understand the\\r\\ntemporal context. To investigate the effect of transformer during pre-training,\\r\\nwe remove the transformer layer and replace the Graphormer module with an\\r\\nMLP. We observe a reduction in the performance by 0.45% for ACC and 3.03% in\\r\\nAUC through pre-training. Accordingly, removing the transformer layer results\\r\\nin a 0.049 larger RMSE and a 3.9% lower F1 score in pre-training.\\r\\nTable 3. Ablations to test Graphormer module by replacing it with an MLP/GCN\\r\\nlayer, a) downstream task performance trained from scratch b) results of fine-tuning\\r\\n(FT) on limited labels (TADPOLE 1%, MIMIC-III 10%), compared to training from\\r\\nscratch (SC) c) pre-training task performance, multi-class accuracy for cognitive tests\\r\\nuses feature-dependent error margins in which predictions are considered correct. The\\r\\nsmall number of imaging features might cause the low std of 0.006/0.008.\\r\\nTADPOLE\\r\\n\\r\\na\\r\\n\\r\\nb\\r\\n\\r\\nModel\\r\\n\\r\\nACC\\r\\n\\r\\nAUC\\r\\n\\r\\nMLP\\r\\nGCN\\r\\nProposed\\r\\n\\r\\n91.14 \\xc2\\xb1 02.62 97.77 \\xc2\\xb1 01.59 67.25 \\xc2\\xb1 01.11 72.69 \\xc2\\xb1 00.97\\r\\n74.27 \\xc2\\xb1 06.41 89.89 \\xc2\\xb1 04.12 68.74 \\xc2\\xb1 01.50 72.64 \\xc2\\xb1 01.10\\r\\n92.59 \\xc2\\xb1 03.64 96.96 \\xc2\\xb1 02.23 70.29 \\xc2\\xb1 01.10 76.17 \\xc2\\xb1 01.02\\r\\n\\r\\nMLP SC\\r\\n54.20 \\xc2\\xb1 08.74\\r\\nMLP FT\\r\\n71.27 \\xc2\\xb1 09.76\\r\\nProposed SC 59.42 \\xc2\\xb1 08.40\\r\\nProposed FT 78.89 \\xc2\\xb1 02.45\\r\\nRMSE\\r\\n\\r\\nc\\r\\n\\r\\n4\\r\\n\\r\\nMIMIC-III\\r\\n\\r\\nMLP\\r\\nProposed\\r\\n\\r\\n00.15 \\xc2\\xb1 0.008\\r\\n0.14 \\xc2\\xb1 0.006\\r\\n\\r\\nACC\\r\\n\\r\\nAUC\\r\\n\\r\\n70.41 \\xc2\\xb1 11.41\\r\\n89.25 \\xc2\\xb1 06.53\\r\\n68.72 \\xc2\\xb1 12.74\\r\\n93.49 \\xc2\\xb1 02.07\\r\\n\\r\\n63.78 \\xc2\\xb1 00.74\\r\\n64.71 \\xc2\\xb1 00.84\\r\\n64.72 \\xc2\\xb1 00.45\\r\\n69.42 \\xc2\\xb1 01.23\\r\\n\\r\\nACC\\r\\n\\r\\nRMSE\\r\\n\\r\\n62.58 \\xc2\\xb1 04.87 00.79 \\xc2\\xb1 0.023\\r\\n63.23 \\xc2\\xb1 04.25 0.78 \\xc2\\xb1 0.011\\r\\n\\r\\n67.72 \\xc2\\xb1 00.68\\r\\n67.94 \\xc2\\xb1 01.20\\r\\n68.97 \\xc2\\xb1 00.66\\r\\n75.09 \\xc2\\xb1 01.29\\r\\nF1\\r\\n81.49 \\xc2\\xb1 00.35\\r\\n81.58 \\xc2\\xb1 00.41\\r\\n\\r\\nConclusion\\r\\n\\r\\nIn this paper, we present an unsupervised pre-training method based on masked\\r\\nimputation, significantly improving prediction results. We propose a graph transformer based architecture for learning on population graphs built from heterogeneous EHR data. We show the superiority of our pipeline in both pre-training\\r\\n\\r\\n\\x0c\\n\\nUnsupervised Pre-Training on Patient Population Graphs\\r\\n\\r\\n9\\r\\n\\r\\nand various prediction tasks for two datasets, TADPOLE and MIMIC-III. Pretraining helps for all dataset sizes but especially in scenarios where only a limited\\r\\namount of labeled data is used for fine-tuning. Our pre-training method is unsupervised and therefore independent from the end task, and further it is well\\r\\nsuited for transfer learning. This work opens the path for the community to deals\\r\\nwith small dataset specially with limited labels.\\r\\n\\r\\nReferences\\r\\n1. Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv\\r\\npreprint arXiv:2106.08254 (2021)\\r\\n2. Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., Rueckert, D.: Selfsupervised learning for medical image analysis using image context restoration.\\r\\nMedical image analysis 58, 101539 (2019)\\r\\n3. Cosmo, L., Kazi, A., Ahmadi, S.A., Navab, N., Bronstein, M.: Latent-graph learning for disease prediction. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer (2020)\\r\\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: Burstein, J., Doran, C., Solorio,\\r\\nT. (eds.) NAACL-HLT (1). Association for Computational Linguistics (2019)\\r\\n5. Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., Leskovec, J.: Strategies\\r\\nfor pre-training graph neural networks. arXiv preprint arXiv:1905.12265 (2019)\\r\\n6. Hu, Z., Dong, Y., Wang, K., Chang, K.W., Sun, Y.: Gpt-gnn: Generative pretraining of graph neural networks. In: Proceedings of the 26th ACM SIGKDD\\r\\nInternational Conference on Knowledge Discovery & Data Mining (2020)\\r\\n7. Johnson, A.E., Pollard, T.J., Shen, L., Lehman, L.w.H., Feng, M., Ghassemi, M.,\\r\\nMoody, B., Szolovits, P., Anthony Celi, L., Mark, R.G.: Mimic-iii, a freely accessible\\r\\ncritical care database. Scientific data 3(1) (2016)\\r\\n8. Kazi, A., Farghadani, S., Navab, N.: Ia-gcn: Interpretable attention based graph\\r\\nconvolutional network for disease prediction. arXiv preprint arXiv:2103.15587\\r\\n(2021)\\r\\n9. Kazi, A., Shekarforoush, S., Kortuem, K., Albarqouni, S., Navab, N., et al.: Selfattention equipped graph convolutions for disease prediction. In: 2019 IEEE 16th\\r\\nInternational Symposium on Biomedical Imaging (ISBI 2019). IEEE (2019)\\r\\n10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\\r\\narXiv:1412.6980 (2014)\\r\\n11. Li, Y., Rao, S., Solares, J.R.A., Hassaine, A., Ramakrishnan, R., Canoy, D., Zhu,\\r\\nY., Rahimi, K., Salimi-Khorshidi, G.: Behrt: transformer for electronic health\\r\\nrecords. Scientific reports 10(1) (2020)\\r\\n12. Lu, Y., Jiang, X., Fang, Y., Shi, C.: Learning to pre-train graph neural networks.\\r\\nAAAI (2021)\\r\\n13. Marinescu, R.V., Oxtoby, N.P., Young, A.L., Bron, E.E., Toga, A.W., Weiner,\\r\\nM.W., Barkhof, F., Fox, N.C., Klein, S., Alexander, D.C., et al.: Tadpole challenge: prediction of longitudinal evolution in alzheimer\\xe2\\x80\\x99s disease. arXiv preprint\\r\\narXiv:1805.03909 (2018)\\r\\n14. McDermott, M., Nestor, B., Kim, E., Zhang, W., Goldenberg, A., Szolovits, P.,\\r\\nGhassemi, M.: A comprehensive ehr timeseries pre-training benchmark. In: Proceedings of the Conference on Health, Inference, and Learning (2021)\\r\\n\\r\\n\\x0c\\n\\n10\\r\\n\\r\\nChantal Pellegrini, Anees Kazi, Nassir Navab\\r\\n\\r\\n15. Mitani, A.A., Haneuse, S.: Small data challenges of studying rare diseases. JAMA\\r\\nnetwork open 3(3) (2020)\\r\\n16. Ouyang, C., Biffi, C., Chen, C., Kart, T., Qiu, H., Rueckert, D.: Self-supervision\\r\\nwith superpixels: Training few-shot medical image segmentation without annotation. In: European Conference on Computer Vision. pp. 762\\xe2\\x80\\x93780. Springer (2020)\\r\\n17. Parisot, S., Ktena, S.I., Ferrante, E., Lee, M., Guerrero, R., Glocker, B., Rueckert,\\r\\nD.: Disease prediction using graph convolutional networks: application to autism\\r\\nspectrum disorder and alzheimer\\xe2\\x80\\x99s disease. Medical image analysis 48 (2018)\\r\\n18. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders: Feature learning by inpainting. In: Proceedings of the IEEE conference on\\r\\ncomputer vision and pattern recognition (2016)\\r\\n19. Pollard, T.J., Johnson, A.E., Raffa, J.D., Celi, L.A., Mark, R.G., Badawi, O.: The\\r\\neicu collaborative research database, a freely available multi-center database for\\r\\ncritical care research. Scientific data 5(1) (2018)\\r\\n20. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training (2018)\\r\\n21. Rasmy, L., Xiang, Y., Xie, Z., Tao, C., Zhi, D.: Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease\\r\\nprediction. NPJ digital medicine 4(1) (2021)\\r\\n22. Rong, Y., Bian, Y., Xu, T., Xie, W., Wei, Y., Huang, W., Huang, J.: Grover: Selfsupervised message passing transformer on large-scale molecular data. Advances\\r\\nin Neural Information Processing Systems (2020)\\r\\n23. Shang, J., Ma, T., Xiao, C., Sun, J.: Pre-training of graph augmented transformers\\r\\nfor medication recommendation. arXiv preprint arXiv:1906.00346 (2019)\\r\\n24. Wang, S., McDermott, M.B., Chauhan, G., Ghassemi, M., Hughes, M.C., Naumann, T.: Mimic-extract: A data extraction, preprocessing, and representation\\r\\npipeline for mimic-iii. In: Proceedings of the ACM conference on health, inference,\\r\\nand learning (2020)\\r\\n25. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., Liu, T.Y.: Do\\r\\ntransformers really perform badly for graph representation? Advances in Neural\\r\\nInformation Processing Systems 34 (2021)\\r\\n26. Zebin, T., Rezvy, S., Chaussalet, T.J.: A deep learning approach for length of\\r\\nstay prediction in clinical settings from medical records. In: 2019 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology\\r\\n(CIBCB). IEEE (2019)\\r\\n27. Zhang, J., Zhang, H., Xia, C., Sun, L.: Graph-bert: Only attention is needed for\\r\\nlearning graph representations. arXiv preprint arXiv:2001.05140 (2020)\\r\\n\\r\\n\\x0c\", b'Unsupervised Salient Object Detection with Spectral Cluster Voting\\r\\n\\r\\narXiv:2203.12614v1 [cs.CV] 23 Mar 2022\\r\\n\\r\\nGyungin Shin1\\r\\nSamuel Albanie2\\r\\nWeidi Xie1,3\\r\\n1\\r\\nVisual Geometry Group, University of Oxford, UK\\r\\n2\\r\\nDepartment of Engineering, University of Cambridge, UK\\r\\n3\\r\\nShanghai Jiao Tong University, China\\r\\ngyungin@robots.ox.ac.uk\\r\\n\\r\\nAbstract\\r\\nIn this paper, we tackle the challenging task of unsupervised salient object detection (SOD) by leveraging spectral\\r\\nclustering on self-supervised features. We make the following contributions: (i) We revisit spectral clustering and\\r\\ndemonstrate its potential to group the pixels of salient objects; (ii) Given mask proposals from multiple applications\\r\\nof spectral clustering on image features computed from various self-supervised models, e.g., MoCov2, SwAV, DINO,\\r\\nwe propose a simple but effective winner-takes-all voting\\r\\nmechanism for selecting the salient masks, leveraging object priors based on framing and distinctiveness; (iii) Using the selected object segmentation as pseudo groundtruth\\r\\nmasks, we train a salient object detector, dubbed S ELF M ASK, which outperforms prior approaches on three unsupervised SOD benchmarks. Code is publicly available at\\r\\nhttps://github.com/NoelShin/selfmask\\r\\n\\r\\nFigure 1: In this work, we propose S ELF M ASK, a framework for salient object detection that employs no human\\r\\nannotation. The figure depicts example segmentations produced by our model on DUT-OMRON [70], DUTS-TE [61]\\r\\nand ECSSD [52], with blue and orange regions denoting the\\r\\nintersection and the difference between the predicted mask\\r\\nand ground-truth label, respectively. Despite no supervision, S ELF M ASK reliably segments a close approximation\\r\\nto the full spatial extent of salient regions. Best viewed in\\r\\ncolour.\\r\\n\\r\\n1. Introduction\\r\\nSalient object detection (SOD)1 , which aims to group\\r\\npixels that attract human visual attention, has been extensively studied in the field of computer vision due to its wide\\r\\nrange of applications such as photo cropping [55, 41], retargeting in images [19, 56] and video [50].\\r\\nIn the literature, early work tackled this problem by utilising low-level features (e.g., colour [15]) together with\\r\\npriors on salient regions in an object such as contrast priors [28], boundary priors [64] and centre priors [29]. Recent SOD models have approached this task from the perspective of representation learning, typically training deep\\r\\nneural networks (DNNs) on a large-scale dataset with manual annotations. However, the scalability of such supervised\\r\\nlearning approach is limited because it is a costly process to\\r\\n\\r\\ncollect ground-truth mask annotations.\\r\\nTo overcome the necessity of large-scale human annotation, many unsupervised methods for saliency detection/object segmentation have recently been proposed [7,\\r\\n13, 59, 60, 42, 73, 76, 46]. Despite these efforts, the gap\\r\\nbetween unsupervised and fully supervised SOD methods\\r\\nremains significant.\\r\\nInterestingly, however, it has been noted that recent selfsupervised models such as DINO [11] exhibit significant\\r\\npotential for object segmentation despite the fact that their\\r\\ntraining objective does not explicitly encourage pixel group-\\r\\n\\r\\n1 In contrast to object detection (which aims to localise and recognise\\r\\nobjects with bounding boxes), salient object detection aims to segment\\r\\nforeground objects by predicting pixel-wise masks for them.\\r\\n\\r\\n1\\r\\n\\r\\n\\x0c\\n\\ning. The focus of this work is to leverage this observation\\r\\nto propose a simple yet effective mechanism for extracting\\r\\nobject regions from self-supervised features that can be employed for the task of unsupervised salient object detection.\\r\\nTo this end, we explore the use of spectral clustering [51], a classical graph-theoretic clustering algorithm,\\r\\nand find that it can generate useful segmentation candidates\\r\\nacross a range of self-supervised features (i.e., DINO [11],\\r\\nMoCov2 [25], and SwAV [10]). Motivated by this finding,\\r\\nwe propose a simple winner-takes-all voting strategy to select the salient object masks among a collection of clusters\\r\\nproduced by repeated applications of spectral clustering to\\r\\nself-supervised features.2 In particular, we base our voting\\r\\nstrategy on two priors: The first is a framing prior that a\\r\\nsalient object should not occupy the full image height or\\r\\nwidth; The second is a distinctiveness prior that assumes\\r\\nthat salient regions are sufficiently distinctive that they will\\r\\nappear as clusters among an appropriately constructed collection of redundant re-clusterings of the data. We then\\r\\nshow that the selected salient masks can be employed as\\r\\npseudo-labels to train a saliency estimation network that\\r\\nachieves state-of-the-art results on a variety of benchmarks.\\r\\nIn summary, we make the following contributions:\\r\\n(i) We revisit spectral clustering and highlight its benefits\\r\\nover k-means clustering as a proposal mechanism to identify object regions from self-supervised features on three\\r\\npopular salient object detection (SOD) datasets; (ii) We propose an effective voting strategy to select the most salient\\r\\nobject mask in an image among multiple segmentations\\r\\ngenerated from different self-supervised features by leveraging saliency priors; (iii) Using the salient masks as pseudo\\r\\nground truth masks (pseudo-masks), we train an object segmentation model, S ELF M ASK, that outperforms the previous unsupervised saliency detection approaches on three\\r\\nSOD benchmarks.\\r\\n\\r\\ncent work exploring the use of self-supervised transformers [68, 11], and by the analysis provided by [11, 45]\\r\\nwho noted that the self-attention of Vision Transformers\\r\\n(ViT) [22] are capable of highlighting spatially coherent\\r\\nobject regions in their input. While prior work [72] has\\r\\ndemonstrated that self-supervised pretraining can be effective for semantic segmentation when coupled with end-toend supervised metric learning on the target dataset, we instead seek a simple way to exploit self-supervised features\\r\\nfor object segmentation without annotation.\\r\\n\\r\\n2.2. Unsupervised saliency detection\\r\\nSupervised object segmentation requires pixel-wise annotations which are time-consuming to acquire. Seeking to\\r\\navoid this cost, many attempts have been made to solve the\\r\\ntask in an unsupervised fashion. Prior to the dominance of\\r\\ndeep neural networks, a broad range of handcrafted methods\\r\\nwere proposed [36, 1, 30, 23, 69, 78, 16] based on one or\\r\\nmore priors relating to foreground regions within an image\\r\\nsuch as the contrast prior [28], centre prior [29], and boundary prior [64]. However, these handcrafted approaches suffer from poor performance relative to recent DNN-based\\r\\nmodels, described next.\\r\\nGenerative models. A common approach for DNN-based\\r\\nunsupervised object segmentation is to utilise generative adversarial networks (GANs) [24]. Specifically, given an image, a generator is adversarially trained to produce an object\\r\\nmask which will be used to composite a realistic image by\\r\\ncopying the corresponding object region in the image into\\r\\na background which is either synthesised [7] or taken from\\r\\na different image [2]. An alternative family of approaches\\r\\naims to discover a direction in the latent space of a pretrained GAN that can be used to segment foreground and\\r\\nbackground regions [59, 60, 42]. Then, a saliency detector\\r\\nis trained on a synthetic data set composed of pairs of images together with their foreground masks generated via the\\r\\ndiscovered latent space structure. In contrast, we seek to\\r\\nexploit representations learned via self-supervision by discriminative, rather than generative, models.\\r\\n\\r\\n2. Related work\\r\\nOur work relates to two themes in the literature:\\r\\nself-supervised representation learning and unsupervised\\r\\nsaliency detection. Each of these is discussed next.\\r\\n\\r\\nNoisy supervision with pseudo-labels. More closely related to our work, the use of weak pseudo saliency masks\\r\\nfor training a DNN has been proposed. SBF [73], the first\\r\\nattempt to train saliency detector without human annotation,\\r\\nproposed to train a model with superpixel-level pseudomasks generated by fusing weak saliency maps from multiple unsupervised methods (i.e., [75, 74, 53]). Similarly,\\r\\nUSD [76] aims to learn from diverse noisy pseudo-labels\\r\\nobtained via distinct unsupervised handcrafted methods in\\r\\nsuch a way that a saliency detector trained with the pseudomasks can predict a saliency map free from label noise.\\r\\nDeepUSPS [46] proposed to refine the pseudo-masks for\\r\\nimages produced by handcrafted saliency methods, by train-\\r\\n\\r\\n2.1. Self-supervised representation learning\\r\\nThere has been a great deal of interest in self-supervised\\r\\napproaches to learning visual representations that obviate the requirement for labels. These include techniques\\r\\nfor solving proxy tasks, such as predicting patch locations [20], patch discrimination [22], grouping through cooccurrence [27], colourisation [31], jigsaw puzzles [47],\\r\\ncommon fate principles [40], clustering [9, 4], and instance discrimination [65, 25]. Our work is inspired by re2 In this work, we use the terms cluster and mask interchangeably.\\r\\nSpecifically, by mask, we mean a (one-hot) mask which encodes the spatial\\r\\nextent of a cluster.\\r\\n\\r\\n2\\r\\n\\r\\n\\x0c\\n\\n3.2. Segmentation with spectral clustering\\r\\n\\r\\ning segmentation networks via a self-supervised iterative refinement process. The refined pseudo-masks are then combined from different handcrafted methods to train a final\\r\\nsegmentation network. In contrast to the methods above, we\\r\\nuse neither handcrafted saliency methods nor an iterative\\r\\nrefinement strategy, resulting in a simpler learning framework.\\r\\n\\r\\nConceptually, segmentation is obtained via spectral clustering [51] with pixel-wise image features by projecting the\\r\\nfeatures onto a representation space such that image partitions can be decided by directly comparing similarities between their corresponding features.\\r\\nConcretely, we first extract dense features from the image, i.e., F \\xe2\\x88\\x88 Rh\\xc3\\x97w\\xc3\\x97D with a pre-trained convolutionalor transformer-based encoder. Then, each feature vector\\r\\nfi \\xe2\\x88\\x88 RD in the dense feature map F can be seen as a vertex\\r\\nin an undirected graph with vertex set V = {f1 , . . . , fN } \\xe2\\x88\\x88\\r\\nRN \\xc3\\x97D , where N = h \\xc2\\xb7 w. Each edge between two vertices\\r\\nfi and fj is associated with a non-negative weight wij \\xe2\\x89\\xa5 0\\r\\ndefined by feature similarity. In particular, the weighted adjacency matrix W of the graph is computed as\\r\\n\\r\\nObject segmentation properties of self-supervised vision\\r\\ntransformers. Another line of related work has sought to\\r\\ninvestigate the observation that self-supervised ViTs [11]\\r\\nexhibit object segmentation potential. LOST [54] propose\\r\\nto pick a seed patch from such a ViT that is likely to contain\\r\\npart of a foreground object, and then expand the seed patch\\r\\nto different patches sharing a high similarity with the seed\\r\\npatch. Concurrent work, TokenCut [63], proposes to use\\r\\nNormalised Cuts [51] to segment the salient object among\\r\\nthe final layer self-attention key features of ViT. S ELF M ASK differs from the TokenCut approach to salient object detection in two key ways: (1) While we similarly employ spectral clustering as part of our pipeline, we demonstrate the significant additional value of integrating cues\\r\\nfrom diverse re-clusterings via voting to bootstrap a pseudolabelling process; (2) Thanks to the flexibility of our clustering approach, we are able to leverage saliency cues from\\r\\nself-supervised convolutional neural networks (CNNs) as\\r\\nwell as ViT architectures, and show the benefits of doing\\r\\nso. We compare our approach with theirs in section 4.\\r\\n\\r\\nW = (wij ) =\\r\\n\\r\\nVVT\\r\\n\\xe2\\x88\\x88 RN \\xc3\\x97N\\r\\nkVkkVk\\r\\n\\r\\nwhere\\r\\nPN the degree of a vertex fi \\xe2\\x88\\x88 V is defined as di =\\r\\nj=1 wij , and the degree matrix D is defined with the degrees d1 , . . . , dn on the diagonal. Given the adjacency matrix W and the degree matrix D, the (un-normalised) graph\\r\\nLaplacian L is defined as:\\r\\nL=D\\xe2\\x88\\x92W\\r\\n\\r\\n(3)\\r\\n\\r\\nGiven L, we can solve the generalised eigenproblem:\\r\\nLu = \\xce\\xbbDu\\r\\n\\r\\n3. Method\\r\\n\\r\\n(4)\\r\\n\\r\\nwhere u \\xe2\\x88\\x88 RN and \\xce\\xbb represent an eigenvector and its eigenvalue. We take the k eigenvectors with the lowest eigenvalues and form a matrix U \\xe2\\x88\\x88 RN \\xc3\\x97k which has the eigenvectors as its columns.\\r\\nFinally, a set of clusters C is obtained by running kmeans algorithm on the row vectors of a matrix U (see supplementary for more detail), producing regions with all pixels from the corresponding cluster. Note that, at this stage,\\r\\nthe resulting clusters are composed of both object and background masks\\xe2\\x80\\x94the object mask itself will be selected by\\r\\nour selection strategy described next.\\r\\n\\r\\nIn this section, we begin by formalising the problem scenario (Sec. 3.1) and briefly summarise spectral clustering\\r\\n(Sec. 3.2). Then, we introduce our approach to address\\r\\nunsupervised salient object detection by selecting pseudo\\r\\nground truth masks via spectral clustering, and train a\\r\\nsaliency prediction network, called S ELF M ASK (Sec. 3.3).\\r\\n\\r\\n3.1. Problem formulation\\r\\nHere, we consider the task of unsupervised salient object detection (SOD), with the goal of training a segmenter (\\xce\\xa6seg ) that seeks to partition the image into two disjoint groups, namely foreground and background. That is,\\r\\n\\xce\\xa6seg (I; \\xce\\x98) = Mseg \\xe2\\x88\\x88 {0, 1}H\\xc3\\x97W\\r\\n\\r\\n(2)\\r\\n\\r\\n3.3. Supervision with pseudo-mask from spectral\\r\\nclusters\\r\\n\\r\\n(1)\\r\\n\\r\\nHere, we first introduce our voting strategy for selecting\\r\\na salient mask from a set of spectral clusters from different\\r\\nfeatures and multiple k values (i.e., cluster numbers), which\\r\\nutilises a framing prior and distinctiveness prior. Then, we\\r\\ndescribe our model, S ELF M ASK, which is trained by using\\r\\nthe selected salient masks as pseudo-masks for supervision.\\r\\n\\r\\nwhere I \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x973 refers to an input image, and \\xce\\x98 represents learnable parameters. Mseg denotes a binary segmentation mask, with 1s denoting a foreground region and 0s\\r\\ndenoting background.\\r\\nTraditionally this has been treated as a clustering problem where the key challenge lies in designing effective features for accurately describing salient regions. In this work,\\r\\nwe instead look for a simple yet effective solution by leveraging self-supervised visual representations.\\r\\n\\r\\n3.3.1\\r\\n\\r\\nSpectral cluster winner-takes-all voting\\r\\n\\r\\nTo choose the salient object among the mixture of foreground and background masks generated by spectral clus3\\r\\n\\r\\n\\x0c\\n\\nFigure 2: Overview of our approach. Given different self-supervised encoders, we first generate a set of pseudo-mask\\r\\ncandidates per image using spectral clustering before the training step. In the figure we show 12 masks from clusterings from\\r\\nthree different encoder features with k=4. We select the most salient mask among them via the proposed voting strategy\\r\\nand use it as a pseudo-mask for the image. Then we train our model to predict nq queries (i.e., predictions), all of which\\r\\nare encouraged to be similar to the salient mask. To make the model aware of the objectness of each prediction, we use the\\r\\nranking loss which encourages the objectness score of a prediction closer to the salient mask to be higher. At inference time,\\r\\nwe select the prediction with the highest objectness score. Please see the text for details.\\r\\ntering, we propose a voting strategy based on two observations: (1) The spatial extent of an object rarely occupies\\r\\nthe entire height and width of an image. (2) Salient object regions are likely to appear in multiple clusters across\\r\\ndifferent self-supervised features as well as with different\\r\\ncluster numbers k. In other words, among k clusters from\\r\\nan application of clustering, we assume that at least one\\r\\ncluster encodes an object region within the image and that\\r\\nthis holds for different features (e.g., DINO, MoCov2, or\\r\\nSwAV). We call these priors the framing prior and distinctiveness prior, respectively. Note that the framing prior\\r\\nbears a resemblance to the centre prior [29], which states\\r\\nthat salient objects are likely to be located near the center\\r\\nof an image, and the boundary prior [64], which presumes\\r\\nthat the foreground object rarely touches the boundary of an\\r\\nimage. However, the framing prior differs from these priors\\r\\nin that it is not related to a location of an object but rather\\r\\nthe scale of an object within an image.\\r\\n\\r\\nremaining masks as the final mask for salient objects (bottom of Fig. 2). There are two edge cases in the background\\r\\nelimination process we handle explicitly: (i) when no masks\\r\\nare left in the candidate set and (ii) when only two masks\\r\\nare left, sharing the same IoU. For the former case, we simply keep all the masks in the candidate set as every mask\\r\\nhighlights regions spanning the spatial extent of the image,\\r\\nbreaking the assumption of the framing prior. For the latter,\\r\\nwe break ties randomly to pick one of the two masks.\\r\\n3.3.2\\r\\n\\r\\nS ELF M ASK\\r\\n\\r\\nHere, we describe our model architecture, training, and inference procedure.\\r\\nArchitecture. We base our salient object detection network, called S ELF M ASK, on a variant of the MaskFormer\\r\\narchitecture [14] which was originally proposed for the semantic/instance segmentation task.\\r\\nS ELF M ASK has two different sets of outputs: mask predictions and objectness scores for each mask. In detail, our\\r\\nmodel comprises an image encoder, a pixel decoder, and a\\r\\ntransformer decoder (upper part of Fig. 2). The image encoder takes an image I \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x973 as input and outputs\\r\\nfeature maps f (I) \\xe2\\x88\\x88 Rh\\xc3\\x97w\\xc3\\x97D . Then, the feature maps are\\r\\nfed into the pixel decoder to produce upsampled features\\r\\ng(f (I)) \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97D . The feature maps are also passed\\r\\n\\r\\nTo use the framing prior and distinctiveness prior in practice, we first form a candidate set of masks by repeatedly\\r\\napplying spectral clustering to different features with multiple k values. Then, we treat masks whose spatial extent is\\r\\nas long as the width or height of the image as background\\r\\nmasks, and eliminate them from the candidate set. Finally,\\r\\nwe employ winner-takes-all voting: we pick the mask with\\r\\nthe highest average pair-wise similarity w.r.t. IoU among all\\r\\n4\\r\\n\\r\\n\\x0c\\n\\n4. Experiments\\r\\n\\r\\nto the transformer decoder which outputs nq per-mask embeddings by using the feature maps as keys and values and\\r\\nthe learnable embeddings as queries. The final mask predictions M are produced via matrix multiplication between the\\r\\nupsampled features and per-mask embeddings, followed by\\r\\nan element-wise sigmoid function \\xcf\\x83 (\\xc2\\xb7):\\r\\nM = {Mi | Mi = \\xcf\\x83 (g (f (I)) qi ) , i = 1, ..., nq }\\r\\n\\r\\nIn this section, we first describe the datasets used in our\\r\\nexperiments (Sec. 4.1) and provide implementation details\\r\\n(Sec. 4.2). We then conduct ablation study (Sec. 4.3) and\\r\\nreport our results for salient object detection (Sec. 4.4).\\r\\n\\r\\n4.1. Datasets\\r\\n\\r\\n(5)\\r\\n\\r\\nWe use DUTS-TR [61], which contains 10,553 images,\\r\\nto train our model with the pseudo-masks generated by following Sec. 3.3. We emphasize that only images are used\\r\\nfor generating pseudo-masks and training, without the corresponding labels. For our ablation study and comparison\\r\\nto previous work, we consider five popular saliency datasets\\r\\nincluding DUT-OMRON [70], which comprises 5,168 images of varied content with ground-truth pixel level masks;\\r\\nDUTS-TE [61], containing 5,019 images selected from the\\r\\nSUN dataset [66] and ImageNet test set [18]; ECSSD [52]\\r\\nwhich contains 1,000 images that were selected to represent complex scenes; HKU-IS [32] which consists of 4,447\\r\\nscene images with foreground/background sharing the similar appearances; SOD [44] which contains 300 images with\\r\\nmany images having multiple salient objects.\\r\\n\\r\\nwhere qi \\xe2\\x88\\x88 RD denotes the ith query (i.e., per-mask embedding). For each mask Mi , an objectness score oi \\xe2\\x88\\x88 [0, 1]\\r\\nis estimated by feeding the corresponding per-mask embedding qi to a simple MLP with two hidden layers followed\\r\\nby a sigmoid function. Note that other encoder and decoder\\r\\narchitectures can also be used here.\\r\\nObjective. For training, we employ two objective functions: a mask loss and a ranking loss, denoted by Lmask and\\r\\nLrank , respectively. Given nq mask predictions for an image\\r\\nfrom the model, we encourage all predictions to be similar\\r\\nto the pseudo-mask. Specifically, following [14], we use the\\r\\nDice coefficient [43], which considers the class-imbalance\\r\\nbetween foreground and background regions within an image, as the mask loss. It is worth noting that, unlike [14],\\r\\nwe do not include the focal loss [34] in the mask loss since\\r\\nwe find that it hinders convergence.\\r\\nTo decide which prediction best highlights the salient\\r\\nregion in the image among the proposed candidates when\\r\\nnq > 1, we rank the predicted masks based on their objectness score. Specifically, we first re-order the indices of the\\r\\npredicted masks by their mask loss in ascending order such\\r\\nthat\\r\\nLmask (Mi , Mpseudo ) \\xe2\\x89\\xa4 Lmask (Mj , Mpseudo )\\r\\n\\r\\n4.2. Implementation details\\r\\nNetworks. We use the ViT-S/8 architecture [21] for the encoder, a bilinear upsampler with a scale factor of 2 for the\\r\\npixel decoder, and 6 transformer layers [58] for the transformer decoder. For the MLP applied to per-mask queries\\r\\n(with a dimensionality of 384) that outputs a scalar value\\r\\nfor the objectness score, we use three fully-connected layers with a ReLU activation between them. We set the same\\r\\nnumber of units for the hidden nodes as the input (i.e., 384)\\r\\nand output a single value followed by a sigmoid.\\r\\n\\r\\n(6)\\r\\n\\r\\nfor any i < j, where Mpseudo denotes the target pseudomask for the image. Then, we enforce the objectness score\\r\\noi of the mask Mi to be higher than the scores oj for any\\r\\nj > i. As a consequence the model is encouraged to produce a higher score for a predicted mask that more closely\\r\\nresembles the pseudo-mask than other predictions. We instantiate this ranking loss as a hinge loss [17]:\\r\\n\\r\\nTraining details. We train our models for 12 epochs and\\r\\noptimise all parameters including the backbone encoder using AdamW [38] with a learning rate of 6e-6 and the Poly\\r\\nlearning rate policy [48, 67, 12]. For data augmentation,\\r\\nwe use random scaling with a scale range of [0.1, 1.0], random cropping to a size of 224\\xc3\\x97224 and random horizontal\\r\\nflipping with a probability of 0.5. In addition, photometric\\r\\ntransformations include random color jittering, color dropping, and Gaussian blurring are applied. We run each model\\r\\nwith three different seeds and report the average.\\r\\n\\r\\nnq \\xe2\\x88\\x921 nq\\r\\n\\r\\nLrank =\\r\\n\\r\\nX X\\r\\n\\r\\nmax (0, oj \\xe2\\x88\\x92 oi ) .\\r\\n\\r\\n(7)\\r\\n\\r\\ni=1 j>i\\r\\n\\r\\nOverall, our final objective function is as follows:\\r\\nL = Lmask + \\xce\\xbbLrank\\r\\n\\r\\n(8)\\r\\n\\r\\nMetrics. In our experiments, we report intersection-overunion (IoU), pixel accuracy (Acc) and maximal F\\xce\\xb2 score\\r\\n(max F\\xce\\xb2 ) with \\xce\\xb2 2 set to 0.3 following [63]. Please refer to\\r\\nthe supplementary for more details on these metrics.\\r\\n\\r\\nwhere \\xce\\xbb is a weighting factor, which is set to 1.0 across our\\r\\nexperiments. Following [8, 14], we compute the loss for\\r\\noutputs from each layer of the transformer decoder.\\r\\nInference. During inference, given nq predicted masks for\\r\\nan image and their objectness score, we pick the mask with\\r\\nthe highest score as the salient object detection and binarise\\r\\nit with a fixed threshold of 0.5.\\r\\n\\r\\n4.3. Ablation study\\r\\nIn this section, we first conduct experiments to compare\\r\\nthe effectiveness of spectral clustering and k-means when\\r\\n5\\r\\n\\r\\n\\x0c\\n\\nFigure 3: Sample visualisations of the pseudo-masks and predictions from our model on the ECSSD, DUT-OMRON, and\\r\\nDUTS-TE benchmarks. From left to right, the input image, ground-truth mask, a pseudo-mask decided by the proposed\\r\\nvoting-based salient mask selection, and the prediction of our model are shown. Blue and orange coloured regions denote the\\r\\nintersection and difference between a ground-truth and a predicted mask. Best viewed in colour.\\r\\n\\r\\nModel\\r\\n\\r\\nArch.\\r\\n\\r\\nCluster.\\r\\n\\r\\nDUT-OMRON DUTS-TE ECSSD\\r\\nk={2, 3, 4} k={2, 3, 4} k={2, 3, 4}\\r\\n\\r\\nModel\\r\\n\\r\\nConvolutional Nets\\r\\n.375\\r\\n.387\\r\\n\\r\\n.415\\r\\n.454\\r\\n\\r\\n.500\\r\\n.627\\r\\n\\r\\nSwAV [10] ResNet50 k-means\\r\\nSwAV [10] ResNet50 spectral\\r\\n\\r\\n.399\\r\\n.401\\r\\n\\r\\n.444\\r\\n.458\\r\\n\\r\\n.542\\r\\n.590\\r\\n\\r\\nResNet [25] ResNet50 k-means\\r\\nResNet [25] ResNet50 spectral\\r\\n\\r\\nDUT-OMRON DUTS-TE ECSSD\\r\\nk={2, 3, 4} k={2, 3, 4} k={2, 3, 4}\\r\\n\\r\\nDINO [11] ViT-S/16 k-means\\r\\nDINO [11] ViT-S/16 spectral\\r\\n\\r\\n.377\\r\\n.394\\r\\n\\r\\n.392\\r\\n.417\\r\\n\\r\\n.541\\r\\n.577\\r\\n\\r\\nViT-S/8 k-means\\r\\nViT-S/8 spectral\\r\\n\\r\\n.369\\r\\n.398\\r\\n\\r\\n.377\\r\\n.411\\r\\n\\r\\n.551\\r\\n.587\\r\\n\\r\\n.337\\r\\n.310\\r\\n\\r\\n.354\\r\\n.327\\r\\n\\r\\n.444\\r\\n.437\\r\\n\\r\\n.411\\r\\n.400\\r\\n\\r\\n.542\\r\\n.551\\r\\n\\r\\nVision Transformer\\r\\nViT [21]\\r\\nViT [21]\\r\\n\\r\\nVision Transformer\\r\\n\\r\\nViT-S/16 k-means\\r\\nViT-S/16 spectral\\r\\n\\r\\n.394\\r\\n.380\\r\\n\\r\\nTable 2: Spectral clustering and k-means perform comparably for fully-supervised features. We report upper\\r\\nbound IoUs to provide a comparison of mask quality comparison between k-means and spectral clustering applied to\\r\\nfully-supervised features. We report the average of the results from k={2, 3, 4}.\\r\\n\\r\\nTable 1: Spectral clustering dominates k-means for selfsupervised features. We report upper bound IoUs to compare the quality of masks produced by k-means and spectral clustering on self -supervised features with two different\\r\\nencoder architectures (i.e., convolution- and transformerbased encoder). We report the average of the results from\\r\\nk={2, 3, 4}.\\r\\n\\r\\nformance of the clustering algorithms, we consider different\\r\\nk values from 2 to 4 and average the results. For the full results with each k value, please refer to the supplementary.\\r\\nAs shown in Tab. 1, we observe that object masks from\\r\\nspectral clustering consistently outperform k-means masks\\r\\nby a large margin. Interestingly, however, when using fullysupervised image encoders, the performance gain of spectral clustering diminishes (Tab. 2). These findings boil down\\r\\nto a simple summary: while using self-supervised visual\\r\\nrepresentations for grouping, spectral clustering is considerably superior to k-means, regardless of the choice of encoder architecture and self-supervised learning algorithm.\\r\\n\\r\\napplied to self-supervised image encoders. Next, we quantitatively verify the performance of our winner-takes-all voting strategy for foreground mask selection and compare it to\\r\\ndifferent saliency selection methods. Lastly, we investigate\\r\\nthe effect of different number of queries on S ELF M ASK.\\r\\n4.3.1\\r\\n\\r\\nCluster.\\r\\n\\r\\nConvolutional Nets\\r\\n\\r\\nMoCov2 [25] ResNet50 k-means\\r\\nMoCov2 [25] ResNet50 spectral\\r\\n\\r\\nDINO [11]\\r\\nDINO [11]\\r\\n\\r\\nArch.\\r\\n\\r\\nSpectral clustering vs k-means clustering\\r\\n\\r\\nWe compare spectral clustering against a k-means [37]\\r\\nclustering baseline on three salient object detection benchmarks. As the resulting segmentations from each algorithm\\r\\nare agnostic to foreground/background regions, we consider\\r\\na best-case evaluation for both algorithms. In detail, given a\\r\\ngroundtruth mask, we pick the cluster with the highest IoU\\r\\nw.r.t. the groundtruth. Such IoUs act as an upper-bound\\r\\nscore among the clusters (i.e., average best overlap in [3]).\\r\\nTo account for the effect of the cluster number k on per-\\r\\n\\r\\n4.3.2\\r\\n\\r\\nVoting for salient object masks\\r\\n\\r\\nHere, we conduct experiments to assess our voting method\\r\\nfor selecting a foreground mask among the mask candidates. Since these experiments include ablations across hyperparameter choices, we conduct them on the HKU-IS [32]\\r\\nand SOD [44], rather than the benchmarks used to compare\\r\\nto prior work.\\r\\nIn detail, we construct an initial corpus of mask candi6\\r\\n\\r\\n\\x0c\\n\\ndates by clustering different self-supervised features with\\r\\ndifferent number of clusters, as described in Sec. 3.2. For\\r\\nthis, we build the candidate set with spectral clusters from\\r\\ndifferent combinations of self-supervised features, e.g., MoCov2/DINO or SwAV/MoCov2. It is important to do so to\\r\\nallow our voting-based method to leverage the distinctiveness prior across different features. In addition, for each\\r\\ncombination, we experiment with 3 different k value settings: k=2, {2, 3} or {2, 3, 4} to account for various object\\r\\nscales, e.g., a lower k tends to cover large regions, while\\r\\na higher k segments smaller objects. We then evaluate the\\r\\nselected masks on the HKU-IS set. For reference we also\\r\\ncompute an upper bound IoU, which is computed in a similar way as done in the previous section.\\r\\n\\r\\nFeatures\\r\\n\\r\\nEffectiveness of clustering various self-supervised models with different number of clusters.\\r\\nAs shown in\\r\\nTab. 3, we make two observations: (i) IoU of both selected\\r\\nmasks and upper bound masks, denoted by pseudo-mask\\r\\nand UB improves by increasing k across all feature combinations; (ii) using all three features (i.e., DINO, MoCov2\\r\\nand SwAV) results in better pseudo-masks than using two\\r\\nof the three features (e.g., MoCov2 and SwAV). These support the distinctiveness prior, which assumes that at least\\r\\none cluster represents a foreground region, and its application to our voting-based saliency selection method.\\r\\n\\r\\nPseudo-mask UB\\r\\n\\r\\n7\\r\\n\\r\\nX\\r\\n\\r\\nX\\r\\n\\r\\n2\\r\\n2, 3\\r\\n2, 3, 4\\r\\n\\r\\n.508\\r\\n.561\\r\\n.580\\r\\n\\r\\n.562\\r\\n.626\\r\\n.658\\r\\n\\r\\nX\\r\\n\\r\\n7\\r\\n\\r\\nX\\r\\n\\r\\n2\\r\\n2, 3\\r\\n2, 3, 4\\r\\n\\r\\n.473\\r\\n.538\\r\\n.559\\r\\n\\r\\n.553\\r\\n.644\\r\\n.682\\r\\n\\r\\nX\\r\\n\\r\\nX\\r\\n\\r\\n7\\r\\n\\r\\n2\\r\\n2, 3\\r\\n2, 3, 4\\r\\n\\r\\n.459\\r\\n.536\\r\\n.566\\r\\n\\r\\n.546\\r\\n.648\\r\\n.688\\r\\n\\r\\nX\\r\\n\\r\\nX\\r\\n\\r\\nX\\r\\n\\r\\n2\\r\\n2, 3\\r\\n2, 3, 4\\r\\n\\r\\n.511\\r\\n.567\\r\\n.590\\r\\n\\r\\n.584\\r\\n.664\\r\\n.698\\r\\n\\r\\nTable 3: Forming a candidate set with various selfsupervised features and multiple k values improves IoU\\r\\nof both pseudo-masks and upper bound masks (UB).\\r\\nWe compare cases with different combinations of selfsupervised features and cluster numbers of k=2, {2, 3} or\\r\\n{2, 3, 4} on the HKU-IS benchmark.\\r\\nSelection\\r\\n\\r\\nEffectiveness of the proposed voting approaches. We\\r\\nfurther validate the effectiveness of our voting scheme by\\r\\ncomparing to different selection methods, i.e., random selection and a centre prior [29]-based strategy. Specifically,\\r\\nwe form a candidate set using DINO/MoCov2/SwAV features with k = {2, 3, 4}, which amounts to 27 masks in\\r\\ntotal. For the random strategy, we simply pick one of the\\r\\nmasks uniformly from the candidates. For the centre prior\\r\\nselection strategy, we choose a mask whose average Euclidean distance to the image centre from its constituent\\r\\npixel locations is lowest. In addition, we consider each\\r\\nmethod with or without utilising the framing prior to assess\\r\\nthe influence of filtering out background mask. We evaluate\\r\\nthe IoU of each case on the HKU-IS and SOD benchmarks.\\r\\nAs can be seen in Tab. 4, deploying the framing prior\\r\\nboosts IoU in all considered selection methods, with the\\r\\nproposed voting selection method performing best. The\\r\\nframing prior plays a crucial role in the voting process: voting without this prior performs much worse than its counterparts in both random and center-based selections on HKUIS, and performs similarly to the random strategy on SOD.\\r\\nThis is caused in large part by mistakes when selecting\\r\\nbackground masks as salient objects.\\r\\n4.3.3\\r\\n\\r\\nk\\r\\n\\r\\nDINO [11] MoCov2 [25] SwAV [10]\\r\\n\\r\\nFraming prior HKU-IS [32] SOD [44]\\r\\n\\r\\nrandom\\r\\n\\r\\n7\\r\\nX\\r\\n\\r\\n.206\\r\\n.464\\r\\n\\r\\n.197\\r\\n.277\\r\\n\\r\\ncenter\\r\\n\\r\\n7\\r\\nX\\r\\n\\r\\n.362\\r\\n.442\\r\\n\\r\\n.122\\r\\n.392\\r\\n\\r\\nvoting (ours)\\r\\n\\r\\n7\\r\\nX\\r\\n\\r\\n.081\\r\\n.590\\r\\n\\r\\n.200\\r\\n.447\\r\\n\\r\\nTable 4: Winner-takes-all voting and the framing prior\\r\\nboth significantly improve mask quality. We compare our\\r\\nvoting strategy to different selection strategies along with\\r\\nthe effect of framing prior under the IoU metric. Selection is\\r\\nperformed from a candidate set including DINO, MoCov2\\r\\nand SwAV features with k = {2, 3, 4}.\\r\\nthe effect of the number of queries nq in the Transformer decoder. For this, we train our model with nq ={5, 10, 20, 50,\\r\\n100} on DUTS-TR and evaluate performance on the HKUIS benchmark in terms of maxF\\xce\\xb2 for two settings, (i) using\\r\\nground-truth masks to pick the best mask out of nq mask\\r\\npredictions, denoted as the SelfMask upper bound (UB); (ii)\\r\\ntaking the mask with the highest object score, denoted SelfMask.\\r\\nAs shown in Figure 4, both SelfMask and SelfMask UB\\r\\nare fairly robust to the number of queries, initially increasing slightly with this hyperparameter (i.e., predictions) before degrading after 20 queries. We conjecture that this\\r\\nis because a handful of queries are enough to localise the\\r\\nsalient objects, while further predictions may make it chal-\\r\\n\\r\\nThe influence of the number of queries\\r\\n\\r\\nAs described in Sec. 3.3, we train S ELF M ASK using the selected salient masks as pseudo-masks. Here, we investigate\\r\\n7\\r\\n\\r\\n\\x0c\\n\\nModel\\r\\nHS [69]\\r\\nwCtr [78]\\r\\nWSC [33]\\r\\nDeepUSPS [46]\\r\\nBigBiGAN [60]\\r\\nE-BigBiGAN [60]\\r\\nMelas-Kyriazi et al. [42]\\r\\nLOST [54]\\r\\nLOST [54] + Bilateral solver [6]\\r\\nTokenCut [63]\\r\\nTokenCut [63] + Bilateral solver [6]\\r\\npseudo-masks (Ours)\\r\\nS ELF M ASK (Ours)\\r\\nS ELF M ASK (Ours) + Bilateral solver [6]\\r\\n\\r\\nDUT-OMRON [70]\\r\\nAcc\\r\\nIoU\\r\\nmaxF\\xce\\xb2\\r\\n.843\\r\\n.433\\r\\n.561\\r\\n.838\\r\\n.416\\r\\n.541\\r\\n.865\\r\\n.387\\r\\n.523\\r\\n.779\\r\\n.305\\r\\n.414\\r\\n.856\\r\\n.453\\r\\n.549\\r\\n.860\\r\\n.464\\r\\n.563\\r\\n.883\\r\\n.509\\r\\n.797\\r\\n.410\\r\\n.473\\r\\n.818\\r\\n.489\\r\\n.578\\r\\n.880\\r\\n.533\\r\\n.600\\r\\n.897\\r\\n.618\\r\\n.697\\r\\n.811\\r\\n.403\\r\\n.901\\r\\n.582\\r\\n.680\\r\\n.919\\r\\n.655\\r\\n.852\\r\\n\\r\\nDUTS-TE [61]\\r\\nAcc\\r\\n.826\\r\\n.835\\r\\n.862\\r\\n.773\\r\\n.878\\r\\n.882\\r\\n.893\\r\\n.871\\r\\n.887\\r\\n.903\\r\\n.914\\r\\n.845\\r\\n.923\\r\\n.933\\r\\n\\r\\nIoU\\r\\n\\r\\nmaxF\\xce\\xb2\\r\\n\\r\\n.369\\r\\n.392\\r\\n.384\\r\\n.305\\r\\n.498\\r\\n.511\\r\\n.528\\r\\n.518\\r\\n.572\\r\\n.576\\r\\n.624\\r\\n.466\\r\\n.626\\r\\n.660\\r\\n\\r\\n.504\\r\\n.522\\r\\n.528\\r\\n.425\\r\\n.608\\r\\n.624\\r\\n.611\\r\\n.697\\r\\n.672\\r\\n.755\\r\\n.750\\r\\n.882\\r\\n\\r\\nECSSD [52]\\r\\nAcc\\r\\n.847\\r\\n.862\\r\\n.852\\r\\n.795\\r\\n.899\\r\\n.906\\r\\n.915\\r\\n.895\\r\\n.916\\r\\n.918\\r\\n.934\\r\\n.893\\r\\n.944\\r\\n.955\\r\\n\\r\\nIoU\\r\\n\\r\\nmaxF\\xce\\xb2\\r\\n\\r\\n.508\\r\\n.517\\r\\n.498\\r\\n.440\\r\\n.672\\r\\n.684\\r\\n.713\\r\\n.654\\r\\n.723\\r\\n.712\\r\\n.772\\r\\n.646\\r\\n.781\\r\\n.818\\r\\n\\r\\n.673\\r\\n.684\\r\\n.683\\r\\n.584\\r\\n.782\\r\\n.797\\r\\n.758\\r\\n.837\\r\\n.803\\r\\n.874\\r\\n.889\\r\\n.956\\r\\n\\r\\nTable 5: Comparison to the state-of-the-art unsupervised saliency detection methods on 3 salient object detection\\r\\nbenchmarks. For all metrics, higher number indicates better results. The best score per column is highlighted in bold. We\\r\\nobserve that S ELF M ASK yields improved performance over prior state-of-the-art approaches across all benchmarks.\\r\\n\\r\\nmaximal F\\r\\n\\r\\n0.90\\r\\n\\r\\nthe best salient mask.\\r\\n\\r\\nSelfMask\\r\\nSelfMask (UB)\\r\\n\\r\\n4.5. Broader Impact\\r\\n0.85\\r\\n\\r\\n0.80 5 10\\r\\n\\r\\n20\\r\\n\\r\\n50\\r\\nNumber of queries\\r\\n\\r\\nThis work contributes a new framework to deliver performant unsupervised salient object detection. As such, it\\r\\noffers the potential to underpin a range of societally beneficial applications that are bottlenecked by annotation costs.\\r\\nThese include improved low-cost medical image segmentation, crop measurement from aerial imagery, and wildlife\\r\\nmonitoring. However, low-cost segmentation is a powerful\\r\\ndual-use technology, and we caution against its deployment\\r\\nas a tool for unlawful surveillance and oppression.\\r\\n\\r\\n100\\r\\n\\r\\nFigure 4: Effect of number of queries on the performance of SelfMask on the HKU-IS dataset. The model\\xe2\\x80\\x99s\\r\\nprediction and its upper bound, denoted by SelfMask and\\r\\nSelfMask (UB) each, are shown.\\r\\n\\r\\n5. Conclusion\\r\\n\\r\\nlenging to appropriately rank the objectness of each prediction. For this reason, in the section that follows, we consider S ELF M ASK with 20 queries, and pick the query with\\r\\nthe highest objectness as our prediction during inference.\\r\\n\\r\\nIn this work, we address the challenging problem of unsupervised salient object detection (SOD). For this, we first\\r\\nobserve that self-supervised features exhibit significantly\\r\\ngreater object segmentation potential with spectral clustering than with k-means. Inspired by this observation, we extract foreground regions among multiple masks generated\\r\\nwith multiple types of features, and varying cluster numbers based on winner-takes-all voting. By using the selected\\r\\nmasks as pseudo-masks, we train a saliency detection network and show promising results compared to previous unsupervised methods on various SOD benchmarks.\\r\\n\\r\\n4.4. Comparison to state-of-the-art unsupervised\\r\\nsaliency detection methods\\r\\nTo compare with existing works on unsupervised SOD,\\r\\nwe evaluate on three popular SOD benchmarks in terms\\r\\nof Acc., IoU, and maxF\\xce\\xb2 . Following [63], we also report results after post-processing predictions with the bilateral solver [6]. As shown in Tab. 5, while the pseudomasks from spectral cluster voting already perform reasonably well compared to previous models, our self-trained\\r\\nmodel outperforms all existing approaches on all benchmarks. This suggests both that the model can learn to generalise effectively from noisy masks, and that the objectness\\r\\nscore trained with the ranking loss is effective for picking\\r\\n\\r\\nAcknowledgements. GS is supported by AI Factory, Inc.\\r\\nin Korea. WX is supported by Visual AI (EP/T028572/1).\\r\\nSA would like to thank Z. Novak and N. Novak for enabling\\r\\nhis contribution. GS would like to thank Jaesung Huh for\\r\\nproof-reading.\\r\\n8\\r\\n\\r\\n\\x0c\\n\\nReferences\\r\\n\\r\\n[20] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In\\r\\nICCV, 2015.\\r\\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\r\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\r\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\r\\nworth 16x16 words: Transformers for image recognition at\\r\\nscale. In ICLR, 2021.\\r\\n[22] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative\\r\\nunsupervised feature learning with exemplar convolutional\\r\\nneural networks. TPAMI, 2015.\\r\\n[23] Stas Goferman, Lihi Zelnik-Manor, and Ayellet Tal.\\r\\nContext-aware saliency detection. TPAMI, 2012.\\r\\n[24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\r\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\r\\nYoshua Bengio. Generative adversarial nets. In Advances in\\r\\nNeural Information Processing Systems, 2014.\\r\\n[25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\r\\nGirshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\\r\\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\r\\nDeep residual learning for image recognition. In CVPR,\\r\\n2016.\\r\\n[27] Phillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H\\r\\nAdelson. Learning visual groups from co-occurrences in\\r\\nspace and time. arXiv:1511.06811, 2015.\\r\\n[28] L. Itti, C. Koch, and E. Niebur. A model of saliency-based\\r\\nvisual attention for rapid scene analysis. TPAMI, 1998.\\r\\n[29] Tilke Judd, Krista Ehinger, Fre\\xcc\\x81do Durand, and Antonio Torralba. Learning to predict where humans look. In ICCV,\\r\\n2009.\\r\\n[30] Dominik A. Klein and Simone Frintrop. Center-surround\\r\\ndivergence of feature statistics for salient object detection.\\r\\nIn ICCV, 2011.\\r\\n[31] Gustav Larsson,\\r\\nMichael Maire,\\r\\nand Gregory\\r\\nShakhnarovich. Colorization as a proxy task for visual\\r\\nunderstanding. In CVPR, 2017.\\r\\n[32] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features. In CVPR, 2015.\\r\\n[33] Nianyi Li, Bilin Sun, and Jingyi Yu. A weighted sparse coding framework for saliency detection. In CVPR, 2015.\\r\\n[34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\\r\\nPiotr Dollar. Focal loss for dense object detection. In ICCV,\\r\\n2017.\\r\\n[35] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng,\\r\\nand Jianmin Jiang. A simple pooling-based design for realtime salient object detection. In CVPR, 2019.\\r\\n[36] Tie Liu, Jian Sun, Nan-Ning Zheng, Xiaoou Tang, and\\r\\nHeung-Yeung Shum. Learning to detect a salient object. In\\r\\nCVPR, 2007.\\r\\n[37] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129\\xe2\\x80\\x93137, 1982.\\r\\n[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\r\\nregularization. In ICLR, 2019.\\r\\n\\r\\n[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada,\\r\\nand Sabine Susstrunk. Frequency-tuned salient region detection. In CVPR, 2009.\\r\\n[2] Relja Arandjelovic\\xcc\\x81 and Andrew Zisserman. Object discovery\\r\\nwith a copy-pasting gan. arXiv:1905.11369, 2019.\\r\\n[3] Pablo Arbela\\xcc\\x81ez, Jordi Pont-Tuset, Jonathan Barron, Ferran Marques, and Jitendra Malik. Multiscale combinatorial\\r\\ngrouping. In CVPR, 2014.\\r\\n[4] Yuki Markus Asano, Christian Rupprecht, and Andrea\\r\\nVedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020.\\r\\n[5] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:\\r\\nBERT pre-training of image transformers. In ICLR, 2022.\\r\\n[6] Jonathan T. Barron and Ben Poole. The fast bilateral solver.\\r\\nIn ECCV, 2016.\\r\\n[7] Adam Bielski and Paolo Favaro. Emergence of object segmentation in perturbed generative models. In NeurIPS, 2019.\\r\\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\r\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In ECCV, 2020.\\r\\n[9] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\\r\\nMatthijs Douze. Deep clustering for unsupervised learning\\r\\nof visual features. In ECCV, 2018.\\r\\n[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In\\r\\nNeurIPS, 2020.\\r\\n[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve\\xcc\\x81 Je\\xcc\\x81gou,\\r\\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In\\r\\nICCV, 2021.\\r\\n[12] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\\r\\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\\r\\nseparable convolution for semantic image segmentation. In\\r\\nECCV, 2018.\\r\\n[13] Mickae\\xcc\\x88l Chen, Thierry Artie\\xcc\\x80res, and Ludovic Denoyer. Unsupervised object segmentation by redrawing. In NeurIPS,\\r\\n2019.\\r\\n[14] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Perpixel classification is not all you need for semantic segmentation. In NeurIPS, 2021.\\r\\n[15] Ming-Ming Cheng, Niloy J. Mitra, Xiaolei Huang, Philip\\r\\nH. S. Torr, and Shi-Min Hu. Global contrast based salient\\r\\nregion detection. TPAMI, 2015.\\r\\n[16] Ming-Ming Cheng, Niloy J. Mitra, Xiaolei Huang, Philip\\r\\nH. S. Torr, and Shi-Min Hu. Global contrast based salient\\r\\nregion detection. TPAMI, 2015.\\r\\n[17] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 1995.\\r\\n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\r\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\r\\ndatabase. In CVPR, 2009.\\r\\n[19] Yuanyuan Ding, Jing Xiao, and Jingyi Yu. Importance filtering for image retargeting. In CVPR, 2011.\\r\\n\\r\\n9\\r\\n\\r\\n\\x0c\\n\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\xc5\\x81 ukasz Kaiser, and Illia\\r\\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\\r\\n[59] Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan latent space. In\\r\\nICML, 2020.\\r\\n[60] Andrey Voynov, Stanislav Morozov, and Artem Babenko.\\r\\nObject segmentation without labels with large-scale generative models. In ICML, 2021.\\r\\n[61] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,\\r\\nDong Wang, Baocai Yin, and Xiang Ruan. Learning to detect salient objects with image-level supervision. In CVPR,\\r\\n2017.\\r\\n[62] Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen,\\r\\nHaibin Ling, and Ruigang Yang. Salient object detection\\r\\nin the deep learning era: An in-depth survey. TPAMI, 2021.\\r\\n[63] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James\\r\\nCrowley, and Dominique Vaufreydaz. Self-supervised transformers for unsupervised object discovery using normalized\\r\\ncut. In CVPR, 2022.\\r\\n[64] Yichen Wei, Fang Wen, Wangjiang Zhu, and Jian Sun.\\r\\nGeodesic saliency using background priors. In Andrew\\r\\nFitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato,\\r\\nand Cordelia Schmid, editors, ECCV, 2012.\\r\\n[65] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\r\\nUnsupervised feature learning via non-parametric instance\\r\\ndiscrimination. In CVPR, 2018.\\r\\n[66] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\\r\\nand Antonio Torralba. Sun database: Large-scale scene\\r\\nrecognition from abbey to zoo. In CVPR, 2010.\\r\\n[67] Shuai Xie, Zunlei Feng, Y. Chen, Songtao Sun, Chao Ma,\\r\\nand Ming-Li Song. Deal: Difficulty-aware active learning\\r\\nfor semantic segmentation. In ACCV, 2020.\\r\\n[68] Kaiming He Xinlei Chen, Saining Xie.\\r\\nAn empirical study of training self-supervised vision transformers.\\r\\narXiv:2104.02057, 2021.\\r\\n[69] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical\\r\\nsaliency detection. In CVPR, 2013.\\r\\n[70] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and\\r\\nMing-Hsuan Yang. Saliency detection via graph-based manifold ranking. In CVPR, 2013.\\r\\n[71] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\r\\nZi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng\\r\\nYan. Tokens-to-token vit: Training vision transformers from\\r\\nscratch on imagenet. In ICCV, 2021.\\r\\n[72] Xiaohang Zhan, Ziwei Liu, Ping Luo, Xiaoou Tang, and\\r\\nChen Loy. Mix-and-match tuning for self-supervised semantic segmentation. In AAAI, 2018.\\r\\n[73] Dingwen Zhang, Junwei Han, and Yu Zhang. Supervision by\\r\\nfusion: Towards unsupervised learning of deep salient object\\r\\ndetector. In ICCV, 2017.\\r\\n[74] Jianming Zhang and Stan Sclaroff. Exploiting surroundedness for saliency detection: A boolean map approach.\\r\\nTPAMI, 2016.\\r\\n[75] Jianming Zhang, Stan Sclaroff, Zhe Lin, Xiaohui Shen,\\r\\nBrian Price, and Radom\\xc4\\xb1\\xcc\\x81r Mech. Minimum barrier salient\\r\\nobject detection at 80 fps. In ICCV, 2015.\\r\\n\\r\\n[39] Ulrike Luxburg. A tutorial on spectral clustering. Statistics\\r\\nand Computing, 2004.\\r\\n[40] Aravindh Mahendran, James Thewlis, and Andrea Vedaldi.\\r\\nCross pixel optical-flow similarity for self-supervised learning. In ACCV, 2018.\\r\\n[41] Luca Marchesotti, Claudio Cifarelli, and Gabriela Csurka. A\\r\\nframework for visual saliency detection with applications to\\r\\nimage thumbnailing. In ICCV, 2009.\\r\\n[42] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\\r\\nAndrea Vedaldi. Finding an unsupervised image segmenter\\r\\nin each of your deep generative models. arXiv:2105.08127,\\r\\n2021.\\r\\n[43] F. Milletari, N. Navab, and S. Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3DV, 2016.\\r\\n[44] Vida Movahedi and James H. Elder. Design and perceptual\\r\\nvalidation of performance measures for salient object segmentation. In CVPR Workshops, 2010.\\r\\n[45] Muzammal Naseer, Kanchana Ranasinghe, Salman Khan,\\r\\nMunawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\\r\\nYang.\\r\\nIntriguing properties of vision transformers.\\r\\narXiv:2105.10497, 2021.\\r\\n[46] Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu Lou,\\r\\nand Thomas Brox. Deepusps: Deep robust unsupervised\\r\\nsaliency prediction via self-supervision. In NeurIPS, 2019.\\r\\n[47] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\\r\\nvisual representations by solving jigsaw puzzles. In ECCV,\\r\\n2016.\\r\\n[48] Yassine Ouali, Celine Hudelot, and Myriam Tami. Semisupervised semantic segmentation with cross-consistency\\r\\ntraining. In CVPR, 2020.\\r\\n[49] Federico Perazzi, Philipp Kra\\xcc\\x88henbu\\xcc\\x88hl, Yael Pritch, and\\r\\nAlexander Hornung. Saliency filters: Contrast based filtering\\r\\nfor salient region detection. In CVPR, 2012.\\r\\n[50] Michael Rubinstein, Ariel Shamir, and Shai Avidan. Improved seam carving for video retargeting. ACM Transactions on Graphics (SIGGRAPH), 2008.\\r\\n[51] Jianbo Shi and Jitendra Malik. Normalized cuts and image\\r\\nsegmentation. TPAMI, 2000.\\r\\n[52] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical\\r\\nimage saliency detection on extended cssd. TPAMI, 2015.\\r\\n[53] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical\\r\\nimage saliency detection on extended cssd. TPAMI, 2016.\\r\\n[54] Oriane Sime\\xcc\\x81oni, Gilles Puy, Huy V. Vo, Simon Roburin,\\r\\nSpyros Gidaris, Andrei Bursuc, Patrick Pe\\xcc\\x81rez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised\\r\\ntransformers and no labels. In BMVC, 2021.\\r\\n[55] Bongwon Suh, Haibin Ling, Benjamin B. Bederson, and\\r\\nDavid W. Jacobs. Automatic thumbnail cropping and its effectiveness. In ACM Symposium on User Interface Software\\r\\nand Technology, 2003.\\r\\n[56] Jin Sun and Haibin Ling. Scale and object aware image retargeting for thumbnail browsing. In ICCV, 2011.\\r\\n[57] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\\r\\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\\r\\ndata-efficient image transformers &; distillation through attention. In ICML, 2021.\\r\\n\\r\\n10\\r\\n\\r\\n\\x0c\\n\\n[76] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi,\\r\\nand Richard Hartley. Deep unsupervised saliency detection:\\r\\nA multiple noisy labeling perspective. In CVPR, 2018.\\r\\n[77] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao,\\r\\nJufeng Yang, and Ming-Ming Cheng. Egnet: Edge guidance\\r\\nnetwork for salient object detection. In ICCV, 2019.\\r\\n[78] Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun.\\r\\nSaliency optimization from robust background detection. In\\r\\nCVPR, 2014.\\r\\n\\r\\n11\\r\\n\\r\\n\\x0c\\n\\nAppendices\\r\\n\\r\\ndense feature maps FCNN \\xe2\\x88\\x88 Rh\\xc3\\x97w\\xc3\\x97D where h= Hs and\\r\\nw= W\\r\\ns with s denoting the total stride of the network and\\r\\nD denotes the dimensionality of the features. That is,\\r\\n\\r\\nIn this supplementary material, we first describe the algorithm for spectral clustering (Sec. A). Then, we briefly\\r\\nreview the overall structures of the convolution- and\\r\\ntransformer-based image encoders and how we extract\\r\\ndense features to which the spectral clustering is applied\\r\\nfrom each type of encoder (Sec. B). The evaluation metrics\\r\\nare described in Sec. C, and the full results for the comparison between k-means and spectral clustering with different cluster numbers, i.e., k = {2, 3, 4} on the three main\\r\\nsaliency benchmarks are shown in Sec. D. Lastly, we describe typical failure cases of our model in Sec. E.\\r\\n\\r\\nFCNN = \\xce\\xa6CNN (I) \\xe2\\x88\\x88 Rh\\xc3\\x97w\\xc3\\x97D\\r\\n\\r\\n(9)\\r\\n\\r\\nwhere the parameters for the CNNs are omitted for simplicity.\\r\\n\\r\\nB.2. Transformer-based visual encoder\\r\\nIn the recent literature, Transformer-based architectures\\r\\nhave shown tremendous success in the computer vision\\r\\ncommunity, including ViT [21], DeiT [57], T2T-ViT [71],\\r\\nand BEiT[5]. Generally speaking, these architectures consist of three components, namely, tokeniser (\\xce\\xa6TK ), linear\\r\\nprojection (\\xce\\xa6LP ), and transformer encoder (\\xce\\xa6TE ):\\r\\n\\r\\nA. Normalised spectral clustering algorithm\\r\\nHere, we describe the normalised spectral clustering algorithm used to generate pseudo-masks for our model in\\r\\nAlg. 1.\\r\\n\\r\\nFTransformer = \\xce\\xa6TE \\xe2\\x97\\xa6 \\xce\\xa6LP \\xe2\\x97\\xa6 \\xce\\xa6TK (I) \\xe2\\x88\\x88 Rh\\xc3\\x97w\\xc3\\x97D\\r\\n\\r\\n(10)\\r\\n\\r\\nwhere FTransformer denotes the dense features from a\\r\\ntransformer-based encoder.\\r\\n\\r\\nALGORITHM 1 Normalised spectral clustering [39, 51]\\r\\n\\r\\nTokeniser. Given an image as input, i.e. I \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x973 , the\\r\\nimage is first divided by a tokeniser into non-overlapping\\r\\npatches of a fixed size P \\xc3\\x97 P , ending up N patches per\\r\\nframe, i.e. N = HW\\r\\nP2 :\\r\\n\\r\\nInput: An adjacency matrix W \\xe2\\x88\\x88 RN \\xc3\\x97N , the number of clusters k to be constructed.\\r\\n1: Compute the degree matrix D with W.\\r\\n2: Compute the unnormalised Laplacian L using W and D using Eqn. 3.\\r\\n3: Compute the first k generalised eigenvectors u1 , . . . , uk of\\r\\nthe generalised eigen problem Lu = \\xce\\xbbDu.\\r\\n4: Let U \\xe2\\x88\\x88 RN \\xc3\\x97k be the matrix containing the vectors\\r\\nu1 , . . . , uk as the columns.\\r\\n5: For i = 1, . . . , N , let yi \\xe2\\x88\\x88 Rk be the vector corresponding\\r\\nto the i-th row of U.\\r\\n6: Cluster the vectors {yi | i = 1, ..., N } \\xe2\\x88\\x88 RN \\xc3\\x97k with kmeans into clusters C1 , . . . , Ck .\\r\\nOutput: Clusters C = {C1 , . . . , Ck }\\r\\n\\r\\n2\\r\\n\\r\\n\\xce\\xa6TK (I) = {xi | xi = \\xce\\xa6TK (I)i \\xe2\\x88\\x88 R3P , i = 1, ..., N }\\r\\n(11)\\r\\nwhere xi denotes the ith patch.\\r\\nLinear projection. Once tokenised, each patch from the\\r\\nimage is fed through a linear layer \\xce\\xa6LP and projected into a\\r\\nvector (a.k.a. token):\\r\\nzi = \\xce\\xa6LP (xi ) + PEi \\xe2\\x88\\x88 RD\\r\\n\\r\\nNote that the adjacency matrix W is computed using\\r\\nEqn. 2 of the main paper, given the dense features from a\\r\\nvisual encoder described next.\\r\\n\\r\\n2\\r\\n\\r\\nwhere xi \\xe2\\x88\\x88 R3P refers to the ith patch, and its corresponding learnable positional embeddings PEi \\xe2\\x88\\x88 RD are\\r\\nadded to the patch token \\xce\\xa6LP (xi ) \\xe2\\x88\\x88 RD . Then, the N\\r\\naugmented patch tokens are concatenated altogether with a\\r\\nclass token [CLS]\\xe2\\x88\\x88 RD , producing the final input form of\\r\\nR(N +1)\\xc3\\x97D to a sequence of transformer layers, described\\r\\nin the following.\\r\\n\\r\\nB. Visual encoder\\r\\nOur approach utilises image representations learned by\\r\\neither convolution-based or transformer-based architectures\\r\\nto which spectral clustering will be applied. Here, we first\\r\\nbriefly review how these feature representations are computed with each model.\\r\\n\\r\\nTransformer encoder. A transformer encoder is composed\\r\\nof multiple transformer layers, each of which is subdivided into a self-attention layer and multi-layer perceptrons\\r\\n(MLPs). The self-attention layer contains three learnable\\r\\nlinear layers, each of which takes the input tokens and outputs either key K, value V , or query Q of the same dimensionality as the input tokens, i.e., R(N +1)\\xc3\\x97D .3 Then,\\r\\n\\r\\nB.1. Convolution-based visual encoder\\r\\nConvolutional neural networks (CNNs) for image representations, denoted by \\xce\\xa6CNN , consist of a series of 2D convolutional layers and non-linear activation functions which\\r\\noperate on an image in a sliding window fashion. Specifically, given an image I \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x973 , the CNNs outputs\\r\\n\\r\\n3 In practice, we use a single linear layer which maps the D dimension\\r\\nof the input tokens to 3 \\xc3\\x97 D and equally splits them into Q, K, and V .\\r\\n\\r\\n12\\r\\n\\r\\n\\x0c\\n\\nModel\\r\\n\\r\\nArch.\\r\\n\\r\\nCluster.\\r\\n\\r\\nResNet [26]\\r\\nResNet [26]\\r\\nViT [21]\\r\\nViT [21]\\r\\n\\r\\nResNet50\\r\\nResNet50\\r\\nViT-S/16\\r\\nViT-S/16\\r\\n\\r\\nk-means\\r\\nspectral\\r\\nk-means\\r\\nspectral\\r\\n\\r\\nMoCov2 [25]\\r\\nMoCov2 [25]\\r\\nSwAV [10]\\r\\nSwAV [10]\\r\\nDINO [11]\\r\\nDINO [11]\\r\\nDINO [11]\\r\\nDINO [11]\\r\\n\\r\\nResNet50\\r\\nResNet50\\r\\nResNet50\\r\\nResNet50\\r\\nViT-S/8\\r\\nViT-S/8\\r\\nViT-S/16\\r\\nViT-S/16\\r\\n\\r\\nk-means\\r\\nspectral\\r\\nk-means\\r\\nspectral\\r\\nk-means\\r\\nspectral\\r\\nk-means\\r\\nspectral\\r\\n\\r\\nDUT-OMRON [70]\\r\\nDUTS-TE [61]\\r\\nk=2 k=3 k=4 avg. k=2 k=3 k=4\\r\\nFully-supervised features\\r\\n.311 .346 .355 .337 .345 .358 .360\\r\\n.258 .326 .346 .310 .297 .341 .343\\r\\n.335 .406 .440 .394 .349 .423 .460\\r\\n.268 .392 .481 .380 .260 .428 .511\\r\\nSelf-supervised features\\r\\n.334 .387 .403 .375 .401 .423 .422\\r\\n.311 .399 .453 .387 .403 .464 .496\\r\\n.356 .412 .429 .399 .415 .456 .462\\r\\n.346 .407 .450 .401 .412 .473 .488\\r\\n.299 .381 .427 .369 .299 .385 .447\\r\\n.315 .417 .463 .398 .311 .435 .486\\r\\n.314 .391 .426 .377 .325 .407 .444\\r\\n.310 .413 .459 .394 .324 .445 .483\\r\\n\\r\\navg.\\r\\n\\r\\nk=2\\r\\n\\r\\nECSSD [52]\\r\\nk=3\\r\\nk=4\\r\\n\\r\\navg.\\r\\n\\r\\n.354\\r\\n.327\\r\\n.411\\r\\n.400\\r\\n\\r\\n.461\\r\\n.424\\r\\n.505\\r\\n.402\\r\\n\\r\\n.445\\r\\n.454\\r\\n.560\\r\\n.613\\r\\n\\r\\n.425\\r\\n.432\\r\\n.562\\r\\n.637\\r\\n\\r\\n.444\\r\\n.437\\r\\n.542\\r\\n.551\\r\\n\\r\\n.415\\r\\n.454\\r\\n.444\\r\\n.458\\r\\n.377\\r\\n.411\\r\\n.392\\r\\n.417\\r\\n\\r\\n.507\\r\\n.602\\r\\n.548\\r\\n.594\\r\\n.497\\r\\n.527\\r\\n.507\\r\\n.528\\r\\n\\r\\n.511\\r\\n.642\\r\\n.552\\r\\n.606\\r\\n.566\\r\\n.616\\r\\n.557\\r\\n.609.\\r\\n\\r\\n.481\\r\\n.638\\r\\n.526\\r\\n.569\\r\\n.591\\r\\n.618\\r\\n.560\\r\\n.596\\r\\n\\r\\n.500\\r\\n.627\\r\\n.542\\r\\n.590\\r\\n.551\\r\\n.587\\r\\n.541\\r\\n.577\\r\\n\\r\\nTable 6: Comparison between k-means algorithm and spectral clustering with three different cluster sizes k on the three\\r\\nbenchmarks. IoU between a ground-truth mask and the closest prediction among k predicted masks is considered. On the\\r\\nfourth column of each benchmark, we report the average of the results from the different k. The higher average scores of\\r\\nk-means and spectral clusterings within the same model are in bold.\\r\\nT\\r\\n\\r\\nprevious work [62, 42, 77, 35], we set \\xce\\xb2 2 to 0.3, putting\\r\\nmore weight on precision. We use F\\xce\\xb2 to compute the\\r\\nmaximal-F\\xce\\xb2 , described next.\\r\\n\\r\\n\\xe2\\x88\\x9a\\r\\n, dim=the self-attention layer outputs softmax( QK\\r\\nD\\r\\n\\r\\n1)V \\xe2\\x88\\x88 R(N +1)\\xc3\\x97D .4 The outputs of the attention layer are\\r\\nfed to the following MLPs, which are composed of two linear layers with a non-linear activation between them and\\r\\noutput tokens with their shape preserved.\\r\\nNote that, as all transformer layers constituting the\\r\\ntransformer encoder share the identical architecture, the\\r\\nfinal outputs from the ViT have the same shape as the input\\r\\ntokens, i.e., R(N +1)\\xc3\\x97D . For image classification task, only\\r\\nthe [CLS] \\xe2\\x88\\x88 RD is taken from the outputs and fed to a\\r\\nlinear classifier. In our work, however, we consider the\\r\\npatch tokens FTransformer \\xe2\\x88\\x88 RN \\xc3\\x97D which are reshaped to\\r\\nW\\r\\nRh\\xc3\\x97w\\xc3\\x97D where h and w equal H\\r\\nP \\xc3\\x97 P . It is worth noting\\r\\nthat, the patch size P plays the role of total stride as in the\\r\\nCNNs.\\r\\n\\r\\n\\xe2\\x80\\xa2 maximal-F\\xce\\xb2 (maxF\\xce\\xb2 ) is a maximum score of F\\xce\\xb2\\r\\namong multiple masks binarised with different thresholds. Specifically, given a non-binarised mask prediction with its value between [0, 255], it computes F\\xce\\xb2\\r\\nfrom 255 binarised masks, each of which is thresholded by an integer among {0, ..., 254} and takes the\\r\\nmaximum F\\xce\\xb2 value for the result.\\r\\n\\xe2\\x80\\xa2 Intersection-over-union (IoU) is the size of overlapped\\r\\nforeground regions between a ground-truth G and a binarised mask prediction M divided by the total size of\\r\\nforeground regions from G and M .\\r\\n\\xe2\\x80\\xa2 Accuracy (Acc) is a metric that measures pixel-wise\\r\\naccuracy based on a ground-truth mask G and a binarised mask prediction M :\\r\\n\\r\\nC. Descriptions of evaluation metrics\\r\\nIn the following, we describe the metrics used for evaluation:\\r\\n\\r\\nAcc =\\r\\n\\r\\n\\xe2\\x80\\xa2 F\\xce\\xb2 [49] is the harmonic mean of precision and recall\\r\\nbetween a ground-truth G \\xe2\\x88\\x88 {0, 1}H\\xc3\\x97W and a binarised mask M \\xe2\\x88\\x88 {0, 1}H\\xc3\\x97W :\\r\\n\\r\\nH X\\r\\nW\\r\\nX\\r\\n1\\r\\n\\xce\\xb4G ,M\\r\\nH \\xc3\\x97 W i=1 j=1 ij ij\\r\\n\\r\\n(13)\\r\\n\\r\\nwhere \\xce\\xb4 denotes the Kroneker-delta.\\r\\n\\r\\n2\\r\\n\\r\\nF\\xce\\xb2 =\\r\\n\\r\\n(1 + \\xce\\xb2 )Precision \\xc3\\x97 Recall\\r\\n,\\r\\n\\xce\\xb2 2 Precision + Recall\\r\\n\\r\\n(12)\\r\\n\\r\\nD. Comparison between k-means and spectral\\r\\nclustering\\r\\n\\r\\nwhere \\xce\\xb2 2 denotes a weight of precision.5 Following\\r\\n4 Here, we consider the single head case for simplicity.\\r\\n\\r\\nIn Sec. 4.3 of the main paper, we show the performance\\r\\nof k-means and spectral clustering applied to different architectures (i.e., ResNet50 and ViT-S/{8, 16}) and features (i.e., fully- and self-superivsed features) averaged over\\r\\n\\r\\nFor more details,\\r\\n\\r\\nplease refer to the original paper [58].\\r\\ntp\\r\\ntp\\r\\n5 Precision=\\r\\nand Recall = tp+f\\r\\nwhere tp, f p, and f n reptp+f p\\r\\nn\\r\\nresent true-positive, false-positive, and false-negative, respectively.\\r\\n\\r\\n13\\r\\n\\r\\n\\x0c\\n\\nFigure 5: Sample visualisations for typical prediction failures from our model on the DUT-OMRON [70] and DUTS-TE [61]\\r\\nbenchmarks. From left to right, input image, ground-truth mask, a pseudo-mask, and a predicted mask by our model are\\r\\nshown. The respective salient regions are highlighted in red. Best viewed in colour. Please zoom in for details.\\r\\n\\r\\nk={2, 3, 4} on the three saliency datasets. Here, we show\\r\\nthe full results for each k in Tab. 6. For the description,\\r\\nplease refer to Sec. 4.3 of the main paper.\\r\\n\\r\\nE. Visualisation of failure cases\\r\\nIn Fig. 5, we visualise some failure predictions from\\r\\nour model on the DUT-OMRON [70] and DUTS-TE [61]\\r\\ndatasets.\\r\\nWe notice there are two typical failure cases. First, when\\r\\na salient object is of small scale, the model tends to undersegment it and prefers the large salient object. For instance,\\r\\nas shown by the top left example in Fig 5, the whole bed\\r\\nis segmented, rather than the pillow; Second, when there\\r\\nare more than one salient region in the image, our model\\r\\nmay only segment one of them. For example, as shown by\\r\\nthe middle right example in Fig 5, both screen and seats\\r\\ncan be thought of as a salient region while the model only\\r\\nhighlights only the latter. We conjecture that these cases\\r\\nare caused by a bias of the dataset (i.e., DUTS-TR [61])\\r\\non which the model is trained. That is, the training images\\r\\nlikely to contain large salient regions composed of either an\\r\\nobject or objects sharing a semantic meaning, thus discouraging the model from predicting a small salient region or\\r\\nmore than one object with different semantics even if all the\\r\\nobjects can be regarded salient.\\r\\n\\r\\n14\\r\\n\\r\\n\\x0c', b'A Hybrid Mesh-neural Representation for 3D Transparent Object\\r\\nReconstruction\\r\\n\\r\\narXiv:2203.12613v1 [cs.CV] 23 Mar 2022\\r\\n\\r\\nJIAMIN XU, State Key Lab of CAD&CG, Zhejiang University, China\\r\\nZIHAN ZHU, State Key Lab of CAD&CG, Zhejiang University, China\\r\\nHUJUN BAO, State Key Lab of CAD&CG, Zhejiang University, China\\r\\nWEIWEI XU\\xe2\\x88\\x97 , State Key Lab of CAD&CG, Zhejiang University, China\\r\\n\\r\\nFig. 1. Our reconstruction results paired with their rendering for three transparent objects. Their fine surface details can be well reconstructed by our method\\r\\nusing hand-held captured images under natural light conditions.\\r\\nWe propose a novel method to reconstruct the 3D shapes of transparent\\r\\nobjects using hand-held captured images under natural light conditions. It\\r\\ncombines the advantage of explicit mesh and multi-layer perceptron (MLP)\\r\\nnetwork, a hybrid representation, to simplify the capture setting used in\\r\\nrecent contributions. After obtaining an initial shape through the multi-view\\r\\nsilhouettes, we introduce surface-based local MLPs to encode the vertex\\r\\ndisplacement field (VDF) for the reconstruction of surface details. The design of local MLPs allows to represent the VDF in a piece-wise manner\\r\\nusing two layer MLP networks, which is beneficial to the optimization algorithm. Defining local MLPs on the surface instead of the volume also\\r\\nreduces the searching space. Such a hybrid representation enables us to\\r\\nrelax the ray-pixel correspondences that represent the light path constraint\\r\\nto our designed ray-cell correspondences, which significantly simplifies\\r\\nthe implementation of single-image based environment matting algorithm.\\r\\nWe evaluate our representation and reconstruction algorithm on several\\r\\n\\xe2\\x88\\x97 Corresponding\\r\\n\\r\\nauthor\\r\\n\\r\\nAuthors\\xe2\\x80\\x99 addresses: Jiamin Xu, superxjm@yeah.net, State Key Lab of CAD&CG,\\r\\nZhejiang University, China; Zihan Zhu, zihan.zhu@zju.edu.cn, State Key Lab of\\r\\nCAD&CG, Zhejiang University, China; Hujun Bao, bao@cad.zju.edu.cn, State Key\\r\\nLab of CAD&CG, Zhejiang University, China; Weiwei Xu, xww@cad.zju.edu.cn, State\\r\\nKey Lab of CAD&CG, Zhejiang University, China.\\r\\nPermission to make digital or hard copies of all or part of this work for personal or\\r\\nclassroom use is granted without fee provided that copies are not made or distributed\\r\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\r\\non the first page. Copyrights for components of this work owned by others than ACM\\r\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\r\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\r\\nfee. Request permissions from permissions@acm.org.\\r\\n\\xc2\\xa9 2022 Association for Computing Machinery.\\r\\n0730-0301/2022/3-ART $15.00\\r\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\r\\n\\r\\ntransparent objects with ground truth models. Our experiments show that\\r\\nour method can produce high-quality reconstruction results superior to\\r\\nstate-of-the-art methods using a simplified data acquisition setup.\\r\\nCCS Concepts: \\xe2\\x80\\xa2 Computing methodologies \\xe2\\x86\\x92 3D Reconstruction, Differentiable Rendering, Neural network.\\r\\nAdditional Key Words and Phrases: Transparent Object, 3D Reconstruction,\\r\\nNeural Rendering\\r\\nACM Reference Format:\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu. 2022. A Hybrid Meshneural Representation for 3D Transparent Object Reconstruction. ACM\\r\\nTrans. Graph. 1, 1 (March 2022), 12 pages. https://doi.org/10.1145/nnnnnnn.\\r\\nnnnnnnn\\r\\n\\r\\n1\\r\\n\\r\\nINTRODUCTION\\r\\n\\r\\nThe acquisition of 3D models is a problem that frequently occurs\\r\\nin computer graphics and computer vision. Most existing methods,\\r\\nsuch as laser scanning and multi-view reconstruction, are based\\r\\non surface color observation. Consequently, the surface is assumed\\r\\nas opaque and approximately Lambertian. These methods cannot\\r\\nbe directly applied to transparent objects because the appearance\\r\\nof a transparent object is indirectly observed due to the complex\\r\\nrefraction and reflection light paths at the interface between air and\\r\\ntransparent materials.\\r\\nA core technical challenge in 3D transparent object reconstruction is handling the dramatic appearance change when observing\\r\\nan object in a multi-view setting. Slight changes in an object\\xe2\\x80\\x99s shape\\r\\ncan lead to non-local changes in appearance due to the complexity\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n2 \\xe2\\x80\\xa2\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nof light paths. To address this issue, ray-pixel correspondence, (i.e.,\\r\\nthe correspondence between a camera ray and a pixel on a static\\r\\nbackground pattern displayed on a monitor), and ray-ray correspondence, (i.e., the correspondence between a camera ray and the\\r\\nincident ray from the background pattern), are utilized to provide\\r\\nlight path constraints for facilitating 3D transparent object reconstruction [Ihrke et al. 2010; Kutulakos and Steger 2008; Wu et al.\\r\\n2018]. A differentiable refraction-tracing technique can be applied\\r\\nto reduce the complexity of the capture setting, and 3D shape can\\r\\nbe recovered through ray-pixel correspondences as shown in [Lyu\\r\\net al. 2020]. In this method, however, a transparent object should\\r\\nbe placed on a turntable under controlled lighting condition. Li et\\r\\nal. [2020] trained a physical-based neural network to handle complex light paths for 3D transparent objects. The network was trained\\r\\non a synthetic dataset with a differentiable path racing rendering\\r\\ntechnique. This method optimizes surface normals in latent space,\\r\\nand thus, it can reconstruct 3D transparent objects under natural\\r\\nlighting conditions when receiving an environment map and a few\\r\\nimages as input. However, it frequently produces overly smooth\\r\\nreconstruction results.\\r\\nIn this paper, we study how to combine the advantages of explicit\\r\\nmesh and multi-layer perceptron (MLP) network, a hybrid representation, to address the transparent object reconstruction problem\\r\\nunder natural lighting conditions using hand-held captured images.\\r\\nSuch representation can be reconstructed through optimization with\\r\\na differentiable path tracing rendering technique. Its key idea is to\\r\\nuse MLP to encode a vertex displacement field (VDF) defined on\\r\\na base mesh for surface details reconstruction, wherein the base\\r\\nmesh is created using multi-view silhouette images. Our design is\\r\\nmotivated by two observations. First, the MLP network parameterizes VDF with weight parameters. It can constrain the change\\r\\nin VDF in a global manner. Representing functions with MLP has\\r\\nbeen demonstrated to be efficient in optimization and robust to\\r\\nnoise [Mildenhall et al. 2020; Oechsle et al. 2021; Yariv et al. 2020].\\r\\nSecond, defining the MLP-parameterized VDF on the base mesh\\r\\nreduces the search space during optimization [Chen et al. 2018b]. It\\r\\nsignificantly accelerates the optimization process, compared with\\r\\nMLP-based volumetric representation.\\r\\nThe advantage of our hybrid representation is that it allows the\\r\\nrelaxation of the capture setting. The ray-pixel correspondence\\r\\nrequired in the optimization can be significantly relaxed to be a raycell correspondence in our pipeline. Therefore, we can simplify the\\r\\nbackground pattern design and develop a robust single image environment matting (EnvMatt) algorithm for handling images captured\\r\\nunder natural lighting conditions. Moreover, we propose representing VDF with a small number of local MLPs. Each MLP is responsible\\r\\nfor encoding a local VDF. This strategy enables us to design smallscale MLPs for accelerating optimization further. A fusion module is\\r\\ndesigned to disperse the gradient information of displacement vectors of vertices to their neighboring local MLPs. This module helps\\r\\nmaintain the global constraint of VDF and produces high-quality\\r\\nreconstruction results.\\r\\nIn summary, the technical contributions of our work include:\\r\\n\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\xe2\\x80\\xa2 A hybrid representation that employs explicit mesh and localMLP based functions to represent the details surface for transparent objects. It enables us to design small-scale MLPs to accelerate our optimization algorithm\\xe2\\x80\\x99s convergence and achieve\\r\\nhigh-quality 3D reconstruction results for transparent objects.\\r\\n\\xe2\\x80\\xa2 A ray-cell correspondence as a relaxed representation of the\\r\\nlight path constraint. The ray-cell correspondence is easier to\\r\\ncapture, leading to a simplified capture setting under natural\\r\\nlight conditions. Furthermore, it also eases the implementation\\r\\nof the environment matting algorithm.\\r\\nExperimental results show that our method can produce 3D models with details for a variety of transparent objects, as illustrated\\r\\nin Fig. 1. With our simplified capture setting under natural light\\r\\nconditions, our reconstruction results are superior to those of stateof-the-art 3D reconstruction algorithms for transparent objects.\\r\\n\\r\\n2\\r\\n\\r\\nRELATED WORK\\r\\n\\r\\nOur algorithm is built on the basis of considerable previous research.\\r\\nHere, we review the literature that is most related to our work, including transparent object reconstruction, differentiable rendering,\\r\\nand environment matting.\\r\\n\\r\\n2.1\\r\\n\\r\\nTransparent Object Reconstruction\\r\\n\\r\\nMany transparent object reconstruction techniques utilize special\\r\\nhardware setups, including polarization [Cui et al. 2017; Huynh et al.\\r\\n2010; Miyazaki and Ikeuchi 2005], time-of-flight camera [Tanaka\\r\\net al. 2016], tomography [Trifonov et al. 2006], a moving point light\\r\\nsource [Chen et al. 2006; Morris and Kutulakos 2007] and light field\\r\\nprobes[Wetzstein et al. 2011]. Our algorithm is most closely related\\r\\nto shape-from-distortion and light path triangulation. Kutulakos\\r\\nand Steger [2008] formulated the reconstruction of a refractive or\\r\\nmirror-like surface as a light path triangulation problem. Given a\\r\\nfunction that maps each point in the image onto a 3D \\xe2\\x80\\x9creference\\r\\npoint\\xe2\\x80\\x9d that is indirectly projected onto it, the authors characterized\\r\\nthe set of reconstructible cases that depended only on the number\\r\\nof points along a light path. The mapping function can be estimated\\r\\nusing the Environment Matting (EnvMatt) algorithm (Sec. 2.3) with\\r\\na calibrated acquisition setup, denoted as ray-point correspondences.\\r\\nA ray-ray correspondence can be uniquely determined with two\\r\\ndistinct reference points along the same ray.\\r\\nIn accordance with light path triangulation, one reconstructible\\r\\ncase is that of single refraction surfaces [Schwartzburg et al. 2014;\\r\\nShan et al. 2012; Yue et al. 2014], particularly fluid surfaces [Morris\\r\\nand Kutulakos 2011; Qian et al. 2017; Zhang et al. 2014]. Another\\r\\ntractable case is that of transparent objects when the rays undergo\\r\\nrefraction two times. For these transparent objects, Tsai et al. [2015]\\r\\nshowed that ambiguity exists between depth and surface normal,\\r\\ngiving a single ray-ray correspondence. Such ambiguity can be\\r\\nreduced by introducing multi-view ray-ray correspondences or enforcing shape models. [Chari and Sturm 2013] proposed the use of\\r\\nradiometric cues to reduce the number of views. Wu et al. [2018]\\r\\nrecently reconstructed the full shape of a transparent object by first\\r\\nextracting ray-ray correspondences refracted that were only two\\r\\ntimes as in [Qian et al. 2016] and then performing separate optimization and multi-view fusion. Lyu et al. [2020] proposed to extract\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n\\r\\nSurface\\r\\nMLPs\\r\\n\\r\\nClusters\\r\\n\\r\\nInitial\\r\\nshape\\r\\n\\r\\n\\xf0\\x9d\\x9c\\xb8(x)\\r\\n\\r\\n\\xf0\\x9d\\x9c\\xb9x\\r\\n\\r\\n\\xf0\\x9d\\x9c\\xb8(x)\\r\\n\\r\\n\\xf0\\x9d\\x9c\\xb9x\\r\\n\\r\\nVSA\\r\\n\\r\\nReconstructed\\r\\nobject\\r\\n\\r\\nx + \\xf0\\x9d\\x9c\\xb9x\\r\\n\\r\\n...\\r\\n\\r\\n...\\r\\n\\r\\nIDR\\r\\n\\r\\nFusion\\r\\nlayer\\r\\n\\r\\n\\xe2\\x80\\xa2 3\\r\\n\\r\\nSilhouette loss\\r\\n\\r\\nSurface optimization\\r\\n\\r\\n...\\r\\n\\r\\nRGB loss, correspondence loss\\r\\n\\r\\nFully connection\\r\\n\\r\\nInput captured images\\r\\nScene mesh\\r\\n\\r\\nRandom patterns\\r\\n\\r\\nFig. 2. The pipeline of our approach.\\r\\n\\r\\nper-view ray-point correspondences by using the EnvMatt algorithm in [Zongker et al. 1999], and utilize differentiable rendering\\r\\nfor progressively optimizing an initial mesh.\\r\\nIn addition to optimization-based methods, deep learning techniques can also be incorporated to resolve depth-normal ambiguity.\\r\\nStets et al. [2019] and Sajjan et al. [2020] proposed the use of a\\r\\npre-trained encoder-decoder network to estimate mask, depth, and\\r\\nnormals from a single image. Li et al. [2020] suggested performing optimization in feature space to obtain surface normals. Subsequently, they conducted multi-view feature mapping and 3D point\\r\\ncloud reconstruction to obtain a 3D shape by using the estimated\\r\\nnormals, total reflection masks, and rendering errors as inputs. With\\r\\na pre-trained network as the prior, their method can work on unconstrained natural images without background patterns. However,\\r\\ntheir reconstructed transparent object may lose some details due to\\r\\nthe domain gap between real-world images and synthetic training\\r\\ndata.\\r\\n\\r\\n2.2\\r\\n\\r\\nDifferentiable Rendering\\r\\n\\r\\nIn accordance with the simulation level of light transport, differentiable rendering algorithms in computer graphics can be roughly\\r\\ndivided into three categories: differentiable rasterization [Kato et al.\\r\\n2018; Laine et al. 2020; Li et al. 2018a; Liu et al. 2019; Loper and\\r\\nBlack 2014], differential volumetric rendering [Oechsle et al. 2021;\\r\\nWang et al. 2021; Yariv et al. 2021, 2020], and differentiable raytracing [Bangaru et al. 2020; Li et al. 2018b, 2015; Luan et al. 2020;\\r\\nNimier-David et al. 2019; Zhang et al. 2020, 2019]. Differentiable\\r\\nrasterization can be used to optimize mesh itself, or features and\\r\\nthe neural network parameters defined on the mesh. Differentiable\\r\\nvolumetric rendering can be used to optimize implicit shape representations such as implicit occupancy function [Chen and Zhang\\r\\n2019; Mescheder et al. 2019], signed distance function (SDF) [Park\\r\\net al. 2019; Yariv et al. 2020], and unsigned distance function [Atzmon and Lipman 2020]. Differentiable rendering was also used\\r\\nto optimize for deep surface light fields [Chen et al. 2018b]. This\\r\\nmethod represents per-vertex view-dependent reflections using an\\r\\n\\r\\nMLP. While we also utilize the surface-based MLPs, our focus is\\r\\ndifferent: our method employs local MLPs to represent VDF locally\\r\\nto reconstruct surface details and design a fusion layer to avoid\\r\\ndiscontinuities at the overlapped surface areas.\\r\\nConsidering that a light path with refraction is determined by\\r\\nthe front and back surfaces of a transparent object, the geometry\\r\\ncan be optimized in an iterative way with forward ray-tracing and\\r\\nbackward gradient propagation. To this end, our algorithm exploits\\r\\ndifferential ray-tracing to handle the light path of reflected and\\r\\nrefracted rays on the surface of transparent objects.\\r\\n\\r\\n2.3\\r\\n\\r\\nEnvironment Matting\\r\\n\\r\\nEnvironment matting (EnvMatt) which captures how an object refracts and reflects environment light, can be viewed as an extension\\r\\nof alpha matting [Levin et al. 2007; Porter and Duff 1984]. Imagebased refraction and reflection are represented as pixel-texel (texture\\r\\npixel) correspondences, while environments are represented as texture maps. The seminal work of Zongker et al. [1999] extracted\\r\\nthe EnvMatt from a series of 1D Gray codes, with the assumption\\r\\nthat each pixel was only related to a rectangular background region. Chuang et al. [2000] extended the work to recover a more\\r\\naccurate model at the expense of using additional structured light\\r\\nbackdrops. They also proposed a simplified EnvMatt algorithm by\\r\\nusing only a single backdrop. A pixel-texel correspondence search\\r\\ncan also be performed in the wavelet [Peers and Dutr\\xc3\\xa9 2003] and\\r\\nfrequency [Qian et al. 2015] domains. The number of required patterns can be reduced when combining them with a compressive\\r\\nsensing technique [Duan et al. 2015]. Chen et al. [2018a] recently\\r\\npresented a deep learning framework called TOM-Net for estimating\\r\\nthe EnvMatt as a refractive flow field. The aforementioned methods\\r\\nrequire images to be captured under controlled lighting condition,\\r\\ne.g., a dark room, to avoid the influence of ambient light. Wexler\\r\\net al. [2002] developed an EnvMatt algorithm for handling natural\\r\\nscene background. However, their method needs to capture a set of\\r\\nimages by using a fixed camera and a moving background.\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n4\\r\\n\\r\\n\\xe2\\x80\\xa2\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nFig. 3. Captured images.\\r\\n\\r\\n3\\r\\n\\r\\nOVERVIEW\\r\\n\\r\\nOur transparent object reconstruction pipeline is illustrated in Fig. 2.\\r\\nThis pipeline starts with reconstructing an object\\xe2\\x80\\x99s rough shape\\r\\n(initial shape) from a collection of multi-view silhouettes. Instead of\\r\\nthe space carving method [Kutulakos and Seitz 2000], we utilize the\\r\\nMLP-based signed distance function (SDF) in IDR [Yariv et al. 2020]\\r\\nto obtain a smooth initial shape as shown in Fig. 2. Afterward, we\\r\\nemploy the MLP network to represent the vertex displacement field\\r\\n(VDF) on that initial shape to reconstruct surface details. This hybrid\\r\\nsurface representation combines explicit mesh and MLP-based neural networks. In the following, we detail the hybrid representation\\r\\nand the optimization algorithm for reconstructing the representation from multi-view images.\\r\\nHybrid Representation: We choose to encode the surface details\\r\\nwith VDF as it is defined on a 2D manifold instead of the entire\\r\\n3D space, reducing the search space of the optimization algorithm\\r\\nand producing high-quality reconstruction results. Moreover, we\\r\\nuse MLPs to represent the displacement field defined on vertices\\r\\nto ease optimization. Such hybrid representation can combine explicit vertex optimization to accelerate convergence and MLP-based\\r\\nneural representation as in IDR [Yariv et al. 2020] to enforce global\\r\\nconstraints among vertices, improving the robustness of the optimization.\\r\\nRather than encoding VDF using a single MLP, we found that\\r\\nrepresenting the field with a couple of small local MLPs can achieve\\r\\nbetter results. As shown in Fig. 2, each local MLP encodes the displacement vectors of vertices within one cluster extracted from the\\r\\nmesh of initial shape using the variational shape approximation\\r\\n(VSA) algorithm [Cohen-Steiner et al. 2004]. To avoid mesh discontinuities across local MLPs, we also add a fusion layer to blend the\\r\\ndisplacement vectors of neighboring vertices based on the geodesic\\r\\ndistances on mesh [Crane et al. 2017].\\r\\nOptimization for VDF: The VDF is optimized based on the multibounce (up to two-bounces) light path constraints and the consistency between the rendering of our representation and the captured\\r\\nRGB images. The rendering procedure is performed via a recursive\\r\\ndifferentiable path tracing algorithm [Li 2019].\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nThe light path constraint due to multi-bounce refraction is approximated by a mapping function that maps each pixel in the input\\r\\nimage onto a pixel of the background pattern image, which can\\r\\nbe obtained using environment matting (EnvMatt) algorithm. We\\r\\nstore the background image as a texture. However, we found that\\r\\nthe traditional EnvMatt algorithms are either restricted to using\\r\\nmultiple images with a fixed camera or sensitive to natural light\\r\\nconditions. Consequently, we design a grid-based background pattern to establish the correspondence between a foreground pixel p\\r\\nthat covers a small part of the object surface and a cell of the grid.\\r\\nSuch a correspondence is recorded as a tuple \\xe2\\x9f\\xa8p, u\\xe2\\x9f\\xa9, where u is the\\r\\npixel coordinate of the cell center on the background image. In this\\r\\nmanner, the mapping function is simplified but remains efficient\\r\\nin providing information of the light path constraint to facilitate\\r\\noptimization.\\r\\nIn the remainder of this paper, we first describe our data preprocessing steps (Sec. 4.1), including our image acquisition setup\\r\\nand grid-based single image environment matting algorithm. Then,\\r\\nwe present the details of the initial shape reconstruction (Sec. 4.2)\\r\\nand the surface optimization steps (Sec. 4.3).\\r\\n\\r\\n4 METHOD\\r\\n4.1 Pre-processing\\r\\nData Acquisition: We capture images by using a Canon EOS 60D\\r\\ndigital single-lens reflex camera. As shown in Fig. 3, the transparent\\r\\nobject to be captured is placed on a desk with pre-printed AprilTags [Olson 2011] underneath. The AprilTags are used to facilitate\\r\\nimage registration. To capture the ray-cell correspondences for\\r\\nenvironment matting, we place an iPad as a monitor behind the\\r\\ntransparent object to display a grid-based background pattern. The\\r\\ndisplayed pattern P\\xf0\\x9d\\x91\\x96 for the \\xf0\\x9d\\x91\\x96th image is changed after capturing\\r\\nevery four images. We also move the position of the iPad manually\\r\\nafter capturing 60 images in our implementation. This setting is designed to incorporate more valid ray-cell correspondences to cover\\r\\nmore surface areas. All the patterns are pre-generated and used\\r\\nin the EnvMatt step. In addition to the images with a background\\r\\npattern, we capture more images without background patterns to\\r\\nprovide further constraints in RGB space. During capturing, the\\r\\niPad is moved two to four times, and the total number of captured\\r\\nimages ranges from 154 to 302, which is related to the complexity\\r\\nof the object\\xe2\\x80\\x99s surface shape.\\r\\nGrid-based Single Image Environment Matting: Similar to [Chuang\\r\\net al. 2000], we can assume that a transparent object has no intrinsic\\r\\ncolor. Therefore, the correspondences between pixels that cover\\r\\nthe object\\xe2\\x80\\x99s surface and pixels on the background pattern can be\\r\\ncalculated through nearest searching in color space. However, we\\r\\nfound that the color ramp pattern in [Chuang et al. 2000] is sensitive\\r\\nto ambient light under natural light conditions due to the invertible\\r\\nand smoothness properties of the pattern image in RGB space. In\\r\\nlight of this, we design grid-based background patterns. The color\\r\\nof each grid cell is constant, designed to create sharp boundaries\\r\\nbetween cells. As shown in Fig. 4, each pattern consists of a 7 \\xc3\\x97 7\\r\\ncheckboard with two different colors {c\\xe2\\x84\\x8e\\xf0\\x9d\\x91\\x96 |\\xf0\\x9d\\x91\\x96 \\xe2\\x88\\x88 {1, 2}}. Then, we randomly sample five cells on the grid and move each cell with random\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n(1,0)\\r\\n\\r\\n(0,0)\\r\\n\\r\\nnone\\r\\n\\r\\n\\xf0\\x9d\\x90\\xae4\\r\\n\\xf0\\x9d\\x90\\xae5\\r\\n\\r\\ninf\\r\\n\\xf0\\x9d\\x90\\xae5\\r\\n\\r\\n\\xf0\\x9d\\x90\\xae1\\r\\n\\r\\n(a) Input image\\r\\n(crop)\\r\\n\\r\\n\\xf0\\x9d\\x90\\x9c\\xe2\\x84\\x8e1\\r\\n\\xf0\\x9d\\x90\\x9c\\xe2\\x84\\x8e2\\r\\n\\r\\n\\xf0\\x9d\\x90\\xae2\\r\\n\\r\\n(b) Correspondences\\r\\n\\r\\n(0,1)\\r\\n\\r\\n(c) Designed\\r\\npattern\\r\\n\\r\\n\\xf0\\x9d\\x90\\x9c\\xf0\\x9d\\x91\\xa01\\r\\n\\xf0\\x9d\\x90\\x9c\\xf0\\x9d\\x91\\xa02\\r\\n\\xf0\\x9d\\x90\\x9c\\xf0\\x9d\\x91\\xa03\\r\\n\\xf0\\x9d\\x90\\x9c\\xf0\\x9d\\x91\\xa04\\r\\n\\xf0\\x9d\\x90\\x9c\\xf0\\x9d\\x91\\xa05\\r\\nSalient\\r\\ncolors\\r\\n\\r\\n\\xf0\\x9d\\x90\\xae4\\r\\n\\xf0\\x9d\\x90\\xae3\\r\\n\\r\\n\\xf0\\x9d\\x90\\xae3\\r\\n\\r\\n(1,1)\\r\\n\\r\\nChessboard\\r\\ncolors\\r\\n\\r\\n(d) Colors\\r\\n\\r\\nFig. 4. Grid-based single image EnvMatt procedure. (a) Input image. (b)\\r\\nEnvMatt results: colored pixels indicate that their traced rays terminate\\r\\ninside the cells with designed salient colors; black pixels indicate that the\\r\\nrays terminate inside cells with checkboard colors, and gray pixels indicates\\r\\nthat rays terminate outside the pattern. (c) The designed pattern. The circles\\r\\nindicate the centers of salient cells. (d) Chosen colors for salient cells and\\r\\ncheckboard cells.\\r\\n\\r\\noffset. These moved cells are assigned with five distinct salient colors {c\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\xa0 |\\xf0\\x9d\\x91\\x96 \\xe2\\x88\\x88 {1..5}} as shown in the rightmost column of Fig. 4. We\\r\\nchoose the five salient colors because they have sufficient mutual\\r\\ndistance in RGB space, making them more robust to the influence of\\r\\nenvironment lighting. These salient cells are used to calculate the\\r\\ncorrespondence tuple.\\r\\nOur EnvMatt algorithm calculates ray-cell correspondences for\\r\\neach pixel. Given a captured image, for each pixel p with color cp ,\\r\\nits corresponding cell center u can be calculated as follows:\\r\\n\\x12\\r\\n\\r\\n\\xef\\xa3\\xb1\\r\\n\\xef\\xa3\\xb4\\r\\n\\xef\\xa3\\xb4\\r\\n\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9f argmin \\xe2\\x88\\xa5c\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\xa0 \\xe2\\x88\\x92 cp \\xe2\\x88\\xa5\\r\\n\\xef\\xa3\\xb4\\r\\n\\xef\\xa3\\xb4\\r\\n\\xef\\xa3\\xb4\\r\\n\\xf0\\x9d\\x91\\x96\\r\\n\\xef\\xa3\\xb2\\r\\n\\xef\\xa3\\xb4\\r\\nu = \\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x93\\r\\n\\xef\\xa3\\xb4\\r\\n\\xef\\xa3\\xb4\\r\\n\\xef\\xa3\\xb4\\r\\n\\xef\\xa3\\xb4\\r\\n\\xef\\xa3\\xb4\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x92\\r\\n\\xef\\xa3\\xb4\\r\\n\\xef\\xa3\\xb3\\r\\n\\r\\n\\x13\\r\\n\\r\\n\\xe2\\x80\\xa2 5\\r\\n\\r\\nFig. 5. Scene mesh and pattern planes. The iPad is moved two times. During\\r\\nimage registration step, they will be registered to the same global coordinates.\\r\\n\\r\\nSpace carving\\r\\n\\r\\nIDR (silhouette loss)\\r\\n\\r\\nFig. 6. Space carving vs. IDR with silhouette loss. The space carving method\\r\\nproduces artifact in the occluded areas (indicated by the red arrow).\\r\\n\\r\\nif min \\xe2\\x88\\xa5c\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\xa0 \\xe2\\x88\\x92 cp \\xe2\\x88\\xa5 < \\xf0\\x9d\\x9b\\xbe 1\\r\\n\\xf0\\x9d\\x91\\x96 \\x10\\r\\n\\x11\\r\\n\\xf0\\x9d\\x91\\x97\\r\\nif min \\xe2\\x88\\xa5c\\xe2\\x84\\x8e\\xf0\\x9d\\x91\\x96 \\xe2\\x88\\x92 cp \\xe2\\x88\\xa5, \\xe2\\x88\\xa5c\\xf0\\x9d\\x91\\xa0 \\xe2\\x88\\x92 cp \\xe2\\x88\\xa5 > \\xf0\\x9d\\x9b\\xbe 2\\r\\n\\xf0\\x9d\\x91\\x96,\\xf0\\x9d\\x91\\x97\\r\\n\\r\\notherwise\\r\\n\\r\\n(1)\\r\\n\\r\\nwhere \\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9f (\\xc2\\xb7) returns a pre-defined cell center. If cp is similar to any\\r\\nsalient color, then u is a valid correspondence. This u will be marked\\r\\nas \\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x93 when no correspondence exists for p on the designed grid\\r\\npattern, indicating that the light path terminates outside the gird.\\r\\nOtherwise, the pixel p corresponds to a pixel on the grid pattern\\r\\nwith checkboard colors. In this case, we can not obtain its precise\\r\\nray-cell correspondence. Therefore, we set the correspondence of p\\r\\nas \\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x92. The parameters \\xf0\\x9d\\x9b\\xbe 1 and \\xf0\\x9d\\x9b\\xbe 2 are set as 0.3 and 0.4 respectively.\\r\\nImage Registration and 3D Reconstruction: After capturing\\r\\ndata, we use the 3D reconstruction software RealityCapture [CapturingReality 2016] to register the captured images enabling us to\\r\\ntrace rays in a unified coordinate frame during differentiable rendering. Given that the position of the iPad is changed after every\\r\\n60 images to provide more ray-cell correspondences, we reconstruct\\r\\nthe 3D scene with textures that contain the iPad every 60 images independently, resulting in a number of components in RealityCapture.\\r\\nEach component records the information of each independently reconstructed 3D scene. All the components are then registered based\\r\\non the AprilTags [Olson 2011] beneath the object as shown in Fig. 5.\\r\\n\\r\\nConsidering that the background pattern displayed on the iPad\\r\\nis changed during the capturing of every 4 images, wrong matching points on the iPad\\xe2\\x80\\x99s surface are produced, resulting in failure\\r\\nin 3D iPad plane reconstruction. To address this issue, we display\\r\\nadditional AprilTags surrounding the background patterns to add\\r\\nextra matching points to guarantee the success of iPad plane reconstruction.\\r\\nEach 3D plane P\\xf0\\x9d\\x91\\x96 of the background pattern is detected using\\r\\nthe RANSAC shape detection algorithm [Schnabel et al. 2010] as\\r\\nillustracted in Fig. 5. We also generate local coordinates for each\\r\\ngrid pattern to map the 3D points on the plane onto the texture\\r\\ncoordinate.\\r\\n\\r\\n4.2\\r\\n\\r\\nInitial Shape Reconstruction\\r\\n\\r\\nWe utilize IDR [Yariv et al. 2020] with silhouette (mask) loss to\\r\\nobtain the initial shape of a transparent object. The object masks are\\r\\nmanually annotated on a number of selected images. The number\\r\\nof masks is provided in Tab. 1. We only use silhouette loss, and\\r\\nthus, the \\xe2\\x80\\x9cneural renderer\\xe2\\x80\\x9d MLP in IDR is removed. As shown in\\r\\nFig. 6, IDR with silhouette loss can produce smoother reconstruction\\r\\nresults than the space carving algorithm.\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\n4.3\\r\\n\\r\\nSurface Optimization through Differentiable\\r\\nRendering\\r\\n\\r\\nVertex \\xf0\\x9d\\x90\\xb1 \\xf0\\x9d\\x92\\x8a (\\xf0\\x9d\\x92\\x8a \\xe2\\x88\\x88 \\xf0\\x9d\\x9f\\x8f \\xe2\\x80\\xa6 \\xf0\\x9d\\x91\\xb5 )\\r\\nCluster \\xf0\\x9d\\x90\\xb6\\xf0\\x9d\\x90\\xb1\\xf0\\x9d\\x92\\x8a\\r\\n\\r\\nGiven the initial shape as the base mesh, we first group mesh triangles into a number of clusters and then assign each cluster with a\\r\\nsurface-based MLP. Thus, VDF can be computed as the fusion of the\\r\\noutput of surface-based local MLPs. In particular, for each vertex\\r\\nx\\xf0\\x9d\\x91\\x96 within cluster \\xf0\\x9d\\x90\\xb6 x\\xf0\\x9d\\x91\\x96 , each local MLP MLP\\xf0\\x9d\\x91\\x98 outputs a displacement\\r\\n^ \\xf0\\x9d\\x91\\x96 as follows:\\r\\nvector \\xf0\\x9d\\x9b\\xbfx\\r\\n^ \\xf0\\x9d\\x91\\x96 = MLP\\xf0\\x9d\\x90\\xb6 (x\\xf0\\x9d\\x91\\x96 ) .\\r\\n\\xf0\\x9d\\x9b\\xbfx\\r\\nx\\xf0\\x9d\\x91\\x96\\r\\n\\r\\n\\xf0\\x9d\\x9b\\xbfx\\xf0\\x9d\\x91\\x96 =\\r\\n\\r\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\r\\n\\r\\n\\x01 ^\\r\\n\\xf0\\x9d\\x91\\xa4 x\\xf0\\x9d\\x91\\x96 , x \\xf0\\x9d\\x91\\x97 \\xc2\\xb7 \\xf0\\x9d\\x9b\\xbfx\\r\\n\\xf0\\x9d\\x91\\x97,\\r\\n\\r\\n(3)\\r\\n\\r\\n\\xf0\\x9d\\x91\\x97\\r\\n\\r\\n\\x01\\r\\n\\x01\\r\\nwhere \\xf0\\x9d\\x91\\xa4 x\\xf0\\x9d\\x91\\x96 , x \\xf0\\x9d\\x91\\x97 = exp(\\xe2\\x88\\x92\\xf0\\x9d\\x91\\x91 x\\xf0\\x9d\\x91\\x96 , x \\xf0\\x9d\\x91\\x97 /\\xf0\\x9d\\x9c\\x8e); and \\xf0\\x9d\\x91\\x91 is the geodesic distance between x\\xf0\\x9d\\x91\\x96 and x \\xf0\\x9d\\x91\\x97 , which is calculated using the heat transportation based method in [Crane et al. 2017]. The parameter \\xf0\\x9d\\x9c\\x8e=0.005\\r\\nin our experiments.\\r\\nThe architecture of each local MLP is shown in Fig. 7 with two\\r\\nfully connected (FC) layers. For each MLP, each input 3D vertex on\\r\\nsurface is firstly mapped to a 99 dimensional feature using positional\\r\\nencoding. Then the first FC layer with weight normalization and\\r\\nReLu activation maps the 99 dimensional feature to a 128 dimensional latent feature. The second FC layer with weight normalization\\r\\nand tanh activation maps the latent feature to a 3D displacement.\\r\\nBefore optimization, all the weights are initialized to produce zero\\r\\ndisplacement field.\\r\\nIn the following, we first describe how to extract clusters from\\r\\nthe base mesh, and then describe the details of designed loss terms\\r\\nand our optimization procedure.\\r\\n4.3.1 Cluster Extraction. We utilize the Variational Shape Approximation (VSA) algorithm [Cohen-Steiner et al. 2004] to segment\\r\\nthe initial shape into several clusters. The VSA algorithm tends to\\r\\nmerge co-planar vertices into the same cluster by minimizing L 2\\r\\ndistortion error which measures the error between the cluster and\\r\\nits linear proxy (plane). To balance the size of each cluster, we add a\\r\\nEuclidean distance error that sums the Euclidean distance between\\r\\neach vertex and its cluster center. This error term is added to L 2\\r\\ndistortion error with a weight of 0.005 to control the clustering\\r\\nprocedure.\\r\\n4.3.2 Loss Terms. We minimize the following loss function to search\\r\\nfor the weight parameters of local MLPs:\\r\\nL\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99 =\\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x8f L\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x8f + \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f L\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f + \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f L\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f\\r\\n\\r\\n(4)\\r\\n\\r\\n+\\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\xa0\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x99 L\\xf0\\x9d\\x91\\xa0\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x99 + \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x91\\x94 L\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x91\\x94 ,\\r\\nwhich is the sum of five terms: RGB loss L\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x8f , ray-cell correspondence loss L\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f , no-correspondence loss L\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f , silhouette loss\\r\\nL\\xf0\\x9d\\x91\\xa0\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x99 [Ravi et al. 2020], and regularization loss L\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x91\\x94 . The default\\r\\nvalues of weights, i.e., \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x8f , \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f , \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f , \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\xa0\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x99 and \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x91\\x94 are set as\\r\\n0.001, 0.1, 0.03, 50.0, and 1.0, respectively, in our experiments.\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nx\\r\\n\\r\\nFusion layer\\r\\n\\r\\n\\xf0\\x9d\\x92\\x8a\\r\\n\\r\\n\\xf0\\x9d\\x9c\\xb9x\\r\\n\\r\\n\\xe1\\x88\\xb6\\r\\n\\xf0\\x9d\\x9c\\xb9x\\r\\n\\r\\n\\xf0\\x9d\\x9c\\xb8(x)\\r\\n\\r\\nGeodesic\\r\\ndistances\\r\\n\\r\\n3\\r\\n\\r\\n99\\r\\n\\r\\nPositional\\r\\nencoding\\r\\n\\r\\n(2)\\r\\n\\r\\nTo avoid VDF discontinuity at cluster boundaries, we introduce a\\r\\ndifferentiable fusion layer to obtain the final displacement vector\\r\\n\\xf0\\x9d\\x9b\\xbfx\\xf0\\x9d\\x91\\x96 :\\r\\n\\r\\nMLP\\xf0\\x9d\\x90\\xb6\\xf0\\x9d\\x90\\xb1\\r\\n\\r\\n3\\r\\n\\r\\n128\\r\\n\\r\\nFixed\\r\\nblending\\r\\nweights\\r\\n\\r\\n...\\r\\n\\r\\n6 \\xe2\\x80\\xa2\\r\\n\\r\\n3\\xc3\\x97\\xf0\\x9d\\x91\\x81\\r\\n\\r\\nSurface MLPs\\r\\n\\r\\nFig. 7. Local MLP representation. Each local MLP is responsible for representing the vertex displacements inside a VSA cluster (shown as the colored\\r\\npatch on the object surface). A fusion layer is used to fuse the vertex displacements output by the local MLPs into a smooth VDF on the surface.\\r\\n\\r\\nRGB Loss: RGB loss measures the difference between the pixel\\r\\ncolor cp and the rendering result c\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\r\\np of its traced ray on the basis\\r\\nof the recursive ray tracing algorithm. For a pixel p on a captured\\r\\nimage I, we first trace a ray l\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\r\\np from the viewpoint associated with\\r\\nI. The recursive ray tracing algorithm is then triggered to obtain\\r\\nthe reflection color c\\xf0\\x9d\\x91\\x9fp1 through the first-bounce reflection ray l\\xf0\\x9d\\x91\\x9fp1\\r\\nand the refraction color c\\xf0\\x9d\\x91\\xa1p2 through the second-bounce refraction\\r\\nray l\\xf0\\x9d\\x91\\xa1p2 by intersecting and fetching textures from a scene mesh or a\\r\\ncorresponding pattern. We only consider single-bounce reflection\\r\\nand double-bounce refraction in our algorithm.\\r\\nIn particular, the camera ray l\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\r\\np first intersects with the front\\r\\nsurface at point x1 , and is refracted by the surface with normal\\r\\nn1 following Snell\\xe2\\x80\\x99s law, as shown in Fig. 8. Then, the first-bounce\\r\\nrefraction ray l\\xf0\\x9d\\x91\\xa1p1 is traced until it hits the back surface at point x2\\r\\nwith normal n2 . The second-bounce refraction ray l\\xf0\\x9d\\x91\\xa1p2 is generated\\r\\nrecursively. Similarly, the reflection ray l\\xf0\\x9d\\x91\\x9fp1 is traced in the mirrorreflection direction. For RGB loss, we only consider the camera rays\\r\\nwith first-bounce reflection ray l\\xf0\\x9d\\x91\\x9fp1 and second-bounce refraction ray\\r\\nl\\xf0\\x9d\\x91\\xa1p2 . We store the valid camera rays whose second-bounce refraction\\r\\nrays can hit the scene mesh or pattern without occlusion and total\\r\\ninternal reflection as a binary mask M\\xf0\\x9d\\x91\\xa1 on image I.\\r\\nFor each refraction ray above, refraction color is attenuated along\\r\\nthe light path according to the Fresnel term F [Born 1999]. Taking\\r\\ntwo successive refraction rays l\\xf0\\x9d\\x91\\xa1p1 and l\\xf0\\x9d\\x91\\xa1p2 as an example, where n2 is\\r\\nthe surface normal, and \\xf0\\x9d\\x9c\\x82\\xf0\\x9d\\x91\\x96 and \\xf0\\x9d\\x9c\\x82\\xf0\\x9d\\x91\\x9c are the two indexes of refraction\\r\\n(IOR) inside and outside the object, the Fresnel term F \\xe2\\x9f\\xa8\\xf0\\x9d\\x91\\xa1 1,\\xf0\\x9d\\x91\\xa1 2 \\xe2\\x9f\\xa9 can\\r\\nbe computed as follows:\\r\\n\\r\\nF\\r\\n\\r\\n\\xe2\\x9f\\xa8\\xf0\\x9d\\x91\\xa1 1,\\xf0\\x9d\\x91\\xa1 2 \\xe2\\x9f\\xa9\\r\\n\\r\\n\\xf0\\x9d\\x91\\x96 \\xf0\\x9d\\x91\\xa11\\r\\n\\xf0\\x9d\\x91\\x9c \\xf0\\x9d\\x91\\xa12\\r\\n1 \\xf0\\x9d\\x9c\\x82 rp \\xc2\\xb7 n2 \\xe2\\x88\\x92 \\xf0\\x9d\\x9c\\x82 rp \\xc2\\xb7 n2\\r\\n=\\r\\n2 \\xf0\\x9d\\x9c\\x82\\xf0\\x9d\\x91\\x96 r\\xf0\\x9d\\x91\\xa1p1 \\xc2\\xb7 n2 + \\xf0\\x9d\\x9c\\x82\\xf0\\x9d\\x91\\x9c r\\xf0\\x9d\\x91\\xa1p2 \\xc2\\xb7 n2\\r\\n\\r\\n!2\\r\\n\\r\\n\\xf0\\x9d\\x91\\x9c \\xf0\\x9d\\x91\\xa11\\r\\n\\xf0\\x9d\\x91\\x96 \\xf0\\x9d\\x91\\xa12\\r\\n1 \\xf0\\x9d\\x9c\\x82 rp \\xc2\\xb7 n2 \\xe2\\x88\\x92 \\xf0\\x9d\\x9c\\x82 rp \\xc2\\xb7 n2\\r\\n+\\r\\n2 \\xf0\\x9d\\x9c\\x82\\xf0\\x9d\\x91\\x9c r\\xf0\\x9d\\x91\\xa1p1 \\xc2\\xb7 n2 + \\xf0\\x9d\\x9c\\x82\\xf0\\x9d\\x91\\x96 r\\xf0\\x9d\\x91\\xa1p2 \\xc2\\xb7 n2\\r\\n(5)\\r\\n\\r\\nc\\xf0\\x9d\\x91\\xa1p1 = (1 \\xe2\\x88\\x92 F \\xe2\\x9f\\xa8\\xf0\\x9d\\x91\\xa1 1,\\xf0\\x9d\\x91\\xa1 2 \\xe2\\x9f\\xa9 )c\\xf0\\x9d\\x91\\xa1p2 .\\r\\n\\r\\n(6)\\r\\n\\r\\nIn our experiments, we set the IOR of air as 1.0003 and leave the IOR\\r\\nof the object material as an unknown parameter to be optimized. As\\r\\nshown in Fig. 8, the reflection color c\\xf0\\x9d\\x91\\x9fp1 and refraction color c\\xf0\\x9d\\x91\\xa1p2 are\\r\\n\\r\\n!2\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n\\r\\nL\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f =\\r\\n\\r\\n\\x11\\r\\n1 \\xe2\\x88\\x91\\xef\\xb8\\x81 \\xf0\\x9d\\x91\\xa1 \\x10\\r\\nMp\\xf0\\x9d\\x91\\x91 vl\\xf0\\x9d\\x91\\xa1p2 , u\\r\\n\\xf0\\x9d\\x91\\xa1\\r\\n|M | p\\r\\n\\r\\n\\xe2\\x80\\xa2 7\\r\\n\\r\\n(8)\\r\\n\\r\\nwhere \\xf0\\x9d\\x91\\x91 (\\xc2\\xb7, \\xc2\\xb7) is a clipped L2 distance that reduces the loss to zero if\\r\\nthe projected pixel is within the cell.\\r\\n(\\r\\n\\xe2\\x88\\xa5q1 \\xe2\\x88\\x92 q2 \\xe2\\x88\\xa5 2 if \\xe2\\x88\\xa5q1 \\xe2\\x88\\x92 q2 \\xe2\\x88\\xa5 \\xe2\\x88\\x9e > \\xf0\\x9d\\x91\\x99/2\\r\\n\\xf0\\x9d\\x91\\x91 (q1, q2 ) =\\r\\n(9)\\r\\n0\\r\\notherwise\\r\\n\\r\\nFig. 8. Recursive ray tracing procedure. During rendering, the refraction\\r\\nand reflection rays fetch colors from the scene texture and background\\r\\npattern. If pixel p has its corresponding cell center u, then the mesh should\\r\\nbe optimized to make the intersection between l\\xf0\\x9d\\x91\\xa1p2 and the 3D pattern plane\\r\\nwithin the corresponding cell (the red point in the illustration).\\r\\n\\r\\nattenuated after ray tracing and composed to obtain the rendered\\r\\ncolor c\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\r\\np.\\r\\nThe reflection and refraction colors are fetched from the textured\\r\\nscene mesh or corresponding pattern. If one reflection ray does not\\r\\nintersect with the scene mesh or pattern, then we only set c\\xf0\\x9d\\x91\\x9fp1 as\\r\\nzero. RGB loss L\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x8f is defined as follows:\\r\\nL\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x8f =\\r\\n\\r\\n1 \\xe2\\x88\\x91\\xef\\xb8\\x81 \\xf0\\x9d\\x91\\xa1 \\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\r\\nMp \\xe2\\x88\\xa5cp \\xe2\\x88\\x92 cp \\xe2\\x88\\xa5 1\\r\\n|M\\xf0\\x9d\\x91\\xa1 | p\\r\\n\\r\\n(7)\\r\\n\\r\\nwhere \\xe2\\x88\\xa5\\xc2\\xb7\\xe2\\x88\\xa5 1 indicates the L1 norm, c\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\r\\np is the color rendered from ray\\r\\ntracing, and cp is the color for pixel p. In our implementation, we\\r\\nsample rays based on 200\\xc3\\x97200 cropped patches instead of individual\\r\\nrays. To match the pixel color and rendered color at the coarse-tofine level, we filter c\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\r\\np and cp with three differentiable Gaussian\\r\\nfilters (7 \\xc3\\x97 7, 11 \\xc3\\x97 11, 13 \\xc3\\x97 13) and then add the outputs together.\\r\\nThese Gaussian filters have 2.5, 7.5, and 7.5 standard deviations\\r\\nrespectively. Moreover, we filter the textures of the scene mesh and\\r\\npatterns with Gaussian filters to smooth the gradients.\\r\\nCorrespondence Loss and No Correspondence Loss: Correspondence loss L\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f and no-correspondence loss L\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f are used to\\r\\nenforce the refraction ray l\\xf0\\x9d\\x91\\xa1p2 to match the ray-cell correspondence\\r\\ndescribed in Sec. 4.1. For each pixel p with its corresponding cell,\\r\\nwhere the cell center is u and the side-length is \\xf0\\x9d\\x91\\x99, we obtain the rayplane intersection point between l\\xf0\\x9d\\x91\\xa1p2 and the current pattern plane\\r\\nP. Then the intersection point is projected onto 2D space and transformed into texture coordinate as vl\\xf0\\x9d\\x91\\xa1p2 . Here, the 2D image space of\\r\\nplanar patterns has been pre-rectified to make the texture coordinate align with the grid boundaries. As shown in Fig. 4, the texture\\r\\ncoordinates of four grid corners are set as (0, 0), (1, 0), (1, 1), and\\r\\n(0, 1), with the texture coordinate of the grid center \\xf0\\x9d\\x91\\x94 = (0.5, 0.5).\\r\\nCorrespondence loss L\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f constrains the projected pixel vl\\xf0\\x9d\\x91\\xa1p2 to its\\r\\ncorresponding grid center u as follows:\\r\\n\\r\\nwhere \\xf0\\x9d\\x91\\x99/2 is the half side-length of one square cell. As we clip the\\r\\ndistance function, our correspondence loss only imposes coarse\\r\\nconstraints at the cell level. However, RGB loss can help refine the\\r\\nsurface and correspondences at a fine level.\\r\\nFor pixels with no salient correspondences, where u = \\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x93 , we\\r\\ncan also add constraints based on no-correspondence loss L\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f .\\r\\nFor each pixel with u = \\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x93 , l\\xf0\\x9d\\x91\\xa1p2 either has no intersection with\\r\\nplane P or the intersection point is out of the grid area. Thus, nocorrespondence loss L\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f is defined as follows:\\r\\nL\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f = \\xe2\\x88\\x92\\r\\n\\r\\n\\x11\\r\\n1 \\xe2\\x88\\x91\\xef\\xb8\\x81 \\xf0\\x9d\\x91\\xa1 ^ \\x10\\r\\nMp\\xf0\\x9d\\x91\\x91 vl\\xf0\\x9d\\x91\\xa1p2 , g\\r\\n\\xf0\\x9d\\x91\\xa1\\r\\n|M | p\\r\\n\\r\\n(10)\\r\\n\\r\\nwhere the clipped distance function \\xf0\\x9d\\x91\\x91^(\\xc2\\xb7, \\xc2\\xb7) clips the L2 distance when\\r\\nthe texture coordinate of the hitting point is located outside the grid\\r\\narea:\\r\\n(\\r\\n\\xe2\\x88\\xa5q1 \\xe2\\x88\\x92 q2 \\xe2\\x88\\xa5 2 if \\xe2\\x88\\xa5q1 \\xe2\\x88\\x92 q2 \\xe2\\x88\\xa5 \\xe2\\x88\\x9e < 0.5\\r\\n^\\r\\n\\xf0\\x9d\\x91\\x91 (q1, q2 ) =\\r\\n(11)\\r\\n0\\r\\notherwise\\r\\nwhere 0.5 is the half side-length of the entire grid pattern in the\\r\\ntexture coordinate.\\r\\nSilhouette Loss and Regularization Loss: We also add silhouette\\r\\nloss [Ravi et al. 2020] to the annotated object masks similar to the\\r\\ninitial shape reconstruction step (Sec. 4.2). Moreover, to constrain\\r\\nour optimization further, we add a regularization loss as follows:\\r\\n(12)\\r\\n\\r\\nL\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x91\\x94 = \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\xa0 L\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\xa0 + \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90 L\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90 + \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9d\\xf0\\x9d\\x91\\x90 L\\xf0\\x9d\\x91\\x9d\\xf0\\x9d\\x91\\x90\\r\\n\\r\\nwhich have three loss terms for the purpose of shape regularization:\\r\\na Laplacian smoothness loss L\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\xa0 similar to that in [Ravi et al. 2020], a\\r\\nnormal consistency loss L\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90 as in [Lyu et al. 2020], and a point cloud\\r\\nregularization loss L\\xf0\\x9d\\x91\\x9d\\xf0\\x9d\\x91\\x90 , which minimize Chamfer distance [Ravi\\r\\net al. 2020] between the optimized points on current mesh and the\\r\\ninitial shape. These losses are defined as follows:\\r\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\r\\n\\r\\nL\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\xa0 =\\r\\n\\r\\nv \\xf0\\x9d\\x91\\x97 \\xe2\\x88\\x88N ( v\\xf0\\x9d\\x91\\x96 )\\r\\n\\r\\nL\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90 =\\r\\n\\r\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\r\\n\\r\\n\\x01\\r\\n1\\r\\nv \\xf0\\x9d\\x91\\x97 \\xe2\\x88\\x92 v\\xf0\\x9d\\x91\\x96\\r\\n|N (v\\xf0\\x9d\\x91\\x96 )|\\r\\n\\r\\n(13)\\r\\n\\r\\n\\x01\\x01\\r\\n\\r\\n(14)\\r\\n\\r\\n1 \\xe2\\x88\\x92 log 1 + n\\xf0\\x9d\\x91\\x921 \\xc2\\xb7 n\\xf0\\x9d\\x91\\x922\\r\\n\\r\\n\\xf0\\x9d\\x91\\x92 \\xe2\\x88\\x88E\\r\\n\\r\\nL\\xf0\\x9d\\x91\\x9d\\xf0\\x9d\\x91\\x90 =\\r\\n\\r\\n1\\r\\nS1\\r\\n\\r\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\r\\nx1 \\xe2\\x88\\x88S 1\\r\\n\\r\\nmin \\xe2\\x88\\xa5x1 \\xe2\\x88\\x92 x2 \\xe2\\x88\\xa5 22 +\\r\\n\\r\\nx2 \\xe2\\x88\\x88S 2\\r\\n\\r\\n1\\r\\nS2\\r\\n\\r\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\r\\nx2 \\xe2\\x88\\x88S 2\\r\\n\\r\\nmin \\xe2\\x88\\xa5x1 \\xe2\\x88\\x92 x2 \\xe2\\x88\\xa5 22\\r\\n\\r\\nx1 \\xe2\\x88\\x88S 2\\r\\n\\r\\n(15)\\r\\n\\r\\nwhere N (v\\xf0\\x9d\\x91\\x96 ) is the neighboring vertices of vertex v\\xf0\\x9d\\x91\\x96 , E is the set of\\r\\nall edges, and n\\xf0\\x9d\\x91\\x921 \\xc2\\xb7n\\xf0\\x9d\\x91\\x922 is the dot product of the normals of two adjacent\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n8 \\xe2\\x80\\xa2\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nFig. 9. Captured images for five transparent objects: the cat object, the cow object, the dog object, the trophy object, and the brick object with bumpy front\\r\\nsurface.\\r\\n\\r\\nFig. 10. The reconstruction results and their corresponding rendering results\\r\\nfor a trophy object and a brick object with bumpy front surface.\\r\\n\\r\\ntriangles sharing the same edge \\xf0\\x9d\\x91\\x92. We set \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\xa0 = 0.2, \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90 = 1.0, and\\r\\n\\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x91\\x9d\\xf0\\x9d\\x91\\x90 = 100.0.\\r\\nRemark: To increase stability during optimization, we will remove\\r\\nthree types of camera rays: 1. nearly perpendicular with the surface\\r\\nnormals at its intersection with the surface during recursive raytracing (|l \\xc2\\xb7 n| < 0.2), 2. traced from the pixels near the boundary of\\r\\nthe 2D mask created by shape projection (less than 7 pixels), and 3.\\r\\ntraced from the pixels with nearly overexposure color (larger than\\r\\n220 in all RGB channels). After these reflection pruning operations,\\r\\nwe found our algorithm can produce high-quality reconstruction\\r\\nresults without environment map because the remaining rays are\\r\\ndominated by refraction or reflection from the scene mesh.\\r\\nAs the light path inside a transparent object is complex, optimizing the surface shape based on local RGB loss is insufficient, even\\r\\nwith our pyramid loss or perceptual loss (VGG loss). Consequently,\\r\\nwe utilize correspondence-based loss to obtain gradients to move\\r\\nl\\xf0\\x9d\\x91\\xa1p2 inside its corresponding cell. Within a cell, the distance between\\r\\nthe pixel in the rendered image and its corresponding pixel in the\\r\\ncaptured image is short. Then, RGB loss can work to improve surface\\r\\ndetails. We verfiy the preceding observation through the ablation\\r\\nstudy discussed in Sec. 5 (with and without correspondence loss\\r\\nand no-correspondence loss).\\r\\n4.3.3 Optimization. As described earlier, our initial shape reconstruction step is based on IDR [Yariv et al. 2020]. We found that with\\r\\nonly silhouette loss, an insufficient number of rays may cause holes\\r\\non surfaces or sometimes generate another surface beneath the object\\xe2\\x80\\x99s surface. Thus, we increase the number of rays sampled from\\r\\nan image to 20800, and each batch contains rays sampled from three\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nimages. We set the learning rate as 1 \\xc3\\x97 10\\xe2\\x88\\x924 in the beginning with\\r\\nthe same decay strategy as in IDR. The initial shape reconstruction\\r\\nstep takes about 1~2 hours after 2000 epochs on NVIDIA GeForce\\r\\nRTX 3090 GPU.\\r\\nIn the surface-based MLP optimization step, we randomly crop\\r\\n200\\xc3\\x97200 image patches from the RGB and silhouette mask images to\\r\\ncompute the RGB and silhouette loss terms. In our implementation,\\r\\nwe project the initial shape to all the views to generate the projected\\r\\nmask images, denoted as M\\xf0\\x9d\\x91\\x9d . During optimization, the RGB image\\r\\npatches are sampled if they overlap with M\\xf0\\x9d\\x91\\x9d to improve the patch\\r\\nsampling efficiency. For silhouette loss, we only sample crops from\\r\\nmasks annotated for initial shape reconstruction at each iteration.\\r\\nThe number of images with annotated masks is much less than the\\r\\ntotal number of captured RGB images in our implementation to ease\\r\\nthe burden of the annotation procedure. The optimizer for surfacebased local MLPs optimization is ADAM [Kingma and Ba 2015], and\\r\\nthe learning rate is set as 1\\xc3\\x9710\\xe2\\x88\\x925 with cosine annealing [Loshchilov\\r\\nand Hutter 2016] scheduler. The number of epochs used to train the\\r\\nnetwork is 300. The optimization procedure takes about 5~6 hours\\r\\non one NVIDIA GeForce RTX 3090 GPU, with pure PyTorch [Paszke\\r\\net al. 2017] implementation. Our code can be further optimized\\r\\nin many aspects. The forward procedure can be accelerated using\\r\\nOptiX engine, and the backward procedure can be accelerated by\\r\\nincreasing the degree of parallelism between the ray segments.\\r\\nWe optimize the index of refraction (IOR) coefficient \\xf0\\x9d\\x9c\\x82 of the\\r\\nobject by using ADAM with a learning rate of 1 \\xc3\\x97 10\\xe2\\x88\\x926 . We initialize\\r\\nthe IOR coefficient of the object as 1.52 and add L2 regularization\\r\\nloss \\xe2\\x88\\xa5\\xf0\\x9d\\x9c\\x82 \\xe2\\x88\\x92 1.52\\xe2\\x88\\xa5 2 to Eq. 4 with a balancing weight equals 0.001.\\r\\n\\r\\n5\\r\\n\\r\\nEXPERIMENTS\\r\\n\\r\\nWe apply our algorithm to reconstruct the 3D shapes of five different\\r\\ntransparent objects as shown in Fig. 1 and Fig. 10, made from glass\\r\\nor crystal. The captured images for these five objects are illustrated\\r\\nin Fig. 9. The size of each object, the number of input images, the\\r\\nnumber of manually annotated masks, and the number of random\\r\\npatterns with moving frequency are listed in Tab. 1. In the following,\\r\\nwe demonstrate the advantage of our surface-based local MLP representation, perform the ablation studies, and compare our method\\r\\nwith state-of-the-art transparent object reconstruction methods.\\r\\nTo evaluate the accuracy of reconstruction quantitatively, we\\r\\npaint each object with DPT-5 developer as in [Wu et al. 2018] and\\r\\nthen scan it with a scanner to obtain a ground truth mesh in SI\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n\\r\\nVert\\r\\n\\r\\nVert-LS\\r\\n\\r\\nSDF-MLP\\r\\n\\r\\nLi et al.\\r\\n\\r\\nOurs\\r\\n\\r\\n\\xe2\\x80\\xa2 9\\r\\n\\r\\nGroud Truth\\r\\n\\r\\nFig. 11. Comparisons with Vert, Vert-LS, SDF-MLP and Li et al. [2020].\\r\\nTable 1. Statistics of data acquisition. #Img denotes the number of captured\\r\\nimages. #Mask, #Pattern, and #Move denote the number of annotated masks,\\r\\nthe number of our grid-based patterns presented on iPad, and the number\\r\\nof iPad movements, respectively.\\r\\n\\r\\nObject\\r\\ncow\\r\\ncat\\r\\ndog\\r\\ntrophy\\r\\nbrick\\r\\n\\r\\nSize (\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9a 3 )\\r\\n9.5 \\xe2\\x88\\x97 4.5 \\xe2\\x88\\x97 17.5\\r\\n8 \\xe2\\x88\\x97 2.5 \\xe2\\x88\\x97 4.5\\r\\n11 \\xe2\\x88\\x97 3.5 \\xe2\\x88\\x97 11.5\\r\\n12 \\xe2\\x88\\x97 7 \\xe2\\x88\\x97 7\\r\\n10 \\xe2\\x88\\x97 4.5 \\xe2\\x88\\x97 20\\r\\n\\r\\n#Img\\r\\n154\\r\\n302\\r\\n300\\r\\n172\\r\\n178\\r\\n\\r\\n#Mask\\r\\n19\\r\\n29\\r\\n19\\r\\n14\\r\\n15\\r\\n\\r\\n#Pattern\\r\\n30\\r\\n60\\r\\n60\\r\\n30\\r\\n30\\r\\n\\r\\n#Move\\r\\n2\\r\\n4\\r\\n4\\r\\n2\\r\\n2\\r\\n\\r\\nunit (meter) as shown in Fig. 11. We compare the reconstructed results with the ground truth after aligning them using ICP [Zhou et al.\\r\\n2018]. After that, we can evaluate the reconstruction by measuring\\r\\nthe Chamfer distance (Eq. 15) between two point clouds.\\r\\n\\r\\n5.1\\r\\n\\r\\nEvaluations\\r\\n\\r\\n5.1.1 Surface-based Local MLP Representation. We perform a comparison to isolate and highlight the importance of our surface-based\\r\\nlocal MLP representation. With our loss function, we compare our\\r\\nresults with different representations, including: 1. explicit mesh\\r\\n\\r\\nvertices as in [Lyu et al. 2020], denoted as Vert, 2. mesh vertices\\r\\nwith an advanced optimizer in [Nicolet et al. 2021], denoted as\\r\\nVert-LS (large step), and 3. SDF encoded by a single MLP similar\\r\\nto IDR [Yariv et al. 2020], denoted as SDF-MLP. We denote our\\r\\nsurface-based MLPs representation as Surf-MLPs. The implementation details of the representations used in the comparison are as\\r\\nfollows:\\r\\nVert: This representation explicitly optimizes the position of each\\r\\nvertex using the same loss as ours and with an ADAM optimizer. The\\r\\nlearning rate is set as 1 \\xc3\\x97 10\\xe2\\x88\\x925 with cosine annealing [Loshchilov\\r\\nand Hutter 2016] scheduler.\\r\\nVert-LS: This representation is similar to Vert but with the gradient\\r\\ncalculation method and the new optimizer proposed in [Nicolet et al.\\r\\n2021]. The gradient steps in [Nicolet et al. 2021] already involve the\\r\\nLaplacian energy. Thus, we remove the explicit Laplacian smoothness loss. We set the regularization weight \\xf0\\x9d\\x9c\\x86 as 10, and the learning\\r\\nrate is set as 2 \\xc3\\x97 10\\xe2\\x88\\x923 as in their paper.\\r\\nSDF-MLP: The SDF-MLP representation is a modified version of\\r\\nIDR [Yariv et al. 2020] that enables to optimize for the SDF-MLP\\r\\nrepresentation with our RGB loss and correspondence loss. The key\\r\\nmodification is propagating the gradients to the SDF MLP based\\r\\nfrom explicit mesh vertices extracted using marching cube, similar\\r\\n\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n10\\r\\n\\r\\n\\xe2\\x80\\xa2\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nPE_6/256_9\\r\\n#MLP 1\\r\\n\\r\\nArch\\r\\n\\r\\nPE_16/256_9\\r\\n#MLP 1\\r\\n\\r\\n#MLP 1\\r\\n\\r\\nPE_6\\r\\n256_9\\r\\nPE_16\\r\\n256_9\\r\\n\\r\\n1.287\\r\\n\\r\\nPE_16\\r\\n128_2\\r\\n\\r\\n1.660\\r\\n\\r\\nPE_16/128_2\\r\\n#MLP 1\\r\\n\\r\\n#MLP 50\\r\\n\\r\\nPE_16/128_2\\r\\n#MLP 50\\r\\n\\r\\n#MLP 100\\r\\n\\r\\nPE_16/128_2\\r\\n#MLP 100\\r\\n\\r\\nPE_16/128_2\\r\\n#MLP 150\\r\\n\\r\\n#MLP 150\\r\\n\\r\\n1.303\\r\\n1.287\\r\\n\\r\\n1.297\\r\\n\\r\\n1.163\\r\\n\\r\\nW/o \\xf0\\x9d\\x93\\x9bcorr\\r\\n\\r\\nto MeshSDF [Remelli et al. 2020]. Considering that the spherical ray\\r\\ntracing in IDR is not differentiable, we perform the marching cube\\r\\nalgorithm at each iteration to extract the mesh from SDF, calculate\\r\\nthe loss and gradient, and perform back-propagation. The derivative\\r\\nof each intersection point x with respect to its SDF value \\xf0\\x9d\\x91\\xa0 is equal\\r\\nto \\xf0\\x9d\\x9c\\x95x/\\xf0\\x9d\\x9c\\x95\\xf0\\x9d\\x91\\xa0 = \\xe2\\x88\\x92n (x), where n (x) is the surface normal at point x. The\\r\\narchitecture of the SDF-MLP network involves eight hidden layers,\\r\\nwith each layer having a dimension of 256. The activation function,\\r\\nthe level of positional encoding, the optimizer, and the learning rate\\r\\nare set same as in IDR.\\r\\nAs shown in Fig. 11, our Surf-MLP representation outperforms\\r\\nthe other representations. Explicit mesh optimization introduces\\r\\nsome high-frequency artifacts. Although Vert-LS method can reduce\\r\\nthe artifacts, it will introduce some folds in some areas. Simultaneously, the SDF-MLP representation may produce over-smooth\\r\\nresults that lose some details. We also compare our method with\\r\\nlearning based method [Li et al. 2020]. As illustrated in the fourth\\r\\ncolumn of Fig. 11, their method produces smooth surfaces that lose\\r\\ndetails near object boundaries. Compared with the ground truth\\r\\nmesh, our reconstructed model illustrated in the fifth column of\\r\\nFig. 11 can preserve relatively high precision details.\\r\\n5.1.2 Number of MLPs. We first perform ablation studies to evaluate the influence of the MLP number (cluster number). Figure 12\\r\\nshows that our surface-based local MLP representation can obtain\\r\\nbetter result while increasing the number of clusters from 1, 50,\\r\\n100, to 150. We also compare our local MLP representation with a\\r\\nglobal MLP with 9 hidden layers of 256 dimension. For the global\\r\\nMLP, we also test it with two different positional encoding settings,\\r\\nusing number of positional encoding frequencies L=6 and L=16. As\\r\\nillustrated in Fig. 12, our multiple local MLP representation outperforms the single global MLP representation with lower Chamfer\\r\\ndistance, despite the number of positional encoding frequencies.\\r\\nThis experiment also shows that our local MLP representation can\\r\\naccelerate the convergence: it can achieve lower Chamfer distance\\r\\nwith the same number of epochs.\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nW/o \\xf0\\x9d\\x93\\x9brgb\\r\\n\\r\\nW/o \\xf0\\x9d\\x93\\x9bsil\\r\\n\\r\\nW/o \\xf0\\x9d\\x93\\x9breg\\r\\n\\r\\nFull\\r\\n\\r\\nFig. 13. Ablation study of loss terms on the cat object.\\r\\n\\r\\nMetric:\\r\\nChamferdistance\\r\\n\\r\\nFig. 12. The influence of MLP number (#MLP) and the MLP architecture\\r\\nmeasured by Chamfer distance between reconstructed mesh and ground\\r\\ntruth mesh. PE_\\xf0\\x9d\\x91\\x98/\\xf0\\x9d\\x91\\x91_\\xf0\\x9d\\x91\\x99: positional encoding L=\\xf0\\x9d\\x91\\x98, MLP with \\xf0\\x9d\\x91\\x99 layer of \\xf0\\x9d\\x91\\x91 dimension. All the results are obtained after training networks with 300 epochs.\\r\\nBack slash means the training can not be performed due to computational\\r\\ncost.\\r\\n\\r\\nW/o \\xf0\\x9d\\x93\\x9bncorr\\r\\n\\r\\n5.2\\r\\n\\r\\nAblation Study\\r\\n\\r\\nWe remove each loss term individually to evaluate its impact on the\\r\\nreconstruction result of the cat object, as shown in Fig. 13. From\\r\\nthe results, we can see that without correspondence loss L\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f ,\\r\\nthe overall shape cannot be reconstructed correctly, e.g. tail of\\r\\nthe cat. As a result, the ray-cell correspondences are essential for\\r\\nour method to obtain surface details. Moreover, when any of the\\r\\nno-correspondence loss L\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x9f , RGB loss L\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x8f , regularization loss\\r\\nL\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x91\\x94 , and silhouette loss L\\xf0\\x9d\\x91\\xa0\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x99 is omitted, the reconstructed result\\r\\ncontains artifacts, especially at folded areas. The results verify that\\r\\nall the losses are essential to the quality of the transparent object\\r\\nreconstruction.\\r\\n\\r\\n6\\r\\n\\r\\nLIMITATION AND FUTURE WORK\\r\\n\\r\\nOur environment matting algorithm can only find correspondences\\r\\nassuming the transparent object has no intrinsic color. As a result, our method can not reconstruct colored transparent objects.\\r\\nAnother limitation is that, compared with ground truth mesh, the\\r\\nresults of our method still lack some details, especially for surfaces\\r\\nwith complex occlusions. In the future, it would be interesting to\\r\\ninvestigate how to integrate the variables for the color or other\\r\\nmaterial properties of the transparent object to overcome the \\xe2\\x80\\x9cno\\r\\nintrinsic color\\xe2\\x80\\x9d limitation. To reduce the capturing efforts, we will\\r\\nalso investigate the relationship between the number of grid cells\\r\\nand the number of images required in optimization. Another direction is to accelerate the training through a carefully tuned CUDA\\r\\nimplementation.\\r\\n\\r\\n7\\r\\n\\r\\nCONCLUSIONS\\r\\n\\r\\nWe have developed a method to reconstruct 3D shapes of transparent objects from hand-held captured images under natural light\\r\\nconditions. Our method has two components: a surface-based MLP\\r\\nrepresentation that encodes the vertex displacement field based\\r\\non an initial shape, the surface optimization through differentiable\\r\\nrendering and environment matting. We use a iPad as background\\r\\nto provide ray-cell correspondences, a simplified capture setting, to\\r\\nfacilitate the optimization. Our method can produce high-quality reconstruction results with fine details under natural light conditions.\\r\\n\\r\\nREFERENCES\\r\\nMatan Atzmon and Yaron Lipman. 2020. SAL: Sign Agnostic Learning of Shapes From\\r\\nRaw Data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition (CVPR).\\r\\n\\r\\n\\x0c\\n\\nA Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction\\r\\n\\r\\nSai Praveen Bangaru, Tzu-Mao Li, and Fr\\xc3\\xa9do Durand. 2020. Unbiased warped-area\\r\\nsampling for differentiable rendering. ACM Transactions on Graphics (TOG) 39, 6\\r\\n(2020), 1\\xe2\\x80\\x9318.\\r\\nM. Born. 1999. Principles of optics: electromagnetic theory of propagation, interference\\r\\nand diffraction of light (7. ed.). Principles of optics - electromagnetic theory of\\r\\npropagation, interference and diffraction of light (7. ed.).\\r\\nCapturingReality. 2016. Reality capture, http://capturingreality.com.\\r\\nVisesh Chari and Peter Sturm. 2013. A theory of refractive photo-light-path triangulation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\r\\nRecognition. 1438\\xe2\\x80\\x931445.\\r\\nAnpei Chen, Minye Wu, Yingliang Zhang, Nianyi Li, Jie Lu, Shenghua Gao, and Jingyi\\r\\nYu. 2018b. Deep surface light fields. Proceedings of the ACM on Computer Graphics\\r\\nand Interactive Techniques 1, 1 (2018), 1\\xe2\\x80\\x9317.\\r\\nGuanying Chen, Kai Han, and Kwan-Yee K Wong. 2018a. Tom-net: Learning transparent\\r\\nobject matting from a single image. In Proceedings of the IEEE Conference on Computer\\r\\nVision and Pattern Recognition. 9233\\xe2\\x80\\x939241.\\r\\nTongbo Chen, Michael Goesele, and H-P Seidel. 2006. Mesostructure from specularity.\\r\\nIn 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\\r\\n(CVPR\\xe2\\x80\\x9906), Vol. 2. IEEE, 1825\\xe2\\x80\\x931832.\\r\\nZhiqin Chen and Hao Zhang. 2019. Learning Implicit Fields for Generative Shape\\r\\nModeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition (CVPR).\\r\\nYung-Yu Chuang, Douglas E Zongker, Joel Hindorff, Brian Curless, David H Salesin, and\\r\\nRichard Szeliski. 2000. Environment matting extensions: Towards higher accuracy\\r\\nand real-time capture. In Proceedings of the 27th annual conference on Computer\\r\\ngraphics and interactive techniques. 121\\xe2\\x80\\x93130.\\r\\nDavid Cohen-Steiner, Pierre Alliez, and Mathieu Desbrun. 2004. Variational shape\\r\\napproximation. In ACM SIGGRAPH 2004 Papers. 905\\xe2\\x80\\x93914.\\r\\nKeenan Crane, Clarisse Weischedel, and Max Wardetzky. 2017. The Heat Method for\\r\\nDistance Computation. Commun. ACM 60, 11 (Oct. 2017), 90\\xe2\\x80\\x9399. https://doi.org/\\r\\n10.1145/3131280\\r\\nZhaopeng Cui, Jinwei Gu, Boxin Shi, Ping Tan, and Jan Kautz. 2017. Polarimetric multiview stereo. In Proceedings of the IEEE conference on computer vision and pattern\\r\\nrecognition. 1558\\xe2\\x80\\x931567.\\r\\nQi Duan, Jianfei Cai, and Jianmin Zheng. 2015. Compressive environment matting. The\\r\\nVisual Computer 31, 12 (2015), 1587\\xe2\\x80\\x931600.\\r\\nCong Phuoc Huynh, Antonio Robles-Kelly, and Edwin Hancock. 2010. Shape and refractive index recovery from single-view polarisation images. In 2010 IEEE Computer\\r\\nSociety Conference on Computer Vision and Pattern Recognition. IEEE, 1229\\xe2\\x80\\x931236.\\r\\nIvo Ihrke, Kiriakos N Kutulakos, Hendrik PA Lensch, Marcus Magnor, and Wolfgang Heidrich. 2010. Transparent and specular object reconstruction. In Computer Graphics\\r\\nForum, Vol. 29. Wiley Online Library, 2400\\xe2\\x80\\x932426.\\r\\nHiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. 2018. Neural 3d mesh renderer.\\r\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition.\\r\\n3907\\xe2\\x80\\x933916.\\r\\nD. Kingma and J. Ba. 2015. Adam: A Method for Stochastic Optimization. CoRR\\r\\nabs/1412.6980 (2015).\\r\\nKiriakos N Kutulakos and Steven M Seitz. 2000. A theory of shape by space carving.\\r\\nInternational journal of computer vision 38, 3 (2000), 199\\xe2\\x80\\x93218.\\r\\nKiriakos N Kutulakos and Eron Steger. 2008. A theory of refractive and specular 3D\\r\\nshape by light-path triangulation. International Journal of Computer Vision 76, 1\\r\\n(2008), 13\\xe2\\x80\\x9329.\\r\\nSamuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo\\r\\nAila. 2020. Modular primitives for high-performance differentiable rendering. ACM\\r\\nTransactions on Graphics (TOG) 39, 6 (2020), 1\\xe2\\x80\\x9314.\\r\\nAnat Levin, Dani Lischinski, and Yair Weiss. 2007. A closed-form solution to natural\\r\\nimage matting. IEEE transactions on pattern analysis and machine intelligence 30, 2\\r\\n(2007), 228\\xe2\\x80\\x93242.\\r\\nTzu-Mao Li. 2019. Differentiable visual computing. arXiv preprint arXiv:1904.12228\\r\\n(2019).\\r\\nTzu-Mao Li, Miika Aittala, Fr\\xc3\\xa9do Durand, and Jaakko Lehtinen. 2018a. Differentiable\\r\\nmonte carlo ray tracing through edge sampling. ACM Transactions on Graphics\\r\\n(TOG) 37, 6 (2018), 1\\xe2\\x80\\x9311.\\r\\nTzu-Mao Li, Miika Aittala, Fr\\xc3\\xa9do Durand, and Jaakko Lehtinen. 2018b. Differentiable\\r\\nMonte Carlo Ray Tracing through Edge Sampling. ACM Trans. Graph. (Proc. SIGGRAPH Asia) 37, 6 (2018), 222:1\\xe2\\x80\\x93222:11.\\r\\nTzu-Mao Li, Jaakko Lehtinen, Ravi Ramamoorthi, Wenzel Jakob, and Fr\\xc3\\xa9do Durand. 2015.\\r\\nAnisotropic Gaussian mutations for Metropolis light transport through HessianHamiltonian dynamics. ACM Transactions on Graphics (TOG) 34, 6 (2015), 1\\xe2\\x80\\x9313.\\r\\nZhengqin Li, Yu-Ying Yeh, and Manmohan Chandraker. 2020. Through the looking\\r\\nglass: neural 3D reconstruction of transparent shapes. In Proceedings of the IEEE/CVF\\r\\nConference on Computer Vision and Pattern Recognition. 1262\\xe2\\x80\\x931271.\\r\\nShichen Liu, Tianye Li, Weikai Chen, and Hao Li. 2019. Soft rasterizer: A differentiable\\r\\nrenderer for image-based 3d reasoning. In Proceedings of the IEEE/CVF International\\r\\nConference on Computer Vision. 7708\\xe2\\x80\\x937717.\\r\\n\\r\\n\\xe2\\x80\\xa2\\r\\n\\r\\n11\\r\\n\\r\\nMatthew M Loper and Michael J Black. 2014. OpenDR: An approximate differentiable\\r\\nrenderer. In European Conference on Computer Vision. Springer, 154\\xe2\\x80\\x93169.\\r\\nIlya Loshchilov and Frank Hutter. 2016. Sgdr: Stochastic gradient descent with warm\\r\\nrestarts. arXiv preprint arXiv:1608.03983 (2016).\\r\\nFujun Luan, Shuang Zhao, Kavita Bala, and Ioannis Gkioulekas. 2020. Langevin monte\\r\\ncarlo rendering with gradient-based adaptation. ACM Trans. Graph. 39, 4 (2020),\\r\\n140.\\r\\nJiahui Lyu, Bojian Wu, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. 2020. Differentiable refraction-tracing for mesh reconstruction of transparent objects. ACM\\r\\nTransactions on Graphics (TOG) 39, 6 (2020), 1\\xe2\\x80\\x9313.\\r\\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas\\r\\nGeiger. 2019. Occupancy Networks: Learning 3D Reconstruction in Function Space.\\r\\nIn Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).\\r\\nB. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and N. Ren.\\r\\n2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In\\r\\nECCV, Springer.\\r\\nDaisuke Miyazaki and Katsushi Ikeuchi. 2005. Inverse polarization raytracing: estimating surface shapes of transparent objects. In 2005 IEEE Computer Society Conference\\r\\non Computer Vision and Pattern Recognition (CVPR\\xe2\\x80\\x9905), Vol. 2. IEEE, 910\\xe2\\x80\\x93917.\\r\\nNigel JW Morris and Kiriakos N Kutulakos. 2007. Reconstructing the surface of inhomogeneous transparent scenes by scatter-trace photography. In 2007 IEEE 11th\\r\\nInternational Conference on Computer Vision. IEEE, 1\\xe2\\x80\\x938.\\r\\nNigel JW Morris and Kiriakos N Kutulakos. 2011. Dynamic refraction stereo. IEEE\\r\\ntransactions on pattern analysis and machine intelligence 33, 8 (2011), 1518\\xe2\\x80\\x931531.\\r\\nBaptiste Nicolet, Alec Jacobson, and Wenzel Jakob. 2021. Large steps in inverse rendering\\r\\nof geometry. ACM Transactions on Graphics (TOG) 40, 6 (2021), 1\\xe2\\x80\\x9313.\\r\\nMerlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wenzel Jakob. 2019. Mitsuba 2:\\r\\nA retargetable forward and inverse renderer. ACM Transactions on Graphics (TOG)\\r\\n38, 6 (2019), 1\\xe2\\x80\\x9317.\\r\\nMichael Oechsle, Songyou Peng, and Andreas Geiger. 2021. Unisurf: Unifying neural\\r\\nimplicit surfaces and radiance fields for multi-view reconstruction.\\r\\nE. Olson. 2011. AprilTag: A robust and flexible visual fiducial system. In Robotics and\\r\\nAutomation (ICRA), 2011 IEEE International Conference on.\\r\\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape\\r\\nRepresentation. In The IEEE Conference on Computer Vision and Pattern Recognition\\r\\n(CVPR).\\r\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\\r\\nDeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch. (2017).\\r\\nPieter Peers and Philip Dutr\\xc3\\xa9. 2003. Wavelet environment matting. In Proceedings of\\r\\nthe 14th Eurographics workshop on Rendering. 157\\xe2\\x80\\x93166.\\r\\nThomas Porter and Tom Duff. 1984. Compositing digital images. In Proceedings of the\\r\\n11th annual conference on Computer graphics and interactive techniques. 253\\xe2\\x80\\x93259.\\r\\nYiming Qian, Minglun Gong, and Yee-Hong Yang. 2015. Frequency-based environment\\r\\nmatting by compressive sensing. In Proceedings of the IEEE International Conference\\r\\non Computer Vision. 3532\\xe2\\x80\\x933540.\\r\\nYiming Qian, Minglun Gong, and Yee Hong Yang. 2016. 3d reconstruction of transparent\\r\\nobjects with position-normal consistency. In Proceedings of the IEEE Conference on\\r\\nComputer Vision and Pattern Recognition. 4369\\xe2\\x80\\x934377.\\r\\nYiming Qian, Minglun Gong, and Yee-Hong Yang. 2017. Stereo-based 3D reconstruction\\r\\nof dynamic fluid surfaces by global optimization. In Proceedings of the IEEE conference\\r\\non computer vision and pattern recognition. 1269\\xe2\\x80\\x931278.\\r\\nNikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin\\r\\nJohnson, and Georgia Gkioxari. 2020. Accelerating 3D Deep Learning with PyTorch3D. arXiv:2007.08501 (2020).\\r\\nEdoardo Remelli, Artem Lukoianov, Stephan R Richter, Beno\\xc3\\xaet Guillard, Timur Bagautdinov, Pierre Baque, and Pascal Fua. 2020. Meshsdf: Differentiable iso-surface\\r\\nextraction. arXiv preprint arXiv:2006.03997 (2020).\\r\\nShreeyak Sajjan, Matthew Moore, Mike Pan, Ganesh Nagaraja, Johnny Lee, Andy Zeng,\\r\\nand Shuran Song. 2020. Clear grasp: 3d shape estimation of transparent objects\\r\\nfor manipulation. In 2020 IEEE International Conference on Robotics and Automation\\r\\n(ICRA). IEEE, 3634\\xe2\\x80\\x933642.\\r\\nR. Schnabel, R. Wahl, and R. Klein. 2010. Efficient RANSAC for Point-Cloud Shape\\r\\nDetection. In John Wiley & Sons, Ltd. 214\\xe2\\x80\\x93226.\\r\\nYuliy Schwartzburg, Romain Testuz, Andrea Tagliasacchi, and Mark Pauly. 2014. Highcontrast computational caustic design. ACM Transactions on Graphics (TOG) 33, 4\\r\\n(2014), 1\\xe2\\x80\\x9311.\\r\\nQi Shan, Sameer Agarwal, and Brian Curless. 2012. Refractive height fields from\\r\\nsingle and multiple images. In 2012 IEEE Conference on Computer Vision and Pattern\\r\\nRecognition. IEEE, 286\\xe2\\x80\\x93293.\\r\\nJonathan Stets, Zhengqin Li, Jeppe Revall Frisvad, and Manmohan Chandraker. 2019.\\r\\nSingle-shot analysis of refractive shape using convolutional neural networks. In 2019\\r\\nIEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 995\\xe2\\x80\\x931003.\\r\\nKenichiro Tanaka, Yasuhiro Mukaigawa, Hiroyuki Kubo, Yasuyuki Matsushita, and\\r\\nYasushi Yagi. 2016. Recovering transparent shape from time-of-flight distortion.\\r\\n\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\n\\x0c\\n\\n12\\r\\n\\r\\n\\xe2\\x80\\xa2\\r\\n\\r\\nJiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu\\r\\n\\r\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\\r\\n4387\\xe2\\x80\\x934395.\\r\\nBorislav Trifonov, Derek Bradley, and Wolfgang Heidrich. 2006. Tomographic reconstruction of transparent objects. In ACM SIGGRAPH 2006 Sketches. 55\\xe2\\x80\\x93es.\\r\\nChia-Yin Tsai, Ashok Veeraraghavan, and Aswin C Sankaranarayanan. 2015. What\\r\\ndoes a single light-ray reveal about a transparent object?. In 2015 IEEE International\\r\\nConference on Image Processing (ICIP). IEEE, 606\\xe2\\x80\\x93610.\\r\\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping\\r\\nWang. 2021. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\\r\\nMulti-view Reconstruction. arXiv preprint arXiv:2106.10689 (2021).\\r\\nGordon Wetzstein, David Roodnick, Wolfgang Heidrich, and Ramesh Raskar. 2011.\\r\\nRefractive shape from light field distortion. In 2011 International Conference on\\r\\nComputer Vision. IEEE, 1180\\xe2\\x80\\x931186.\\r\\nYdo Wexler, Andrew Fitzgibbon, and Andrew Zisserman. 2002. Image-based environment matting. (2002).\\r\\nBojian Wu, Yang Zhou, Yiming Qian, Minglun Gong, and Hui Huang. 2018. Full 3D\\r\\nreconstruction of transparent objects. arXiv preprint arXiv:1805.03482 (2018).\\r\\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural\\r\\nimplicit surfaces. Advances in Neural Information Processing Systems 34 (2021).\\r\\n\\r\\nACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: March 2022.\\r\\n\\r\\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, and\\r\\nYaron Lipman. 2020. Multiview neural surface reconstruction by disentangling\\r\\ngeometry and appearance. arXiv preprint arXiv:2003.09852 (2020).\\r\\nYonghao Yue, Kei Iwasaki, Bing-Yu Chen, Yoshinori Dobashi, and Tomoyuki Nishita.\\r\\n2014. Poisson-based continuous surface generation for goal-based caustics. ACM\\r\\nTransactions on Graphics (TOG) 33, 3 (2014), 1\\xe2\\x80\\x937.\\r\\nCheng Zhang, Bailey Miller, Kan Yan, Ioannis Gkioulekas, and Shuang Zhao. 2020.\\r\\nPath-space differentiable rendering. ACM transactions on graphics 39, 4 (2020).\\r\\nCheng Zhang, Lifan Wu, Changxi Zheng, Ioannis Gkioulekas, Ravi Ramamoorthi, and\\r\\nShuang Zhao. 2019. A differential theory of radiative transfer. ACM Transactions on\\r\\nGraphics (TOG) 38, 6 (2019), 1\\xe2\\x80\\x9316.\\r\\nMingjie Zhang, Xing Lin, Mohit Gupta, Jinli Suo, and Qionghai Dai. 2014. Recovering\\r\\nscene geometry under wavy fluid via distortion and defocus analysis. In European\\r\\nConference on Computer Vision. Springer, 234\\xe2\\x80\\x93250.\\r\\nQian-Yi Zhou, Jaesik Park, and Vladlen Koltun. 2018. Open3D: A Modern Library for\\r\\n3D Data Processing. arXiv:1801.09847 (2018).\\r\\nDouglas E Zongker, Dawn M Werner, Brian Curless, and David H Salesin. 1999. Environment matting and compositing. In Proceedings of the 26th annual conference on\\r\\nComputer graphics and interactive techniques. 205\\xe2\\x80\\x93214.\\r\\n\\r\\n\\x0c', b\"StructToken : Rethinking Semantic\\r\\nSegmentation with Structural Prior\\r\\nFangjian Lin1, \\xe2\\x8b\\x86 , Zhanhao Liang1, 2, \\xe2\\x8b\\x86 , Junjun He1 , Miao Zheng3 , Shengwei\\r\\nTian, Kai Chen1, 3,\\xe2\\x8b\\x86\\xe2\\x8b\\x86 ,\\r\\n1\\r\\n\\r\\nShanghai AI Laboratory, Shanghai, China\\r\\nBeijing University of Posts and Telecommunications\\r\\n3\\r\\nSenseTime Research\\r\\nlinfangjian@pjlab.org.cn, alexleung@bupt.edu.cn,\\r\\nhejunjun@sjtu.edu.cn, {zhengmiao, chenkai}@sensetime.com\\r\\n2\\r\\n\\r\\nAbstract. In this paper, we present structure token (StructToken), a\\r\\nnew paradigm for semantic segmentation. From a perspective on semantic segmentation as per-pixel classification, the previous deep learningbased methods learn the per-pixel representation first through an encoder\\r\\nand a decoder head and then classify each pixel representation to a specific category to obtain the semantic masks. Differently, we propose a\\r\\nstructure-aware algorithm that takes structural information as prior to\\r\\npredict semantic masks directly without per-pixel classification. Specifically, given an input image, the learnable structure token interacts with\\r\\nthe image representations to reason the final semantic masks. Three interaction approaches are explored and the results not only outperform\\r\\nthe state-of-the-art methods but also contain more structural information. Experiments are conducted on three widely used datasets including\\r\\nADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could serve as an alternative for semantic segmentation and inspire\\r\\nfuture research.\\r\\nKeywords: semantic segmentation, transformer, new paradigm, structural information\\r\\n\\r\\n1\\r\\n\\r\\nIntroduction\\r\\n\\r\\nWith the development of self-driving technology [14], human-computer interaction [16], and augmented reality [1], semantic segmentation has attracted more\\r\\nand more attention. Since Long et al. [29] proposed fully convolutional networks\\r\\n(FCN), per-pixel classification has become the classical paradigm of semantic\\r\\nimage segmentation, which aims to classify each pixel into a semantic category.\\r\\nThe semantic segmentation methods of the classical paradigm classify each\\r\\npixel by applying 1\\xc3\\x971 convolution to the feature map. Much work [42,6,38,24,35]\\r\\n\\xe2\\x8b\\x86\\r\\n\\xe2\\x8b\\x86\\xe2\\x8b\\x86\\r\\n\\r\\nEqual contributions\\r\\nCorresponding author\\r\\n\\r\\n\\x0c\\n\\n2\\r\\nFeature learning\\r\\n\\r\\n\\xc3\\x97\\r\\nK\\r\\n\\r\\nC\\r\\n(C\\xc3\\x97K)\\r\\nStatic Classifier\\r\\n\\r\\n(a): static per-pixel classification\\r\\n\\r\\nC\\r\\n(C\\xc3\\x97K)\\r\\n\\r\\nFeature learning\\r\\n1\\r\\n2\\r\\nKernel learning\\r\\n\\r\\nClass kernels\\r\\n\\r\\n\\xc3\\x97\\r\\n\\r\\nbackbone\\r\\n\\r\\nC\\r\\n\\r\\nK\\r\\n(K\\xc3\\x97H\\xc3\\x97W)\\r\\n\\r\\nDynamic Classifier\\r\\n\\r\\n(b): dynamic per-pixel classification\\r\\n\\r\\nKernel learning\\r\\n\\r\\nStructure Token\\r\\n\\r\\nK\\r\\n\\r\\n(c): structural prior paradigm\\r\\n\\r\\nFig. 1. Comparison with three semantic segmentation paradigms. C is the channel\\r\\nnumber, and K is the number of categories in the dataset. (a). means segmentation\\r\\nwith per-pixel classification, using a static classifier, i.e., 1 \\xc3\\x97 1 convolution kernels. (b).\\r\\nindicates segmentation mask prediction with per-pixel classification, using a dynamic\\r\\nclassifier to perform prediction. The \\xe2\\x80\\x9cred line\\xe2\\x80\\x9d represents the way of generating the\\r\\ndynamic classifier. \\xe2\\x80\\x9c\\xe2\\x91\\xa0\\xe2\\x80\\x9d: dynamic classifier is generated from the feature map. \\xe2\\x80\\x9c\\xe2\\x91\\xa1\\xe2\\x80\\x9d:\\r\\ndynamic classifier is generated from the class kernels. (c). represents segmentation\\r\\nmask prediction with structural prior, which uses structure token to predict score map\\r\\ndirectly.\\r\\n\\r\\nleveraged the enhancement of context information and the fusion of multi-scale\\r\\nfeatures to acquire excellent results. By exploring the information fusion of pixel\\r\\nrepresentations and expanding the receptive field of each pixel, the convolution\\r\\nin the tail of the network can classify each pixel better and produce more refined\\r\\nscore maps. However, this kind of work focuses on improving the representation\\r\\nability of the per-pixel features and doesn\\xe2\\x80\\x99t consider structural information in\\r\\nimages. Since the weights of convolution have been fixed after the training is\\r\\ncompleted, the per-pixel classification paradigm can be regarded as a static perpixel classification.\\r\\nIn contrast to static per-pixel classification, some recent work [32,9,8,41]\\r\\nstarts to study mask prediction, which uses a set of learnable tokens as a dynamic\\r\\nclassifier and obtains the score map by matrix multiplication of the tokens and\\r\\nthe feature map. Such methods take advantage of the classical paradigm mentioned above and are able to adapt the classifier to different feature maps (i.e.,\\r\\nthe learnable tokens are updated through the interaction with feature maps). As\\r\\nthe learnable tokens are dynamic to input, these methods achieve higher performance than static per-pixel classification. However, although the classifier is\\r\\ndynamic, it still acts on each pixel, and the nature of per-pixel classification does\\r\\nnot change. Thus, the mask prediction paradigm can be regarded as a dynamic\\r\\nper-pixel classification method.\\r\\nAs shown in Fig. 1 (a) and Fig. 1 (b), for the static per-pixel classification\\r\\nparadigm and dynamic per-pixel classification paradigm described above, their\\r\\nfundamental principles are the same. These two paradigms both learn a linear\\r\\ndiscriminant function for each class, which is the last 1 \\xc3\\x97 1 convolution in the\\r\\nstatic per-pixel classification paradigm and matrix multiplication in the dynamic\\r\\nper-pixel classification paradigm. Then each pixel is assigned a specific class with\\r\\nthe largest output of the linear discriminant function. The formula to describe\\r\\nthis process is:\\r\\n(1)\\r\\n  \\\\begin {aligned} y_k(x) = w_k ^T \\\\cdot x. \\\\end {aligned} \\\\label {eq1} \\r\\n\\r\\n\\x0c\\n\\n3\\r\\n\\r\\nwhere wk is the weights of yk (x), the discriminant function for k th class. Specifically, it assigns a pixel x to class Ck if yk (x) > yj (x) for all j \\xcc\\xb8= k.\\r\\nThe difference between them is that the classification weights of the classical\\r\\nper-pixel paradigm is fixed (static) in inference, while the classification weights\\r\\nof the dynamic per-pixel classification paradigm will be dynamically adjusted\\r\\n(dynamic) according to the input feature maps.\\r\\nJumping out of the existing semantic segmentation frameworks, we rethink\\r\\nsemantic segmentation tasks from a more anthropomorphic viewpoint. When\\r\\nobserving an object, people see the general shape and structure of the object first\\r\\nand then focus on the details. That is to say, when we want to identify objects of\\r\\ndifferent classes in a scene, we usually capture the structural features of semantic\\r\\nclasses first and then go to the details of the interior. For example, to label an\\r\\nimage into different classes, humans usually recognize the objects in the image\\r\\nbased on the understanding of the shape (structure) of the objects and roughly\\r\\ndetermine the masks of each semantic class. After that, humans concentrate on\\r\\na semantic class and gradually refine its mask. In summary, when we want to\\r\\nsegment an image into different semantic classes, we usually generate a rough\\r\\nmask according to the structure first and then adjust the details of each mask.\\r\\nHowever, for the two existing paradigms, the classical static per-pixel classification paradigm and dynamic per-pixel classification paradigm, a score map is\\r\\nobtained by classifying each pixel in the feature map of the penultimate layer.\\r\\nThis characteristic of per-pixel classification paradigm encourage the network to\\r\\noptimize the representation of single pixel to facilitate the classification process.\\r\\nWhereas, the most important structural feature is ignored and even destroyed.\\r\\nWe propose a structural prior paradigm to solve this problem, as shown in\\r\\nFig. 1 (c). Unlike the static per-pixel classification paradigm and dynamic perpixel classification paradigm, ours directly construct a score map from structure\\r\\ntoken to segment an image into different semantic classes. As shown in the\\r\\nvisual Fig. 3, our paradigm has splendid capacity for capturing the structural\\r\\nfeatures of the objects. Instead of learning a representation for each pixel then\\r\\nclassify it into one specific semantic class, structural prior paradigm imitates the\\r\\nway humans perform semantic segmentation. It produces a rough mask for each\\r\\nclass according to the structural prior and then refines the mask gradually, for\\r\\nexample, adjusting the edges. We believe that this paradigm is more suitable for\\r\\nsemantic segmentation and that structural features should play a more important\\r\\nrole in semantic segmentation.\\r\\nIn this work, unlike the classical paradigm of per-pixel semantic segmentation, we establish a new paradigm that directly uses structure token to generate\\r\\nscore maps. We mainly study how to extract useful information from the feature\\r\\nmaps according to the priors provided by the structure token. In particular, we\\r\\nexplore three interaction approaches to adapt structure token to different features and demonstrate the effectiveness of our methods by conducting extensive\\r\\nablation studies. Furthermore, we evaluate our StructToken on three challenging\\r\\nsemantic segmentation benchmarks, including ADE20K[44], COCO-Stuff 10K\\r\\n[2], and Cityscapes[10], achieving 54.18%, 49.07%, and 82.07% mIoU, respec-\\r\\n\\r\\n\\x0c\\n\\n4\\r\\n\\r\\ntively, which outperforms the state-of-the-art methods. Our main contributions\\r\\ninclude:\\r\\n\\xe2\\x80\\x93 We state that structural information plays a very important role in semantic\\r\\nsegmentation.\\r\\n\\xe2\\x80\\x93 We solve the problem that per-pixel classification paradigms destroy structural features.\\r\\n\\xe2\\x80\\x93 We are the first to propose a new structural prior paradigm for semantic\\r\\nsegmentation and explore a new development direction of semantic segmentation in the future.\\r\\n\\r\\n2\\r\\n\\r\\nRelated Work\\r\\n\\r\\nThis section briefly reviews the work related to semantic segmentation, including static per-pixel classification paradigm and dynamic per-pixel classification\\r\\nparadigm, and introduces the new structural prior paradigm.\\r\\n2.1\\r\\n\\r\\nStatic Per-pixel Classification\\r\\n\\r\\nSince fully convolutional Networks (FCN) [29] were proposed, per-pixel classification has dominated semantic segmentation. In addition, from 2015 to 2021,\\r\\nthe research of semantic segmentation mainly focuses on the aggregation of context information and multi-scale fusion methods. PSPNet [42] uses a pyramid\\r\\npooling module; the DeepLab series [4,5] introduce the dilated convolution. Furthermore, DANet [15], DSANet [20], CCNet [21], OCRNet [38] use non-local\\r\\nmodules to model context information. Recent work [43,31,32] begin to use ViT\\r\\n[13] as the backbone to capture long-range context information. Inspired by FPN\\r\\n[27], Semantic FPN [24] begins to employ a top-down pathway and lateral connections for progressively fusing multi-scale features. SETR [43] and DPT [31]\\r\\nuse ViT as the backbone to inherit and extend this architecture. The SegFormer\\r\\n[36] uses a transformer-based encoder and designs a lightweight MLP decoder to\\r\\nperform semantic segmentation. FPT [39] starts bringing transformer into the\\r\\ndesign of the decoder, using \\xe2\\x80\\x9cGrounding Transformer\\xe2\\x80\\x9d to fuse high-level and\\r\\nlow-level features.\\r\\nThe key point of the above work is to strengthen the semantic-level and\\r\\nspatial-level information of the final feature map, so that the pixel-level features\\r\\nof different semantic classes are linear separable in a high dimensional space.\\r\\nFinally, the static convolution kernel is used as the discriminant function to\\r\\nperform pixel-by-pixel classification.\\r\\n2.2\\r\\n\\r\\nDynamic Per-pixel Classification\\r\\n\\r\\nDynamic per-pixel classification updates the learnable class tokens by the interaction between the tokens and the feature map output from the backbone\\r\\nnetwork. To get a score map, the learnable class tokens perform matrix multiplication with the feature map. In this case, learnable class tokens that adjust\\r\\n\\r\\n\\x0c\\n\\n5\\r\\n\\r\\nN\\xc3\\x97\\r\\n\\r\\nFFN\\r\\n\\r\\n(a): Cross-Slice Extraction\\r\\n\\r\\nN layers\\r\\n\\r\\nC concatenation\\r\\nC\\xc3\\x97H\\xc3\\x97W\\r\\n\\r\\nk\\r\\n\\r\\nC v\\r\\n\\r\\nSplit\\r\\n\\r\\nq\\r\\n\\r\\n(b): Self-Slice Extraction\\r\\n\\r\\nC\\r\\n\\r\\nPWE\\r\\n\\r\\nq\\r\\n\\r\\nN\\xc3\\x97\\r\\n\\r\\nConv\\r\\nBlock\\r\\n\\r\\nK\\xc3\\x97H\\xc3\\x97W\\r\\n\\r\\nSSE\\r\\n\\r\\nCSE\\r\\n\\r\\nSplit\\r\\n\\r\\nFFN\\r\\n\\r\\nInteraction\\r\\n\\r\\nFFN\\r\\n\\r\\nInteraction\\r\\nv\\r\\nk\\r\\n\\r\\nbackbone\\r\\n\\r\\nSplit\\r\\n\\r\\n(c): Point-Wise Extraction\\r\\n\\r\\nInteraction\\r\\n\\r\\nFig. 2. Illustrating the pipeline of Structure Token(StructToken). (a), (b), and (c)\\r\\nrepresent three different interaction methods, respectively. See the method section for\\r\\ndetails.\\r\\n\\r\\ndynamically according to the feature maps replace the static weights of the last\\r\\n1 \\xc3\\x97 1 convolution in static per-pixel classification.\\r\\nLike ViT[13], Segmenter [32] employs the transformer to jointly process the\\r\\npatch and class embeddings (tokens) during the decoding phase and let the\\r\\nclass tokens perform matrix multiplication with the feature map to produce\\r\\nthe final score map. Maskformer [9] unified instance segmentation and semantic\\r\\nsegmentation architecture by performing matrix multiplication between class\\r\\ntokens and feature maps and using a binary matching mechanism. Mask2former\\r\\n[8] and K-Net [41] use learnable semantic tokens, which is equivalent to the class\\r\\ntokens, to replace 1\\xc3\\x971 convolution kernel, and use binary matching to unify\\r\\nsemantic, instance, and panoptic segmentation tasks.\\r\\nThe difference between dynamic per-pixel classification and static per-pixel\\r\\nclassification is that the former\\xe2\\x80\\x99s weights of the pixel classifier is dynamic, changing according to the feature map and it is involved in the computation before\\r\\nthe classification process. By contrast, the latter is static and is only involved in\\r\\nthe classification process. The adaptability to different feature maps makes the\\r\\ndynamic per-pixel classification paradigm classify each pixel more precisely.\\r\\n2.3\\r\\n\\r\\nStructural Prior Paradigm\\r\\n\\r\\nFor both static classifier and dynamic classifier, their essence is not divorced from\\r\\nmapping the feature of each pixel to a linear separable space and using the kernel\\r\\nto classify each pixel into a category. This pixel-by-pixel classification destroys\\r\\nthe structural information of the original image. To preserve and utilize the\\r\\nstructural information, we introduce the structural prior paradigm. It leverages\\r\\nstructural priors for each semantic class and extracts useful information from\\r\\nthe feature maps to directly constructs the score maps from the priors.\\r\\n\\r\\n\\x0c\\n\\n6\\r\\n\\r\\n3\\r\\n\\r\\nMethod\\r\\n\\r\\nThis section first describes our framework and then details the three variations of the structural prior paradigm: StructToken-CSE, StructToken-SSE, and\\r\\nStructToken-PWE.\\r\\n\\r\\n3.1\\r\\n\\r\\nFramework\\r\\n\\r\\nThe overall framework of our StructToken is shown in Figure 2. Each basic block\\r\\nconsists of an Interaction module and a Feed Forward Networks (FFN) module.\\r\\nGiven an input image I \\xe2\\x88\\x88 R3\\xc3\\x97H\\xc3\\x97W , where H and W denote the height and\\r\\nwidth, respectively. We first use a transformer backbone (such as ViT [13]) to\\r\\nH\\r\\nW\\r\\nget the feature map F \\xe2\\x88\\x88 RC\\xc3\\x97 16 \\xc3\\x97 16 . where C indicates the number of channels.\\r\\nThe learnable structure token S \\xe2\\x88\\x88 RK\\xc3\\x97N are randomly initialized. where K\\r\\nmeans the number of categories in the corresponding dataset. To avoid being\\r\\nconfused with K(key) in the transformer, we use Kclass to represent the number\\r\\nH\\r\\nof categories in the dataset in the following description. N is equal to 16\\r\\n\\xc3\\x97\\r\\nW\\r\\n,\\r\\nthe\\r\\nresulting\\r\\nnumber\\r\\nof\\r\\npatches.\\r\\nThen,\\r\\nwe\\r\\nuse\\r\\nthe\\r\\nInteraction\\r\\nmodule\\r\\nto\\r\\n16\\r\\ntransform S so that S can capture the structural information from the feature\\r\\nmap and construct a rough mask for each class according to the learned priors.\\r\\nNext, FFN module is used to refine the structure token and transform feature\\r\\nmap. The feature map and structure token pass through several basic blocks to\\r\\nproduce finer masks. Finally, we use a ConvBlock [19] to refine our constructed\\r\\nsegmentation masks and get the segmentation result. The ConvBlock consists of\\r\\ntwo convolution layers and a skip connection.\\r\\n\\r\\n3.2\\r\\n\\r\\nInteraction module\\r\\n\\r\\nMerely having the priors in structure token cannot construct the segmentation\\r\\nmasks. The primary purpose of the Interaction module is to extract the structure information from the feature map guided by the structural priors from S.\\r\\n[13] utilizes a weighted sum mechanism to extract information from all tokens.\\r\\nInspired by that, we use a similar idea to do the extraction. We produce a weight\\r\\nfor each slice of the feature map and use a weighted sum to extract the information from the feature map. Here we propose three methods to produce the\\r\\nweights.\\r\\n\\r\\nCross-Slice Extraction In the NLP transformer decoder [33], output embeddings extract the information from input embeddings using cross attention.\\r\\nInspired by that, we introduce cross attention to extract useful information. The\\r\\nclassical multi-head cross attention aims at the similarity between pixels and\\r\\ngenerates query, key, and value matrices through the linear layer. This process\\r\\n\\r\\n\\x0c\\n\\n7\\r\\n\\r\\ncan be described as follows:\\r\\n Q=\\\\phi (\\\\mathcal {F}_{1})\\\\in \\\\mathbb {R}^{N_1 \\\\times C},\\\\ K=\\\\phi (\\\\mathcal {F}_{2}) \\\\in \\\\mathbb {R}^{N_2 \\\\times C},\\\\ V = \\\\phi (\\\\mathcal {F}_{2}) \\\\in \\\\mathbb {R}^{N_2 \\\\times C}, \\\\\\\\ Attention=SoftMax( \\\\frac { Q \\\\times K^{T} }{\\\\sqrt C} )V \\\\in \\\\mathbb {R}^{N_1 \\\\times C}, \\\\label {eq2}\\r\\n(3)\\r\\nwhere F1 and F2 mean feature maps of different sizes or layers. Ni is the sequence length of features Fi , i=1, 2. The \\xcf\\x95 is a simple fully connected layer.\\r\\nNote that Q \\xc3\\x97 K T \\xe2\\x88\\x88 RN1 \\xc3\\x97N2 calculates the similarity of each pixel of F1 concerning each pixel of F2 , where the goal is to improve representation at the\\r\\npixel-level. However, for our Cross-Slice Extraction (CSE) module, we need to\\r\\nhave a clear goal to re-model structure token, extract information from feature\\r\\nmap and construct score map. Based on this, we design CSE module, which can\\r\\nbe formulated as follow:\\r\\n Q=\\\\mathscr {L}(\\\\mathcal {S}) \\\\in \\\\mathbb {R}^{K_{class} \\\\times N},\\\\ K=\\\\mathscr {L}(\\\\mathcal {F}) \\\\in \\\\mathbb {R}^{C \\\\times N},\\\\ V = \\\\mathscr {L}(\\\\mathcal {F}) \\\\in \\\\mathbb {R}^{C \\\\times N}, \\\\\\\\ \\\\mathcal {S}^{'}=SoftMax( \\\\frac { Q \\\\times K^{T} }{\\\\sqrt C} )V \\\\in \\\\mathbb {R}^{K_{class} \\\\times N}. \\\\label {eq3}\\r\\n(5)\\r\\nwhere S is the learnable structure token, and F refers to the feature map output\\r\\nby the backbone. Note that Q\\xc3\\x97K T \\xe2\\x88\\x88 RKclass \\xc3\\x97C calculates the similarity between\\r\\nthe slices of the feature map and the structure token. To generate query, key\\r\\nand value, the model requires a fully connected layer mapping of dimension N .\\r\\nHowever, we find that this makes it impossible to support multi-scale inference\\r\\nbecause N is fixed after training. To solve this problem, we redesign a mapping\\r\\nstrategy L , which consists of a 1\\xc3\\x971 convolution, a 3\\xc3\\x973 depth-wise convolution,\\r\\nand a 1\\xc3\\x971 convolution.\\r\\n\\r\\nSelf-Slice Extraction Our Self-Slice Extraction (SSE) module is a variant of\\r\\nthe CSE module. Model structure token through multi-head self-attention. The\\r\\nobjectives of SSE and CSE are the same. To be specific, first combine structure\\r\\ntoken with feature map via concatenation at the channel dimension, and then\\r\\nperform self-attention. The process can be described as follows:\\r\\n \\\\widehat {\\\\mathcal {F}}=Concat(\\\\mathcal {S}, \\\\mathcal {F}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ Q,\\\\ K,\\\\ V=\\\\mathscr {L}(\\\\widehat {\\\\mathcal {F}}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ Attention=SoftMax( \\\\frac { Q \\\\times K^{T} }{\\\\sqrt C} )V \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ \\\\mathcal {S}^{'}, \\\\mathcal {F}^{'} = Split(Attention). \\\\label {eq4}\\r\\n\\r\\n(9)\\r\\nwhere Q \\xc3\\x97 K T \\xe2\\x88\\x88 R(C+Kclass )\\xc3\\x97(C+Kclass ) calculates the similarity between each\\r\\nchannel of the concatenated feature map. \\xe2\\x80\\x9cSplit\\xe2\\x80\\x9d operation is used to split \\xe2\\x80\\x9cAt\\xe2\\x80\\xb2\\r\\n\\xe2\\x80\\xb2\\r\\ntention\\xe2\\x80\\x9d into feature map F and structure token S .\\r\\n\\r\\n\\x0c\\n\\n8\\r\\n\\r\\nPoint-Wise Extraction \\xe2\\x80\\x9cAttention\\xe2\\x80\\x9d \\xe2\\x88\\x88 R(C+Kclass )\\xc3\\x97(C+Kclass ) matrix is the\\r\\nweights for every slice of the feature map. After the production of attention matrix, weighted sum is computed to extract useful information. Instead of computing the similarity between slices to get the attention matrix, we can produce the\\r\\nattention matrix through predicting directly. To be specific, we use a point-wise\\r\\nconvolution(i.e., the kernel size of the point-wise convolution is also (C + Kclass )\\r\\n\\xc3\\x97 (C + Kclass )) to compute the weighted sum between slices, and the attention\\r\\nmatrix is learned by updating the weights of the point-wise convolution during\\r\\ntraining. Therefore, based on SSE module, we only replace the \\xe2\\x80\\x9cAttention\\xe2\\x80\\x9d part\\r\\nwith a 1\\xc3\\x971 convolution. So for Eq. (6\\xe2\\x80\\x939), everything else is the same except for\\r\\nEq. (7\\xe2\\x80\\x938). Here we rewrite it:\\r\\n \\\\widehat {\\\\mathcal {F}}^{'}=\\\\mathscr {L}(\\\\widehat {\\\\mathcal {F}}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ Attention= 1 \\\\times 1 \\\\ Convolution(\\\\widehat {\\\\mathcal {F}}^{'}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}. \\\\label {eq5}\\r\\n(11)\\r\\n3.3\\r\\n\\r\\nFFN module\\r\\n\\r\\nAfter Interaction, the feature map will pass through Feed Forward Networks\\r\\n(FFN). Normal FFN consists of several fully connected layers. However, as we\\r\\nsaid in the introduction, the human visual process often describes the structural\\r\\nfeatures of objects first and then details. The Interaction process can be regarded\\r\\nas extracting structural features from the feature map. We added a lightweight\\r\\n3\\xc3\\x973 grouped convolution to refine local features. It is also viewed as an implicit\\r\\nposition encoding. The pipeline of the whole FFN is composed of: a fully connected layer, a 3\\xc3\\x973 grouped convolution, and a fully connected layer. Finally,\\r\\n\\xe2\\x80\\xb2\\r\\n\\xe2\\x80\\xb2\\r\\nFFN is used to enhance the representative ability of S and F .\\r\\n\\r\\n4\\r\\n\\r\\nExperiments\\r\\n\\r\\nWe first introduce the datasets and implementation details. Then, we compare\\r\\nour method with the recent state-of-the-arts on three challenging semantic segmentation benchmarks. Finally, comprehensive ablation studies and visualizations analyses are conducted to evaluate the effectiveness of our approach.\\r\\n4.1\\r\\n\\r\\nDatasets\\r\\n\\r\\nADE20K[44] is a challenging scene parsing dataset, which is split into 20210,\\r\\n2000 images for training and validation, respectively. It has 150 fine-grained\\r\\nobject categories and diverse scenes with 1,038 image-level labels.\\r\\nCityscapes[10] carefully annotates 19 object categories of urban driveway landscape images. It contains 5K finely annotated images and is divided into 2975\\r\\nand 500 images for training and validation. It is a high-quality dataset.\\r\\nCOCO-Stuff-10K [2] is a significant scene parsing benchmark with 9000 training images and 1000 testing images. It has 171 categories. It is referred to as\\r\\nCOCO-Stuff in this paper.\\r\\n\\r\\n\\x0c\\n\\n9\\r\\nTable 1. Comparison with the state-of-the-art methods on the ADE20K dataset. \\xe2\\x80\\x9cSS\\xe2\\x80\\x9d\\r\\nand \\xe2\\x80\\x9cMS\\xe2\\x80\\x9d indicate single-scale inference and multi-scale inference, respectively. \\xe2\\x80\\x9c\\xe2\\x80\\xa0 \\xe2\\x80\\x9d\\r\\nmeans the ViT models trained from scratch with SAM optimizer on ImageNet-21k.\\r\\n\\xe2\\x80\\x9c\\xe2\\x88\\x97 \\xe2\\x80\\x9d represents our implementation under the same settings as the official repo.\\r\\nMethod\\r\\nBackbone\\r\\nGFLOPs Params mIoU(SS) mIoU(MS)\\r\\nFCN[29]\\r\\nResNet-101\\r\\n276\\r\\n69M\\r\\n39.91\\r\\n41.40\\r\\nEncNet[40]\\r\\nResNet-101\\r\\n219\\r\\n55M\\r\\n44.65\\r\\nOCRNet[38]\\r\\nHRNet-W48\\r\\n165\\r\\n71M\\r\\n43.25\\r\\n44.88\\r\\nCCNet[21]\\r\\nResNet-101\\r\\n278\\r\\n69M\\r\\n43.71\\r\\n45.04\\r\\nResNet-101\\r\\n263\\r\\n65M\\r\\n45.24\\r\\nANN[46]\\r\\nResNet-101\\r\\n256\\r\\n68M\\r\\n44.39\\r\\n45.35\\r\\nPSPNet[42]\\r\\nFPT[39]\\r\\nResNet-101\\r\\n45.90\\r\\nDeepLabV3+[6]\\r\\nResNet-101\\r\\n255\\r\\n63M\\r\\n45.47\\r\\n46.35\\r\\nDMNet[17]\\r\\nResNet-101\\r\\n274\\r\\n72M\\r\\n45.42\\r\\n46.76\\r\\nISNet[23]\\r\\nResNeSt-101\\r\\n47.55\\r\\nDPT[31]\\r\\nViT-Hybrid\\r\\n49.02\\r\\nViT-L\\xe2\\x80\\xa0\\r\\n328\\r\\n338M\\r\\n49.16\\r\\n49.52\\r\\nDPT\\xe2\\x88\\x97\\r\\nUperNet\\xe2\\x88\\x97\\r\\nViT-L\\xe2\\x80\\xa0\\r\\n710\\r\\n354M\\r\\n48.64\\r\\n50.00\\r\\nSETR[43]\\r\\nViT-L\\r\\n214\\r\\n310M\\r\\n48.64\\r\\n50.28\\r\\nMCIBI[22]\\r\\nViT-L\\r\\n50.80\\r\\nMiT-B5\\r\\n183\\r\\n85M\\r\\n51.80\\r\\nSegFormer[36]\\r\\nSETR-MLA\\xe2\\x88\\x97\\r\\nViT-L\\xe2\\x80\\xa0\\r\\n214\\r\\n310M\\r\\n50.45\\r\\n52.06\\r\\nSwin-L\\r\\n647\\r\\n234M\\r\\n52.10\\r\\n53.50\\r\\nUperNet[28]\\r\\nSegmenter[32]\\r\\nViT-L \\xe2\\x80\\xa0\\r\\n380\\r\\n342M\\r\\n51.80\\r\\n53.60\\r\\nStructToken-SSE\\r\\nVIT-L\\xe2\\x80\\xa0\\r\\n486\\r\\n395M\\r\\n52.82\\r\\n54.00\\r\\nStructToken-PWE\\r\\nVIT-L\\xe2\\x80\\xa0\\r\\n442\\r\\n379M\\r\\n52.95\\r\\n54.03\\r\\nStructToken-CSE\\r\\nVIT-L\\xe2\\x80\\xa0\\r\\n398\\r\\n350M\\r\\n52.84\\r\\n54.18\\r\\n\\r\\nTable 2. Comparison with the state-of-the-art methods on the Cityscapes validation\\r\\nset.\\r\\nMethod\\r\\nFCN[29]\\r\\nEncNet[40]\\r\\nPSPNet[42]\\r\\nGCNet[3]\\r\\nDNLNet[37]\\r\\nCCNet[21]\\r\\nSegmenter[32]\\r\\nSegmenter[32]\\r\\nStructToken-CSE\\r\\nStructToken-SSE\\r\\nStructToken-PWE\\r\\n\\r\\nBackbone GFLOPs Params mIoU(SS) mIoU(MS)\\r\\nResNet-101\\r\\n633\\r\\n69M\\r\\n75.52\\r\\n76.61\\r\\nResNet-101\\r\\n502\\r\\n55M\\r\\n76.10\\r\\n76.97\\r\\nResNet-101\\r\\n585\\r\\n68M\\r\\n78.87\\r\\n80.04\\r\\nResNet-101\\r\\n632\\r\\n69M\\r\\n79.18\\r\\n80.71\\r\\nResNet-101\\r\\n637\\r\\n69M\\r\\n79.41\\r\\n80.68\\r\\nResNet-101\\r\\n639\\r\\n69M\\r\\n79.45\\r\\n80.66\\r\\nDeiT-B\\r\\n79.00\\r\\n80.60\\r\\nViT-L\\r\\n553\\r\\n340M\\r\\n79.10\\r\\n81.30\\r\\nVIT-L\\r\\n567\\r\\n349M\\r\\n79.64\\r\\n81.98\\r\\nVIT-L\\r\\n651\\r\\n377M\\r\\n80.01\\r\\n82.02\\r\\nVIT-L\\r\\n600\\r\\n364M\\r\\n80.05\\r\\n82.07\\r\\n\\r\\n\\x0c\\n\\n10\\r\\nTable 3. Comparison with the state-of-the-art methods on the COCO-Stuff 10K\\r\\ndataset.\\r\\nMethod\\r\\nBackbone\\r\\nmIoU(MS)\\r\\nPSPNet[42]\\r\\nResNet-101\\r\\n38.86\\r\\nOCRNet[38]\\r\\nHRNet-W48\\r\\n39.50\\r\\nSVCNet[11]\\r\\nResNet-101\\r\\n39.60\\r\\nDANet[15]\\r\\nResNet-101\\r\\n39.70\\r\\nMaskFormer[9]\\r\\nResNet-101\\r\\n39.80\\r\\nEMANet[26]\\r\\nResNet-101\\r\\n39.90\\r\\nSpyGR[25]\\r\\nResNet-101\\r\\n39.90\\r\\nResNet-101\\r\\n40.10\\r\\nACNet[12]\\r\\nOCRNet[38]\\r\\nHRNetV2-W48\\r\\n40.50\\r\\nGINet[34]\\r\\nResNet-101\\r\\n40.60\\r\\nRecoNet[7]\\r\\nResNet-101\\r\\n41.50\\r\\nResNeSt-101\\r\\n42.08\\r\\nISNet[23]\\r\\nMCIBI[22]\\r\\nViT-L\\r\\n44.89\\r\\nStructToken-PWE\\r\\nVIT-L\\r\\n48.24\\r\\nVIT-L\\r\\n48.71\\r\\nStructToken-CSE\\r\\nStructToken-SSE\\r\\nVIT-L\\r\\n49.07\\r\\n\\r\\n4.2\\r\\n\\r\\nImplementation details\\r\\n\\r\\nWe use ViT[13] as the backbone. Following the default setting (e.g., data augmentation and training schedule) of public codebase mmsegmentation [30]. During training, data augmentation consists of three steps:(i) random horizontal\\r\\nflipping, (ii) random resize with the ratio between 0.5 and 2, (iii) random cropping (512\\xc3\\x97512 for ADE20K and COCO-Stuff-10K, and 768\\xc3\\x97768 for Cityscapes.\\r\\nIn addition, 640\\xc3\\x97640 with ViT-Large for ADE20K). For optimization, we adopt\\r\\na polynomial learning rate decay schedule; following prior works [28], we employ\\r\\nAdamW to optimize our model with 0.9 momenta and 0.01 weight decay; we set\\r\\nthe initial learning rate at 2e-5. The batch size is set to 16 for all datasets. The\\r\\ntotal iterations are 160k, 80k, 80k for ADE20K, Cityscapes and COCO-Stuff10k, respectively. For inference, we follow previous works [28,43] to average the\\r\\nmulti-scale (0.5, 0.75, 1.0, 1.25, 1.5, 1.75) predictions of our model. Interpolation\\r\\noperations are used for multi-scale inference. The slide-window test is applied\\r\\nhere. The performance is measured by the standard mean intersection of union\\r\\n(mIoU) in all experiments. Considering the effectiveness and efficiency, we adopt\\r\\nthe ViT-Tiny [13] as the backbone in the ablation study and also report comprehensive indicators on ADE20K.\\r\\n4.3\\r\\n\\r\\nComparisons with the state-of-the-art methods\\r\\n\\r\\nResults on ADE20K. Table 1 reports the comparison with the state-of-the-art\\r\\nmethods on the ADE20K validation set. From these results, it can be seen that\\r\\nour StructToken is +1.02%, +1.15% and +1.04% mIoU (52.82, 52.95 and 52.84\\r\\nvs. 51.80) higher than Segmenter [32] with the same input size (640\\xc3\\x97 640),\\r\\n\\r\\n\\x0c\\n\\n11\\r\\nTable 4. Compare the performance of ViT variants on the ADE20K dataset.\\r\\nMethod\\r\\nBackbone GFLOPs Params mIoU(SS) mIoU(MS)\\r\\nSegmenter\\r\\n6\\r\\n7M\\r\\n38.10\\r\\n38.80\\r\\nUpernet\\r\\n35\\r\\n11M\\r\\n38.93\\r\\n39.19\\r\\n10\\r\\n11\\r\\n39.88\\r\\n41.09\\r\\nSETR-MLA\\r\\nDPT\\r\\nViT-Tiny\\r\\n104\\r\\n17M\\r\\n40.82\\r\\n42.13\\r\\nStructToken-CSE\\r\\n7\\r\\n9M\\r\\n39.12\\r\\n40.23\\r\\nStructToken-SSE\\r\\n13\\r\\n14M\\r\\n40.81\\r\\n42.24\\r\\nStructToken-PWE\\r\\n10\\r\\n12M\\r\\n41.87\\r\\n42.99\\r\\nUpernet\\r\\n140\\r\\n42M\\r\\n45.53\\r\\n46.14\\r\\nSETR-MLA\\r\\n21\\r\\n27M\\r\\n44.85\\r\\n46.30\\r\\nSegmenter\\r\\nViT-Small\\r\\n22\\r\\n27M\\r\\n45.00\\r\\n46.90\\r\\nDPT\\r\\n118\\r\\n36M\\r\\n46.37\\r\\n47.45\\r\\nStructToken-CSE\\r\\n23\\r\\n30M\\r\\n45.86\\r\\n47.44\\r\\n37\\r\\n41M\\r\\n47.11\\r\\n49.07\\r\\nStructToken-SSE\\r\\nStructToken-PWE\\r\\n31\\r\\n38M\\r\\n47.36\\r\\n48.89\\r\\nUpernet\\r\\n292\\r\\n128M\\r\\n46.58\\r\\n47.47\\r\\nDPT\\r\\n171\\r\\n110M\\r\\n47.20\\r\\n47.86\\r\\nSETR-MLA\\r\\n65\\r\\n92M\\r\\n48.21\\r\\n49.32\\r\\nSegmenter\\r\\nViT-Base\\r\\n81\\r\\n107M\\r\\n49.00\\r\\n50.00\\r\\nStructToken-CSE\\r\\n86\\r\\n113M\\r\\n49.51\\r\\n50.87\\r\\n123\\r\\n142M\\r\\n50.72\\r\\n51.85\\r\\nStructToken-SSE\\r\\nStructToken-PWE\\r\\n105\\r\\n132M\\r\\n50.92\\r\\n51.82\\r\\n\\r\\nrespectively. When multi-scale testing is adopted, our StructToken is +0.4%,\\r\\n+0.43% and +0.58% mIoU (54.00, 54.03 and 54.18 vs. 53.60) higher than Segmenter, respectively. For ViT-Tiny, as shown in Table 4, our best results is\\r\\n+0.86% mIoU (42.99 vs. 42.13) higher than DPT[31] with the same input size\\r\\n(512 \\xc3\\x97 512), respectively. For ViT-Small, our best results is +1.44% mIoU (48.89\\r\\nvs. 47.45) higher than DPT. For ViT-Base, our best results is +1.82% mIoU\\r\\n(51.82 vs. 50.00) higher than Segmenter. Furthermore, the larger the model is,\\r\\nthe better the performance of StructToken is.\\r\\nResults on Cityscapes. Table 2 demonstrates the comparison results on the\\r\\nvalidation set of Cityscapes. The previous state-of-the-art method Segmenter\\r\\nwith ViT-Large achieves 79.10% mIoU. Our StructToken is +0.54%, +0.91% and\\r\\n+0.95% mIoU (79.64, 80.01 and 80.05 vs. 79.10) higher than it, respectively. For\\r\\nmulti-scale inference, our method is +0.68%, +0.72% and +0.77% mIou (81.98,\\r\\n82.02, 82.07 vs. 81.30) higher than Segmenter, respectively.\\r\\nResults on COCO-Stuff 10K. Table 3 compares the segmentation results on\\r\\nthe COCO-Stuff 10K testing set. It can be seen that our StructToken-SSE can\\r\\nachieve 49.07%, and our method is +4.18% mIoU higher than MCIBI [22] (49.07\\r\\nvs. 44.89).\\r\\n\\r\\n\\x0c\\n\\n12\\r\\n\\r\\n4.4\\r\\n\\r\\nAblation study\\r\\n\\r\\nIn this section, we conduct extensive ablation studies to illustrate the effectiveness of each module in our method. We also give several design choices and\\r\\nshow their effects on the results. Our baseline model is ViT [13] followed by CSE\\r\\nmodule and FFN module without grouped convolution. Note that we did not\\r\\nperform ablation experiments using a fully connected layer to map q, k, and v\\r\\nmatrices because this approach does not support the multi-scale inference (i.e.,\\r\\nmap the length of the feature map). All the models in the following experiments\\r\\nadopt ViT-Tiny as the backbone and are trained on ADE20K training set for\\r\\n160K iterations.\\r\\nAs shown in Table 5, we experimented with adding a grouped convolution(GPConv) layer to the FFN module and a ConvBlock to the model. In\\r\\naddition, the FLOPs of FFN-DWConv are only 0.002G, which is ignored in Table 5. It is a lightweight convolution layer, and the performance of the model\\r\\nreaches 39.12% mIoU after the FFN module and ConvBlock module are added,\\r\\nwhich is +1.41% mIoU (39.12 vs. 37.71) higher than the base model, and +1.33\\r\\n% mIoU (40.23 vs. 38.90) for multi-scale inference.\\r\\nFurthermore, we use StructToken-CSE to conduct experiments with different numbers of blocks. Each block is composed of an Interaction module and\\r\\na FFN module. As shown in Table 6, as the number of blocks increases, the\\r\\nmodel\\xe2\\x80\\x99s performance also increases. However, considering the trade-off between\\r\\nperformance and computation complexity and the number of parameters, we\\r\\nchose 4 as the number of blocks for all experiments, including StructToken-SSE\\r\\nand StructToken-PWE. It is worth mentioning that as the number of blocks\\r\\nincreases, the performance also increases, so the results listed above are still far\\r\\nfrom the best performance the StrucToken can have.\\r\\nTable 5. Ablation study on the ADE20K dataset. \\xe2\\x80\\x9cBase\\xe2\\x80\\x9d means using StructTokenCSE as the base module and ViT-Tiny as the backbone. \\xe2\\x80\\x9cFFN-GPConv\\xe2\\x80\\x9d indicates the\\r\\nFFN module with grouped convolution.\\r\\nBase FFN-GPConv ConvBlock GFLOPs Params mIoU(SS) mIoU(MS)\\r\\n\\xe2\\x88\\x9a\\r\\n6.74\\r\\n8.5M\\r\\n37.71\\r\\n38.90\\r\\n\\xe2\\x88\\x9a\\r\\n\\xe2\\x88\\x9a\\r\\n6.74\\r\\n8.5M\\r\\n38.17\\r\\n38.85\\r\\n\\xe2\\x88\\x9a\\r\\n\\xe2\\x88\\x9a\\r\\n7.16\\r\\n8.9M\\r\\n38.01\\r\\n39.29\\r\\n\\xe2\\x88\\x9a\\r\\n\\xe2\\x88\\x9a\\r\\n\\xe2\\x88\\x9a\\r\\n7.16\\r\\n8.9M\\r\\n39.12\\r\\n40.23\\r\\n\\r\\n4.5\\r\\n\\r\\nVisual analysis\\r\\n\\r\\nWe design a per-pixel classification paradigm as a counterpart to prove that\\r\\nour structural prior paradigm preserves the structural information better. This\\r\\ncounterpart maps the feature map from the backbone network to the dimension\\r\\nof the number of classes. By doing this, the feature map in the network have the\\r\\n\\r\\n\\x0c\\n\\n13\\r\\nTable 6. Compare the performance of different number of Blocks on the ADE20K\\r\\ndataset. Here, ViT-Tiny is used as the backbone.\\r\\nMethod\\r\\n\\r\\nBlocks GFLOPs Params mIoU(SS) mIoU(MS)\\r\\n1\\r\\n4.73\\r\\n6.7M\\r\\n37.66\\r\\n38.83\\r\\n2\\r\\n5.54\\r\\n7.4M\\r\\n37.08\\r\\n38.05\\r\\nStructToken-CSE\\r\\n4\\r\\n7.16\\r\\n8.9M\\r\\n39.12\\r\\n40.23\\r\\n6\\r\\n8.77\\r\\n10.3M\\r\\n39.29\\r\\n40.61\\r\\n8\\r\\n10.39 11.8M\\r\\n39.61\\r\\n41.04\\r\\n\\r\\nproperty of structure token that each slice of the feature map corresponds to the\\r\\nmask of one class. Then we use four residual blocks [18] to transform the feature\\r\\nmap. Finally, we use a 1 \\xc3\\x97 1 convolution to generate score map. The feature map\\r\\noutput from each residual block is compared with the structure token output\\r\\nfrom each interaction block. To demonstrate the outstanding structure capturing\\r\\ncapability of our structure token, we visualize the slices of structure token and\\r\\nthat of the feature map from the counterpart in Fig. 3.\\r\\nSpecifically, we get the structure tokens/feature maps before transform blocks\\r\\nand after each transform block. Then we visualize the corresponding slices of\\r\\nclass person, car, airplane, and tree in these structure tokens/feature maps. The\\r\\nslices of these four classes in the output score map are also visualized. We can\\r\\nsee that even though the output score maps are similar, the structure tokens and\\r\\nfeature maps in the counterpart are pretty different in previous blocks. In our\\r\\nstructural prior paradigm, we can explicitly see the structure of the semantic\\r\\nclass.\\r\\nIn the first row, even though many people are in the image, the structure\\r\\ntoken can still distinguish each person. This visualization verifies that our proposed paradigm captures the structure of each semantic class to produce the\\r\\ninitial masks. Then the network refines the initial masks to make the final score\\r\\nmap. The refinement process can be seen in row 9. Since the structure of trees is\\r\\ncomplex, the structure token after the first block highlights the rough tree structure with low confidence. We can see the clear structure in the slice of structure\\r\\ntoken after the second block, and with the structure token going deeper, the\\r\\nstructure is more detailed. In contrast, in the visualization of the slices of the\\r\\ncounterpart, we can only see the blurry structure(row 2 and row 4) or even no\\r\\nstructure(row 6, 8, and 10) of the semantic class until the 1 \\xc3\\x97 1 convolution\\r\\ntransform the feature maps to score maps.\\r\\nVisualization makes our point that the classical paradigm aims at learning\\r\\nbetter representation for each pixel. Therefore, the structural information in the\\r\\nspatial dimension is destroyed. This visual analysis provides strong evidence of\\r\\nthe strength of our paradigm in retaining structural information.\\r\\n\\r\\n\\x0c\\n\\n14\\r\\nImage\\r\\n\\r\\nStructToken / Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nours\\r\\n\\r\\n88 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n86 %\\r\\n\\r\\nours\\r\\n\\r\\n92 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n91 %\\r\\n\\r\\nours\\r\\n\\r\\n96 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n95 %\\r\\n\\r\\nours\\r\\n\\r\\n87 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n85 %\\r\\n\\r\\nours\\r\\n\\r\\n82 %\\r\\n\\r\\nperPixel\\r\\n\\r\\n86 %\\r\\n\\r\\nFig. 3. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to \\xe2\\x80\\x9cperson\\xe2\\x80\\x9d, \\xe2\\x80\\x9ccar\\xe2\\x80\\x9d, \\xe2\\x80\\x9cairplane\\xe2\\x80\\x9d and\\r\\n\\xe2\\x80\\x9ctree\\xe2\\x80\\x9d classes in structure token/feature map from top to bottom with similar performance of the two paradigms. \\xe2\\x80\\x9cStructToken\\xe2\\x80\\x9d refers to the slices from structure token\\r\\nand \\xe2\\x80\\x9cFeature map\\xe2\\x80\\x9d refers to the slices from feature maps. We design a four blocks\\r\\ndecoder as a per-pixel classification paradigm counterpart, and \\xe2\\x80\\x9cBlock 1-4\\xe2\\x80\\x9d represents\\r\\nthe slices from the structure token/feature maps after each block. \\xe2\\x80\\x9cOutput\\xe2\\x80\\x9d means the\\r\\nslices from the output score map. \\xe2\\x80\\x9cIoU\\xe2\\x80\\x9d represents the intersect over union score of\\r\\nthe specific class in this image. Details are at section 4.5.\\r\\n\\r\\n5\\r\\n\\r\\nConclusion\\r\\n\\r\\nIn this paper, we solve the problem that the classical per-pixel classification\\r\\nparadigm destroys the structure information and put forward a new structural\\r\\nprior paradigm different from the classical per-pixel classification paradigm. The\\r\\nsemantic segmentation task is reconsidered and analyzed in a more anthropomorphic way. We state that structural information plays an important role in\\r\\nscene analysis. Furthermore, the structure prior paradigm can perform different\\r\\ntasks by defining the size of the core, so it has universality for image-level tasks.\\r\\nWe hope this work can bring some fundamental enlightenment for semantic segmentation and even other tasks. Let us rethink the essential characteristics of\\r\\neach task more profoundly to analyze images and each task better.\\r\\n\\r\\n\\x0c\\n\\n15\\r\\n\\r\\nA\\r\\n\\r\\nDetails of the Point-Wise Extraction Module\\r\\n\\r\\nSelf-attention generates query, key and value for each token, and computes the\\r\\nweighted sum of values of all tokens to fuse the global information into one token.\\r\\nThe weight of each token is generated by computing the the similarity between\\r\\nthe query of the token into which the global information will be fused and the\\r\\nkeys of all tokens. The computation of similarity scores have quadratic computation complexity to the input image size, and thus the computational overhead\\r\\nis significant. [45] uses a linear layer to predict attention weights directly. We\\r\\nexplore another way to generate the attention weights, that is to \\xe2\\x80\\x9clearn\\xe2\\x80\\x9d the\\r\\nattention weights.\\r\\nIn our case, we want to compute the weighted sum of all the slices in the\\r\\nfeature map. 1 \\xc3\\x97 1 convolution can be deemed the calculation of the weighted\\r\\nsum of slices in a feature map, and the weights are the parameters of the convolution. To be specific, let F with k channels be the input feature map, F can\\r\\nbe represented as:\\r\\n \\\\mathcal {F} = [\\\\mathcal {S}_1, \\\\mathcal {S}_2, ..., \\\\mathcal {S}_k], \\\\mathcal {S}_i \\\\in \\\\mathbb {R}^{H \\\\times W}\\r\\n\\r\\n(12)\\r\\n\\r\\nwhere Si is a slice of the feature map.\\r\\n\\xe2\\x80\\xb2\\r\\nLet Sj (a,b) be the pixel in the j th slice of the output feature map with coordinate\\r\\n(a, b):\\r\\n \\\\mathcal {S}^{'}_{j\\\\_{}(a, b)} = w_{j, 1} \\\\times \\\\mathcal {S}_{1\\\\_{}(a, b)} + w_{j, 2} \\\\times \\\\mathcal {S}_{2\\\\_{}(a, b)} + ... + w_{j, k} \\\\times \\\\mathcal {S}_{k\\\\_{}(a, b)}\\r\\n\\r\\n(13)\\r\\n\\r\\nwhere the wj,1 , wj,2 , ..., wj,k are the parameters of the 1 \\xc3\\x97 1 convolution corresponding to the j th slice of the output feature map. Deriving from one pixel to\\r\\n\\xe2\\x80\\xb2\\r\\nall pixels in a slice, for slice Sj in the output feature map, it can be represented\\r\\nas:\\r\\n \\\\mathcal {S}^{'}_j = w_{j, 1} \\\\times \\\\mathcal {S}_1 + w_{j, 2} \\\\times \\\\mathcal {S}_2 + ... + w_{j, k} \\\\times \\\\mathcal {S}_k\\r\\n\\r\\n(14)\\r\\n\\r\\nTherefore, each slice of the output feature map is the weighted sum of all\\r\\nslices of the input feature map, and the weights are the parameters of the 1 \\xc3\\x97 1\\r\\nconvolution. They behave the same as the attention weights in self-attention.\\r\\nBy updating the parameters of the convolution during training, we can \\xe2\\x80\\x9clearn\\xe2\\x80\\x9d\\r\\nthe attention weights to better fuse the values mapping from the input of the\\r\\nPoint-Wise Extraction module. In summary, the Point-Wise Extraction module\\r\\ncan be formulated as:\\r\\n \\\\widehat {\\\\mathcal {F}}=Concat(\\\\mathcal {S}, \\\\mathcal {F}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ \\\\widehat {\\\\mathcal {F}}^{'}=\\\\mathscr {L}(\\\\widehat {\\\\mathcal {F}}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ Attention= 1 \\\\times 1 \\\\ Convolution(\\\\widehat {\\\\mathcal {F}}^{'}) \\\\in \\\\mathbb {R}^{(C+K_{class}) \\\\times N}, \\\\\\\\ \\\\mathcal {S}^{'}, \\\\mathcal {F}^{'} = Split(Attention). \\\\label {eq5}\\r\\n\\r\\n(18)\\r\\nwhere S refers to the input feature map, F refers to the structure token and L\\r\\nis our redesigned mapping strategy used to generate values from the slices of the\\r\\nconcatenated feature map.\\r\\n\\r\\n\\x0c\\n\\n16\\r\\n\\r\\nB\\r\\n\\r\\nVisualization\\r\\n\\r\\nWe add the visualization of slices of structure tokens from StructToken-CSE and\\r\\nStructToken-SSE. It can be seen from the visualization that all of our proposed\\r\\nmodels can capture the structure information well from the feature maps. Details\\r\\nare at section 4.5 of the body of our paper.\\r\\n\\r\\nImage StructToken/Feature map Block 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n96%\\r\\n\\r\\nSSE\\r\\n\\r\\n96%\\r\\n\\r\\nPWE\\r\\n\\r\\n97%\\r\\n\\r\\nperpixel\\r\\n\\r\\n97%\\r\\n\\r\\nFig. 4. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to \\xe2\\x80\\x9cbed\\xe2\\x80\\x9d class in structure token/feature map from top to bottom with similar performance of the two paradigms.\\r\\n\\xe2\\x80\\x9cStructToken\\xe2\\x80\\x9d refers to the slices from structure token and \\xe2\\x80\\x9cFeature map\\xe2\\x80\\x9d refers to the\\r\\nslices from feature maps. We design a four blocks decoder as a per-pixel classification\\r\\nparadigm counterpart. \\xe2\\x80\\x9cCSE\\xe2\\x80\\x9d, \\xe2\\x80\\x9cSSE\\xe2\\x80\\x9d, \\xe2\\x80\\x9cPWE\\xe2\\x80\\x9d, \\xe2\\x80\\x9cper-pixel\\xe2\\x80\\x9d refers to StructToken-CSE,\\r\\nStructToken-SSE, StructToken-PWE and our per-pixel classification paradigm counterpart respectively. \\xe2\\x80\\x9cBlock 1-4\\xe2\\x80\\x9d represents the slices from the structure token/feature\\r\\nmaps after each block. \\xe2\\x80\\x9cOutput\\xe2\\x80\\x9d means the slices from the output score map. \\xe2\\x80\\x9cIoU\\xe2\\x80\\x9d\\r\\nrepresents the intersect over union score of the specific class in this image.\\r\\n\\r\\n\\x0c\\n\\n17\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n85%\\r\\n\\r\\nSSE\\r\\n\\r\\n84%\\r\\n\\r\\nPWE\\r\\n\\r\\n84%\\r\\n\\r\\nperpixel\\r\\n\\r\\n85%\\r\\n\\r\\nFig. 5. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to \\xe2\\x80\\x9cbuilding\\xe2\\x80\\x9d class in structure token/feature map from top to bottom. The meaning of the labels is the same as in Fig.\\r\\n4.\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n95%\\r\\n\\r\\nSSE\\r\\n\\r\\n95%\\r\\n\\r\\nPWE\\r\\n\\r\\n96%\\r\\n\\r\\nperpixel\\r\\n\\r\\n95%\\r\\n\\r\\nFig. 6. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to \\xe2\\x80\\x9ccar\\xe2\\x80\\x9d class in structure token/feature map from top to bottom. The meaning of the labels is the same as in\\r\\nFig. 4.\\r\\n\\r\\n\\x0c\\n\\n18\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n93%\\r\\n\\r\\nSSE\\r\\n\\r\\n94%\\r\\n\\r\\nPWE\\r\\n\\r\\n94%\\r\\n\\r\\nperpixel\\r\\n\\r\\n92%\\r\\n\\r\\nFig. 7. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to \\xe2\\x80\\x9cdoor\\xe2\\x80\\x9d class in structure token/feature map from top to bottom. The meaning of the labels is the same as in\\r\\nFig. 4.\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n94%\\r\\n\\r\\nSSE\\r\\n\\r\\n94%\\r\\n\\r\\nPWE\\r\\n\\r\\n94%\\r\\n\\r\\nperpixel\\r\\n\\r\\n94%\\r\\n\\r\\nFig. 8. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to \\xe2\\x80\\x9cmountain\\xe2\\x80\\x9d class in structure token/feature map from top to bottom. The meaning of the labels is the same as in Fig.\\r\\n4.\\r\\n\\r\\n\\x0c\\n\\n19\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n90%\\r\\n\\r\\nSSE\\r\\n\\r\\n91%\\r\\n\\r\\nPWE\\r\\n\\r\\n90%\\r\\n\\r\\nperpixel\\r\\n\\r\\n90%\\r\\n\\r\\nFig. 9. Another visualization of the slices corresponding to \\xe2\\x80\\x9cmountain\\xe2\\x80\\x9d class in structure token/feature map from top to bottom. The meaning of the labels is the same as\\r\\nin Fig. 4.\\r\\n\\r\\nImage\\r\\n\\r\\nStructToken/Feature map\\r\\n\\r\\nBlock 1\\r\\n\\r\\nBlock 2\\r\\n\\r\\nBlock 3\\r\\n\\r\\nBlock 4\\r\\n\\r\\nOutput\\r\\n\\r\\nIoU\\r\\n\\r\\nCSE\\r\\n\\r\\n91%\\r\\n\\r\\nSSE\\r\\n\\r\\n89%\\r\\n\\r\\nPWE\\r\\n\\r\\n90%\\r\\n\\r\\nperpixel\\r\\n\\r\\n90%\\r\\n\\r\\nFig. 10. Visualization of our structural prior paradigm and per-pixel classification\\r\\nparadigm. We visualize the slices corresponding to \\xe2\\x80\\x9ctree\\xe2\\x80\\x9d class in structure token/feature map from top to bottom. The meaning of the labels is the same as in\\r\\nFig. 4.\\r\\n\\r\\n\\x0c\\n\\n20\\r\\n\\r\\nReferences\\r\\n1. Alhaija, H.A., Mustikovela, S.K., Mescheder, L., Geiger, A., Rother, C.: Augmented reality meets deep learning for car instance segmentation in urban scenes.\\r\\nIn: British machine vision conference. vol. 1, p. 2 (2017)\\r\\n2. Caesar, H., Uijlings, J., Ferrari, V.: Coco-stuff: Thing and stuff classes in context.\\r\\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition.\\r\\npp. 1209\\xe2\\x80\\x931218 (2018)\\r\\n3. Cao, Y., Xu, J., Lin, S., Wei, F., Hu, H.: Gcnet: Non-local networks meet squeezeexcitation networks and beyond. In: Proceedings of the IEEE/CVF International\\r\\nConference on Computer Vision Workshops. pp. 0\\xe2\\x80\\x930 (2019)\\r\\n4. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and\\r\\nfully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence 40(4), 834\\xe2\\x80\\x93848 (2018). https://doi.org/10.1109/TPAMI.2017.2699184\\r\\n5. Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution\\r\\nfor semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)\\r\\n6. Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with\\r\\natrous separable convolution for semantic image segmentation. In: Proceedings of\\r\\nthe European conference on computer vision (ECCV). pp. 801\\xe2\\x80\\x93818 (2018)\\r\\n7. Chen, W., Zhu, X., Sun, R., He, J., Li, R., Shen, X., Yu, B.: Tensor low-rank\\r\\nreconstruction for semantic segmentation. In: European Conference on Computer\\r\\nVision. pp. 52\\xe2\\x80\\x9369. Springer (2020)\\r\\n8. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Maskedattention mask transformer for universal image segmentation. arXiv preprint\\r\\narXiv:2112.01527 (2021)\\r\\n9. Cheng, B., Schwing, A.G., Kirillov, A.: Per-pixel classification is not all you need\\r\\nfor semantic segmentation (2021)\\r\\n10. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\\r\\nFranke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\\r\\nunderstanding. In: Proceedings of the IEEE conference on computer vision and\\r\\npattern recognition. pp. 3213\\xe2\\x80\\x933223 (2016)\\r\\n11. Ding, H., Jiang, X., Shuai, B., Liu, A.Q., Wang, G.: Semantic correlation promoted shape-variant context for segmentation. In: Proceedings of the IEEE/CVF\\r\\nConference on Computer Vision and Pattern Recognition. pp. 8885\\xe2\\x80\\x938894 (2019)\\r\\n12. Ding, X., Guo, Y., Ding, G., Han, J.: Acnet: Strengthening the kernel skeletons for\\r\\npowerful cnn via asymmetric convolution blocks. In: Proceedings of the IEEE/CVF\\r\\nInternational Conference on Computer Vision. pp. 1911\\xe2\\x80\\x931920 (2019)\\r\\n13. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\\r\\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\\r\\nAn image is worth 16x16 words: Transformers for image recognition at scale. ICLR\\r\\n(2021)\\r\\n14. Feng, D., Haase-Schu\\xcc\\x88tz, C., Rosenbaum, L., Hertlein, H., Glaeser, C., Timm, F.,\\r\\nWiesbeck, W., Dietmayer, K.: Deep multi-modal object detection and semantic\\r\\nsegmentation for autonomous driving: Datasets, methods, and challenges. IEEE\\r\\nTransactions on Intelligent Transportation Systems (2020)\\r\\n15. Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu, H.: Dual attention network for\\r\\nscene segmentation. In: Proceedings of the IEEE/CVF Conference on Computer\\r\\nVision and Pattern Recognition. pp. 3146\\xe2\\x80\\x933154 (2019)\\r\\n\\r\\n\\x0c\\n\\n21\\r\\n16. Harders, M., Szekely, G.: Enhancing human-computer interaction in medical segmentation. Proceedings of the IEEE 91(9), 1430\\xe2\\x80\\x931442 (2003)\\r\\n17. He, J., Deng, Z., Qiao, Y.: Dynamic multi-scale filters for semantic segmentation.\\r\\nIn: Proceedings of the IEEE/CVF International Conference on Computer Vision.\\r\\npp. 3562\\xe2\\x80\\x933572 (2019)\\r\\n18. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\\r\\nProceedings of the IEEE conference on computer vision and pattern recognition.\\r\\npp. 770\\xe2\\x80\\x93778 (2016)\\r\\n19. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.\\r\\nIn: European conference on computer vision. pp. 630\\xe2\\x80\\x93645. Springer (2016)\\r\\n20. Huang, S., Wang, D., Wu, X., Tang, A.: Dsanet: Dual self-attention network for\\r\\nmultivariate time series forecasting. In: Proceedings of the 28th ACM international\\r\\nconference on information and knowledge management. pp. 2129\\xe2\\x80\\x932132 (2019)\\r\\n21. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: Ccnet: Criss-cross\\r\\nattention for semantic segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 603\\xe2\\x80\\x93612 (2019)\\r\\n22. Jin, Z., Gong, T., Yu, D., Chu, Q., Wang, J., Wang, C., Shao, J.: Mining contextual information beyond image for semantic segmentation. In: Proceedings of the\\r\\nIEEE/CVF International Conference on Computer Vision. pp. 7231\\xe2\\x80\\x937241 (2021)\\r\\n23. Jin, Z., Liu, B., Chu, Q., Yu, N.: Isnet: Integrate image-level and semantic-level\\r\\ncontext for semantic segmentation. In: Proceedings of the IEEE/CVF International\\r\\nConference on Computer Vision. pp. 7189\\xe2\\x80\\x937198 (2021)\\r\\n24. Kirillov, A., Girshick, R., He, K., Dolla\\xcc\\x81r, P.: Panoptic feature pyramid networks.\\r\\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition. pp. 6399\\xe2\\x80\\x936408 (2019)\\r\\n25. Li, X., Yang, Y., Zhao, Q., Shen, T., Lin, Z., Liu, H.: Spatial pyramid based graph\\r\\nreasoning for semantic segmentation. In: Proceedings of the IEEE/CVF Conference\\r\\non Computer Vision and Pattern Recognition. pp. 8950\\xe2\\x80\\x938959 (2020)\\r\\n26. Li, X., Zhong, Z., Wu, J., Yang, Y., Lin, Z., Liu, H.: Expectation-maximization\\r\\nattention networks for semantic segmentation. In: Proceedings of the IEEE/CVF\\r\\nInternational Conference on Computer Vision. pp. 9167\\xe2\\x80\\x939176 (2019)\\r\\n27. Lin, T.Y., Dolla\\xcc\\x81r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature\\r\\npyramid networks for object detection. In: Proceedings of the IEEE conference on\\r\\ncomputer vision and pattern recognition. pp. 2117\\xe2\\x80\\x932125 (2017)\\r\\n28. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\\r\\ntransformer: Hierarchical vision transformer using shifted windows. International\\r\\nConference on Computer Vision (ICCV) (2021)\\r\\n29. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\\r\\nsegmentation. In: Proceedings of the IEEE conference on computer vision and\\r\\npattern recognition. pp. 3431\\xe2\\x80\\x933440 (2015)\\r\\n30. MMSegmentation Contributors: OpenMMLab Semantic Segmentation Toolbox\\r\\nand Benchmark (7 2020), https://github.com/open-mmlab/mmsegmentation\\r\\n31. Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction.\\r\\nICCV (2021)\\r\\n32. Strudel, R., Garcia, R., Laptev, I., Schmid, C.: Segmenter: Transformer for semantic segmentation. In: Proceedings of the IEEE/CVF International Conference on\\r\\nComputer Vision (ICCV). pp. 7262\\xe2\\x80\\x937272 (October 2021)\\r\\n33. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\\r\\nL., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017)\\r\\n\\r\\n\\x0c\\n\\n22\\r\\n34. Wu, T., Lu, Y., Zhu, Y., Zhang, C., Wu, M., Ma, Z., Guo, G.: Ginet: Graph interaction network for scene parsing. In: European Conference on Computer Vision.\\r\\npp. 34\\xe2\\x80\\x9351. Springer (2020)\\r\\n35. Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene\\r\\nunderstanding. In: Proceedings of the European Conference on Computer Vision\\r\\n(ECCV). pp. 418\\xe2\\x80\\x93434 (2018)\\r\\n36. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer:\\r\\nSimple and efficient design for semantic segmentation with transformers. arXiv\\r\\npreprint arXiv:2105.15203 (2021)\\r\\n37. Yin, M., Yao, Z., Cao, Y., Li, X., Zhang, Z., Lin, S., Hu, H.: Disentangled nonlocal neural networks. In: European Conference on Computer Vision. pp. 191\\xe2\\x80\\x93207.\\r\\nSpringer (2020)\\r\\n38. Yuan, Y., Chen, X., Wang, J.: Object-contextual representations for semantic segmentation. In: Computer Vision\\xe2\\x80\\x93ECCV 2020: 16th European Conference, Glasgow,\\r\\nUK, August 23\\xe2\\x80\\x9328, 2020, Proceedings, Part VI 16. pp. 173\\xe2\\x80\\x93190. Springer (2020)\\r\\n39. Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., Sun, Q.: Feature pyramid\\r\\ntransformer. In: European Conference on Computer Vision. pp. 323\\xe2\\x80\\x93339. Springer\\r\\n(2020)\\r\\n40. Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A.: Context\\r\\nencoding for semantic segmentation. In: Proceedings of the IEEE conference on\\r\\nComputer Vision and Pattern Recognition. pp. 7151\\xe2\\x80\\x937160 (2018)\\r\\n41. Zhang, W., Pang, J., Chen, K., Loy, C.C.: K-Net: Towards unified image segmentation. In: NeurIPS (2021)\\r\\n42. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\\r\\nProceedings of the IEEE conference on computer vision and pattern recognition.\\r\\npp. 2881\\xe2\\x80\\x932890 (2017)\\r\\n43. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang,\\r\\nT., Torr, P.H., Zhang, L.: Rethinking semantic segmentation from a sequence-tosequence perspective with transformers. In: CVPR (2021)\\r\\n44. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.: Semantic understanding of scenes through the ade20k dataset. International Journal\\r\\nof Computer Vision 127(3), 302\\xe2\\x80\\x93321 (2019)\\r\\n45. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable\\r\\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159\\r\\n(2020)\\r\\n46. Zhu, Z., Xu, M., Bai, S., Huang, T., Bai, X.: Asymmetric non-local neural networks for semantic segmentation. In: Proceedings of the IEEE/CVF International\\r\\nConference on Computer Vision. pp. 593\\xe2\\x80\\x93602 (2019)\\r\\n\\r\\n\\x0c\", b'AI Poincare\\xcc\\x81 2.0: Machine Learning Conservation Laws from Differential Equations\\r\\nZiming Liu,1 Varun Madhavan,2 and Max Tegmark1\\r\\n1\\r\\n\\r\\nDepartment of Physics, Massachusetts Institute of Technology, Cambridge, USA\\r\\n2\\r\\nIndian Institute of Technology Kharagpur, India\\r\\n(Dated: March 24, 2022)\\r\\n\\r\\narXiv:2203.12610v1 [cs.LG] 23 Mar 2022\\r\\n\\r\\nWe present a machine learning algorithm that discovers conservation laws from differential equations, both numerically (parametrized as neural networks) and symbolically, ensuring their functional independence (a non-linear generalization of linear independence). Our independence module\\r\\ncan be viewed as a nonlinear generalization of singular value decomposition. Our method can readily\\r\\nhandle inductive biases for conservation laws. We validate it with examples including the 3-body\\r\\nproblem, the KdV equation and nonlinear Schro\\xcc\\x88dinger equation.\\r\\n\\r\\nI.\\r\\n\\r\\nINTRODUCTION\\r\\n\\r\\nThe importance of conservation laws (CLs) in physics\\r\\ncan hardly be overstated [1]. Physicists usually derive\\r\\nconservation laws with time-consuming pencil and paper methods, using different hand-crafted strategies for\\r\\neach specific specific problem. This motivates searching\\r\\nfor a general-purpose problem-agnostic approach. A few\\r\\nrecent papers have exploited machine learning to autodiscover conservation laws [2\\xe2\\x80\\x935], but despite of promising\\r\\npreliminary results, these techniques are not guaranteed\\r\\nto discover all conservation laws. In this paper, we start\\r\\nwith differential equations defining a dynamical system\\r\\nand aim to discover all its conservations laws, either in in\\r\\nnumerical form (parameterized as neural networks) or in\\r\\nsymbolic form. The new method, named AI Poincare\\xcc\\x81 2.0\\r\\nsince it builds on [2], involves three major contributions:\\r\\nFirst, we present an algorithm based on manifold learning for finding a complete set of algebraically independent functions spanning a manifold. This is a non-linear\\r\\ngeneralization of singular value decomposition, and undoubtedly has applications far beyond the current paper.\\r\\nSecond, the problem setting in this paper is more abinitio than prior work in the sense that we deal with\\r\\ndifferential equations directly as opposed to trajectories.\\r\\nThe new setting is more analogous to Henry Poincare\\xcc\\x81\\xe2\\x80\\x99s\\r\\nsituation, so it should appeal to theoretical physicists and\\r\\nmathematicians who have symbolic equations for which\\r\\nnumerical simulations might be expensive or problematic.\\r\\nThird, selecting conservation laws with certain simple\\r\\nproperties is often desirable in physics. To the best of\\r\\nour knowledge, this paper is the first attempt to allow\\r\\nphysical inductive biases when searching for conservation\\r\\nlaws. Our framework can benefit from the ever-growing\\r\\nliterature of physics-informed learning [6] and physicsaugmented learning [7] to make the results more interpretable.\\r\\nIn the Methods section, we introduce our notation and\\r\\nAI Poincare\\xcc\\x81 2.0 algorithm. In the Results section, we\\r\\napply AI Poincare\\xcc\\x81 2.0 to ordinary and partial differential\\r\\nequations (illustrated in FIG. 2) to test its ability to autodiscover conservation laws.\\r\\n\\r\\n(a) Pipeline\\r\\n\\r\\n(b) Train CL networks\\r\\n\\r\\nf\\r\\n\\r\\nDifferential\\r\\nEquations\\r\\nNN front\\r\\n(1) Training\\r\\nCL\\r\\nnetworks\\r\\n\\r\\n\\xf0\\x9d\\x9b\\xbbH1\\r\\n\\r\\n\\xf0\\x9d\\x9b\\xbbH3\\r\\n\\r\\nSymbolic front\\r\\n\\xf0\\x9d\\x9b\\xbbH2\\r\\n\\r\\n(3) Symbolic\\r\\nSearch with\\r\\nfast rejection\\r\\n\\r\\n(2) Manifold\\r\\nLearning\\r\\nwhen to\\r\\nhalt\\r\\nNumber\\r\\nCL\\r\\nof CL\\r\\nformulas\\r\\n\\r\\nH1\\r\\n\\r\\nH2\\r\\n\\r\\n\\xf0\\x9d\\x90\\xb3\\r\\n\\r\\nConservation Loss\\r\\n\\r\\nRegularization Loss\\r\\n\\r\\nH3\\r\\n\\r\\n\\xf0\\x9d\\x90\\xb3\\r\\n\\r\\n\\xf0\\x9d\\x90\\xb3\\r\\n\\r\\nFIG. 1: (a) The AI Poincare\\xcc\\x81 2.0 pipeline: The NN front\\r\\nleverages neural networks to learn conservation laws,\\r\\nwhile the symbolic front uses a fast rejection method to\\r\\nsearch for formulas. (b) Training multiple networks\\r\\nrequires minimizing each network\\xe2\\x80\\x99s conservation loss\\r\\ncombined with a function correlation penalty.\\r\\n\\r\\nII.\\r\\n\\r\\nMETHOD\\r\\n\\r\\nProblem and Notation We consider a first-order\\r\\nordinary differential equation (ODE) dz\\r\\ndt = f (z) where\\r\\nz \\xe2\\x88\\x88 Rs is the state vector and f : Rs \\xe2\\x86\\x92 Rs is a vector\\r\\nfield. Hamiltonian systems\\r\\n\\x10 correspond\\r\\n\\x11 to the special case\\r\\n\\xe2\\x88\\x82H0\\r\\n0\\r\\nwhere s is even and f = \\xe2\\x88\\x82H\\r\\n,\\r\\n\\xe2\\x88\\x92\\r\\nfor a Hamiltonian\\r\\n\\xe2\\x88\\x82p\\r\\n\\xe2\\x88\\x82x\\r\\nfunction H0 . A conserved quantity is a scalar function\\r\\nH(z) whose value remains constant along a trajectory\\r\\nz(t) determined by dz\\r\\ndt = f (z) with any initial condition\\r\\nz(t = 0) = z0 . A necessary and sufficient condition\\r\\nfor a scalar function H(z) being a conservation law is\\r\\nd\\r\\n\\xe2\\x88\\x87H \\xc2\\xb7 f = 0, because dt\\r\\nH (z(t)) = \\xe2\\x88\\x87H \\xc2\\xb7 dz\\r\\ndt = \\xe2\\x88\\x87H \\xc2\\xb7 f .\\r\\nb\\r\\nWe use hats to denote unit vectors, e.g., f \\xe2\\x89\\xa1 f /|f |. Our\\r\\ngoal is to discover all nc independent conserved quantities {H1 (z), H2 (z), \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , Hnc (z)} numerically and symbolically, optionally with user-specified properties.\\r\\nDynamical systems of the form dz\\r\\ndt = f (z) are very\\r\\ngeneral because (1) higher-order ODEs, e.g. Newtonian mechanics, can always be transformed to first-order\\r\\nODEs by including derivatives as new variables in z, and\\r\\n\\r\\n\\x0c\\n\\nModel\\r\\n\\r\\n2\\r\\n\\r\\nIllustration\\r\\n\\r\\n2\\r\\n\\r\\n\\xf0\\x9d\\x91\\x96\\r\\n\\r\\n\\xf0\\x9d\\x9c\\x95\\xf0\\x9d\\x9c\\x93\\r\\n1 \\xf0\\x9d\\x9c\\x95 2\\xf0\\x9d\\x9c\\x93\\r\\n=\\xe2\\x88\\x92\\r\\n+ \\xf0\\x9d\\x91\\x98 \\xf0\\x9d\\x9c\\x93 2\\xf0\\x9d\\x9c\\x93\\r\\n\\xf0\\x9d\\x9c\\x95\\xf0\\x9d\\x91\\xa1\\r\\n2 \\xf0\\x9d\\x9c\\x95\\xf0\\x9d\\x91\\xa5 2\\r\\n\\r\\n\\xf0\\x9d\\x91\\xa5\\r\\n\\r\\n\\xf0\\x9d\\x91\\xa5, \\xf0\\x9d\\x91\\x981\\r\\n\\r\\n\\xf0\\x9d\\x91\\xa5, \\xf0\\x9d\\x91\\x981\\r\\n\\r\\n\\xf0\\x9d\\x9c\\x95\\xf0\\x9d\\x9c\\x99\\r\\n\\xf0\\x9d\\x9c\\x95\\xf0\\x9d\\x9c\\x99 \\xf0\\x9d\\x9c\\x95 3 \\xf0\\x9d\\x9c\\x99\\r\\n\\xe2\\x88\\x92 6\\xf0\\x9d\\x9c\\x99\\r\\n+\\r\\n=0\\r\\n\\xf0\\x9d\\x9c\\x95\\xf0\\x9d\\x91\\xa1\\r\\n\\xf0\\x9d\\x9c\\x95\\xf0\\x9d\\x91\\xa5 \\xf0\\x9d\\x9c\\x95\\xf0\\x9d\\x91\\xa5 3\\r\\n\\r\\nNonlinear\\r\\nSchr\\xc3\\x96dinger Equation\\r\\n\\r\\n3\\r\\n\\r\\n\\xf0\\x9d\\x91\\x982 = \\xf0\\x9d\\x91\\x981\\r\\n\\r\\n\\xf0\\x9d\\x91\\x981 = 4\\xf0\\x9d\\x91\\x982\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2 (\\xf0\\x9d\\x9c\\x99), 3 (\\xf0\\x9d\\x9c\\x99, \\xf0\\x9d\\x9c\\x99\\xf0\\x9d\\x91\\xa5 ),\\r\\n4 (\\xf0\\x9d\\x9c\\x99, \\xf0\\x9d\\x9c\\x99\\xf0\\x9d\\x91\\xa5 , \\xf0\\x9d\\x9c\\x99\\xf0\\x9d\\x91\\xa5\\xf0\\x9d\\x91\\xa5 )\\r\\n\\r\\n4\\r\\n\\r\\n1 (\\xf0\\x9d\\x9c\\x93), 2 (\\xf0\\x9d\\x9c\\x93, \\xf0\\x9d\\x9c\\x93\\xf0\\x9d\\x91\\xa5 ),\\r\\n3 (\\xf0\\x9d\\x9c\\x93, \\xf0\\x9d\\x9c\\x93\\xf0\\x9d\\x91\\xa5 , \\xf0\\x9d\\x9c\\x93\\xf0\\x9d\\x91\\xa5\\xf0\\x9d\\x91\\xa5 )\\r\\n\\r\\nDifferential Rank\\r\\n\\r\\nRank\\r\\n\\r\\nCLs\\r\\n\\r\\n1\\r\\n\\r\\n\\xf0\\x9d\\x91\\xa5\\r\\n\\r\\n3\\r\\n\\r\\n\\xf0\\x9d\\x91\\xa6\\r\\n\\r\\n\\xf0\\x9d\\x91\\xa6, \\xf0\\x9d\\x91\\x982\\r\\n\\r\\n\\xf0\\x9d\\x91\\xa6, \\xf0\\x9d\\x91\\x982\\r\\n\\r\\n\\xf0\\x9d\\x91\\xa6\\r\\n\\r\\nKdV wave\\r\\nEquation\\r\\n\\r\\nThree-body\\r\\nProblem\\r\\n\\r\\n2D Anisotropic\\r\\n2D isotropic\\r\\nHarmonic Oscillator Harmonic Oscillator\\r\\n\\r\\nKepler\\r\\n\\r\\nFIG. 2: Tested ordinary and partial differential equation examples. AI Poincare\\xcc\\x81 2.0 is seen to find the correct\\r\\nnumber of CLs by computing rank (read off as the low flat region of the thick nef f curve as defined in [2]) or\\r\\ndifferential rank.\\r\\n\\r\\n(2) partial differential equations (PDEs) can be approximated by ODEs by discretizing space.\\r\\nAI Poincare\\xcc\\x81 2.0 consists of three steps: (a) learn\\r\\nconservation laws parameterized by neural networks, (b)\\r\\ncount the number of independent conservation laws and\\r\\n(c) find symbolic formulas for conservation laws. The\\r\\npipeline is illustrated in FIG. 1.\\r\\n(a) Parameterizing conservation laws by neural\\r\\nnetworks: We parameterize a conserved quantity as a\\r\\nneural network H(z; \\xce\\xb8) where \\xce\\xb8 are model parameters.\\r\\nOur loss function is defined as\\r\\n`(\\xce\\xb8) \\xe2\\x89\\xa1\\r\\n\\r\\nP\\r\\n2\\r\\n1 X b (i) d (i)\\r\\nf (z ) \\xc2\\xb7 \\xe2\\x88\\x87H(z ; \\xce\\xb8) ,\\r\\nP i=1\\r\\n\\r\\nR(\\xce\\xb81 , \\xce\\xb82 ) \\xe2\\x89\\xa1\\r\\n\\r\\nwhere z(i) denotes the ith sample in phase space. \\xe2\\x88\\x87H(z)\\r\\ncan be easily computed with automatic differentiation [8].\\r\\nd are normalized unit vectors, to make\\r\\nNote that b\\r\\nf and \\xe2\\x88\\x87H\\r\\nthe loss function dimensionless and invariant under uninteresting re-scaling of H. We update \\xce\\xb8 by trying to\\r\\nminimize the loss function until it drops below a small\\r\\nthreshold \\x0f.\\r\\nTo obtain multiple conserved quantities, one can repeat the above method with different random seeds and\\r\\nhope to discover algebraically independent ones. In practice, however, we find that learned conservation laws are\\r\\noften highly correlated for different initializations [9]. To\\r\\nencourage linear independence between two neural networks, say, H1 and H2 , we add a regularization term\\r\\n\\r\\n2\\r\\n\\r\\n(2)\\r\\n\\r\\nto the loss function. Since we know that there cannot\\r\\nbe more conservation laws than degrees of freedom s,\\r\\nwe train n = s models together by minimizing the loss\\r\\nfunction `1 + \\xce\\xbb`2 defined by\\r\\n`=\\r\\n\\r\\nn\\r\\nn\\r\\nn\\r\\nX\\r\\nX\\r\\n1X\\r\\n2\\r\\nR(\\xce\\xb8i , \\xce\\xb8j ), (3)\\r\\n`(\\xce\\xb8i ) +\\xce\\xbb \\xc3\\x97\\r\\nn i=1\\r\\nn(n \\xe2\\x88\\x92 1) i=1 j=i+1\\r\\n| {z }\\r\\n|\\r\\n{z\\r\\n}\\r\\n`1\\r\\n\\r\\n(1)\\r\\n\\r\\nP\\r\\n1 X [ (i)\\r\\n[2 (z(i) ; \\xce\\xb82 )\\r\\n\\xe2\\x88\\x87H1 (z ; \\xce\\xb81 ) \\xc2\\xb7 \\xe2\\x88\\x87H\\r\\nP i=1\\r\\n\\r\\n`2\\r\\n\\r\\nwhere \\xce\\xbb is a penalty coefficient. We refer to `1 and `2 as\\r\\nconservation loss and independence loss, respectively.\\r\\n(b) Counting the number of independent conserved quantities: After training, we aim to determine\\r\\n(in)dependence among these neural networks. Specifically, we are interested in functional independence, a direct generalization linear independence that we define\\r\\nand compute as described below.\\r\\nDefinition II.1. Functional independence. A set of\\r\\nnon-zero functions H1 (z), H2 (z), \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , Hn (z) is independent if\\r\\nf (H1 (z), H2 (z), \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , Hn (z)) = 0 =\\xe2\\x87\\x92 f = 0\\r\\n\\r\\n(4)\\r\\n\\r\\nor, equivalently, if no function Hi (z) can be constructed\\r\\nfrom (possibly nonlinear and multivalued) combinations\\r\\nof the other functions.\\r\\n\\r\\n\\x0c\\n\\n3\\r\\nDefinition II.2. Function set rank. The function set\\r\\nH = {H1 (z), H2 (z), \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 Hn (z)} has rank k \\xe2\\x89\\xa4 n if it contains k but not k + 1 functions that are independent.\\r\\nComputing the function set rank We determine\\r\\nthe rank k with a nonlinear manifold learning method.\\r\\nWe define the matrix A such that Aij is the value of the\\r\\nj th neural network evaluated at the ith sample point:\\r\\n\\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\nH1 (z(1) ) H2 (z(1) ) \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 Hn (z(1) )\\r\\n\\xef\\xa3\\xac H1 (z(2) ) H2 (z(2) ) \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 Hn (z(2) ) \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7,\\r\\nA=\\xef\\xa3\\xac\\r\\n(5)\\r\\n\\xef\\xa3\\xad \\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\r\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\r\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\r\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7 \\xef\\xa3\\xb8\\r\\n(P )\\r\\n(P )\\r\\n(P )\\r\\nH1 (z ) H2 (z ) \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 Hn (z )\\r\\nwhere P \\x1d n is the number of data points z(i) . If we interpret each row of A as a point in Rn , then the matrix\\r\\ncorresponds to a point cloud in Rn located on a a manifold, whose dimensionality k is equal to the function set\\r\\nrank. If there are k independent linear conserved quantities (where Hi (z) are linear functions), then the point\\r\\ncloud will lie on a k-dimensional hyperplane that can\\r\\nreadily be discovered using singular value decomposition\\r\\n(SVD): k is then then the number of non-zero singular\\r\\nvalues, i.e., the rank of the matrix A. For our more\\r\\ngeneral nonlinear case, we wish to discover the manifold that the point cloud lies on even if it is curved. For\\r\\nthis, we exploit the manifold learning algorithm proposed\\r\\nin Poincare 1.0 [2] to measure the manifold dimensionality [10], which performs local Monte Carlo sampling\\r\\nfollowed by a linear dimensionality estimation method.\\r\\nTaking the derivative of f (H1 (z), H2 (z) \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , Hn (z)) =\\r\\n0 from equation (4) with respect to zi gives.\\r\\n\\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\xef\\xa3\\xab \\xef\\xa3\\xb6\\r\\nH1,1 H2,1 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 Hn,1\\r\\nf,1\\r\\n\\xef\\xa3\\xacH1,2 H2,2 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 Hn,2 \\xef\\xa3\\xb7 \\xef\\xa3\\xac f,2 \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\xef\\xa3\\xac \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac ..\\r\\n..\\r\\n.. \\xef\\xa3\\xb7 \\xef\\xa3\\xac .. \\xef\\xa3\\xb7 = 0.\\r\\n\\xef\\xa3\\xad .\\r\\n(6)\\r\\n.\\r\\n. \\xef\\xa3\\xb8\\xef\\xa3\\xad . \\xef\\xa3\\xb8\\r\\n|\\r\\n\\r\\nH1,s H2,s \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 Hn,s\\r\\nf,n\\r\\n{z\\r\\n} | {z }\\r\\nB\\r\\n\\r\\n\\xe2\\x88\\x87f\\r\\n\\r\\nThis means that, if {H1 , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , Hn } and f are differentiable\\r\\nfunctions and B has full rank, then \\xe2\\x88\\x87f (z) and therefore\\r\\nf (z) itself must vanish identically, so the functions Hi\\r\\nmust be independent. We exploit this to define differentiable independence and differentiable rank as follows:\\r\\nDefinition II.3. Differential functional independence. A set of n non-zero differentiable functions H is\\r\\ndifferentially independent if their gradients are linearly\\r\\nindependent, i.e., if rank B(z) = n almost everywhere\\r\\n(for all z except for a set of measure zero).\\r\\nDefinition\\r\\nII.4. differential\\r\\nfunction\\r\\nset\\r\\nrank.\\r\\nThe differential rank of the function\\r\\nset H = {H1 (z), H2 (z), \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 Hn (z)} is defined as\\r\\nkD = max rank B(z).\\r\\nz\\r\\n\\r\\nIn practice, it suffices to compute the maximum over\\r\\na finite number of points P \\x1d n: it is exponentially\\r\\nunlikely that such sampling will underestimate the true\\r\\n\\r\\nmanifold dimensionality, just as it is exponentially unlikely that P random points in 3-dimensional space will\\r\\nhappen to lie on a plane.\\r\\nNumerically, one can apply singular value decomposition to B to obtain singular values {\\xcf\\x831 , \\xcf\\x832 , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , \\xcf\\x83n }, and\\r\\ndefine the rank as the number of non-zero singular values.\\r\\nIn practice, we treat components as vanishing\\r\\nif the exP\\r\\nplained fraction of the total variance, \\xcf\\x83i2 / j \\xcf\\x83j2 , is below\\r\\n\\x0f = 10\\xe2\\x88\\x922 .\\r\\nEquipped with the tools above, we can obtain the number of independent conserved quantities and determine\\r\\nthem numerically.\\r\\n(c) Discovering symbolic formulas When no domain knowledge is available for a physical system, we\\r\\nperform a brute-force search over symbolic formulas ordered by increasing complexity as in [11, 12]. We leverd = 0 to determine if a candidate\\r\\nage the criterion f\\xcc\\x82 \\xc2\\xb7 \\xe2\\x88\\x87H\\r\\nfunction H(z) is a conserved quantity or not. We implement a brute force algorithm in C++ for speed and\\r\\nemploy a fast rejection strategy for further speedup: we\\r\\nprepare np = 10 test points in advance, and reject H\\r\\nd\\r\\nimmediately if b\\r\\nf (z) \\xc2\\xb7 \\xe2\\x88\\x87H(z)\\r\\n> \\x0fs = 10\\xe2\\x88\\x924 for any test\\r\\npoint z. If a formula survives at the np test points, we\\r\\ntest thoroughly by checking the condition numerically\\r\\non the whole dataset, or test the condition symbolically.\\r\\nWe determine whether the new conserved quantity is independent of already discovered ones by checking if the\\r\\ndifferential function set rank increases by 1 when adding\\r\\nthe new conserved quantity. Appendix B provides further technical details.\\r\\nIncluding inductive biases in conservation law\\r\\nlearning Above we did not distinguish between integrals\\r\\nof motion and conserved quantities. As clarified in [13]\\r\\nand Appendix C , conserved quantities are special cases\\r\\nof Integrals of Motion. Conservation laws are usually derived from homogeneity and isotropy of time and space,\\r\\nand have the feature of being additive, i.e., expressible\\r\\nas a sum of simple terms involving only a small subset\\r\\nof the degrees of freedom. Conserved quantities of PDEs\\r\\nusually take the form of integral over space. We incorporate any such desired inductive biases into our method\\r\\nby restricting the neural networks parametrizing Hi (z)\\r\\nto have the corresponding properties.\\r\\n\\r\\nIII.\\r\\n\\r\\nRESULTS\\r\\n\\r\\nNumerical experiments We test AI Poincare\\xcc\\x81 2.0 on\\r\\nsix systems: the Kepler problem, isotropic/anisotropic\\r\\nharmonic oscillators, the gravitational three-body problem, the KdV wave equation and the nonlinear\\r\\nSchro\\xcc\\x88dinger equation. The details of these systems are\\r\\nincluded in Appendix A. We obtain discrete samples\\r\\n(z(i) , f (i) ) , i = 1, \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , P = 104 , by first sampling z uniformly from a box, z \\xe2\\x88\\xbc U [\\xe2\\x88\\x922, 2]s , and then computing\\r\\nf (i) = f (z(i) ). For the Kepler problem and three-body\\r\\nproblem, we drop samples whenever a two-body distance\\r\\n\\r\\n\\x0c\\n\\n4\\r\\nSystem\\r\\nKepler Problem\\r\\n\\r\\nIntegrals of Motion or Conservation Laws\\r\\nH1 = 12 (p2x + p2y ) \\xe2\\x88\\x92 \\xe2\\x88\\x9a 21 2\\r\\nx +y\\r\\n\\r\\nReverse Polish Notation Discovered\\r\\npx Qpy Q+rIoYes\\r\\n\\r\\nH2 = xpy \\xe2\\x88\\x92 ypx\\r\\nxpy *ypx *H3 = (xpy \\xe2\\x88\\x92 ypx )py + r\\xcc\\x82x\\r\\nxpy *ypx *-py *xr/+\\r\\nH1 = 12 (x2 + p2x )\\r\\nxQ*px Q+\\r\\nIsotropic Oscillator\\r\\nH2 = 21 (y 2 + p2y )\\r\\nyQpy Q+\\r\\nH3 = xpy \\xe2\\x88\\x92 ypx\\r\\nxpy *ypx H1 = 12 (x2 + p2x )\\r\\nxQ*px Q+\\r\\n2\\r\\n2\\r\\n1\\r\\nAnisotropic Oscillator\\r\\nH\\r\\nyQOOpy Q+\\r\\n2 = 2 (4y + py )\\r\\n\\xe2\\x88\\x9a\\r\\nH3 = x H1 H2 \\xe2\\x88\\x92 l2 \\xe2\\x88\\x92 lpx (l = xpy \\xe2\\x88\\x92 2ypx )\\r\\nH H *lQ-Rx*lpx *P\\r\\nP 1 2\\r\\n1\\r\\n1\\r\\n1\\r\\nH1 = 3i=1 21 (p2i,x + p2i,y ) \\xe2\\x88\\x92 ( r12\\r\\n+ r13\\r\\n+ r23\\r\\n)\\r\\npi,x Qpi,y Q+ri(i+1) IOiP\\r\\nP3\\r\\nx\\r\\np\\r\\n\\xe2\\x88\\x92\\r\\ny\\r\\np\\r\\nH\\r\\n=\\r\\npi,y *yi pi,x *i\\r\\ni,y\\r\\ni\\r\\ni,x\\r\\n2\\r\\ni=1P\\r\\ni xiP\\r\\nThree Body Problem\\r\\nH3 = 3i=1 pi,x\\r\\np\\r\\nP\\r\\nPi i,x\\r\\nH4 = R3i=1 pi,y\\r\\ni pi,y\\r\\nH1 = R \\xcf\\x86 dx\\r\\n\\xcf\\x86\\r\\nKdV\\r\\nH2R = \\xcf\\x862 dx\\r\\n\\xcf\\x86Q\\r\\n3\\r\\n2\\r\\nH3 = (2\\xcf\\x86\\r\\n\\xcf\\x86Q\\xcf\\x86*O|\\xcf\\x88x |QR \\xe2\\x88\\x922\\xcf\\x86x ) dx\\r\\nHR1 = |\\xcf\\x88| dx\\r\\n|psi|Q\\r\\nNonlinear Schro\\xcc\\x88dinger\\r\\nH2 = (|\\xcf\\x88x |2 + |\\xcf\\x88|4 ) dx\\r\\n|\\xcf\\x88x |Q|\\xcf\\x88|QQ+\\r\\n\\r\\nYes\\r\\nNo\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nNo\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes\\r\\n\\r\\nTABLE I: 16 of the 18 conservation laws were discovered not only numerically, but also symbolically using our\\r\\nfast-rejection brute force search limited to 9 distinct symbols.\\r\\n\\r\\nis smaller than rc = 0.5. For the two PDEs, we we\\r\\nrepresent each function by a 240-dimensional vector z\\r\\nparametrizing the first 6 terms of its Taylor expansion at\\r\\n40 uniformly spaced points on the interval [\\xe2\\x88\\x9210, 10], and\\r\\nwe generate random sample points z(i) for random linear combination of Gaussians as detailed in Appendix D.\\r\\nThe neural network has 2 hidden layers, each containing\\r\\n256 neurons with SiLU activation, and is trained with\\r\\nthe Adam optimizer [14] for 100 epochs. When training\\r\\nmultiple networks simultaneously, we choose the regularization coefficient \\xce\\xbb = 0.02 as detailed in Appendix D.\\r\\nIntegrals of Motion (IOMs): Any first-order differential equation with s degrees of freedom has s \\xe2\\x88\\x92 1\\r\\nIOMs, simply because trajectories are one-dimensional\\r\\n[13]. For example, the 2D Kepler problem (s = 4) has\\r\\ns \\xe2\\x88\\x92 1 = 3 IOMs, which are all recognized by physicists as\\r\\nconservation laws: energy, angular momentum and the\\r\\nLaplace-Runge-Lenz vector specifying the ellipse orientation. In FIG. 2, we test two other systems with s = 4\\r\\ndegrees of freedom: a 2D isotropic oscillator and a 2D\\r\\nanisotropic oscillator. We see that our NN front (using\\r\\neither rank definition) indeed obtains nc = 3 independent\\r\\nIOMs.\\r\\nSomething amusing happened for the anisotropic oscillator example. The first author, despite passing his\\r\\nclassical mechanics exam with full score, expected two\\r\\nIOMs rather than three because the angular momentum\\r\\nis not conserved for the anisotropic oscillator. However,\\r\\nAI Poincare\\xcc\\x81 insisted there were three IOMs. The authors\\r\\neventually realized that AI Poincare\\xcc\\x81 was right: a third\\r\\nIOM is indeed present, although poorly known among\\r\\nphysicists [15].\\r\\nConservation laws For the famous 2D gravitational\\r\\nthree-body problem, the 12-dimensional state vector z =\\r\\n{xi , yi , vi,x , vi,y }i=1,2,3 specifies the positions and veloci-\\r\\n\\r\\nties of three bodies. Although there are 12-1=11 IOMs,\\r\\nonly 4 are recognized as conservation laws by physicists:\\r\\nenergy, momentum (along the x- and y-axes) and angular momentum (along the z-axis). To incorporate the\\r\\nabove-mentioned inductive biases, we assume that a conserved quantity decomposes into 1-body terms and 2body terms. We assume nothing about the 1-body terms,\\r\\nbut assume translational and rotational invariance for the\\r\\n2-body terms. As a result, a candidate conservation law\\r\\nmust have the form:\\r\\n\\r\\nH=\\r\\n\\r\\n3\\r\\nX\\r\\ni=1\\r\\n\\r\\ng(xi , yi , vi,x , vi,y ) +\\r\\n\\r\\n3\\r\\n3\\r\\nX\\r\\nX\\r\\n\\r\\nh(rij )\\r\\n\\r\\n(7)\\r\\n\\r\\ni=1 j=i+1\\r\\n\\r\\np\\r\\nwhere rij \\xe2\\x89\\xa1\\r\\n(xj \\xe2\\x88\\x92 xi )2 + (yj \\xe2\\x88\\x92 yi )2 .\\r\\nBy parameterizing g and h as two separate neural networks,\\r\\nthe learned conservation laws automatically satisfy the\\r\\nabove-mentioned desired physical properties. Our algorithm now discovers precisely 4 independent conservation\\r\\nlaws is indeed 4, agreeing with physicists\\xe2\\x80\\x99 intuition.\\r\\nAnother set of interesting systems are partial differential equations (PDE) in the form ut = f (u, ux , uxx , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ).\\r\\nSince a field has infinite number of degrees of freedom\\r\\n(hence infinitely many IOMs), it is crucial to constrain\\r\\nthe form of conservation laws to exclude trivial ones. In\\r\\nquantum mechanics, for example, any projector onto an\\r\\neigenstate is an IOM, but these less profound than probability conservation (known as unitarity), energy conservation etc. Thus we focus on conservation laws with an\\r\\nintegral form obeying translational invariance:\\r\\nZ\\r\\nH = h(u, |ux |, |uxx |, \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ) dx\\r\\n(8)\\r\\nIn practice, we replace the integral by a sum over the\\r\\n\\r\\n\\x0c\\n\\n5\\r\\n40 aforementioned grid points. Moreover, we take the\\r\\nabsolute value of derivatives, e.g., |ux | and |uxx |, to avoid\\r\\ntrivial \\xe2\\x80\\x9cconserved quantities\\xe2\\x80\\x9d of the total derivative form\\r\\nd\\r\\nF (u, ux , uxx , ...), e.g., ux , uux , or uxx , which are\\r\\nh = dx\\r\\nconserved simply due to zero boundary conditions.\\r\\nThe Korteweg\\xe2\\x80\\x93De Vries (KdV) equation \\xcf\\x86t + \\xcf\\x86xxx \\xe2\\x88\\x92\\r\\n6\\xcf\\x86\\xcf\\x86x = 0 describes shallow water waves and is of particular interest in mathematical physics. It is known to\\r\\nhave infinitely many conservation laws [16]. Of course\\r\\nour framework cannot handle infinity, but conservation\\r\\nlaws involving low-order derivatives are finite and important in physics. For example, the first three conservation laws involve no more than the first derivative, corresponding to mass, momentum and energy conservation\\r\\nrespectively. We feed our neural network with (1) \\xcf\\x86 only;\\r\\n(2) \\xcf\\x86 and |\\xcf\\x86x |; (3) \\xcf\\x86, |\\xcf\\x86x | and |\\xcf\\x86xx |, and our method\\r\\npredicts 2, 3 and 4 conservation laws respectively, which\\r\\nagree exactly with the ground truth.\\r\\nThe Nonlinear Schro\\xcc\\x88dinger equation (NLS) i\\xcf\\x88t =\\r\\n\\xe2\\x88\\x92 12 \\xcf\\x88xx + \\xce\\xba|\\xcf\\x88|2 \\xcf\\x88 can describe interesting phenomena including Bose-Einstein condensation. It also has infinitely\\r\\nmany conservation laws, but as in the last example, conservation laws of interest to physicists involve only loworder derivatives. For example, probability, momentum\\r\\nand energy conservation include no more than the first\\r\\nderivative. We feed the neural network with (1) \\xcf\\x88 only;\\r\\n(2) \\xcf\\x88 and |\\xcf\\x88x |; (3) \\xcf\\x88, |\\xcf\\x88x | and |\\xcf\\x88xx |, and our method predicts 1, 2 and 3 conservation laws respectively, which ba-\\r\\n\\r\\nsically agree with the ground truth, although our method\\r\\nis unable to discover the momentum which involves \\xcf\\x88x\\r\\nbecause the input |\\xcf\\x88x | lacks the phase information. We\\r\\nwould like to investigate how to include more information to facilitate learning the momentum while avoiding\\r\\ntrivial solutions.\\r\\n\\r\\nIV.\\r\\n\\r\\nCONCLUSIONS\\r\\n\\r\\nWe have presented a method that, given some differential equations, can determine not only the number of\\r\\nindependent conserved quantities, but also obtain neural\\r\\n(or even symbolic) representations of them. Conservation laws and integrability have many competing definitions listed in Appendix C, and AI Poincare\\xcc\\x81 2.0 is able to\\r\\nadapt to all of them. This new tool may accelerate future\\r\\nprogress on exciting open physics problems, for example\\r\\nintegrability of quantum many-body systems and manybody localization.\\r\\nAcknowledgements We thank Bohan Wang, Di Luo\\r\\nand Sijing Du for helpful discussions and the Center\\r\\nfor Brains, Minds, and Machines (CBMM) for hospitality. This work was supported by The Casey and Family\\r\\nFoundation, the Foundational Questions Institute, the\\r\\nRothberg Family Fund for Cognitive Science and IAIFI\\r\\nthrough NSF grant PHY-2019786.\\r\\n\\r\\n[1] P.\\r\\nW.\\r\\nAnderson,\\r\\nMore\\r\\nis\\r\\ndif[9]\\r\\nferent,\\r\\nScience\\r\\n177,\\r\\n393\\r\\n(1972),\\r\\nhttps://science.sciencemag.org/content/177/4047/393.full.pdf.\\r\\n[10]\\r\\n[2] Z. Liu and M. Tegmark, Machine learning conservation\\r\\nlaws from trajectories, Phys. Rev. Lett. 126, 180604\\r\\n(2021).\\r\\n[3] Y. ichi Mototake, Interpretable conservation law estimation by deriving the symmetries of dynamics from trained\\r\\ndeep neural networks, in Machine Learning and the Physical Sciences Workshop at the 33rd Conference on Neural Information Processing Systems (NeurIPS) (2019)\\r\\narXiv:2001.00111 [physics.data-an].\\r\\n[4] S. J. Wetzel, R. G. Melko, J. Scott, M. Panju, and\\r\\n[11]\\r\\nV. Ganesh, Discovering symmetry invariants and conserved quantities by interpreting siamese neural networks, Phys. Rev. Research 2, 033499 (2020).\\r\\n[5] S. Ha and H. Jeong, Discovering conservation laws\\r\\n[12]\\r\\nfrom trajectories via machine learning, arXiv preprint\\r\\narXiv:2102.04008 (2021).\\r\\n[6] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris,\\r\\nS. Wang, and L. Yang, Physics-informed machine learn[13]\\r\\ning, Nature Reviews Physics 3, 422 (2021).\\r\\n[7] Z. Liu, Y. Chen, Y. Du, and M. Tegmark, Physicsaugmented learning: A new paradigm beyond physics[14]\\r\\ninformed learning, arXiv preprint arXiv:2109.13901\\r\\n(2021).\\r\\n[15]\\r\\n[8] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning\\r\\n(MIT press, 2016).\\r\\n\\r\\nThis seems to imply some \\xe2\\x80\\x98simpler\\xe2\\x80\\x99 conservation laws are\\r\\npreferred by neural networks over others.\\r\\nAlthough the nonlinear manifold learning method introduced in AI Poincare\\xcc\\x81 1.0 also applies here, the ways to\\r\\ncompute the number of conserved quantities nc is different and actually dual. In Poincare\\xcc\\x81 1.0, nc is the phase\\r\\nspace dimension minus the dimension of the trajectory\\r\\nmanifold. While in this paper, nc is equal to the dimension of the manifold. Because of this duality, the\\r\\nexplained ratio diagram (ERD) in Poincare 1.0 resembles a hill while in FIG. 2 the ERD is upside down and\\r\\nresembles a valley.\\r\\nS.-M. Udrescu and M. Tegmark, Ai feynman:\\r\\nA physics-inspired method for symbolic regression,\\r\\nScience\\r\\nAdvances\\r\\n6,\\r\\neaay2631\\r\\n(2020),\\r\\nhttps://www.science.org/doi/pdf/10.1126/sciadv.aay2631.\\r\\nS.-M. Udrescu, A. Tan, J. Feng, O. Neto, T. Wu, and\\r\\nM. Tegmark, Ai feynman 2.0: Pareto-optimal symbolic\\r\\nregression exploiting graph modularity, Advances in Neural Information Processing Systems 33, 4860 (2020).\\r\\nL. Landau and E. Lifshitz, Mechanics third edition: Volume 1 of course of theoretical physics, Elsevier Science\\r\\n(1976).\\r\\nD. P. Kingma and J. Ba, Adam: A method for stochastic\\r\\noptimization, arXiv preprint arXiv:1412.6980 (2014).\\r\\nG. Arutyunov, Liouville integrability, in Elements of\\r\\nClassical and Quantum Integrable Systems (Springer International Publishing, Cham, 2019) pp. 1\\xe2\\x80\\x9368.\\r\\n\\r\\n\\x0c\\n\\n6\\r\\n[16] R. M. Miura, C. S. Gardner, and M. D. Kruskal,\\r\\nKorteweg-de vries equation and generalizations. ii. existence of conservation laws and constants of motion, Journal of Mathematical Physics 9, 1204 (1968),\\r\\nhttps://doi.org/10.1063/1.1664701.\\r\\n[17] V. A. Dulock and H. V. McIntosh, On the degeneracy of the two-dimensional harmonic oscillator, American Journal of Physics 33, 109 (1965),\\r\\nhttps://doi.org/10.1119/1.1971258.\\r\\n[18] R. M. Miura, C. S. Gardner, and M. D. Kruskal,\\r\\nKorteweg-de vries equation and generalizations. ii. existence of conservation laws and constants of motion, Journal of Mathematical Physics 9, 1204 (1968),\\r\\nhttps://doi.org/10.1063/1.1664701.\\r\\n[19] J. Barrett, Title : The local conservation laws of the\\r\\nnonlinear schrodinger equation (2013).\\r\\n\\r\\n[20] Informally speaking, an integrable system is a dynamical\\r\\nsystem with sufficiently many conserved quantities.\\r\\n[21] Wikipedia contributors, Integrable system \\xe2\\x80\\x94 Wikipedia,\\r\\nthe free encyclopedia, https://en.wikipedia.org/\\r\\nw/index.php?title=Integrable_system&oldid=\\r\\n1058752403 (2021), [Online; accessed 5-February-2022].\\r\\n[22] J. VICKERS, Integrable systems: Twistors, loop groups,\\r\\nand riemann surfaces (oxford graduate texts in mathematics 4) by n. j. hitchin, g. b. segal and r. s. ward: 136\\r\\npp., \\xc2\\xa325.00, isbn 0-19-850421-7 (clarendon press, oxford,\\r\\n1999)., Bulletin of the London Mathematical Society 33,\\r\\n116\\xe2\\x80\\x93127 (2001).\\r\\n[23] Z. Liu and M. Tegmark, Machine-learning hidden symmetries, arXiv preprint arXiv:2109.09721 (2021).\\r\\n[24] Wikipedia contributors, Frobenius theorem (differential\\r\\ntopology) \\xe2\\x80\\x94 Wikipedia, the free encyclopedia, https:\\r\\n//en.wikipedia.org/w/index.php?title=Frobenius_\\r\\ntheorem_(differential_topology)&oldid=1049676730\\r\\n(2021), [Online; accessed 5-February-2022].\\r\\n\\r\\n\\x0c\\n\\n7\\r\\nAppendix A: Dynamical Systems\\r\\n\\r\\nOur method can be applied to dynamical systems of the form\\r\\nphysical systems used to test it.\\r\\n\\r\\n1.\\r\\n\\r\\ndz\\r\\ndt\\r\\n\\r\\n= f (z). Here we include technical details of the\\r\\n\\r\\nHarmonic Oscillator (2D)\\r\\n\\r\\nThe Harmonic Oscillator (2D) is described by two coordinates (x, y) and two momenta (px , py ).\\r\\n\\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\n\\xef\\xa3\\xb6\\r\\npx /m\\r\\nx\\r\\n\\xef\\xa3\\xac \\xe2\\x88\\x92k x \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xacp \\xef\\xa3\\xb7\\r\\nz = \\xef\\xa3\\xad x \\xef\\xa3\\xb8 , f (z) = \\xef\\xa3\\xad 1 \\xef\\xa3\\xb8 ,\\r\\npy /m\\r\\ny\\r\\n\\xe2\\x88\\x92k2 y\\r\\npy\\r\\n\\xef\\xa3\\xab\\r\\n\\r\\n(A1)\\r\\n\\r\\nwhere m is the mass, and k1 and k2 are spring constants. When k1 6= k2 , the system is anisotropic and has two obvious\\r\\n1 2\\r\\n1 2\\r\\nconserved quantities: (1) x-energy H1 = 21 k1 x2 + 2m\\r\\npx and (2) y-energy H2 = 12 k2 y 2 + 2m\\r\\npy . The third conserved\\r\\np\\r\\nquantity is less studied by physicists but still exists if k1 /k2 is a rational number [15]. When k1 = k2 , the system is\\r\\nisotropic and has three conserved quantities. Besides H1 and H2 , angular momentum H3 = xpy \\xe2\\x88\\x92ypx is also conserved.\\r\\nFor the isotropic case, we choose m = k1 = k2 = 1; for the anisotropic case, we choose m = k1 = 1, k2 = 4. Samples\\r\\nare drawn from the uniform distribution z \\xe2\\x88\\xbc U [\\xe2\\x88\\x922, 2]4 . We include more physics discussion below for completeness:\\r\\nisotropic case In the isotropic case k1 = k2 = m = 1, there are four conservation laws [17]:\\r\\n\\r\\n2Ex = x2 + p2x ,\\r\\n\\r\\n2Ey = y 2 + p2y ,\\r\\n\\r\\nL = ypx \\xe2\\x88\\x92 xpy ,\\r\\n\\r\\nK = xy + px py\\r\\n\\r\\n(A2)\\r\\n\\r\\nbut they are dependent because L2 + K 2 = 4Ex Ey . Ex , Ey and L are more common in physics, while K is less\\r\\ncommon. However, there is no need to prefer L over K. In fact, our symbolic module discovers the three conserved\\r\\nquantities 2Ex , 2Ey , K and then ignores L because of its dependence on the other three quantities.\\r\\nAnisotropic case Although conservation laws are explicitly constructed in [15], let us consider the specific case\\r\\nm = k1 = 1, k2 = 4. The equations of motion are:\\r\\n\\xef\\xa3\\xab \\xef\\xa3\\xb6 \\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\nx\\r\\nvx\\r\\nd \\xef\\xa3\\xacvx \\xef\\xa3\\xb7 \\xef\\xa3\\xac \\xe2\\x88\\x92x \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xad \\xef\\xa3\\xb8 = \\xef\\xa3\\xad v \\xef\\xa3\\xb8.\\r\\ndt y\\r\\ny\\r\\nvy\\r\\n\\xe2\\x88\\x924y\\r\\n\\r\\n(A3)\\r\\n\\r\\n\\xef\\xa3\\xab \\xef\\xa3\\xb6 \\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\nx\\r\\nAx sin(t + \\xcf\\x95x )\\r\\n\\xef\\xa3\\xacvx \\xef\\xa3\\xb7 \\xef\\xa3\\xac Ax cos(t + \\xcf\\x95x ) \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xad y \\xef\\xa3\\xb8 = \\xef\\xa3\\xad A sin(2t + \\xcf\\x95 ) \\xef\\xa3\\xb8\\r\\ny\\r\\ny\\r\\nvy\\r\\n2Ay cos(2t + \\xcf\\x95y )\\r\\n\\r\\n(A4)\\r\\n\\r\\nSolving the equation yields the trajectory\\r\\n\\r\\nwith arbitraty constants Ax , Ay , \\xcf\\x95x and \\xcf\\x95y .\\r\\nWe define angular momentum\\r\\nL(1) \\xe2\\x89\\xa1 xpy \\xe2\\x88\\x92 ypx = 2Ax Ay (sin(t + \\xcf\\x951 \\xe2\\x88\\x92 \\xcf\\x952 )).\\r\\n\\r\\n(A5)\\r\\n\\r\\np\\r\\nAlthough L(1) is not conserved, nor is K (1) \\xe2\\x89\\xa1 (2Ax Ay )2 \\xe2\\x88\\x92 L(1)2 = 2Ax Ay cos(t + \\xcf\\x951 \\xe2\\x88\\x92 \\xcf\\x952 ). The trajectory of\\r\\nz0 \\xe2\\x89\\xa1 (x, vx , L(1) , K (1) ) can be generated from an isotropic harmonic oscillator, because all components have the same\\r\\nangular frequency. Hence the \\xe2\\x80\\x98angular momentum\\xe2\\x80\\x99 is conserved:\\r\\nL(2) \\xe2\\x89\\xa1 xK (1) \\xe2\\x88\\x92 yL(1) = x(xpy \\xe2\\x88\\x92 ypx ) \\xe2\\x88\\x92 y\\r\\n\\r\\nq\\r\\n(x2 + p2x )(y 2 + p2y ) \\xe2\\x88\\x92 (xpy \\xe2\\x88\\x92 ypx )2\\r\\n\\r\\n(A6)\\r\\n\\r\\n\\x0c\\n\\n8\\r\\n2.\\r\\n\\r\\n2D Kepler Problem\\r\\n\\r\\nThe 2D Kepler Problem is described by two coordinates (x, y) and two velocity components (vx , vy ),\\r\\n\\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\n\\xef\\xa3\\xab \\xef\\xa3\\xb6\\r\\nvx\\r\\nx\\r\\n\\xef\\xa3\\xac\\xe2\\x88\\x92GM x/(x2 + y 2 )3/2 \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xacv \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\nz = \\xef\\xa3\\xad x \\xef\\xa3\\xb8 , f (z) = \\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xad\\r\\n\\xef\\xa3\\xb8\\r\\nvy\\r\\ny\\r\\nvy\\r\\n\\xe2\\x88\\x92GM y/(x2 + y 2 )3/2\\r\\n\\r\\n(A7)\\r\\n\\r\\nwhere G is the gravitational constant, M and m are the\\r\\nsun and the planet, respectively. The system has\\r\\np mass of the\\r\\nm 2\\r\\n2\\r\\n2\\r\\nthree conserved quantities: (1) energy H1 = \\xe2\\x88\\x92GM m/ x + y + 2 (vx + vy2 ); (2) angular momentum H2 = m(xvy \\xe2\\x88\\x92\\r\\nv H +GM r\\xcc\\x82\\r\\nyvx ); (3) The direction of the Runge-lenz vector H3 = arctan( \\xe2\\x88\\x92vxy H22 +GM r\\xcc\\x82yx ) where r\\xcc\\x82 \\xe2\\x89\\xa1 (r\\xcc\\x82x , r\\xcc\\x82y ) = ( \\xe2\\x88\\x9a 2x 2 , \\xe2\\x88\\x9a 2y 2 ).\\r\\nx +y\\r\\n\\r\\nx +y\\r\\n\\r\\nWithout loss of generality, GM = 1.\\r\\np\\r\\nsymbols) which is quite expensive. To facilitate symbolic\\r\\nThe reverse Polish notation for x2 + y 2 is xQyQ+R (6 p\\r\\nlearning, one may wish to add in the radius variable r = x2 + y 2 to exploit the symmetry of the problem. To do\\r\\nso, we augment the original system with the extra variable r into an augmented system:\\r\\n\\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\n\\xef\\xa3\\xab \\xef\\xa3\\xb6\\r\\nvx\\r\\nx\\r\\n\\xef\\xa3\\xac\\xe2\\x88\\x92GM x/(x2 + y 2 )3/2 \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac vx \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac \\xef\\xa3\\xb7 0 0\\r\\n0\\r\\n\\xef\\xa3\\xb7\\r\\nvy\\r\\nz = \\xef\\xa3\\xac y \\xef\\xa3\\xb7 , f (z ) = \\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xadv \\xef\\xa3\\xb8\\r\\n2\\r\\n2 3/2 \\xef\\xa3\\xb8\\r\\n\\xef\\xa3\\xad\\r\\n\\xe2\\x88\\x92GM y/(x + y )\\r\\ny\\r\\nr\\r\\n(xvx + yvy )/r\\r\\n3.\\r\\n\\r\\n(A8)\\r\\n\\r\\n2D gravitational Three-body Problem\\r\\n\\r\\nThe three-body problem has 12 degrees of freedom: 6 positions (xi , yi )(i = 1, 2, 3) and 6 velocities (vx,i , vy,i )(i =\\r\\n1, 2, 3),\\r\\n\\xef\\xa3\\xb6\\r\\n\\r\\n\\xef\\xa3\\xab\\r\\n\\r\\nv1,x\\r\\n\\xef\\xa3\\xac\\r\\nv1,y\\r\\nx1\\r\\n\\xef\\xa3\\xac\\r\\nGm2 (x2 \\xe2\\x88\\x92x1 )\\r\\n\\xef\\xa3\\xac y1 \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\xe2\\x88\\x92\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac ((x2 \\xe2\\x88\\x92x1 )2 +(y2 \\xe2\\x88\\x92y1 )2 )3/2 \\xe2\\x88\\x92\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xacv1,x \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n2 (y2 \\xe2\\x88\\x92y1 )\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\xe2\\x88\\x92 ((x \\xe2\\x88\\x92xGm\\r\\n\\xef\\xa3\\xb7\\r\\n2\\r\\n2 3/2 \\xe2\\x88\\x92\\r\\n2\\r\\n1 ) +(y2 \\xe2\\x88\\x92y1 ) )\\r\\n\\xef\\xa3\\xacv1,y \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\nv2,x\\r\\n\\xef\\xa3\\xac x2 \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\nv2,y\\r\\n\\xef\\xa3\\xacy \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\nGm1 (x1 \\xe2\\x88\\x92x2 )\\r\\nz = \\xef\\xa3\\xac 2 \\xef\\xa3\\xb7 , f = \\xef\\xa3\\xac\\xe2\\x88\\x92\\r\\n\\xef\\xa3\\xacv2,x \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac ((x1 \\xe2\\x88\\x92x2 )2 +(y1 \\xe2\\x88\\x92y2 )2 )3/2 \\xe2\\x88\\x92\\r\\n\\xef\\xa3\\xacv \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n1 (y1 \\xe2\\x88\\x92y2 )\\r\\n\\xef\\xa3\\xac 2,y \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\xe2\\x88\\x92 ((x \\xe2\\x88\\x92xGm\\r\\n2\\r\\n2 3/2 \\xe2\\x88\\x92\\r\\n1\\r\\n2 ) +(y1 \\xe2\\x88\\x92y2 ) )\\r\\n\\xef\\xa3\\xacx \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac 3\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\nv3,x\\r\\n\\xef\\xa3\\xacy \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac 3\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\nv3,y\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xadv3,x \\xef\\xa3\\xb8\\r\\nGm1 (x1 \\xe2\\x88\\x92x3 )\\r\\n\\xef\\xa3\\xac\\xe2\\x88\\x92\\r\\n\\xef\\xa3\\xad ((x1 \\xe2\\x88\\x92x3 )2 +(y1 \\xe2\\x88\\x92y3 )2 )3/2 \\xe2\\x88\\x92\\r\\nv3,y\\r\\n1 (y1 \\xe2\\x88\\x92y3 )\\r\\n\\xe2\\x88\\x92 ((x1 \\xe2\\x88\\x92xGm\\r\\n2\\r\\n2 3/2 \\xe2\\x88\\x92\\r\\n3 ) +(y1 \\xe2\\x88\\x92y3 ) )\\r\\n\\xef\\xa3\\xab\\r\\n\\r\\n\\xef\\xa3\\xb6\\r\\n\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\r\\nGm3 (x3 \\xe2\\x88\\x92x1 )\\r\\n\\xef\\xa3\\xb7\\r\\n((x3 \\xe2\\x88\\x92x1 )2 +(y3 \\xe2\\x88\\x92y1 )2 )3/2 \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\nGm3 (y3 \\xe2\\x88\\x92y1 )\\r\\n\\xef\\xa3\\xb7\\r\\n((x3 \\xe2\\x88\\x92x1 )2 +(y3 \\xe2\\x88\\x92y1 )2 )3/2 \\xef\\xa3\\xb7\\r\\n\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\nGm3 (x3 \\xe2\\x88\\x92x2 )\\r\\n\\xef\\xa3\\xb7,\\r\\n((x3 \\xe2\\x88\\x92x2 )2 +(y3 \\xe2\\x88\\x92y2 )2 )3/2 \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\nGm3 (y3 \\xe2\\x88\\x92y2 )\\r\\n\\xef\\xa3\\xb7\\r\\n((x3 \\xe2\\x88\\x92x2 )2 +(y3 \\xe2\\x88\\x92y2 )2 )3/2 \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\nGm2 (x2 \\xe2\\x88\\x92x3 )\\r\\n\\xef\\xa3\\xb7\\r\\n((x2 \\xe2\\x88\\x92x3 )2 +(y2 \\xe2\\x88\\x92y3 )2 )3/2 \\xef\\xa3\\xb8\\r\\n\\r\\n(A9)\\r\\n\\r\\nGm2 (y2 \\xe2\\x88\\x92y3 )\\r\\n((x2 \\xe2\\x88\\x92x3 )2 +(y2 \\xe2\\x88\\x92y3 )2 )3/2\\r\\n\\r\\nwhere G is the gravitational constant, and mi (i = 1, 2, 3) are three masses. The system has 4 conserved quanP3\\r\\nP3\\r\\ntities: (1) x-momentum: H1 =\\r\\n; (2) y-momentum: H2 =\\r\\ni=1 mi vi,x\\r\\ni=1 mi vi,y ; (3) angular momentum:\\r\\nP3 1\\r\\nP3\\r\\nGm1 m3\\r\\n2\\r\\n2\\r\\n1 m2\\r\\nH3 = i=1 mi (xi vi,y \\xe2\\x88\\x92yi vi,x ); (4) energy H = i=1 2 mi (vi,x\\r\\n+vi,y\\r\\n)+( ((x1 \\xe2\\x88\\x92x2 )Gm\\r\\n2 +(y \\xe2\\x88\\x92y )2 )1/2 + ((x \\xe2\\x88\\x92x )2 +(y \\xe2\\x88\\x92y )2 )1/2 +\\r\\n1\\r\\n2\\r\\n1\\r\\n3\\r\\n1\\r\\n3\\r\\nGm2 m3\\r\\n).\\r\\n((x2 \\xe2\\x88\\x92x3 )2 +(y2 \\xe2\\x88\\x92y3 )2 )1/2\\r\\n\\r\\nIn numerical experiments, we set G = m1 = m2 = m3 = 1. Similar to the Kepler problem, we\\r\\ncan simplify symbolic search by adding three distance variables:\\r\\np\\r\\nr12 = (x1 \\xe2\\x88\\x92 x2 )2 + (y1 \\xe2\\x88\\x92 y2 )2\\r\\np\\r\\n(A10)\\r\\nr13 = (x1 \\xe2\\x88\\x92 x3 )2 + (y1 \\xe2\\x88\\x92 y3 )2\\r\\np\\r\\n2\\r\\n2\\r\\nr23 = (x2 \\xe2\\x88\\x92 x3 ) + (y2 \\xe2\\x88\\x92 y3 )\\r\\nThe equations are augmented correspondingly:\\r\\n\\r\\n\\x0c\\n\\n9\\r\\n\\r\\n\\xef\\xa3\\xb6\\r\\n\\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\n..\\r\\n..\\r\\n.\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac . \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac \\xef\\xa3\\xb7\\r\\nz = \\xef\\xa3\\xacr12 \\xef\\xa3\\xb7 , f = \\xef\\xa3\\xac((x1 \\xe2\\x88\\x92 x2 )(v1,x \\xe2\\x88\\x92 v2,x ) + (y1 \\xe2\\x88\\x92 y2 )(v1,y \\xe2\\x88\\x92 v2,y ))/r12 \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xad((x1 \\xe2\\x88\\x92 x3 )(v1,x \\xe2\\x88\\x92 v3,x ) + (y1 \\xe2\\x88\\x92 y3 )(v1,y \\xe2\\x88\\x92 v3,y ))/r13 \\xef\\xa3\\xb8\\r\\n\\xef\\xa3\\xadr13 \\xef\\xa3\\xb8\\r\\n((x2 \\xe2\\x88\\x92 x3 )(v2,x \\xe2\\x88\\x92 v3,x ) + (y2 \\xe2\\x88\\x92 y3 )(v2,y \\xe2\\x88\\x92 v3,y ))/r23\\r\\nr23\\r\\n\\xef\\xa3\\xab\\r\\n\\r\\n4.\\r\\n\\r\\n(A11)\\r\\n\\r\\nKorteweg\\xe2\\x80\\x93De Vries equation (KdV) wave equation\\r\\n\\r\\nThe Korteweg\\xe2\\x80\\x93De Vries (KdV) equation is a mathematical model for shallow water surfaces. It is a nonlinear\\r\\npartial differential equation for a function \\xcf\\x86 with two real variables, x (space) and t (time):\\r\\n\\r\\n\\xcf\\x86t + \\xcf\\x86xxx \\xe2\\x88\\x92 6\\xcf\\x86\\xcf\\x86x = 0\\r\\n\\r\\n(A12)\\r\\n\\r\\nZero boundary conditions are imposed at the ends of the interval [a, b]. The KdV equation is known to have infinitely\\r\\nmany conserved quantities [18], which can be written explicitly as\\r\\nZ\\r\\n\\r\\nb\\r\\n\\r\\nP2n\\xe2\\x88\\x921 (\\xcf\\x86, \\xcf\\x86x , \\xcf\\x86xx , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 )dx,\\r\\n\\r\\n(A13)\\r\\n\\r\\na\\r\\n\\r\\nwhich follows from locality and translational symmetry. The polynomials Pn are defined recursively by\\r\\nP1 = \\xcf\\x86\\r\\nn\\xe2\\x88\\x922\\r\\n\\r\\ndPn\\xe2\\x88\\x921 X\\r\\nPn = \\xe2\\x88\\x92\\r\\n+\\r\\nPi Pn\\xe2\\x88\\x921\\xe2\\x88\\x92i\\r\\ndx\\r\\ni=1\\r\\n\\r\\n(A14)\\r\\n\\r\\nThe first few conservation laws are\\r\\nZ\\r\\nZ\\r\\nZ\\r\\n\\r\\n\\xcf\\x86dx\\r\\n\\r\\n(mass)\\r\\n\\r\\n\\xcf\\x862 dx\\r\\n\\r\\n(momentum)\\r\\n\\r\\n(2\\xcf\\x863 \\xe2\\x88\\x92 \\xcf\\x862x )dx\\r\\n\\r\\n(energy)\\r\\n\\r\\n(A15)\\r\\n\\r\\nDespite infinite many conservation laws, useful ones in physics are usually constrained to contain only \\xcf\\x86 and low-order\\r\\nderivatives (\\xcf\\x86x , \\xcf\\x86xx , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ). On the numerical front, our algorithm successfully discovers 2, 3, 4 conserved quantities\\r\\nwhich are dependent on \\xcf\\x86, (\\xcf\\x86, \\xcf\\x86x ) and (\\xcf\\x86, \\xcf\\x86x , \\xcf\\x86xx ) respectively. On the symbolic front, we constrain the input variables\\r\\nto be (\\xcf\\x86, \\xcf\\x86x ), and all three conservation laws (mass, momentum and energy) can be discovered.\\r\\nConverting to the canonical form z\\xcc\\x87 = f (z) Since our framework can only deal with systems with finite degrees\\r\\nof freedom, we need to discretize space. We discretize the interval x \\xe2\\x88\\x88 [\\xe2\\x88\\x9210, 10] uniformly into Np = 40 points,\\r\\ndenoted x1 , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , xNp and only store derivatives up to fifth order on each grid point, using them to parametrize our\\r\\n(i)\\r\\n\\xcf\\x86(x). This transforms our PDE into an ordinary differential equation with 3Np degrees of freedom (\\xcf\\x86(i) = \\xcf\\x86(xi ), \\xcf\\x86x =\\r\\n\\xcf\\x86x (xi ), \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ): Eq. (A12) implies that\\r\\n\\xef\\xa3\\xab\\r\\n\\r\\n\\xef\\xa3\\xb6 \\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\n\\xe2\\x88\\x92\\xcf\\x86xxx + 6\\xcf\\x86\\xcf\\x86x\\r\\n\\xcf\\x86\\r\\n\\xef\\xa3\\xac \\xcf\\x86x \\xef\\xa3\\xb7 \\xef\\xa3\\xac \\xe2\\x88\\x92\\xcf\\x86xxxx + 6(\\xcf\\x862x + \\xcf\\x86\\xcf\\x86xx ) \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7 \\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xe2\\x88\\x82t \\xef\\xa3\\xac\\xcf\\x86xx \\xef\\xa3\\xb7 = \\xef\\xa3\\xac\\xe2\\x88\\x92\\xcf\\x86xxxxx + 6(3\\xcf\\x86x \\xcf\\x86xx + \\xcf\\x86\\xcf\\x86xxx )\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xad\\r\\n\\xef\\xa3\\xb8 \\xef\\xa3\\xad\\r\\n\\xef\\xa3\\xb8\\r\\n..\\r\\n..\\r\\n.\\r\\n.\\r\\n\\r\\n(A16)\\r\\n\\r\\n\\x0c\\n\\n10\\r\\nso our discretized PDE problem becomes\\r\\n\\xef\\xa3\\xab\\r\\n\\r\\n\\xcf\\x86(1)\\r\\n(1)\\r\\n\\xcf\\x86x\\r\\n(1)\\r\\n\\xcf\\x86xx\\r\\n..\\r\\n.\\r\\n\\r\\n\\xef\\xa3\\xab\\r\\n\\r\\n\\xef\\xa3\\xb6\\r\\n\\r\\n(1)\\r\\n\\r\\n(1)\\r\\n\\r\\n\\xe2\\x88\\x92\\xcf\\x86xxx + 6\\xcf\\x86(1) \\xcf\\x86x\\r\\n(1)\\r\\n(1)2\\r\\n(1)\\r\\n\\xe2\\x88\\x92\\xcf\\x86xxxx + 6(\\xcf\\x86x + \\xcf\\x86(1) \\xcf\\x86xx )\\r\\n(1)\\r\\n(1) (1)\\r\\n(1)\\r\\n\\xe2\\x88\\x92\\xcf\\x86xxxxx + 6(3\\xcf\\x86x \\xcf\\x86xx + \\xcf\\x86(1) \\xcf\\x86xxx )\\r\\n..\\r\\n.\\r\\n\\r\\n\\xef\\xa3\\xb6\\r\\n\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7 , f (z) \\xe2\\x89\\xa1 \\xe2\\x88\\x82t z = \\xef\\xa3\\xac\\r\\nz\\xe2\\x89\\xa1\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac (Np ) \\xef\\xa3\\xb7\\r\\n(Np )\\r\\n(N\\r\\n)\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\np\\r\\n(Np )\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\xcf\\x86\\r\\n\\xe2\\x88\\x92\\xcf\\x86\\r\\n+\\r\\n6\\xcf\\x86\\r\\n\\xcf\\x86\\r\\nxxx\\r\\nx\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac (Np ) \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n(Np )\\r\\n(N )2\\r\\n(N )\\r\\n\\xef\\xa3\\xad\\xcf\\x86x \\xef\\xa3\\xb8\\r\\n\\xef\\xa3\\xad \\xe2\\x88\\x92\\xcf\\x86xxxx\\r\\n+ 6(\\xcf\\x86x p + \\xcf\\x86(Np ) \\xcf\\x86xx p ) \\xef\\xa3\\xb8\\r\\n(N )\\r\\n(Np )\\r\\n(N ) (N )\\r\\n(N )\\r\\n\\xcf\\x86xx p\\r\\n\\xe2\\x88\\x92\\xcf\\x86xxxxx\\r\\n+ 6(3\\xcf\\x86x p \\xcf\\x86xx p + \\xcf\\x86(Np ) \\xcf\\x86xxxp )\\r\\n\\r\\n(A17)\\r\\n\\r\\nSample generation We represent \\xcf\\x86 as a Gaussian mixture, so all derivatives can be computed analytically. In\\r\\nparticular,\\r\\n\\r\\n\\xcf\\x86(x) =\\r\\n\\r\\nNg\\r\\nX\\r\\n\\r\\nAi ( \\xe2\\x88\\x9a\\r\\n\\r\\ni=1\\r\\n\\r\\n1\\r\\nexp(\\xe2\\x88\\x92(x \\xe2\\x88\\x92 \\xc2\\xb5i )2 )/2\\xcf\\x83i2 ),\\r\\n2\\xcf\\x80\\xcf\\x83i\\r\\n\\r\\n\\xe2\\x88\\x9210 \\xe2\\x89\\xa4 x \\xe2\\x89\\xa4 10\\r\\n\\r\\n(A18)\\r\\n\\r\\nwhere coefficients are set or drawn randomly accordingly to Ai \\xe2\\x88\\xbc U [\\xe2\\x88\\x925, 5], \\xc2\\xb5i \\xe2\\x88\\xbc U [\\xe2\\x88\\x923, 3], \\xcf\\x83i = 1.5. These distributions\\r\\nare chosen such that (1) \\xcf\\x86(x) is (almost) zero at two boundary points x = \\xe2\\x88\\x9210, 10; and (2) every single term in f (z)\\r\\nhave similar magnitudes. We choose Ng = 5 and generate P = 104 profiles of \\xcf\\x86.\\r\\nConstraining\\r\\nR conservation laws The conservation laws of partial differential equations usually have the integral\\r\\nform, i.e., H = h(x0 )dx where x0 = (\\xcf\\x86, \\xcf\\x86x , \\xcf\\x86xx , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ). When space is discretized, we constrain the conservation law\\r\\nPNp\\r\\nto the form H = i=1\\r\\nh(x0 ). On the numerical front, we parameterize h(x0 ) (as opposed to H) by a neural network;\\r\\nOn the symbolic front, we search the symbolic formula of h(x0 ) (as opposed to H). The summation operation is hard\\r\\ncoded for both fronts.\\r\\nAvoiding trivial conservation laws Due to zero boundary conditions, if h(x0 ) is an x-derivative of another\\r\\nRb\\r\\nfunction g(x0 ), then it is obvious that a h(x0 )dx = g(x0 )|b \\xe2\\x88\\x92 g(x0 )|a = 0 which is a trivial conserved quantity. For\\r\\nexample, h(x0 ) = \\xcf\\x86x , \\xcf\\x86\\xcf\\x86x , \\xcf\\x86xx , \\xcf\\x862x + \\xcf\\x86\\xcf\\x86xx are all trivial. We observe that each of them has at least one term that is\\r\\nan odd function of a derivative. Consequently a simple solution is to use absolute values (|\\xcf\\x86x |, |\\xcf\\x86xx |, \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ) instead of\\r\\n(\\xcf\\x86x , \\xcf\\x86xx , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ) so that these trivial conservation laws are avoided in the first place.\\r\\n\\r\\n5.\\r\\n\\r\\nNonlinear Schro\\xcc\\x88dinger Equation\\r\\n\\r\\nThe (1D nonlinear Schro\\xcc\\x88dinger equation (NLS) is a nonlinear generalization of the Schro\\xcc\\x88dinger equation. Its\\r\\nprincipal applications are to the propagation of light in nonlinear optical fibres and planar waveguides and to BoseEinstein condensates. The classical field equation (in dimensionless form) is\\r\\n1\\r\\ni\\xcf\\x88t = \\xe2\\x88\\x92 \\xcf\\x88xx + \\xce\\xba|\\xcf\\x88|2 \\xcf\\x88.\\r\\n2\\r\\n\\r\\n(A19)\\r\\n\\r\\nZero boundary conditions are imposed at infinity [19]. Like the KdV equation, the NLS has infinite many conserved\\r\\nquantities of the integral form\\r\\nZ\\r\\n\\r\\n\\xe2\\x88\\x9e\\r\\n\\r\\nh(\\xcf\\x88, \\xcf\\x88x , \\xcf\\x88xx , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 )dx.\\r\\n\\r\\nH(x) =\\r\\n\\r\\n(A20)\\r\\n\\r\\n\\xe2\\x88\\x92\\xe2\\x88\\x9e\\r\\n\\r\\nUseful conservation laws in physics usually contain only low-order derivatives, e.g.,\\r\\nZ\\r\\nunitarity : |\\xcf\\x88|2 dx\\r\\nZ\\r\\n\\x01\\r\\n1\\r\\nenergy :\\r\\n|\\xcf\\x88x |2 + \\xce\\xba|\\xcf\\x88|4 dx\\r\\n2\\r\\n\\r\\n(A21)\\r\\n\\r\\n\\x0c\\n\\n11\\r\\nConverting to the canonical form z\\xcc\\x87 = f (z) Similar to the KdV equation, we treat (\\xcf\\x88, \\xcf\\x88x , \\xcf\\x88xx , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ) as different\\r\\nvariables. We denote \\xcf\\x88r \\xe2\\x89\\xa1 Re(\\xcf\\x88), \\xcf\\x88i \\xe2\\x89\\xa1 Im(\\xcf\\x88), Re(\\xcf\\x88x ) = \\xcf\\x88x,r , Im(\\xcf\\x88x ) = \\xcf\\x88x,i , etc.\\r\\n\\xef\\xa3\\xb6 \\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\n1\\r\\n2\\r\\n\\xcf\\x88\\r\\n2 i\\xcf\\x88xx \\xe2\\x88\\x92 i\\xce\\xba|\\xcf\\x88| \\xcf\\x88\\r\\n1\\r\\n2\\r\\n\\xef\\xa3\\xac \\xcf\\x88x \\xef\\xa3\\xb7 \\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n2 i\\xcf\\x88xxx \\xe2\\x88\\x92 i\\xce\\xba(|\\xcf\\x88| \\xcf\\x88x + (\\xcf\\x88r \\xcf\\x88x,r + \\xcf\\x88i \\xcf\\x88x,i )\\xcf\\x88)\\r\\n\\xef\\xa3\\xb7 \\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n2\\r\\n2\\r\\n(A22)\\r\\n\\xe2\\x88\\x82t \\xef\\xa3\\xac\\xcf\\x88xx \\xef\\xa3\\xb7 = \\xef\\xa3\\xac 1 i\\xcf\\x88xxxx \\xe2\\x88\\x92 i\\xce\\xba(|\\xcf\\x88|2 \\xcf\\x88xx + 2(\\xcf\\x88r \\xcf\\x88x,r + \\xcf\\x88i \\xcf\\x88x,i )\\xcf\\x88x + (\\xcf\\x88x,r\\r\\n+ \\xcf\\x88r \\xcf\\x88xx,r + \\xcf\\x88x,i + \\xcf\\x88i \\xcf\\x88xx,i )\\xcf\\x88)\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb8 \\xef\\xa3\\xad2\\r\\n\\xef\\xa3\\xad\\r\\n\\xef\\xa3\\xb8\\r\\n..\\r\\n..\\r\\n.\\r\\n.\\r\\nSince \\xcf\\x88 is a complex number, we should treat real and imaginary parts separately.\\r\\n\\xef\\xa3\\xb6\\r\\n\\xef\\xa3\\xb6 \\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xab\\r\\n\\xe2\\x88\\x92 21 \\xcf\\x88xx,i + \\xce\\xba|\\xcf\\x88|2 \\xcf\\x88i\\r\\n\\xcf\\x88r\\r\\n1\\r\\n2\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac \\xcf\\x88i \\xef\\xa3\\xb7 \\xef\\xa3\\xac\\r\\n2 \\xcf\\x88xx,r \\xe2\\x88\\x92 \\xce\\xba|\\xcf\\x88| \\xcf\\x88r\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7 \\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n1\\r\\n2\\r\\n\\xef\\xa3\\xb7\\r\\n\\xcf\\x88\\r\\n+\\r\\n\\xce\\xba(|\\xcf\\x88|\\r\\n\\xcf\\x88\\r\\n+\\r\\n(\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n+\\r\\n\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n)\\xcf\\x88\\r\\n)\\r\\n\\xe2\\x88\\x92\\r\\n\\xef\\xa3\\xac \\xcf\\x88x,r \\xef\\xa3\\xb7 \\xef\\xa3\\xac\\r\\nxxx,i\\r\\nx\\r\\nr\\r\\nx,r\\r\\ni\\r\\nx,i\\r\\ni\\r\\ni\\r\\n2\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\xcf\\x88 \\xef\\xa3\\xb7 \\xef\\xa3\\xac\\r\\n1\\r\\n2\\r\\n\\xef\\xa3\\xb7\\r\\n\\xcf\\x88\\r\\n\\xe2\\x88\\x92\\r\\n\\xce\\xba(|\\xcf\\x88|\\r\\n\\xcf\\x88\\r\\n+\\r\\n(\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n+\\r\\n\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n)\\xcf\\x88\\r\\n)\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\nxxx,r\\r\\nx,r\\r\\nr\\r\\nx,r\\r\\ni\\r\\nx,i\\r\\nr\\r\\nx,i\\r\\n2\\r\\n=\\r\\n\\xe2\\x88\\x82t \\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7 (A23)\\r\\n\\xef\\xa3\\xb7\\r\\n1\\r\\n2\\r\\n2\\r\\n2\\r\\n\\xef\\xa3\\xb7\\r\\n+\\r\\n\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n)\\xcf\\x88\\r\\n)\\r\\n\\xe2\\x88\\x92\\r\\n\\xcf\\x88\\r\\n+\\r\\n\\xce\\xba(|\\xcf\\x88|\\r\\n\\xcf\\x88\\r\\n+\\r\\n2(\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n+\\r\\n\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n)\\xcf\\x88\\r\\n+\\r\\n(\\xcf\\x88\\r\\n+\\r\\n\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n+\\r\\n\\xcf\\x88\\r\\n\\xef\\xa3\\xac\\xcf\\x88xx,r \\xef\\xa3\\xb7 \\xef\\xa3\\xac\\r\\ni\\r\\nxx,i\\r\\ni\\r\\nxxxx,i\\r\\nxx,i\\r\\nr\\r\\nx,r\\r\\ni\\r\\nx,i\\r\\nx,i\\r\\nr,i\\r\\nxx,r\\r\\nx,r\\r\\nx,i\\r\\n2\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\xcf\\x88\\r\\n1\\r\\n2\\r\\n2\\r\\n2\\r\\n\\xcf\\x88\\r\\n\\xe2\\x88\\x92\\r\\n\\xce\\xba(|\\xcf\\x88|\\r\\n\\xcf\\x88\\r\\n+\\r\\n2(\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n+\\r\\n\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n)\\xcf\\x88\\r\\n+\\r\\n(\\xcf\\x88\\r\\n+\\r\\n\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n+\\r\\n\\xcf\\x88\\r\\n+\\r\\n\\xcf\\x88\\r\\n\\xcf\\x88\\r\\n)\\xcf\\x88\\r\\n)\\r\\n\\xef\\xa3\\xad xx,i \\xef\\xa3\\xb8 \\xef\\xa3\\xac\\r\\nxx,r\\r\\nr x,r\\r\\ni x,i\\r\\nx,r\\r\\nr xx,r\\r\\ni xx,i\\r\\nr \\xef\\xa3\\xb7\\r\\nx,r\\r\\nx,i\\r\\n2 xxxx,r\\r\\n\\xef\\xa3\\xad\\r\\n\\xef\\xa3\\xb8\\r\\n..\\r\\n..\\r\\n.\\r\\n.\\r\\nJust as in the KdV example, to avoid trivial solutions, we consider only the equations for magnitude\\r\\n(|\\xcf\\x88|, |\\xcf\\x88x |, |\\xcf\\x88xx |, \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ).\\r\\n\\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6 \\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\n|\\xcf\\x88|\\r\\n(\\xcf\\x88r \\xe2\\x88\\x82t \\xcf\\x88r + \\xcf\\x88i \\xe2\\x88\\x82t \\xcf\\x88i )/|\\xcf\\x88|\\r\\n\\xef\\xa3\\xac |\\xcf\\x88x | \\xef\\xa3\\xb7 \\xef\\xa3\\xac (\\xcf\\x88x,r \\xe2\\x88\\x82t \\xcf\\x88x,r + \\xcf\\x88x,i \\xe2\\x88\\x82t \\xcf\\x88x,i )/|\\xcf\\x88x | \\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7 \\xef\\xa3\\xac\\r\\n\\xef\\xa3\\xb7\\r\\n(A24)\\r\\n\\xe2\\x88\\x82t \\xef\\xa3\\xac|\\xcf\\x88xx |\\xef\\xa3\\xb7 = \\xef\\xa3\\xac(\\xcf\\x88xx,r \\xe2\\x88\\x82t \\xcf\\x88xx,r + \\xcf\\x88xx,i \\xe2\\x88\\x82t \\xcf\\x88xx,i )/|\\xcf\\x88xx |\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xad\\r\\n\\xef\\xa3\\xb8 \\xef\\xa3\\xad\\r\\n\\xef\\xa3\\xb8\\r\\n..\\r\\n..\\r\\n.\\r\\n.\\r\\n| {z } |\\r\\n{z\\r\\n}\\r\\nz\\r\\n\\r\\nf\\r\\n\\r\\nSample generation is similar to the KdV equations, with the only difference that real and imaginary parts are\\r\\nboth treated as (independent) Gaussian mixtures.\\r\\nAppendix B: How to determine (in)dependence of multiple conserved quantities\\r\\n\\r\\nSuppose we already have n independent conserved quantities H = {H1 (z), \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , Hn (z), z \\xe2\\x88\\x88 Rs }, which are parameterized as neural networks or symbolic formulas. How do we determine a new conserved quantity Hn+1 (z) is dependent\\r\\non or independent of H?\\r\\nMethod S\\r\\nA: differential rank. We know kD (H) = n due to the function independence of H. We then compute\\r\\nk 0 \\xe2\\x89\\xa1 kD (H Hn+1 ). If k 0 = n + 1, then Hn+1 is independent of Hn ; otherwise k 0 = n, Hn+1 is depedent on\\r\\nH. In practice, we compute the singular value decomposition of B (defined in Eq. (6)). If the least singular value\\r\\n\\xcf\\x83n+1 < \\x0f\\xcf\\x83 = 10\\xe2\\x88\\x923 , we consider it vanishing hence k 0 = n; otherwise k 0 = n + 1. However the complexity of SVD is\\r\\nO(sn2 ), which is more expensive to compute than the method B.\\r\\nMethod B: orthogonality test. Because H is an independent set of functions, their gradients at all z (except\\r\\nzero measure) should span a linear subspace S(z) \\xe2\\x89\\xa1 span(\\xe2\\x88\\x87H1 (z), \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , \\xe2\\x88\\x87Hn (z)) with dimensionality n. We construct\\r\\na random unit vector b\\r\\nt(z) that is orthogonal to S, which can be implemented as a Gram-Schmidt process of a\\r\\nrandom vector and n gradient vectors. If Hn+1 (z) is not independent of H, then the gradient \\xe2\\x88\\x87Hn+1 (z) \\xe2\\x88\\x88 S(z), so\\r\\n\\xe2\\x88\\x923\\r\\n\\\\\\r\\n\\\\\\r\\nb\\r\\nb\\r\\nt \\xc2\\xb7 \\xe2\\x88\\x87H\\r\\nand reject it. If Hn+1 (z)\\r\\nn+1 (z) = 0. We consider Hn+1 to be not independent if t(z) \\xc2\\xb7 \\xe2\\x88\\x87Hn+1 (z) < \\x0fi = 10\\r\\n\\\\\\r\\nis independent of H, then b\\r\\nt(z) \\xc2\\xb7 \\xe2\\x88\\x87H\\r\\nn+1 (z) > \\x0fi is true with high probability. To further reduce probability of errors,\\r\\none may test on nt points, which incurs an O(nt s) complexity.\\r\\nOnce Hn+1 is verified as being independent of H, we append Hn+1 to H. This process is repeated until |H| (the\\r\\nnumber of functions) is equal the number of conserved quantities (obtained from the neural network front) or the\\r\\nbrute force search reaches the maximum computation budget.\\r\\nAppendix C: Various definitions of integrability and relations to AI Poincare\\xcc\\x81 1.0/2.0\\r\\n\\r\\nConservation laws are closed related to the notion of integrability [20], which in turn has various definitions from\\r\\ndifferent perspectives [21, 22]. In the following, we list five definitions of integrability and corresponding definitions\\r\\n\\r\\n\\x0c\\n\\n12\\r\\n\\r\\nAI Poincare\\xcc\\x81 1.0\\r\\nAI Poincare\\xcc\\x81 2.0\\r\\na\\r\\n\\r\\nGeneral Frobenius Liouville Landau solvable\\r\\nYes\\r\\nNo\\r\\nNo\\r\\nNo\\r\\nYes\\r\\nYes\\r\\nYes\\r\\nYes a\\r\\nYes\\r\\nYes\\r\\n\\r\\nThis case is not included in paper, but is doable when we combine the techniques of searching for hidden symmetries in [23].\\r\\n\\r\\nTABLE II: Five integrability definitions and whether AI Poincare\\xcc\\x81 1.0/2.0 can deal with them.\\r\\n\\r\\nof conserved quantities.\\r\\n(1) General integrability [global geometry/topology]. In the context of differential dynamical systems, the\\r\\nnotion of integrability refers to the existence of an invariant regular foliation of phase space [21]. Consequently, a\\r\\nconserved quantity should be a well-behaved function globally, not demonstrating any fractal or other pathological\\r\\nbehavior.\\r\\n(2) Frobenius integrability [local geometry/topology]. A dynamical system is said to be Frobenius integrable\\r\\nif, locally, the phase space has a foliation of invariant manifolds [21]. One major corollary of the Frobenius theorem\\r\\nis that a first-order dynamical system with s degrees of freedom has s \\xe2\\x88\\x92 1 (local) integrals of motion. Consequently,\\r\\na conserved quantity in the sense of Frobenius integrability does not require the foliation to be regular in the global\\r\\nsense. The visual differences between local and global conserved quantities are shown in FIG. (3) and FIG. (4).\\r\\n(3) Liouville integrability [algebra]. In the special setting of Hamiltonian systems, we have Liouville integrability, which focuses on algebraic properties of a Hamiltonian system [15]. Liouville integrability states that there exists\\r\\na maximal set of Poisson commuting invariants. Consequently, conserved quantities corresponds to Poisson commuting quantities. A system in the 2n-dimensional phase space is Liouville integrable if it has n independent conserved\\r\\nquantities which commutate with each other, i.e., {Hi , Hj } = 0. According to the Liouville-Arnold theorem [15], such\\r\\nsystems can be solved exactly by quadrature, which is a special case of solvability integrability (the fifth criterion\\r\\nbelow).\\r\\n(4) Landau integrability [concept simplicity] Landau stated in his textbook [13] that physicists prefer symmetric and additive IOMs and promote them as fundamental \\xe2\\x80\\x9cconservation laws\\xe2\\x80\\x9d.\\r\\n(5) Solvable integrability [symbolic simplicity]. Solvable integrability requires the determination of solutions\\r\\nin an explicit functional form [22]. This property is intrinsic, but can be very useful to simplify and theoretically\\r\\nunderstand problems.\\r\\n(6) Experimental integrability [robustness]. In physics, we consider a conserved quantity useful if a measurement of it at some time t can constrain the state at some later time. In experimental physics, a measurement\\r\\nof a physical quantity always contains some finite error. Hence a useful conserved quantity must not be infinitely\\r\\nsensitive to measurement error. In contrast, FIG. 4 (top row) shows that, although a conserved quantity exists for\\r\\nall possible frequency pairs (\\xcf\\x89x , \\xcf\\x89y ), their robustness to noise differ widely. Once the noise scale significantly exceeds\\r\\nthe width of stripe pattern, an accurate measurement of the conserved quantity is impossible, and a measurement\\r\\nof the \\xe2\\x80\\x9cconserved quantity\\xe2\\x80\\x9d provides essentially zero useful information for predicting the future state. When the\\r\\nfrequency ratio is an irrational number, the \\xe2\\x80\\x9cconserved quantity\\xe2\\x80\\x9d becomes discontinuous and pathological throughout\\r\\nphase space and completely useless for making physics predictions. This experimental integrability criterion is thus\\r\\ncompatible with general integrability, but does not follow from Frobenius integrability.\\r\\nIn summary, the various notions of integrability are used to study dynamical systems, but have different motivations\\r\\nand scopes. General and the Frobenius integrability characterize global and local geometry; Liouville integrability\\r\\ntakes an algebraic perspective and applies only to Hamiltonian systems; Landau and solvable integrability instead\\r\\nfocus on simplicity based on concepts and symbolic equations, respectively. To the best of our knowledge, there is no\\r\\nagreement on whether one particular definition outperform others in all senses. We believe they are complementary\\r\\nto each other, rather than being contradictory or redundant. In AI Poincare\\xcc\\x81 1.0 and 2.0 (the current paper), we\\r\\nmostly did not mentioned explicitly which sense of integrability/conserved quantities we referred to. Fortunately, AI\\r\\nPoincare\\xcc\\x81 2.0 is quite flexible to adapt to all definitions, as summarized in Table II.\\r\\nAI Poincare\\xcc\\x81 1.0 defines a trajectory manifold, which is orthogonal to the invariant manifold. The trajectory manifold\\r\\nis globally defined, and its dimensionality is a topological invariant. As a consequence, in AI Poincare\\xcc\\x81 1.0, conserved\\r\\nquantities satisfy general integrability. The symbolic part of AI Poincare\\xcc\\x81 1.0 looks for formulas with simple symbolic\\r\\nforms, in the spirit of solvable integrability.\\r\\nAI Poincare\\xcc\\x81 2.0 addresses the problem of finding a maximal set of independent conserved quantities, in analogy to\\r\\nthe goal of the Frobenius theorem [24] which searches for a maximal set of solutions of a regular system of first-order\\r\\nlinear homogeneous partial differential equations. The loss formulation in Eq. (3) can be viewed as a variational\\r\\nformulation of the system of PDEs to be satisfied for conserved quantities. Consequently, AI Poincare\\xcc\\x81 2.0 (neural\\r\\nnetwork front) is aligned with the Frobenius integrability if there is only one training sample z. In the presence of many\\r\\ntraining samples over the phase space, our algorithm becomes aligned with the notion of the general integrability,\\r\\n\\r\\n\\x0c\\n\\n13\\r\\n=0\\r\\n\\r\\n3\\r\\n\\r\\n= 0.01\\r\\n\\r\\n3\\r\\n\\r\\n= 0.1\\r\\n\\r\\n3\\r\\n\\r\\n=1\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n= 10\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\nFIG. 3: The conserved quantity of the 1D damped harmonic oscillator with different \\xce\\xb3. Neural networks cannot\\r\\nperfectly learn the singular behavior near at the origin, and also struggle when the stripes get too narrow.\\r\\n\\r\\nbecause the conserved quantity is parameterized as a neural network which has an implicit bias towards smooth\\r\\nand regular functions globally. Although we did not explicitly deal with Liouville integrability in this paper, the\\r\\nalgebraic nature of Liouville integrability makes it simply a \\xe2\\x80\\x9chidden symmetry problem\\xe2\\x80\\x9d that is defined and solved\\r\\nby [23], and the techniques in the current paper can further polish the story by determining functional dependence\\r\\namong invariants learned by neural networks. The symmetry and additivity in Landau integrability is known in the\\r\\nmachine learning literature as physical inductive biases, which can be elegantly handled by adding constraints to the\\r\\narchitectures or loss functions [6, 7]. Finally, the symbolic front of AI Poincare\\xcc\\x81 2.0 addresses the problem of finding\\r\\nconserved quantities with simple symbolic formulas.\\r\\nTo gain more intuition about the difference between local and global conserved quantities in the sense of Frobenius\\r\\nand general integrability respectively, we visualize conserved quantities for two simple yet illustrative examples. The\\r\\nmessage we want to deliver here is that local conserved quantities may be ill-behaved globally, thus just being more\\r\\nof a mathematical curiosity than something physically useful.\\r\\n1D damped harmonic oscillator is described by the equation\\r\\n\\x12 \\x13 \\x12\\r\\n\\x13\\r\\nd x\\r\\np\\r\\n=\\r\\n(C1)\\r\\n\\xe2\\x88\\x92x \\xe2\\x88\\x92 \\xce\\xb3p\\r\\ndt p\\r\\nwhere \\xce\\xb3 is the damping coefficient. In the sense of Frobenius integrability, the system has 1 conserved quantity. We\\r\\nfirst attempt to construct the quantity analytically. The family of solutions is\\r\\n\\x12\\r\\n\\x13 \\x12 \\xe2\\x88\\x92\\xce\\xb3t\\r\\n\\x13\\r\\nx(t)\\r\\ne cos(t + \\xcf\\x95)\\r\\n=\\r\\n, \\xcf\\x95 \\xe2\\x88\\x88 [0, 2\\xcf\\x80)\\r\\n(C2)\\r\\np(t)\\r\\ne\\xe2\\x88\\x92\\xce\\xb3t sin(t + \\xcf\\x95)\\r\\nDefine the complex variable z(t) \\xe2\\x89\\xa1 x(t) + ip(t) = e(\\xe2\\x88\\x92\\xce\\xb3+i)t+i\\xcf\\x95 and its complex conjugate z\\xcc\\x84 = e(\\xe2\\x88\\x92\\xce\\xb3\\xe2\\x88\\x92i)t\\xe2\\x88\\x92i\\xcf\\x95 . Then\\r\\n\\r\\nH \\xe2\\x89\\xa1 z (\\xe2\\x88\\x92\\xce\\xb3\\xe2\\x88\\x92i) /z\\xcc\\x84 (\\xe2\\x88\\x92\\xce\\xb3+i) =\\r\\n\\r\\n\\x10 z \\x11\\xe2\\x88\\x92\\xce\\xb3\\r\\nz\\xcc\\x84\\r\\n\\r\\n(z z\\xcc\\x84)\\xe2\\x88\\x92i = e\\xe2\\x88\\x922i\\xce\\xb3\\xcf\\x95\\r\\n\\r\\n(C3)\\r\\n\\r\\nis a conserved quantity. When \\xce\\xb3 = 0, H \\xe2\\x88\\xbc (z z\\xcc\\x84) = |z|2 = x2 + p2 which is the energy; when \\xce\\xb3 \\xe2\\x86\\x92 \\xe2\\x88\\x9e, H \\xe2\\x88\\xbc (z/z\\xcc\\x84) \\xe2\\x88\\xbc\\r\\ni\\r\\narg(z) = arctan(p/x) which is the polar angle. For visualization purposes, we define H 0 \\xe2\\x89\\xa1 2\\xce\\xb3\\r\\nlnH = \\xce\\xb8 + lnr\\r\\n\\xce\\xb3 , where\\r\\np\\r\\np\\r\\n0\\r\\n2\\r\\n2\\r\\n\\xce\\xb8 = arctan x and r = x + p . We visualize cosH in FIG. 3 for different \\xce\\xb3. The function looks regular for \\xce\\xb3 = 0\\r\\nand large \\xce\\xb3 = 10, while looks ill-behaved for \\xce\\xb3 \\xe2\\x86\\x92 0+ , say \\xce\\xb3 = 0.01 and 0.1.\\r\\n2D anisotropic harmonic oscillator is described by the equation\\r\\n\\xef\\xa3\\xb6 \\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\npx\\r\\nx\\r\\nd \\xef\\xa3\\xacpx \\xef\\xa3\\xb7 \\xef\\xa3\\xac\\xe2\\x88\\x92\\xcf\\x89x2 x\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xad \\xef\\xa3\\xb8=\\xef\\xa3\\xad p \\xef\\xa3\\xb8\\r\\ny\\r\\ndt y\\r\\n\\xe2\\x88\\x92\\xcf\\x89y2 y\\r\\npy\\r\\n\\xef\\xa3\\xab\\r\\n\\r\\n(C4)\\r\\n\\r\\nwhere \\xcf\\x89x and \\xcf\\x89y are angular frequencies along the x and y direction respectively. Two apparent conserved quantities\\r\\nare energies along the x and y direction, i.e., H1 = 21 (p2x + \\xcf\\x89x2 x2 ) and H2 = 21 (p2y + \\xcf\\x89y2 y 2 ). There exists a third\\r\\nconserved quantity in the sense of Frobenius integrability, as we construct below (also in [17]). The family of solutions\\r\\n\\r\\n\\x0c\\n\\n14\\r\\n\\r\\n2\\r\\n\\r\\n( x, y) = (1, 1)\\r\\n\\r\\n( x, y) = (1, 2)\\r\\n\\r\\n( x, y) = (2, 3)\\r\\n\\r\\n( x, y) = (17, 23)\\r\\n\\r\\n( x, y) = (67, 97)\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n( x, y) = (1, 1)\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n( x, y) = (1, 2)\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n4\\r\\n\\r\\n1\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n( x, y) = (2, 3)\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n( x, y) = (17, 23)\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n6\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n5\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n4\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n3\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n2\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n1\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n4\\r\\n\\r\\n1\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\n( x, y) = (67, 97)\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n6\\r\\n\\r\\n0\\r\\n\\r\\n0\\r\\n\\r\\n1\\r\\n\\r\\n2\\r\\n\\r\\n3\\r\\n\\r\\n1\\r\\n\\r\\n4\\r\\n\\r\\n5\\r\\n\\r\\n6\\r\\n\\r\\nFIG. 4: The conserved quantity of the 2D harmonic oscillator with different frequency pairs (\\xcf\\x89x , \\xcf\\x89y ). Top: ground\\r\\nTruth; bottom: learned results by neural networks. A neural network can only learn this conserved quantity if the\\r\\nfrequency ratio q \\xe2\\x89\\xa1 \\xcf\\x89x /\\xcf\\x89y is a ratio of small integers; if q is irrational, the conserved quantity is an everywhere\\r\\ndiscontinuous function that is completely useless to physicists.\\r\\nis\\r\\n\\xef\\xa3\\xb6 \\xef\\xa3\\xab\\r\\n\\xef\\xa3\\xb6\\r\\nx\\r\\nAx cos(\\xcf\\x89x t + \\xcf\\x95x )\\r\\n\\xef\\xa3\\xacpx \\xef\\xa3\\xb7 \\xef\\xa3\\xac\\xe2\\x88\\x92\\xcf\\x89x Ax sin(\\xcf\\x89x t + \\xcf\\x95x )\\xef\\xa3\\xb7\\r\\n\\xef\\xa3\\xad y \\xef\\xa3\\xb8 = \\xef\\xa3\\xad A cos(\\xcf\\x89 t + \\xcf\\x95 ) \\xef\\xa3\\xb8\\r\\ny\\r\\ny\\r\\ny\\r\\npy\\r\\n\\xe2\\x88\\x92\\xcf\\x89y Ay sin(\\xcf\\x89y t + \\xcf\\x95y )\\r\\n\\xef\\xa3\\xab\\r\\n\\r\\nWe define z1 \\xe2\\x89\\xa1\\r\\n\\r\\n1\\r\\nAx (x\\r\\n\\r\\n+ i \\xcf\\x89pxx ) = ei(\\xcf\\x89x t+\\xcf\\x95x ) , and z2 \\xe2\\x89\\xa1\\r\\n\\r\\n1\\r\\nAy (y\\r\\n\\r\\n(C5)\\r\\n\\r\\np\\r\\n\\r\\n+ i \\xcf\\x89yy ) = ei(\\xcf\\x89y t+\\xcf\\x95y ) . Hence\\r\\n\\r\\n\\xcf\\x89\\r\\n\\r\\nH3 \\xe2\\x89\\xa1 z1 y /z2\\xcf\\x89x = ei(\\xcf\\x89y \\xcf\\x95x \\xe2\\x88\\x92\\xcf\\x89x \\xcf\\x95y )\\r\\n\\r\\n(C6)\\r\\n\\r\\nis a conserved quantity. In the isotropic case when \\xcf\\x89x = \\xcf\\x89y = \\xcf\\x89, H3 simplifies to\\r\\nH3 = (\\xcf\\x89 2 xy + px py + i\\xcf\\x89(xpy \\xe2\\x88\\x92 ypx ))/H2\\r\\n\\r\\n(C7)\\r\\n\\r\\nwhose imaginary part is the well-known angular momentum. Since the norm of H3 is 1, the real and imaginary part\\r\\nare not independent. We plot \\xe2\\x88\\x92ilnH3 in FIG. 4 with different (\\xcf\\x89x , \\xcf\\x89y ). We set Ax = Ay = 1. In the cases when \\xcf\\x89y /\\xcf\\x89x\\r\\nis an integer or simple fractional number, H3 is regular; however when \\xcf\\x89y /\\xcf\\x89x is a complicated fractional number, H3\\r\\nis ill-behaved, demonstrating fractal behavior.\\r\\nNeural networks cannot learn ill-behaved conserved quantities well Neural networks have an implicit bias\\r\\ntowards smooth functions, so they are unable to learn ill-behaved conserved quantities. To verify the argument, we\\r\\nrun AI Poincare\\xcc\\x81 2.0 (only n = 1 model, hence no regularization) on the 1D damped harmonic oscillator with different\\r\\ndamping coefficient \\xce\\xb3, and plot `1 as a function of \\xce\\xb3 in FIG. 5. We found that: (1) the conservation loss `1 is almost\\r\\nvanishing at small \\xce\\xb3 = 0.01 and large \\xce\\xb3 = 100; (2) `1 peaks around \\xce\\xb3 = 1, which agrees with the visualization in\\r\\nFIG. 3. We visualize functions learned by neural p\\r\\nnetworks in FIG. 6 top row, each column displaying results of a\\r\\nspecific \\xce\\xb3. We also compare the function with r = x2 + p2 and x in the bottom row. When \\xce\\xb3 = 0.01, the conserved\\r\\nquantity is equivalent to r up to a nonlinear re-parameterization; When \\xce\\xb3 = 100, the conserved quantity is equivalent\\r\\nto x up to a nonlinear re-parameterization. We also run AI Poincare\\xcc\\x81 2.0 (n = 4 models are trained) on the 2D\\r\\nharmonic oscillator example with different frequency ratios \\xcf\\x89y /\\xcf\\x89x . In FIG. ??, we visualize the worst conserved\\r\\nquantity, i.e., the one with the highest conservation loss, out of 4 neural networks. To map the four-dimensional\\r\\nfunction to a 2D plot, we constrain x = cos\\xcf\\x951 , px = sin\\xcf\\x951 , y = cos\\xcf\\x952 , py = sin\\xcf\\x952 . When (\\xcf\\x89x , \\xcf\\x89y ) = (1, 1) or (1, 2),\\r\\nthe neural network prediction of the third conserved quantity aligns well with our expectation (visualized in FIG. 4).\\r\\nFor more complicated \\xcf\\x89y /\\xcf\\x89x ratios, the prediction looks similar to the (\\xcf\\x89x , \\xcf\\x89y ) = (1, 1) case, but they have high\\r\\nconservation loss, as shown in TABLE III.\\r\\n\\r\\n\\x0c\\n\\n15\\r\\n(\\xcf\\x89x , \\xcf\\x89y )\\r\\n(1, 1)\\r\\n(1, 2)\\r\\n(2, 3)\\r\\n(17, 23)\\r\\n(67, 97)\\r\\nWorst conservation loss 1.1 \\xc3\\x97 10\\xe2\\x88\\x924 5.1 \\xc3\\x97 10\\xe2\\x88\\x924 7.9 \\xc3\\x97 10\\xe2\\x88\\x924 1.2 \\xc3\\x97 10\\xe2\\x88\\x923 1.4 \\xc3\\x97 10\\xe2\\x88\\x923\\r\\nAverage conservation loss 7.7 \\xc3\\x97 10\\xe2\\x88\\x925 4.6 \\xc3\\x97 10\\xe2\\x88\\x924 4.7 \\xc3\\x97 10\\xe2\\x88\\x924 1.0 \\xc3\\x97 10\\xe2\\x88\\x923 1.1 \\xc3\\x97 10\\xe2\\x88\\x923\\r\\n\\r\\n1\\r\\n\\r\\nTABLE III: 2D harmonic oscillator: worst and average conservation loss for different ratios \\xcf\\x89y /\\xcf\\x89x .\\r\\n\\r\\n10\\r\\n\\r\\n3\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n5\\r\\n\\r\\n10\\r\\n\\r\\n6\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n1\\r\\n\\r\\n100\\r\\n\\r\\n101\\r\\n\\r\\n102\\r\\n\\r\\nFIG. 5: 1D damped harmonic oscillator: conservation loss `1 as a function of \\xce\\xb3.\\r\\n\\r\\nAppendix D: phase transitions and how to choose \\xce\\xbb\\r\\n\\r\\nIn Eq. (3), a hyperparameter is the regularization coefficient \\xce\\xbb. If \\xce\\xbb is too small, then multiple networks may learn\\r\\ndependent conserved quantities. If \\xce\\xbb is too large, then the regularization loss dominates the conservation loss, making\\r\\nthe conservation laws inaccurate. As we argue below, the proper choice of \\xce\\xbb has a lower bound which is determined\\r\\nby the tolerance of approximation error \\x0f, and an upper bound O(1).\\r\\nWe first use two analytical toy examples to provide some insights. In both cases, the number of neural networks\\r\\nn is equal to the dimension of the problem s, just to demonstrate all possible phase transitions. In practice, it is\\r\\nsufficient to choose n = s \\xe2\\x88\\x92 1. The geometric intuition of minimizing the loss function Eq. (3) is that: `1 encourages\\r\\n\\r\\nFIG. 6: 1D damped harmonic oscillator: Each column corresponds to a damping coefficient \\xce\\xb3. Top: visualizations of\\r\\nneural network predictions of the conserved quantity. Bottom:\\r\\nComparison of neural network predictions with x and\\r\\np\\r\\n2\\r\\nr = x + p2 .\\r\\n\\r\\n\\x0c\\n\\n16\\r\\n\\r\\nFIG. 7: 2D Toy example: With different \\xce\\xbb, the global minima may have different geometric configurations. Assume\\r\\nthe single conserved quantity can be approximated by a neural network with error \\x0f.\\r\\n\\r\\n\\xe2\\x88\\x87Hi to be orthogonal to f , while the regularization loss `2 encourages \\xe2\\x88\\x87Hi and \\xe2\\x88\\x87Hj (j 6= i) to be orthogonal.\\r\\nToy example 1: The first toy example is inspired by the 1D damped harmonic oscillator. We consider a 2D phase\\r\\nspace. There is only one conserved quantity in the sense of Frobenius integrability, and the approximation error of\\r\\na neural network is \\x0f. We train 2 networks to learn the conserved quantities. At the global minima, two possible\\r\\ngeometric configurations (gradients of neural conserved quantities) are shown in FIG. 7. It is easy to check that any\\r\\nother configuration has higher loss than at least one of the two configurations. Which configuration has lower loss\\r\\ndepends on \\xce\\xbb: when \\xce\\xbb < 1\\xe2\\x88\\x92\\x0f\\r\\n2 , two networks represent the same function (i.e., the only conserved quantity); when\\r\\n,\\r\\ntwo\\r\\nnetworks\\r\\nrepresent\\r\\ntwo independent functions, one of which is not a conserved quantity even in the\\r\\n\\xce\\xbb > 1\\xe2\\x88\\x92\\x0f\\r\\n2\\r\\nsense of Frobenius integrability. Since only the first phase is desirable, we need to set \\xce\\xbb < 1\\xe2\\x88\\x92\\x0f\\r\\n2 . The condition can be\\r\\neasily satisfied if we assume \\x0f \\x1c 1.\\r\\nToy example 2: The second toy example is inspired by the 2D anisotropic harmonic oscillator. To better visualize\\r\\nthe example, we consider a 3D phase space (rather than 4D), but the intrinsic nature of the problem does not change.\\r\\nThere are two conserved quantities in the sense of Frobenius integrability. One conserved quanitity is easy for neural\\r\\nnetworks to fit, hence the approximation error can be minimized to zero; another conserved quantity is hard, so a\\r\\nneural network can at best approximate the function up to error \\x0f. Similar to the analysis above, there are three\\r\\npossible configurations corresponding to the global minima. We train three neural networks to learn the conserved\\r\\nquantities. When \\xce\\xbb < 2\\x0f , three models represent only one conserved quantity (the easy one); when 2\\x0f < \\xce\\xbb < 1,\\r\\nthree models represent two independent conserved quantities (both the easy and the hard one); when \\xce\\xbb > 1, a third\\r\\nfalse conserved quantity is learned. Both the first phase and the second phase are acceptable, depending on different\\r\\nnotions of integrability, since a hard conserved quantity may be locally well-behaved but globally ill-behaved. If we\\r\\nsearch for global conserved quantities, the first phase is desired. However, if we allow local conserved quantities, the\\r\\nsecond phase is desired. All the experiments in the main text are conducted with \\xce\\xbb = 0.02, which is equivalent to\\r\\nsaying we only care about conserved quantities whose approximation errors are less than 0.02 \\xc3\\x97 c, c = 2 in the current\\r\\ntoy example, but we expect c \\xe2\\x88\\xbc O(1) in general.\\r\\nBased on the analysis of two toy examples above, we can have a basic picture of phase transitions for more\\r\\ncomplicated systems: for n conserved quantities with different difficulty (approximation error \\x0f1 < \\x0f2 < \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 < \\x0fn ), we\\r\\nexpect there are n + 1 phases. At each phase transition, only one conserved quantity is learned or un-learned, and the\\r\\norder of phase transitions depends on the order of \\x0f. From the picture of phase transitions, one not only knows the\\r\\nnumber of conserved quantities, but also knows their difficulty hierarchy. In practice, the phase transition diagram\\r\\nmay not be as clean as these toy examples due to inefficiency of neural network training. We would like to investigate\\r\\nthis more in future works. In the following, we show that the phase transition diagram agrees creditably well with\\r\\nour theory above for the 1D damped harmonic oscillator and 2D harmonic oscillator.\\r\\n1D damped harmonic oscillator Toy example 1 can apply to the 1D damped harmonic oscillator without any\\r\\nmodification. FIG. 9 shows that we find a phase transition of `1 /`2 at \\xce\\xbb \\xe2\\x89\\x88 12 for both \\xce\\xb3 = 0 and \\xce\\xb3 = 1. When \\xce\\xb3 = 1,\\r\\nthe non-zero `1 in the first phase implies the irregularity of the conserved quantity.\\r\\n\\r\\n\\x0c\\n\\n17\\r\\n\\r\\nFIG. 8: 3D Toy example: With different \\xce\\xbb, the global minima may have different geometric configurations. Assume\\r\\nthe first and second conserved quantity can be approximated by a neural network with zero error (easy) and \\x0f > 0\\r\\nerror (hard), respectively.\\r\\n\\r\\n=0\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n6\\r\\n\\r\\n10\\r\\n\\r\\n8\\r\\n\\r\\n1\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n101\\r\\n\\r\\n100\\r\\n\\r\\n1\\r\\n\\r\\n=1\\r\\n\\r\\n100\\r\\n\\r\\nLoss\\r\\n\\r\\nLoss\\r\\n\\r\\n100\\r\\n\\r\\n102\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n6\\r\\n\\r\\n10\\r\\n\\r\\n8\\r\\n\\r\\n10\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n101\\r\\n\\r\\n100\\r\\n\\r\\n1\\r\\n\\r\\n102\\r\\n\\r\\nFIG. 9: 1D damped harmonic oscillator: `1 /`2 as functions of \\xce\\xbb demonstrate phase transition behavior.\\r\\n\\r\\n2D harmonic oscillator Toy example 2 is a good abstract of the 2D harmonic oscillator, but should not be\\r\\nconsidered to be exact in the quantitative sense. The two energies are easy conserved quantities, while the third\\r\\nconserved quantity regarding phases are harder to learn due to its irregularity when \\xcf\\x89y /\\xcf\\x89x is not a fractional number.\\r\\nFIG.\\r\\n\\xe2\\x88\\x9a 10 shows that: when (\\xcf\\x89x , \\xcf\\x89y ) = (1, 1), only one clear phase transition happens around \\xce\\xbb = 1. When (\\xcf\\x89x , \\xcf\\x89y ) =\\r\\n(1, 2), two phase transitions are present, one around \\xce\\xbb = 1, another around 10\\xe2\\x88\\x923 < \\xce\\xbb < 10\\xe2\\x88\\x922 .\\r\\n\\r\\n( x, y) = (1, 1)\\r\\n\\r\\n0.40\\r\\n\\r\\n1\\r\\n2\\r\\n\\r\\n0.35\\r\\n0.30\\r\\n\\r\\n0.35\\r\\n0.30\\r\\n0.25\\r\\n\\r\\nLoss\\r\\n\\r\\nLoss\\r\\n\\r\\n0.25\\r\\n0.20\\r\\n0.15\\r\\n\\r\\n0.20\\r\\n0.15\\r\\n\\r\\n0.10\\r\\n\\r\\n0.10\\r\\n\\r\\n0.05\\r\\n\\r\\n0.05\\r\\n\\r\\n0.00\\r\\n\\r\\n( x, y) = (1, 2)\\r\\n\\r\\n0.40\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n3\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n1\\r\\n\\r\\n100\\r\\n\\r\\n101\\r\\n\\r\\n102\\r\\n\\r\\n103\\r\\n\\r\\n0.00\\r\\n\\r\\n10\\r\\n\\r\\n4\\r\\n\\r\\n10\\r\\n\\r\\n3\\r\\n\\r\\n10\\r\\n\\r\\n2\\r\\n\\r\\n10\\r\\n\\r\\n1\\r\\n\\r\\n100\\r\\n\\r\\n101\\r\\n\\r\\n102\\r\\n\\r\\n103\\r\\n\\r\\nFIG. 10: 2D isotropic/anisotropic harmonic oscillator: `1 /`2 as functions of \\xce\\xbb demonstrate phase transition behavior.\\r\\n\\r\\n\\x0c']\n"
     ]
    }
   ],
   "source": [
    "#Encoding all the pdf text in UTF-8\n",
    "elements=[]\n",
    "for i in array_pdf_text:\n",
    "    elements.append(i.encode(\"utf-8\"))\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a4129a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bao_Dong_Wei',\n",
       "  'Chen_Bentley_Mori',\n",
       "  'Fujiwara_Rueckert',\n",
       "  'Cosmo_Kazi_Ahmadi',\n",
       "  'Navab_Bronstein',\n",
       "  'Devlin_Chang',\n",
       "  'Lee_Toutanova',\n",
       "  'Bert_Pre',\n",
       "  'Doran_Solorio',\n",
       "  'Hu_Liu',\n",
       "  'Hu_Dong_Wang_Chang',\n",
       "  'Shen_Lehman',\n",
       "  'Feng_Ghassemi_Nmoody',\n",
       "  'Anthony_Celi_Mark',\n",
       "  'Kazi_Farghadani_Navab',\n",
       "  'Kazi',\n",
       "  'Kortuem_Albarqouni',\n",
       "  'Ba',\n",
       "  'Li_Rao_Solares',\n",
       "  'Ramakrishnan_Canoy',\n",
       "  'Lu_Jiang_Fang_Shi',\n",
       "  'Barkhof_Fox',\n",
       "  'Klein_Alexander',\n",
       "  'Kim_Zhang_Goldenberg',\n",
       "  'Nassir_Navab',\n",
       "  'Ouyang_Biffi_Chen_Kart',\n",
       "  'Qiu_Rueckert',\n",
       "  'Parisot_Ktena',\n",
       "  'Ferrante_Lee_Guerrero_Glocker_Rueckert_Nd',\n",
       "  'Xe_X_Disease',\n",
       "  'Pathak_Krahenbuhl',\n",
       "  'Darrell_Efros',\n",
       "  'Badawi',\n",
       "  'Rasmy_Xiang_Xie_Tao_Zhi',\n",
       "  'Rong_Bian',\n",
       "  'Wei_Huang_Huang',\n",
       "  'Grover_Selfsupervised',\n",
       "  'Shang_Ma_Xiao_Sun',\n",
       "  'Wang_Mcdermott',\n",
       "  'Ghassemi_Hughes',\n",
       "  'Ying_Cai_Luo_Zheng_Ke',\n",
       "  'Shen_Liu',\n",
       "  'Zebin_Rezvy_Chaussalet',\n",
       "  'Zhang_Zhang_Xia_Sun',\n",
       "  'Graph'],\n",
       " ['Carl_Doersch',\n",
       "  'Abhinav_Gupta_Alexei_Efros',\n",
       "  'Alexander_Kolesnikov',\n",
       "  'Zhai_Thomas',\n",
       "  'Neil_Houlsby',\n",
       "  'Martin_Riedmiller',\n",
       "  'Thomas_Brox',\n",
       "  'Tpami_Stas_Goferman',\n",
       "  'Tal_Ncontext',\n",
       "  'Tpami_Ian_Goodfellow_Jean_Pouget',\n",
       "  'Abadie_Mehdi_Mirza_Bing_Nxu',\n",
       "  'David_Warde',\n",
       "  'Aaron_Courville',\n",
       "  'Nyoshua_Bengio',\n",
       "  'Wu_Saining',\n",
       "  'Ross_Ngirshick',\n",
       "  'Jian_Sun_Ndeep',\n",
       "  'Daniel_Zoran',\n",
       "  'Dilip_Krishnan',\n",
       "  'Edward_Nadelson',\n",
       "  'Durand_Antonio_Torralba',\n",
       "  'Nmichael_Maire',\n",
       "  'Guanbin_Li',\n",
       "  'Yizhou_Yu',\n",
       "  'Nianyi_Li_Bilin',\n",
       "  'Jingyi_Yu',\n",
       "  'Tsung_Yi_Lin',\n",
       "  'Jiang_Jiang',\n",
       "  'Liu_Qibin_Hou_Ming_Ming',\n",
       "  'Jiashi_Feng',\n",
       "  'Jianmin_Jiang',\n",
       "  'Tie_Liu',\n",
       "  'Tang_Nheung_Yeung_Shum',\n",
       "  'Frank_Hutter',\n",
       "  'Relja_Arandjelovic',\n",
       "  'Andrew_Zisserman',\n",
       "  'Pablo_Arbela',\n",
       "  'Jonathan_Barron_Ferran',\n",
       "  'Jitendra_Malik',\n",
       "  'Yuki_Markus_Asano',\n",
       "  'Andrea_Nvedaldi',\n",
       "  'Iclr_Hangbo_Bao_Li_Dong',\n",
       "  'Piao_Furu_Wei',\n",
       "  'Barron_Ben_Poole',\n",
       "  'Paolo_Favaro',\n",
       "  'Kirillov_Sergey_Zagoruyko',\n",
       "  'Joulin_Nmatthijs',\n",
       "  'Je',\n",
       "  'Liang_Chieh',\n",
       "  'Chen_Yukun',\n",
       "  'Zhu_George_Papandreou',\n",
       "  'Encoder',\n",
       "  'Chen_Thierry_Artie',\n",
       "  'Ludovic_Denoyer',\n",
       "  'Bowen_Cheng',\n",
       "  'Alex_Schwe_Alexander_Kirillov',\n",
       "  'Cheng_Niloy',\n",
       "  'Xiaolei_Huang_Philip_Nh',\n",
       "  'Torr_Shi_Min_Hu',\n",
       "  'Vladimir_Vapnik',\n",
       "  'Machine_Learning',\n",
       "  'Jia_Deng',\n",
       "  'Wei_Dong',\n",
       "  'Richard_Socher',\n",
       "  'Li_Jia_Li_Kai_Li',\n",
       "  'Li_Fei_Fei',\n",
       "  'Yuanyuan_Ding_Jing',\n",
       "  'Parmar_Jakob',\n",
       "  'Aidan_Gomez',\n",
       "  'Andrey_Voynov',\n",
       "  'Babenko',\n",
       "  'Babenko_Nobject',\n",
       "  'Lijun_Wang',\n",
       "  'Lu_Yifan',\n",
       "  'Mengyang_Feng',\n",
       "  'Wang_Baocai_Yin_Xiang_Ruan',\n",
       "  'Lai_Huazhu_Fu_Jianbing',\n",
       "  'Nhaibin_Ling_Ruigang_Yang',\n",
       "  'Tpami_Yangtao',\n",
       "  'Wang_Xi',\n",
       "  'Shell_Xu_Hu',\n",
       "  'Dominique_Vaufreydaz',\n",
       "  'Yichen_Wei_Fang_Wen_Wangjiang_Zhu',\n",
       "  'Jian_Sun_Ngeodesic',\n",
       "  'Andrew_Nfitzgibbon',\n",
       "  'Lazebnik_Pietro',\n",
       "  'Cordelia_Schmid',\n",
       "  'Eccv_Zhirong',\n",
       "  'Wu_Yuanjun',\n",
       "  'Stella_Yu_Dahua_Lin_Nunsupervised',\n",
       "  'Jianxiong_Xiao',\n",
       "  'Antonio_Torralba',\n",
       "  'Abbey_Zoo',\n",
       "  'Shuai_Xie_Zunlei_Feng',\n",
       "  'Chen_Songtao_Sun',\n",
       "  'Chao_Ma',\n",
       "  'Ming_Li_Song',\n",
       "  'Chen_Saining_Xie',\n",
       "  'Qiong_Yan_Li',\n",
       "  'Chuan_Yang',\n",
       "  'Lihe_Zhang',\n",
       "  'Xiang_Ruan',\n",
       "  'Hsuan_Yang',\n",
       "  'Li_Yuan',\n",
       "  'Chen_Tao_Wang',\n",
       "  'Weihao_Yu',\n",
       "  'Yujun_Shi',\n",
       "  'Jiang_Francis',\n",
       "  'Tay_Jiashi',\n",
       "  'Feng_Shuicheng_Nyan',\n",
       "  'Xiaohang_Zhan_Ziwei',\n",
       "  'Liu_Ping_Luo_Xiaoou',\n",
       "  'Tang_Nchen_Loy',\n",
       "  'Mix',\n",
       "  'Dingwen_Zhang',\n",
       "  'Yu_Zhang',\n",
       "  'Jianming_Zhang',\n",
       "  'Stan_Sclaroff',\n",
       "  'Zhe_Lin_Xiaohui',\n",
       "  'Mech',\n",
       "  'James_Thewlis_Andrea_Vedaldi_Ncross',\n",
       "  'Claudio_Cifarelli',\n",
       "  'Gabriela_Csurka',\n",
       "  'Luke_Melas_Kyriazi',\n",
       "  'Laina_Nandrea_Vedaldi',\n",
       "  'Fully',\n",
       "  'Shahbaz_Khan_Ming_Hsuan_Nyang',\n",
       "  'Tam_Nguyen',\n",
       "  'Maximilian_Dax_Chaithanya',\n",
       "  'Nguyen_Zhongyu_Lou',\n",
       "  'Myriam_Tami',\n",
       "  'Yael_Pritch_Nalexander_Hornung',\n",
       "  'Michael_Rubinstein',\n",
       "  'Ariel_Shamir',\n",
       "  'Shai_Avidan',\n",
       "  'Yan_Li_Xu_Jiaya_Jia',\n",
       "  'Gilles_Puy_Huy',\n",
       "  'Simon_Roburin',\n",
       "  'Bursuc_Patrick',\n",
       "  'Jean_Ponce',\n",
       "  'Haibin_Ling_Benjamin',\n",
       "  'Bederson_Ndavid',\n",
       "  'Sun_Haibin_Ling',\n",
       "  'Cord_Matthijs',\n",
       "  'Douze_Francisco',\n",
       "  'Sablayrolles_Herve_Jegou',\n",
       "  'Jing_Zhang_Tong_Zhang',\n",
       "  'Yuchao_Dai',\n",
       "  'Richard_Hartley',\n",
       "  'Jia_Xing_Zhao',\n",
       "  'Jiang_Jiang_Liu',\n",
       "  'Deng_Ping',\n",
       "  'Wangjiang_Zhu',\n",
       "  'Shuang_Liang_Yichen',\n",
       "  'Xd_Hs',\n",
       "  'Xce_Xacnn',\n",
       "  'Tt',\n",
       "  'Xce_Xatk_Linear',\n",
       "  'Xce_Xate_Na',\n",
       "  'Xce_Xate',\n",
       "  'Xe_Xa_Xce',\n",
       "  'Xe_Rh_Xc',\n",
       "  'Lu',\n",
       "  'Yi_Xe',\n",
       "  'Ck',\n",
       "  'Nzi_Xce',\n",
       "  'Nmocov_Nmocov',\n",
       "  'Nself',\n",
       "  'Xce_Xb',\n",
       "  'Xe_Xa_Maximal',\n",
       "  'Xce_Xb_Namong',\n",
       "  'Xe_Xa_Intersection',\n",
       "  'Xe_Xa',\n",
       "  'Accuracy_Acc',\n",
       "  'Tab',\n",
       "  'Fig'],\n",
       " ['Yaron_Lipman',\n",
       "  'Tzu_Mao',\n",
       "  'Xado_Durand',\n",
       "  'Bear',\n",
       "  'Ed',\n",
       "  'Peter_Sturm',\n",
       "  'Chen',\n",
       "  'Minye_Wu_Yingliang_Zhang_Nianyi',\n",
       "  'Jie_Lu',\n",
       "  'Shenghua_Gao_Jingyi_Nyu',\n",
       "  'Chen_Kai',\n",
       "  'Yee_Wong',\n",
       "  'Tom',\n",
       "  'Chen_Michael_Goesele_Seidel',\n",
       "  'Ieee_Xe',\n",
       "  'Chen_Hao_Zhang',\n",
       "  'Learning_Implicit_Fields',\n",
       "  'Shape_Nmodeling',\n",
       "  'Nyung_Yu',\n",
       "  'Douglas_Zongker',\n",
       "  'Joel_Hindorff_Brian_Curless',\n",
       "  'Proceedings',\n",
       "  'Ndavid_Cohen',\n",
       "  'Steiner_Pierre_Alliez_Mathieu_Desbrun',\n",
       "  'Weischedel_Max_Wardetzky',\n",
       "  'Nzhaopeng_Cui_Jinwei_Gu_Boxin_Shi_Ping',\n",
       "  'Jan_Kautz',\n",
       "  'Jianfei_Cai_Jianmin_Zheng',\n",
       "  'Phuoc_Huynh',\n",
       "  'Robles_Kelly',\n",
       "  'Edwin_Hancock',\n",
       "  'Shape',\n",
       "  'Magnor_Wolfgang_Heidrich',\n",
       "  'Wiley_Online_Library',\n",
       "  'Adam_Method_Stochastic_Optimization',\n",
       "  'Steven_Seitz',\n",
       "  'Eron_Steger',\n",
       "  'Laine_Janne_Hellsten',\n",
       "  'Lehtinen_Timo_Naila',\n",
       "  'Dani_Lischinski_Yair_Weiss',\n",
       "  'Mao_Li',\n",
       "  'Xado',\n",
       "  'Jaakko_Lehtinen',\n",
       "  'Carlo_Ray',\n",
       "  'Lehtinen_Ravi_Ramamoorthi_Wenzel',\n",
       "  'Jakob_Fr',\n",
       "  'Metropolis',\n",
       "  'Li_Yu',\n",
       "  'Yeh_Manmohan_Chandraker',\n",
       "  'Nshichen_Liu',\n",
       "  'Tianye_Li',\n",
       "  'Chen_Hao_Li',\n",
       "  'Soft',\n",
       "  'Michael_Black',\n",
       "  'Frank_Hutter',\n",
       "  'Sgdr_Stochastic',\n",
       "  'Nfujun_Luan_Shuang_Zhao_Kavita_Bala_Ioannis_Gkioulekas',\n",
       "  'Langevin_Monte',\n",
       "  'Njiahui_Lyu',\n",
       "  'Wu_Dani_Lischinski',\n",
       "  'Daniel_Cohen',\n",
       "  'Hui_Huang',\n",
       "  'Michael_Oechsle',\n",
       "  'Michael_Niemeyer',\n",
       "  'Sebastian_Nowozin_Andreas_Ngeiger',\n",
       "  'Mildenhall',\n",
       "  'Springer_Ndaisuke',\n",
       "  'Katsushi_Ikeuchi',\n",
       "  'Morris_Kiriakos_Kutulakos',\n",
       "  'Nicolet_Alec_Jacobson_Wenzel_Jakob',\n",
       "  'David_Delio',\n",
       "  'Vicini_Tizian_Zeltner_Wenzel_Jakob',\n",
       "  'Peng_Andreas_Geiger',\n",
       "  'Unisurf_Unifying',\n",
       "  'Richard_Newcombe_Steven_Lovegrove',\n",
       "  'Sam_Gross',\n",
       "  'Edward_Yang',\n",
       "  'Ndevito_Zeming',\n",
       "  'Philip_Dutr',\n",
       "  'Porter_Tom_Duff',\n",
       "  'Yee_Hong_Yang',\n",
       "  'Frequency',\n",
       "  'Qian_Minglun',\n",
       "  'Jeremy_Reizenstein',\n",
       "  'Gordon_Wan_Yen_Lo_Justin',\n",
       "  'Georgia_Gkioxari',\n",
       "  'Deep_Learning',\n",
       "  'Nedoardo_Remelli_Artem',\n",
       "  'Lukoianov_Stephan',\n",
       "  'Guillard_Timur_Bagautdinov',\n",
       "  'Pascal_Fua',\n",
       "  'Nshreeyak',\n",
       "  'Matthew_Moore',\n",
       "  'Mike_Pan_Ganesh',\n",
       "  'Johnny_Lee_Andy_Zeng',\n",
       "  'Shuran_Song',\n",
       "  'Efficient_Ransac_Point_Cloud_Shape_Ndetection',\n",
       "  'John_Wiley_Sons_Ltd',\n",
       "  'Li_Jeppe',\n",
       "  'Xe_Nkenichiro',\n",
       "  'Nacm_Trans',\n",
       "  'Graph_Vol',\n",
       "  'Njiamin_Xu_Zihan',\n",
       "  'Zhu_Hujun_Bao_Weiwei',\n",
       "  'Derek_Bradley_Wolfgang_Heidrich',\n",
       "  'Tomographic',\n",
       "  'Nchia_Yin_Tsai_Ashok_Veeraraghavan',\n",
       "  'Aswin_Sankaranarayanan',\n",
       "  'Npeng_Wang_Lingjie',\n",
       "  'Liu_Christian',\n",
       "  'Wetzstein_David',\n",
       "  'Andrew_Fitzgibbon',\n",
       "  'Andrew_Zisserman',\n",
       "  'Wu_Yang',\n",
       "  'Zhou_Yiming',\n",
       "  'Gu_Yoni_Kasten',\n",
       "  'Galun_Matan',\n",
       "  'Atzmon_Ronen',\n",
       "  'Nyaron_Lipman',\n",
       "  'Nyonghao_Yue_Kei',\n",
       "  'Iwasaki_Bing_Yu',\n",
       "  'Chen_Yoshinori_Dobashi_Tomoyuki_Nishita',\n",
       "  'Ncheng_Zhang',\n",
       "  'Bailey_Miller',\n",
       "  'Kan_Yan_Ioannis',\n",
       "  'Shuang_Zhao',\n",
       "  'Wu_Changxi',\n",
       "  'Zheng_Ioannis',\n",
       "  'Nshuang_Zhao',\n",
       "  'Nmingjie_Zhang',\n",
       "  'Xing_Lin_Mohit',\n",
       "  'Gupta_Jinli',\n",
       "  'Suo_Qionghai_Dai',\n",
       "  'Nqian_Yi_Zhou',\n",
       "  'Jaesik_Park_Vladlen_Koltun',\n",
       "  'Ndouglas_Zongker',\n",
       "  'Dawn_Werner_Brian_Curless_David_Salesin'],\n",
       " ['Alhaija',\n",
       "  'Mustikovela',\n",
       "  'Geiger_Rother',\n",
       "  'Cao_Xu',\n",
       "  'Lin_Wei_Hu',\n",
       "  'Chen',\n",
       "  'Papandreou_Kokkinos_Murphy_Yuille',\n",
       "  'Papandreou_Schroff_Adam',\n",
       "  'Zhu_Papandreou_Schroff_Adam',\n",
       "  'Encoder',\n",
       "  'Chen_Zhu_Sun',\n",
       "  'Li_Shen_Yu',\n",
       "  'Cheng_Misra_Schwing',\n",
       "  'Kirillov_Girdhar',\n",
       "  'Cheng_Schwing',\n",
       "  'Ramos_Rehfeld',\n",
       "  'Benenson_Nfranke_Roth_Schiele',\n",
       "  'Ding_Jiang_Shuai_Liu',\n",
       "  'Ding_Guo_Ding_Han',\n",
       "  'Dosovitskiy_Beyer',\n",
       "  'Zhai_Unterthiner_Nt',\n",
       "  'Dehghani_Minderer_Heigold',\n",
       "  'Feng_Haase_Schu',\n",
       "  'Fu_Liu',\n",
       "  'Tian_Li_Bao_Fang_Lu',\n",
       "  'Xe_Xc',\n",
       "  'Harders_Szekely',\n",
       "  'Deng_Qiao',\n",
       "  'He_Zhang_Ren_Sun',\n",
       "  'Huang_Wang_Wu_Tang',\n",
       "  'Huang_Wang_Huang_Huang_Wei_Liu',\n",
       "  'Jin_Gong_Yu',\n",
       "  'Chu_Wang_Wang_Shao',\n",
       "  'Jin_Liu_Chu_Yu',\n",
       "  'Kirillov_Girshick',\n",
       "  'Dolla',\n",
       "  'Li_Yang_Zhao',\n",
       "  'Lin_Liu',\n",
       "  'Spatial',\n",
       "  'Li_Zhong',\n",
       "  'Wu_Yang_Lin_Liu',\n",
       "  'Lin',\n",
       "  'Liu_Lin_Cao_Hu',\n",
       "  'Wei_Zhang',\n",
       "  'Lin_Guo',\n",
       "  'Swin',\n",
       "  'Long_Shelhamer_Darrell',\n",
       "  'Bochkovskiy_Koltun',\n",
       "  'Garcia_Laptev_Schmid',\n",
       "  'Xe_October',\n",
       "  'Parmar_Uszkoreit_Jones_Gomez',\n",
       "  'Wu_Lu',\n",
       "  'Zhu_Zhang',\n",
       "  'Wu_Ma_Guo',\n",
       "  'Ginet_Graph',\n",
       "  'Xiao_Liu',\n",
       "  'Zhou_Jiang_Sun',\n",
       "  'Xie_Wang_Yu_Anandkumar_Alvarez',\n",
       "  'Luo',\n",
       "  'Yin_Yao_Cao_Li',\n",
       "  'Lin_Hu',\n",
       "  'Xe_Nspringer',\n",
       "  'Yuan_Chen_Wang',\n",
       "  'Zhang_Zhang_Tang',\n",
       "  'Wang_Hua_Sun',\n",
       "  'Zhang_Dana',\n",
       "  'Shi_Zhang',\n",
       "  'Wang_Tyagi',\n",
       "  'Zhang_Pang_Chen_Loy',\n",
       "  'Zhao_Shi_Qi',\n",
       "  'Zheng_Lu',\n",
       "  'Zhao_Zhu_Luo_Wang_Fu',\n",
       "  'Feng_Xiang_Nt_Torr,_Zhang',\n",
       "  'Zhou_Zhao_Puig_Xiao_Fidler_Barriuso_Torralba',\n",
       "  'Zhu_Su_Lu',\n",
       "  'Li_Wang_Dai',\n",
       "  'Zhu_Xu_Bai_Huang_Bai'],\n",
       " ['Ai_Poincare',\n",
       "  'Henry_Poincare',\n",
       "  'Manifold_Nlearning',\n",
       "  'Ode',\n",
       "  'R_Xe_R',\n",
       "  'Xe_Xa',\n",
       "  'Nkepler',\n",
       "  'Xce_Xb_Xce',\n",
       "  'Xce_Xb_Xe_Xa',\n",
       "  'Xce_Xb_Xce_Xb_Xe_Xa',\n",
       "  'Xe_Xh',\n",
       "  'Xf_Nto',\n",
       "  'Xce_Xbb',\n",
       "  'Xbi_Xce',\n",
       "  'Xbj_Xce_Xbi_Xce',\n",
       "  'Xce',\n",
       "  'Xef_Xa_Xab',\n",
       "  'Xef_Xa_Xb',\n",
       "  'Xa_Xac',\n",
       "  'Xa_Xad',\n",
       "  'Poincare',\n",
       "  'Xef_Xa_Xach',\n",
       "  'Xef_Xa_Xb_Xef_Xa_Xad',\n",
       "  'Xef_Xa',\n",
       "  'Xe_Xf',\n",
       "  'Appendix',\n",
       "  'Kepler',\n",
       "  'Xe_Xbc_Xe_Computing_Nf',\n",
       "  'Xe_Xe_Xa_Nx',\n",
       "  'Px_Nxq',\n",
       "  'Xcf_Xx',\n",
       "  'Xcf_Xx_Xcf',\n",
       "  'Taylor',\n",
       "  'Appendix_Nintegrals_Motion',\n",
       "  'Xe_Nioms',\n",
       "  'Nlaplace_Runge_Lenz',\n",
       "  'Nioms_Nsomethe',\n",
       "  'Xi_Yi',\n",
       "  'Xe_Xa_Xj',\n",
       "  'Xcf_Xt_Xcf_Xxxx',\n",
       "  'Xe_Xcf_Xcf_Xx',\n",
       "  'Xcf_Xcf',\n",
       "  'Xcf_Xx_Xcf_Xxx',\n",
       "  'Xcf_Xt_Xe_Xcf_Xxx',\n",
       "  'Xce_Xba',\n",
       "  'Bose_Einstein',\n",
       "  'Xcf_Xx_Nbecause',\n",
       "  'Xcf_Xx_Lack',\n",
       "  'Appendix_Ai',\n",
       "  'Bohan_Wang_Di_Luo',\n",
       "  'Liu',\n",
       "  'Tegmark_Machine',\n",
       "  'Wetzel',\n",
       "  'Panju_Nv',\n",
       "  'Jeong_Discovering',\n",
       "  'Lu',\n",
       "  'Perdikaris_Ns',\n",
       "  'Chen',\n",
       "  'Courville_Deep',\n",
       "  'Xe_Xsimpler',\n",
       "  'Tegmark_Ai',\n",
       "  'Feng',\n",
       "  'Wu_Nm',\n",
       "  'Landau',\n",
       "  'Kingma',\n",
       "  'Ba_Adam',\n",
       "  'Xe_Xc',\n",
       "  'Gardner',\n",
       "  'Kruskal_Nkorteweg',\n",
       "  'Dulock',\n",
       "  'Barrett_Title',\n",
       "  'Bulletin_London',\n",
       "  'Frobenius',\n",
       "  'Xe',\n",
       "  'Xacp',\n",
       "  'Xef_Xa_Xb_Nz',\n",
       "  'Npy_Xef_Xa_Xab',\n",
       "  'Sample',\n",
       "  'Xe_Xbc_Xe',\n",
       "  'Xy_Px',\n",
       "  'Py_Nbut',\n",
       "  'Ex_Ey',\n",
       "  'Xef_Xa_Xb_Nx_Nvx',\n",
       "  'Xe_Xx_Xef_Xa_Xb_Xef_Xa_Xad',\n",
       "  'Ax_Ay_Xcf_Xx',\n",
       "  'Xcf_Xe_Xcf',\n",
       "  'Xe_Yl',\n",
       "  'Xef_Xa_Xb_Nvx',\n",
       "  'Xef_Xa_Xb_Xef_Xa',\n",
       "  'Xef_Xa_Xb_Xef_Xa_Xb',\n",
       "  'Xe_Xgm',\n",
       "  'Xe_Nv_Gm',\n",
       "  'Runge_Lenz',\n",
       "  'Xyx',\n",
       "  'Xx',\n",
       "  'Xef_Xa_Xb_Xef_Xa_Xac',\n",
       "  'Nv',\n",
       "  'Xa',\n",
       "  'Xef_Xa_Xac',\n",
       "  'Xe_Xy_Xef_Xa_Xac',\n",
       "  'Xef_Xa_Xb_Nv',\n",
       "  'Xe_Xx',\n",
       "  'Xy',\n",
       "  'Similar_Kepler',\n",
       "  'Xe_Xe_Np',\n",
       "  'Xe_Xe',\n",
       "  'Xcf_Xt_Xcf',\n",
       "  'Xe_Xcf',\n",
       "  'Kdv',\n",
       "  'Xcf_Nn',\n",
       "  'Xcf_Xe_Xcf_Xx',\n",
       "  'Ndespite',\n",
       "  'Xcf_Xcf_Xcf_Xx_Xcf_Xcf_Xx_Xcf_Xxx',\n",
       "  'Xe_Xcf_Xxxxx',\n",
       "  'Xcf_Xx_Xcf_Xcf_Xxx',\n",
       "  'Xcf_Xxx',\n",
       "  'Xcf_Xcf_Xxxx',\n",
       "  'Xef_Xa_Xab_Xcf',\n",
       "  'Xac',\n",
       "  'Xa_Xb',\n",
       "  'Xcf_Xe_Xcf_Xcf_Xcf_Nxxx',\n",
       "  'Gaussian',\n",
       "  'Ai_Xe',\n",
       "  'Xcf_Xcf_Xx',\n",
       "  'Xcf_Xx_Xcf_Xcf',\n",
       "  'Xce_Xba_Xcf',\n",
       "  'Xcf_Xx_Xcf_Xx',\n",
       "  'Xef_Xa_Xb_Xcf_Xcf_Xxx',\n",
       "  'Xce_Xba_Xcf_Xcf',\n",
       "  'Xef_Xa_Xb_Xcf_Xxxx',\n",
       "  'Xcf_Xxxxx_Xe_Xce_Xba',\n",
       "  'Xcf_Xcf_Xxx',\n",
       "  'Xcf_Xr_Xcf_Xx',\n",
       "  'Xcf_Xef',\n",
       "  'Xe_Xcf_Xxx',\n",
       "  'Xce_Xba_Xcf_Xcf_Xi',\n",
       "  'Xce_Xba_Xcf_Xcf_Xr',\n",
       "  'Xce_Xba_Xcf_Xcf_Xcf_Xcf_Xcf_Xcf_Xcf_Xe',\n",
       "  'Ni',\n",
       "  'Xac_Xcf',\n",
       "  'Xef_Xa_Xb_Xcf',\n",
       "  'Xcf_Xce_Xba_Xcf',\n",
       "  'Ni_Nx',\n",
       "  'Xx_Ni_Xx',\n",
       "  'Xef_Xa_Xb_Nx_Nx',\n",
       "  'Xef_Xa_Xb_Xcf_Xcf',\n",
       "  'Xe_Xt_Xcf_Xr',\n",
       "  'Xac_Xcf_Xx_Xef_Xa_Xb_Xef_Xa_Xac',\n",
       "  'Xt_Xcf_Xx',\n",
       "  'Xe_Xt_Xcf_Xx',\n",
       "  'Xe_Xt_Xcf_Xxx',\n",
       "  'Xe_Xa_Kd',\n",
       "  'Eq',\n",
       "  'Xf_Xcf',\n",
       "  'Subspace_Xe_Xa',\n",
       "  'Gram_Schmidt',\n",
       "  'Xfi_Ni',\n",
       "  'Liouville_Landau',\n",
       "  'Poisson',\n",
       "  'Liouville_Arnold',\n",
       "  'Nof_Xe',\n",
       "  'Liouville',\n",
       "  'Ai',\n",
       "  'Xce_Xb',\n",
       "  'Xe_Xchidden',\n",
       "  'Xe_Xce_Xbt',\n",
       "  'Xe_Xce_Xb_Xcf',\n",
       "  'Xce_Xb_Xe_Xbc_Xcc',\n",
       "  'Xe_Xa_Xce',\n",
       "  'Xce_Xb_Arctan',\n",
       "  'Xef_Xa_Xb_Npx_Nx_Nd',\n",
       "  'Xe_Xcf_Xx',\n",
       "  'Px_Xcf_Xx_Py_Xcf_Xy',\n",
       "  'Xcf_Xx_Xcf_Xy',\n",
       "  'Xcf_Xy_Xcf_Xy',\n",
       "  'Xcf_Xy',\n",
       "  'Xcf_Xx_Xcf_Xy_Xcf',\n",
       "  'Py_Xcf',\n",
       "  'Xilnh_Fig',\n",
       "  'Ax_Ay',\n",
       "  'Xcf_Xy_Xcf_Xx_Ni_Integer',\n",
       "  'Xcf_Xy_Xcf_Xx',\n",
       "  'Xcf_Px',\n",
       "  'Xy_Xcf_Xx_Nfig',\n",
       "  'Nin_Eq',\n",
       "  'Bottom_Ncomparison',\n",
       "  'Xe_Xhj',\n",
       "  'Toy',\n",
       "  'Xe_Xce_Xb']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All function in one\n",
    "\n",
    "entities=[]\n",
    "def main():\n",
    "    for text in elements:\n",
    "        temp=after_references(str(text))\n",
    "        temp=preprocess_text(temp)\n",
    "        temp=extract(temp)\n",
    "        entities.append(temp)\n",
    "    return entities\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a35d1365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897bff1",
   "metadata": {},
   "source": [
    "## Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "164621c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready import *\n",
    "\n",
    "onto_path.append(\"owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8470daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Creating new ontology onto <http://test.org/onto.owl>.\n"
     ]
    }
   ],
   "source": [
    "onto = Ontology(\"http://test.org/onto.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5aad8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class References(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class quoted_by(Property):\n",
    "    ontolgy = onto\n",
    "    domain = [References]\n",
    "    range = [Author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd4f26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.sax.saxutils import escape\n",
    "\n",
    "def invalid_xml_remove(char):\n",
    "    \"\"\"Tracks illegal unicode characters\"\"\"\n",
    "    #http://stackoverflow.com/questions/1707890\n",
    "    # /fast-way-to-filter-illegal-xml-unicode-chars-in-python\n",
    "    illegal_unichrs = [ (0x00, 0x08), (0x0B, 0x1F), (0x7F, 0x84), (0x86, 0x9F),\n",
    "                    (0xD800, 0xDFFF), (0xFDD0, 0xFDDF), (0xFFFE, 0xFFFF),\n",
    "                    (0x1FFFE, 0x1FFFF), (0x2FFFE, 0x2FFFF), (0x3FFFE, 0x3FFFF),\n",
    "                    (0x4FFFE, 0x4FFFF), (0x5FFFE, 0x5FFFF), (0x6FFFE, 0x6FFFF),\n",
    "                    (0x7FFFE, 0x7FFFF), (0x8FFFE, 0x8FFFF), (0x9FFFE, 0x9FFFF),\n",
    "                    (0xAFFFE, 0xAFFFF), (0xBFFFE, 0xBFFFF), (0xCFFFE, 0xCFFFF),\n",
    "                    (0xDFFFE, 0xDFFFF), (0xEFFFE, 0xEFFFF), (0xFFFFE, 0xFFFFF),\n",
    "                    (0x10FFFE, 0x10FFFF) ]\n",
    "\n",
    "    illegal_ranges = [f\"{chr(low)}-{chr(high)}\"\n",
    "                  for (low, high) in illegal_unichrs\n",
    "                  if low < sys.maxunicode]\n",
    "\n",
    "    illegal_xml_re = re.compile(f'[{\"\".join(illegal_ranges)}]')\n",
    "    if illegal_xml_re.search(char) is not None:\n",
    "        #Replace with space\n",
    "        return ''\n",
    "    else:\n",
    "        return char\n",
    "\n",
    "def clean_char(char):\n",
    "    \"\"\"\n",
    "    Function for remove invalid XML characters from\n",
    "    incoming data.\n",
    "    \"\"\"\n",
    "    #Get rid of the ctrl characters first.\n",
    "    #http://stackoverflow.com/questions/1833873/python-regex-escape-characters\n",
    "    char = re.sub('\\x1b[^m]*m', '', char)\n",
    "    #Clean up invalid xml\n",
    "    char = invalid_xml_remove(char)\n",
    "    replacements = [\n",
    "        ('\\u201c', '\\\"'),\n",
    "        ('\\u0141', '\\\"'),\n",
    "        ('\\u201d', '\\\"'),\n",
    "        (\"\\u001B\", ''), #http://www.fileformat.info/info/unicode/char/1b/index.htm\n",
    "        (\"\\u0019\", ''), #http://www.fileformat.info/info/unicode/char/19/index.htm\n",
    "        (\"\\u0016\", ''), #http://www.fileformat.info/info/unicode/char/16/index.htm\n",
    "        (\"\\u001C\", ''), #http://www.fileformat.info/info/unicode/char/1c/index.htm\n",
    "        (\"\\u0003\", ''), #http://www.utf8-chartable.de/unicode-utf8-table.pl?utf8=0x\n",
    "        (\"\\u000C\", ''),\n",
    "        (\"\\u03b1\", ''),\n",
    "        (\"u\\u039C\", ''),\n",
    "        (\"\\u03C3\", ''),\n",
    "        (\"\\u0141\", ''),\n",
    "        (\"\\u0308\", ''),\n",
    "        (\"\\u2032\", ''),\n",
    "        (\"\\u03b8\", '')\n",
    "\n",
    "    \n",
    "    ]\n",
    "    for rep, new_char in replacements:\n",
    "        if char == rep:\n",
    "            return new_char\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4794f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_value(text: str):\n",
    "        \"\"\"Escape the illegal characters for an ontology property\"\"\"\n",
    "        if text is None:\n",
    "            return None\n",
    "        # function to escape XML character data\n",
    "        text = escape(text)\n",
    "        text = text.replace('\\n', '')\n",
    "        text = text.replace('\\r', '')\n",
    "        text = text.replace('\\f', '')\n",
    "        text = text.replace('\\b', '')\n",
    "        text = text.replace('\"', '')\n",
    "        text = text.replace('[', '')\n",
    "        text = text.replace(']', '')\n",
    "        text = text.replace('{', '')\n",
    "        text = text.replace('}', '')\n",
    "        text = text.replace('#', '')\n",
    "        text = text.replace('|', '')\n",
    "        text = text.replace(' ', '_')\n",
    "        text = text.replace('σ', '')\n",
    "        text = text.replace('ℓ', '')\n",
    "        text = text.replace('Σ', '')\n",
    "        text = text.replace('ı', '')\n",
    "        text = text.replace('ē', '')\n",
    "        text = text.replace('μ', '')\n",
    "        text = text.replace('Μ', '')\n",
    "        text = text.replace('Ł', '')\n",
    "        text = text.replace('ł', '')\n",
    "        text = text.replace('θ', '')\n",
    "        text = text.replace('φ', '')\n",
    "        text = text.replace('Φ', '')\n",
    "        text = text.replace('̈', '')\n",
    "        text = text.replace('́', '')\n",
    "        text = text.replace('′', '')\n",
    "        text = text.replace('∈', '')\n",
    "        text = text.replace('ć', '')\n",
    "        text = text.replace('ź', '')\n",
    "        text = clean_char(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2126e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Chantal.Pellegrini]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Anees.Kazi]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Nassir.Navab]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Gyungin.Shin]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Samuel.Albanie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Weidi.Xie]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Jiamin.Xu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Zihan.Zhu]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Hujun.Bao]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Wewei.Xu]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Fangjian.Lin]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Zhanhao.Liang]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Junjun.He]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Miao.Zheng]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Shengwei.Tian]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Kai.Chen]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Ziming.Liu]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Varun.Madhavan]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n",
      "[onto.Max.Tegmark]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(array_authors)):\n",
    "    for j in range(len(array_authors[i])):\n",
    "        aut_num = Author(escape_value(array_authors[i][j]))\n",
    "        for z in range(len(entities[i])):\n",
    "            ref_num = References(escape_value(entities[i][z]))\n",
    "            ref_num.quoted_by.append(aut_num)\n",
    "            print(ref_num.quoted_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e27e8710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Saving ontology onto to owl\\onto.owl...\n"
     ]
    }
   ],
   "source": [
    "onto.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472064be",
   "metadata": {},
   "source": [
    "### To conlude, we will change the xml encoding to UTF-8 otherwise, Protégé may not read the file ont.owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e6f08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# iconv -f ISO-8859-1 -t UTF-8 owl/onto.owl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
