{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126a7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdftotext in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: arxiv in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: feedparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from arxiv) (6.0.8)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\admin\\anaconda3\\lib\\site-packages (from feedparser->arxiv) (1.0.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.2.3-cp38-cp38-win_amd64.whl (11.6 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: spacy\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.2.0\n",
      "    Uninstalling spacy-3.2.0:\n",
      "      Successfully uninstalled spacy-3.2.0\n",
      "Successfully installed spacy-3.2.3\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 13:29:31.906404: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-29 13:29:31.906754: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nameparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.5.8)\n",
      "Requirement already satisfied: pdfminer in c:\\users\\admin\\anaconda3\\lib\\site-packages (20191125)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pdfminer) (3.10.4)\n",
      "Requirement already satisfied: refextract in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.1.4)\n",
      "Requirement already satisfied: autosemver>=0.5.3,~=0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from refextract) (0.5.5)\n",
      "Requirement already satisfied: requests>=2.18.4,~=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from refextract) (2.25.1)\n",
      "Requirement already satisfied: six>=1.10.0,~=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from refextract) (1.15.0)\n",
      "Requirement already satisfied: PyPDF2>=1.26.0,~=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from refextract) (1.26.0)\n",
      "Requirement already satisfied: inspire-utils>=3.0.0,~=3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from refextract) (3.0.24)\n",
      "Requirement already satisfied: python-magic>=0.4.15,~=0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from refextract) (0.4.25)\n",
      "Requirement already satisfied: unidecode>=1.0.22,~=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from refextract) (1.3.2)\n",
      "Requirement already satisfied: dulwich<0.20,>=0.19.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from autosemver>=0.5.3,~=0.0->refextract) (0.19.16)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dulwich<0.20,>=0.19.6->autosemver>=0.5.3,~=0.0->refextract) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dulwich<0.20,>=0.19.6->autosemver>=0.5.3,~=0.0->refextract) (1.26.4)\n",
      "Requirement already satisfied: lxml>=4.4.0,~=4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from inspire-utils>=3.0.0,~=3.0->refextract) (4.6.3)\n",
      "Requirement already satisfied: nameparser>=0.5.3,~=0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from inspire-utils>=3.0.0,~=3.0->refextract) (0.5.8)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1,~=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from inspire-utils>=3.0.0,~=3.0->refextract) (2.8.1)\n",
      "Requirement already satisfied: babel>=2.5.1,~=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from inspire-utils>=3.0.0,~=3.0->refextract) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from babel>=2.5.1,~=2.0->inspire-utils>=3.0.0,~=3.0->refextract) (2021.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.18.4,~=2.0->refextract) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.18.4,~=2.0->refextract) (4.0.0)\n",
      "Requirement already satisfied: pdfx in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: pdfminer.six==20201018 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pdfx) (20201018)\n",
      "Requirement already satisfied: chardet==4.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pdfx) (4.0.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pdfminer.six==20201018->pdfx) (3.4.7)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pdfminer.six==20201018->pdfx) (2.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six==20201018->pdfx) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography->pdfminer.six==20201018->pdfx) (2.20)\n",
      "Requirement already satisfied: textblob in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm->nltk>=3.1->textblob) (0.4.4)\n",
      "Requirement already satisfied: owlready in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.21.11)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from boto3) (0.5.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.11 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from boto3) (1.24.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from botocore<1.25.0,>=1.24.11->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from botocore<1.25.0,>=1.24.11->boto3) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.11->boto3) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#!!! Install a conda package in the current Jupyter kernel !!!\n",
    "# !pip install pdftotext\n",
    "# !pip install arxiv\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install nameparser\n",
    "# !pip install pdfminer\n",
    "# !pip install refextract\n",
    "# !pip install pdfx\n",
    "# !pip install -U textblob\n",
    "# !pip install owlready\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2ac646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "* Owlready * Creating new ontology owl <http://www.w3.org/2002/07/owl>.\n",
      "* Owlready * Creating new ontology 22-rdf-syntax-ns <http://www.w3.org/1999/02/22-rdf-syntax-ns>.\n",
      "* Owlready * Creating new ontology rdf-schema <http://www.w3.org/2000/01/rdf-schema>.\n",
      "* Owlready * Creating new ontology XMLSchema <http://www.w3.org/2001/XMLSchema>.\n",
      "* Owlready * Creating new ontology anonymous <http://anonymous>.\n",
      "* Owlready * Creating new ontology owlready_ontology <http://www.lesfleursdunormal.fr/static/_downloads/owlready_ontology.owl>.\n",
      "* Owlready *     ...loading ontology owlready_ontology from C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\owlready\\owlready_ontology.owl...\n"
     ]
    }
   ],
   "source": [
    "#Import of librairies\n",
    "\n",
    "#ARVIX\n",
    "import arxiv\n",
    "\n",
    "#PDFTEXT\n",
    "import urllib.request\n",
    "import pdftotext\n",
    "\n",
    "# Prevent future/deprecation warnings from showing in output\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#Local\n",
    "import os\n",
    "import requests as r\n",
    "import sys\n",
    "\n",
    "#AI\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk.tag.stanford as st\n",
    "from nltk.tag.stanford import StanfordNERTagger \n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "import boto3\n",
    "\n",
    "from owlready import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf6f98",
   "metadata": {},
   "source": [
    "## Download pdf file from Arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63be17ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203.14957v1\n",
      "2203.14956v1\n",
      "2203.14954v1\n",
      "2203.14952v1\n",
      "2203.14949v1\n"
     ]
    }
   ],
   "source": [
    "#You can choose here how many pdf you want. Here 5 because it is enough to see the result\n",
    "max_results=5\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = \"Computer Science & AI\",\n",
    "    max_results = max_results,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "for result in search.results():\n",
    "    print(result.pdf_url[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b3ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link:  ['http://arxiv.org/pdf/2203.14957v1', 'http://arxiv.org/pdf/2203.14956v1', 'http://arxiv.org/pdf/2203.14954v1', 'http://arxiv.org/pdf/2203.14952v1', 'http://arxiv.org/pdf/2203.14949v1']\n",
      "Authors:  [['Minghao.Chen', 'Fangyun.Wei', 'Chong.Li', 'Deng.Cai'], ['Yi.Wei', 'Zibu.Wei', 'Yongming.Rao', 'Jiaxin.Li', 'Jie.Zhou', 'Jiwen.Lu'], ['Yang.Xue', 'Yuheng.Li', 'Krishna.Kumar.Singh', 'Yong.Jae.Lee'], ['K.J.Joseph', 'Salman.Khan', 'Fahad.Shahbaz.Khan', 'Rao.Muhammad.Anwer', 'Vineeth.N.Balasubramanian'], ['Dripta.S..Raychaudhuri', 'Yumin.Suh', 'Samuel.Schulter', 'Xiang.Yu', 'Masoud.Faraki', 'Amit.K..Roy.Chowdhury', 'Manmohan.Chandraker']]\n",
      "Title:  ['Frame-wise Action Representations for Long Videos via Sequence Contrastive Learning', 'LiDAR Distillation: Bridging the Beam-Induced Domain Gap for 3D Object Detection', 'GIRAFFE HD: A High-Resolution 3D-aware Generative Model', 'Energy-based Latent Aligner for Incremental Learning', 'Controllable Dynamic Multi-Task Architectures']\n"
     ]
    }
   ],
   "source": [
    "#Store in array all informations on pdf (title, authors and links)\n",
    "\n",
    "array_link=[]\n",
    "array_authors=[]\n",
    "array_title=[]\n",
    "\n",
    "for result in search.results():\n",
    "    array_link.append(result.pdf_url)\n",
    "    array_title.append(result.title)\n",
    "    temp=result.authors\n",
    "    array_authors.append([re.sub(\"[^A-Za-z0-9]\",\".\",str(i)) for i in temp])\n",
    "    #Download pdf as link.pdf\n",
    "    result.download_pdf(dirpath=\".\\Download\", filename=f'{result.pdf_url[21:]}.pdf')\n",
    "    \n",
    "print(\"Link: \",array_link)\n",
    "print(\"Authors: \",array_authors)\n",
    "print(\"Title: \",array_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0bf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pdf link to a pdf text to work on it\n",
    "\n",
    "def load_pdf(value):\n",
    "    with open(f'Download/{value}.pdf', \"rb\") as f:\n",
    "        text = pdftotext.PDF(f)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c06a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frame-wise Action Representations for Long Videos via\\r\\nSequence Contrastive Learning\\r\\n\\r\\narXiv:2203.14957v1 [cs.CV] 28 Mar 2022\\r\\n\\r\\nMinghao Chen1∗ Fangyun Wei2† Chong Li2 Deng Cai1\\r\\n1\\r\\nState Key Lab of CAD&CG, College of Computer Science, Zhejiang University\\r\\n2\\r\\nMicrosoft Research Asia\\r\\nminghaochen01@gmail.com\\r\\n\\r\\n{fawe, chol}@microsoft.com\\r\\n\\r\\ndengcai@cad.zju.edu.cn\\r\\n\\r\\nAbstract\\r\\nPrior works on action representation learning mainly focus on designing various architectures to extract the global\\r\\nrepresentations for short video clips. In contrast, many\\r\\npractical applications such as video alignment have strong\\r\\ndemand for learning dense representations for long videos.\\r\\nIn this paper, we introduce a novel contrastive action representation learning (CARL) framework to learn frame-wise\\r\\naction representations, especially for long videos, in a selfsupervised manner. Concretely, we introduce a simple yet\\r\\nefficient video encoder that considers spatio-temporal context to extract frame-wise representations. Inspired by the\\r\\nrecent progress of self-supervised learning, we present a\\r\\nnovel sequence contrastive loss (SCL) applied on two correlated views obtained through a series of spatio-temporal\\r\\ndata augmentations. SCL optimizes the embedding space by\\r\\nminimizing the KL-divergence between the sequence similarity of two augmented views and a prior Gaussian distribution of timestamp distance. Experiments on FineGym,\\r\\nPennAction and Pouring datasets show that our method\\r\\noutperforms previous state-of-the-art by a large margin\\r\\nfor downstream fine-grained action classification. Surprisingly, although without training on paired videos, our\\r\\napproach also shows outstanding performance on video\\r\\nalignment and fine-grained frame retrieval tasks. Code\\r\\nand models are available at https://github.com/\\r\\nminghchen/CARL_code.\\r\\n\\r\\nQuery\\r\\n\\r\\nTop-3 Retrieved Results\\r\\n\\r\\n(a) Fine-grained frame retrieval on FineGym dataset.\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\nHand touches\\r\\nbottle\\r\\n\\r\\nLiquid starts\\r\\nexiting\\r\\n\\r\\n…\\r\\n\\r\\nPouring\\r\\ncomplete\\r\\n\\r\\nBottle back on\\r\\ntable\\r\\n\\r\\n(b) Phase boundary detection on Pouring dataset.\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n(c) Temporal video alignment on PennAction dataset.\\r\\n\\r\\nFigure 1. Multiple applications of our frame-wise representation\\r\\nlearning on various datasets: (a) Fine-grained frame retrieval on\\r\\nFineGym [37]. (b) Phase boundary detection on Pouring [36]. (c)\\r\\nTemporal video alignment on PennAction [50]. As shown in the\\r\\nfigures, the representations obtained through our method (CARL)\\r\\nare invariant to the appearance, viewpoint and background.\\r\\n\\r\\n1. Introduction\\r\\nIn the last few years, deep learning for video understanding [1, 9, 17, 33, 39, 41, 44, 48] has achieved great success\\r\\non video classification task [9, 19, 40]. Networks such as\\r\\nI3D [9] and SlowFast [17] always take short video clips\\r\\n(e.g., 32 frames or 64 frames) as input and extract global\\r\\n*Accomplished during Minghao Chen’s internship at MSRA.\\r\\n† Corresponding author.\\r\\n\\r\\nrepresentations to predict the action category. In contrast,\\r\\nmany practical applications, e.g., sign language translation [4, 5, 13], robotic imitation learning [29, 36], action\\r\\nalignment [6, 21, 23] and phase classification [16, 27, 37, 50]\\r\\nrequire algorithms having ability to model long videos with\\r\\nhundreds of frames and extract frame-wise representations\\r\\nrather than the global features (Fig. 1).\\r\\nPrevious methods [27, 35, 37] have made an effort to\\r\\n\\r\\n\\x0c\\n\\nlearn frame-wise representations via supervised learning,\\r\\nwhere sub-actions or phase boundaries are annotated. However, it is time-consuming and even impractical to manually\\r\\nlabel each frame and exact action boundaries [21] on largescale datasets, which hinders the generalization of models trained with fully supervised learning in realistic scenarios. To reduce the dependency of labeled data, some\\r\\nmethods such as TCC [16], LAV [23] and GTA [21] explored weakly-supervised learning by using either cycleconsistency loss [16] or soft dynamic time warping [21,23].\\r\\nAll these methods rely on video-level annotations and the\\r\\ntraining is conducted on the paired videos describing the\\r\\nsame action. This setting obstructs them from applying on\\r\\nmore generic video datasets where no labels are available.\\r\\nThe goal of this work is to learn frame-wise representations with spatio-temporal context information for long\\r\\nvideos in a self-supervised manner. Inspired by the recent\\r\\nprogress of contrastive representation learning [8, 11, 12,\\r\\n20], we present a novel framework named contrastive action\\r\\nrepresentation learning (CARL) to achieve our goal. We assume no labels are available during training, and videos in\\r\\nboth training and testing sets have long durations (hundreds\\r\\nof frames). Moreover, we do not rely on video pairs of the\\r\\nsame action for training. Thus it is practical to scale up our\\r\\ntraining set with less cost.\\r\\nModeling long videos with hundreds of frames is challenging. It is non-trivial to directly use off-the-shelf backbones designed for short video clip classification, since our\\r\\ntask is to extract frame-wise representations for long videos.\\r\\nIn our work, we present a simple yet efficient video encoder\\r\\nthat consists of a 2D network to encode spatial information\\r\\nper frame and a Transformer [42] encoder to model temporal interaction. The frame-wise features are then used for\\r\\nrepresentation learning.\\r\\nRecently, SimCLR [11] uses instance discrimination [46] as the pretext task and introduces a contrastive\\r\\nloss named NT-Xent, which maximizes the agreement between two augmented views of the same data. In their implementation, all instances other than the positive reference\\r\\nare considered as negatives. Unlike image data, videos provide more abundant instances (each frame is regarded as\\r\\nan instance), and the neighboring frames have high semantic similarities. Directly regarding these frames as negatives may hurt the learning. To avoid this issue, we present\\r\\na novel sequence contrastive loss (SCL), which optimizes\\r\\nthe embedding space by minimizing the KL-divergence\\r\\nbetween the sequence similarity of two augmented video\\r\\nviews and a prior Gaussian distribution.\\r\\nThe main contributions of this paper are summarized as\\r\\nfollows:\\r\\n• We propose a novel framework named contrastive action representation learning (CARL) to learn framewise action representations with spatio-temporal con-\\r\\n\\r\\ntext information for long videos in a self-supervised\\r\\nmanner. Our method does not rely on any data annotations and has no assumptions on datasets.\\r\\n• We introduce a Transformer-based network to efficiently encode long videos and a novel sequence contrastive loss (SCL) for representation learning. Meanwhile, a series of spatio-temporal data augmentations\\r\\nare designed to increase the variety of training data.\\r\\n• Our framework outperforms the state-of-the-art methods by a large margin on multiple tasks across different datasets. For example, under the linear evaluation protocol on FineGym [37] dataset, our framework\\r\\nachieves 41.75% accuracy, which is +13.94% higher\\r\\nthan the existing best method GTA [21]. On PennAction [50] dataset, our method achieves 91.67% for\\r\\nfine-grained classification, 99.1% for Kendall’s Tau,\\r\\nand 90.58% top-5 accuracy for fine-grained frame retrieval, which all surpass the existing best methods.\\r\\n\\r\\n2. Related Works\\r\\nConventional Action Recognition. Various challenging\\r\\nvideo datasets [9, 25, 32, 38, 40] have been constructed to\\r\\nreason deeply about diverse scenes and situations. These\\r\\ndatasets provide labels of high-level concepts or detailed\\r\\nphysical aspects for short videos or trimmed clips. To tackle\\r\\nvideo recognition, large amounts of architectures have been\\r\\nproposed [1, 3, 9, 17, 33, 39, 41, 43, 44]. Most networks are\\r\\nbased on 3D Convolution layers and combined with the\\r\\ntechniques in image recognition [9, 17, 41], e.g., residual\\r\\nconnections [24] and ImageNet pre-training [14]. Some\\r\\nworks [33, 44] find that 3D ConvNets have insufficient receptive fields and become the bottleneck of the computational budget.\\r\\nRecently, Transformers [42] achieved great success in\\r\\nthe field of computer vision, e.g., ViT [15] and DETR [7].\\r\\nThere are also several works that extend Transformers to\\r\\nvideo recognition, such as TimeSformer [3] and ViViT [1].\\r\\nDue to the strong capacity of Transformers and the global\\r\\nreceptive field, these methods have become new stateof-the-art. Combining 2D backbones and Transformers,\\r\\nVTN [33] can efficiently process long video sequences.\\r\\nHowever, these architecture are all designed for video classification and predict one global class for a video.\\r\\nFine-grained Action Recognition. There are also some\\r\\ndatasets [27, 35, 37, 50] that investigate fine-grained action\\r\\nrecognition. They decompose an action into some action\\r\\nunits, sub-actions, or phases. As a result, each video contains multiple simple stages, e.g., wash the cucumber, peel\\r\\nthe cucumber, place the cucumber, take a knife, and make\\r\\na slice in preparing cucumber [35]. However, these finelevel labels are more expensive to collect, resulting in a\\r\\nlimited size of these datasets. GTA [21] argues that these\\r\\n\\r\\n\\x0c\\n\\nView 1\\r\\n\\r\\nFrame-wise Representations for View 1\\r\\n\\r\\n…\\r\\n\\r\\nRandom Sampling\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\nLong Video Sequence\\r\\n\\r\\nTemporal\\r\\nRandom\\r\\nCrop\\r\\n\\r\\nSpatial\\r\\nAug\\r\\n\\r\\nRandom Sampling\\r\\n\\r\\nCommon\\r\\nFrames\\r\\n\\r\\nFVE\\r\\nProjection\\r\\n\\r\\n…\\r\\n\\r\\nSequence\\r\\nContrastive\\r\\nLoss\\r\\n\\r\\nFrame-wise Representations for View 2\\r\\n\\r\\nView 2\\r\\n\\r\\nData Preprocessing\\r\\n\\r\\nRepresentation Learning\\r\\n\\r\\nFigure 2. Overview of our framework (CARL). Two augmented views are constructed from a training video through a series of spatiotemporal data augmentations. The frame-level video encoder (FVE) and the projection head are optimized by minimizing the proposed\\r\\nsequence contrastive loss (SCL) between two views.\\r\\n\\r\\nboundary of manual annotations are subjective. Therefore,\\r\\nself-supervised learning for fine-level representations is a\\r\\npromising direction.\\r\\nSelf-supervised Learning in Videos. Previous methods of\\r\\nself-supervised learning in videos construct pretext tasks,\\r\\nincluding inferring the future [22], discriminating shuffled\\r\\nframes [31] and predicting speed [2]. There are also some\\r\\nalignment-based methods, where a pair of videos are trained\\r\\nwith cycle-consistent loss [16] or soft dynamic time warping (DTW) [10, 21, 23]. Recently, the contrastive learning\\r\\nmethods [11, 12, 20, 45] based on instance discrimination\\r\\nhave shown superior performance on 2D image tasks. Some\\r\\nworks [18, 26, 34, 36, 49] also use this contrastive loss for\\r\\nvideo representation learning. They treat different frames\\r\\nin a video [26, 36, 49] or different clips [18, 34] in other\\r\\nvideos as negative samples. Different from these methods,\\r\\nour goal is fine-grained temporal understanding of videos\\r\\nand we treat a long sequence of frames as input data. The\\r\\nmost relevant work to ours is [28], which utilizes 3D human keypoints for self-supervised acton discovery in long\\r\\nkinematic videos.\\r\\n\\r\\nis named data preprocessing. Then we feed two augmented\\r\\nviews into our frame-level video encoder (FVE) to extract\\r\\ndense representations. Following SimCLR [11], FVE is appended with a small projection network which is a twolayer MLP for obtaining latent embeddings. Due to the\\r\\nfact that temporally adjacent frames are highly correlated,\\r\\nwe assume that the similarity distribution between two augmented views should follow a prior Gaussian distribution.\\r\\nBased on the assumption, we propose a novel sequence contrastive loss (SCL) to optimize frame-wise representations\\r\\nin the embedding space.\\r\\n\\r\\n3. Method\\r\\n\\r\\nConcretely, for a training video V with S frames, we\\r\\naim to construct two augmented videos with T frames independently through a series spatio-temporal data augmentations. For temporal data augmentation, we first perform\\r\\ntemporal random crop on V to generate two randomly\\r\\ncropped clips with the length of [T, αT ] frames, where α\\r\\nis a hyper-parameter controlling maximum crop size. During this process, we guarantee at least β percent of overlapped frames existing between two clips. Then we randomly sample T frames for each video sequence, and obtain V 1 = {v 1i | 1 ≤ i ≤ T }, and V 2 = {v 2i | 1 ≤ i ≤ T },\\r\\nwhere v 1i and v 2i represent i-th frame from V 1 and V 2 , respectively. We set T = 240 by default. For the videos with\\r\\nless than T frames, empty frames are padded before cropping. Finally, we apply several temporal-consistent spatial\\r\\ndata augmentations, including random resize and crop, horizontal flip, random color distortions, and random Gaussian\\r\\nblur, on V 1 and V 2 independently.\\r\\n\\r\\nIn this section, we introduce a novel framework named\\r\\ncontrastive action representation learning (CARL) to learn\\r\\nframe-wise action representations in a self-supervised manner. In particular, our framework is designed to model\\r\\nlong video sequences by considering spatio-temporal context. We first present an overview of the proposed framework in Section 3.1. Then we introduce the details of view\\r\\nconstruction and data augmentation in Section 3.2. Next,\\r\\nwe describe our frame-level video encoder in Section 3.3.\\r\\nFinally, the proposed sequence contrastive loss (SCL) and\\r\\nits design principles are introduced in Section 3.4.\\r\\n\\r\\n3.1. Overview\\r\\nFigure 2 displays an overview of our framework. We first\\r\\nconstruct two augmented views for an input video through\\r\\na series of spatio-temporal data augmentations. This step\\r\\n\\r\\n3.2. View Construction\\r\\nWe first introduce the view construction step of our\\r\\nmethod, as shown in the ‘data preprocessing’ part in Figure 2. Data augmentation is crucial to avoid trivial solutions\\r\\nin self-supervised learning [11, 12]. Different from prior\\r\\nmethods designed for image data which only require spatial augmentations, we introduce a series of spatio-temporal\\r\\ndata augmentations to further increase the variety of videos.\\r\\n\\r\\n\\x0c\\n\\nPrior Gaussian Distribution of\\r\\nTimestamp Distance between 𝑠𝑖1 and S 2\\r\\n\\r\\nFrame-wise Representations\\r\\n\\r\\n𝑇 × 128\\r\\n\\r\\nTimeline of\\r\\nthe Video\\r\\n\\r\\nLinear\\r\\n\\r\\n𝑠𝑖1\\r\\n\\r\\n𝑇 × 256\\r\\n\\r\\nTransformer Encoder\\r\\n𝑇 × 256\\r\\n\\r\\nPositional\\r\\nEncoding\\r\\n\\r\\nSample Frames\\r\\n𝑽1\\r\\n\\r\\nResNet-50\\r\\n\\r\\n𝑆2\\r\\n\\r\\n…\\r\\n𝑽2\\r\\n\\r\\n𝑣𝑖1\\r\\n\\r\\nTransformation\\r\\n𝑇 × 2048\\r\\n\\r\\n…\\r\\n\\r\\nFVE + Projection\\r\\n𝒁1\\r\\n\\r\\n𝒁2\\r\\n\\r\\n𝑧𝑖1\\r\\nCosine Similarity (𝑧𝑖1 , 𝒁2 )\\r\\n\\r\\n𝑇 × 224 ×224 ×3\\r\\nInput Video\\r\\n\\r\\nFigure 3. Architecture of the proposed frame-level video encoder\\r\\n(FVE). The input is a long video with T frames and the outputs\\r\\nare frame-wise representations. ResNet-50 is pre-trained on ImageNet. We freeze the first four residual blocks of ResNet-50 and\\r\\nonly finetune the last block.\\r\\n\\r\\n3.3. Frame-level Video Encoder\\r\\nIt is non-trivial to directly apply video classification\\r\\nbackbones [9, 17, 41] to model long video sequences with\\r\\nhundreds of frames due to the huge computational cost.\\r\\nTCC [16] presents a video encoder that combines 2D\\r\\nResNet and 3D Convolution to generate frame-wise features. However, stacking too many 3D Convolutional layers\\r\\nleads to unaffordable computational costs. As a result, this\\r\\nkind of design may have limited receptive fields to capture\\r\\ntemporal context. Recently, Transformers [42] achieved\\r\\ngreat progress in computer vision [7, 15]. Transformers utilize the attention mechanism to solve sequence-to-sequence\\r\\ntasks while handling long-range dependencies with ease. In\\r\\nour network implementation, we adopt the Transformer encoder as an alternative to model temporal context.\\r\\nFigure 3 shows our frame-level video encoder (FVE). To\\r\\nseek the tradeoff between representation performance and\\r\\ninference speed, we first use a 2D network, e.g., ResNet50 [24], along temporal dimension to extract spatial features\\r\\nfor the RGB video sequence of size T × 224 × 224 × 3.\\r\\nThen a transformation block that consists of two fully connected layers with batch normalization and ReLU is applied to project the spatial features to the intermediate embeddings of size T × 256. Following common practice,\\r\\nwe add the sine-cosine positional encoding [42] on top of\\r\\nthe intermediate embeddings to encode the order information. Next, the encoded embeddings are fed into the 3-layer\\r\\nTransformer encoder to model temporal context. At last, a\\r\\nlinear layer is adopted to obtain the final frame-wise representations H ∈ RT ×128 . We use hi (1 ≤ i ≤ T ) to denote\\r\\nthe representation of i-th frame.\\r\\n\\r\\nSequence Contrastive\\r\\nLoss of 𝐿1𝑖\\r\\n\\r\\nFigure 4. Illustration of the proposed sequence contrastive loss.\\r\\nWe use the loss computation of v 1i ∈ V 1 as the example. We\\r\\nfirst compute a prior Gaussian distribution of timestamp distance\\r\\n(s1i −s21 , · · · , s1i −s2T ). Then the embedding similarity distribution\\r\\nbetween z 1i and Z 2 is calculated. We minimize the KL-divergence\\r\\nof two distributions in the embedding space.\\r\\n\\r\\nThe 2D ResNet-50 network is pre-trained on ImageNet [14]. Considering the limited computational budget,\\r\\nwe freeze the first four residual blocks since they already\\r\\nlearned favorable low-level visual representations by pretraining. This simple design ensures that our network can\\r\\nbe trained and tested on videos with more than 500 frames.\\r\\nVTN [33] adopt a similar hybrid Transformer-based network to perform video classification. They use the [CLS]\\r\\ntoken to generate a global feature, while our network is designed to extract frame-wise representations by considering\\r\\nthe spatio-temporal context. In addition, our network explores modeling much more prolonged video sequences.\\r\\n\\r\\n3.4. Sequence Contrastive Loss\\r\\nSimCLR [11] introduces a contrastive loss named NTXent by maximizing agreement between augmented views\\r\\nof the same instance.\\r\\nUnlike self-supervised learning for images, videos provide abundant sequential information, which is a vital supervisory signal. For typical instance discrimination, all\\r\\ninstances other than the positive reference are considered\\r\\nas negatives. However, the neighboring frames around the\\r\\nreference frame are highly correlated. Directly regarding\\r\\nthese frames as negatives may hurt the learning. Learning\\r\\nprinciples should be carefully designed to avoid this issue.\\r\\nTo optimize frame-wise representations, we propose a novel\\r\\nsequence contrastive loss (SCL) which minimizes the KLdivergence between the embedding similarity of two augmented views and the prior Gaussian distribution, as shown\\r\\n\\r\\n\\x0c\\n\\nin Figure 4.\\r\\nConcretely, following SimCLR, we use a small projection network g(·) which is a two-layer MLP to project\\r\\nframe-wise representations H encoded by the proposed\\r\\nFVE to the latent embeddings Z = g(H). Let Z 1 =\\r\\n{z 1i | 1 ≤ i ≤ T } and Z 2 = {z 2i | 1 ≤ i ≤ T } denote the latent embeddings of V 1 and V 2 , where z 1i and\\r\\nz 2i represent the latent embedding of i-th frame in V 1 and\\r\\nV 2 respectively. Let S 1 = {s1i | 1 ≤ i ≤ T } denote timestamp vector of V 1 , where s1i is the corresponding raw video\\r\\ntimestamp of the i-th frame in V 1 (see Figure 4). In the\\r\\nsame way, we can define S 2 = {s2i | 1 ≤ i ≤ T }.\\r\\nGiven the i-th reference frame in V 1 and its corresponding latent embedding z 1i , due to the fact that temporally\\r\\nadjacent frames are more highly correlated than those faraway ones, we assume the embedding similarity between\\r\\nz 1i and Z 2 = {z 2i | 1 ≤ i ≤ T } should follow a prior\\r\\nGaussian distribution of timestamp distance between s1i and\\r\\nS 2 = {s2i | 1 ≤ i ≤ T }. This assumption motivates\\r\\nus to use KL-divergence to optimize the embedding space.\\r\\nSpecifically, let sim(u, v) = u> v/kukkvk denote cosine\\r\\nx2\\r\\nsimilarity, and G(x) = σ√12π exp(− 2σ\\r\\n2 ) denote the Gaussian function, where σ 2 is the variance. We formulate the\\r\\nloss of i-th reference frame in V 1 as follows:\\r\\n\\r\\nPennAction Dataset. Videos in this dataset show humans doing different kinds of sports or exercise. Following TCC [16], we use 13 actions of PennAction dataset. In\\r\\ntotal, there are 1140 videos for training and 966 videos for\\r\\ntesting. Each action set has 40-134 videos for training and\\r\\n42-116 videos for testing. We obtain per-frame labels from\\r\\nLAV [23]. The video frames are from 18 to 663.\\r\\nFineGym Dataset. FineGym is a recent large-scale finegrained action recognition dataset that requires representation learning methods to distinguish different sub-actions\\r\\nwithin the same video. We chunk the original YouTube\\r\\nvideos according to the action boundaries so that each\\r\\ntrimmed video data only describes a single action type\\r\\n(Floor Exercise, Balance Beam, Uneven Bars, or VaultWomen). Finally, we obtained 3182 videos for training and\\r\\n1442 videos for testing. The video frames vary from 140\\r\\nto 5153. FineGym provides two data splits according to the\\r\\ncategory number, namely FineGym99 with 99 sub-action\\r\\nclasses and FineGym288 with 288 sub-action classes.\\r\\nPouring Dataset. In this dataset, videos record the process of hand pouring water from one object to another. The\\r\\nphase labels (5 phase classes) are obtained from TCC [16].\\r\\nFollowing TCC [16], we use 70 videos for training and 14\\r\\nvideos for testing. The video frames are from 186 to 797.\\r\\n\\r\\nT\\r\\nX\\r\\nexp(sim(z 1i , z 2j )/τ )\\r\\n=−\\r\\nwij log PT\\r\\n,\\r\\n1\\r\\n2\\r\\nk=1 exp(sim(z i , z k )/τ )\\r\\nj=1\\r\\n\\r\\nEvaluation Metrics. For each dataset, We first optimize\\r\\nour network on the training set, without using any labels,\\r\\nand then use the following four metrics to evaluate the\\r\\nframe-wise representations:\\r\\n\\r\\nL1i\\r\\n\\r\\nG(s1i − s2j )\\r\\n,\\r\\nwij = PT\\r\\n1\\r\\n2\\r\\nk=1 G(si − sk )\\r\\n\\r\\n(1)\\r\\n\\r\\n(2)\\r\\n\\r\\nwhere wij is the normalized Gaussian weight and τ is the\\r\\ntemperature parameter. Then the overall loss for V 1 can be\\r\\ncomputed across all frames:\\r\\nT\\r\\n1X 1\\r\\nL .\\r\\nL =\\r\\nT i=1 i\\r\\n1\\r\\n\\r\\n(3)\\r\\n\\r\\nSimilarly, we can calculate the loss L2 for V 2 . Our sequence contrastive loss is defined as LSCL = L1 + L2 .\\r\\nNoticeably, our loss does not rely on frame-to-frame correspondence between V 1 and V 2 , which supports the diversity of spatial-temporal data augmentation.\\r\\n\\r\\n• Phase Classification (or Fine-grained Action Classification) [16] is the averaged per-frame classification\\r\\naccuracy on testing set. Before testing, we fix the network and train a linear classifier by using per-frame\\r\\nlabels (phase class or sub-action category) of the training set.\\r\\n• Phase Progression [16] measures the representation\\r\\nability to predict the phase progress. We fix the\\r\\nnetwork and train a linear regressor to predict the\\r\\nphase progression values (timestamp distance between\\r\\na query frame and phase boundaries) for all frames.\\r\\nThen it is computed as the average R-squared measure.\\r\\n\\r\\n4.1. Datasets and Metrics\\r\\n\\r\\n• Kendall’s Tau [16] is calculated over every pair of testing videos by sampling two frames in the first video\\r\\nand retrieving the corresponding nearest frames in the\\r\\nsecond video, and checking whether their orders are\\r\\nshuffled. It measures how well-aligned two sequences\\r\\nare in time. No more training or finetuning is needed.\\r\\n\\r\\nWe use three video datasets, namely PennAction [50],\\r\\nFineGym [37] and Pouring [36] to evaluate the performance\\r\\nof our method. We compare our method with sate-of-thearts on all three datasets. Unless otherwise specified, all\\r\\nablation studies on conducted on PennAction dataset.\\r\\n\\r\\n• Average Precision@K [23] is computed as how many\\r\\nframes in the retrieved K frames have the same phase\\r\\nlabels as the query frame. It measures the fine-grained\\r\\nframe retrieval accuracy. K = 5, 10, 15 are evaluated.\\r\\nNo more training or finetuning is needed.\\r\\n\\r\\n4. Experiments\\r\\n\\r\\n\\x0c\\n\\nTraining Strategy\\r\\n\\r\\nAnnotation\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nTCC [16]\\r\\nLAV [23]\\r\\n\\r\\nPer-action\\r\\n\\r\\nWeakly\\r\\n\\r\\n81.35\\r\\n84.25\\r\\n\\r\\n0.664\\r\\n0.661\\r\\n\\r\\n0.701\\r\\n0.805\\r\\n\\r\\nTCC [16]\\r\\nLAV [23]\\r\\nGTA [21]\\r\\n\\r\\nJoint\\r\\n\\r\\nWeakly\\r\\n\\r\\n74.39\\r\\n78.68\\r\\n-\\r\\n\\r\\n0.591\\r\\n0.625\\r\\n0.789\\r\\n\\r\\n0.641\\r\\n0.684\\r\\n0.748\\r\\n\\r\\nSaL [31]\\r\\nTCN [36]\\r\\nOurs\\r\\n\\r\\nJoint\\r\\n\\r\\nNone\\r\\n\\r\\n68.15\\r\\n68.09\\r\\n93.07\\r\\n\\r\\n0.390\\r\\n0.383\\r\\n0.918\\r\\n\\r\\n0.474\\r\\n0.542\\r\\n0.985\\r\\n\\r\\nMethod\\r\\n\\r\\nTable 1. Comparison with state-of-the-art methods on PennAction, using various evaluation metrics: Phase Classification (Classification),\\r\\nPhase Progression (Progress) and Kendall’s Tau (τ ). The top row results are from per-action models, i.e., separate models are trained for\\r\\ndifferent actions. The results in middle and bottom row are obtained from training a single model for all actions.\\r\\n\\r\\nFollowing [16, 23, 36], Phase Classification, Phase\\r\\nProgression and Kendall’s Tau are evaluated on Pouring\\r\\ndataset. For PennAction, all four metrics are evaluated\\r\\nwithin each action category, and the final results are averaged across the 13 action categories. Following [21], we use\\r\\nFine-grained Action Classification to evaluate our method\\r\\non FineGym dataset.\\r\\n\\r\\n4.2. Implementation Details\\r\\nIn our network, we adopt ResNet-50 [24] pre-trained by\\r\\nBYOL [20] as frame-wise spatial encoder. Unless otherwise specified, we use a 3-layer Transformer encoder [42]\\r\\nwith 256 hidden size and 8 heads to model temporal context. We train the model using Adam optimizer with learning rate 10−4 and weight decay 10−5 . We decay the learning rate with cosine decay schedule without restarts [30]. In\\r\\nour loss, we set σ 2 = 10 and τ = 0.1 as default. Following SimCLR [11], random image cropping, horizontal flipping, random color distortions, and random Gaussian blur\\r\\nare employed as the spatial augmentations. For our temporal data augmentations described in Section 3.2, we set\\r\\nhyper-parameters α = 1.5 and β = 20%. The video batch\\r\\nsize is set as 4 (8 views), and our model is trained on 4\\r\\nNvidia V100 GPUs for 300 epochs. During training, we\\r\\nsample T = 240 frames for Pouring and FineGym, T = 80\\r\\nframes for PennAction. During testing, we feed the whole\\r\\nvideo into the model at once, without any temporal downsampling. We L2-normalize the frame-wise representations\\r\\nfor evaluation.\\r\\n\\r\\n4.3. Main Results\\r\\nResults on PennAction Dataset. In Table 1, our method\\r\\nis compared with state-of-the-art methods on PennAction.\\r\\nTCC [16] and LAV [23] train a separate model for each action (‘Per-action’ in the table), which results in 13 expert\\r\\nmodels for 13 action classes correspondingly. In contrast,\\r\\nwe train only one model for all 13 action classes (‘Joint’\\r\\nin the table). Noticeably, our approach not only outper-\\r\\n\\r\\nMethod\\r\\n\\r\\nAP@5\\r\\n\\r\\nAP@10\\r\\n\\r\\nAP@15\\r\\n\\r\\nTCN [36]\\r\\nTCC [16]\\r\\nLAV [23]\\r\\n\\r\\n77.84\\r\\n76.74\\r\\n79.13\\r\\n\\r\\n77.51\\r\\n76.27\\r\\n78.98\\r\\n\\r\\n77.28\\r\\n75.88\\r\\n78.90\\r\\n\\r\\nOurs\\r\\n\\r\\n92.28\\r\\n\\r\\n92.10\\r\\n\\r\\n91.82\\r\\n\\r\\nTable 2. Fine-grained frame retrieval results on PennAction.\\r\\n\\r\\nMethod\\r\\n\\r\\nFineGym99\\r\\n\\r\\nFineGym288\\r\\n\\r\\n3\\r\\n\\r\\nD TW [10]\\r\\nSpeedNet [2]\\r\\nTCN [36]\\r\\nSaL [31]\\r\\nTCC [16]\\r\\nGTA [21]\\r\\n\\r\\n15.28\\r\\n16.86\\r\\n20.02\\r\\n21.45\\r\\n25.18\\r\\n27.81\\r\\n\\r\\n14.07\\r\\n15.57\\r\\n17.11\\r\\n19.58\\r\\n20.82\\r\\n24.16\\r\\n\\r\\nOurs\\r\\n\\r\\n41.75\\r\\n\\r\\n35.23\\r\\n\\r\\nTable 3. Comparison with state-of-the-art methods on FineGym,\\r\\nunder the evaluation of Fine-grained Action Classification.\\r\\n\\r\\nforms the methods using joint training, but also outperforms\\r\\nthe methods adopting per-action training strategy by a large\\r\\nmargin under different evaluation metrics. In Table 2, we\\r\\nreport the results under the Average Precision@K metric,\\r\\nwhich measures the performance of fine-grained frame retrieval. Surprisingly, although our model is not trained on\\r\\npaired data, it can successfully find frames with similar semantics from other videos. For all AP@K, our method is at\\r\\nleast +11% better than previous methods.\\r\\nResults on FineGym Dataset. Table 3 summarizes the\\r\\nexperimental results of Fine-grained Action Classification\\r\\non FineGym99 and FineGym288. Our method outperforms the other self-supervised [2, 31, 36] and weakly\\r\\nsupervised [10, 16, 21] methods. The performance of\\r\\nour method surpasses the previous state-of-the-art method\\r\\nGTA [21] by +13.94% on FineGym99 and +11.07% on FineGym288. The weakly supervised methods, i.e., TCC [16],\\r\\nD3 TW [10] and GTA [21], assume there exists an optimal\\r\\n\\r\\n\\x0c\\n\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\n#Layers\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nTCN [36]\\r\\nTCC [16]\\r\\nLAV [23]\\r\\n\\r\\n89.53\\r\\n91.53\\r\\n92.84\\r\\n\\r\\n0.804\\r\\n0.837\\r\\n0.805\\r\\n\\r\\n0.852\\r\\n0.864\\r\\n0.856\\r\\n\\r\\nOurs\\r\\n\\r\\n93.73\\r\\n\\r\\n0.935\\r\\n\\r\\n0.992\\r\\n\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n\\r\\n92.15\\r\\n92.61\\r\\n93.07\\r\\n92.81\\r\\n\\r\\n0.909\\r\\n0.913\\r\\n0.918\\r\\n0.910\\r\\n\\r\\n0.985\\r\\n0.990\\r\\n0.985\\r\\n0.990\\r\\n\\r\\nMethod\\r\\n\\r\\nTable 4. Comparison with state-of-the-art methods on Pouring.\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nResNet-50 only\\r\\nResNet-50+C3D\\r\\n\\r\\n68.63\\r\\n83.96\\r\\n\\r\\n0.296\\r\\n0.705\\r\\n\\r\\n0.440\\r\\n0.778\\r\\n\\r\\nResNet-50+\\r\\nTransformer\\r\\n\\r\\n93.07\\r\\n\\r\\n0.918\\r\\n\\r\\n0.985\\r\\n\\r\\nArchitecture\\r\\n\\r\\nTable 6. Study on the effects of using different number of layers\\r\\nin Transformer encoder.\\r\\n\\r\\nLearnable Blocks\\r\\nNone\\r\\nBlock5\\r\\nBlock4+Block5\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\n90.63\\r\\n93.07\\r\\n92.98\\r\\n\\r\\n0.907\\r\\n0.918\\r\\n0.919\\r\\n\\r\\n0.994\\r\\n0.985\\r\\n0.989\\r\\n\\r\\nTable 7. Ablation study on learnable blocks of ResNet-50.\\r\\n\\r\\nTable 5. Ablation study on different architectures.\\r\\n\\r\\nalignment between two videos from the training set. However, for FineGym dataset, even in two videos describing\\r\\nthe same action, the set and order of sub-actions may differ.\\r\\nTherefore, the alignment found by these methods can be incorrect, which impedes learning. The great improvement\\r\\nverifies the effectiveness of our framework.\\r\\nResults on Pouring Dataset. As shown in Table 4, our\\r\\nmethod also achieves the best performance on a relatively\\r\\nsmall dataset, Pouring. These results further demonstrate\\r\\nthe great generalization ability of our approach.\\r\\nVisualization Results. We present the visualization of finegrained frame retrieval and video alignment in Section A.\\r\\n\\r\\n4.4. Ablation Study\\r\\nIn this section, we perform multiple experiments to analyze the different components of our framework. Unless\\r\\notherwise specified, experiments are conducted on the PennAction dataset.\\r\\nNetwork Architecture. In Table 5, we investigate the network architecture. ‘ResNet-50+Transformer’ denotes our\\r\\ndefault frame-level video encoder introduced in Section 3.3.\\r\\n‘ResNet-50 only’ means we remove the Transformer encoder in our network, and only use 2D ResNet-50 and\\r\\nlinear transformation layers to extract representations per\\r\\nframe. ‘ResNet-50+C3D’ represents that two 3D convolutional layers [41] are added on top of the ResNet-50 before the spatial pooling, which is the same as the model\\r\\nadopted in TCC [16] and LAV [23]. These models are all\\r\\ntrained with the proposed sequence contrastive loss. Our\\r\\ndefault network outperforms the other two networks, which\\r\\nattributes to the long-range dependency modeling ability of\\r\\nTransformers.\\r\\nLayer Number of Transformer Encoder. Table 6 shows\\r\\nstudies using different numbers of layers in Transformers.\\r\\nWe find that Phase Classification increases with more lay-\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nTCN†\\r\\nTCC†\\r\\n\\r\\n86.31\\r\\n86.35\\r\\n\\r\\n0.898\\r\\n0.899\\r\\n\\r\\n0.832\\r\\n0.980\\r\\n\\r\\nOurs\\r\\n\\r\\n93.07\\r\\n\\r\\n0.918\\r\\n\\r\\n0.985\\r\\n\\r\\nMethod\\r\\n\\r\\nTable 8. Applying our network to TCN and TCC. † denotes we reimplement the method and replace the network with ours. “Contrastive baseline” uses the corresponding frame at the other view\\r\\nas the positive sample.\\r\\n\\r\\ners. However, Phase Progression slightly drops when there\\r\\nare too many layers. We use 3 layers by default.\\r\\nTraining Different Blocks of ResNet. In our implementation, ResNet-50 is pre-trained on ImageNet. In Table 7, we\\r\\nstudy the effects of finetuning different blocks of ResNet50. The standard ResNet contains 5 blocks, namely Block1Block5. ‘None’ denotes that all layers of ResNet are frozen.\\r\\n‘Block5’ denotes we freeze the first four residual blocks\\r\\nof ResNet and only make the last residual block learnable,\\r\\nwhich is our default setting. Similarly, ‘Block4+Block5’\\r\\nmeans we freeze the first three blocks and only train the last\\r\\ntwo blocks. Table 7 shows that encoding dataset-related\\r\\nspatial information is important (‘None’ vs. ‘Block5’),\\r\\nand training more blocks does not lead to improvement\\r\\n(‘Block5’ vs. ‘Block4+Block5’).\\r\\nApplying Our Network to Other Methods. We study\\r\\nwhether our frame-level video encoder (FVE) introduced\\r\\nin Section 3.3 can boost the performances of TCC [16] and\\r\\nTCN [36]. We replace the C3D-based network with ours.\\r\\nTable 8 shows the results. We find that the proposed network can dramatically improve the performance of their\\r\\nmethods (compared with the results in Table 3). In addition, our method still keeps a large performance gain, which\\r\\nattributes to the proposed sequence contrastive loss.\\r\\nHyper-parameters of Sequence Contrastive Loss. We\\r\\nstudy the hyper-parameters, i.e., temperature parameter τ\\r\\nand Gaussian variance σ 2 in our sequence contrastive loss\\r\\n\\r\\n\\x0c\\n\\nHyper-parameters\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nα\\r\\n\\r\\nSampling\\r\\n\\r\\nβ (%)\\r\\n\\r\\nFineGym99\\r\\n\\r\\nτ =0.1, σ 2 =1\\r\\nτ =0.1, σ 2 =25\\r\\n\\r\\n92.95\\r\\n92.03\\r\\n\\r\\n0.903\\r\\n0.922\\r\\n\\r\\n0.963\\r\\n0.993\\r\\n\\r\\nRandom\\r\\n\\r\\n20\\r\\n\\r\\nτ =1.0, σ 2 =10\\r\\nτ =0.3, σ 2 =10\\r\\nτ =0.1, σ 2 =10\\r\\n\\r\\n91.57\\r\\n92.13\\r\\n93.07\\r\\n\\r\\n0.889\\r\\n0.903\\r\\n0.918\\r\\n\\r\\n0.993\\r\\n0.992\\r\\n0.985\\r\\n\\r\\n0\\r\\n1.5\\r\\n1\\r\\n\\r\\n36.72\\r\\n41.75\\r\\n39.03\\r\\n\\r\\n1.5\\r\\n\\r\\nEven\\r\\n\\r\\n20\\r\\n\\r\\n38.44\\r\\n\\r\\nRandom\\r\\n\\r\\n0\\r\\n20\\r\\n50\\r\\n80\\r\\n100\\r\\n\\r\\n38.15\\r\\n41.75\\r\\n39.14\\r\\n37.94\\r\\n35.53\\r\\n\\r\\nTable 9. Ablation study on Gaussian variance σ 2 and the temperature τ in sequence contrastive loss.\\r\\n\\r\\n(see Eq. 2). The variance σ 2 of the prior Gaussian distribution controls how the adjacent frames are semantically similar to the reference frame, on the assumption. As Table 9\\r\\nshows, too small variance (σ 2 = 1) or too large variance\\r\\n(σ 2 = 25) degrades the performance. We use σ 2 = 10 by\\r\\ndefault. In addition, we observe an appropriate temperature\\r\\n(τ = 0.1) facilitates the learning from hard negatives, which\\r\\nis consistent with the conclusion in SimCLR [11].\\r\\nStudy on Different Temporal Data Augmentations. We\\r\\nstudy the different temporal data augmentations described\\r\\nin Section 3.2, including maximum crop size α, overlap\\r\\nratio β between views, and different sampling strategies,\\r\\nnamely random sampling and even sampling. Table 10\\r\\nshows the results. From the table, we can see that the performance drops dramatically when we crop the video with a\\r\\nfixed length (α = 1). The performance also decreases when\\r\\nwe perform even sampling on the cropped clips. As described in Section 3.4, our sequence contrastive loss does\\r\\nnot rely on frame-to-frame correspondence between two\\r\\naugmented views. Experimentally, constructing two views\\r\\nwith β = 100% percent of overlapped frames degrades the\\r\\nperformance, since the variety of augmented data decreases.\\r\\nIn addition, we also observe the performance drops when\\r\\ntwo views are constructed independently (β = 0% ). The\\r\\nreason is that in this setting, the training may bring the representations of temporally distant frames closer, which hinders the optimization.\\r\\nNumber of Training Frames and Linear Evaluation Under Different Data Protocols. As described in Section 3.2,\\r\\nour network takes augmented views with T frames as input. We study the effects of different frame numbers T on\\r\\nFineGym99. Table 11 shows the results. We observe that\\r\\ntaking long sequences as input is essential for frame-wise\\r\\nrepresentation learning. However, a too large frame number degrades the performance. We thus set T = 240 by default. We also conduct linear evaluation under different data\\r\\nprotocols. Concretely, we use 10%, 50% and 100% labeled\\r\\ndata to train the linear classifier. Compared with the supervised model (all layers are learnable), our method achieves\\r\\nbetter performance when the labeled data is limited (10%\\r\\ndata protocol).\\r\\n\\r\\n1.5\\r\\n\\r\\nTable 10. Ablation study on hyper-parameters of temporal data\\r\\naugmentations. Effects of maximum crop size α, overlap ratio β\\r\\nand random sampling strategy are studied. The experiments are\\r\\nconducted on FineGym99 dataset.\\r\\n\\r\\n% of Labeled Data →\\r\\n\\r\\n10\\r\\n\\r\\nNumber of training frames:\\r\\n80\\r\\n27.10\\r\\n160\\r\\n30.28\\r\\n33.53\\r\\n240\\r\\n480\\r\\n31.46\\r\\nSupervised\\r\\n\\r\\n24.51\\r\\n\\r\\n50\\r\\n\\r\\n100\\r\\n\\r\\n32.78\\r\\n36.46\\r\\n39.89\\r\\n37.92\\r\\n\\r\\n34.02\\r\\n38.06\\r\\n41.75\\r\\n39.45\\r\\n\\r\\n48.75\\r\\n\\r\\n60.37\\r\\n\\r\\nTable 11. Ablation studies on number of training frames under\\r\\ndifferent data protocols. Study is conducted on FineGym99 Finegrained Action Classification task. ‘Supervised’ means all layers\\r\\nare trained with supervised learning.\\r\\n\\r\\n5. Conclusion\\r\\nIn this paper, we present a novel framework named\\r\\ncontrastive action representation learning (CARL) to learn\\r\\nframe-wise action representations, especially for long\\r\\nvideos, in a self-supervised manner. To model long videos\\r\\nwith hundreds of frames, we introduce a simple yet efficient\\r\\nnetwork named frame-level video encoder (FVE), which\\r\\nconsiders spatio-temporal context during training. In addition, we propose a novel sequence contrastive loss (SCL)\\r\\nfor frame-wise representation learning. SCL optimizes the\\r\\nembedding space by minimizing the KL-divergence between the sequence similarity of two augmented views\\r\\nand a prior Gaussian distribution. Experiments on various\\r\\ndatasets and tasks show effectiveness and generalization of\\r\\nour method.\\r\\n\\r\\nAcknowledgments\\r\\nThis work was supported in part by The National\\r\\nKey Research and Development Program of China (Grant\\r\\nNos: 2018AAA0101400), in part by The National Nature Science Foundation of China (Grant Nos: 62036009,\\r\\n61936006), in part by Innovation Capability Support Program of Shaanxi (Program No. 2021TD-05).\\r\\n\\r\\n\\x0c\\n\\nFigure 5. Visualization of video alignment on FineGym dataset.\\r\\nPlease refer to video demos in our supplementary materials for\\r\\nmore visualization results.\\r\\n\\r\\nQuery\\r\\n\\r\\nTop-5 Retrieved Frames\\r\\n\\r\\nFigure 7. Visualization of fine-grained frame retrieval on FineGym\\r\\ndatast by using our method.\\r\\n\\r\\ngive video demos in our supplementary materials.\\r\\n(a) Pouring dataset.\\r\\n\\r\\n(b) PennAction dataset.\\r\\n\\r\\nFigure 6. We randomly select two videos recording the same process (or action) from Pouring (or PennAction) dataset and compute the similarity matrix for frame-wise representations extracted\\r\\nby our method. The similarities are normalized for better visualization.\\r\\n\\r\\nA. More Results\\r\\nIn this section, we show visualization results of video\\r\\nalignment and fine-grained frame retrieval.\\r\\n\\r\\nA.1. Video Alignment\\r\\nGiven two videos recording the similar action or process,\\r\\nthe goal of video alignment is to find the temporal correspondence between them. Firstly, we use our framework\\r\\nto extract the frame-wise representations for two randomly\\r\\nselected videos. Then we compute the cosine similarities\\r\\nbetween the frame-wise representations of two videos and\\r\\nutilize the famous dynamic time warping (DTW) algorithm\\r\\non the similarity matrix to find the best temporal alignment.\\r\\nFigure 5 shows an example from FineGym test set. Please\\r\\nrefer to video demos in our supplementary materials for\\r\\nmore visualization results.\\r\\nWe also randomly select two videos recording the same\\r\\nprocess (or action) from Pouring (or PennAction) dataset,\\r\\nand similarly, we can compute the similarity matrix which is\\r\\nrendered as a heatmap in Figure 6. We observe that the diagonal is highlighted, which means our approach find the favorable alignment between two correlated videos. We also\\r\\n\\r\\nA.2. Fine-grained Frame Retrieval\\r\\nIn Figure 7, we present the visualization results of finegrained frame retrieval on FineGym dataset. To be specific, we feed the video containing the query frames into\\r\\nour CARL framework to generate query features, and similarly, we can extract frame-wise features for the rest videos\\r\\nin the test set. We simply compute the cosine similarity between query features and frame-wise features from candidate videos to obtain top-5 retrieved frames as shown in Figure 7. The retrieved frames have similar semantics with the\\r\\nquery frame, though the appearances, the camera views, and\\r\\nthe backgrounds are different, which suggests our method is\\r\\nrobust to these factors.\\r\\n\\r\\nA.3. Action Localization\\r\\nTo show the potential of our method on large datasets and\\r\\nmore downstream tasks, we optimize the frame-wise features via our self-supervised method on ActivityNet [25].\\r\\nThen we use G-TAD [47] on the top of the features (without fine-tuning) to perform temporal action localization. As\\r\\nshown in Table 12, we use mAP(%) at {0.5, 0.75, 0.95}\\r\\ntIoU thresholds and the average mAP across 10 tIoU levels for evaluation. In contrast to the supervised two-stream\\r\\nmodel [41], our method does not need any video labels\\r\\nwhile achieving better performance.\\r\\n\\r\\nA.4. Compare with Contrastive Baseline\\r\\nWe compare our SCL with the contrastive baseline which\\r\\nonly uses the corresponding frame in the other view as\\r\\n\\r\\n\\x0c\\n\\nMethod\\r\\nG-TAD w. 2stream\\r\\nG-TAD w. ours\\r\\n\\r\\n0.5\\r\\n\\r\\n0.75\\r\\n\\r\\n0.95\\r\\n\\r\\nAverage\\r\\n\\r\\n50.36\\r\\n51.22\\r\\n\\r\\n34.60\\r\\n35.19\\r\\n\\r\\n9.02\\r\\n8.54\\r\\n\\r\\n34.09\\r\\n34.46\\r\\n\\r\\n[6]\\r\\n\\r\\nTable 12. Temporal action localization on ActivityNet v1.3.\\r\\n\\r\\nMethod\\r\\nContrastive baseline\\r\\nSCL (ours)\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\n88.05\\r\\n93.07\\r\\n\\r\\n0.898\\r\\n0.918\\r\\n\\r\\n0.891\\r\\n0.985\\r\\n\\r\\nTable 13. Compare our SCL with contrastive baseline, which uses\\r\\nthe corresponding frame in the other view as the positive sample.\\r\\n\\r\\nTraining Dataset\\r\\nK400\\r\\nK400 → PennAction\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\n91.9\\r\\n93.9\\r\\n\\r\\n0.903\\r\\n0.908\\r\\n\\r\\n0.949\\r\\n0.977\\r\\n\\r\\n[7]\\r\\n\\r\\n[8]\\r\\n\\r\\n[9]\\r\\n\\r\\n[10]\\r\\n\\r\\n[11]\\r\\nTable 14. Our CARL pre-trained on Kinetics-400 shows outstanding transfer ability on PennAction. Fine-tuning the pre-trained\\r\\nmodel on PennAction further boosts the performance.\\r\\n[12]\\r\\n\\r\\nthe positive sample and ignores temporal adjacent frames.\\r\\nAs Table 13 shows, our SCL can more efficiently employ\\r\\nthe sequential information and thus achieves better performance.\\r\\n\\r\\nA.5. Kinetics-400 Pre-training\\r\\nTo show our method can benefit from large-scale datasets\\r\\nwithout any labels, we train our CARL on Kinetics-400 [9].\\r\\nAs Table 14 shows, the frame-wise representations trained\\r\\non Kinetics-400 shows outstanding generalization on PennAction dataset. Moreover, fine-tuning the pre-trained model\\r\\non PennAction by using our CARL further boosts the performance, e.g., + 2% classification improvement.\\r\\n\\r\\n[13]\\r\\n\\r\\n[14]\\r\\n\\r\\n[15]\\r\\n\\r\\n[16]\\r\\n\\r\\nReferences\\r\\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\\r\\nSun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. ArXiv, 2021. 1, 2\\r\\n[2] Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri,\\r\\nWilliam T. Freeman, Michael Rubinstein, Michal Irani, and\\r\\nTali Dekel. Speednet: Learning the speediness in videos. In\\r\\nCVPR, 2020. 3, 6\\r\\n[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\\r\\nspace-time attention all you need for video understanding?\\r\\nArXiv, 2021. 2\\r\\n[4] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. Neural sign language translation. In CVPR, 2018. 1\\r\\n[5] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and\\r\\nRichard Bowden. Sign language transformers: Joint end-\\r\\n\\r\\n[17]\\r\\n\\r\\n[18]\\r\\n\\r\\n[19]\\r\\n\\r\\n[20]\\r\\n\\r\\nto-end sign language recognition and translation. In CVPR,\\r\\n2020. 1\\r\\nKaidi Cao, Jingwei Ji, Zhangjie Cao, C. Chang, and\\r\\nJuan Carlos Niebles. Few-shot video classification via temporal alignment. In CVPR, 2020. 1\\r\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\r\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. ArXiv, 2020. 2, 4\\r\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning\\r\\nof visual features by contrasting cluster assignments. ArXiv,\\r\\n2020. 2\\r\\nJoão Carreira and Andrew Zisserman. Quo vadis, action\\r\\nrecognition? a new model and the kinetics dataset. In CVPR,\\r\\n2017. 1, 2, 4, 10\\r\\nC. Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and\\r\\nJuan Carlos Niebles. D3tw: Discriminative differentiable\\r\\ndynamic time warping for weakly supervised action alignment and segmentation. In CVPR, 2019. 3, 6\\r\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3, 4, 6,\\r\\n8\\r\\nXinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming\\r\\nHe. Improved baselines with momentum contrastive learning. ArXiv, 2020. 2, 3\\r\\nYutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and\\r\\nStephen Lin. A simple multi-modality transfer learning baseline for sign language translation. arXiv preprint\\r\\narXiv:2203.04287, 2022. 1\\r\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\r\\nand Fei-Fei Li. Imagenet: A large-scale hierarchical image\\r\\ndatabase. In CVPR, 2009. 2, 4\\r\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\r\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\r\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\r\\nworth 16x16 words: Transformers for image recognition at\\r\\nscale. In ICLR, 2021. 2, 4\\r\\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre\\r\\nSermanet, and Andrew Zisserman.\\r\\nTemporal cycleconsistency learning. In CVPR, 2019. 1, 2, 3, 4, 5, 6, 7\\r\\nChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\\r\\nKaiming He. Slowfast networks for video recognition. In\\r\\nICCV, 2019. 1, 2, 4\\r\\nChristoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross B. Girshick, and Kaiming He. A large-scale study on unsupervised\\r\\nspatiotemporal representation learning. In CVPR, 2021. 3\\r\\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\\r\\nValentin Haenel, Ingo Fründ, Peter N. Yianilos, Moritz\\r\\nMueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax,\\r\\nand Roland Memisevic. The “something something” video\\r\\ndatabase for learning and evaluating visual common sense.\\r\\nIn ICCV, 2017. 1\\r\\nJean-Bastien Grill, Florian Strub, Florent Altch’e, Corentin\\r\\nTallec, Pierre H. Richemond, Elena Buchatskaya, Carl Do-\\r\\n\\r\\n\\x0c\\n\\n[21]\\r\\n\\r\\n[22]\\r\\n\\r\\n[23]\\r\\n\\r\\n[24]\\r\\n\\r\\n[25]\\r\\n\\r\\n[26]\\r\\n\\r\\n[27]\\r\\n\\r\\n[28]\\r\\n\\r\\n[29]\\r\\n\\r\\n[30]\\r\\n[31]\\r\\n\\r\\n[32]\\r\\n\\r\\n[33]\\r\\n[34]\\r\\n\\r\\n[35]\\r\\n\\r\\n[36]\\r\\n\\r\\nersch, Bernardo Ávila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi\\r\\nMunos, and Michal Valko. Bootstrap your own latent: A new\\r\\napproach to self-supervised learning. ArXiv, 2020. 2, 3, 6\\r\\nIsma Hadji, Konstantinos G. Derpanis, and Allan D. Jepson.\\r\\nRepresentation learning via global temporal alignment and\\r\\ncycle-consistency. In CVPR, 2021. 1, 2, 3, 6\\r\\nTengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In ICCVW,\\r\\n2019. 3\\r\\nSanjay Haresh, Sateesh Kumar, Huseyin Coskun,\\r\\nShahram Najam Syed, Andrey Konin, M. Zeeshan Zia,\\r\\nand Quoc-Huy Tran. Learning by aligning videos in time.\\r\\nIn CVPR, 2021. 1, 2, 3, 5, 6, 7\\r\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\r\\nDeep residual learning for image recognition. In CVPR,\\r\\n2016. 2, 4, 6\\r\\nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\\r\\nand Juan Carlos Niebles. Activitynet: A large-scale video\\r\\nbenchmark for human activity understanding. In CVPR,\\r\\n2015. 2, 9\\r\\nHaofei Kuang, Yi Zhu, Zhi Zhang, Xinyu Li, Joseph Tighe,\\r\\nSören Schwertfeger, C. Stachniss, and Mu Li. Video contrastive learning with global context. ArXiv, 2021. 3\\r\\nHilde Kuehne, Ali Bilgin Arslan, and Thomas Serre. The\\r\\nlanguage of actions: Recovering the syntax and semantics of\\r\\ngoal-directed human activities. In CVPR, 2014. 1, 2\\r\\nKenneth Li, Xiao Sun, Zhirong Wu, Fangyun Wei, and\\r\\nStephen Lin. Towards tokenized human dynamics representation. arXiv preprint arXiv:2111.11433, 2021. 3\\r\\nYuxuan Liu, Abhishek Gupta, P. Abbeel, and Sergey Levine.\\r\\nImitation from observation: Learning to imitate behaviors\\r\\nfrom raw video via context translation. 2018 IEEE International Conference on Robotics and Automation (ICRA),\\r\\n2018. 1\\r\\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\\r\\ndescent with warm restarts. arXiv: Learning, 2017. 6\\r\\nIshan Misra, C. Lawrence Zitnick, and Martial Hebert. Shuffle and learn: Unsupervised learning using temporal order\\r\\nverification. In ECCV, 2016. 3, 6\\r\\nMathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa M. Brown,\\r\\nQuanfu Fan, Dan Gutfreund, Carl Vondrick, and Aude Oliva.\\r\\nMoments in time dataset: One million videos for event understanding. PAMI, 2020. 2\\r\\nDaniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. ArXiv, 2021. 1, 2, 4\\r\\nRui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang,\\r\\nH. Wang, Serge J. Belongie, and Yin Cui. Spatiotemporal\\r\\ncontrastive video representation learning. In CVPR, 2021. 3\\r\\nMarcus Rohrbach, Anna Rohrbach, Michaela Regneri,\\r\\nSikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, and\\r\\nBernt Schiele. Recognizing fine-grained and composite activities using hand-centric features and script data. In IJCV,\\r\\n2015. 1, 2\\r\\nPierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine\\r\\nHsu, Eric Jang, Stefan Schaal, and Sergey Levine. Timecontrastive networks: Self-supervised learning from video.\\r\\n\\r\\n[37]\\r\\n\\r\\n[38]\\r\\n\\r\\n[39]\\r\\n\\r\\n[40]\\r\\n\\r\\n[41]\\r\\n\\r\\n[42]\\r\\n\\r\\n[43]\\r\\n\\r\\n[44]\\r\\n[45]\\r\\n\\r\\n[46]\\r\\n\\r\\n[47]\\r\\n\\r\\n[48]\\r\\n\\r\\n[49]\\r\\n\\r\\n[50]\\r\\n\\r\\n2018 IEEE International Conference on Robotics and Automation (ICRA), 2018. 1, 3, 5, 6, 7\\r\\nDian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A\\r\\nhierarchical video dataset for fine-grained action understanding. In CVPR, 2020. 1, 2, 5\\r\\nGunnar A. Sigurdsson, Gül Varol, X. Wang, Ali Farhadi,\\r\\nIvan Laptev, and Abhinav Gupta. Hollywood in homes:\\r\\nCrowdsourcing data collection for activity understanding. In\\r\\nECCV, 2016. 2\\r\\nKaren Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In\\r\\nNIPS, 2014. 1, 2\\r\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\\r\\nUcf101: A dataset of 101 human actions classes from videos\\r\\nin the wild. ArXiv, 2012. 1, 2\\r\\nDu Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features\\r\\nwith 3d convolutional networks. In ICCV, 2015. 1, 2, 4, 7, 9\\r\\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\\r\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\\r\\nIllia Polosukhin. Attention is all you need. In NIPS, 2017.\\r\\n2, 4, 6\\r\\nLimin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\\r\\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\\r\\nnetworks for action recognition in videos. PAMI, 2019. 2\\r\\nX. Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming\\r\\nHe. Non-local neural networks. In CVPR, 2018. 1, 2\\r\\nFangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen\\r\\nLin. Aligning pretraining for detection via object-level contrastive learning. Advances in Neural Information Processing Systems, 34, 2021. 3\\r\\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\r\\nUnsupervised feature learning via non-parametric instance\\r\\ndiscrimination. In CVPR, 2018. 2\\r\\nMengmeng Xu, Chen Zhao, David S. Rojas, Ali Thabet, and\\r\\nBernard Ghanem. G-tad: Sub-graph localization for temporal action detection. In CVPR, 2020. 9\\r\\nYinghao Xu, Fangyun Wei, Xiao Sun, Ceyuan Yang, Yujun Shen, Bo Dai, Bolei Zhou, and Stephen Lin. Crossmodel pseudo-labeling for semi-supervised action recognition. arXiv preprint arXiv:2112.09690, 2021. 1\\r\\nTing Yao, Yiheng Zhang, Zhaofan Qiu, Yingwei Pan, and\\r\\nTao Mei. Seco: Exploring sequence supervision for unsupervised representation learning. In AAAI, 2021. 3\\r\\nWeiyu Zhang, Menglong Zhu, and Konstantinos G. Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In ICCV, 2013.\\r\\n1, 2, 5\\r\\n\\r\\n\\x0c',\n",
       " \"LiDAR Distillation: Bridging the Beam-Induced\\r\\nDomain Gap for 3D Object Detection\\r\\nYi Wei1,2 , Zibu Wei1 , Yongming Rao1,2 , Jiaxin Li3 , Jie Zhou1,2 , Jiwen Lu1,2⋆\\r\\n1\\r\\n\\r\\nDepartment of Automation, Tsinghua University, China\\r\\nState Key Lab of Intelligent Technologies and Systems, China\\r\\n3\\r\\nGaussian Robotics, China\\r\\ny-wei19@mails.tsinghua.edu.cn,\\r\\nweizb18@mails.tsinghua.edu.cn,raoyongming95@gmail.com\\r\\nlijx1992@gmail.com, jzhou@tsinghua.edu.cn,lujiwen@tsinghua.edu.cn\\r\\n2\\r\\n\\r\\nAbstract. In this paper, we propose the LiDAR Distillation to bridge\\r\\nthe domain gap induced by different LiDAR beams for 3D object detection. In many real-world applications, the LiDAR points used by\\r\\nmass-produced robots and vehicles usually have fewer beams than that\\r\\nin large-scale public datasets. Moreover, as the LiDARs are upgraded\\r\\nto other product models with different beam amount, it becomes challenging to utilize the labeled data captured by previous versions’ highresolution sensors. Despite the recent progress on domain adaptive 3D\\r\\ndetection, most methods struggle to eliminate the beam-induced domain\\r\\ngap. We find that it is essential to align the point cloud density of the\\r\\nsource domain with that of the target domain during the training process. Inspired by this discovery, we propose a progressive framework to\\r\\nmitigate the beam-induced domain shift. In each iteration, we first generate low-beam pseudo LiDAR by downsampling the high-beam point\\r\\nclouds. Then the teacher-student framework is employed to distill rich\\r\\ninformation from the data with more beams. Extensive experiments on\\r\\nWaymo, nuScenes and KITTI datasets with three different LiDAR-based\\r\\ndetectors demonstrate the effectiveness of our LiDAR Distillation. Notably, our approach does not increase any additional computation cost\\r\\nfor inference. Code is available at https://github.com/weiyithu/LiDARDistillation.\\r\\nKeywords: Unsupervised domain adaptation, 3D object detection, Knowledge distillation\\r\\n\\r\\n1\\r\\n\\r\\nIntroduction\\r\\n\\r\\nRecently, LiDAR-based 3D object detection [20,34,35,33,47,58] has attracted\\r\\nmore and more attention due to its crucial role in autonomous driving. While\\r\\nthe great improvement has been made, most of the existing works focus on\\r\\nthe single domain, where the training and testing data are captured with the\\r\\nsame LiDAR sensors. In many real-world scenarios, we can only get low-beam\\r\\n⋆\\r\\n\\r\\nCorresponding author\\r\\n\\r\\n\\x0c\\n\\n2\\r\\n\\r\\nYi Wei et al.\\r\\n\\r\\nFig. 1. The beam-induced domain gap. The high-beam data (source domain) and the\\r\\nlow-beam data (target domain) are from Waymo [36] and nuScenes[2] datasets respectively. We generate pseudo low-beam data by downsampling high-beam data to align\\r\\nthe point cloud density of the source domain to the target domain. The models are\\r\\ntrained on this synthetic data with a teacher-student framework.\\r\\n\\r\\n(e.g. 16 beams) point clouds since high-resolution LiDAR (e.g. 64 beams) is\\r\\nprohibitively expensive. For example, the price of a 64-beam Velodyne LiDAR\\r\\nis even higher than a cleaning robot or an unmanned delivery vehicle. However,\\r\\nsince 3D detectors [20,47,33] cannot simply transfer the knowledge from the\\r\\nhigh-beam data to the low-beam one, it is challenging to make use of largescale datasets [9,36,18] collected by high-resolution sensors. Moreover, with the\\r\\nupdate of product models, the robots or vehicles will be equipped with the\\r\\nLiDAR sensors with different beams. It is unrealistic to collect and annotate\\r\\nmassive data for each kind of product. One solution is to use the highest-beam\\r\\nLiDAR to collect informative training data and adapt them to the low-beam\\r\\nLiDARs. Thus, it is an important direction to bridge the domain gap induced\\r\\nby different LiDAR beams.\\r\\nSince beam-induced domain gap directly comes from the sensors, it is specific\\r\\nand important among many domain-variant factors. Different with researchguided datasets, the training data in industry tends to have similar environments\\r\\nwith that during inference, where the environmental domain gaps such as object\\r\\nsizes, weather conditions are marginal. The main gap comes from the beam\\r\\ndifference of LiDARs used during training and inference. Moreover, although\\r\\nLiDAR Distillation is designed for beam-induced domain gap, we can easily\\r\\ncombine our method with other 3D unsupervised domain adaptation methods\\r\\nto solve general domain gaps.\\r\\nWhile there are several works [23,32,40,45,49,55] that concentrate on the\\r\\nunsupervised domain adaptation (UDA) task in 3D object detection, most of\\r\\nthem aim to address general UDA issues. However, due to the fact that most 3D\\r\\n\\r\\n\\x0c\\n\\nLiDAR Distillation\\r\\n\\r\\n3\\r\\n\\r\\ndetectors are sensitive to point cloud densities, the beam-induced domain gap\\r\\ncannot be handled well with these methods. As mentioned in ST3D [49], it is\\r\\ndifficult for their UDA method to adapt detectors from the data with more beams\\r\\nto the data with fewer beams. Moreover, since the structure of point clouds\\r\\nis totally different from images, the UDA methods [6,60,29,31,30,56,3,44,21,59]\\r\\ndesigned for image tasks are not suitable for 3D object detection.\\r\\nTo address the issue, we propose the LiDAR Distillation to bridge the beaminduced domain gap in 3D object detection task. The key insight is to align the\\r\\npoint cloud density and distill the rich knowledge from the high-beam data in\\r\\na progressive way. First, we downsample high-beam data to pseudo low-beam\\r\\npoint clouds, where downsampling operations are both conducted on beams and\\r\\npoints in each beam. To achieve this goal, we split the point clouds to each beam\\r\\nvia a clustering algorithm. Then, we present a teacher-student pipeline to boost\\r\\nthe performance of the model trained on low-beam pseudo data. Unlike knowledge distillation [15,28,28,4,22] used in model compression, we aim to reduce\\r\\nthe performance gap caused by data compression. Specifically, the teacher and\\r\\nstudent networks have the same structure, and the only difference comes from\\r\\nthe different beams of training data. Initialized by the weights of the teacher\\r\\nmodel, the student network mimics the region of interest (ROI) of bird’s eye\\r\\nview (BEV) feature maps predicted by the teacher model.\\r\\nExperimental results on both cross-dataset [36,2] and single-dataset [9] settings demonstrate the effectiveness of LiDAR Distillation. For cross-dataset\\r\\nadaptation [36,2], although there exists many other kinds of domain gap (e.g.\\r\\nobject sizes), our method still outperforms state-of-the-art methods [49,40].\\r\\nIn addition, our method is complementary to other general UDA approaches\\r\\n[23,32,40,49,55] and we can get more promising results combining LiDAR Distillation with ST3D [49]. To exclude other domain-variant factors, we further conduct single-dataset experiments on KITTI [9] benchmark. The proposed method\\r\\nsignificantly improves the direct transfer baseline with a great margin while the\\r\\ngeneral UDA method ST3D [49] surprisingly degrades the performance.\\r\\n\\r\\n2\\r\\n\\r\\nRelated Work\\r\\n\\r\\nLiDAR-based 3D Object Detection: LiDAR-based 3D object detection\\r\\n[20,34,35,33,47,58,50,25] focuses on the problem of localizing and classifying objects in 3D space. It has attracted increasing attention in recent works due to its\\r\\neager demand in computer vision and robotics. As the pioneer works, [5,48,19]\\r\\ndirectly project point clouds to 2D BEV maps and adopt 2D detection methods\\r\\nto extract features and localize objects. Beyond these works, as a commonly\\r\\nused 3D detector in the industry due to its good trade-off of efficiency and accuracy, PointPillars [20] leverages an encoder to learn the representation of point\\r\\nclouds organized in pillars. Recently, voxel-based methods [47,58,33,8] achieve\\r\\nremarkable performances thanks to the development of 3D sparse convolution\\r\\n[10,7]. As a classical backbone, SECOND [47] investigates an improved sparse\\r\\nconvolution method embedded with voxel feature encoding layers. Combining\\r\\n\\r\\n\\x0c\\n\\n4\\r\\n\\r\\nYi Wei et al.\\r\\n\\r\\nFig. 2. An overview of LiDAR Distillation. Our method aligns the point cloud density\\r\\nof the source domain with that of the target domain in an progressive way. In each\\r\\niteration, we first generate pseudo low-beam data by downsampling high-beam point\\r\\nclouds. Then we employ a teacher-student framework to improve the model trained\\r\\non synthetic low-beam data. Specifically, teacher and student networks are trained on\\r\\nhigh-beam and low-beam data respectively, which have the same architecture. The\\r\\nstudent network is initialized with the pretrained weights of the teacher model. As\\r\\nsummarized in [37], many 3D detectors employ 3D backbones and 2D backbones, and\\r\\npredict dense BEV features. Based on this fact, we conduct the mimicking operation\\r\\non the ROI regions of BEV feature maps.\\r\\n\\r\\nvoxel-based networks with point-based set abstraction via a two-step strategy,\\r\\nPV-RCNN [33] becomes one of the state-of-the-art methods. In our work, we\\r\\nadopt PointPillars [20], SECOND [47] and PV-RCNN [33] to demonstrate the\\r\\neffectiveness of our LiDAR Distillation.\\r\\nUnsupervised Domain Adaptation on Point Clouds: The goal of unsupervised domain adaptation [6,60,29,31,30,56,3,44,21,59,52] is to transfer the model\\r\\ntrained on source domain to unlabeled target domains. Recently, some works\\r\\nbegin to concentrate on the UDA problem in 3D tasks. [26,57] explore objectlevel feature alignment between global features and local features. Among the\\r\\nUDA methods for 3D semantic segmentation [43,17,51], [51] tries to solve the\\r\\ndomain shift induced by different LiDAR sensors. However, their method adopts\\r\\n3D convolutions for scene completion, which will bring huge additional computation cost. Moreover, the complete scenes can be easily used in 3D segmentation\\r\\n\\r\\n\\x0c\\n\\nLiDAR Distillation\\r\\n\\r\\n5\\r\\n\\r\\nbut they are not suitable for 3D object detection. Compared with great development in 3D object detection, only a few methods [23,32,40,45,49,55] have\\r\\nbeen proposed for the UDA of 3D detection. Wang et al.[40] conclude that the\\r\\nkey factor of the domain gap is the difference in car size and they propose SN\\r\\nto normalize the object sizes of the source and target domains. Yang et al.[49]\\r\\npropose ST3D which redesigns the self-training pipeline in 2D UDA methods\\r\\nfor 3D object detection task. While most of these works target at general UDA\\r\\nmethods, we find that they struggle to bridge the beam-induced domain gap.\\r\\nKnowledge Distillation: Knowledge distillation (KD) [15,28,28,4,22] has become one of the most effective techniques for model compression. KD leverages\\r\\nthe teacher-student framework and distills the knowledge from teacher models to\\r\\nimprove the performance of student models. As one of the pioneer works, Hinton\\r\\net al.[15] transfer the knowledge through soft targets. Compared with the KD\\r\\nin classification task [1,14,38,53,54], compressing object detectors [4,22,41,11,39]\\r\\nis more challenging. Chen et al.[4] improve the students by mimicking all components, including intermediate features and soft outputs. Beyond this work, Li\\r\\net al.[22] only mimic the areas sampled from region proposals. Recently, Guo\\r\\net al.[11] point out that features from background regions are also important.\\r\\nDifferent from above methods, we utilize knowledge distillation for data compression (high-beam data to low-beam data) instead of model compression. In\\r\\naddition, few works study the problem of knowledge distillation in 3D object\\r\\ndetection. Due to the differences in backbones, it is not trivial to directly apply\\r\\n2D methods to 3D tasks. In our method, we adopt feature imitation on bird’s\\r\\neye view (BEV) feature maps.\\r\\n\\r\\n3\\r\\n3.1\\r\\n\\r\\nApproach\\r\\nProblem Statement\\r\\n\\r\\nThe goal of unsupervised domain adaptation is to mitigate the domain shift\\r\\nbetween the source domain and the target domain and maximize the performance\\r\\non the target domain. Specifically, in this work, we aim to transfer a 3D object\\r\\ns\\r\\ndetector trained on high-beam source labeled data {(Xis , Yis )}N\\r\\ni=1 to low-beam\\r\\nt Nt\\r\\nunlabeled target domain {Xi }i=1 , where s and t represent source and target\\r\\ndomains respectively. The Ns is the samples number of the source domain while\\r\\nXis and Yis mean the ith source point cloud and its corresponding label. The 3D\\r\\nbounding box label Yis is parameterized by its center location of (cx , cy , cz ), the\\r\\nsize (dx , dy , dz ), and the orientation θ.\\r\\n3.2\\r\\n\\r\\nOverview\\r\\n\\r\\nWe propose the LiDAR Distillation to bridge the domain gap induced by LiDAR\\r\\nbeam difference between source and target domains. Figure 2 shows the pipeline\\r\\nof our method. A key factor of our approach is to progressively generate low-beam\\r\\ns\\r\\npseudo LiDAR {Xim }N\\r\\ni=1 in the source domain, which has the similar density\\r\\n\\r\\n\\x0c\\n\\n6\\r\\n\\r\\nYi Wei et al.\\r\\n\\r\\n(a) Original data\\r\\n\\r\\n(b) Zenith angle distribution\\r\\n\\r\\n(c) Conventional methods\\r\\n\\r\\n(d) Ours\\r\\n\\r\\nFig. 3. Pseudo low-beam data generation. Top row: (a) original 64-beam LiDAR\\r\\npoints. (b) the distribution of zenith angles θ. Bottom row: (c) pseudo 32-beam data\\r\\ngenerated by conventional methods [42,24]. (d) pseudo 32-beam data generated by our\\r\\nclustering method. Although we can see the distinct peaks in the distribution of θ,\\r\\nthere also exists some noise. Conventional methods will produce short broken lines\\r\\nwith the wrong beam labels while our method is more robust and can generate more\\r\\nrealistic data.\\r\\n\\r\\nwith the point clouds of the target domain. The density indicates the density of\\r\\nbeams and point number per beam. Section 3.3 introduces this part in details.\\r\\nThen we leverage the teacher-student framework to improve the performance\\r\\nof the model trained on pseudo low-beam data, which is discussed in Section\\r\\n3.4. Finally, Section 3.5 shows how we progressively distill knowledge from the\\r\\ninformative high-beam real point clouds.\\r\\nWe find that the general UDA method [49] will surprisingly degrade the performance when dealing with the domain shift only caused by LiDAR beams.\\r\\nThe observation indicates that it is hard to directly transfer the knowledge from\\r\\nthe high-beam source domain to the low-beam target domain. Although subsampling high-beam source domain data to low-beam point clouds will lose rich\\r\\ndata knowledge, it guarantees the point cloud density of the source domain is\\r\\nsimilar to that of the target domain, which is more important compared with\\r\\ndata information gain. In this work, we focus on the margin between real highbeam data and generated low-beam data without leveraging the training set in\\r\\nthe target domain. Therefore, our approach can be easily integrated with other\\r\\nUDA methods by substituting the source domain with the generated middle\\r\\ndomain.\\r\\n3.3\\r\\n\\r\\nPseudo Low-beam Data Generation\\r\\n\\r\\nThe objective of this stage is to close the gap of input point cloud densities\\r\\nbetween two domains. An alternative solution is to increase the beams of the\\r\\n\\r\\n\\x0c\\n\\nLiDAR Distillation\\r\\n\\r\\n7\\r\\n\\r\\npoint clouds in the target domain. This can be easily implemented by upsampling\\r\\nthe range images converted from point clouds. However, our experiments show\\r\\nthat the upsampled point clouds have many noisy points that heavily destroy\\r\\nobjects’ features. In this work, we propose to downsample the real high-beam\\r\\ndata in the source domain to realize point cloud density’s alignment. To this end,\\r\\nwe first need to split point clouds into different beams. Although some datasets\\r\\nhave beam annotations, many LiDAR points do not have these labels. Here we\\r\\npresent a clustering algorithm to get the beam label of each point. We assume\\r\\nthe beam number and mean point number in each beam of source and target\\r\\ndomains are Bs , Ps and Bt , Pt respectively.\\r\\nWe first transfer cartesian coordinates (x, y, z) to spherical coordinates:\\r\\n  \\\\begin {aligned} \\\\theta = & \\\\arctan \\\\frac {z}{\\\\sqrt {x^2 + y^2}} \\\\\\\\ \\\\phi = & \\\\arcsin \\\\frac {y}{\\\\sqrt {x^2 + y^2}} \\\\end {aligned} \\r\\n\\r\\n(1)\\r\\n\\r\\nwhere ϕ and θ are azimuth and zenith angles. Figure 3 shows one case in KITTI\\r\\ndataset [9] and the subfigure (b) is the distribution of zenith angles θ. Conventional methods [42,24] assume that beam angles are uniformly distributed\\r\\nin a range (e.g. [−23.6◦ , 3.2◦ ] in KITTI dataset) and assign a beam label to\\r\\neach point according to the distance between its zenith angle and beam angles.\\r\\nHowever, this does not always hold true due to the noise in raw data. To tackle\\r\\nthe problem, we apply K-Means algorithm on zenith angles to find Bs cluster\\r\\ncenters as beam angles. As shown in Figure 3 (c)(d), compared to the conventional methods, our method is more robust to the noise and can generate more\\r\\nrealistic pseudo data. Moreover, from Table 2, we can see that the vertical field\\r\\nof view (the range of beam angles) of datasets are different. Intuitively aligning\\r\\nBs beams to Bt beams is not correct, which will cause different beam densities.\\r\\nWe define the equivalent beams Bt′ to the source domain as follows:\\r\\n  \\\\label {eq:beam} \\\\begin {aligned} B_t' = [\\\\frac {\\\\alpha _s - \\\\beta _s}{\\\\alpha _t - \\\\beta _t} B_t] \\\\end {aligned} \\r\\n\\r\\n(2)\\r\\n\\r\\nwhere [αs , βs ] and [αt , βt ] represent the vertical field of view in source and target\\r\\ndomains. With the beam label of each point, we can easily downsample Bs beams\\r\\nto Bt′ beams. Moreover, we should also align Ps to Pt by sampling points in each\\r\\nbeam. Instead of random sampling, we sort points according to their azimuth\\r\\nangles ϕ and subsample them orderly.\\r\\n3.4\\r\\n\\r\\nDistillation from High-beam Data\\r\\n\\r\\nKnowledge distillation usually employs a teacher-student framework, where the\\r\\nteacher network is a large model while the student network is a small model.\\r\\nThe goal is to improve the performance of the student network by learning the\\r\\nprediction of teacher network. There are two kinds of mimic targets: classification\\r\\nlogits and intermediate feature maps. In object detection task, since feature maps\\r\\n\\r\\n\\x0c\\n\\n8\\r\\n\\r\\nYi Wei et al.\\r\\n\\r\\nare crucial for both object classification and localization, they are the commonly\\r\\nused mimic targets. Formally, regression loss is applied between the feature maps\\r\\npredicted by student and teacher networks:\\r\\n  L_m = \\\\frac {1}{N} \\\\sum _i ||f_t^i - r(f_s^i)||_2 \\r\\n\\r\\n(3)\\r\\n\\r\\nwhere fti and fsi are the teacher and student networks’ feature maps of the ith\\r\\nsample. r is a transformation layer to align the dimensions of feature maps.\\r\\nDuring the mimic training, the weights of teacher are fixed and we only train\\r\\nthe student network.\\r\\nDifferent from knowledge distillation methods aiming at model compression,\\r\\ns\\r\\nour goal is to distill the knowledge from the high-beam data {Xis }N\\r\\ni=1 to the\\r\\nm Ns\\r\\npseudo low-beam data {Xi }i=1 , which can be regarded as a data compression\\r\\nproblem. Unlike model compression, our student and teacher models employ the\\r\\nsame network architecture. The only difference between them is that the input\\r\\nto the student and teacher models are X m and X s respectively. To get higher\\r\\naccuracy, we use the pretrained teacher model’s parameters as the initial weights\\r\\nof the student network.\\r\\nHowever, compared with 2D detectors, the architectures of 3D detectors are\\r\\nmore diverse. To the best of our knowledge, few works study the mimicking\\r\\nmethods for 3D object detection. Moreover, it is not trivial to mimic sparse 3D\\r\\nfeatures since the occupied voxels of student and teacher networks are different.\\r\\nWe observe that many 3D object detection methods [20,5,48,19,47,33,8] project\\r\\n3D features to bird’s eye view (BEV). Based on this fact, we conduct mimicking\\r\\noperation on dense BEV features.\\r\\nAs mentioned in [22,41], the dimension of feature maps can be million magnitude and it is difficult to perform regression of two high-dimension features.\\r\\nThis phenomenon also exists in BEV maps. In addition, many regions on feature\\r\\nmaps have weak responses and not all elements are crucial for final prediction.\\r\\nThe student network should pay more attention to the features in important\\r\\nregions. Here, we propose to mimic the BEV features sampled from the region\\r\\nof interest (ROI). The ROIs contain both positive and negative samples with\\r\\ndifferent ratios and sizes, which are informative and have significant guidance\\r\\neffects. These ROIs generated by the teacher model will force the student network to learn more representative features. For the 3D detectors which do not\\r\\nuse ROIs (e.g. PointPillars [20] and SECOND [47]), we can add a light-weight\\r\\nROI head assigning negative and positive targets during training and remove it\\r\\nduring inference.\\r\\nThe loss function that the student network intends to minimize is defined as\\r\\nfollows:\\r\\n  \\\\begin {aligned} L &= L_{gt} + \\\\lambda L_m^{3D} \\\\\\\\ L_m^{3D} = \\\\frac {1}{N} &\\\\sum _i \\\\frac {1}{M_i} \\\\sum _j ||b_t^{ij} - b_s^{ij}||_2 \\\\end {aligned} \\r\\n\\r\\n(4)\\r\\n\\r\\n\\x0c\\n\\nLiDAR Distillation\\r\\n\\r\\n9\\r\\n\\r\\nwhere bij indicates the jth ROI of the ith sample’s BEV feature maps. N\\r\\nand Mi represent sample number and ROI number in the ith sample. Lgt is\\r\\nthe original objective function for 3D object detection and λ is the mimic loss\\r\\nweight. Note that there is no need for transformation layer r since student and\\r\\nteacher networks have the same architecture.\\r\\n3.5\\r\\n\\r\\nProgressive Knowledge Transfer\\r\\n\\r\\nIf the beam difference between real high-beam data and pseudo low-beam data\\r\\nis too large (e.g. 128 vs 16), the student network cannot learn from the teacher\\r\\nmodel effectively. To solve the issue, we further present a progressive pipeline\\r\\nto distill the knowledge step by step, which is described in Algorithm 1. For\\r\\nexample, if the source domain is 64-beam and the target domain is 16-beam,\\r\\nwe first need to generate pseudo 32-beam data and train a student model on it.\\r\\nThis student model will be used as the teacher network in the next iteration.\\r\\nThen we downsample both the beams and points per beam of 32-beam data to\\r\\n16-beam data. We utilize the student model trained on the generated 16-beam\\r\\ndata as the final model for the target domain. We downsample beams for 2 times\\r\\nin each iteration since lots of LiDAR data contains 2k beams.\\r\\n\\r\\n4\\r\\n4.1\\r\\n\\r\\nExperiments\\r\\nExperimental Setup\\r\\n\\r\\nDatasets: We conduct experiments on three popular autonomous driving datasets:\\r\\nKITTI [9], Waymo [36] and nuScenes [2]. Specifically, cross-dataset experiments\\r\\nare conducted from Waymo to nuScenes while single-dataset adaptation uses\\r\\nKITTI benchmark. Table 2 shows an overview of three datasets. Following\\r\\n[49,40], we adopt the KITTI evaluation metric for all datasets on the commonly\\r\\nused car category (the vehicle category in Waymo). We report the average precision (AP) over 40 recall positions. The IoU thresholds are 0.7 for both the BEV\\r\\nIoUs and 3D IoUs evaluation. The models are evaluated on the validation set of\\r\\neach dataset.\\r\\nImplementation Details: We evaluate our LiDAR Distillation on three commonly used 3D detectors: PointPillars [20], SECOND [47] and PV-RCNN [33].\\r\\nFollowing [49], we also add an extra IoU head to SECOND, which is named as\\r\\nSECOND-IoU. To guarantee the same input format in each dataset, we only use\\r\\n(x, y, z) as the raw points’ features. To fairly compare with ST3D [49], we adopt\\r\\nsame data augmentation without using GT sampling. The data augmentation\\r\\nis performed simultaneously in a low-beam and high-beam point clouds pair.\\r\\nWe utilize 128 ROIs to sample the features on the BEV feature maps and the\\r\\nratio of positive and negative samples of ROIs was 1:1. The mimic loss weight\\r\\nis set to 1 to balance multiple objective functions. We run full training epochs\\r\\nfor each iteration in the progressive knowledge transfer pipeline, i.e. , 80 epochs\\r\\nfor KITTI and 30 epochs for Waymo. Our work is built upon the 3D object\\r\\ndetection codebase OpenPCDet [37].\\r\\n\\r\\n\\x0c\\n\\n10\\r\\n\\r\\nYi Wei et al.\\r\\n\\r\\nAlgorithm 1 Progressive Knowledge Transfer\\r\\ns\\r\\nRequire: Source domain labeled data {(Xis , Yis )}N\\r\\ni=1 . The (equivalent) beam number\\r\\nand mean point number in each beam of source and target domains are Bs , Ps and\\r\\nBt′ , Pt respectively.\\r\\nEnsure: The 3D object detection model for target domain.\\r\\n1: Calculate the iteration n for progressive knowledge transfer: n = ⌊log2 (Bs /Bt′ )⌋\\r\\ns\\r\\n2: Pretrain 3D detector D0 on {(Xis , Yis )}N\\r\\ni=1 .\\r\\n3: for j = 1 to n: do\\r\\nm j Ns\\r\\ns\\r\\n4:\\r\\nUse the method introduced in Section 3.3 to downsample {Xis }N\\r\\ni=1 to {Xi }i=1\\r\\nj\\r\\nwhich have [Bs /2 ] beams. If j == n, also downsample the mean point number\\r\\nper beam Ps to Pt .\\r\\n5:\\r\\nAdopt Dj−1 as the teacher model to train the student model Dj on pseudo\\r\\nm\\r\\ns\\r\\nlow-beam data {(Xi j , Yis )}N\\r\\ni=1 with the distillation method described in Section\\r\\n3.4.\\r\\n6: end for\\r\\n7: Dn is the final model for the target domain.\\r\\n\\r\\nSECOND-IoU\\r\\nAPBEV / AP3D\\r\\nDirect Transfer 32.91 / 17.24\\r\\nSN [40]\\r\\n33.23 / 18.57\\r\\nST3D\\r\\n35.92 / 20.19\\r\\nHegde et al.[12]\\r\\n- / 20.47\\r\\nWaymo → nuScenes\\r\\nUMT [13]\\r\\n35.10 / 21.05\\r\\n3D-CoCo [52]\\r\\n-/Ours\\r\\n40.66 / 22.86\\r\\nOurs (w/ ST3D) 42.04 / 24.50\\r\\nTask\\r\\n\\r\\nMethod\\r\\n\\r\\nPV-RCNN\\r\\nAPBEV / AP3D\\r\\n34.50 / 21.47\\r\\n34.22 / 22.29\\r\\n36.42 / 22.99\\r\\n-/-/-/43.31 / 25.63\\r\\n44.08 / 26.37\\r\\n\\r\\nPointPillars\\r\\nAPBEV / AP3D\\r\\n27.8 / 12.1\\r\\n28.31 / 12.98\\r\\n30.6 / 15.6\\r\\n-/-/33.1 / 20.7\\r\\n40.23 / 19.12\\r\\n40.83/ 20.97\\r\\n\\r\\nTable 1. Results of cross-dataset adaptation from Waymo [36] to nuScenes [2]. Direct\\r\\nTransfer indicates that the model trained on the Waymo dataset is directly tested on\\r\\nnuScenes. We report APBEV and AP3D over 40 recall positions of the car category at\\r\\nIoU = 0.7.\\r\\n\\r\\n4.2\\r\\n\\r\\nCross-dataset Adaptation\\r\\n\\r\\nTable 1 shows the results of cross-dataset experiments. We compare our method\\r\\nwith two state-of-the-art general UDA methods SN [40] and ST3D [49]. The\\r\\nreported numbers of these two methods are from [49]. Although our LiDAR Distillation only aims to bridge the domain gap caused by LiDAR beams and there\\r\\nexist many other domain-variant factors (e.g. car sizes, geography and weather)\\r\\nin two datasets, our method still outperforms other general UDA methods for\\r\\na large margin. The improvements over Direct Transfer baseline are also significant. Moreover, our LiDAR Distillation does not use the training set of the\\r\\ntarget domain and is complementary to other general UDA methods. Take the\\r\\nST3D as an example: we can simply replace the model pretrained on the source\\r\\ndomain with the model pretrained on pseudo low-beam data. From Table 1, we\\r\\ncan see that our method greatly boosts the performance of ST3D (APBEV : 35.92\\r\\n→ 42.04), which demonstrates that point cloud density is the key factor among\\r\\nmany domain-variant factors.\\r\\n\\r\\n\\x0c\\n\\nLiDAR Distillation\\r\\n\\r\\n11\\r\\n\\r\\nDataset LiDAR Type\\r\\nVFOV\\r\\nPoints Per Beam\\r\\nWaymo [36] 64-beam\\r\\n[-17.6◦ , 2.4◦ ]\\r\\n2258\\r\\nKITTI [9]\\r\\n64-beam\\r\\n[-23.6◦ , 3.2◦ ]\\r\\n1863\\r\\nnuScenes [2] 32-beam [-30.0◦ , 10.0◦ ]\\r\\n1084\\r\\nTable 2. Datasets overview. We use version 1.0 of Waymo Open Dataset. VFOV\\r\\nmeans vertical field of view (the range of beam angles). Statistical information is computed from whole dataset.\\r\\n\\r\\n4.3\\r\\n\\r\\nSingle-dataset Adaptation\\r\\n\\r\\nTo decouple the influence of other factors, we further conduct single-dataset\\r\\nadaptation experiments, where the domain gap is only induced by LiDAR beams.\\r\\nIndeed, in many practical applications, the scenarios of training data are usually similar to that during inference. The main domain gap comes from different\\r\\nbeams, which are caused by the upgrading of LiDAR sensors. Therefore, the\\r\\nsingle-dataset setting is also frequently used in real world. Unfortunately, the\\r\\npopular autonomous driving datasets [9,2,36] are captured by only one type of\\r\\nLiDAR sensor. To solve the problem, we build the synthetic low-beam target domain by uniformly downsampling the validation set of KITTI. We adopt widely\\r\\nused 3D detector PointPillars [20] in this experiment.\\r\\nTable 3 shows the results of single-dataset adaptation on KITTI dataset.\\r\\n32∗ and 16∗ mean that we not only reduce LiDAR beams but also subsample\\r\\n1/2 points in each beam. We do not compare with SN since car sizes are the\\r\\nsame in training and validation sets. We can see that our LiDAR Distillation\\r\\nsignificantly improves Direct Transfer baseline. As the difference between point\\r\\ncloud densities increases, the performance gap becomes larger. Surprisingly, we\\r\\nfind that the general UDA method ST3D [49] will degrade the performance. This\\r\\nis mainly due to different point cloud densities in source and target domains,\\r\\nwhich are not friendly to the self-training framework. Although it is hard for 3D\\r\\ndetectors to adapt from high-beam data to the low-beam beam, it is much easier\\r\\nto conduct adaptation in the reverse direction. As shown in Table 4, the model\\r\\ntrained on low-beam source domain surprisingly achieves better performance on\\r\\nhigh-beam target domain. In this case, Direct Transfer baseline is good enough\\r\\nand we do not need domain adaptation method. The results also show that point\\r\\ndensity alignment is not trival since in some cases we may get better results when\\r\\nbeams of source domain and target domain are not equal.\\r\\n4.4\\r\\n\\r\\nAblation Studies\\r\\n\\r\\nIn this section, we conduct experiments to verify the effectiveness of each component. To exclude the effects of other domain variables, all experiments are\\r\\nconducted under the single-dataset setting with PointPillars [20].\\r\\nComponent Analysis of Teacher-student Framework: The experiment\\r\\nuses 32-beam data as the target domain. As shown in Table 5, all components\\r\\ncontribute to the final performance. While high-beam data is informative and\\r\\n\\r\\n\\x0c\\n\\n12\\r\\n\\r\\nYi Wei et al.\\r\\nTarget Domain\\r\\nBeams\\r\\n\\r\\n32\\r\\n\\r\\n32∗\\r\\n\\r\\n16\\r\\n\\r\\n16∗\\r\\n\\r\\nPointPillars\\r\\nMethod\\r\\nDirect Transfer\\r\\nST3D [49]\\r\\nOurs\\r\\nImprovement\\r\\nDirect Transfer\\r\\nST3D [49]\\r\\nOurs\\r\\nImprovement\\r\\nDirect Transfer\\r\\nST3D [49]\\r\\nOurs\\r\\nImprovement\\r\\nDirect Transfer\\r\\nST3D [49]\\r\\nOurs\\r\\nImprovement\\r\\n\\r\\nEasy\\r\\n91.19\\r\\n86.65\\r\\n91.59\\r\\n+0.40\\r\\n88.39\\r\\n83.81\\r\\n90.60\\r\\n+2.21\\r\\n83.11\\r\\n75.49\\r\\n89.98\\r\\n+6.87\\r\\n77.22\\r\\n76.04\\r\\n87.44\\r\\n+10.22\\r\\n\\r\\nAPBEV\\r\\nModerate\\r\\n79.81\\r\\n71.29\\r\\n82.22\\r\\n+2.41\\r\\n73.56\\r\\n67.08\\r\\n79.47\\r\\n+5.91\\r\\n64.91\\r\\n57.58\\r\\n74.32\\r\\n+9.41\\r\\n56.32\\r\\n55.63\\r\\n70.43\\r\\n+14.11\\r\\n\\r\\nHard\\r\\n76.53\\r\\n66.77\\r\\n79.57\\r\\n+3.04\\r\\n68.22\\r\\n62.57\\r\\n76.58\\r\\n+8.36\\r\\n58.32\\r\\n52.33\\r\\n71.54\\r\\n+13.22\\r\\n49.51\\r\\n49.18\\r\\n66.35\\r\\n+16.84\\r\\n\\r\\nAP3D\\r\\nEasy Moderate Hard\\r\\n80.79\\r\\n65.91\\r\\n61.09\\r\\n75.36\\r\\n57.57\\r\\n52.88\\r\\n86.00 70.15 66.86\\r\\n+5.21 +4.24 +5.77\\r\\n74.59\\r\\n57.77\\r\\n51.45\\r\\n71.09\\r\\n53.30\\r\\n48.34\\r\\n82.83 66.96 62.51\\r\\n+8.24 +9.19 +11.06\\r\\n67.64\\r\\n47.48\\r\\n41.41\\r\\n60.36\\r\\n42.40\\r\\n37.93\\r\\n80.21 59.87 55.32\\r\\n+12.57 +12.39 +13.91\\r\\n57.36\\r\\n38.75\\r\\n32.88\\r\\n55.17\\r\\n37.02\\r\\n31.84\\r\\n75.35 55.24 50.96\\r\\n+17.99 +16.49 +18.08\\r\\n\\r\\nTable 3. Results of single-dataset adaptation on KITTI dataset [9]. Direct Transfer\\r\\nindicates that the model trained on the original KITTI training set is directly evaluated\\r\\non the low-beam validation set. For 32∗ and 16∗ , we not only reduce LiDAR beams but\\r\\nalso subsample 1/2 points in each beam. We report APBEV and AP3D over 40 recall\\r\\npositions of the car category at IoU = 0.7. The improvement is calculated according\\r\\nto Direct Transfer baseline. We mark the best result of each target domain in bold.\\r\\n\\r\\nwe can get high-performance model, it is hard to directly transfer the knowledge to the low-beam domain. With the alignment of the point cloud density,\\r\\n3D detectors can learn more robust domain-invariant features in the source domain training set. The distillation from real high-beam data will introduce rich\\r\\nknowledge, which can be maximumly preserved in teacher pretrained weights.\\r\\nTo better understand the effects of point cloud density alignment and knowledge distillation, we further conduct experiments of all synthetic target domains\\r\\nin supplementary material, where we find that knowledge distillation plays an\\r\\nimportant role when the domain gap is small while density alignment is more\\r\\ncrucial when the domain gap is large.\\r\\n\\r\\nSource Domain\\r\\nBeams\\r\\n\\r\\nTarget Domain\\r\\nBeams\\r\\n\\r\\n16∗\\r\\n\\r\\n16∗\\r\\n16\\r\\n32∗\\r\\n32\\r\\n\\r\\nPointPillars\\r\\nEasy\\r\\n85.85\\r\\n86.92\\r\\n88.66\\r\\n87.87\\r\\n\\r\\nAPBEV\\r\\nModerate\\r\\n69.51\\r\\n72.38\\r\\n74.83\\r\\n75.98\\r\\n\\r\\nHard\\r\\n66.57\\r\\n70.09\\r\\n73.00\\r\\n74.63\\r\\n\\r\\nAP3D\\r\\nEasy Moderate Hard\\r\\n70.70 51.24 47.60\\r\\n71.38 51.68 48.02\\r\\n75.29 57.26 54.96\\r\\n73.46 57.46 54.83\\r\\n\\r\\nTable 4. Results of low-beam data to high-beam data adaptation on KITTI dataset\\r\\n[9]. The model trained on the source doamin is directly tested on target domain without\\r\\ndomain adaptation methods.\\r\\n\\r\\n\\x0c\\n\\nLiDAR Distillation\\r\\n\\r\\n13\\r\\n\\r\\ndensity knowledge\\r\\nAP3D\\r\\npretrained\\r\\nalignment distillation\\r\\nEasy Moderate Hard\\r\\n80.79 65.91 61.09\\r\\n✓\\r\\n82.80 67.01 63.82\\r\\n✓\\r\\n✓\\r\\n83.05 68.66 64.79\\r\\n✓\\r\\n✓\\r\\n83.71 69.01 64.97\\r\\n✓\\r\\n✓\\r\\n✓\\r\\n86.00 70.15 66.86\\r\\nTable 5. The ablation study of the teacher-student framework. Density alignment\\r\\nmeans that we use generated low-beam data to train the network. Pretrained indicates\\r\\nthat we employ pretrained teacher weights as the initial weights for the student network.\\r\\n32-beam data is set as the target domain.\\r\\nAP3D\\r\\nEasy Moderate Hard\\r\\n64\\r\\n78.75 58.80 54.47\\r\\n32\\r\\n78.31 58.74 55.62\\r\\n32 progressive 80.21 59.87 55.32\\r\\nteacher model\\r\\n\\r\\nTable 6. The ablation study of progressive\\r\\nknowledge transfer. 64 and 32 represent\\r\\nthat the teacher model is trained without\\r\\nprogressive distillation. The teacher model\\r\\nobtained from our method is named as 32\\r\\nprogressive. The result is evaluated on\\r\\n16-beam data.\\r\\n\\r\\nmimicking regions\\r\\nall\\r\\ngroundtruth\\r\\nROI\\r\\n\\r\\nAP3D\\r\\nEasy Moderate Hard\\r\\n83.83 69.36 64.98\\r\\n84.46 69.99 65.41\\r\\n86.00 70.15 66.86\\r\\n\\r\\nTable 7. The ablation study of mimicking\\r\\nregions. all means that we perform mimicking operation on the whole BEV feature maps while groundtruth indicates\\r\\nthat we only mimic the regions inside\\r\\ngroundtruth bounding boxes.\\r\\n\\r\\nEffectiveness of Progressive Knowledge Transfer: We further investigate\\r\\nthe effectiveness of progressive knowledge transfer. From Table 6, we can see that\\r\\nif the teacher model is trained on original 64-beam data, it cannot provide effective guidance for the student model since there exist large gaps between 64-beam\\r\\nand 16-beam data. Moreover, compared with the teacher model directly trained\\r\\non 32-beam data, the teacher network obtained from the proposed progressive\\r\\npipeline gets better results.\\r\\nAnalysis for Mimicking Regions: The experimental result is shown in Table\\r\\n7. We find that it is not effective to mimic all elements of BEV feature maps. We\\r\\nalso try mimicking the regions inside groundtruth bounding boxes. However, the\\r\\nresults are not promising since some background features are also useful. With a\\r\\ngood balance of foreground and background information, mimicking ROI regions\\r\\nperforms best.\\r\\n4.5\\r\\n\\r\\nFurther Analysis\\r\\n\\r\\nThe Necessity of Point Cloud Density Alignment: In this part, we study\\r\\nwhether the point cloud density is a key factor to bridge the domain gap among\\r\\nmany domain-variant factors. Although the data in nuScenes [2] is 32-beam,\\r\\naccording to the VFOV in Table 2 and Equation 2, its equivalent beam in the\\r\\nsource domain is 16. In addition, the points per beam of Waymo data is about\\r\\ntwice as much as that of nuScenes data. Therefore, 16∗ beam is the most similar\\r\\n\\r\\n\\x0c\\n\\n14\\r\\n\\r\\nYi Wei et al.\\r\\n\\r\\nSource Domain\\r\\nBeams\\r\\n64\\r\\n32\\r\\n16\\r\\n16∗\\r\\n\\r\\nAPBEV /AP3D\\r\\nnuScenes\\r\\nWaymo\\r\\n32.91/17.24 67.72/54.01\\r\\n37.35/20.66 64.67/51.18\\r\\n40.08/21.51 57.04/43.41\\r\\n40.66/22.86 53.23/39.97\\r\\n\\r\\nTable 8. The Necessity of Point Cloud\\r\\nDensity Alignment. We downsample\\r\\nWaymo [36] data to different beams and\\r\\nevaluate models on nuScenes [2] (target domain) and Waymo [36] (source domain). Notice that the equivalent beam of\\r\\nnuScenes is 16∗ . The experiment is conducted with SECOND-IoU backbone.\\r\\n\\r\\nAP3D\\r\\nEasy Moderate Hard\\r\\ninterpolation 74.43 54.56 49.72\\r\\nsuper resolution 79.78 62.58 58.04\\r\\nOurs\\r\\n86.00 70.15 66.86\\r\\nMethod\\r\\n\\r\\nTable 9. Comparison with LiDAR upsampling methods. We first convert point\\r\\nclouds of the target domain to range images. Then we upsample range images\\r\\nwith bilinear interpolation and a lightweight super-resolution network [16], and\\r\\nconvert them back to point clouds. 32beam KITTI data is used as the target\\r\\ndomain.\\r\\n\\r\\npoint cloud density compared with target domain data. Experimental results\\r\\nare shown in Table 8, where evaluation results on target and source domain\\r\\nare illustrated in the first and second columns respectively. We observe that we\\r\\ncan get better results when point cloud densities of source and target domain\\r\\nbecome closer. Indeed, both the beams of Waymo and nuScenes obtain nonuniform distribution and our method does not require them to strictly align.\\r\\nWhile the alignment of the point cloud density will sacrifice the performance on\\r\\nthe source domain due to the information loss, it will bring huge improvements\\r\\non the target domain.\\r\\nComparison with LiDAR Upsampling Methods: An alternative solution\\r\\nfor point cloud density alignment is to upsample the low-beam data in the target domain. Different from scene completion methods [46,27] which utilize 3D\\r\\nconvolutions, we do not need to complete the whole scenes but need to process\\r\\npoint clouds efficiently. To solve the problem, we first convert point clouds to\\r\\nrange images. Then we upsample range images by naive bilinear interpolation\\r\\nand a super-resolution network [16]. As shown in Table 9, these two upsampling\\r\\nmethods fail to boost the accuracy, which demonstrates that it is not trivial to\\r\\naccurately upsample LiDAR points with high efficiency. This is mainly because\\r\\nthere exist some noisy points, which will lead to incorrect object shapes.\\r\\n\\r\\n5\\r\\n\\r\\nConclusion\\r\\n\\r\\nIn this work, we propose the LiDAR Distillation to bridge the domain gap caused\\r\\nby different LiDAR beams in 3D object detection task. Inspired by the discovery\\r\\nthat point cloud density is an important factor for this problem, we first generate\\r\\npseudo low-beam data by downsampling real high-beam LiDAR points. Then\\r\\nknowledge distillation technique is used to progressively boost the performance\\r\\nof the model trained on synthetic low-beam point clouds. Experimental results\\r\\non three popular autonomous driving datasets demonstrate the effectiveness of\\r\\nour method.\\r\\n\\r\\n\\x0c\\n\\nLiDAR Distillation\\r\\n\\r\\n15\\r\\n\\r\\nReferences\\r\\n1. Anil, R., Pereyra, G., Passos, A., Ormandi, R., Dahl, G.E., Hinton, G.E.: Large\\r\\nscale distributed neural network training through online distillation. arXiv preprint\\r\\narXiv:1804.03235 (2018) 5\\r\\n2. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,\\r\\nPan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous\\r\\ndriving. In: CVPR. pp. 11621–11631 (2020) 2, 3, 9, 10, 11, 13, 14, 19\\r\\n3. Chen, C., Zheng, Z., Ding, X., Huang, Y., Dou, Q.: Harmonizing transferability and\\r\\ndiscriminability for adapting object detectors. In: CVPR. pp. 8869–8878 (2020) 3,\\r\\n4\\r\\n4. Chen, G., Choi, W., Yu, X., Han, T., Chandraker, M.: Learning efficient object\\r\\ndetection models with knowledge distillation. NeurIPS (2017) 3, 5\\r\\n5. Chen, X., Ma, H., Wan, J., Li, B., Xia, T.: Multi-view 3d object detection network\\r\\nfor autonomous driving. In: CVPR. pp. 1907–1915 (2017) 3, 8\\r\\n6. Chen, Y., Li, W., Sakaridis, C., Dai, D., Van Gool, L.: Domain adaptive faster\\r\\nr-cnn for object detection in the wild. In: CVPR. pp. 3339–3348 (2018) 3, 4\\r\\n7. Choy, C., Gwak, J., Savarese, S.: 4d spatio-temporal convnets: Minkowski convolutional neural networks. In: CVPR. pp. 3075–3084 (2019) 3\\r\\n8. Deng, J., Shi, S., Li, P., Zhou, W., Zhang, Y., Li, H.: Voxel r-cnn: Towards high\\r\\nperformance voxel-based 3d object detection. In: AAAI. pp. 1201–1209 (2021) 3,\\r\\n8\\r\\n9. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti\\r\\nvision benchmark suite. In: CVPR (2012) 2, 3, 7, 9, 11, 12, 19, 20\\r\\n10. Graham, B., Engelcke, M., Van Der Maaten, L.: 3d semantic segmentation with\\r\\nsubmanifold sparse convolutional networks. In: CVPR. pp. 9224–9232 (2018) 3\\r\\n11. Guo, J., Han, K., Wang, Y., Wu, H., Chen, X., Xu, C., Xu, C.: Distilling object\\r\\ndetectors via decoupled features. In: CVPR. pp. 2154–2164 (2021) 5\\r\\n12. Hegde, D., Patel, V.: Attentive prototypes for source-free unsupervised domain\\r\\nadaptive 3d object detection. arXiv preprint arXiv:2111.15656 (2021) 10\\r\\n13. Hegde, D., Sindagi, V., Kilic, V., Cooper, A.B., Foster, M., Patel, V.: Uncertaintyaware mean teacher for source-free unsupervised domain adaptive 3d object detection. arXiv preprint arXiv:2109.14651 (2021) 10\\r\\n14. Heo, B., Kim, J., Yun, S., Park, H., Kwak, N., Choi, J.Y.: A comprehensive overhaul\\r\\nof feature distillation. In: ICCV. pp. 1921–1930 (2019) 5\\r\\n15. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.\\r\\narXiv preprint arXiv:1503.02531 (2015) 3, 5\\r\\n16. Hui, Z., Gao, X., Yang, Y., Wang, X.: Lightweight image super-resolution with\\r\\ninformation multi-distillation network. In: ACMMM. pp. 2024–2032 (2019) 14\\r\\n17. Jaritz, M., Vu, T.H., Charette, R.d., Wirbel, E., Pérez, P.: xmuda: Cross-modal\\r\\nunsupervised domain adaptation for 3d semantic segmentation. In: CVPR. pp.\\r\\n12605–12614 (2020) 4\\r\\n18. Kesten, R., Usman, M., Houston, J., Pandya, T., Nadhamuni, K., Ferreira, A.,\\r\\nYuan, M., Low, B., Jain, A., Ondruska, P., Omari, S., Shah, S., Kulkarni, A.,\\r\\nKazakova, A., Tao, C., Platinsky, L., Jiang, W., Shet, V.: Lyft level 5 perception\\r\\ndataset 2020. https://level5.lyft.com/dataset/ (2019) 2\\r\\n19. Ku, J., Mozifian, M., Lee, J., Harakeh, A., Waslander, S.L.: Joint 3d proposal\\r\\ngeneration and object detection from view aggregation. In: IROS. pp. 1–8 (2018)\\r\\n3, 8\\r\\n\\r\\n\\x0c\\n\\n16\\r\\n\\r\\nYi Wei et al.\\r\\n\\r\\n20. Lang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars:\\r\\nFast encoders for object detection from point clouds. In: CVPR. pp. 12697–12705\\r\\n(2019) 1, 2, 3, 4, 8, 9, 11\\r\\n21. Li, C., Du, D., Zhang, L., Wen, L., Luo, T., Wu, Y., Zhu, P.: Spatial attention\\r\\npyramid network for unsupervised domain adaptation. In: ECCV. pp. 481–497\\r\\n(2020) 3, 4\\r\\n22. Li, Q., Jin, S., Yan, J.: Mimicking very efficient network for object detection. In:\\r\\nCVPR. pp. 6356–6364 (2017) 3, 5, 8\\r\\n23. Luo, Z., Cai, Z., Zhou, C., Zhang, G., Zhao, H., Yi, S., Lu, S., Li, H., Zhang, S.,\\r\\nLiu, Z.: Unsupervised domain adaptive 3d detection with multi-level consistency.\\r\\nIn: ICCV. pp. 8866–8875 (2021) 2, 3, 5\\r\\n24. Milioto, A., Vizzo, I., Behley, J., Stachniss, C.: Rangenet++: Fast and accurate\\r\\nlidar semantic segmentation. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp. 4213–4220. IEEE (2019) 6, 7\\r\\n25. Qi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J.: Frustum pointnets for 3d object\\r\\ndetection from rgb-d data. In: CVPR. pp. 918–927 (2018) 3\\r\\n26. Qin, C., You, H., Wang, L., Kuo, C.C.J., Fu, Y.: Pointdan: A multi-scale 3d domain\\r\\nadaption network for point cloud representation. In: NeurIPS. pp. 7192–7203 (2019)\\r\\n4\\r\\n27. Roldão, L., de Charette, R., Verroust-Blondet, A.: Lmscnet: Lightweight multiscale\\r\\n3d semantic completion. In: 3DV (2020) 14\\r\\n28. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets:\\r\\nHints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014) 3, 5\\r\\n29. Saito, K., Ushiku, Y., Harada, T.: Asymmetric tri-training for unsupervised domain\\r\\nadaptation. In: ICML. pp. 2988–2997 (2017) 3, 4\\r\\n30. Saito, K., Ushiku, Y., Harada, T., Saenko, K.: Strong-weak distribution alignment\\r\\nfor adaptive object detection. In: CVPR. pp. 6956–6965 (2019) 3, 4\\r\\n31. Saito, K., Watanabe, K., Ushiku, Y., Harada, T.: Maximum classifier discrepancy\\r\\nfor unsupervised domain adaptation. In: CVPR. pp. 3723–3732 (2018) 3, 4\\r\\n32. Saltori, C., Lathuilière, S., Sebe, N., Ricci, E., Galasso, F.: SF-UDA 3D: SourceFree Unsupervised Domain Adaptation for LiDAR-Based 3D Object Detection. In:\\r\\n3DV. pp. 771–780 (2020) 2, 3, 5\\r\\n33. Shi, S., Guo, C., Jiang, L., Wang, Z., Shi, J., Wang, X., Li, H.: Pv-rcnn: Point-voxel\\r\\nfeature set abstraction for 3d object detection. In: CVPR. pp. 10529–10538 (2020)\\r\\n1, 2, 3, 4, 8, 9\\r\\n34. Shi, S., Wang, X., Li, H.: Pointrcnn: 3d object proposal generation and detection\\r\\nfrom point cloud. In: CVPR. pp. 770–779 (2019) 1, 3\\r\\n35. Shi, S., Wang, Z., Shi, J., Wang, X., Li, H.: From points to parts: 3d object detection\\r\\nfrom point cloud with part-aware and part-aggregation network. arXiv preprint\\r\\narXiv:1907.03670 (2019) 1, 3\\r\\n36. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo,\\r\\nJ., Zhou, Y., Chai, Y., Caine, B., et al.: Scalability in perception for autonomous\\r\\ndriving: Waymo open dataset. In: CVPR. pp. 2446–2454 (2020) 2, 3, 9, 10, 11, 14,\\r\\n19\\r\\n37. Team, O.D.: Openpcdet: An open-source toolbox for 3d object detection from point\\r\\nclouds. https://github.com/open-mmlab/OpenPCDet (2020) 4, 9\\r\\n38. Tung, F., Mori, G.: Similarity-preserving knowledge distillation. In: ICCV. pp.\\r\\n1365–1374 (2019) 5\\r\\n39. Wang, T., Yuan, L., Zhang, X., Feng, J.: Distilling object detectors with finegrained feature imitation. In: CVPR. pp. 4933–4942 (2019) 5\\r\\n\\r\\n\\x0c\\n\\nLiDAR Distillation\\r\\n\\r\\n17\\r\\n\\r\\n40. Wang, Y., Chen, X., You, Y., Li, L.E., Hariharan, B., Campbell, M., Weinberger,\\r\\nK.Q., Chao, W.L.: Train in germany, test in the usa: Making 3d object detectors\\r\\ngeneralize. In: CVPR. pp. 11713–11723 (2020) 2, 3, 5, 9, 10\\r\\n41. Wei, Y., Pan, X., Qin, H., Ouyang, W., Yan, J.: Quantization mimic: Towards very\\r\\ntiny cnn for object detection. In: ECCV. pp. 267–283 (2018) 5, 8\\r\\n42. Wu, B., Wan, A., Yue, X., Keutzer, K.: Squeezeseg: Convolutional neural nets with\\r\\nrecurrent crf for real-time road-object segmentation from 3d lidar point cloud. In:\\r\\n2018 IEEE International Conference on Robotics and Automation (ICRA). pp.\\r\\n1887–1893. IEEE (2018) 6, 7\\r\\n43. Wu, B., Zhou, X., Zhao, S., Yue, X., Keutzer, K.: Squeezesegv2: Improved model\\r\\nstructure and unsupervised domain adaptation for road-object segmentation from\\r\\na lidar point cloud. In: ICRA. pp. 4376–4382 (2019) 4\\r\\n44. Xu, C.D., Zhao, X.R., Jin, X., Wei, X.S.: Exploring categorical regularization for\\r\\ndomain adaptive object detection. In: CVPR. pp. 11724–11733 (2020) 3, 4\\r\\n45. Xu, Q., Zhou, Y., Wang, W., Qi, C.R., Anguelov, D.: SPG: Unsupervised Domain\\r\\nAdaptation for 3D Object Detection via Semantic Point Generation. In: ICCV.\\r\\npp. 15446–15456 (2021) 2, 5\\r\\n46. Yan, X., Gao, J., Li, J., Zhang, R., Li, Z., Huang, R., Cui, S.: Sparse single sweep\\r\\nlidar point cloud segmentation via learning contextual shape priors from scene\\r\\ncompletion. In: AAAI. pp. 3101–3109 (2021) 14\\r\\n47. Yan, Y., Mao, Y., Li, B.: Second: Sparsely embedded convolutional detection.\\r\\nSensors 18(10), 3337 (2018) 1, 2, 3, 4, 8, 9, 20\\r\\n48. Yang, B., Luo, W., Urtasun, R.: Pixor: Real-time 3d object detection from point\\r\\nclouds. In: CVPR. pp. 7652–7660 (2018) 3, 8\\r\\n49. Yang, J., Shi, S., Wang, Z., Li, H., Qi, X.: ST3D: Self-training for Unsupervised\\r\\nDomain Adaptation on 3D Object Detection. In: CVPR. pp. 10368–10378 (2021)\\r\\n2, 3, 5, 6, 9, 10, 11, 12, 19\\r\\n50. Yang, Z., Sun, Y., Liu, S., Shen, X., Jia, J.: Std: Sparse-to-dense 3d object detector\\r\\nfor point cloud. In: ICCV. pp. 1951–1960 (2019) 3\\r\\n51. Yi, L., Gong, B., Funkhouser, T.: Complete & label: A domain adaptation approach\\r\\nto semantic segmentation of lidar point clouds. In: CVPR. pp. 15363–15373 (2021)\\r\\n4\\r\\n52. Yihan, Z., Wang, C., Wang, Y., Xu, H., Ye, C., Yang, Z., Ma, C.: Learning transferable features for point cloud detection via 3d contrastive co-training. NeurIPS\\r\\n34 (2021) 4, 10\\r\\n53. Yim, J., Joo, D., Bae, J., Kim, J.: A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In: CVPR. pp. 4133–4141\\r\\n(2017) 5\\r\\n54. You, S., Xu, C., Xu, C., Tao, D.: Learning from multiple teacher networks. In:\\r\\nSIGKDD. pp. 1285–1294 (2017) 5\\r\\n55. Zhang, W., Li, W., Xu, D.: Srdan: Scale-aware and range-aware domain adaptation\\r\\nnetwork for cross-dataset 3d object detection. In: CVPR. pp. 6769–6779 (2021) 2,\\r\\n3, 5\\r\\n56. Zheng, Y., Huang, D., Liu, S., Wang, Y.: Cross-domain object detection through\\r\\ncoarse-to-fine feature adaptation. In: CVPR. pp. 13766–13775 (2020) 3, 4\\r\\n57. Zhou, X., Karpur, A., Gan, C., Luo, L., Huang, Q.: Unsupervised domain adaptation for 3d keypoint estimation via view consistency. In: ECCV. pp. 137–153\\r\\n(2018) 4\\r\\n58. Zhou, Y., Tuzel, O.: Voxelnet: End-to-end learning for point cloud based 3d object\\r\\ndetection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\\r\\nRecognition. pp. 4490–4499 (2018) 1, 3\\r\\n\\r\\n\\x0c\\n\\n18\\r\\n\\r\\nYi Wei et al.\\r\\n\\r\\n59. Zhu, X., Pang, J., Yang, C., Shi, J., Lin, D.: Adapting object detectors via selective\\r\\ncross-domain alignment. In: CVPR. pp. 687–696 (2019) 3, 4\\r\\n60. Zou, Y., Yu, Z., Vijaya Kumar, B., Wang, J.: Unsupervised domain adaptation\\r\\nfor semantic segmentation via class-balanced self-training. In: ECCV. pp. 289–305\\r\\n(2018) 3, 4\\r\\n\\r\\n\\x0c\\n\\nLiDAR Distillation\\r\\n\\r\\n19\\r\\n\\r\\nAppendix\\r\\n\\r\\nTarget\\r\\nDomain\\r\\nBeams\\r\\n32\\r\\n\\r\\n32∗\\r\\n\\r\\n16\\r\\n\\r\\n16∗\\r\\n\\r\\ndensity knowledge\\r\\nalignment distillation\\r\\n✓\\r\\n✓\\r\\n\\r\\n✓\\r\\n\\r\\n✓\\r\\n✓\\r\\n\\r\\n✓\\r\\n\\r\\n✓\\r\\n✓\\r\\n\\r\\n✓\\r\\n\\r\\n✓\\r\\n✓\\r\\n\\r\\nEasy\\r\\n80.79\\r\\n82.80\\r\\n86.00\\r\\n74.59\\r\\n78.74\\r\\n82.83\\r\\n67.64\\r\\n76.12\\r\\n80.21\\r\\n57.36\\r\\n70.70\\r\\n75.35\\r\\n\\r\\nAP3D\\r\\nModerate\\r\\n65.91\\r\\n67.01\\r\\n70.15\\r\\n57.77\\r\\n63.02\\r\\n66.96\\r\\n47.48\\r\\n57.75\\r\\n59.87\\r\\n38.75\\r\\n51.24\\r\\n55.24\\r\\n\\r\\nPointPillars\\r\\nImprovement\\r\\nHard Easy Moderate Hard\\r\\n61.09\\r\\n63.82 +2.01 +1.10 +2.73\\r\\n66.86 +3.20 +3.14 +3.04\\r\\n51.45\\r\\n58.94 +4.15 +5.25 +7.49\\r\\n62.51 +4.09 +3.94 +3.57\\r\\n41.41\\r\\n53.85 +8.48 +10.27 +12.44\\r\\n55.32 +4.09 +2.12 +1.47\\r\\n32.88\\r\\n47.60 +13.34 +12.49 +14.72\\r\\n50.96 +4.65 +4.00 +3.36\\r\\n\\r\\n✓\\r\\nTable 10. Component analysis on all target domains of KITTI dataset [9]. For 32∗\\r\\nand 16∗ , we not only reduce LiDAR beams but also subsample 1/2 points in each\\r\\nbeam. The improvement is calculated in a progressive way.\\r\\n\\r\\nA\\r\\n\\r\\nMore Dataset and Implementation Details\\r\\n\\r\\nAs a popular 3D object detection benchmark, KITTI [9] contains 3,712 training\\r\\nsamples and 3,769 validation samples. Since KITTI dataset only provide the 3D\\r\\nbounding box labels for the objects within the field of view of the front RGB\\r\\ncamera, we remove points outside of the front regions both for training and\\r\\nevaluation. According to the occlusion, truncation and 2D bounding box height,\\r\\nthe objects are divided into three difficulty levels (Easy, Moderate and Hard).\\r\\nThe Waymo Open Dataset [36] is a large-scale dataset, which contains 1000\\r\\nsequences in total, including 798 sequences (158,081 frames) in the training set\\r\\nand 202 sequences (39,987 frames) in the validation set. We used 1.0 version of\\r\\nWaymo Open Dataset. Same to ST3D [49], we also subsampled 1/2 training samples. Note that Waymo data is captured by a 64-beam LiDAR and 4 200-beam\\r\\nshort-range LiDAR. The 200-beam LiDAR only captures data in a limited range\\r\\nand most of points come from 64-beam LiDAR. Thus, we only downsampled the\\r\\npoints from 64-beam LiDAR.\\r\\nThe nuScenes dataset [2] consists of 28,130 training samples and 6,019 validation samples. The point clouds in nuScenes are 32-beam data while the equivalent\\r\\nbeam to Waymo is 16∗ . We only used nuScenes for evaluation.\\r\\nThe voxel size of SECOND and PV-RCNN are set to (0.05m, 0.05m, 0.1m)\\r\\non KITTI dataset and (0.1m, 0.1m, 0.15m) on Waymo and nuScenes datasets.\\r\\nThe models are trained on 8 RTX 2080 Tis.\\r\\n\\r\\n\\x0c\\n\\n20\\r\\n\\r\\nYi Wei et al.\\r\\n\\r\\nFig. 4. Qualitative results of Waymo → nuScenes adaptation task. The green and blue\\r\\nbounding boxes represent detector predictions and groundtruths respectively.\\r\\n\\r\\nB\\r\\n\\r\\nComponent Analysis on Different Target Domains\\r\\n\\r\\nTo better understand the effects of point cloud density alignment and knowledge\\r\\ndistillation, we conduct ablation studies on all synthetic target domains of KITTI\\r\\n[9] dataset. Table 10 shows the results. We observe that both point cloud density\\r\\nalignment and knowledge distillation contribute to the final results. On the one\\r\\nhand, when the domain gap is not large (e.g. 64 → 32), the knowledge distillation\\r\\nplays an more important role. On the other hand, when the domain gap is huge\\r\\n(e.g. 64 → 16∗ ), it is more crucial to align the point cloud density.\\r\\n\\r\\nC\\r\\n\\r\\nQualitative Results\\r\\n\\r\\nTo better illustrate the superiority of our method, we finally provide some visualizations. Figure 4 shows qualitative results of cross-dataset adaptation equipped\\r\\nwith SECOND-IoU [47]. We can see that our method can predict high-quality\\r\\n3D bounding boxes.\\r\\n\\r\\n\\x0c\",\n",
       " 'GIRAFFE HD: A High-Resolution 3D-aware Generative Model\\r\\nYang Xue1 Yuheng Li2 Krishna Kumar Singh3 Yong Jae Lee2\\r\\n1\\r\\nUC Davis 2 UW–Madison 3 Adobe Research\\r\\n\\r\\nGIRAFFE\\r\\n\\r\\nGIRAFFE HD\\r\\n(Ours)\\r\\n\\r\\nFigure 1. Our model, GIRAFFE HD, inherits all of GIRAFFE’s [38] 3D controllability—change in camera viewpoint, object translation,\\r\\nscale, rotation, appearance, shape, and background—while generating higher quality, higher resolution images. Moreover, it achieves\\r\\nbetter foreground-background disentanglement; e.g., when changing the car’s shape (fourth and fifth columns), notice how parts of the\\r\\nroad and building in the background change in the GIRAFFE images, whereas they remain constant in ours.\\r\\n\\r\\nAbstract\\r\\n\\r\\nentanglement and controllability in 2D space (e.g., color\\r\\nand shape changes), their lack of explicit 3D information\\r\\nmakes it difficult to impose 3D-level control over the generated image content. Meanwhile, the recent NeRF [35]\\r\\nbased GANs [5, 38, 43] have shown that explicit modeling\\r\\nof the scene in 3D space conditioned on camera pose can\\r\\nenable effective 3D-level control. However, the computationally expensive nature of 3D representations has limited\\r\\ncurrent 3D-aware generative models from directly learning\\r\\nand rendering images in high resolutions.\\r\\n\\r\\n3D-aware generative models have shown that the introduction of 3D information can lead to more controllable\\r\\nimage generation. In particular, the current state-of-the-art\\r\\nmodel GIRAFFE [38] can control each object’s rotation,\\r\\ntranslation, scale, and scene camera pose without corresponding supervision. However, GIRAFFE only operates\\r\\nwell when the image resolution is low. We propose GIRAFFE HD, a high-resolution 3D-aware generative model\\r\\nthat inherits all of GIRAFFE’s controllable features while\\r\\ngenerating high-quality, high-resolution images (5122 resolution and above). The key idea is to leverage a stylebased neural renderer, and to independently generate the\\r\\nforeground and background to force their disentanglement\\r\\nwhile imposing consistency constraints to stitch them together to composite a coherent final image. We demonstrate\\r\\nstate-of-the-art 3D controllable high-resolution image generation on multiple natural image datasets.\\r\\n\\r\\nGIRAFFE [38] is the current state-of-the-art 3D-aware\\r\\ngenerative model for both image realism and controllability.\\r\\nIt models the foreground and background as two separate\\r\\n3D objects, uses volume rendering to render the combined\\r\\n3D features into low-resolution 2D feature maps, and finally\\r\\nuses a neural renderer to further render the feature maps\\r\\ninto higher resolution images. These design choices enable\\r\\nGIRAFFE to change the background’s appearance independent of the foreground, translate or rotate the foreground\\r\\nobject in 3D, and change the foreground object’s shape and\\r\\ncolor. However, the neural renderer is specifically designed\\r\\nto provide only spatially small refinements in order to avoid\\r\\nentangling global scene properties and losing controllability. Thus, it is significantly less powerful than style-based\\r\\nrenderers, and hence the highest image resolution that GIRAFFE can generate is 2562 .\\r\\n\\r\\n1. Introduction\\r\\nIn image generation, two of the most important objectives are image realism and controllability. Style-based\\r\\nGANs (i.e., StyleGAN [25] and its variants [24, 27]) can\\r\\ngenerate high-resolution, photorealistic images. However,\\r\\nwhile their latent style code design provides a level of dis1\\r\\n\\r\\n\\x0c\\n\\nIn this work, we propose a two-stage style-based 3Daware generative model that inherits all of GIRAFFE’s controllability while generating high-quality, high-resolution\\r\\nimages (up to 10242 resolution); see Fig. 1. Our design\\r\\nis motivated by three key observations when replacing GIRAFFE’s neural renderer with a style-based neural render\\r\\n(based on StyleGAN2 [27]): 1) Using the style renderer\\r\\nto upsample the volume-rendered low-res 2D feature maps\\r\\nleads to high-quality, high-resolution image generations\\r\\nwhile still preserving controllability over the foreground object’s 3D properties (translation, rotation). However, due to\\r\\nits high capacity, the style renderer 2) now gains full control\\r\\nover color as well as some control over shape, and 3) entangles and loses controllability over the foreground and background features (i.e., changing the foreground color/shape\\r\\nalso changes the background color/shape).\\r\\nIn order to regain controllability over the foreground and\\r\\nbackground, we generate them independently using two different style-based renderers and combine them into a coherent image by imposing geometric and photometric compatibility constraints that eliminate inconceivable combinations. Furthermore, to disentangle color and shape, we\\r\\nexploit the well-known emergent properties of StyleGAN,\\r\\nnamely that the early layers control coarse shape, mid layers control fine-grained shape, and later layers control color.\\r\\nSpecifically, we inject the shape code into the 3D feature\\r\\ngenerator as well as the early layers of the style renderer to\\r\\ncontrol shape, and the appearance code into the later layers\\r\\nof the style renderer to control color.\\r\\n\\r\\nenables these models to be trained on unstructured image\\r\\ncollections, as opposed to images from a single scene. Our\\r\\nGIRAFFE HD builds upon this line of work; in particular, it\\r\\nextends GIRAFFE [38] to higher-resolution image domains\\r\\nwhile retaining all its 3D understanding capabilities.\\r\\nHigh-resolution image synthesis. Generative Adversarial Networks (GANs) [4, 15, 41] can generate photorealistic images, and the state-of-the-art for high-resolution image synthesis are style-based GANs [24, 25, 27]. By injecting style codes [21] to the network, these models achieve\\r\\nnot only high-resolution outputs but also some level of feature disentanglement (e.g., pose, shape, lighting). Some\\r\\nrecent work [29, 30] have demonstrated that using StyleGAN2 as a neural renderer can effectively upsample the\\r\\nlow-resolution feature maps produced by another network\\r\\ninto high-resolution images. We leverage the StyleGAN2\\r\\narchitecture as our model’s neural renderer to generate highresolution 2D images (5122 resolution and higher).\\r\\nDisentanglement and controllability. Generative models that learn disentangled representations [2, 8, 13, 18–20,\\r\\n25, 28, 32, 40] provide extra control in their generations,\\r\\nfor example, the ability to control different factors in the\\r\\nscene (e.g., object pose, shape, appearance). However, most\\r\\nmethods only operate in the 2D domain without considering\\r\\nthe 3D structure of the objects/scenes.\\r\\nAmong generative models that learn 3D disentanglement [16, 36–38, 44], GIRAFFE [38] is the current stateof-the-art. It represents a 3D scene as a composition of\\r\\nforeground and background 3D objects, which enables it\\r\\nto disentangle object shape, appearance, position, camera\\r\\nviewpoint, as well as the foreground and background during image synthesis. However, we observe that this disentanglement comes as a trade-off to image quality – replacing GIRAFFE’s low capacity neural renderer with a\\r\\nstyle-based renderer leads to high-resolution synthesis but\\r\\nat the loss of foreground-background disentanglement. Several supervised methods exploit StyleGAN’s style-based\\r\\ndisentanglement properties to control the generation process [1, 39, 49]. However, they have yet to demonstrate accurate foreground-background disentanglement, which suggests that the vanilla StyleGAN architecture has a limitation when it comes to foreground-background disentanglement. The most intuitive and reliable foregroundbackground disentanglement methods are two-stage image\\r\\ngenerators, which generate the foreground and background\\r\\nindependently and use 2D composition to form the final output image [3, 31, 45, 50]. However, these methods fall short\\r\\nin terms of image quality, presumably due to a lack of explicit foreground and background information sharing and\\r\\ncompatibility constraints. Our approach also generates the\\r\\nforeground and background in two separate stages, but it\\r\\nimposes explicit geometric and photometric compatibility\\r\\n\\r\\nContributions. Our approach, GIRAFFE HD, preserves\\r\\nthe 3D controllability of GIRAFFE, including independent\\r\\ncontrol over foreground and background, while generating\\r\\nmuch higher-resolution and higher-quality images (up to\\r\\n10242 vs. GIRAFFE’s 2562 ). We validate our approach on\\r\\nmultiple natural image datasets (CompCar [51], FFHQ [26],\\r\\nAFHQ Cat [12], CelebA-HQ [23], LSUN Church [52])\\r\\nand demonstrate better foreground-background disentanglement and image realism compared to GIRAFFE in\\r\\nhigher resolution domains. Finally, we perform ablation\\r\\nstudies to justify the different design choices for our model.\\r\\n\\r\\n2. Related Work\\r\\n3D-aware image synthesis. In recent years, using implicit neural representations to represent 3D scenes and\\r\\nvolume render into 2D images has shown great potential [9–11, 14, 22, 34, 42, 46]. For example, NeRF [35]\\r\\ncan effectively learn the 3D geometry using multiple images of a scene from different viewpoints and generate\\r\\nnew images from new viewpoints. NeRF-based 3D-aware\\r\\nGANs [5, 6, 38, 43] condition the neural representations on\\r\\nsample noise or appearance/shape codes to represent different 3D scenes with a single network. This improvement also\\r\\n2\\r\\n\\r\\n\\x0c\\n\\nForeground generation\\r\\n\\r\\nForeground refinement and final composition\\r\\n\"#\\r\\n\\r\\n𝑧$\\r\\n𝑇\"#\\r\\n\\r\\n3D feature\\r\\ngenerator\\r\\n\\r\\n\"#\\r\\n\\r\\n𝑧$\\r\\n\\r\\nvolume\\r\\nrender\\r\\n\\r\\n\"#\\r\\n\\r\\n𝑧!\\r\\n\\r\\n2D\\r\\nfeature\\r\\nmaps\\r\\n16x16\\r\\n\\r\\n%#\\r\\n\\r\\n𝑧!\\r\\n2D\\r\\nfeature\\r\\nmaps\\r\\n512x512\\r\\n\\r\\n2D neural\\r\\nrenderer\\r\\n\\r\\nrefine\\r\\nrenderer\\r\\n\\r\\n!+\\r\\n\\r\\n!+\\r\\n\\r\\n𝐼&\\'(\")*$%\\r\\n\\r\\n𝐼\"#\"/\"$%\\r\\n\"#\\r\\n𝑡& ,\\r\\n\\r\\n𝜉\\r\\n\\r\\n𝐼,$(-\\r\\n\\r\\nBackground generation\\r\\n%#\\r\\n\\r\\n𝑧$\\r\\n%#\\r\\n\\r\\n𝑧$\\r\\n\\r\\n3D feature\\r\\ngenerator\\r\\n\\r\\n𝑇%#\\r\\n\\r\\nvolume\\r\\nrender\\r\\n\\r\\n%#\\r\\n\\r\\n𝑧!\\r\\n\\r\\n2D\\r\\nfeature\\r\\nmaps\\r\\n16x16\\r\\n\\r\\n𝐼 !+\\r\\n\\r\\n2D neural\\r\\nrenderer\\r\\n\\r\\n!\\r\\n\\r\\n𝐼.+\\r\\n\\r\\n𝐼 !\"#$%\\r\\n\\r\\nFigure 2. GIRAFFE HD Architecture. Our model independently generates the foreground and background and uses a generated mask to\\r\\ncomposite the final image. The camera pose ξ and foreground object’s z-translation tfz g are shared between the foreground and background\\r\\n3D feature generators to ensure geometric compatibility. To ensure photometric compatibility, the refinement renderer injects environment\\r\\nfg\\r\\ninformation conditioned on background appearance zabg to generate the foreground residual image Iresidual\\r\\n, which is added to the initial\\r\\nfg\\r\\nfg\\r\\noutput image of the foreground 2D neural renderer Iinitial to form the final foreground image I . During evaluation, latent codes are\\r\\nstrategically injected into the 2D neural renderers to ensure disentanglement over appearance and fine-grained shape.\\r\\n\\r\\n3D object representation. GIRAFFE represents the foreground and background objects using two separate MLPs\\r\\nassociated with separate affine transformations T f g and\\r\\nT bg , respectively, sampled from a dataset-dependent distribution T = {s, t, R}, where s, t ∈ R3 are scale and translation parameters, and R ∈ SO(3) is a rotation matrix. The\\r\\naffine transformation T transforms the scene’s world location to the object’s local location for each object:\\r\\n\\r\\nconstraints. This leads to accurate foreground-background\\r\\ndisentanglement in the high-resolution image domain.\\r\\n\\r\\n3. Approach\\r\\nGiven an image collection containing a single object category (e.g., cars), our goal is to learn a 3D-aware image generation model that generates photo-realistic, high-resolution\\r\\nimages while also providing 3D-level control without human supervision. To this end, our architecture builds upon\\r\\nGIRAFFE [38], but replaces its low-capacity neural render\\r\\nwith a StyleGAN2 [27] based renderer, and has two separate\\r\\nparallel streams to generate separate foreground and background images to enforce their disentanglement. We create the final output by combining the foreground and background while imposing compatibility constraints to ensure\\r\\na coherent image; see Fig. 2.\\r\\n\\r\\n  \\\\kappa (x) = R \\\\cdot sE \\\\cdot x + t \\r\\n\\r\\nwhere E is the 3 × 3 identity matrix. This representation\\r\\nenables 3D object-level control.\\r\\nVolume rendering. For a given camera pose ξ, let\\r\\nNs\\r\\n{xj }j=1\\r\\nbe Ns sample points along camera ray d for a given\\r\\npixel. Then\\r\\n  (\\\\sigma _{j}, f_{j}) = F_\\\\theta (\\\\gamma ^{L_x}(\\\\kappa ^{-1}(x_{j})), \\\\gamma ^{L_d}(d), z). \\\\vspace {-1pt} \\r\\n\\r\\n3.1. Background on GIRAFFE\\r\\n\\r\\n(3)\\r\\n\\r\\n2\\r\\n\\r\\nLet δj = ∥xj +1 − xj ∥ denote the distance between\\r\\nneighboring sampled points, αj = 1 − e−σj δj denote the\\r\\nQj−1\\r\\nalpha value for xj , and τj = i=1 1 − αj denote the transmittance along the ray. Pixel feature vector f can then be\\r\\ncomputed using numerical integration:\\r\\n\\r\\nGenerative neural feature fields. GIRAFFE [38] represents a 3D scene with a neural feature field [35, 38], which\\r\\nis a continuous function F that maps a 3D location x ∈ R3\\r\\nand a 2D camera viewing direction d ∈ S2 to a density\\r\\nσ ∈ R+ and an appearance feature f ∈ RMf . It uses an\\r\\nMLP to learn F , and conditions it on z ∼ N (0, I) so that\\r\\neach z corresponds to a different 3D scene:\\r\\n  \\\\vspace {-1pt} F_\\\\theta : (\\\\gamma ^{L_x}(x), \\\\gamma ^{L_d}(d), z) \\\\mapsto (\\\\sigma , f) \\\\vspace {-1pt} \\r\\n\\r\\n(2)\\r\\n\\r\\n  f = {\\\\sum }^{N_s}_{j=1} \\\\tau _{j} \\\\alpha _{j} f_{j} \\r\\n\\r\\n(4)\\r\\n\\r\\nFor efficiency, the rendered feature images are at 162 resolution. The volume-rendered feature map fvol can then be\\r\\nprocessed by a neural renderer (i.e., a convnet) to output the\\r\\nfinal RGB image.\\r\\nNote that in GIRAFFE, the foreground and background’s\\r\\n3D object representations are composed into a single 3D\\r\\n\\r\\n(1)\\r\\n\\r\\nwhere θ indicates the network parameters, γ is a positional\\r\\nencoding [47] which maps the 5D input (x, d) into a higher\\r\\ndimensional space, and Lx and Ld are the positional encoding dimensions of x and d, respectively.\\r\\n3\\r\\n\\r\\n\\x0c\\n\\n3.3. Enforcing foreground-background consistency\\r\\n\\r\\nscene representation and volume-rendered into a single\\r\\n2D feature representation. However, in our approach, we\\r\\nwill independently volume render the foreground and background’s 3D representations, as explained in detail next.\\r\\n\\r\\nIn order to combine the separately generated foreground\\r\\nand background images into a coherent final image, we need\\r\\nto impose geometric and photometric consistency between\\r\\nthe foreground and background objects. Geometric consistency requires the foreground and background objects to\\r\\nobey physical world rules; for example, objects in the same\\r\\nimage have to share the same viewing perspective, or a car\\r\\ncannot be floating in the air. Photometric consistency requires the foreground and background objects to appear to\\r\\nreside in the same environment by sharing the same lighting, hue or saturation, etc. To this end, we devise two mechanisms to satisfy the two consistency requirements: position\\r\\nsharing and environment sharing.\\r\\n\\r\\n3.2. Neural style rendering\\r\\nIn GIRAFFE, the neural renderer is purposely designed\\r\\nto be simple and provide only spatially small refinements\\r\\nto the volume-rendered feature maps, in order to avoid entangling global scene properties and losing controllability.\\r\\nWith its default renderer, the highest resolution it can generate is 2562 .\\r\\nIn order to generate higher-resolution (≥ 5122 ) outputs,\\r\\nwe first replace GIRAFFE’s default neural renderer with\\r\\none based on StyleGAN2 [27]. Specifically, we take all the\\r\\nblocks of StyleGAN2 starting from 162 resolution to convert the volume-rendered 162 resolution 2D feature maps\\r\\nfvol into a higher resolution image I. As in StyleGAN2, we\\r\\nalso use a mapping network to map z ∼ N (0, I) to latent\\r\\ncode w:\\r\\n\\r\\nPosition sharing. We activate position sharing when the\\r\\nbackground contains a ground surface that the foreground\\r\\nrests on (e.g., car on road or church on land). We simplify the problem and assume that the ground can only be a\\r\\nplane surface. With this simplification, simply placing the\\r\\nbottom of the foreground object onto the ground and then\\r\\nsynchronizing the viewing angles for the foreground and\\r\\nbackground object will satisfy the geometric consistency requirement. We perform this by copying the foreground object’s z-translation and view perspective to the background\\r\\nobject’s z-translation and view perspective. In this way, the\\r\\ngenerated background can actively accommodate all foreground objects.\\r\\nFor datasets where the foreground object is not expected\\r\\nto rest on a ground surface (e.g., frontal human faces), we\\r\\nonly synchronize the viewing perspective without sharing\\r\\nthe z-translation between foreground and background.\\r\\n\\r\\n  \\\\psi _\\\\theta :& z \\\\mapsto w \\\\\\\\ \\\\pi ^{render}_\\\\theta :& (f_{vol}, w) \\\\mapsto I\\r\\n(6)\\r\\nWhile our style renderer leads to higher-resolution outputs, we observe several behavioral distinctions compared\\r\\nto GIRAFFE’s default renderer. First, the model now loses\\r\\nits ability to independently control the foreground and background. Second, the 3D representation no longer fully controls the object’s shape. Though it still determines overall\\r\\ncoarse shape, the earlier stages of the style renderer gain\\r\\nfiner level control over the shapes since the 3D representation is volume-rendered to 2D feature maps at a much lower\\r\\nresolution than the final image. Third, the 3D representation\\r\\nalmost does not control color at all. Instead, the control of\\r\\ncolor is transferred to the later stages of the style renderer.\\r\\nThese behaviors resemble those of vanilla StyleGAN2.\\r\\nTo regain independent control over foreground and background, and to better disentangle object color and shape,\\r\\nwe make the following design choices. First, instead of\\r\\ncompositing the scene at the 3D level and then rendering\\r\\nit into a single final 2D image, we first render the foreground and background independently into two 2D images\\r\\nand then perform 2D composition to get the final image.\\r\\nSecond, unlike GIRAFFE, which conditions the 3D representation on the object’s shape code zs as well as its appearance code za , we remove the dependency of each point’s\\r\\nfeature f on za . Instead, during training, we perform style\\r\\nmixing in the style renderer (as described in StyleGAN2)\\r\\nwith ws = ψθ (zs ) and wa = ψθ (za ). During evaluation, we\\r\\ninject ws into the earlier stages and wa into the later stages\\r\\nof the style renderer (we vary the injection index depending on final image resolution). This way of injecting codes\\r\\nenables our model to disentangle color and shape finely.\\r\\n\\r\\nEnvironment sharing. Besides enforcing geometric consistency between the foreground and background objects,\\r\\nwe also need to ensure photometric consistency; i.e., the\\r\\nforeground object should naturally be immersed into the\\r\\nenvironment created by the background. To this end, we\\r\\ndesignate the background appearance latent code wabg to encode the scene environment configuration. Our refinement\\r\\nnetwork consists of several layers of style-based convolufg\\r\\ntions. It takes as input the foreground feature maps fout\\r\\n,\\r\\nwhich are also used to render the initial foreground image,\\r\\nand wabg as the style code, and outputs the foreground image\\r\\nfg\\r\\nresidual Iresidual\\r\\nand foreground object mask Imask :\\r\\n  \\\\pi ^{refine}_\\\\theta : (f_{out}^{fg}, w_a^{bg}) \\\\mapsto (I_{residual}^{fg}, I_{mask}) \\r\\n\\r\\n(7)\\r\\n\\r\\nWe add the foreground image residual to the initial foreground image to get the final foreground image:\\r\\n  I^{fg} = I_{initial}^{fg} + I_{residual}^{fg} \\r\\n\\r\\n(8)\\r\\n\\r\\nWe observe that the initial foreground image already determines the foreground object’s true appearance. The re4\\r\\n\\r\\n\\x0c\\n\\ncontainment loss Lbbox , foreground coverage loss Lcvg , and\\r\\nmask binarization loss Lbin . Lcvg and Lbin are adapted\\r\\nfrom [3]. Since the style neural renderer is very powerful on its own, the three auxiliary losses are necessary to\\r\\nprevent either the foreground or background renderer from\\r\\ngenerating the entire image by itself.\\r\\nSee supp. for the full expression of the loss functions,\\r\\nincluding how the sampled appearance, shape, and camera/transformation latent codes are used.\\r\\n\\r\\nFigure 3. Enforcing Photometric Consistency. First column:\\r\\nfg\\r\\ninitial foreground image Iinitial\\r\\nand mask Imask . First row: forefg\\r\\nground residuals Iresidual ; second row: final images. Notice how\\r\\nfg\\r\\nIresidual\\r\\nchanges based on the background so that the refined foreground I f g becomes more compatible with it.\\r\\n\\r\\nBounding box containment loss. Each randomly sampled foreground affine transformation T determines a 3D\\r\\nbounding box within which the foreground object should reside. After projecting both the 3D foreground object and the\\r\\n3D bounding box to 2D, the 2D foreground object should\\r\\nstill reside in the 2D bounding box. Our bounding box containment loss minimizes the mean of the foreground object\\r\\nmask values that fall outside the 2D bounding box:\\r\\n\\r\\nfinement operation only adjusts the shading/shine of the\\r\\nforeground without altering its true appearance; see Fig. 3.\\r\\n\\r\\n3.4. Compositing the final image\\r\\n\\r\\n  L_{bbox} = \\\\frac {1}{|S|}{\\\\sum }_{i\\\\in S} I_{mask}[i] \\\\cdot (1 - I_{2Dbbox}[i]) \\r\\n\\r\\nFinally, we use the foreground object mask Imask generated by the refinement network to perform alpha composition of the foreground and background images:\\r\\n  I^{final} = (1-I_{mask}) \\\\cdot I^{bg} + I_{mask} \\\\cdot I^{fg} \\\\label {eq:final} \\r\\n\\r\\nwhere S is the set of all pixels in final image. This loss prevents the foreground renderer from generating background\\r\\nfeatures.\\r\\n\\r\\n(9)\\r\\n\\r\\nwhere I f inal is our model’s final generated image.\\r\\nLike GIRAFFE, we can generalize our model to generate multiple foreground objects. To do this, we first render the background and the foregrounds as described previously. We then compute occlusion relations between the\\r\\nforeground objects (by ordering based on their depth i.e.,\\r\\nx-translation). Finally, we recursively perform 2D composition (Eqn. 9) from the furthest to the nearest foreground\\r\\nobject, where in each recursive iteration, the 2D composition result becomes the new background image.\\r\\n\\r\\nForeground coverage loss. This is a hinge loss on the\\r\\nmean mask value to ensure that the foreground is not empty:\\r\\n  L_{cvg} = \\\\max (0, \\\\eta - \\\\frac {1}{|S|}{\\\\sum }_{i\\\\in S} I_{mask}[i]) \\r\\n\\r\\n(12)\\r\\n\\r\\nwhere η is the minimum coverage threshold. This prevents\\r\\nthe background renderer from generating the entire image.\\r\\nMask binarization loss. This loss encourages binarization (i.e., 0 or 1 values) of the mask:\\r\\n\\r\\n3.5. Training\\r\\n\\r\\n  L_{bin} = \\\\frac {1}{|S|}{\\\\sum }_{i\\\\in S} \\\\min (I_{mask}[i]-0, 1-I_{mask}[i]).  (13)\\r\\n\\r\\nDiscriminator. We use the same residual discriminator as\\r\\nStyleGAN2 [27].\\r\\n\\r\\n4. Experiments\\r\\n\\r\\nTraining. During training, we follow [38] and sample latent codes zak , zsk ∼ N , T k ∼ pT and ξ ∼ pξ , where\\r\\nk ∈ {fg, bg}, pξ and pT are uniform distributions over\\r\\ndataset-dependent camera elevation angles and valid object\\r\\ntransformations, respectively.\\r\\n\\r\\nWe evaluate GIRAFFE HD’s 3D controllability, with a\\r\\nfocus on foreground and background disentanglement and\\r\\ntheir geometric/photometric consistency. We also evaluate\\r\\nhow well it generates high-quality, high-resolution images.\\r\\nFinally, we perform ablation studies to evaluate its different\\r\\ncomponents and losses.\\r\\n\\r\\nObjectives. Our overall objective function is:\\r\\n  L = L_{GAN} + \\\\frac {\\\\lambda }{2}L_{R1} + \\\\beta _1 L_{bbox} + \\\\beta _2 L_{cvg} + \\\\beta _3 L_{bin} \\r\\n\\r\\n(11)\\r\\n\\r\\nImp. details. Foreground and background generative neural radiance fields are MLPs with ReLU activations. We\\r\\nuse 8 layers with hidden dimension of 256/64 (foreground/background respectively), density of 1, and feature\\r\\nhead of dimensionality Mf = 256 for the MLPs. We sample Ns = 64 points along each ray and render 2D feature maps at 162 pixels. Both foreground and background’s\\r\\nshape and appearance codes are 256 dimensions. We use 4\\r\\n\\r\\n(10)\\r\\n\\r\\nwhere λ = 10, and β1 , β2 , β3 are dataset specific. To enforce image realism, we use the non-saturating GAN objective LGAN [15] with R1 regularization LR1 [33].\\r\\nIn addition, we employ three auxiliary losses to guide\\r\\n2D foreground-background disentanglement: bounding box\\r\\n5\\r\\n\\r\\n\\x0c\\n\\n(a) Object Appearance\\r\\n\\r\\n(b) Object Shape\\r\\n\\r\\n(c) Camera Elevation\\r\\n\\r\\n(d) Object Depth & Horizontal Translation\\r\\n\\r\\n(e) Vertical Translation\\r\\n\\r\\n(f) Rotation\\r\\n\\r\\n(g) Multiple Objects\\r\\n\\r\\nFigure 4. 3D Controllability. GIRAFFE HD preserves all of GIRAFFE’s 3D controllable features while generating images at significantly\\r\\nhigher quality and resolution. Importantly, compared to GIRAFFE, our background remains more consistent when the foreground changes.\\r\\n\\r\\nDatasets. We evaluate on five high-resolution singleobject real-world datasets used in GIRAFFE [38]: CompCar [51], FFHQ [26], AFHQ Cat [12], CelebA-HQ [23],\\r\\nLSUN Church [52].\\r\\n\\r\\nlayer MLPs to map noise vectors to style renderers’ latent\\r\\ncodes. Refine renderer has 4 style-based convolution layers.\\r\\nWe use a minimum coverage threshold of 0.2. Foreground\\r\\nand background images go through a tanh activation before\\r\\n2D composition. Foreground mask goes through a sigmoid\\r\\nactivation before 2D composition. We use Adam optimizer\\r\\nwith a learning rate of 0.0005 and batch size of 16.\\r\\n\\r\\nMetrics. We use FID [17] to quantify image quality. We\\r\\nuse 20,000 real and fake samples to calculate the FID score\\r\\nin order for a direct comparison to [38].\\r\\nTo quantify foreground-background disentanglement,\\r\\nwe propose the mutual background similarity (MBS) metric. It measures the consistency in background between\\r\\n\\r\\nBaselines. We compare to GIRAFFE [38], pi-GAN [6],\\r\\nGRAF [44], HoloGAN [36], and HoloGAN w/o 3D Conv,\\r\\na HoloGAN variant proposed in [44] for higher resolutions.\\r\\n6\\r\\n\\r\\n\\x0c\\n\\nHoloGAN [36]†\\r\\nw/o 3D Conv [44]†\\r\\nGRAF [44]†\\r\\nGIRAFFE [38]†\\r\\npi-GAN [6]\\r\\nOurs\\r\\n\\r\\nCat\\r\\n\\r\\nCelebA-HQ\\r\\n\\r\\nFFHQ\\r\\n\\r\\nCompCar\\r\\n\\r\\nChurch\\r\\n\\r\\n33.39\\r\\n38.92\\r\\n12.36\\r\\n\\r\\n61\\r\\n33\\r\\n49\\r\\n21\\r\\n36.27\\r\\n8.09\\r\\n\\r\\n192\\r\\n70\\r\\n59\\r\\n32\\r\\n43.19\\r\\n11.93\\r\\n\\r\\n34\\r\\n49\\r\\n95\\r\\n26\\r\\n64.01\\r\\n7.22\\r\\n\\r\\n58\\r\\n66\\r\\n87\\r\\n30\\r\\n56.80\\r\\n10.28\\r\\n\\r\\nGIRAFFE [38]\\r\\nOurs\\r\\n\\r\\nGIRAFFE [38]\\r\\nOurs\\r\\n\\r\\nFFHQ 10242\\r\\n\\r\\n40.81\\r\\n8.36\\r\\n\\r\\n70.08\\r\\n10.13\\r\\n\\r\\nCompCar\\r\\n\\r\\n99.15\\r\\n15.02\\r\\n\\r\\n88.89\\r\\n22.88\\r\\n\\r\\nTable 3. Foreground-Background Disentanglement. We report\\r\\nthe MBS score (↓) for all methods on FFHQ and CompCar at 2562 .\\r\\n\\r\\nTable 1. 2562 Resolution Image Quality. We report the FID\\r\\nscore (↓) for all methods. † scores (except Cat) taken from [38].\\r\\nCompCar 5122\\r\\n\\r\\nFFHQ\\r\\n\\r\\nCompCar 2562\\r\\nw/o position sharing\\r\\nw/o environment sharing\\r\\nfull\\r\\n\\r\\n10.89\\r\\n11.55\\r\\n7.22\\r\\n\\r\\nTable 4. Ablation: Removing Position/Environment Sharing.\\r\\n‘full’ denotes the full GIRAFFE HD model. We report FID (↓).\\r\\n\\r\\nTable 2. 5122 and 10242 Resolution Image Quality. We report\\r\\nthe FID score (↓) for GIRAFFE and GIRAFFE HD.\\r\\n\\r\\n\"#\\r\\n\\r\\nChanging 𝑧!\\r\\n\\r\\n\"#\\r\\n\\r\\nChanging 𝑧$\\r\\n\\r\\nFigure 6. Ablation: Single Style-based Renderer Baseline. Notice how the foreground and background are entangled.\\r\\nFigure 5. GIRAFFE [38] image generations. We show these to\\r\\nenable direct comparison. (More comparisons in the supp.)\\r\\n\\r\\n4.2. Controllable scene generation\\r\\n\\r\\ntwo generated images that are supposed to share the same\\r\\nbackground. A low MBS indicates more consistent background between the pair of images. For each generated image, we randomly sample an operation that should change\\r\\nits foreground (i.e., a combination of change in scale, x,\\r\\ny-translation, rotation, shape, and appearance) without altering the background, then perform that operation to generate a new image. We then use a pretrained DeepLabV3\\r\\nResNet101 [7] semantic segmentation model to compute\\r\\nthe background mask for each image, and multiply the two\\r\\nmasks to get a single mutual background mask. The image pair’s MBS is computed as the fraction of pixels inside\\r\\nthe mutual background area whose RGB value has changed.\\r\\nWe compute the final MBS as the mean of 10,000 image\\r\\npairs’ MBS’s ×102 . Please refer to the supp. for details.\\r\\n\\r\\nIn Fig. 4, we qualitatively demonstrate that our method\\r\\npreserves all of GIRAFFE’s controllable features. For vertical translation, note how our position sharing enables the\\r\\nground to move up with the car. Also, compared to GIRAFFE, our background remains more consistent when the\\r\\nforeground changes, as shown in our lower MBS in Table 3\\r\\n(qualitative comparisons are in Figs. 1 and 5 and supp). This\\r\\nis due to our explicit separation of foreground and background generation. For objects that rest on a ground surface\\r\\n(e.g., cars), our model will also include the object’s shadow\\r\\nas part of the foreground (see Fig. 8 for examples), which\\r\\nis the ideal behavior. However, the DeepLabV3 model [7]\\r\\nthat is used to compute MBS does not segment the shadow\\r\\nas being part of the object, which is why our MBS is higher\\r\\non CompCar than on FFHQ. In Fig. 8, we show comprehensive intermediate and final image generations.\\r\\n\\r\\n4.1. Image generation quality\\r\\n\\r\\n4.3. Ablation studies\\r\\n\\r\\nWe first evaluate the quality of GIRAFFE HD’s generated images. Since it is demonstrated in [38] that GIRAFFE\\r\\ncan reliably operate at 2562 resolution, we start our comparison at 2562 against all baselines (Table 1). We then\\r\\ncompare to GIRAFFE at the higher resolutions of 5122 for\\r\\nCompCar dataset and 10242 for FFHQ dataset (Table 2).\\r\\nOur method outperforms the baselines in terms of image\\r\\nquality by a large margin. This can be attributed largely to\\r\\nour style-based neural renderer, which is able to model finer\\r\\ndetails than GIRAFFE’s low-capacity neural renderer.\\r\\n\\r\\nImportance of two-stage. The most naive method for\\r\\nimproving GIRAFFE’s image quality is to simply replace\\r\\nGIRAFFE’s neural renderer with a single style-based renderer. For this baseline, during training, we use style mixing of zsf g and zaf g as latent codes to the renderer. During evaluation, we still inject zsf g into the earlier levels\\r\\nand zaf g into the later levels to ensure disentanglement between fine-grained shape and appearance. However, we\\r\\nobserve that this single renderer baseline loses foregroundbackground disentanglement (Fig. 6). Even though the car’s\\r\\n7\\r\\n\\r\\n\\x0c\\n\\nw/o bounding box\\r\\ncontainment loss\\r\\n\\r\\n𝐼\\r\\n\\r\\nw/o foreground\\r\\ncoverage loss\\r\\n\\r\\n𝐼!\"#$\\r\\n\\r\\n𝐼 %&\\r\\n\\r\\nfull\\r\\n\\r\\n𝐼!\"#$\\r\\n\\r\\n𝐼 %&\\r\\n\\r\\n𝐼\\'&\\r\\n\\r\\n𝐼\\r\\n\\r\\nFigure 7. Ablation: Removing Auxiliary Losses. Even after\\r\\njust 4000 training iterations, the baselines that lack bounding box\\r\\ncontainment or foreground coverage loss generate all 1 or 0 masks.\\r\\n\\r\\n𝐼\\'&\\r\\n\\r\\n%&\\r\\n\\r\\n𝐼()#*+,\"-\\r\\n\\r\\n%&\\r\\n\\r\\n𝐼*.*/*\"-\\r\\n\\r\\nLSUN Church\\r\\n\\r\\ncolor remains the same when changing its shape, the background’s shape changes as well. Similarly, even though the\\r\\ncar’s shape remains the same when changing its color, the\\r\\nbackground’s color also changes. Even though the foreground and background are disentangled at the 3D feature level, since a single style-based renderer cannot separately control foreground and background, the foregroundbackground disentanglement is lost in the final 2D image.\\r\\n\\r\\nCelebA-HQ\\r\\n\\r\\nAFHQ Cat\\r\\n\\r\\nCompCar\\r\\n\\r\\nFigure 8. Comprehensive Outputs. We show all of GIRAFFE\\r\\nHD’s intermediate and final output images for different datasets.\\r\\n\\r\\ntions when training a single model on multiple categories\\r\\n(e.g., cats, dogs, wildlife). Although the generated images\\r\\nare still high quality, there can be incompatible foreground\\r\\nand background combinations. Training a single model that\\r\\nworks well on multiple datasets would be an interesting avenue for future work.\\r\\nThird, our current architecture cannot handle “interlocking” object relationships, i.e., there exist some rays along\\r\\nwhich an object appears in front of another object and also\\r\\nsome rays behind. Our model works by composing an image with 2D layers, and the layer masks (object masks) for\\r\\nthe composition are generated by considering only the object itself and not other objects in the scene. This can generally hold for most real-world domains. However, when\\r\\nobjects “interlock”, the objects’ masks need to take into account other objects’ 3D geometries. We think that a module\\r\\nthat renders object masks based on 3D occlusion reasoning\\r\\ncould be a possible extension to address this problem.\\r\\n\\r\\nImportance of foreground-background consistency enforcement. Table 4 shows that removing either position or\\r\\nenvironment sharing hurts the model’s FID, as final images\\r\\nwhose foreground-background combinations are incompatible in geometry/photometry can be generated.\\r\\nImportance of auxiliary losses. In Fig. 7, we show our\\r\\nmodel’s 642 renderings on FFHQ after 4000 training iterations, in three configurations. Without the bounding box\\r\\ncontainment loss the foreground branch generates the entire\\r\\nimage, and without the foreground coverage loss the background generates the entire image. Hence these two losses\\r\\nare critical for foreground-background disentanglement.\\r\\n\\r\\n5. Discussion and Conclusion\\r\\n\\r\\nBroader Impact. There are many possible applications\\r\\nfor controllabe image generation including those in the entertainment and design industry. For example, it could enhance the productivity of designers by allowing them to use\\r\\nsuch tools to control each object in the scene independently\\r\\nwhen creating new visual content. Since our approach does\\r\\nnot require supervision apart from having a collection of images of the same object, it is easily scalable to many different categories. However, there could also be potential misuses, such as creating fake content to fool law enforcement\\r\\nor to spread misinformation on social media. Recent work\\r\\non models that can detect fake images (e.g., [48]) could potentially be useful to prevent such unethical applications.\\r\\n\\r\\nWe proposed GIRAFFE HD, a high-resolution 3D-aware\\r\\ngenerative model that inherits all of GIRAFFE’s [38] 3D\\r\\ncontrollable features while generating high-quality, highresolution images.\\r\\nLimitations. First, we notice our model sometimes lacks\\r\\n3D consistency. For example, when trained from scratch on\\r\\nthe CompCar dataset, our model struggles to perform full\\r\\n360 rotation. Instead, some shape codes correspond to front\\r\\nfacing cars while others correspond to back facing cars, and\\r\\neach can only perform 180 rotation, even though the underlying 3D model has rotated 360 degrees. However, when\\r\\nwe initialize the 3D feature generator with the weights of\\r\\na pretrained GIRAFFE 3D feature generator and continue\\r\\ntraining, the model is then able to perform full 360 rotation.\\r\\nSecond, our current model (as well as GIRAFFE) cannot handle cross-domain foreground-background correla-\\r\\n\\r\\nAcknowledgements. This work was supported in part by\\r\\na Sony Focused Research Award and NSF CAREER IIS2150012. We thank the anonymous reviewers for their constructive comments.\\r\\n8\\r\\n\\r\\n\\x0c\\n\\nReferences\\r\\n[1] Rameen Abdal, Peihao Zhu, Niloy Mitra, and Peter Wonka.\\r\\nStyleflow: Attribute-conditioned exploration of stylegangenerated images using conditional continuous normalizing\\r\\nflows. ACM Transactions on Graphics (TOG), 40, 2021. 2\\r\\n[2] Y. Bengio, A. Courville, and P. Vincent. Representation\\r\\nlearning: A review and new perspectives. TPAMI, 2013. 2\\r\\n[3] Adam Bielski and Paolo Favaro. Emergence of object segmentation in perturbed generative models. In NeurIPS, 2019.\\r\\n2, 5\\r\\n[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\\r\\nscale GAN training for high fidelity natural image synthesis.\\r\\nIn ICLR, 2019. 2\\r\\n[5] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and\\r\\nGordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In CVPR,\\r\\n2021. 1, 2\\r\\n[6] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\\r\\nand Gordon Wetzstein. pi-gan: Periodic implicit generative\\r\\nadversarial networks for 3d-aware image synthesis. In Proceedings of the IEEE/CVF conference on computer vision\\r\\nand pattern recognition, pages 5799–5809, 2021. 2, 6, 7\\r\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\r\\nHartwig Adam. Rethinking atrous convolution for semantic\\r\\nimage segmentation. 2017. 7, 11\\r\\n[8] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya\\r\\nSutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative\\r\\nadversarial nets. In NeurIPS, 2016. 2\\r\\n[9] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In CVPR, 2021. 2\\r\\n[10] Zhiqin Chen and Hao Zhang. Learning implicit fields for\\r\\ngenerative shape modeling. In CVPR, 2019. 2\\r\\n[11] Julian Chibane, Aymen Mir, and Gerard Pons-Moll. Neural\\r\\nunsigned distance fields for implicit function learning. In\\r\\nNeurIPS, 2020. 2\\r\\n[12] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.\\r\\nStargan v2: Diverse image synthesis for multiple domains.\\r\\nIn CVPR, 2020. 2, 6, 11, 12, 14, 16, 18, 20, 24, 28\\r\\n[13] Emily L Denton and vighnesh Birodkar. Unsupervised learning of disentangled representations from video. In NeurIPS,\\r\\n2017. 2\\r\\n[14] Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna,\\r\\nWilliam T Freeman, and Thomas Funkhouser. Learning\\r\\nshape templates with structured implicit functions. In ICCV,\\r\\n2019. 2\\r\\n[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\r\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\r\\nYoshua Bengio. Generative adversarial nets. In NeurIPS,\\r\\n2014. 2, 5\\r\\n[16] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escaping plato’s cave: 3d shape from adversarial rendering. In\\r\\nCVPR, 2019. 2\\r\\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\\r\\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\\r\\n\\r\\n[18]\\r\\n\\r\\n[19]\\r\\n[20]\\r\\n\\r\\n[21]\\r\\n\\r\\n[22]\\r\\n\\r\\n[23]\\r\\n\\r\\n[24]\\r\\n\\r\\n[25]\\r\\n\\r\\n[26]\\r\\n\\r\\n[27]\\r\\n\\r\\n[28]\\r\\n\\r\\n[29]\\r\\n\\r\\n[30]\\r\\n\\r\\n[31]\\r\\n\\r\\n[32]\\r\\n\\r\\n[33]\\r\\n\\r\\n[34]\\r\\n\\r\\n9\\r\\n\\r\\ntwo time-scale update rule converge to a local nash equilibrium. 2017. 6\\r\\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,\\r\\nXavier Glorot, Matthew Botvinick, Shakir Mohamed, and\\r\\nAlexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In ICLR,\\r\\n2017. 2\\r\\nGeoffrey E. Hinton, Alex Krizhevsky, and Sida D. Wang.\\r\\nTransforming auto-encoders. In ICANN, 2011. 2\\r\\nQiyang Hu, Attila Szabó, Tiziano Portenier, Paolo Favaro,\\r\\nand Matthias Zwicker. Disentangling factors of variation by\\r\\nmixing them. In CVPR, 2018. 2\\r\\nXun Huang and Serge Belongie. Arbitrary style transfer in\\r\\nreal-time with adaptive instance normalization. In ICCV,\\r\\n2017. 2\\r\\nChiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei\\r\\nHuang, Matthias Niessner, and Thomas A. Funkhouser. Local implicit grid representations for 3d scenes. In CVPR,\\r\\n2020. 2\\r\\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\\r\\nProgressive growing of gans for improved quality, stability,\\r\\nand variation. In ICLR, 2018. 2, 6, 11, 12\\r\\nTero Karras, Miika Aittala, Samuli Laine, Erik Härkönen,\\r\\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\\r\\ngenerative adversarial networks. In NeurIPS, 2021. 1, 2\\r\\nTero Karras, Samuli Laine, and Timo Aila. A style-based\\r\\ngenerator architecture for generative adversarial networks. In\\r\\nCVPR, 2018. 1, 2\\r\\nTero Karras, Samuli Laine, and Timo Aila. A style-based\\r\\ngenerator architecture for generative adversarial networks. In\\r\\nCVPR, 2019. 2, 6, 11, 12, 13, 15, 17, 19, 23, 27\\r\\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\\r\\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\\r\\nthe image quality of StyleGAN. In CVPR, 2020. 1, 2, 3, 4, 5\\r\\nWonkwang Lee, Donggyun Kim, Seunghoon Hong, and\\r\\nHonglak Lee. High-fidelity synthesis with disentangled representation. In ECCV, 2020. 2\\r\\nKathleen M Lewis, Srivatsan Varadharajan, and Ira\\r\\nKemelmacher-Shlizerman. Tryongan: Body-aware try-on\\r\\nvia layered interpolation. SIGGRAPH, 40(4), 2021. 2\\r\\nYuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae\\r\\nLee, and Krishna Kumar Singh. Collaging class-specific\\r\\ngans for semantic image synthesis. ICCV, 2021. 2\\r\\nYuheng Li, Krishna Kumar Singh, Utkarsh Ojha, and\\r\\nYong Jae Lee. Mixnmatch: Multifactor disentanglement and\\r\\nencoding for conditional image generation. In CVPR, 2020.\\r\\n2\\r\\nYuheng Li, Krishna Kumar Singh, Yang Xue, and Yong Jae\\r\\nLee. Partgan: Weakly-supervised part decomposition for image generation and segmentation. In BMVC, 2021. 2\\r\\nLars Mescheder, Andreas Geiger, and Sebastian Nowozin.\\r\\nWhich training methods for gans do actually converge? In\\r\\nICML, 2018. 5\\r\\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks:\\r\\nLearning 3d reconstruction in function space. In CVPR,\\r\\n2019. 2\\r\\n\\r\\n\\x0c\\n\\n[51] Linjie Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang.\\r\\nA large-scale car dataset for fine-grained categorization and\\r\\nverification. In CVPR, 2015. 2, 6, 11, 12, 13, 15, 17, 19, 21,\\r\\n22, 26\\r\\n[52] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image\\r\\ndataset using deep learning with humans in the loop. arXiv,\\r\\nabs/1506.03365, 2015. 2, 6, 11, 12, 14, 16, 18, 20, 21, 25,\\r\\n29\\r\\n[53] Weiwei Zhang, Jian Sun, and Xiaoou Tang. Cat head detection - how to effectively exploit shape and texture features.\\r\\nIn ECCV, 2008. 11\\r\\n\\r\\n[35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\\r\\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\r\\nRepresenting scenes as neural radiance fields for view synthesis. In ECCV, 2020. 1, 2, 3\\r\\n[36] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\\r\\nRichardt, and Yong-Liang Yang. Hologan: Unsupervised\\r\\nlearning of 3d representations from natural images. In ICCV,\\r\\n2019. 2, 6, 7\\r\\n[37] Thu Nguyen-Phuoc, Christian Richardt, Long Mai, YongLiang Yang, and Niloy Mitra. Blockgan: Learning 3d objectaware scene representations from unlabelled images. In\\r\\nNeurIPS, 2020. 2\\r\\n[38] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields.\\r\\nIn CVPR, 2021. 1, 2, 3, 5, 6, 7, 8, 26, 27, 28, 29\\r\\n[39] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\\r\\nand Dani Lischinski. Styleclip: Text-driven manipulation of\\r\\nstylegan imagery. In CVPR, 2021. 2\\r\\n[40] William S. Peebles, John Peebles, Jun-Yan Zhu, Alexei A.\\r\\nEfros, and Antonio Torralba. The hessian penalty: A weak\\r\\nprior for unsupervised disentanglement. In ECCV, 2020. 2\\r\\n[41] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR, 2016. 2\\r\\n[42] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned\\r\\nimplicit function for high-resolution clothed human digitization. In ICCV, 2019. 2\\r\\n[43] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\\r\\nGeiger. Graf: Generative radiance fields for 3d-aware image\\r\\nsynthesis. In NeurIPS, 2020. 1, 2\\r\\n[44] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\\r\\nGeiger. Graf: Generative radiance fields for 3d-aware image\\r\\nsynthesis. 2020. 2, 6, 7\\r\\n[45] Krishna Kumar Singh, Utkarsh Ojha, and Yong Jae Lee.\\r\\nFinegan: Unsupervised hierarchical disentanglement for\\r\\nfine-grained object generation and discovery. In CVPR,\\r\\n2019. 2\\r\\n[46] Vincent Sitzmann, Julien N.P. Martel, Alexander W.\\r\\nBergman, David B. Lindell, and Gordon Wetzstein. Implicit\\r\\nneural representations with periodic activation functions. In\\r\\nNeurIPS, 2020. 2\\r\\n[47] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\\r\\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In NeurIPS, 2020. 3\\r\\n[48] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\\r\\nOwens, and Alexei A. Efros. Cnn-generated images are surprisingly easy to spot... for now. In CVPR, 2020. 8\\r\\n[49] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace\\r\\nanalysis: Disentangled controls for stylegan image generation. In CVPR, 2021. 2\\r\\n[50] Jianwei Yang, Anitha Kannan, Dhruv Batra, and Devi\\r\\nParikh. Lr-gan: Layered recursive generative adversarial networks for image generation. ICLR, 2017. 2\\r\\n\\r\\n10\\r\\n\\r\\n\\x0c\\n\\nAppendix\\r\\n\\r\\nHD (ours) on FFHQ [26] and CompCar [51] datasets. For\\r\\nGIRAFFE HD on FFHQ, the mutual background difference\\r\\nmainly comes from the imprecision of the segmentation (as\\r\\nDeepLabV3 cannot properly segment thin, floating hair).\\r\\nFor GIRAFFE HD on CompCar, the mutual background\\r\\ndifference mainly comes from the segmentor not including\\r\\nthe car’s shadow as part of the foreground.\\r\\n\\r\\nA. Full Loss Expression\\r\\nFor a given generator-discriminator pair {G, D}, the\\r\\noverall objective function can be formalized as\\r\\n  \\\\begin {aligned} &L(G,D)= \\\\\\\\ &\\\\mathbb {E}_{z_a^{k}, z_s^{k} \\\\sim \\\\mathcal {N}, \\\\xi \\\\sim p_\\\\xi , T^k \\\\sim p_T}[f(D(G(\\\\{z_a^{k}, z_s^{k}, T^k\\\\}_k, \\\\xi )))]\\\\\\\\ &+ \\\\mathbb {E}_{I \\\\sim p_D}[f(-D(I))- \\\\frac {\\\\lambda }{2} \\\\Vert \\\\nabla D(I) \\\\Vert ^2]\\\\\\\\ &+ \\\\beta _1 L_{bbox} + \\\\beta _2 L_{cvg} + \\\\beta _3 L_{bin} \\\\end {aligned} \\r\\n\\r\\nC. Dataset Details\\r\\n\\r\\n(14)\\r\\n\\r\\nDataset parameters. We report the dataset-dependent\\r\\ncamera elevation angle and valid object transformation parameters used for all the datasets in Table 5. We use the\\r\\nsame dataset parameters as GIRAFFE for CompCar, FFHQ,\\r\\nLSUN Church and CelebA-HQ datasets (except for CompCar’s vertical translation). Since GIRAFFE was not evaluated on AFHQ Cat, we use the same dataset parameters\\r\\nGIRAFFE uses for Cats [53].\\r\\n\\r\\nwhere f (t) = − log(1+exp(−t)), λ = 10, pD indicates the\\r\\ndata distribution, and β1 , β2 , β3 are dataset specific. Lbbox ,\\r\\nLcvg , and Lbin are as defined in the main paper.\\r\\n\\r\\nB. Mutual Background Similarity (MBS) Details\\r\\n\\r\\nD. Additional Qualitative Results\\r\\n\\r\\nWe denote the generator to evaluate as G, which takes as\\r\\ninput randomly sampled foreground parameters Pf g ∼ pf g\\r\\nand background parameters Pbg ∼ pbg to generate an image I. We denote a pretrained semantic segmentation model\\r\\nDeepLabV3 ResNet101 [7] as R which takes an image I\\r\\nand outputs the semantic prediction map for I, which can\\r\\nthen be converted into the background mask M . We compute the mutual background similarity (MBS) by first randomly sampling an image I1 = G(Pf g1 , Pbg ), then generating another image by sampling another Pf g2 ∼ pf g\\r\\nwhile keeping Pbg fixed, I2 = G(Pf g2 , Pbg ). Then we\\r\\ncompute the background masks for the two images M1 =\\r\\nR(I1 ), M2 = R(I2 ) and the mask for the two images’\\r\\nmutual background area can be computed as Mmultbg =\\r\\nM1 · M2 . We define that a pixel’s RGB value has changed if\\r\\none or more channels of the pixel’s RGB value has changed\\r\\nover some small threshold η. Then the total number of pixels inside the mutual background area whose RGB value\\r\\nhas changed is computed as\\r\\n  N = {\\\\sum }_{i\\\\in M_{multbg}} \\\\delta \\r\\n\\r\\nIn Figs. 11 to 31, we show additional qualitative results\\r\\non controllable scene generation on four datasets: CompCar [51], FFHQ [26], AFHQ Cat [12], LSUN Church [52].\\r\\nSince the results on CelebA-HQ [23] are very similar to\\r\\nthose on FFHQ, we do not show the CelebA-HQ results\\r\\nhere. We also include GIRAFFE samples on the four\\r\\ndatasets to enable direct comparison with our method. We\\r\\nshow the highest resolution models that we’ve trained for\\r\\neach dataset: CompCar at 5122 , FFHQ at 10242 , AFHQ\\r\\nCat at 2562 , and LSUN Church at 2562 .\\r\\n\\r\\n(15)\\r\\n\\r\\n  \\\\text {where } \\\\delta = \\\\begin {cases} 0,& \\\\text {if } \\\\eta > |I_1[i][c]-I_2[i][c]| , c\\\\in \\\\{R, G, B\\\\}\\\\\\\\ 1, & \\\\text {otherwise} \\\\end {cases} \\r\\nThe image is normalized to [0,1] before feeding into R, and\\r\\n1\\r\\nη is set to be 255\\r\\n. Then the MBS for image pair {I1 , I2 } is\\r\\n  MBS = \\\\frac {N}{|M_{multbg}|} \\\\times 100 \\r\\n\\r\\n(16)\\r\\n\\r\\nIn Figures 9 and 10, we show the segmentations produced by DeepLabV3 ResNet101 [7] and the mutual background difference map for both GIRAFFE and GIRAFFE\\r\\n11\\r\\n\\r\\n\\x0c\\n\\nCompCar [51]\\r\\nFFHQ [26]\\r\\nAFHQ Cat [12]\\r\\nLSUN Church [52]\\r\\nCelebA-HQ [23]\\r\\n\\r\\nNumber of Images\\r\\n\\r\\nObject Rotation Range\\r\\n\\r\\nBackground Rotation Range\\r\\n\\r\\nCamera Elevation Range\\r\\n\\r\\nHorizontal Translation\\r\\n\\r\\nDepth Translation\\r\\n\\r\\nVertical Translation\\r\\n\\r\\nObject Scale\\r\\n\\r\\nField of View\\r\\n\\r\\n136,726\\r\\n70,000\\r\\n5,558\\r\\n126,227\\r\\n30,000\\r\\n\\r\\n360◦\\r\\n70◦\\r\\n70◦\\r\\n360◦\\r\\n90◦\\r\\n\\r\\n0◦\\r\\n0◦\\r\\n0◦\\r\\n0◦\\r\\n90◦\\r\\n\\r\\n10◦\\r\\n10◦\\r\\n10◦\\r\\n0◦\\r\\n10◦\\r\\n\\r\\n-0.12 - 0.12\\r\\n-0.15 - 0.15\\r\\n-\\r\\n\\r\\n-0.22 - 0.22\\r\\n-0.15 - 0.15\\r\\n-\\r\\n\\r\\n-0.06 - 0.08\\r\\n-\\r\\n\\r\\n0.8 - 1\\r\\n0.8 - 1\\r\\n-\\r\\n\\r\\n10◦\\r\\n10◦\\r\\n10◦\\r\\n30◦\\r\\n10◦\\r\\n\\r\\nTable 5. Dataset parameters. We report relevant parameters for all datasets.\\r\\n\\r\\n𝐼𝟏\\r\\n\\r\\n𝐼𝟏\\r\\n\\r\\n𝒎𝒂𝒔𝒌𝒆𝒅 𝐼𝟏\\r\\n\\r\\n𝒎𝒂𝒔𝒌𝒆𝒅 𝐼𝟏\\r\\n\\r\\n𝐼𝟐\\r\\n\\r\\n𝐼𝟐\\r\\n\\r\\n𝒎𝒂𝒔𝒌𝒆𝒅 𝐼𝟐\\r\\n\\r\\n𝒎𝒂𝒔𝒌𝒆𝒅 𝐼𝟐\\r\\n\\r\\nMutual BG\\r\\nDiff\\r\\n\\r\\nMutual BG\\r\\nDiff\\r\\nFFHQ\\r\\n\\r\\nFFHQ\\r\\n\\r\\nCompCar\\r\\n\\r\\nCompCar\\r\\n\\r\\nFigure 10. GIRAFFE HD (ours) MBS Calculation. DeepLabV3\\r\\nbackground segmentations and mutual background differences\\r\\n(white pixels) used for computing MBS on our GIRAFFE HD\\r\\nsamples.\\r\\n\\r\\nFigure 9. GIRAFFE MBS Calculation. DeepLabV3 background\\r\\nsegmentations and mutual background differences (white pixels)\\r\\nused for computing MBS on GIRAFFE samples.\\r\\n\\r\\n12\\r\\n\\r\\n\\x0c\\n\\nFigure 11. Controllable Image Synthesis. Changing background results on CompCar [51] and FFHQ [26]. Notice how the appearance of\\r\\nthe foreground adapts to the changing background.\\r\\n\\r\\n13\\r\\n\\r\\n\\x0c\\n\\n(a) AFHQ Cat #!&! change background\\r\\n\\r\\n(b) LSUN Church #!&! change background\\r\\nFigure 12. Controllable Image Synthesis. Changing background results on AFHQ Cat [12] and LSUN Church [52]. Notice how the\\r\\nappearance of the foreground adapts to the changing background. We also observe that for datasets where the foreground object does not\\r\\nhave great variation in appearance (e.g., LSUN Church), the refine foreground renderer tends to take more control over the final foreground\\r\\nobject’s appearance than the initial foreground renderer. In such cases, making changes to the background tends to change the foreground\\r\\nappearance more.\\r\\n\\r\\n14\\r\\n\\r\\n\\x0c\\n\\nFigure 13. Controllable Image Synthesis. Changing appearance results on CompCar [51] and FFHQ [26].\\r\\n\\r\\n15\\r\\n\\r\\n\\x0c\\n\\n(a) AFHQ Cat #!&! change appearance\\r\\n\\r\\n(b) LSUN Church #!&! change appearance\\r\\nFigure 14. Controllable Image Synthesis. Changing appearance results on AFHQ Cat [12] and LSUN Church [52]. As mentioned\\r\\npreviously, for datasets where the foreground object does not have great variation in appearance (e.g., LSUN Church), the refine foreground\\r\\nrenderer tends to take more control over the final foreground object’s appearance than the initial foreground renderer. In such cases, making\\r\\nchanges to the foreground appearance code tends to have relatively less effect on the appearance of the foreground object.\\r\\n\\r\\n16\\r\\n\\r\\n\\x0c\\n\\nFigure 15. Controllable Image Synthesis. Changing shape results on CompCar [51] and FFHQ [26].\\r\\n\\r\\n17\\r\\n\\r\\n\\x0c\\n\\n(a) AFHQ Cat #!&! change shape\\r\\n\\r\\n(b) LSUN Church #!&! change shape\\r\\nFigure 16. Controllable Image Synthesis. Changing shape results on AFHQ Cat [12] and LSUN Church [52].\\r\\n\\r\\n18\\r\\n\\r\\n\\x0c\\n\\nFigure 17. Controllable Image Synthesis. Changing rotation and camera elevation results on CompCar [51] and FFHQ [26].\\r\\n\\r\\n19\\r\\n\\r\\n\\x0c\\n\\n(a) AFHQ Cat #!&! rotation\\r\\n\\r\\n(b) LSUN Church #!&! rotation\\r\\n\\r\\nFigure 18. Controllable Image Synthesis. Changing rotation and camera elevation results on AFHQ Cat [12] and changing rotation\\r\\nresults on LSUN Church [52] (the model is trained with a fixed camera elevation on the LSUN Church dataset). We observe that changing\\r\\nthe camera elevation has little effect on the AFHQ Cat results. We attribute this to its small dataset size.\\r\\n\\r\\n20\\r\\n\\r\\n\\x0c\\n\\n(a) Depth Translation\\r\\n\\r\\n(b) Horizontal Translation\\r\\n\\r\\n(c) Vertical Translation\\r\\n\\r\\n(d) Scaling\\r\\n\\r\\nFigure 19. Controllable Image Synthesis. Translation and scaling results on CompCar [51] and LSUN Church [52].\\r\\n\\r\\n21\\r\\n\\r\\n\\x0c\\n\\nFigure 20. Comprehensive Outputs. Intermediate and final output images for CompCar [51] 5122 .\\r\\n\\r\\n22\\r\\n\\r\\n\\x0c\\n\\nFigure 21. Comprehensive Outputs. Intermediate and final output images for FFHQ [26] 10242 .\\r\\n\\r\\n23\\r\\n\\r\\n\\x0c\\n\\nFigure 22. Comprehensive Outputs. Intermediate and final output images for AFHQ Cat [12] 2562 .\\r\\n\\r\\n24\\r\\n\\r\\n\\x0c\\n\\nFigure 23. Comprehensive Outputs. Intermediate and final output images for LSUN Church [52] 2562 .\\r\\n\\r\\n25\\r\\n\\r\\n\\x0c\\n\\nFigure 24. Our samples. GIRAFFE HD samples on CompCar [51] 5122 .\\r\\n\\r\\nFigure 25. GIRAFFE [38] samples. GIRAFFE samples on CompCar 2562 .\\r\\n\\r\\n26\\r\\n\\r\\n\\x0c\\n\\nFigure 26. Our samples. GIRAFFE HD samples on FFHQ [26] 10242 .\\r\\n\\r\\nFigure 27. GIRAFFE [38] samples. GIRAFFE samples on FFHQ 2562 .\\r\\n\\r\\n27\\r\\n\\r\\n\\x0c\\n\\nFigure 28. Our samples. GIRAFFE HD samples on AFHQ Cat [12] 2562 .\\r\\n\\r\\nFigure 29. GIRAFFE [38] samples. GIRAFFE samples on AFHQ Cat 2562 .\\r\\n\\r\\n28\\r\\n\\r\\n\\x0c\\n\\nFigure 30. Our samples. GIRAFFE HD samples on LSUN Church [52] 2562 .\\r\\n\\r\\nFigure 31. GIRAFFE [38] samples. GIRAFFE samples on LSUN Church 2562 .\\r\\n\\r\\n29\\r\\n\\r\\n\\x0c',\n",
       " \"Energy-based Latent Aligner for Incremental Learning\\r\\nK J Joseph†‡\\r\\n\\r\\nSalman Khan‡? Fahad Shahbaz Khan‡\\x05\\r\\nVineeth N Balasubramanian†\\r\\n\\r\\nRao Muhammad Anwer‡¶\\r\\n\\r\\n†\\r\\n\\r\\n?\\r\\n\\r\\nIndian Institute of Technology Hyderabad, India ‡ Mohamed bin Zayed University of AI, UAE\\r\\nAustralian National University, Australia \\x05 Linköping University, Sweden ¶ Aalto University, Finland\\r\\n\\r\\narXiv:2203.14952v1 [cs.CV] 28 Mar 2022\\r\\n\\r\\n{cs17m18p100001, vineethnb}@iith.ac.in, {salman.khan, fahad.khan, rao.anwer}@mbzuai.ac.ae\\r\\n\\r\\nAbstract\\r\\nDeep learning models tend to forget their earlier knowledge while incrementally learning new tasks. This behavior\\r\\nemerges because the parameter updates optimized for the\\r\\nnew tasks may not align well with the updates suitable for\\r\\nolder tasks. The resulting latent representation mismatch\\r\\ncauses forgetting. In this work, we propose ELI: Energybased Latent Aligner for Incremental Learning, which first\\r\\nlearns an energy manifold for the latent representations\\r\\nsuch that previous task latents will have low energy and the\\r\\ncurrent task latents have high energy values. This learned\\r\\nmanifold is used to counter the representational shift that\\r\\nhappens during incremental learning. The implicit regularization that is offered by our proposed methodology can\\r\\nbe used as a plug-and-play module in existing incremental\\r\\nlearning methodologies. We validate this through extensive\\r\\nevaluation on CIFAR-100, ImageNet subset, ImageNet 1k\\r\\nand Pascal VOC datasets. We observe consistent improvement when ELI is added to three prominent methodologies\\r\\nin class-incremental learning, across multiple incremental\\r\\nsettings. Further, when added to the state-of-the-art incremental object detector, ELI provides over 5% improvement\\r\\nin detection accuracy, corroborating its effectiveness and\\r\\ncomplementary advantage to the existing art. Code is available at: https:// github.com/ JosephKJ/ ELI .\\r\\n\\r\\n1. Introduction\\r\\nLearning experiences are dynamic in the real-world, requiring models to incrementally learn new capabilities over\\r\\ntime. Incremental Learning (also called continual learning) is a paradigm that learns a model MTt at time step\\r\\nt, such that it is competent in solving a continuum of\\r\\ntasks Tt = {τ1 , τ2 , · · · , τt } introduced to it during its lifetime. Each task τi contains instances from a disjoint set of\\r\\nclasses. Importantly, the training data for the previous tasks\\r\\n{τ1 , · · · , τt−1 } cannot be accessed while learning τt , due to\\r\\nprivacy, memory and/or computational constraints.\\r\\nWe can represent an incremental model MTt , as a composition of a latent feature extractor FθTt and a trailing network FφTt that solves the task using the extracted features:\\r\\n\\r\\nFigure 1. We illustrate an Incremental Learning model trained on a\\r\\ncontinuum of tasks in the top part of the figure. While learning the\\r\\ncurrent task τt (zoomed-in), the latent representation of Task τt−1\\r\\ndata gets disturbed, as shown by red arrows. ELI learns an energy\\r\\nmanifold, and uses it to counteract this inherent representational\\r\\nshift, as illustrated by green arrows, thereby alleviating forgetting.\\r\\n\\r\\nMTt (x) = (FφTt ◦ FθTt )(x); where x ∈ Tt . A naive approach for learning incrementally would be to use data samples from the current task τt to finetune the model trained\\r\\nuntil the previous task MTt−1 . Doing so will bias the internal representations of the network to perform well on τt , inturn significantly degrading the performance on old tasks.\\r\\nThis phenomenon is called catastrophic forgetting [13, 35].\\r\\nThe incremental learning problem requires accumulating knowledge over a long range of learning tasks without catastrophic forgetting. The main challenge is how to\\r\\nconsolidate conflicting implicit representations across different training episodes to learn a generalized model applicable to all the learning experiences. To this end, existing approaches investigate regularization-based methods\\r\\n[2, 23, 24, 29, 40, 55] that constrain θ and φ such that the\\r\\nmodel performs well on all the tasks. Exemplar replaybased methods [7, 8, 21, 33, 41] retain a subset of datapoints from each task, and rehearse them to learn a continual model. Dynamically expanding models [34, 44, 45],\\r\\nenlarge θ and φ while learning incrementally.\\r\\nComplementary to the existing methodologies, we introduce a novel approach which minimizes the representational shift in the latent space of an incremental model,\\r\\nusing a learned energy manifold. The energy modeling offers a natural mechanism to deal with catastrophic forget-\\r\\n\\r\\n\\x0c\\n\\nting which we build upon. Fig. 1 illustrates how our proposed methodology, ELI: Energy-based Latent Aligner for\\r\\nIncremental Learning, helps to alleviate forgetting. After\\r\\nlearning the current task τt , the features from the feature\\r\\nextractor (referred to as latents henceforth), of the previous\\r\\ntask data zTt = FθTt (x), x ∈ τt−1 drift as shown by the\\r\\nred arrows. The first step in our approach is to learn an\\r\\nenergy manifold where the latent representations from the\\r\\nmodel trained until the current task MTt have higher energy, while the latents from the model trained till the previous task MTt−1 have lower energy. Next, the learned\\r\\nenergy-based model (EBM) is used to transform the previous task latents zTt (obtained via passing the previous task\\r\\ndata through current model) which had drifted away, to alternate locations in the latent space such that the representational shift is undone (as shown by the green arrows). This\\r\\nhelps alleviate forgetting in incremental learning. We explain how this transformation can be achieved in Sec. 3. We\\r\\nalso present a proof-of-concept with MNIST (Fig. 3) which\\r\\nmimics the above setting. The latent space visualization and\\r\\naccuracy regain after learning the new task correlates with\\r\\nthe illustration in Fig. 1, which reinforces our intuition.\\r\\nA unique characteristic of our energy-based latent\\r\\naligner is its ability to extend and enhance existing continual learning methodologies, without any change to their\\r\\nmethodology. We verify this by adding ELI to three prominent class-incremental methods: iCaRL [41], LUCIR [20]\\r\\nand AANet [31] and the state-of-the-art incremental Object\\r\\nDetector: iOD [22]. We conduct thorough experimental\\r\\nevaluation on incremental versions of large-scale classification datasets like CIFAR-100 [25], ImageNet subset [41]\\r\\nand ImageNet 1k [9]; and Pascal VOC [12] object detection dataset. For incremental classification experiments, we\\r\\nconsider two prominent setups: adding classes to a model\\r\\ntrained with half of all the classes as first task, and the\\r\\ngeneral incremental learning setting which considers equal\\r\\nnumber of classes for all tasks. ELI consistently improves\\r\\nperformance across all datasets and on all methods in incremental classification settings, and obtains impressive performance gains on incremental Object Detection, compared\\r\\nto current state-of-the-art [22], by 5.4%, 7% and 3% while\\r\\nincrementally learning 10, 5 and a single class respectively.\\r\\nTo summarize, the key highlights of our work are:\\r\\n• We introduce a novel methodology ELI, which helps to\\r\\ncounter the representational shift that happens in the latent space of incremental learning models.\\r\\n• Our energy-based latent aligner can act as an add-on module to existing incremental classifiers and object detectors, without any changes to their methodology.\\r\\n• ELI shows consistent improvement on over 45 experiments across three large scale incremental classification\\r\\ndatasets, and improves the current state-of-the-art incremental object detector by over 5% mAP on average.\\r\\n\\r\\n2. Related Work\\r\\nIncremental Learning: In this setting a model consistently\\r\\nimproves itself on new tasks, without compromising its performance on old tasks. One popular approach to achieve\\r\\nthis behaviour is by constraining the parameters to not deviate much from previously tuned values [7,10,28,32,41,52].\\r\\nIn this regard, knowledge distillation [19] has been used\\r\\nextensively to enforce explicit regularization in incremental classification [7, 28, 41] and object detection [15, 21, 46]\\r\\nsettings. In replay based methods, typically a small subset of exemplars is stored to recall and retain representations useful for earlier tasks [6, 20, 24, 32, 41]. Another\\r\\nset of isolated parameter learning methods dedicate separate subsets of parameters to different tasks, thus avoiding\\r\\ninterference e.g., by new network blocks or gating mechanisms [1, 31, 38, 39, 44]. Further, meta-learning approaches\\r\\nhave been explored to learn the update directions which are\\r\\nshared among multiple incremental tasks [22, 40, 43]. In\\r\\ncontrast to these approaches, we propose to learn an EBM\\r\\nto align implicit feature distributions between incremental\\r\\ntasks. ELI can enhance these existing methods without any\\r\\nmethodological modifications, by enforcing an implicit latent space regularization using the learned energy manifold.\\r\\nEnergy-based Models: EBMs [26] are a type of maximum\\r\\nlikelihood estimation models that can assign low energies to\\r\\nobserved data-label pairs and high energies otherwise [11].\\r\\nEBMs have been used for out-of-distribution sample detection [30, 47], structured prediction [4, 5, 48] and improving\\r\\nadversarial robustness [11, 17]. Joint Energy-based Model\\r\\n(JEM) [14] shows that any classifier can be reinterpreted as\\r\\na generative model that can model the joint likelihood of\\r\\nlabels and data. While JEM requires alternating between\\r\\na discriminative and generative objective, Wang et al. [49]\\r\\npropose an energy-based open-world softmax objective that\\r\\ncan jointly perform discriminative learning and generative\\r\\nmodeling. EBMs have also been used for synthesizing images [3, 53, 57, 58]. Xie et al. [54] represents EBM using a\\r\\nCNN and utilizes Langevin dynamics for MCMC sampling\\r\\nto generate realistic images. In contrast to these methods,\\r\\nwe explore the utility of the EBMs to alleviate forgetting\\r\\nin a continual learning paradigm. Most of these methods\\r\\noperate in the data space, where sampling from the EBM\\r\\nwould be expensive [57]. Differently, we learn the energy\\r\\nmanifold with the latent representations, which is faster and\\r\\neffective in controlling the representational shift that affects\\r\\nincremental models. A recent unpublished work [27] proposes to replace the standard softmax layer of an incremental model with an energy-based classifier head. Our\\r\\napproach introduces an implicit regularization in the latent\\r\\nspace using the learned energy manifold which is fundamentally different from their approach, scales well to harder\\r\\ndatasets and diverse settings (classification and detection).\\r\\n\\r\\n\\x0c\\n\\n3. Energy-based Latent Aligner\\r\\nOur proposed methodology ELI utilizes an Energybased Model (EBM) [26] to optimally adapt the latent representations of an incremental model, such that it alleviates\\r\\ncatastrophic forgetting. We refer to the intermediate feature\\r\\nvector extracted from the backbone network of the model as\\r\\nlatent representations in our discussion. After a brief introduction to the problem setting in Sec. 3.1, we explain how\\r\\nthe EBM is learned and used for aligning in Sec. 3.2. We\\r\\nconclude with a discussion on a toy experiment in Sec. 3.3.\\r\\n\\r\\n3.1. Problem Setting\\r\\nIn the incremental learning paradigm, a set of tasks\\r\\nTt = {τ1 , τ2 , · · · , τt } is introduced to the model over time.\\r\\nτt denotes the task introduced at time step t, which is composed of images Xτt and labels yτt sampled from its cort\\r\\n. Each\\r\\nresponding task data distribution: (xτi t , yiτt ) ∼ pτdata\\r\\ntask τt , contains instances from a disjoint set of classes. We\\r\\nseek to build a model MTt , which is competent in solving all the tasks Tt . Without loss of generality MTt can be\\r\\nexpressed as a composition of two functions: MTt (x) =\\r\\n(FφTt ◦ FθTt )(x), where FθTt is a feature extractor and FφTt is\\r\\na classifier in the case of a classification model and a composite classification and localization branch for an object\\r\\ndetector, solving all the tasks Tt introduced to it so far.\\r\\nWhile training MTt on current task τt , the model does\\r\\nnot have access to all the data from previous tasks1 . This\\r\\nimbalance between present and previous task data can bias\\r\\nthe model to focus on the latest task, while catastrophically\\r\\ndegrading its performance on the earlier ones. Making an\\r\\nincremental learner robust against such forgetting is a challenging research question. Regularization methods [2, 23],\\r\\nexemplar-replay methods [8, 33, 41] and progressive model\\r\\nexpansion methods [34,44,45] have emerged as the standard\\r\\nways to address forgetting. Our proposed methodology is\\r\\ncomplementary to all these developments in the field, and is\\r\\ngeneric enough to serve as an add-on to any such continual\\r\\nlearning methodology, with minimal overhead.\\r\\n\\r\\n3.2. Latent Aligner\\r\\nWe perform energy-based modeling in the latent space\\r\\nof continual learning models. Our latent aligner approach\\r\\navoids the need to explicitly identify which latent representations should be adapted or retained to preserve knowledge\\r\\nacross tasks while learning new skills. It implicitly identifies which representations are ideal to be shared between\\r\\ntasks, preserves them, and simultaneously adapts representations which negatively impact incremental learning.\\r\\nLet us consider a concrete incremental learning setting\\r\\nwhere we introduce a new task τt to a model that is trained\\r\\n1 Such restricted memory is considered due to practical limitations such\\r\\n\\r\\nas bounded storage, computational budget and privacy issues.\\r\\n\\r\\nEnergy-based Latent Aligner (ELI)\\r\\nLearning Energy Manifold\\r\\n\\r\\nUsing Energy Manifold to Align\\r\\nPrevious Task Latents\\r\\n\\r\\nLatent space of\\r\\nData from\\r\\n\\r\\nBackbone\\r\\n\\r\\nLatent Features\\r\\nof Data from\\r\\n\\r\\nOld position of latents\\r\\nfrom Task\\r\\n\\r\\nCurrent position of\\r\\nlatents from Task\\r\\n\\r\\nFigure 2. We learn an energy manifold using the latent representations of the current task data passed though the current model FθTt\\r\\nT\\r\\nand previous model Fθ t−1 . This manifold is used to align the\\r\\nlatents from τt−1 that were shifted while learning the new task.\\r\\n\\r\\nto perform well until the previous tasks MTt−1 . Training\\r\\ndata to learn the new task is sampled from the correspondt\\r\\n. We may use any\\r\\ning data distribution: (xτi t , yiτt ) ∼ pτdata\\r\\nexisting continual learning algorithm A, to learn an incremental model MTt . The latent representations of MTt−1\\r\\nwould be optimized for learning τt , which causes degraded\\r\\nperformance of MTt on the previous tasks. Depending on\\r\\nthe efficacy of A, MTt can have varying degrees of effectiveness in alleviating the inherent forgetting. Our proposed\\r\\nmethod helps to undo this representational shift that happens to previous task instances, when passed through MTt .\\r\\nAs illustrated in Fig. 2, in the first step, we learn an energy manifold using three ingredients: (i) images from the\\r\\nt\\r\\ncurrent task: x ∼ pτdata\\r\\n, (ii) latent representations of x from\\r\\nT\\r\\nthe model trained till previous task: zTt−1 = Fθ t−1 (x) and\\r\\n(iii) latent representations of x from the model trained till\\r\\nthe current task: zTt = FθTt (x). An energy-based model\\r\\nEψ is learned to assign low energy values for zTt−1 , and\\r\\nhigh energy values for zTt . Next, during inference, the\\r\\nlearned energy manifold Eψ is used to counteract the representational shift that happens to the latent representations\\r\\nof previous task instances when passed through the current\\r\\nmodel: zTt = FθTt (x) where x ∈ Tt−1 . Due to the representational shift in the latent space, zTt will have higher\\r\\nenergy values in the energy manifold. We align zTt to alternate locations in latent space such that their energy on the\\r\\nmanifold is minimized, as illustrated in right part of Fig. 2.\\r\\nThese shifted latents demonstrate less forgetting, which we\\r\\nempirically verify through large scale experiments on incremental classification and object detection in Sec. 4.\\r\\nIt is interesting to note the following: 1) Our method\\r\\nadds implicit regularization in the latent space without making any changes to the incremental learning algorithm A,\\r\\nwhich is used to learn MTt , 2) ELI does not require access\\r\\nto previous task data to learn the energy manifold. Current\\r\\nT\\r\\ntask data, passed though the model Fθ t−1 indeed acts as a\\r\\nproxy for previous task data while learning the EBM.\\r\\n3.2.1 Learning the Latent Aligner: EBMs provide a simple and flexible way to model data likelihoods [11]. We use\\r\\ncontinuous energy-based models, formulated using a neu-\\r\\n\\r\\n\\x0c\\n\\nral network, which can generically model a diverse range\\r\\nof function mappings. Specifically, for a given latent feature vector z ∈ RD in ELI, we learn an energy function\\r\\nEψ (z) : RD → R to map it to a scalar energy value. An\\r\\nEBM is defined as Gibbs distribution pψ (z) over Eψ (z):\\r\\npψ (z) = R\\r\\n\\r\\nexp (−Eψ (z))\\r\\n,\\r\\nexp (−Eψ (z))dz\\r\\nz\\r\\n\\r\\n(1)\\r\\n\\r\\nR\\r\\nwhere z exp (−Eψ (z))dz is an intractable partition function. EBM is trained by maximizing the data log-likelihood\\r\\non a sample set drawn from the true distribution ptrue (z):\\r\\nL(ψ) = Ez∼ptrue [log pψ (z)].\\r\\n\\r\\n(2)\\r\\n\\r\\nThe derivative of the above objective is as follows [51]:\\r\\n∂ψ L(ψ) = Ez∼ptrue [−∂ψ Eψ (z)]+Ez∼pψ [∂ψ Eψ (z)]. (3)\\r\\nThe first term in Eq. 3 ensures that the energy for a sample z drawn from the true data distribution ptrue will be\\r\\nminimized, while the second term ensures that the samples\\r\\ndrawn from the model itself, will have higher energy values. In ELI, ptrue corresponds to the distribution of latent\\r\\nrepresentations from the model trained till the previous task\\r\\nat any point in time. Sampling from pψ (x) is intractable\\r\\nowing to the normalization constant in Eq. 1. Approximate samples are recursively drawn using Langevin dynamics [36, 50], which is a popular MCMC algorithm,\\r\\n√\\r\\nλ\\r\\nzi+1 = zi − ∂z Eψ (z) + λωi , ωi ∼ N (0, I)\\r\\n(4)\\r\\n2\\r\\nwhere λ is the step size and ω captures data uncertainty.\\r\\nEq. 4 yields a Markov chain that stabilizes to an stationary\\r\\ndistribution within few iterations, starting from an initial zi .\\r\\nAlgorithm 1 illustrates how the energy manifold is\\r\\nlearned in ELI. The energy function Eψ is realised by a\\r\\nmulti-layer perceptron with a single neuron in the output\\r\\nlayer, which quantifies energy of the input sample. It is\\r\\nKaiming initialized in Line 1. Until a few number of iterations, we sample mini-batches from the current task data\\r\\nt\\r\\ndistribution pτdata\\r\\n. Next, the latent representation of the\\r\\ndata in the mini-batch is retrieved from the model trained\\r\\nT\\r\\nuntil the previous task Fθ t−1 and the model trained till the\\r\\nTt\\r\\ncurrent task Fθ , in Line 4 and 5 respectively. From here\\r\\non, we prepare to compute the gradients according to Eq. 3,\\r\\nwhich is required for training the energy function. The first\\r\\nterm in Eq. 3 minimizes the expectation over in-distribution\\r\\nenergies, which is computed in Line 7, while the second\\r\\nterm maximizes the expectation over out-of-distribution energies (Line 8). The Langevin sampling which is required\\r\\nto compute the out-of-distribution energies takes the latents from the current model as initial starting points of the\\r\\nMarkov chain, as illustrated in Line 6. Finally, the loss is\\r\\ncomputed in Line 9 and the energy function Eψ is optimized\\r\\nwith RMSprop [18] optimizer in Line 10.\\r\\n\\r\\nAlgorithm 1 Algorithm L EARN EBM\\r\\nInput: Feature extractor of model trained till current task:\\r\\nFθTt ; Feature extractor of model trained till previous\\r\\nT\\r\\nt\\r\\ntask: Fθ t−1 ; Data distribution of the current task: pτdata\\r\\n1: Eψ ←Initialize the Energy function.\\r\\n2: while until required iterations do\\r\\nt\\r\\n3:\\r\\nx ∼ pτdata\\r\\n. Sample a mini-batch\\r\\nTt−1\\r\\nTt−1\\r\\n4:\\r\\nz\\r\\n← Fθ (x)\\r\\n5:\\r\\nzTt ← FθTt (x)\\r\\nt\\r\\n6:\\r\\nzTsampled\\r\\n← Sample from EBM with zTt as starting\\r\\npoints.\\r\\n. Refer Equation 4\\r\\n7:\\r\\nin_dist_energy ← Eψ (zTt−1 )\\r\\nt\\r\\n8:\\r\\nout_of_dist_energy ← Eψ (zTsampled\\r\\n)\\r\\n9:\\r\\nLoss ← (−in_dist_energy + out_of_dist_energy)\\r\\n. Refer Equation 3\\r\\n10:\\r\\nOptimize Eψ with Loss.\\r\\n11: return Eψ\\r\\n3.2.2 Alignment using ELI: After learning a task τt in an\\r\\nincremental setting, we use Algorithm 1 to learn the energy\\r\\nmanifold. This manifold is used to align the latent representations of previous task instances from the current model\\r\\nMTt using Algorithm 2. The gradient of energy function\\r\\nEψ with respect to the latent representation z is computed\\r\\n(Line 2). These latents are then successively updated to\\r\\nreduce their energy (Line 3). We repeat this for Lsteps\\r\\nnumber of Langevin iterations. The aligner assumes that\\r\\na high-level task information is available during inference\\r\\ni.e., whether a latent belongs to the current task or not.\\r\\nAlgorithm 2 Algorithm A LIGN L ATENTS\\r\\nInput: Latent vector to be adapted: z; EBM: Eψ ; Number\\r\\nof Langevin steps: Lsteps ; Learning rate: λ\\r\\n1: while until Lsteps iterations do\\r\\n2:\\r\\ngrad ← ∇z Eψ (z)\\r\\n3:\\r\\nz ← z − λ ∗ grad\\r\\n4: return z\\r\\n\\r\\n3.3. Toy Example\\r\\nOur methodology is build on a key premise that latent\\r\\nrepresentations of an incremental learning model will get\\r\\ndisturbed after training on new tasks, and that an energybased manifold can aid in successfully mitigating this unwarranted representational shift in a post-hoc fashion. In\\r\\nFig. 3, we present a proof-of-concept that our hypothesis indeed holds. We consider a two task experiment\\r\\nwith incremental MNIST, where the first task is to learn\\r\\nthe first 5 classes, while the second is to learn the rest:\\r\\nT1 = {τ0 . . . τ4 } and T2 = {τ5 . . . τ9 }. We first learn\\r\\nMT1 (x) = (FφT1 ◦ FθT1 )(x), where x ∈ T1 , and then incrementally update it to MT2 (x) = (FφT2 ◦ FθT2 )(x), where\\r\\n\\r\\n\\x0c\\n\\nTask 1 Accuracy; |z| = 32.\\r\\n\\r\\n100\\r\\n\\r\\n99.16 99.54\\r\\n\\r\\nTask 1 Accuracy; |z| = 512.\\r\\n\\r\\n89.14\\r\\n\\r\\n99.04\\r\\n\\r\\n83.44\\r\\n\\r\\n50\\r\\n20.88\\r\\n\\r\\n0\\r\\n\\r\\nAfter Learning After Learning After Aligning\\r\\nTask 1\\r\\nTask 2\\r\\nusing ELI\\r\\n\\r\\n(a) Performance of Task 1 after learning Task\\r\\n1, Task 2 and after adaptation using ELI.\\r\\n\\r\\n(b) Visualizing Task 1 latents after\\r\\nlearning Task 1.\\r\\n\\r\\n(c) Visualizing Task 1 latents after\\r\\nlearning Task 2.\\r\\n\\r\\n(d) Visualizing Task 1 latents after\\r\\naligning using ELI.\\r\\n\\r\\nFigure 3. A key hypothesis that we base our methodology is that while learning a new task, the latent representations will get disturbed,\\r\\nwhich will in-turn cause catastrophic forgetting of the previous task, and that an energy manifold can be used to align these latents, such\\r\\nthat it alleviates forgetting. Here, we illustrate a proof-of-concept that our hypothesis is indeed true. We consider a two task experiment on\\r\\nMNIST; T1 = {τ0 , τ1 , τ2 , τ3 , τ4 }, T2 = {τ5 , τ6 , τ7 , τ8 , τ9 }. After learning the second task, the accuracy on T1 test set drops to 20.88%,\\r\\nwhile experimenting with a 32-dim latent space. The latent aligner in ELI provides 62.56% improvement in test accuracy to 83.44%. The\\r\\nvisualization of a 512-dim latent space after learning T2 in sub-figure (c), indeed shows cluttering due to representational shift. ELI is able\\r\\nto align the latents as shown in sub-figure (d), which alleviates the drop in accuracy from 89.14% to 99.04%.\\r\\n\\r\\nx ∈ T2 . When evaluating the Task 1 classification accuracy using (FφT1 ◦ FθT2 )(x), where x ∈ T1test , we see catastrophic forgetting in action. There is a significant drop in\\r\\nperformance from 99.2% to 20.9%, when we use a 32 dimensional latent space. Let FψELI represent our proposed\\r\\nlatent aligner. While re-evaluating the classification accuracy using (FφT1 ◦FψELI ◦FθT2 )(x), where x ∈ T1test , we see\\r\\nan improvement of 62.6% to 83.4%. We also try increasing\\r\\nthe latent space dimension to 512. Consistent to our earlier\\r\\nobservation, we observe a drop in accuracy from 99.54% to\\r\\n89.14%. ELI helps to improve it to 99.04%. The absolute\\r\\ndrop in performance due to forgetting is lower than the 32\\r\\ndimensional latent space because of the larger capacity of\\r\\nthe model. The visualization of latent space in sub-figure\\r\\n(c) also suggests more cluttering. Sub-figure (d) explicitly\\r\\nreinforces the utility of ELI to realign the latents. Specifically, note how Class 3 latents which were intermingled\\r\\nwith Class 2 latents are now nicely moved around in the latent space by ELI. These results strongly motivate the utility\\r\\nof our method. By making FθT2 more stronger using mainstream incremental learning methodologies, we would improve the performance further. We illustrate this on harder\\r\\ndatasets for class-incremental learning and incremental object detection setting in Sec. 4.1 and Sec. 4.2 respectively.\\r\\n\\r\\n4. Experiments and Results\\r\\nWe conduct extensive experiments with incremental\\r\\nclassifiers and object detectors to evaluate ELI. To the best\\r\\nof our knowledge, ours is the first methodology, which\\r\\nworks across both these settings without any modification.\\r\\nProtocols: In both problem domains, we study classincremental setting where a group of classes constitutes an\\r\\nincremental task. For class-incremental learning of classifiers, we experiment with two prominent protocols that exist in the literature: a) train with half the total number of\\r\\nclasses as the first task [20,31], and equal number of classes\\r\\n\\r\\nper task thereafter, b) ensure that each task (including the\\r\\nfirst) has equal number of classes [7, 24, 38, 41]. The former tests extreme class incremental learning setting, where\\r\\nin the 25 task setting we incrementally add only two classes\\r\\nat each stage for a dataset with 100 classes. It has the advantage of learning a strong initial classifier as it has access to half of the dataset in Task 1. The later setting has a\\r\\nuniform class distribution across tasks. Both these settings\\r\\ntest different plausible dynamics of an incremental classifier. For incremental object detection, similar to existing\\r\\nworks [22, 37, 46], we follow a two task setting where the\\r\\nsecond task contains 10, 5 or a single incremental class.\\r\\nDatasets and Evaluation Metrics: Following existing\\r\\nworks [7, 20, 22, 31, 41, 46] we use incremental versions\\r\\nof CIFAR-100 [25], ImageNet subset [41], ImageNet 1k\\r\\n[9] and Pascal VOC [12] datasets. CIFAR-100 [25] contains 50k training images, corresponding to 100 classes,\\r\\neach with spatial dimensions of 32 × 32. ImageNet-subset\\r\\n[41] contains 100 randomly selected classes from ImageNet\\r\\ndatasets. We also experiment with the full ImageNet 2012\\r\\ndataset [9] which contains 1000 classes. In contrast to\\r\\nCIFAR-100, there are over 1300 images per class with\\r\\n224 × 224 size in both ImageNet-subset and ImageNet-1k.\\r\\nPascal VOC 2007 [12] contains 9963 images, where each\\r\\nobject instance is annotated with its class label and location\\r\\nin an image. Instances from 20 classes are annotated in Pascal VOC. Average accuracy across tasks [31, 41] and mean\\r\\naverage precision (mAP) [12] is used as the evaluation metric for incremental classification and detection, respectively.\\r\\nImplementation Details: Following the standard practice\\r\\n[31, 41], we use ResNet-18 [16] for CIFAR-100 experiments and ResNet-32 [16] for ImageNet experiments. We\\r\\nuse a batch size of 128 and train for 160 epochs. We start\\r\\nwith an initial learning rate of 0.1, which is decayed by 0.1\\r\\nafter 80th and 120th epochs. The EBM is a three layer neural network with 64 neurons in the first two layers and sin-\\r\\n\\r\\n\\x0c\\n\\nTable 1. The table shows class-incremental learning results when our latent aligner ELI is added to three prominent and top-performing\\r\\nincremental approaches [20, 31, 41]. ELI is able to provide additional latent space regularization to these methods, consistently improving\\r\\nthem across all the settings. The green subscript highlights the relative improvement. Refer to Sec. 4.1 for detailed analysis.\\r\\nSettings →\\r\\n\\r\\nHalf of all the classes is used to learn the first task\\r\\n\\r\\nDatasets →\\r\\nMethods\\r\\n\\r\\nVenue\\r\\n\\r\\nCIFAR-100\\r\\n5 Tasks\\r\\n\\r\\n10 Tasks\\r\\n\\r\\nSame number of classes for each task\\r\\n\\r\\nImageNet subset\\r\\n25 Tasks\\r\\n\\r\\n5 Tasks\\r\\n\\r\\n10 Tasks\\r\\n\\r\\n25 Tasks\\r\\n\\r\\nCIFAR-100\\r\\n5 Tasks\\r\\n\\r\\n10 Tasks\\r\\n\\r\\nImageNet subset\\r\\n20 Tasks\\r\\n\\r\\n5 Tasks\\r\\n\\r\\n10 Tasks\\r\\n\\r\\n20 Tasks\\r\\n\\r\\niCaRL [41]\\r\\nCVPR 17 56.97\\r\\n53.28\\r\\n50.98\\r\\n58.24\\r\\n51.6\\r\\n49.02\\r\\n61.59\\r\\n60.05\\r\\n57.81\\r\\n71.46\\r\\n65.25\\r\\n60.21\\r\\niCaRL + ELI\\r\\n63.68 + 6.71 58.92 + 5.64 54.00 + 3.02 68.94 + 10.73 61.48 + 9.88 56.11 + 7.08 70.13 + 8.54 67.81 + 7.75 63.06 + 5.25 78.51 + 7.04 71.66 + 6.41 66.77 + 6.56\\r\\nLUCIR [20] CVPR 19 64.37\\r\\n62.57\\r\\n59.91\\r\\n71.38\\r\\n68.99\\r\\n64.65\\r\\n62.01\\r\\n58.95\\r\\n54.2\\r\\n74.22\\r\\n67.97\\r\\n62.2\\r\\nLUCIR + ELI\\r\\n66.06 + 1.69 63.50 + 0.93 60.30 + 0.39 74.58 + 3.21 71.62 + 2.61 66.35 + 1.71 64.55 + 2.49 59.51 + 0.56 54.98 + 0.78 75.38 + 1.16 70.28 + 2.31 65.51 + 3.31\\r\\nAANet [31] CVPR 21 67.53\\r\\n66.25\\r\\n64.28\\r\\n70.84\\r\\n70.3\\r\\n69.07\\r\\n63.89\\r\\n60.94\\r\\n56.88\\r\\n65.86\\r\\n54.13\\r\\n44.96\\r\\n68.78 + 1.25 66.62 + 0.37 64.72 + 0.44 73.54 + 2.73 71.82 + 1.52 70.32 + 1.25 66.36 + 2.47 61.72 + 0.78 57.65 + 0.77 67.43 + 1.57 55.47 + 1.34 46.93 + 1.97\\r\\nAANet + ELI\\r\\n\\r\\nFigure 4. Here, we plot the average accuracy after learning each incremental task on ImageNet 1k dataset. ELI is able to consistently\\r\\nimprove iCaRL [41], LUCIR [20] and AANet [31] on 5 task, 10 task and 25 task setting. On average, we see 8.17%, 3.05% and 2.53%\\r\\nimprovement to the three base methods. (Best viewed in color)\\r\\n\\r\\ngle neuron in the last layer. The features that are passed\\r\\non to the final softmax classifier of the base network, are\\r\\nused for learning the EBM. It is trained for 1500 iterations\\r\\nwith mini-batches of size 128. The learning rate is set to\\r\\n0.0001. We use 30 langevin iterations to sample from the\\r\\nEBM. We found that keeping an exponential moving average of the EBM model was effective. The implementations of the three prominent class-incremental methodologies (iCaRL [41], LUCIR [20] and AANet [31]) follows the\\r\\nofficial code from AANet [31] authors, released under an\\r\\nMIT license. They use an exemplar store of 20 images per\\r\\nclass. Note that our latent aligner does not use exemplars.\\r\\nThe iCaRL inference is modified to use fully-connected layers following Castro et al. [7]. All results are mean of three\\r\\nruns. We use an incremental version of Faster R-CNN [42]\\r\\nfor object detection experiments, following iOD [22]. The\\r\\n2048 dimensional penultimate feature vector from the RoI\\r\\nHead is used to learning the EBM.\\r\\n\\r\\n4.1. Incremental Classification Results\\r\\nWe augment three popular class-incremental learning\\r\\nmethods: iCaRL [41], LUCIR [20] and AANet [31] with\\r\\nour proposed latent aligner. Table 1 showcases the results\\r\\non CIFAR-100 [25] and ImageNet subset [41] datasets. As\\r\\nexplained earlier, we conduct experiments on the setting\\r\\nwhere half of the classes are learned in the first task, and\\r\\n\\r\\nwhen all tasks has equal number of classes. In the former,\\r\\nwe group 10, 5 and 2 classes each to create 5, 10 and 25\\r\\nlearning tasks respectively, after training the model on 50\\r\\ninitial classes. In the second setting, we group 20, 10 and\\r\\n5 classes each to create 5, 10 and 20 incremental tasks. We\\r\\nsee consistent improvement across all these settings when\\r\\nwe add ELI to the corresponding base methodology. In both\\r\\nthe settings, the improvement is more pronounced on harder\\r\\ndatasets. LUCIR [20] and AANet [31] use an explicit latent\\r\\nspace regularizer in their methodology. ELI is able to improve them further. Simpler methods like iCaRL [41] benefit more from the implicit regularization that ELI offers (this\\r\\naspect is explored further in Sec. 5.1). In Fig. 4, we plot the\\r\\naverage accuracy after learning each task in 5 task, 10 task\\r\\nand 25 task settings on ImageNet 1k. We see a similar trend,\\r\\nbut with larger improvements on this harder dataset. When\\r\\nadded to iCaRL [41], LUCIR [20] and AANet [31], ELI\\r\\nprovides 8.17%, 3.05% and 2.53% improvement on average in ImageNet 1k experiments, respectively.\\r\\nWhen we consider adding same number of classes in\\r\\neach incremental task, simple logit distillation provided by\\r\\niCaRL [41], along with our proposed latent aligner outperforms complicated methods by a significant margin. This\\r\\nis because the feature learning that happens with half of the\\r\\nclasses in the first task, is a major prerequisite for good performance of approaches like LUCIR [20] and AANet [31].\\r\\n\\r\\n\\x0c\\n\\nTable 2. Incremental Object Detection is evaluated in a two task setting with Pascal VOC 2007 dataset [12]. We consider adding 10, 5\\r\\nand one class (highlighted in color ) to a detector trained on the rest of the classes. When added to the state-of-the-art incremental Object\\r\\nDetector iOD [22], ELI provide a competitive improvement of 5.4%, 7% and 3% mAP in 10 + 10, 15 + 5 and 19 + 1 settings respectively.\\r\\n10 + 10 Setting\\r\\n\\r\\naero\\r\\n\\r\\ncycle\\r\\n\\r\\nbird\\r\\n\\r\\nboat\\r\\n\\r\\nbottle\\r\\n\\r\\nbus\\r\\n\\r\\ncar\\r\\n\\r\\ncat\\r\\n\\r\\nchair\\r\\n\\r\\ncow\\r\\n\\r\\ntable\\r\\n\\r\\ndog\\r\\n\\r\\nhorse\\r\\n\\r\\nbike\\r\\n\\r\\nperson\\r\\n\\r\\nplant\\r\\n\\r\\nsheep\\r\\n\\r\\nsofa\\r\\n\\r\\ntrain\\r\\n\\r\\ntv\\r\\n\\r\\nmAP\\r\\n\\r\\nAll 20\\r\\nFirst 10\\r\\nStd Training\\r\\n\\r\\n79.4\\r\\n78.6\\r\\n35.7\\r\\n\\r\\n83.3\\r\\n78.6\\r\\n9.1\\r\\n\\r\\n73.2\\r\\n72\\r\\n16.6\\r\\n\\r\\n59.4\\r\\n54.5\\r\\n7.3\\r\\n\\r\\n62.6\\r\\n63.9\\r\\n9.1\\r\\n\\r\\n81.7\\r\\n81.5\\r\\n18.2\\r\\n\\r\\n86.6\\r\\n87\\r\\n9.1\\r\\n\\r\\n83\\r\\n78.2\\r\\n26.4\\r\\n\\r\\n56.4\\r\\n55.3\\r\\n9.1\\r\\n\\r\\n81.6\\r\\n84.4\\r\\n6.1\\r\\n\\r\\n71.9\\r\\n57.6\\r\\n\\r\\n83\\r\\n57.1\\r\\n\\r\\n85.4\\r\\n72.6\\r\\n\\r\\n81.5\\r\\n67.5\\r\\n\\r\\n82.7\\r\\n73.9\\r\\n\\r\\n49.4\\r\\n33.5\\r\\n\\r\\n74.4\\r\\n53.4\\r\\n\\r\\n75.1\\r\\n61.1\\r\\n\\r\\n79.6\\r\\n66.5\\r\\n\\r\\n73.6\\r\\n57\\r\\n\\r\\n75.2\\r\\n73.4\\r\\n37.3\\r\\n\\r\\nShmelkov et al. [46]\\r\\nFaster ILOD [37]\\r\\nORE [21]\\r\\n\\r\\n69.9\\r\\n72.8\\r\\n63.5\\r\\n\\r\\n70.4\\r\\n75.7\\r\\n70.9\\r\\n\\r\\n69.4\\r\\n71.2\\r\\n58.9\\r\\n\\r\\n54.3\\r\\n60.5\\r\\n42.9\\r\\n\\r\\n48\\r\\n61.7\\r\\n34.1\\r\\n\\r\\n68.7\\r\\n70.4\\r\\n76.2\\r\\n\\r\\n78.9\\r\\n83.3\\r\\n80.7\\r\\n\\r\\n68.4\\r\\n76.6\\r\\n76.3\\r\\n\\r\\n45.5\\r\\n53.1\\r\\n34.1\\r\\n\\r\\n58.1\\r\\n72.3\\r\\n66.1\\r\\n\\r\\n59.7\\r\\n36.7\\r\\n56.1\\r\\n\\r\\n72.7\\r\\n70.9\\r\\n70.4\\r\\n\\r\\n73.5\\r\\n66.8\\r\\n80.2\\r\\n\\r\\n73.2\\r\\n67.6\\r\\n72.3\\r\\n\\r\\n66.3\\r\\n66.1\\r\\n81.8\\r\\n\\r\\n29.5\\r\\n24.7\\r\\n42.7\\r\\n\\r\\n63.4\\r\\n63.1\\r\\n71.6\\r\\n\\r\\n61.6\\r\\n48.1\\r\\n68.1\\r\\n\\r\\n69.3\\r\\n57.1\\r\\n77\\r\\n\\r\\n62.2\\r\\n43.6\\r\\n67.7\\r\\n\\r\\n63.1\\r\\n62.2\\r\\n64.6\\r\\n\\r\\niOD [22]\\r\\niOD + ELI\\r\\n\\r\\n76\\r\\n78.5\\r\\n\\r\\n74.6\\r\\n81.6\\r\\n\\r\\n67.5\\r\\n73.8\\r\\n\\r\\n55.9\\r\\n65.5\\r\\n\\r\\n57.6\\r\\n63.2\\r\\n\\r\\n75.1\\r\\n80.2\\r\\n\\r\\n85.4\\r\\n87.7\\r\\n\\r\\n77\\r\\n82.5\\r\\n\\r\\n43.7\\r\\n52.4\\r\\n\\r\\n70.8\\r\\n81.2\\r\\n\\r\\n60.1\\r\\n55.5\\r\\n\\r\\n66.4\\r\\n73.1\\r\\n\\r\\n76\\r\\n80.5\\r\\n\\r\\n72.6\\r\\n76.5\\r\\n\\r\\n74.6\\r\\n80.4\\r\\n\\r\\n39.7\\r\\n42.2\\r\\n\\r\\n64\\r\\n68.8\\r\\n\\r\\n60.2\\r\\n66\\r\\n\\r\\n68.5\\r\\n72.6\\r\\n\\r\\n60.5\\r\\n70.8\\r\\n\\r\\n66.3\\r\\n71.7\\r\\n\\r\\n15 + 5 Setting\\r\\n\\r\\naero\\r\\n\\r\\ncycle\\r\\n\\r\\nbird\\r\\n\\r\\nboat\\r\\n\\r\\nbottle\\r\\n\\r\\nbus\\r\\n\\r\\ncar\\r\\n\\r\\ncat\\r\\n\\r\\nchair\\r\\n\\r\\ncow\\r\\n\\r\\ntable\\r\\n\\r\\ndog\\r\\n\\r\\nhorse\\r\\n\\r\\nbike\\r\\n\\r\\nperson\\r\\n\\r\\nplant\\r\\n\\r\\nsheep\\r\\n\\r\\nsofa\\r\\n\\r\\ntrain\\r\\n\\r\\ntv\\r\\n\\r\\nmAP\\r\\n\\r\\nAll 20\\r\\nFirst 15\\r\\nStd Training\\r\\n\\r\\n79.4\\r\\n78.1\\r\\n12.7\\r\\n\\r\\n83.3\\r\\n82.6\\r\\n0.6\\r\\n\\r\\n73.2\\r\\n74.2\\r\\n9.1\\r\\n\\r\\n59.4\\r\\n61.8\\r\\n9.1\\r\\n\\r\\n62.6\\r\\n63.9\\r\\n3\\r\\n\\r\\n81.7\\r\\n80.4\\r\\n0\\r\\n\\r\\n86.6\\r\\n87\\r\\n8.5\\r\\n\\r\\n83\\r\\n81.5\\r\\n9.1\\r\\n\\r\\n56.4\\r\\n57.7\\r\\n0\\r\\n\\r\\n81.6\\r\\n80.4\\r\\n3\\r\\n\\r\\n71.9\\r\\n73.1\\r\\n9.1\\r\\n\\r\\n83\\r\\n80.8\\r\\n0\\r\\n\\r\\n85.4\\r\\n85.8\\r\\n3.3\\r\\n\\r\\n81.5\\r\\n81.6\\r\\n2.3\\r\\n\\r\\n82.7\\r\\n83.9\\r\\n9.1\\r\\n\\r\\n49.4\\r\\n37.6\\r\\n\\r\\n74.4\\r\\n51.2\\r\\n\\r\\n75.1\\r\\n57.8\\r\\n\\r\\n79.6\\r\\n51.5\\r\\n\\r\\n73.6\\r\\n59.8\\r\\n\\r\\n75.2\\r\\n53.2\\r\\n16.8\\r\\n\\r\\nShmelkov et al. [46]\\r\\nFaster ILOD [37]\\r\\nORE [21]\\r\\n\\r\\n70.5\\r\\n66.5\\r\\n75.4\\r\\n\\r\\n79.2\\r\\n78.1\\r\\n81\\r\\n\\r\\n68.8\\r\\n71.8\\r\\n67.1\\r\\n\\r\\n59.1\\r\\n54.6\\r\\n51.9\\r\\n\\r\\n53.2\\r\\n61.4\\r\\n55.7\\r\\n\\r\\n75.4\\r\\n68.4\\r\\n77.2\\r\\n\\r\\n79.4\\r\\n82.6\\r\\n85.6\\r\\n\\r\\n78.8\\r\\n82.7\\r\\n81.7\\r\\n\\r\\n46.6\\r\\n52.1\\r\\n46.1\\r\\n\\r\\n59.4\\r\\n74.3\\r\\n76.2\\r\\n\\r\\n59\\r\\n63.1\\r\\n55.4\\r\\n\\r\\n75.8\\r\\n78.6\\r\\n76.7\\r\\n\\r\\n71.8\\r\\n80.5\\r\\n86.2\\r\\n\\r\\n78.6\\r\\n78.4\\r\\n78.5\\r\\n\\r\\n69.6\\r\\n80.4\\r\\n82.1\\r\\n\\r\\n33.7\\r\\n36.7\\r\\n32.8\\r\\n\\r\\n61.5\\r\\n61.7\\r\\n63.6\\r\\n\\r\\n63.1\\r\\n59.3\\r\\n54.7\\r\\n\\r\\n71.7\\r\\n67.9\\r\\n77.7\\r\\n\\r\\n62.2\\r\\n59.1\\r\\n64.6\\r\\n\\r\\n65.9\\r\\n67.9\\r\\n68.5\\r\\n\\r\\niOD [22]\\r\\niOD + ELI\\r\\n\\r\\n78.4\\r\\n80.1\\r\\n\\r\\n79.7\\r\\n85.8\\r\\n\\r\\n66.9\\r\\n73.6\\r\\n\\r\\n54.8\\r\\n68.8\\r\\n\\r\\n56.2\\r\\n66.3\\r\\n\\r\\n77.7\\r\\n85.2\\r\\n\\r\\n84.6\\r\\n87.5\\r\\n\\r\\n79.1\\r\\n84.1\\r\\n\\r\\n47.7\\r\\n59.9\\r\\n\\r\\n75\\r\\n81.2\\r\\n\\r\\n61.8\\r\\n74.6\\r\\n\\r\\n74.7\\r\\n83.7\\r\\n\\r\\n81.6\\r\\n85.3\\r\\n\\r\\n77.5\\r\\n77.9\\r\\n\\r\\n80.2\\r\\n80.3\\r\\n\\r\\n37.8\\r\\n45.2\\r\\n\\r\\n58\\r\\n63.4\\r\\n\\r\\n54.6\\r\\n66.2\\r\\n\\r\\n73\\r\\n77.6\\r\\n\\r\\n56.1\\r\\n69.5\\r\\n\\r\\n67.8\\r\\n74.8\\r\\n\\r\\n19 + 1 Setting\\r\\n\\r\\naero\\r\\n\\r\\ncycle\\r\\n\\r\\nbird\\r\\n\\r\\nboat\\r\\n\\r\\nbottle\\r\\n\\r\\nbus\\r\\n\\r\\ncar\\r\\n\\r\\ncat\\r\\n\\r\\nchair\\r\\n\\r\\ncow\\r\\n\\r\\ntable\\r\\n\\r\\ndog\\r\\n\\r\\nhorse\\r\\n\\r\\nbike\\r\\n\\r\\nperson\\r\\n\\r\\nplant\\r\\n\\r\\nsheep\\r\\n\\r\\nsofa\\r\\n\\r\\ntrain\\r\\n\\r\\ntv\\r\\n\\r\\nmAP\\r\\n\\r\\nAll 20\\r\\nFirst 19\\r\\nStd Training\\r\\n\\r\\n79.4\\r\\n76.3\\r\\n16.6\\r\\n\\r\\n83.3\\r\\n77.3\\r\\n9.1\\r\\n\\r\\n73.2\\r\\n68.4\\r\\n9.1\\r\\n\\r\\n59.4\\r\\n55.4\\r\\n9.1\\r\\n\\r\\n62.6\\r\\n59.7\\r\\n9.1\\r\\n\\r\\n81.7\\r\\n81.4\\r\\n8.3\\r\\n\\r\\n86.6\\r\\n85.3\\r\\n35.3\\r\\n\\r\\n83\\r\\n80.3\\r\\n9.1\\r\\n\\r\\n56.4\\r\\n47.8\\r\\n0\\r\\n\\r\\n81.6\\r\\n78.1\\r\\n22.3\\r\\n\\r\\n71.9\\r\\n65.7\\r\\n9.1\\r\\n\\r\\n83\\r\\n77.5\\r\\n9.1\\r\\n\\r\\n85.4\\r\\n83.5\\r\\n9.1\\r\\n\\r\\n81.5\\r\\n76.2\\r\\n13.7\\r\\n\\r\\n82.7\\r\\n77.2\\r\\n9.1\\r\\n\\r\\n49.4\\r\\n46.6\\r\\n9.1\\r\\n\\r\\n74.4\\r\\n71.4\\r\\n23.1\\r\\n\\r\\n75.1\\r\\n65.8\\r\\n9.1\\r\\n\\r\\n79.6\\r\\n76.5\\r\\n15.4\\r\\n\\r\\n73.6\\r\\n50.7\\r\\n\\r\\n75.2\\r\\n67.5\\r\\n14.3\\r\\n\\r\\nShmelkov et al. [46]\\r\\nFaster ILOD [37]\\r\\nORE [21]\\r\\n\\r\\n69.4\\r\\n64.2\\r\\n67.3\\r\\n\\r\\n79.3\\r\\n74.7\\r\\n76.8\\r\\n\\r\\n69.5\\r\\n73.2\\r\\n60\\r\\n\\r\\n57.4\\r\\n55.5\\r\\n48.4\\r\\n\\r\\n45.4\\r\\n53.7\\r\\n58.8\\r\\n\\r\\n78.4\\r\\n70.8\\r\\n81.1\\r\\n\\r\\n79.1\\r\\n82.9\\r\\n86.5\\r\\n\\r\\n80.5\\r\\n82.6\\r\\n75.8\\r\\n\\r\\n45.7\\r\\n51.6\\r\\n41.5\\r\\n\\r\\n76.3\\r\\n79.7\\r\\n79.6\\r\\n\\r\\n64.8\\r\\n58.7\\r\\n54.6\\r\\n\\r\\n77.2\\r\\n78.8\\r\\n72.8\\r\\n\\r\\n80.8\\r\\n81.8\\r\\n85.9\\r\\n\\r\\n77.5\\r\\n75.3\\r\\n81.7\\r\\n\\r\\n70.1\\r\\n77.4\\r\\n82.4\\r\\n\\r\\n42.3\\r\\n43.1\\r\\n44.8\\r\\n\\r\\n67.5\\r\\n73.8\\r\\n75.8\\r\\n\\r\\n64.4\\r\\n61.7\\r\\n68.2\\r\\n\\r\\n76.7\\r\\n69.8\\r\\n75.7\\r\\n\\r\\n62.7\\r\\n61.1\\r\\n60.1\\r\\n\\r\\n68.3\\r\\n68.6\\r\\n68.9\\r\\n\\r\\niOD [22]\\r\\niOD + ELI\\r\\n\\r\\n78.2\\r\\n84.7\\r\\n\\r\\n77.5\\r\\n79.2\\r\\n\\r\\n69.4\\r\\n73.7\\r\\n\\r\\n55\\r\\n60.1\\r\\n\\r\\n56\\r\\n61.8\\r\\n\\r\\n78.4\\r\\n82.8\\r\\n\\r\\n84.2\\r\\n85.4\\r\\n\\r\\n79.2\\r\\n82.9\\r\\n\\r\\n46.6\\r\\n51.3\\r\\n\\r\\n79\\r\\n82.7\\r\\n\\r\\n63.2\\r\\n64.5\\r\\n\\r\\n78.5\\r\\n82.3\\r\\n\\r\\n82.7\\r\\n82.9\\r\\n\\r\\n79.1\\r\\n75.9\\r\\n\\r\\n79.9\\r\\n78.7\\r\\n\\r\\n44.1\\r\\n50.7\\r\\n\\r\\n73.2\\r\\n73.9\\r\\n\\r\\n66.3\\r\\n74.7\\r\\n\\r\\n76.4\\r\\n76.7\\r\\n\\r\\n57.6\\r\\n59.2\\r\\n\\r\\n70.2\\r\\n73.2\\r\\n\\r\\nFigure 5. In these qualitative results of incremental Object Detection, instances of plant, sheep, sofa, train and tvmonitor were introduced to a detector trained on the rest. We detect instances of old and new classes alike. More results are in supplementary materials.\\r\\n\\r\\n70.3\\r\\n\\r\\n71.5\\r\\n\\r\\n72.9\\r\\n69.3\\r\\n\\r\\n69\\r\\n\\r\\n70.3\\r\\n\\r\\n71\\r\\n\\r\\n71.8\\r\\n\\r\\n73\\r\\n\\r\\n69.1\\r\\n\\r\\n75\\r\\n\\r\\n68.9\\r\\n\\r\\n77\\r\\n73.5\\r\\n\\r\\n5.1 ELI as an Implicit Regularizer:\\r\\nTo showcase the effectiveness of the\\r\\nimplicit regularization that ELI offers, we remove the\\r\\nexplicit latent regularization term (referred to as ER in\\r\\nFig. 6) from our top\\r\\nperforming method\\r\\n\\r\\n76.1\\r\\n\\r\\n5. Discussions and Analysis\\r\\n\\r\\n70.7\\r\\n\\r\\nFollowing the standard evaluation protocol [22, 46] for\\r\\nincremental object detection, we group classes from Pascal\\r\\nVOC 2007 [12] into two tasks. Three different task combinations are considered here. We initially learn 10, 15 or 19\\r\\nclasses, and then introduce 10, 5 or one class as the second\\r\\ntask, respectively. Table 2 shows the results of this experiment. The first two rows in each section give the upperbound and the accuracy after learning the first task. The\\r\\n‘Std Training’ row shows how the performance on previous\\r\\nclasses deteriorate when simply finetuning the model on the\\r\\nnew class instances. The next three rows titled Shmelkov et\\r\\nal. [46], Faster ILOD [37] and ORE [21] show how existing methods help to address catastrophic forgetting. We add\\r\\nELI to iOD [22], the current state-of-the-art method, to improve its mAP by 5.4%, 7% and 3% while adding 10, 5 and\\r\\none class respectively, to a detector trained on the rest. This\\r\\nimprovement can be attributed to the effectiveness of ELI\\r\\n\\r\\nin aligning the latent representations to reduce forgetting.\\r\\nThese results also demonstrate that ELI is an effective plugand-play method to reduce forgetting, across classification\\r\\nand detection tasks. Fig. 8 shows our qualitative results.\\r\\n\\r\\n70.8\\r\\n\\r\\n4.2. Incremental Object Detection Results\\r\\n\\r\\n67\\r\\n65\\r\\n63\\r\\n5 Tasks\\r\\nAANet\\r\\nAANet - ER\\r\\n\\r\\n10 Tasks\\r\\n25 Tasks\\r\\nAANet + ELI\\r\\nAANet - ER + ELI\\r\\n\\r\\nFigure 6. ELI as an Implicit Regularizer on ImageNet subset.\\r\\n\\r\\n\\x0c\\n\\nTable 3. We vary the number of\\r\\nLangevin steps Lsteps , required to\\r\\nsample from the EBM. The latents get\\r\\naligned even within a few steps.\\r\\n\\r\\nTable 4. We change the number of iterations for training the EBM in Algo. 1.\\r\\nThe EBM converges within 1k iterations,\\r\\nwith moderate improvement thereafter.\\r\\n\\r\\n# of steps\\r\\n\\r\\n5 Tasks\\r\\n\\r\\n10 Tasks\\r\\n\\r\\n25 Tasks\\r\\n\\r\\n# of iterations\\r\\n\\r\\n5 Tasks\\r\\n\\r\\n10 Tasks\\r\\n\\r\\n25 Tasks\\r\\n\\r\\nArchitecture\\r\\n\\r\\n5 Tasks\\r\\n\\r\\n10 Tasks\\r\\n\\r\\n25 Tasks\\r\\n\\r\\n5\\r\\n10\\r\\n20\\r\\n30\\r\\n60\\r\\n90\\r\\n\\r\\n63.30\\r\\n63.66\\r\\n63.63\\r\\n63.68\\r\\n63.73\\r\\n63.79\\r\\n\\r\\n58.61\\r\\n58.85\\r\\n58.90\\r\\n58.92\\r\\n59.01\\r\\n58.97\\r\\n\\r\\n52.81\\r\\n53.49\\r\\n53.76\\r\\n54.00\\r\\n54.07\\r\\n54.04\\r\\n\\r\\n10\\r\\n100\\r\\n1000\\r\\n1500\\r\\n2000\\r\\n3000\\r\\n\\r\\n56.90\\r\\n60.53\\r\\n63.60\\r\\n63.68\\r\\n63.80\\r\\n63.67\\r\\n\\r\\n53.85\\r\\n57.08\\r\\n58.88\\r\\n58.92\\r\\n58.97\\r\\n58.85\\r\\n\\r\\n49.02\\r\\n50.41\\r\\n53.66\\r\\n54.00\\r\\n54.03\\r\\n54.06\\r\\n\\r\\ni-o\\r\\ni - 64 - o\\r\\ni - 64 - 64 - o\\r\\ni - 64 - 64 - 64 - o\\r\\ni - 256 - 256 - o\\r\\ni - 512 - 512 - o\\r\\n\\r\\n60.97\\r\\n63.72\\r\\n63.68\\r\\n63.71\\r\\n63.53\\r\\n63.66\\r\\n\\r\\n57.52\\r\\n59.02\\r\\n58.92\\r\\n58.9\\r\\n58.68\\r\\n58.66\\r\\n\\r\\n53.92\\r\\n54.59\\r\\n54.00\\r\\n54.44\\r\\n54.16\\r\\n54.00\\r\\n\\r\\nAANet [31] on ImageNet subset [41] experiments. There\\r\\nis a consistent drop in accuracy when ER is removed from\\r\\nthe base method (green bars). ELI is able to improve the\\r\\nperformance of such a model by 5.41%, 3.58% and 2.57%\\r\\non 5, 10 and 25 task experiments respectively (violet bars).\\r\\nWe note that the gain is more significant when we compare\\r\\nwith adding ELI to AANet with explicit regularization, corroborating the effectiveness of our implicit regularizer.\\r\\n5.2 Aligning the Final Layer Logits: ELI aligns\\r\\nthe latent representations from the feature extractor z =\\r\\nFθTt (x). An alternative would be to align the final logits\\r\\nFφTt (FθTt (x)). We re-evaluate incremental CIFAR-100 experiments in this setting. We find that latent space alignment\\r\\nis more effective than aligning the logit space (referred to\\r\\nas ‘+ Logit Aligner’ in Tab. 6). This is because the logits\\r\\nare specific to the end task while the latent representations\\r\\nmodel generalizable features across tasks.\\r\\n5.3 Aligning across Different-sized Latent Spaces: ELI\\r\\ncan align latent representations of varied dimensions. Our\\r\\ntoy experiment on MNIST uses 32 and 512 dimensional latent space, while CIFAR-100 experiments use a 64 dimensional space. ImageNet and Pascal VOC experiments uses\\r\\na latent space of 512 and 2048 dimensions each.\\r\\n5.4 Sensitivity to Hyper-parameters: We alter parameters\\r\\nthat can affect the ELI performance in Tab. 3, 4 and 5. The\\r\\nexperiments are on CIFAR-100 in ‘iCaRL + ELI’ setting.\\r\\nThe highlighted rows represent the default configuration.\\r\\nNumber of Langevin Steps: In Tab. 3, we experiment with\\r\\nchanging the number of Langevin steps Lsteps required to\\r\\nTable 6. Latent representations alignment is more effective than\\r\\naligning logits. Subscripts show change in accuracy from baseline.\\r\\nMethod\\r\\n\\r\\n5 Tasks\\r\\n\\r\\n10 Tasks\\r\\n\\r\\n25 Tasks\\r\\n\\r\\niCaRL [41]\\r\\niCaRL [41] + Logit Aligner\\r\\niCaRL [41] + ELI\\r\\n\\r\\n56.97\\r\\n57.97 + 1.00\\r\\n63.68 + 6.71\\r\\n\\r\\n53.28\\r\\n54.42 + 1.14\\r\\n58.92 + 5.64\\r\\n\\r\\n50.98\\r\\n51.49 + 0.51\\r\\n54.00 + 3.02\\r\\n\\r\\nLUCIR [20]\\r\\nLUCIR [20] + Logit Aligner\\r\\nLUCIR [20] + ELI\\r\\n\\r\\n64.37\\r\\n62.50 - 1.87\\r\\n66.06 + 1.69\\r\\n\\r\\n62.57\\r\\n61.67 - 0.9\\r\\n63.50 + 0.93\\r\\n\\r\\n59.91\\r\\n59.22 - 0.69\\r\\n60.30 + 0.39\\r\\n\\r\\nAANet [31]\\r\\nAANet [31] + Logit Aligner\\r\\nAANet [31] + ELI\\r\\n\\r\\n67.53\\r\\n66.16 - 1.37\\r\\n68.78 + 1.25\\r\\n\\r\\n66.25\\r\\n65.29 - 0.96\\r\\n66.62 + 0.37\\r\\n\\r\\n64.28\\r\\n63.81 - 0.47\\r\\n64.72 + 0.44\\r\\n\\r\\nTable 5. We vary the architecture of the\\r\\nEBM here. i and o refers to input and output\\r\\nlayer, while the values in-between represent\\r\\nthe number of neurons in each layer.\\r\\n\\r\\nsample from EBM in Algo. 2. ELI is able to align latents\\r\\nwith very few number of steps as the energy manifold is\\r\\nadept in guiding the alignment of latent representations.\\r\\nNumber of Iterations Required: While training the EBM\\r\\nusing Algo. 1, we change the number of iterations required,\\r\\nand report the accuracy in Tab. 4. At around 1000 iterations,\\r\\nthe EBM converges. Increasing the number of iterations\\r\\nfurther, does not lead to significant improvement.\\r\\nArchitecture: We experiment with EBM models of different\\r\\ncapacities in Tab. 5. We find that using a smaller architecture or a significantly larger architecture does not help. We\\r\\nsee this as a desirable characteristic since we learn the energy manifold of the latent space and not the data space.\\r\\n5.5 Compute and Memory: We record the compute,\\r\\nmemory and time requirements of ELI for CIFAR-100.\\r\\nWe use a single Nvidia Tesla K80 GPU for these metrics. The EBM, which is a two layer network with 64 neurons each, has 8.385K parameters and takes 1.057M flops\\r\\nwhen trying to learn 64 dimensional latent features. It takes\\r\\n0.039 ± 0.003 secs to sample from this EBM when aligning\\r\\n64 dimensional latents. We use 30 Langevin iterations for\\r\\nsampling. Mini-batch size is 128 for both the experiments.\\r\\n\\r\\n6. Conclusion\\r\\nWe demonstrate the use of energy-based models (EBMs)\\r\\nas a promising solution for incremental learning, by extending their natural mechanism to deal with representational\\r\\nshift. This is achieved by modeling the likelihoods in the\\r\\nlatent feature space, measuring the distributional shifts experienced across learning tasks and in-turn realigning them\\r\\nto optimize learning across all the tasks. Our proposed approach ELI, is complementary to existing methods and can\\r\\nbe used as an add-on module without modifying their base\\r\\npipeline. ELI offers consistent improvement to three prominent class-incremental classification methodologies when\\r\\nevaluated across multiple settings. Further, on the harder incremental object detection task, our methodology provides\\r\\nsignificant improvement over state-of-the-art.\\r\\n\\r\\nAcknowledgements\\r\\nWe thank Yaoyao Liu for his prompt clarifications on AANET [31]\\r\\ncode. KJJ thanks TCS Research for their PhD fellowship. VNB\\r\\nthanks DST, Govt of India, for partially supporting this work\\r\\nthrough the IMPRINT and ICPS programs.\\r\\n\\r\\n\\x0c\\n\\nReferences\\r\\n[1] Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone\\r\\nCalderara, Rita Cucchiara, and Babak Ehteshami Bejnordi.\\r\\nConditional channel gated networks for task-aware continual\\r\\nlearning. In CVPR, pages 3931–3940, 2020. 2\\r\\n[2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,\\r\\nMarcus Rohrbach, and Tinne Tuytelaars. Memory aware\\r\\nsynapses: Learning what (not) to forget. In Proceedings\\r\\nof the European Conference on Computer Vision (ECCV),\\r\\npages 139–154, 2018. 1, 3\\r\\n[3] Michael Arbel, Liang Zhou, and Arthur Gretton. Generalized energy based models. In ICLR, 2021. 2\\r\\n[4] Anton Bakhtin, Yuntian Deng, Sam Gross, Myle Ott,\\r\\nMarc’Aurelio Ranzato, and Arthur Szlam. Residual energybased models for text. Journal of Machine Learning Research, 22(40):1–41, 2021. 2\\r\\n[5] David Belanger and Andrew McCallum. Structured prediction energy networks. pages 983–992. PMLR, 2016. 2\\r\\n[6] Eden Belouadah and Adrian Popescu. Il2m: Class incremental learning with dual memory. In ICCV, pages 583–592,\\r\\n2019. 2\\r\\n[7] Francisco M Castro, Manuel J Marín-Jiménez, Nicolás Guil,\\r\\nCordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In ECCV, pages 233–248, 2018. 1, 2, 5,\\r\\n6\\r\\n[8] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,\\r\\nand Mohamed Elhoseiny. Efficient lifelong learning with agem. In ICLR, 2019. 1, 3\\r\\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\r\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\r\\ndatabase. In 2009 IEEE conference on computer vision and\\r\\npattern recognition, pages 248–255. Ieee, 2009. 2, 5\\r\\n[10] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas\\r\\nRobert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In ICCV, pages\\r\\n86–102. Springer, 2020. 2\\r\\n[11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In H. Wallach, H. Larochelle,\\r\\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, NeurIPS, volume 32, 2019. 2, 3\\r\\n[12] Mark Everingham, Luc Van Gool, Christopher KI Williams,\\r\\nJohn Winn, and Andrew Zisserman. The pascal visual object\\r\\nclasses (voc) challenge. International journal of computer\\r\\nvision, 88(2):303–338, 2010. 2, 5, 7, 11\\r\\n[13] Robert M French. Catastrophic forgetting in connectionist\\r\\nnetworks. Trends in cognitive sciences, 3(4):128–135, 1999.\\r\\n1\\r\\n[14] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen,\\r\\nDavid Duvenaud, Mohammad Norouzi, and Kevin Swersky.\\r\\nYour classifier is secretly an energy based model and you\\r\\nshould treat it like one. In ICLR, 2019. 2\\r\\n[15] Akshita Gupta, Sanath Narayan, KJ Joseph, Salman Khan,\\r\\nFahad Shahbaz Khan, and Mubarak Shah. Ow-detr: Openworld detection transformer. CVPR, 2021. 2\\r\\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\r\\nIdentity mappings in deep residual networks. In European\\r\\n\\r\\n[17]\\r\\n\\r\\n[18]\\r\\n\\r\\n[19]\\r\\n\\r\\n[20]\\r\\n\\r\\n[21]\\r\\n\\r\\n[22]\\r\\n\\r\\n[23]\\r\\n\\r\\n[24]\\r\\n\\r\\n[25]\\r\\n[26]\\r\\n\\r\\n[27]\\r\\n\\r\\n[28]\\r\\n[29]\\r\\n\\r\\n[30]\\r\\n\\r\\n[31]\\r\\n\\r\\n[32]\\r\\n\\r\\n[33]\\r\\n\\r\\nconference on computer vision, pages 630–645. Springer,\\r\\n2016. 5\\r\\nMitch Hill, Jonathan Craig Mitchell, and Song-Chun Zhu.\\r\\nStochastic security: Adversarial defense using long-run dynamics of energy-based models. In ICLR, 2021. 2\\r\\nGeoffrey Hinton, Nitish Srivastava, and Kevin Swersky.\\r\\nNeural networks for machine learning lecture 6a overview\\r\\nof mini-batch gradient descent. Cited on, 14(8):2, 2012. 4\\r\\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling\\r\\nthe knowledge in a neural network. In NIPS Deep Learning\\r\\nand Representation Learning Workshop, 2015. 2\\r\\nSaihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and\\r\\nDahua Lin. Learning a unified classifier incrementally via\\r\\nrebalancing. In CVPR, pages 831–839, 2019. 2, 5, 6, 8\\r\\nKJ Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth N Balasubramanian. Towards open world object detection. In CVPR, pages 5830–5840, 2021. 1, 2, 7\\r\\nKJ Joseph, Jathushan Rajasegaran, Salman Khan, Fahad\\r\\nKhan, and Vineeth N Balasubramanian. Incremental object\\r\\ndetection via meta-learning. IEEE Transactions on Pattern\\r\\nAnalysis & Machine Intelligence, Nov 2021. 2, 5, 6, 7, 11\\r\\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel\\r\\nVeness, Guillaume Desjardins, Andrei A Rusu, Kieran\\r\\nMilan, John Quan, Tiago Ramalho, Agnieszka GrabskaBarwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. 1, 3\\r\\nJoseph KJ and Vineeth Nallure Balasubramanian. Metaconsolidation for continual learning. NeurIPS, 33, 2020. 1,\\r\\n2, 5\\r\\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple\\r\\nlayers of features from tiny images. Citeseer, 2009. 2, 5, 6\\r\\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and\\r\\nF Huang. A tutorial on energy-based learning. Predicting\\r\\nstructured data, 1(0), 2006. 2, 3\\r\\nShuang Li, Yilun Du, Gido M van de Ven, and Igor Mordatch. Energy-based models for continual learning. arXiv\\r\\npreprint arXiv:2011.12216, 2020. 2\\r\\nZhizhong Li and Derek Hoiem. Learning without forgetting.\\r\\nPAMI, 40(12):2935–2947, 2017. 2\\r\\nZhizhong Li and Derek Hoiem. Learning without forgetting.\\r\\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947, 2018. 1\\r\\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li.\\r\\nEnergy-based out-of-distribution detection. NeurIPS, 2020.\\r\\n2\\r\\nYaoyao Liu, Bernt Schiele, and Qianru Sun. Adaptive aggregation networks for class-incremental learning. In CVPR,\\r\\npages 2544–2553, 2021. 2, 5, 6, 8\\r\\nYaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and\\r\\nQianru Sun. Mnemonics training: Multi-class incremental\\r\\nlearning without forgetting. In CVPR, pages 12245–12254,\\r\\n2020. 2\\r\\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient\\r\\nepisodic memory for continual learning. In NeurIPS, pages\\r\\n6467–6476, 2017. 1, 3\\r\\n\\r\\n\\x0c\\n\\n[34] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In CVPR,\\r\\npages 7765–7773, 2018. 1, 3\\r\\n[35] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning\\r\\nproblem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier, 1989. 1\\r\\n[36] Radford M Neal et al. Mcmc using hamiltonian dynamics.\\r\\nHandbook of markov chain monte carlo, 2(11):2, 2011. 4\\r\\n[37] Can Peng, Kun Zhao, and Brian C Lovell. Faster ilod: Incremental learning for object detectors based on faster rcnn.\\r\\nPattern Recognition Letters, 140:109–115, 2020. 5, 7\\r\\n[38] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling Shao. Random path selection\\r\\nfor incremental learning. NeurIPS, 2019. 2, 5\\r\\n[39] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, and Ming-Hsuan Yang. An\\r\\nadaptive random path selection approach for incremental\\r\\nlearning. arXiv preprint arXiv:1906.01120, 2019. 2\\r\\n[40] Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Mubarak Shah. itaml: An incremental task-agnostic meta-learning approach. In CVPR, pages\\r\\n13588–13597, 2020. 1, 2\\r\\n[41] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\\r\\nSperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In CVPR, pages 2001–2010,\\r\\n2017. 1, 2, 3, 5, 6, 8\\r\\n[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\r\\nFaster r-cnn: Towards real-time object detection with region\\r\\nproposal networks. NeurIPS, 28:91–99, 2015. 6\\r\\n[43] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,\\r\\nIrina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn\\r\\nwithout forgetting by maximizing transfer and minimizing\\r\\ninterference. In ICLR, 2019. 2\\r\\n[44] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,\\r\\nHubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks.\\r\\narXiv preprint arXiv:1606.04671, 2016. 1, 2, 3\\r\\n[45] Joan Serra, Didac Suris, Marius Miron, and Alexandros\\r\\nKaratzoglou. Overcoming catastrophic forgetting with hard\\r\\nattention to the task. pages 4548–4557, 2018. 1, 3\\r\\n[46] Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. Incremental learning of object detectors without catastrophic forgetting. In Proceedings of the IEEE international\\r\\nconference on computer vision, pages 3400–3409, 2017. 2,\\r\\n5, 7\\r\\n[47] Francesco Tonin, Arun Pandey, Panagiotis Patrinos, and Johan A. K. Suykens. Unsupervised energy-based out-ofdistribution detection using stiefel-restricted kernel machine.\\r\\nIn 2021 International Joint Conference on Neural Networks\\r\\n(IJCNN), pages 1–8, 2021. 2\\r\\n[48] Lifu Tu and Kevin Gimpel. Learning approximate inference\\r\\nnetworks for structured prediction. In ICLR, 2018. 2\\r\\n[49] Yezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Ziwei Liu,\\r\\nand Dongsheng Li. Energy-based open-world uncertainty\\r\\nmodeling for confidence calibration. In ICCV, pages 9302–\\r\\n9311, 2021. 2\\r\\n\\r\\n[50] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. pages 681–688, 2011. 4\\r\\n[51] Oliver Woodford. Notes on contrastive divergence. Department of Engineering Science, University of Oxford, Tech.\\r\\nRep, 2006. 4\\r\\n[52] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,\\r\\nZicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In CVPR, pages 374–382, 2019. 2\\r\\n[53] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat.\\r\\nVaebm: A symbiosis between variational autoencoders and\\r\\nenergy-based models. In ICLR, 2021. 2\\r\\n[54] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu.\\r\\nA theory of generative convnet. pages 2635–2644. PMLR,\\r\\n2016. 2\\r\\n[55] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. pages 3987–3995,\\r\\n2017. 1\\r\\n[56] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\\r\\nDavid Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018. 11\\r\\n[57] Yang Zhao and Changyou Chen. Unpaired image-to-image\\r\\ntranslation via latent energy transport. In CVPR, pages\\r\\n16418–16427, 2021. 2\\r\\n[58] Yang Zhao, Jianwen Xie, and Ping Li. Learning energybased generative models via coarse-to-fine expanding and\\r\\nsampling. In ICLR, 2020. 2\\r\\n\\r\\n\\x0c\\n\\nSupplementary Material\\r\\nIn this supplementary material, we provide additional details\\r\\nand experimental analysis regarding the behaviour of proposed latent alignment approach (ELI). They are:\\r\\n• An illustration for the adaptation process of latent representation with ELI. (Sec. A)\\r\\n• Effect of using mixup for data augmentation. (Sec. B)\\r\\n• Comments on the broader societal impacts. (Sec. C)\\r\\n• Qualitative results on incremental detection. (Sec. D)\\r\\n• A summary of notations used in the paper. (Sec. E)\\r\\n\\r\\nA. Recognizing Important Latents Implicitly\\r\\n\\r\\nsee that using mixup does not enhance performance, even with different values of α. This is because the EBM is a small two layer\\r\\nnetwork which is not prone to overfitting, and can perform well\\r\\neven without this extra augmentation.\\r\\nTable 7. The performance of EBM is comparable with and without\\r\\nusing mixup augmentation as the EBM network is small.\\r\\nα\\r\\n\\r\\n5 Tasks\\r\\n\\r\\n10 Tasks\\r\\n\\r\\n25 Tasks\\r\\n\\r\\nWithout mixup [56]\\r\\n0.1\\r\\n0.3\\r\\n0.5\\r\\n1.0\\r\\n\\r\\n63.68\\r\\n63.67\\r\\n63.53\\r\\n63.54\\r\\n63.44\\r\\n\\r\\n58.92\\r\\n58.85\\r\\n58.81\\r\\n58.79\\r\\n58.53\\r\\n\\r\\n54.00\\r\\n54.01\\r\\n53.85\\r\\n53.88\\r\\n53.83\\r\\n\\r\\nC. Broader Impact\\r\\nWhen a model incrementally learns without forgetting, an\\r\\nequivalently important desiderata would be to selectively forget,\\r\\nin adherence to any privacy or legislative reasons. Such an unlearning can be possible by treating such instances as out-ofdistribution samples, however, a dedicated treatment of the same\\r\\nis beyond the current scope of our work. Our current work aims to\\r\\nreduce the catastrophic forgetting and interference while learning\\r\\ncontinually, and to the best of our knowledge, our methodology\\r\\ndoes not have any detrimental social impacts that make us different from other research efforts geared in this direction.\\r\\n\\r\\nD. Qualitative Results\\r\\n\\r\\nFigure 7. Each row i shows how ith latent dimension is updated\\r\\nby ELI. We see that different dimensions have different degrees of\\r\\nchange, which is implicitly decided by our energy-based model.\\r\\nFig. 7 shows how each latent dimension of a 32 dimensional\\r\\nlatent vector (y-axis) gets adapted in each Langevin iteration (xaxis). For an initial latent representation z0 , each column shows\\r\\nthe difference from its aligned version from the ith Langevin step:\\r\\nzi − z0 . We consider MNIST experiment (Sec. 3.3) for this illustration. Our proposed latent aligner is able to implicitly identify\\r\\nwhich latent dimension is important to be preserved or modified.\\r\\nThis characteristic is difficult to achieve in alternate regularization\\r\\nmethods like distillation, which gives equal weightage to each dimension. We can see that the specialization happens within a few\\r\\nnumber of iterations, similar to the results in Tab. 3.\\r\\n\\r\\nB. Augmenting Data with mixup\\r\\nAs detailed in Sec. 3.2, we use datapoints sampled form the\\r\\ncurrent task distribution to learn the energy-based model xi ∼\\r\\nt\\r\\npτdata\\r\\n. Here we use mixup, an augmentation technique introduced by Zhang et al. [56], where each datapoint is modified as\\r\\nx̂ = λxi + (1 − λ)xj , s.t. λ ∼ Beta(α, α), and report the results\\r\\nin Tab. 7. In these experiments with incremental CIFAR-100, we\\r\\n\\r\\nIn Figure 8, we show more qualitative results for incremental\\r\\nObject Detection in the 15 + 5 setting with Pascal VOC dataset\\r\\n[12]. Instances of plant, sheep, sofa, train and tvmonitor are\\r\\nadded to a detector trained on the rest. The considerable improvement of ELI over the state-of-the-art-method [22] as shown in\\r\\nTab. 2, is due to the implicit latent space regularization that ELI\\r\\noffers. To the best of our knowledge, ELI is the first method that\\r\\nadds latent space regularization to large scale incremental object\\r\\ndetection models.\\r\\n\\r\\nE. Summary of Notations\\r\\nFor clarity, Tab. 8 summarizes the main notations used in our\\r\\npaper along with their concise description.\\r\\nTable 8. To enhance readability, this table summarises the notations used in the manuscript, along with their meaning.\\r\\nNotation\\r\\n\\r\\nStands for\\r\\n\\r\\nτi\\r\\nx ∈ τi\\r\\nTt = {τ1 , τ2 , · · · , τt }\\r\\nx ∈ Tt\\r\\nMTt\\r\\nFθTt\\r\\nFφTt\\r\\nzT t\\r\\nt\\r\\npτdata\\r\\nt\\r\\n(xτi t , yiτt ) ∼ pτdata\\r\\n\\r\\nith task\\r\\nImage from the ith task\\r\\nContinuum or set of tasks seen until time t\\r\\nImage from any of the task in Tt\\r\\nModel trained until time t\\r\\nFeature extractor of MTt\\r\\nTask specific part of MTt\\r\\nLatent representation from FθTt\\r\\nData distribution of task τt\\r\\nt\\r\\nSamples from pτdata\\r\\n\\r\\n\\x0c\\n\\nFigure 8. Qualitative results of incremental Object Detection. We consider the 10 + 5 setting on Pascal VOC, where instances of plant,\\r\\nsheep, sofa, train and tvmonitor are added to a detector trained on the rest of the classes.\\r\\n\\r\\n\\x0c\",\n",
       " 'Controllable Dynamic Multi-Task Architectures\\r\\nDripta S. Raychaudhuri1 Yumin Suh2 Samuel Schulter2 Xiang Yu2 Masoud Faraki2\\r\\nAmit K. Roy-Chowdhury1 Manmohan Chandraker2,3\\r\\n1\\r\\nUniversity of California, Riverside 2 NEC Labs America 3 University of California, San Diego\\r\\n\\r\\nAbstract\\r\\nMulti-task learning commonly encounters competition\\r\\nfor resources among tasks, specifically when model capacity is limited. This challenge motivates models which allow control over the relative importance of tasks and total\\r\\ncompute cost during inference time. In this work, we propose such a controllable multi-task network that dynamically adjusts its architecture and weights to match the desired task preference as well as the resource constraints.\\r\\nIn contrast to the existing dynamic multi-task approaches\\r\\nthat adjust only the weights within a fixed architecture,\\r\\nour approach affords the flexibility to dynamically control\\r\\nthe total computational cost and match the user-preferred\\r\\ntask importance better. We propose a disentangled training of two hypernetworks, by exploiting task affinity and\\r\\na novel branching regularized loss, to take input preferences and accordingly predict tree-structured models with\\r\\nadapted weights. Experiments on three multi-task benchmarks, namely PASCAL-Context, NYU-v2, and CIFAR-100,\\r\\nshow the efficacy of our approach. Project page is available\\r\\nat https://www.nec-labs.com/˜mas/DYMU .\\r\\n\\r\\n1. Introduction\\r\\nMulti-task learning [7, 42] (MTL) solves multiple tasks\\r\\nusing a single model, with potential advantages of fast inference and improved generalization by sharing representations\\r\\nacross related tasks. However, in practical scenarios, simultaneously optimizing all tasks is difficult due to task conflicts\\r\\nand limited model capacity [58]. Consequently, a trade-off\\r\\nbetween the competing tasks has to be found, necessitating\\r\\nprecise balancing of the different task losses during optimization. In many applications, the desired trade-off can change\\r\\nover time, requiring a new model to be retrained from scratch.\\r\\nTo overcome this lack of flexibility, recent methods propose\\r\\ndynamic networks for multi-task learning [28, 38]. These\\r\\nframeworks enable a single multi-task model to learn the\\r\\nentire trade-off curve, and allow users to control the desired\\r\\ntrade-off during inference via task preferences denoting the\\r\\nrelative task importance.\\r\\n\\r\\nConventional dynamic\\r\\nmulti-task networks\\r\\n\\r\\nProposed framework\\r\\n\\r\\n!\\r\\n\\r\\n!\\r\\n\\r\\n!\\r\\n\\r\\nTask 1 Task 2 Task 3\\r\\n\\r\\nTask 1 Task 2 Task 3\\r\\n\\r\\nTask 1 Task 2 Task 3\\r\\n\\r\\nFixed compute cost\\r\\nFixed architecture\\r\\n\\r\\nControl over compute\\r\\nFlexible architecture\\r\\n\\r\\nFigure 1. Problem setup. Our goal is to enable users to control\\r\\nresource allocation dynamically among multiple tasks at inference\\r\\ntime. Conventional dynamic networks (PHN [38]) for MTL achieve\\r\\nthis in terms of weight changes within a fixed model (color gradients indicate proportion of weights allocated for each task). In\\r\\ncontrast, we perform resource allocation in terms of both architecture and weights. This enables us to control total compute cost in\\r\\naddition to task preference. Dashed circle represents maximum\\r\\ncompute budget, while filled circle represents the desired budget.\\r\\nPortion of colors represents the user-defined task importance.\\r\\n\\r\\nConventional dynamic approaches for MTL assume a\\r\\nfixed model architecture, with all but the last prediction\\r\\nlayers shared, and control trade-offs by changing the weights\\r\\nof this model. While such hard-parameter sharing is helpful\\r\\nin saving resources, the performance is inevitably lower than\\r\\nsingle task baselines when task conflicts exist due to oversharing of parameters between tasks [42] . Furthermore, the\\r\\nfixed architecture suffers from a lack of flexibility, leading\\r\\nto a constant compute cost irrespective of the given task\\r\\npreference or compute budget changes. In many applications\\r\\nwhere the budget can change over time, these approaches\\r\\nmay fail to take advantage of the increased resources in order\\r\\nto improve performance or accordingly lower the compute\\r\\ncost in order to satisfy stricter budget requirements.\\r\\nTo address the aforementioned issue and strike a balance\\r\\nbetween flexibility and performance, we propose a more\\r\\nexpressive tree-structured [15] dynamic multi-task network\\r\\nwhich can adapt its architecture in addition to its weights at\\r\\ntest-time, as illustrated in Figure 1. Specifically, we design a\\r\\ncontroller using two hypernetworks [17] that predict archi-\\r\\n\\r\\n\\x0c\\n\\ntectures and weights, respectively, given a user preference\\r\\nthat specifies test-time trade-offs of relative task importance\\r\\nand resource availability. This increases flexibility by changing branching locations to re-allocate resources over tasks\\r\\nto match user-preferred task importance, and enhance or\\r\\ncompromise task accuracy given computation budget requirements at any given moment. However, this comes at the\\r\\ncost of increase in complexity: 1) generalizing architecture\\r\\nprediction to unseen preferences, and 2) performing dynamic\\r\\nweight changes on potentially thousands of different models.\\r\\nTo tackle these challenges, we develop a two-stage training scheme that starts from an N -stream network, termed\\r\\nthe anchor net, which is initialized using weights from N\\r\\npre-trained single-task models. This guides the architecture\\r\\nsearch as a prior that is preference-agnostic yet captures\\r\\ninter-task relations. In the first stage, we exploit inter-task\\r\\nrelations derived from the anchor net to train the first hypernetwork that predicts connections between the different\\r\\nstreams. We introduce a branching regularized loss that encourages more resource allocation for dominant tasks while\\r\\nreducing the network cost from the less preferred ones. The\\r\\npredicted architectures contain edges that have not been observed during the anchor net initialization. These are denoted\\r\\nas cross-task edges since they connect nodes that belong to\\r\\ndifferent streams. In the second stage, to improve the performance of the predicted architectures with cross-task edges,\\r\\nwe train a secondary hypernetwork for cross-task adaptation\\r\\nvia modulation of the normalization parameters.\\r\\nOur framework is evaluated on three MTL datasets\\r\\n(PASCAL-Context, NYU-v2 and CIFAR-100) in terms of\\r\\ntask performance, computational cost, and controllability\\r\\n(for both task importance and computational cost). Achieving performance comparable to state-of-the-art MTL architecture search methods under uniform task preference, our\\r\\ncontroller can further approximate efficient architectures\\r\\nfor non-uniform preferences with provisions for reducing\\r\\nnetwork size depending on computational constraints.\\r\\nThe primary contributions of our work are as follows:\\r\\n• A controllable multi-task framework which allows users to\\r\\nassign task preference and the trade-off between task performance and network capacity via architectural changes.\\r\\n• A controller, composed of two hypernetworks, to provide\\r\\ndynamic network structure and adapted network weights.\\r\\n• A new joint learning objective including task-related losses\\r\\nand network complexity regularization to achieve the user\\r\\ndefined trade-offs.\\r\\n• Experiments on several MTL benchmarks (PASCALContext [37], NYU-v2 [48], CIFAR-100 [25]) demonstrate\\r\\nthe efficacy of our framework.\\r\\n\\r\\n2. Related Work\\r\\nMulti-Task Learning. Multi-task learning seeks to learn a\\r\\nsingle model to simultaneously solve a variety of learning\\r\\n\\r\\ntasks by sharing information among the tasks [7]. In the context of deep learning, current works focus mostly on designing novel network architectures and constructing efficient\\r\\nshared representation among tasks [42, 59]. Typically, these\\r\\nworks can be grouped into two classes - hard-parameter\\r\\nsharing and soft-parameter sharing. In the soft sharing\\r\\nsetting [14, 36, 43], each task has its own set of backbone\\r\\nparameters with some sort of regularization mechanisms\\r\\nto enforce the distance between weights of the model to be\\r\\nclose. In contrast, the hard sharing setting entails all the tasks\\r\\nsharing the same set of backbone parameters, with branches\\r\\ntowards the outputs [23, 24, 33]. More recent works have attempted learning the optimal architectures via differentiable\\r\\narchitecture search [5, 15, 50]. The overwhelming majority\\r\\nof these approaches are trained using a simple weighted sum\\r\\nof the individual task losses, where a proper set of weights\\r\\nis commonly selected using grid search or using techniques\\r\\nsuch as gradient balancing [9]. Other approaches [29, 35, 47]\\r\\nattempt to model multi-task learning as a multi-objective\\r\\noptimization problem and find Pareto stationary solutions\\r\\namong different tasks. Recently, optimization methods have\\r\\nalso been proposed to manipulate gradients in order to avoid\\r\\nconflicts across tasks [10, 56]. None of these methods are\\r\\nsuitable for dynamically modeling performance trade-offs,\\r\\nwhich is the focus of our work.\\r\\nHypernetworks. A hypernetwork is used to learn context\\r\\ndependent parameters for a dynamic network [17, 46], thus,\\r\\nobtaining multiple customizable models using a single network. Such hypernetworks have been successfully applied\\r\\nin different scenarios, e.g., recurrent networks [17], 3D point\\r\\ncloud prediction [30], video frame prediction [22], neural\\r\\narchitecture search [4] and reinforcement learning [40, 45].\\r\\nRecent works [28,38] propose using hypernetworks to model\\r\\nthe Pareto front of competing multi-task objectives. Our\\r\\napproach is closely related to these works, however, these\\r\\nmethods focus on generating weights for a fixed, handcrafted\\r\\narchitecture, while we use hypernetworks to model the tradeoffs in multi-task learning by varying the architecture. This\\r\\nallows us to take dynamic resource allocation into account,\\r\\nan aspect largely ignored in previous works.\\r\\nDynamic Networks. Dynamic neural networks, as opposed\\r\\nto usual static models, can adapt their structures during inference, leading to notable improvements in performance\\r\\nand computational efficiency [18]. Previous works focus on\\r\\nadjusting the network depth [3, 20, 52, 54], width [26, 57],\\r\\nor perform dynamic routing within a fixed supernet that includes multiple possible paths [27, 32, 39]. Dynamic depth\\r\\nis realised by either early exiting, i.e. allowing “easy” samples to be processed at shallow layers without executing the\\r\\ndeeper layers [3, 20], or layer skipping, i.e. selectively skipping intermediate network layers conditioned on each sample [52, 54]. Dynamic width is an alternative to the dynamic\\r\\ndepth where instead of layers, filters are selectively pruned\\r\\n\\r\\n\\x0c\\n\\nAnchor net training\\r\\n\\r\\nHypernet training\\r\\n\\r\\nInference\\r\\nUse case 1\\r\\n\\r\\nTask preference r\\r\\nResource preference\\r\\n<latexit sha1_base64=\"nROIzrZ3ZbXvAmvV3K3wGwNebNo=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuiG5cV7APasWQymTY0kwxJRilD/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJEs60cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpugDXlTNCWYYbTbqIojgNOO8H4Jvc7j1RpJsW9mSTUj/FQsIgRbKz00A8kD/UktlempoNqza27M6Bl4hWkBgWag+pXP5QkjakwhGOte56bGD/DyjDC6bTSTzVNMBnjIe1ZKnBMtZ/NUk/RiVVCFElljzBopv7eyHCs82h2MsZmpBe9XPzP66UmuvIzJpLUUEHmD0UpR0aivAIUMkWJ4RNLMFHMZkVkhBUmxhZVsSV4i19eJu2zundRP787rzWuizrKcATHcAoeXEIDbqEJLSCg4Ble4c15cl6cd+djPlpyip1D+APn8wdNHJMM</latexit>\\r\\n\\r\\nBranching regularization\\r\\n\\r\\nc\\r\\n<latexit sha1_base64=\"89J+JuC5FxMnvUsR7pwslvrJLdI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOlJuuXK27VnYOsEi8nFcjR6Je/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSvqh6l9Vas1ap3+RxFOEETuEcPLiCOtxBA1rAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHyVOM8A==</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"6tzJeELpzXNCD3srzL9xICxVe30=\">AAACH3icbVDLSsNAFJ34rPUVdekmWAQXUhIp1WXRjcsK9gFNCDfTaTt0JgkzN0IJ/RM3/oobF4qIu/6N08dCWw8MczjnXu69J0oF1+i6E2ttfWNza7uwU9zd2z84tI+OmzrJFGUNmohEtSPQTPCYNZCjYO1UMZCRYK1oeDf1W09MaZ7EjzhKWSChH/Mep4BGCu2qHyWiq0fSfLk/AMz9iCGMx6F3uWr1QcqZF9olt+zO4KwSb0FKZIF6aH/73YRmksVIBWjd8dwUgxwUcirYuOhnmqVAh9BnHUNjkEwH+ey+sXNulK7TS5R5MToz9XdHDlJP9zSVEnCgl72p+J/XybB3E+Q8TjNkMZ0P6mXCwcSZhuV0uWIUxcgQoIqbXR06AAUUTaRFE4K3fPIqaV6VvWq58lAp1W4XcRTIKTkjF8Qj16RG7kmdNAglz+SVvJMP68V6sz6tr3npmrXoOSF/YE1+AMcepKs=</latexit>\\r\\n\\r\\nr 1 c1\\r\\n<latexit sha1_base64=\"Kz4tDovN/gIOBKwQzOMZsJH3pbE=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0wPpev1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vktZF1bus1u5rlfpNHkcRTuAUzsGDK6jDHTSgCQyG8Ayv8OYI58V5dz4WrQUnnzmGP3A+fwDukY2U</latexit>\\r\\n\\r\\nr ⇠ Dir(⌘)\\r\\n\\r\\nc ⇠ Unif(0, 1)\\r\\n\\r\\n<latexit sha1_base64=\"6KeXcoay8Z21X1i+yCDCdl+H99o=\">AAACBHicbVDLSsNAFJ3UV62vqMtugkVwVRIp6rLoxmUF+4AmhJvJpBk6eTAzEUrIwo2/4saFIm79CHf+jZM2C209MMzhnHu59x4vZVRI0/zWamvrG5tb9e3Gzu7e/oF+eDQQScYx6eOEJXzkgSCMxqQvqWRklHICkcfI0JvelP7wgXBBk/hezlLiRDCJaUAxSCW5etP2EuaLWaS+3A5B5jawNISicC1Xb5ltcw5jlVgVaaEKPVf/sv0EZxGJJWYgxNgyU+nkwCXFjBQNOxMkBTyFCRkrGkNEhJPPjyiMU6X4RpBw9WJpzNXfHTlEotxTVUYgQ7HsleJ/3jiTwZWT0zjNJInxYlCQMUMmRpmI4VNOsGQzRQBzqnY1cAgcsFS5NVQI1vLJq2Rw3rYu2p27Tqt7XcVRR010gs6QhS5RF92iHuojjB7RM3pFb9qT9qK9ax+L0ppW9RyjP9A+fwDCP5jQ</latexit>\\r\\n\\r\\nh̄( ¯)\\r\\n<latexit sha1_base64=\"PA/iO9iacJEmHLfybY4K81tMQFw=\">AAAB+nicbVDLSsNAFL2pr1pfqS7dDBahbkoiRV0W3bisYB/QhDKZTpqhkwczE6XEfoobF4q49Uvc+TdO0yy09cDlHs65l7lzvIQzqSzr2yitrW9sbpW3Kzu7e/sHZvWwK+NUENohMY9F38OSchbRjmKK034iKA49Tnve5Gbu9x6okCyO7tU0oW6IxxHzGcFKS0Oz6nhYoKCeNycJ2NnQrFkNKwdaJXZBalCgPTS/nFFM0pBGinAs5cC2EuVmWChGOJ1VnFTSBJMJHtOBphEOqXSz/PQZOtXKCPmx0BUplKu/NzIcSjkNPT0ZYhXIZW8u/ucNUuVfuRmLklTRiCwe8lOOVIzmOaARE5QoPtUEE8H0rYgEWGCidFoVHYK9/OVV0j1v2BeN5l2z1rou4ijDMZxAHWy4hBbcQhs6QOARnuEV3own48V4Nz4WoyWj2DmCPzA+fwDURZMW</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"Jv7CEMs4qXCDLpManfE+dY3CQwk=\">AAAB+XicbVDLSsNAFL2pr1pfUZduBovgqiRS1GXRjcsK9gFtCJPJpB06mYSZSaGE/okbF4q49U/c+TdO2iy09cAwh3PuZc6cIOVMacf5tiobm1vbO9Xd2t7+weGRfXzSVUkmCe2QhCeyH2BFORO0o5nmtJ9KiuOA014wuS/83pRKxRLxpGcp9WI8EixiBGsj+bY9DBIeqllsrlzOfde3607DWQCtE7ckdSjR9u2vYZiQLKZCE46VGrhOqr0cS80Ip/PaMFM0xWSCR3RgqMAxVV6+SD5HF0YJUZRIc4RGC/X3Ro5jVYQzkzHWY7XqFeJ/3iDT0a2XM5FmmgqyfCjKONIJKmpAIZOUaD4zBBPJTFZExlhiok1ZNVOCu/rlddK9arjXjeZjs966K+uowhmcwyW4cAMteIA2dIDAFJ7hFd6s3Hqx3q2P5WjFKndO4Q+szx/1lpPh</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"yjzw6Q8uKJ9Mp1mnW/aT+6SHB8o=\">AAACG3icbVDLSgMxFM3UV62vqks3wSJUKGWmFHVZdOPOCvYBnVLupJlOaOZBkhHKMP/hxl9x40IRV4IL/8a0nUVtPRByOOde7r3HiTiTyjR/jNza+sbmVn67sLO7t39QPDxqyzAWhLZIyEPRdUBSzgLaUkxx2o0EBd/htOOMb6Z+55EKycLgQU0i2vdhFDCXEVBaGhRr9p1PR1C2nZAP5cTXXyLSCqnYHqhkUbWBRx6k6fmgWDKr5gx4lVgZKaEMzUHxyx6GJPZpoAgHKXuWGal+AkIxwmlasGNJIyBjGNGepgH4VPaT2W0pPtPKELuh0C9QeKYudiTgy+mGutIH5cllbyr+5/Vi5V71ExZEsaIBmQ9yY45ViKdB4SETlCg+0QSIYHpXTDwQQJSOs6BDsJZPXiXtWtW6qNbv66XGdRZHHp2gU1RGFrpEDXSLmqiFCHpCL+gNvRvPxqvxYXzOS3NG1nOM/sD4/gWo/6J0</latexit>\\r\\n\\r\\nˆ\\r\\n⌦(r, c, ↵)\\r\\n\\r\\n<latexit sha1_base64=\"bKwNiC4DHcMSQHZ2Bs/6mNqc+KU=\">AAACMHicbVBdS8MwFE3n16xfVR99CQ7HBjJaGerjUEEfJ9htsI6RZtkWlrQlSYVR+pN88afoi4IivvorTLc9uM0LIYdz7k3uOX7EqFS2/W7kVlbX1jfym+bW9s7unrV/0JBhLDBxcchC0fKRJIwGxFVUMdKKBEHcZ6Tpj64zvflIhKRh8KDGEelwNAhon2KkNNW1bovQ80PWk2Our0Sk0JOUQ48jNRQ8uaEiLXlEoTL0PLMI8bzs6qfSkn3qlLtWwa7Yk4LLwJmBAphVvWu9eL0Qx5wECjMkZduxI9VJkFAUM5KaXixJhPAIDUhbwwBxIjvJxHAKTzTTg/1Q6BMoOGH/TiSIy8yR7sw2lYtaRv6ntWPVv+wkNIhiRQI8/agfM6hCmKUHe1QQrNhYA4QF1btCPEQCYaUzNnUIzqLlZdA4qzjnlep9tVC7msWRB0fgGJSAAy5ADdyBOnABBk/gFXyAT+PZeDO+jO9pa86YzRyCuTJ+fgHwrqhC</latexit>\\r\\n\\r\\n↵\\r\\nˆ1\\r\\n\\r\\n<latexit sha1_base64=\"TUwc5JUXLkRW1tXLWckO+skY4rs=\">AAAB7nicbVBNSwMxEJ2tX7V+VT16CRahXsqulOqx6MVjBfsB7VKyabYbms2GJCuUpT/CiwdFvPp7vPlvTNs9aOuDgcd7M8zMCyRn2rjut1PY2Nza3inulvb2Dw6PyscnHZ2kitA2SXiiegHWlDNB24YZTntSURwHnHaDyd3c7z5RpVkiHs1UUj/GY8FCRrCxUjeqDmTELofliltzF0DrxMtJBXK0huWvwSghaUyFIRxr3fdcafwMK8MIp7PSINVUYjLBY9q3VOCYaj9bnDtDF1YZoTBRtoRBC/X3RIZjradxYDtjbCK96s3F/7x+asIbP2NCpoYKslwUphyZBM1/RyOmKDF8agkmitlbEYmwwsTYhEo2BG/15XXSuap5jVr9oV5p3uZxFOEMzqEKHlxDE+6hBW0gMIFneIU3RzovzrvzsWwtOPnMKfyB8/kDooKPHw==</latexit>\\r\\n\\r\\nh( )\\r\\n\\r\\n(Eq. 6)\\r\\n\\r\\nˆ1 , ˆ 1\\r\\n\\r\\nc\\r\\n\\r\\nr\\r\\n\\r\\n<latexit sha1_base64=\"89J+JuC5FxMnvUsR7pwslvrJLdI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOlJuuXK27VnYOsEi8nFcjR6Je/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSvqh6l9Vas1ap3+RxFOEETuEcPLiCOtxBA1rAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHyVOM8A==</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"nROIzrZ3ZbXvAmvV3K3wGwNebNo=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuiG5cV7APasWQymTY0kwxJRilD/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJEs60cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpugDXlTNCWYYbTbqIojgNOO8H4Jvc7j1RpJsW9mSTUj/FQsIgRbKz00A8kD/UktlempoNqza27M6Bl4hWkBgWag+pXP5QkjakwhGOte56bGD/DyjDC6bTSTzVNMBnjIe1ZKnBMtZ/NUk/RiVVCFElljzBopv7eyHCs82h2MsZmpBe9XPzP66UmuvIzJpLUUEHmD0UpR0aivAIUMkWJ4RNLMFHMZkVkhBUmxhZVsSV4i19eJu2zundRP787rzWuizrKcATHcAoeXEIDbqEJLSCg4Ble4c15cl6cd+djPlpyip1D+APn8wdNHJMM</latexit>\\r\\n\\r\\n↵\\r\\nˆ\\r\\n<latexit sha1_base64=\"ddV0gkQr1t2fjXmPQKTNGE+DwJ0=\">AAACAnicdVDLSgMxFM3UV62vUVfiJlgEVyUjtba7ohuXFewDOqXcyaRtaOZBkhHKUNz4K25cKOLWr3Dn35hpK6jogZDDOfdy7z1eLLjShHxYuaXlldW1/HphY3Nre8fe3WupKJGUNWkkItnxQDHBQ9bUXAvWiSWDwBOs7Y0vM799y6TiUXijJzHrBTAM+YBT0Ebq2weuFwlfTQLzpe4IdOqCiEcwnfbtIimdEadWqWBSIsQpVx1DarWqEbFjlAxFtECjb7+7fkSTgIWaClCq65BY91KQmlPBpgU3USwGOoYh6xoaQsBUL52dMMXHRvHxIJLmhRrP1O8dKQQq29JUBqBH6reXiX953UQPqr2Uh3GiWUjngwaJwDrCWR7Y55JRLSaGAJXc7IrpCCRQbVIrmBC+LsX/k9ZpyamUytflYv1iEUceHaIjdIIcdI7q6Ao1UBNRdIce0BN6tu6tR+vFep2X5qxFzz76AevtE/GqmHU=</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"TUwc5JUXLkRW1tXLWckO+skY4rs=\">AAAB7nicbVBNSwMxEJ2tX7V+VT16CRahXsqulOqx6MVjBfsB7VKyabYbms2GJCuUpT/CiwdFvPp7vPlvTNs9aOuDgcd7M8zMCyRn2rjut1PY2Nza3inulvb2Dw6PyscnHZ2kitA2SXiiegHWlDNB24YZTntSURwHnHaDyd3c7z5RpVkiHs1UUj/GY8FCRrCxUjeqDmTELofliltzF0DrxMtJBXK0huWvwSghaUyFIRxr3fdcafwMK8MIp7PSINVUYjLBY9q3VOCYaj9bnDtDF1YZoTBRtoRBC/X3RIZjradxYDtjbCK96s3F/7x+asIbP2NCpoYKslwUphyZBM1/RyOmKDF8agkmitlbEYmwwsTYhEo2BG/15XXSuap5jVr9oV5p3uZxFOEMzqEKHlxDE+6hBW0gMIFneIU3RzovzrvzsWwtOPnMKfyB8/kDooKPHw==</latexit>\\r\\n\\r\\nh( )\\r\\n\\r\\nUse case 2\\r\\n\\r\\nNetwork structure\\r\\n\\r\\n<latexit sha1_base64=\"solfB5WK9+GukOYBUSpf7T8CjQ8=\">AAACH3icbVDLSsNAFJ3UV62vqks3wSK4kJKUUl0W3bisYB/QhHAznbZDZ5IwcyOU0D9x46+4caGIuOvfOH0stPXAMIdz7uXee8JEcI2OM7VyG5tb2zv53cLe/sHhUfH4pKXjVFHWpLGIVScEzQSPWBM5CtZJFAMZCtYOR3czv/3ElOZx9IjjhPkSBhHvcwpopKBY88JY9PRYmi/zhoCZFzKEySSoXK1bA5By7gXFklN25rDXibskJbJEIyh+e72YppJFSAVo3XWdBP0MFHIq2KTgpZolQEcwYF1DI5BM+9n8vol9YZSe3Y+VeRHac/V3RwZSz/Y0lRJwqFe9mfif102xf+NnPEpSZBFdDOqnwsbYnoVl97hiFMXYEKCKm11tOgQFFE2kBROCu3ryOmlVym6tXH2oluq3yzjy5Iyck0vikmtSJ/ekQZqEkmfySt7Jh/VivVmf1teiNGcte07JH1jTH8pCpK0=</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"oUrZfFHrjoHN2+J2AmPoqEYCTvQ=\">AAACDnicdVBNa9tAEF05beO6X2py7GWpMfRQzMq4jn0zySXHFOoPsIQZrdf24l1J7I4KRugX9NK/kksOCaXXnHPLv8kqdqEt7YNlH+/NMDMvzpS0yNi9Vzt48vTZYf1548XLV6/f+G+PxjbNDRcjnqrUTGOwQslEjFCiEtPMCNCxEpN4c1b5k6/CWJkmX3CbiUjDKpFLyQGdNPdbYZyqhd1q9xXhGrAIY4FQftzxFWgNZTn3m6w96PdYt0NZm7F+h/Uc+cSCQTCggVMqNMkeF3P/LlykPNciQa7A2lnAMowKMCi5EmUjzK3IgG9gJWaOJqCFjYrHc0racsqCLlPjXoL0Uf29owBtq41dpQZc27+9SvyXN8tx2Y8KmWQ5ioTvBi1zRTGlVTZ0IY3gqLaOADfS7Ur5GgxwdAk2XAi/LqX/J+NOO+i1u5+7zeHpPo46eUfekw8kICdkSM7JBRkRTr6RS3JNbrzv3pX3w/u5K615+55j8ge82wdfWZ2c</latexit>\\r\\n\\r\\nh̄( ¯)\\r\\n<latexit sha1_base64=\"PA/iO9iacJEmHLfybY4K81tMQFw=\">AAAB+nicbVDLSsNAFL2pr1pfqS7dDBahbkoiRV0W3bisYB/QhDKZTpqhkwczE6XEfoobF4q49Uvc+TdO0yy09cDlHs65l7lzvIQzqSzr2yitrW9sbpW3Kzu7e/sHZvWwK+NUENohMY9F38OSchbRjmKK034iKA49Tnve5Gbu9x6okCyO7tU0oW6IxxHzGcFKS0Oz6nhYoKCeNycJ2NnQrFkNKwdaJXZBalCgPTS/nFFM0pBGinAs5cC2EuVmWChGOJ1VnFTSBJMJHtOBphEOqXSz/PQZOtXKCPmx0BUplKu/NzIcSjkNPT0ZYhXIZW8u/ucNUuVfuRmLklTRiCwe8lOOVIzmOaARE5QoPtUEE8H0rYgEWGCidFoVHYK9/OVV0j1v2BeN5l2z1rou4ijDMZxAHWy4hBbcQhs6QOARnuEV3own48V4Nz4WoyWj2DmCPzA+fwDURZMW</latexit>\\r\\n\\r\\nL1\\r\\n<latexit sha1_base64=\"z7lq4xAHxZF7ezwqGmk83FFU6j4=\">AAAB9HicdVDNSsNAGNzUv1r/qh69LBbBU0jS0jS3ohcPHipYW2hD2Ww37dLNJu5uCiX0Obx4UMSrD+PNt3HTVlDRgYVh5vv4ZidIGJXKsj6Mwtr6xuZWcbu0s7u3f1A+PLqTcSowaeOYxaIbIEkY5aStqGKkmwiCooCRTjC5zP3OlAhJY36rZgnxIzTiNKQYKS35/QipMUYsu54P7EG5YpmeV6/aDrTMhlP3PEuTquO6bhXaprVABazQGpTf+8MYpxHhCjMkZc+2EuVnSCiKGZmX+qkkCcITNCI9TTmKiPSzReg5PNPKEIax0I8ruFC/b2QoknIWBXoyDyl/e7n4l9dLVdjwM8qTVBGOl4fClEEVw7wBOKSCYMVmmiAsqM4K8RgJhJXuqaRL+Pop/J/cOaZdN2s3tUrzYlVHEZyAU3AObOCCJrgCLdAGGNyDB/AEno2p8Wi8GK/L0YKx2jkGP2C8fQI4V5Js</latexit>\\r\\n\\r\\nL2\\r\\n<latexit sha1_base64=\"7lu9AbgOWwQHkh3jtmtJf9earfc=\">AAAB9HicdVDLSgMxFM3UV62vqks3wSK4GjLt2E53RTcuXFSwD2iHkkkzbWjmYZIplKHf4caFIm79GHf+jZm2gooeCBzOuZd7cryYM6kQ+jBya+sbm1v57cLO7t7+QfHwqC2jRBDaIhGPRNfDknIW0pZiitNuLCgOPE473uQq8ztTKiSLwjs1i6kb4FHIfEaw0pLbD7AaE8zTm/mgPCiWkFlFdrlSh8isOXWEyprYzoVTq0DLRAuUwArNQfG9P4xIEtBQEY6l7FkoVm6KhWKE03mhn0gaYzLBI9rTNMQBlW66CD2HZ1oZQj8S+oUKLtTvGykOpJwFnp7MQsrfXib+5fUS5TtuysI4UTQky0N+wqGKYNYAHDJBieIzTTARTGeFZIwFJkr3VNAlfP0U/k/aZdOqmvatXWpcrurIgxNwCs6BBWqgAa5BE7QAAffgATyBZ2NqPBovxutyNGesdo7BDxhvny3QkmU=</latexit>\\r\\n\\r\\nL3\\r\\n<latexit sha1_base64=\"oCOQuqiB7S6qwSVmnTtJuAdz71Y=\">AAAB9HicdVDLSsNAFJ3UV62vqks3g0VwFRKNaborunHhooJ9QBvKZDpph04mcWZSKKHf4caFIm79GHf+jZO2gooeGDiccy/3zAkSRqWyrA+jsLK6tr5R3Cxtbe/s7pX3D1oyTgUmTRyzWHQCJAmjnDQVVYx0EkFQFDDSDsZXud+eECFpzO/UNCF+hIachhQjpSW/FyE1wohlN7P+eb9cscwLx3PtGrTMqlVzXVsTr+rVXAfapjVHBSzR6Jffe4MYpxHhCjMkZde2EuVnSCiKGZmVeqkkCcJjNCRdTTmKiPSzeegZPNHKAIax0I8rOFe/b2QoknIaBXoyDyl/e7n4l9dNVej5GeVJqgjHi0NhyqCKYd4AHFBBsGJTTRAWVGeFeIQEwkr3VNIlfP0U/k9aZ6btms6tU6lfLusogiNwDE6BDaqgDq5BAzQBBvfgATyBZ2NiPBovxutitGAsdw7BDxhvn0rfknk=</latexit>\\r\\n\\r\\nˆ, ˆ\\r\\n\\r\\nBatch-norm weights\\r\\n\\r\\nh̄( ¯)\\r\\n<latexit sha1_base64=\"PA/iO9iacJEmHLfybY4K81tMQFw=\">AAAB+nicbVDLSsNAFL2pr1pfqS7dDBahbkoiRV0W3bisYB/QhDKZTpqhkwczE6XEfoobF4q49Uvc+TdO0yy09cDlHs65l7lzvIQzqSzr2yitrW9sbpW3Kzu7e/sHZvWwK+NUENohMY9F38OSchbRjmKK034iKA49Tnve5Gbu9x6okCyO7tU0oW6IxxHzGcFKS0Oz6nhYoKCeNycJ2NnQrFkNKwdaJXZBalCgPTS/nFFM0pBGinAs5cC2EuVmWChGOJ1VnFTSBJMJHtOBphEOqXSz/PQZOtXKCPmx0BUplKu/NzIcSjkNPT0ZYhXIZW8u/ucNUuVfuRmLklTRiCwe8lOOVIzmOaARE5QoPtUEE8H0rYgEWGCidFoVHYK9/OVV0j1v2BeN5l2z1rou4ijDMZxAHWy4hBbcQhs6QOARnuEV3own48V4Nz4WoyWj2DmCPzA+fwDURZMW</latexit>\\r\\n\\r\\nr 2 c2\\r\\n<latexit sha1_base64=\"tJNs+JB1fXFcKJB0r0XypTNRXNk=\">AAAB+XicbVBPS8MwHP11/pvzX9Wjl+AQPI12DPU49OJxgpuDrZQ0TbewNC1JOhhl38SLB0W8+k28+W1Mtx5080HI473fj7y8IOVMacf5tiobm1vbO9Xd2t7+weGRfXzSU0kmCe2ShCeyH2BFORO0q5nmtJ9KiuOA06dgclf4T1MqFUvEo56l1IvxSLCIEayN5Nv2MEh4qGaxuXI595u+XXcazgJonbglqUOJjm9/DcOEZDEVmnCs1MB1Uu3lWGpGOJ3XhpmiKSYTPKIDQwWOqfLyRfI5ujBKiKJEmiM0Wqi/N3IcqyKcmYyxHqtVrxD/8waZjm68nIk001SQ5UNRxpFOUFEDCpmkRPOZIZhIZrIiMsYSE23KqpkS3NUvr5Nes+FeNVoPrXr7tqyjCmdwDpfgwjW04R460AUCU3iGV3izcuvFerc+lqMVq9w5hT+wPn8A9xqT4g==</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"au9Q6pcuOAuEpFDMuI3IuqzA6I4=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKUY9FLx4r2lpoQ9lsJ+3SzSbsboQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgbjm5n/+IRK81g+mEmCfkSHkoecUWOle9av9csVt+rOQVaJl5MK5Gj2y1+9QczSCKVhgmrd9dzE+BlVhjOB01Iv1ZhQNqZD7FoqaYTaz+anTsmZVQYkjJUtachc/T2R0UjrSRTYzoiakV72ZuJ/Xjc14ZWfcZmkBiVbLApTQUxMZn+TAVfIjJhYQpni9lbCRlRRZmw6JRuCt/zyKmnXqt5FtX5XrzSu8ziKcAKncA4eXEIDbqEJLWAwhGd4hTdHOC/Ou/OxaC04+cwx/IHz+QPwFY2V</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"9rve+kHtF2nuu2jGsKdX97j9g5s=\">AAACPHicbVC7TsMwFHV4U14BRhaLCgmWKkEVsCAhWBgYQNCH1ESR47pg1U4i+wZRRfkwFj6CjYmFAYRYmXFKhrZwJctH59xrn3vCRHANjvNiTU3PzM7NLyxWlpZXVtfs9Y2mjlNFWYPGIlbtkGgmeMQawEGwdqIYkaFgrbB/Vuite6Y0j6MbGCTMl+Q24j1OCRgqsK89SeCOEpFd5EHmAXsAJTMgup/nu14Yi64eSHNlKt/Dx9jTqQw4HhcKYuQVHthVp+YMC/8FbgmqqKzLwH72ujFNJYuACqJ1x3US8DOigFPB8oqXapYQ2ie3rGNgRCTTfjZcPsc7huniXqzMiQAP2dGJjEhdWDWdhUk9qRXkf1onhd6Rn/EoSYFF9PejXiowxLhIEne5YhTEwABCFTdeMb0jilAweVdMCO7kyn9Bc7/mHtTqV/XqyWkZxwLaQttoF7noEJ2gc3SJGoiiR/SK3tGH9WS9WZ/W12/rlFXObKKxsr5/AAnVsPk=</latexit>\\r\\n\\r\\nLtask (r) =\\r\\n\\r\\nX\\r\\n\\r\\nTask loss\\r\\n\\r\\ni\\r\\n\\r\\n↵\\r\\nˆ2\\r\\nˆ2 , ˆ 2\\r\\n<latexit sha1_base64=\"J+e4Wz9VBRVfE/l5RbpWSKDdCxc=\">AAACBHicbVDLSsNAFJ3UV62vqMtugkVwVZJS1GXRjcsKthWaEG4mk2bo5MHMRCghCzf+ihsXirj1I9z5N07aLLT1wDCHc+7l3nu8lFEhTfNbq62tb2xu1bcbO7t7+wf64dFQJBnHZIATlvB7DwRhNCYDSSUj9yknEHmMjLzpdemPHggXNInv5CwlTgSTmAYUg1SSqzdtL2G+mEXqy+0QZG4DS0MoCrfj6i2zbc5hrBKrIi1Uoe/qX7af4CwiscQMhBhbZiqdHLikmJGiYWeCpICnMCFjRWOIiHDy+RGFcaoU3wgSrl4sjbn6uyOHSJR7qsoIZCiWvVL8zxtnMrh0chqnmSQxXgwKMmbIxCgTMXzKCZZspghgTtWuBg6BA5Yqt4YKwVo+eZUMO23rvN297bZ6V1UcddREJ+gMWegC9dAN6qMBwugRPaNX9KY9aS/au/axKK1pVc8x+gPt8wfDw5jR</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"TUwc5JUXLkRW1tXLWckO+skY4rs=\">AAAB7nicbVBNSwMxEJ2tX7V+VT16CRahXsqulOqx6MVjBfsB7VKyabYbms2GJCuUpT/CiwdFvPp7vPlvTNs9aOuDgcd7M8zMCyRn2rjut1PY2Nza3inulvb2Dw6PyscnHZ2kitA2SXiiegHWlDNB24YZTntSURwHnHaDyd3c7z5RpVkiHs1UUj/GY8FCRrCxUjeqDmTELofliltzF0DrxMtJBXK0huWvwSghaUyFIRxr3fdcafwMK8MIp7PSINVUYjLBY9q3VOCYaj9bnDtDF1YZoTBRtoRBC/X3RIZjradxYDtjbCK96s3F/7x+asIbP2NCpoYKslwUphyZBM1/RyOmKDF8agkmitlbEYmwwsTYhEo2BG/15XXSuap5jVr9oV5p3uZxFOEMzqEKHlxDE+6hBW0gMIFneIU3RzovzrvzsWwtOPnMKfyB8/kDooKPHw==</latexit>\\r\\n\\r\\nh( )\\r\\n\\r\\nr i Li\\r\\n\\r\\nFigure 2. Overview of framework. We initialize our framework using an anchor net which consists of single-task networks. During\\r\\ntraining, we first train the edge hypernet h(ϕ) using sampled preferences (r, c) to optimize the task loss and a branching regularizer, for\\r\\npreference aware branching. Next, we optimize the weight hypernet h̄(ϕ̄) in a similar fashion by minimizing only the task loss. At inference,\\r\\nthe hypernets jointly predict architecture and weights according to the user preferences.\\r\\n\\r\\nconditioned on the input [26, 57]. Dynamic routing can be\\r\\nimplemented by learning controllers to selectively execute\\r\\none of multiple candidate modules at each layer [32, 39].\\r\\nDue to the non-differentiable nature of the discrete choices,\\r\\nreinforcement learning is employed to learn these controllers.\\r\\nIn [27], the routing modules utilize a differentiable activation\\r\\nfunction which conditionally outputs zero values, facilitating\\r\\nthe end-to-end training of routing decisions. Recent works\\r\\nhave also proposed learning dynamic weights for modeling\\r\\ndifferent hyperparameter configurations [11] and domain\\r\\nadaptation [53]. In contrast to most of the existing works\\r\\nwhich intrinsically adapt network structures as a function\\r\\nof input, our method enables explicit control of the total\\r\\ncomputational cost as well as the task trade-offs.\\r\\nWeight Sharing Neural Architecture Search. Weight\\r\\nsharing has evolved as a powerful tool to amortize computational cost across models for neural architecture search\\r\\n(NAS). These methods integrate the whole search space of\\r\\narchitectures into a weight sharing supernet and optimize\\r\\nnetwork architectures by pursuing the best performing subnetworks. Joint optimization methods [6,31,55] optimize the\\r\\nweights of the supernet and a differentiable routing policy\\r\\nsimultaneously. In contrast, one-shot methods [1, 2, 4, 16]\\r\\ndisentangle the training into two steps: first, the weights of\\r\\nthe supernet are trained, after which the agent is trained with\\r\\nthe fixed supernet. We utilize such a weight sharing strategy\\r\\nin our framework for dynamic resource allocation.\\r\\n\\r\\n3. Method\\r\\nGiven a set of N tasks T = {T1 , T2 , . . . , TN }, conventional multi-task learning seeks to minimize\\r\\na weighted sum\\r\\nP\\r\\nof task-specific losses: Ltask (r) = i ri Li , where each Li\\r\\nrepresents the loss associated with task Ti , and r denotes\\r\\na task preference vector. This vector signifies the desired\\r\\nperformance trade-off across the different tasks, with larger\\r\\nvalues of ri denoting higher importance to task Ti . Here\\r\\n\\r\\nP\\r\\nr ∈ SN , where SN = {r ∈ RN | i ri = 1, ri ≥ 0}\\r\\nrepresents the N -dimensional simplex [42]. We seek to approximate the trade-off curve defined by different values of\\r\\nr using tree-structured sub-networks [15] within a single\\r\\nmulti-task model, given a total computational budget defined\\r\\nby a resource preference variable c ∈ [0, 1], where larger c\\r\\ndenotes more frugal resource usage. This is formulated as a\\r\\nminimization of the expected value of the task loss over the\\r\\nuser preference distribution, with regularization Ω to control\\r\\nresource usage, i.e., E(r,c)∼P(r,c) Ltask (r) + Ω(r, c). Optimizing this directly is equivalent to solving NAS [31] for every possible (r, c) simultaneously. Thus, instead of solving\\r\\ndirectly, we cast it as a search to find tree sub-structures and\\r\\nthe corresponding modulation of features for every (r, c),\\r\\nwithin an N -stream anchor network with fixed weights.\\r\\nOur framework consists of two hypernets (h and h̄) [17]\\r\\nand an anchor net F, as shown in Figure 2. At test-time,\\r\\ngiven an input preference, we utilize the network connections\\r\\nand adapted weights predicted by the hypernets to modulate\\r\\nF, to obtain the final model. We propose a two-stage training\\r\\nscheme to train the framework. First, we initialize a preference agnostic anchor net, which provides the anchor weights\\r\\nat test time (Section 3.1). Based on this anchor net, the\\r\\ntree-structured architecture search space is then defined (Section 3.2). Next, we train the edge hypernet using prior task\\r\\nrelations obtained from the anchor net by optimizing a novel\\r\\nbranching regularized loss function derived by inducing a\\r\\ndichotomy over the tasks (Section 3.3.1). Finally, we train a\\r\\nweight hypernet, keeping the anchor net and edge hypernet\\r\\nfixed, to modulate the anchor net weights (Section 3.3.2).\\r\\n\\r\\n3.1. Anchor Network\\r\\nWe introduce an anchor net F as an alternative approach to model weight generation in dynamic networks\\r\\nfor MTL [28, 38]. Previous methods adopt chunking [17] to\\r\\nmitigate the large computation and memory required for generating entire network weights at the expense of limiting the\\r\\n\\r\\n\\x0c\\n\\n...\\r\\n\\r\\ndetermines a unique tree structure. This suggests learning\\r\\nα = {αjl }0≤j≤N,0≤l<L , conditioned on a preference (r, c),\\r\\nin a manner which satisfies the desired task trade-offs. Here,\\r\\nL denotes the total number of layers.\\r\\n\\r\\n<latexit sha1_base64=\"t6iHjZmz9uJaFfSHS0x8E4VrNJE=\">AAAB7XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae0Q8mkmTY2kxmSO0Ip/Qc3LhRx6/+482/MtLPQ1gOBwzn3kHtPkEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7Gt5nffuLaiFg94CThfkSHSoSCUbRSqzeI0ZT65Ypbdecgq8TLSQVyNPrlLxtkacQVMkmN6Xpugv6UahRM8lmplxqeUDamQ961VNGIG38633ZGzqwyIGGs7VNI5urvxJRGxkyiwE5GFEdm2cvE/7xuiuG1PxUqSZErtvgoTCXBmGSnk4HQnKGcWEKZFnZXwkZUU4a2oKwEb/nkVdK6qHqX1dp9rVK/yesowgmcwjl4cAV1uIMGNIHBIzzDK7w5sfPivDsfi9GCk2eO4Q+czx8qj47f</latexit>\\r\\n\\r\\ny2l\\r\\n<latexit sha1_base64=\"Re4POen4A1Apj+WbKPb1bjcej5w=\">AAAB7HicbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6rHoxWMF0xbaWDbbTbt0dxN2N0II/Q1ePCji1R/kzX/jts1BWx8MPN6bYWZemHCmjet+O2vrG5tb26Wd8u7e/sFh5ei4reNUEeqTmMeqG2JNOZPUN8xw2k0UxSLktBNObmd+54kqzWL5YLKEBgKPJIsYwcZKfjaoP/JBperW3DnQKvEKUoUCrUHlqz+MSSqoNIRjrXuem5ggx8owwum03E81TTCZ4BHtWSqxoDrI58dO0blVhiiKlS1p0Fz9PZFjoXUmQtspsBnrZW8m/uf1UhNdBzmTSWqoJItFUcqRidHsczRkihLDM0swUczeisgYK0yMzadsQ/CWX14l7XrNu6w17hvV5k0RRwlO4QwuwIMraMIdtMAHAgye4RXeHOm8OO/Ox6J1zSlmTuAPnM8fkCOOiQ==</latexit>\\r\\n\\r\\ny1l\\r\\n<latexit sha1_base64=\"XqiR/L7vdaY7RGae/4vJ8gh3r5Y=\">AAAB7HicbVBNS8NAEJ2tX7V+VT16WSyCp5JIUY9FLx4rmLbQxrLZbtqlm03Y3Qgh9Dd48aCIV3+QN/+N2zYHbX0w8Hhvhpl5QSK4No7zjUpr6xubW+Xtys7u3v5B9fCoreNUUebRWMSqGxDNBJfMM9wI1k0UI1EgWCeY3M78zhNTmsfywWQJ8yMykjzklBgrednAfRSDas2pO3PgVeIWpAYFWoPqV38Y0zRi0lBBtO65TmL8nCjDqWDTSj/VLCF0QkasZ6kkEdN+Pj92is+sMsRhrGxJg+fq74mcRFpnUWA7I2LGetmbif95vdSE137OZZIaJuliUZgKbGI8+xwPuWLUiMwSQhW3t2I6JopQY/Op2BDc5ZdXSfui7l7WG/eNWvOmiKMMJ3AK5+DCFTThDlrgAQUOz/AKb0iiF/SOPhatJVTMHMMfoM8fjp2OiA==</latexit>\\r\\n\\r\\n↵1l (1)\\r\\n\\r\\nl\\r\\nyN\\r\\n<latexit sha1_base64=\"CuH3nXAwv0a5hBiKw5MrffsHtfs=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRiyepYNpCG8tmu2mXbnbD7kYIob/BiwdFvPqDvPlv3LY5aOuDgcd7M8zMCxPOtHHdb6e0srq2vlHerGxt7+zuVfcPWlqmilCfSC5VJ8Saciaob5jhtJMoiuOQ03Y4vpn67SeqNJPiwWQJDWI8FCxiBBsr+Vn/7pH3qzW37s6AlolXkBoUaParX72BJGlMhSEca9313MQEOVaGEU4nlV6qaYLJGA9p11KBY6qDfHbsBJ1YZYAiqWwJg2bq74kcx1pncWg7Y2xGetGbiv953dREV0HORJIaKsh8UZRyZCSafo4GTFFieGYJJorZWxEZYYWJsflUbAje4svLpHVW9y7q5/fntcZ1EUcZjuAYTsGDS2jALTTBBwIMnuEV3hzhvDjvzse8teQUM4fwB87nD7rLjqU=</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"e6z91mYk/5UATcVx9O+F4g62S/I=\">AAAB9HicbVDLTgJBEOzFF+IL9ehlIjHBC9k1RD0SvXjERB4JrKR3mIUJsw9nZknIhu/w4kFjvPox3vwbB9iDgpV0UqnqTneXFwuutG1/W7m19Y3Nrfx2YWd3b/+geHjUVFEiKWvQSESy7aFigoesobkWrB1LhoEnWMsb3c781phJxaPwQU9i5gY4CLnPKWojuV0U8RAfRc8pO+e9Ysmu2HOQVeJkpAQZ6r3iV7cf0SRgoaYCleo4dqzdFKXmVLBpoZsoFiMd4YB1DA0xYMpN50dPyZlR+sSPpKlQk7n6eyLFQKlJ4JnOAPVQLXsz8T+vk2j/2k15GCeahXSxyE8E0RGZJUD6XDKqxcQQpJKbWwkdokSqTU4FE4Kz/PIqaV5UnMtK9b5aqt1kceThBE6hDA5cQQ3uoA4NoPAEz/AKb9bYerHerY9Fa87KZo7hD6zPH3pVkUM=</latexit>\\r\\n\\r\\n↵1l (2)\\r\\n<latexit sha1_base64=\"WhzVOswy5cpy86j6LYKMkO6IAdg=\">AAAB9HicbVBNS8NAEJ34WetX1aOXYBHqpSSlqMeiF48V7Ae0sUy2m3bpZhN3N4US+ju8eFDEqz/Gm//GbZuDtj4YeLw3w8w8P+ZMacf5ttbWNza3tnM7+d29/YPDwtFxU0WJJLRBIh7Jto+KciZoQzPNaTuWFEOf05Y/up35rTGVikXiQU9i6oU4ECxgBLWRvC7yeIiPvOeWKhe9QtEpO3PYq8TNSBEy1HuFr24/IklIhSYcleq4Tqy9FKVmhNNpvpsoGiMZ4YB2DBUYUuWl86On9rlR+nYQSVNC23P190SKoVKT0DedIeqhWvZm4n9eJ9HBtZcyESeaCrJYFCTc1pE9S8DuM0mJ5hNDkEhmbrXJECUSbXLKmxDc5ZdXSbNSdi/L1ftqsXaTxZGDUziDErhwBTW4gzo0gMATPMMrvFlj68V6tz4WrWtWNnMCf2B9/gB72pFE</latexit>\\r\\n\\r\\n↵1l (i)\\r\\n<latexit sha1_base64=\"8Vamo8cPWpOrHkzygCZG8K7u6Nw=\">AAAB9HicbVDLTgJBEOzFF+IL9ehlIjHBC9k1RD0SvXjERB4JrKR3mIUJsw9nZknIhu/w4kFjvPox3vwbB9iDgpV0UqnqTneXFwuutG1/W7m19Y3Nrfx2YWd3b/+geHjUVFEiKWvQSESy7aFigoesobkWrB1LhoEnWMsb3c781phJxaPwQU9i5gY4CLnPKWojuV0U8RAfRc8p8/NesWRX7DnIKnEyUoIM9V7xq9uPaBKwUFOBSnUcO9ZuilJzKti00E0Ui5GOcMA6hoYYMOWm86On5MwofeJH0lSoyVz9PZFioNQk8ExngHqolr2Z+J/XSbR/7aY8jBPNQrpY5CeC6IjMEiB9LhnVYmIIUsnNrYQOUSLVJqeCCcFZfnmVNC8qzmWlel8t1W6yOPJwAqdQBgeuoAZ3UIcGUHiCZ3iFN2tsvVjv1seiNWdlM8fwB9bnD89tkXs=</latexit>\\r\\n\\r\\n⇠\\r\\n<latexit sha1_base64=\"DyL84uHYKouib8MrcE4AVzBROKw=\">AAAB63icbVBNSwMxEJ3Ur1q/qh69BIvgqeyKqMeiF48V7Ae0S8mm2TY0yS5JVihL/4IXD4p49Q9589+YbfegrQ8GHu/NMDMvTAQ31vO+UWltfWNzq7xd2dnd2z+oHh61TZxqylo0FrHuhsQwwRVrWW4F6yaaERkK1gknd7nfeWLa8Fg92mnCAklGikecEptLfcPloFrz6t4ceJX4BalBgeag+tUfxjSVTFkqiDE930tskBFtORVsVumnhiWETsiI9RxVRDITZPNbZ/jMKUMcxdqVsniu/p7IiDRmKkPXKYkdm2UvF//zeqmNboKMqyS1TNHFoigV2MY4fxwPuWbUiqkjhGrubsV0TDSh1sVTcSH4yy+vkvZF3b+qXz5c1hq3RRxlOIFTOAcfrqEB99CEFlAYwzO8whuS6AW9o49FawkVM8fwB+jzByKdjlA=</latexit>\\r\\n\\r\\nxl+1\\r\\n1\\r\\n<latexit sha1_base64=\"isc76EIgam+9+b1KsWXe1s9m0Ns=\">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRZBEMqulOqx6MVjBfsh7VqyabYNTbJLkhXL0l/hxYMiXv053vw3pu0etPXBwOO9GWbmBTFn2rjut5NbWV1b38hvFra2d3b3ivsHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC0fXUbz1SpVkk78w4pr7AA8lCRrCx0v1Tz3tI+Zk36RVLbtmdAS0TLyMlyFDvFb+6/YgkgkpDONa647mx8VOsDCOcTgrdRNMYkxEe0I6lEguq/XR28ASdWKWPwkjZkgbN1N8TKRZaj0VgOwU2Q73oTcX/vE5iwks/ZTJODJVkvihMODIRmn6P+kxRYvjYEkwUs7ciMsQKE2MzKtgQvMWXl0nzvOxVy5XbSql2lcWRhyM4hlPw4AJqcAN1aAABAc/wCm+Ocl6cd+dj3ppzsplD+APn8wctnZAD</latexit>\\r\\n\\r\\n⇠\\r\\n<latexit sha1_base64=\"DyL84uHYKouib8MrcE4AVzBROKw=\">AAAB63icbVBNSwMxEJ3Ur1q/qh69BIvgqeyKqMeiF48V7Ae0S8mm2TY0yS5JVihL/4IXD4p49Q9589+YbfegrQ8GHu/NMDMvTAQ31vO+UWltfWNzq7xd2dnd2z+oHh61TZxqylo0FrHuhsQwwRVrWW4F6yaaERkK1gknd7nfeWLa8Fg92mnCAklGikecEptLfcPloFrz6t4ceJX4BalBgeag+tUfxjSVTFkqiDE930tskBFtORVsVumnhiWETsiI9RxVRDITZPNbZ/jMKUMcxdqVsniu/p7IiDRmKkPXKYkdm2UvF//zeqmNboKMqyS1TNHFoigV2MY4fxwPuWbUiqkjhGrubsV0TDSh1sVTcSH4yy+vkvZF3b+qXz5c1hq3RRxlOIFTOAcfrqEB99CEFlAYwzO8whuS6AW9o49FawkVM8fwB+jzByKdjlA=</latexit>\\r\\n\\r\\nxl+1\\r\\n2\\r\\n<latexit sha1_base64=\"EVySsrDuMqRwWJ/E6oOD0LLD69o=\">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRZBEMpuKeqx6MVjBfsh7VqyabYNTbJLkhXL0l/hxYMiXv053vw3pu0etPXBwOO9GWbmBTFn2rjut5NbWV1b38hvFra2d3b3ivsHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC0fXUbz1SpVkk78w4pr7AA8lCRrCx0v1Tr/KQ8jNv0iuW3LI7A1omXkZKkKHeK351+xFJBJWGcKx1x3Nj46dYGUY4nRS6iaYxJiM8oB1LJRZU++ns4Ak6sUofhZGyJQ2aqb8nUiy0HovAdgpshnrRm4r/eZ3EhJd+ymScGCrJfFGYcGQiNP0e9ZmixPCxJZgoZm9FZIgVJsZmVLAheIsvL5Nmpeydl6u31VLtKosjD0dwDKfgwQXU4Abq0AACAp7hFd4c5bw4787HvDXnZDOH8AfO5w8vJ5AE</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"TbIc47gTmsGGERnF8Vqn22OPnx4=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBAiStiVoB6DXjxGMA9IljA76U2GzM4uM7NCCPkILx4U8er3ePNvnCR70MSChqKqm+6uIBFcG9f9dlZW19Y3NnNb+e2d3b39wsFhQ8epYlhnsYhVK6AaBZdYN9wIbCUKaRQIbAbDu6nffEKleSwfzShBP6J9yUPOqLFSsyTOvQvvrFsoumV3BrJMvIwUIUOtW/jq9GKWRigNE1Trtucmxh9TZTgTOMl3Uo0JZUPax7alkkao/fHs3Ak5tUqPhLGyJQ2Zqb8nxjTSehQFtjOiZqAXvan4n9dOTXjjj7lMUoOSzReFqSAmJtPfSY8rZEaMLKFMcXsrYQOqKDM2obwNwVt8eZk0LsveVbnyUClWb7M4cnAMJ1ACD66hCvdQgzowGMIzvMKbkzgvzrvzMW9dcbKZI/gD5/MHTJaOPw==</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"euM+sJFGeE+y9c6puwHU8d0c5KA=\">AAAB7nicbVBNSwMxEJ2tX7V+VT16CRahopTdUtRj0YvHCvYD2qVk02wbms2GJCuUpT/CiwdFvPp7vPlvTNs9aOuDgcd7M8zMCyRn2rjut5NbW9/Y3MpvF3Z29/YPiodHLR0nitAmiXmsOgHWlDNBm4YZTjtSURwFnLaD8d3Mbz9RpVksHs1EUj/CQ8FCRrCxUrvML7zL6nm/WHIr7hxolXgZKUGGRr/41RvEJImoMIRjrbueK42fYmUY4XRa6CWaSkzGeEi7lgocUe2n83On6MwqAxTGypYwaK7+nkhxpPUkCmxnhM1IL3sz8T+vm5jwxk+ZkImhgiwWhQlHJkaz39GAKUoMn1iCiWL2VkRGWGFibEIFG4K3/PIqaVUr3lWl9lAr1W+zOPJwAqdQBg+uoQ730IAmEBjDM7zCmyOdF+fd+Vi05pxs5hj+wPn8AU4bjkA=</latexit>\\r\\n\\r\\n(l + 1, 1)\\r\\n\\r\\n(l + 1, 2)\\r\\n\\r\\n3.3. Preference Conditioned Hypernetworks\\r\\n\\r\\n⇠\\r\\n<latexit sha1_base64=\"DyL84uHYKouib8MrcE4AVzBROKw=\">AAAB63icbVBNSwMxEJ3Ur1q/qh69BIvgqeyKqMeiF48V7Ae0S8mm2TY0yS5JVihL/4IXD4p49Q9589+YbfegrQ8GHu/NMDMvTAQ31vO+UWltfWNzq7xd2dnd2z+oHh61TZxqylo0FrHuhsQwwRVrWW4F6yaaERkK1gknd7nfeWLa8Fg92mnCAklGikecEptLfcPloFrz6t4ceJX4BalBgeag+tUfxjSVTFkqiDE930tskBFtORVsVumnhiWETsiI9RxVRDITZPNbZ/jMKUMcxdqVsniu/p7IiDRmKkPXKYkdm2UvF//zeqmNboKMqyS1TNHFoigV2MY4fxwPuWbUiqkjhGrubsV0TDSh1sVTcSH4yy+vkvZF3b+qXz5c1hq3RRxlOIFTOAcfrqEB99CEFlAYwzO8whuS6AW9o49FawkVM8fwB+jzByKdjlA=</latexit>\\r\\n\\r\\nxl+1\\r\\nN\\r\\n<latexit sha1_base64=\"qCFw7eo69s0cDLsTf8wBsQBGuDw=\">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRZBEMquFPVY9OJJKtgPadeSTbNtaJJdkqxYlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL4g508Z1v53c0vLK6lp+vbCxubW9U9zda+goUYTWScQj1QqwppxJWjfMcNqKFcUi4LQZDK8mfvORKs0ieWdGMfUF7ksWMoKNle6fujcPKT/xxt1iyS27U6BF4mWkBBlq3eJXpxeRRFBpCMdatz03Nn6KlWGE03Ghk2gaYzLEfdq2VGJBtZ9ODx6jI6v0UBgpW9Kgqfp7IsVC65EIbKfAZqDnvYn4n9dOTHjhp0zGiaGSzBaFCUcmQpPvUY8pSgwfWYKJYvZWRAZYYWJsRgUbgjf/8iJpnJa9s3LltlKqXmZx5OEADuEYPDiHKlxDDepAQMAzvMKbo5wX5935mLXmnGxmH/7A+fwBWj+QIA==</latexit>\\r\\n\\r\\n...\\r\\n<latexit sha1_base64=\"t6iHjZmz9uJaFfSHS0x8E4VrNJE=\">AAAB7XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae0Q8mkmTY2kxmSO0Ip/Qc3LhRx6/+482/MtLPQ1gOBwzn3kHtPkEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7Gt5nffuLaiFg94CThfkSHSoSCUbRSqzeI0ZT65Ypbdecgq8TLSQVyNPrlLxtkacQVMkmN6Xpugv6UahRM8lmplxqeUDamQ961VNGIG38633ZGzqwyIGGs7VNI5urvxJRGxkyiwE5GFEdm2cvE/7xuiuG1PxUqSZErtvgoTCXBmGSnk4HQnKGcWEKZFnZXwkZUU4a2oKwEb/nkVdK6qHqX1dp9rVK/yesowgmcwjl4cAV1uIMGNIHBIzzDK7w5sfPivDsfi9GCk2eO4Q+czx8qj47f</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"KkrD1G+dkha5f3Ikin99CzRps3A=\">AAAB7nicbVBNSwMxEJ2tX7V+VT16CRahopRdKeqx6MWTVLAf0C4lm2bb0Gw2JFmhLP0RXjwo4tXf481/Y9ruQVsfDDzem2FmXiA508Z1v53cyura+kZ+s7C1vbO7V9w/aOo4UYQ2SMxj1Q6wppwJ2jDMcNqWiuIo4LQVjG6nfuuJKs1i8WjGkvoRHggWMoKNlVplfuad35/2iiW34s6AlomXkRJkqPeKX91+TJKICkM41rrjudL4KVaGEU4nhW6iqcRkhAe0Y6nAEdV+Ojt3gk6s0kdhrGwJg2bq74kUR1qPo8B2RtgM9aI3Ff/zOokJr/2UCZkYKsh8UZhwZGI0/R31maLE8LElmChmb0VkiBUmxiZUsCF4iy8vk+ZFxbusVB+qpdpNFkcejuAYyuDBFdTgDurQAAIjeIZXeHOk8+K8Ox/z1pyTzRzCHzifP3injlw=</latexit>\\r\\n\\r\\n(l + 1, N )\\r\\n\\r\\nFigure 3. Branching block. Illustration of the parent sampling\\r\\noperation in Section 3.2. Nodes in layer l are\\r\\nin accordance\\r\\n\\x01\\r\\nPsampled\\r\\nl\\r\\nto a categorical distribution defined by αjl\\r\\nα\\r\\n(i)\\r\\n=\\r\\n1 for each\\r\\ni j\\r\\nnode (l + 1, j) in layer l + 1.\\r\\n\\r\\nhypernet capacity. The anchor net, consisting of N -stream\\r\\nbackbones trained for N individual tasks (Figure 2), overcomes this bottleneck by providing the weights in the tree\\r\\nstructures predicted by the edge hypernet. Our choice of\\r\\nthe anchor net is motivated by the need for an initialization\\r\\nthat reflects inter-task relations and is based on observations\\r\\nfrom [51], where branching in tree-structured MTL networks\\r\\nis shown to be contingent on how similar task features are\\r\\nat any layer. It can also be interpreted as a supernet used\\r\\nin one-shot NAS approaches [2], which is capable of emulating any architecture in the search space. Subsequently,\\r\\nthe base weights of the anchor net are further modulated via\\r\\nthe weight hypernet to address the cross-task connections\\r\\nunseen in the anchor net (Section 3.3.2).\\r\\n\\r\\nWe use two hypernets [17] to construct our controller\\r\\nfor architectural changes. The edge hypernet h, parameterized by ϕ, predicts the branching parameters α̂ = h(r, c; ϕ)\\r\\nwithin the anchor net. Subsequently, the weight hypernet h̄,\\r\\nparameterized by ϕ̄, predicts the normalization parameters\\r\\n{β̂, γ̂} = h̄(r, c; ϕ̄) to adapt the predicted network.\\r\\nOptimizing the task loss Ltask only takes into account\\r\\nthe individual task performances without considering computational cost. Consequently, we introduce a branching\\r\\nregularizer Ω(r, c, α̂) to encourage node sharing (or branching) based on the preference. This regularizer contains two\\r\\nterms, the active loss, which encourages limited sharing of\\r\\nfeatures among the high preference tasks, and the inactive\\r\\nloss, which aims to reduce resource utilization for the less\\r\\nimportant ones. In particular, the active loss is additionally\\r\\nweighted by the cost preference c to enable the control of\\r\\ntotal computational cost. Formally, our objective is formulated as to find the controller (ϕ and ϕ̄) that minimizes the\\r\\nexpectation of the branching regularized task loss over the\\r\\ndistribution of user preferences P(r,c) :\\r\\n  \\\\min _{\\\\phi , \\\\bar {\\\\phi }} \\\\mathbb {E}_{(\\\\bm {r}, c)\\\\sim P_{(\\\\bm {r},c)}} \\\\left [\\\\mathcal {L}_{\\\\textrm {task}}(\\\\bm {r}, \\\\hat {\\\\bm {\\\\alpha }}, \\\\hat {\\\\bm {\\\\beta }}, \\\\hat {\\\\bm {\\\\gamma }}) + \\\\Omega (\\\\bm {r}, c, \\\\hat {\\\\bm {\\\\alpha }})\\\\right ],\\r\\n\\r\\n(2)\\r\\n\\r\\n3.2. Architecture Search Space\\r\\nWe utilize a tree-structured network topology which has\\r\\nbeen shown to be highly effective for multi-task learning\\r\\nin [15]. It shares common low-level features over tasks while\\r\\nextracting task-specific ones in the higher layers, enabling\\r\\ncontrol of the trade-off between tasks by changing branching locations conditioned on the desired preference (r, c).\\r\\nThe search space is represented as a directed acyclic graph\\r\\n(DAG), where vertices in the graph represent different operations and edges denote the data flow through the network.\\r\\nFigure 3 shows a block of such a graph, containing N parent\\r\\nand child nodes. In this work, we realize a tree-structure by\\r\\nstacking such blocks sequentially and allowing a child node\\r\\nto sample a path from the candidate paths between itself and\\r\\nall its parent nodes. Concretely, we formulate the stochastic\\r\\nbranching operation at layer l as\\r\\n  x^{l+1}_j = d_j\\\\cdot Y^l, \\\\quad d_j\\\\sim p_{\\\\alpha _j^l}, \\r\\n\\r\\n(1)\\r\\n\\r\\nwhere xl+1\\r\\ndenotes the input to the j-th node in layer l + 1,\\r\\nj\\r\\ndj is a one-hot vector indicating the parent node sampled\\r\\nfrom the categorical distribution parameterized by αjl and,\\r\\nl\\r\\nY l = [y1l , . . . , yN\\r\\n] concatenates outputs from all parent\\r\\nnodes at layer l. Note that selecting a parent from every node\\r\\n\\r\\nWe disentangle the training of the hypernetworks for stability – the edge hypernet is trained first, followed by the\\r\\nweight hypernet. At test time, when a preference (r, c) is\\r\\npresented to the controller, the maximum likelihood architecture corresponding to the supplied preference is first sampled\\r\\nfrom the branching distribution parameterized by the predictions of h. The weights of this tree-structure are then\\r\\ninherited from the anchor net, supplemented via adapted\\r\\nnormalization parameters predicted by h̄.\\r\\n3.3.1\\r\\n\\r\\nRegularizing the Edge Hypernet\\r\\n\\r\\nWe illustrate the idea of branching regularization in Figure 4:\\r\\ntasks with higher preferences should have a greater influence\\r\\non the branching structure while tasks with smaller preferences may be de-emphasized by encouraging them to follow existing branching choices. Specifically, we define two\\r\\nlosses, active and inactive losses, based on the task division\\r\\ninto two groups, active tasks A = {Ti | ri ≥ τ, ∀i ∈ [N ]},\\r\\nand inactive tasks I = {Ti | ri < τ, ∀i ∈ [N ]} with some\\r\\nthreshold τ . Although individual tasks are already weighted\\r\\nby r in task loss Ltask , this explicit emphasizing of certain\\r\\ntasks over others was found to be crucial to induce better\\r\\ncontrollability, as shown in Section 4.6.\\r\\n\\r\\n\\x0c\\n\\nActive loss. The active loss Lactive encourages nodes in the\\r\\nanchor net, corresponding to the active tasks, to be shared\\r\\nin order to avoid the whole network being split up by tasks\\r\\nwith little knowledge shared among them. Specifically, we\\r\\nencourage any pair of nodes that are likely to be sampled in\\r\\nthe final architecture (P ) and are from two similar tasks (A)\\r\\nto take the same parent node. Formally, we define Lactive as,\\r\\n  \\\\label {eq:active} \\\\mathcal {L}_{\\\\textrm {active}} = \\\\sum _{l=1}^L \\\\sum _{\\\\substack {i,j \\\\in \\\\mathcal {A} \\\\\\\\ i\\\\neq j}} \\\\frac {L-l}{L} \\\\cdot A(i,j) \\\\cdot P(l,i,j) \\\\cdot \\\\|\\\\nu ^{l}_{i} - \\\\nu ^{l}_{j}\\\\|^2,  (3)\\r\\n\\r\\nwhere\\r\\nP (l, i, j) = Puse (l, i) · Puse (l, j). Puse (l, i) = 1 −\\r\\nQ\\r\\n{1\\r\\n−\\r\\nPuse (l + 1, k) · νkl (i)} denotes the probability that\\r\\nk\\r\\nthe nodes i in layer l are used in the sampled tree structure. A(i, j) captures the task affinity between tasks Ti and\\r\\nTj , where we adopt Representational Similarity Analysis\\r\\n(RSA) [12] to compute the affinity. The factor L−l\\r\\nL encourages more sharing of nodes which contain low-level features.\\r\\nThe full derivations of Puse and A are detailed in the supplementary document.\\r\\nWe use the Gumbel-Softmax reparameterization trick [21]\\r\\nto obtain the samples νil from the predicted logits α̂,\\r\\n\\r\\nActive\\r\\nInactive\\r\\n\\r\\nr\\r\\n<latexit sha1_base64=\"nROIzrZ3ZbXvAmvV3K3wGwNebNo=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIqMuiG5cV7APasWQymTY0kwxJRilD/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJEs60cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpugDXlTNCWYYbTbqIojgNOO8H4Jvc7j1RpJsW9mSTUj/FQsIgRbKz00A8kD/UktlempoNqza27M6Bl4hWkBgWag+pXP5QkjakwhGOte56bGD/DyjDC6bTSTzVNMBnjIe1ZKnBMtZ/NUk/RiVVCFElljzBopv7eyHCs82h2MsZmpBe9XPzP66UmuvIzJpLUUEHmD0UpR0aivAIUMkWJ4RNLMFHMZkVkhBUmxhZVsSV4i19eJu2zundRP787rzWuizrKcATHcAoeXEIDbqEJLSCg4Ble4c15cl6cd+djPlpyip1D+APn8wdNHJMM</latexit>\\r\\n\\r\\nActive (Eq.3)\\r\\n\\r\\nInactive (Eq.5)\\r\\n\\r\\nFigure 4. Branching loss. Illustration of the branching regularization, consisting of active and inactive losses. The active loss\\r\\nencourages limited sharing between high importance tasks, while\\r\\nthe inactive loss tries to limit branching for less preferred tasks as\\r\\nmuch as possible.\\r\\n\\r\\nnodes that belong to different streams in F. Consequently,\\r\\nthe performance of the sampled network is sub-optimal. To\\r\\nrectify this issue, we propose to modulate the weights of\\r\\nthe anchor net to adaptively update the unseen edges using\\r\\nan additional weight hypernet h̄. Inspired from the prior\\r\\nworks [34, 53] that estimate normalization statistics and optimize channel-wise affine transformations, we modulate only\\r\\nthe normalization parameters using a hypernetwork. Concretely, we modulate the original batch normalization operaxl −µl\\r\\ntion at layer l, BNli (xli ) = γil iσl i +βil , to BNli (xli ) = (γil +\\r\\ni\\r\\n\\r\\n  \\\\label {eq:gumbel} \\\\nu ^{l}_{i}(k) = \\\\frac {\\\\exp \\\\left ((\\\\log \\\\alpha ^l_i(k) + G^l_i(k))/\\\\zeta \\\\right )}{\\\\sum _{m=1}^N\\\\exp \\\\left ((\\\\log \\\\alpha ^l_i(m) + G^l_i(m))/\\\\zeta \\\\right )}. \\r\\n\\r\\n(4)\\r\\n\\r\\nHere, Gli = − log(− log Uil ) is a standard Gumbel distribution with Uil sampled i.i.d. from the uniform distribution\\r\\nUnif(0, 1), and ζ denotes the temperature of the softmax.\\r\\nInactive loss. The inactive tasks should have minimal effect\\r\\nin terms of branching. Inactive loss, Linactive , encourages\\r\\nthese tasks to mimic the most closely related branching\\r\\npattern,\\r\\n  \\\\label {eq:inactive} \\\\mathcal {L}_{\\\\textrm {inactive}} = \\\\sum _{l=1}^{L} \\\\sum _{j \\\\in \\\\mathcal {I}} \\\\min _{i \\\\in \\\\mathcal {A}} \\\\|\\\\nu ^{l}_{i} - \\\\nu ^{l}_{j}\\\\|^2. \\r\\n\\r\\n(5)\\r\\n\\r\\nThis ensures that the network branching is controlled by the\\r\\nactive tasks, with the inactive tasks sharing nodes with the\\r\\nactive tasks.\\r\\nThus, the branching regularizer is defined as follows,\\r\\n  \\\\Omega (\\\\bm {r}, c, \\\\hat {\\\\bm {\\\\alpha }}) = c\\\\cdot \\\\lambda _{\\\\mathcal {A}}\\\\mathcal {L}_{\\\\textrm {active}} + \\\\lambda _{\\\\mathcal {I}}\\\\mathcal {L}_{\\\\textrm {inactive}}, \\r\\n\\r\\n(6)\\r\\n\\r\\nwhere λA , λI are hyperparameters to determine the weighting of the losses. Typically, we set λA = 1 and λI = 0.1.\\r\\nHere, the active loss is additionally weighted by the resource\\r\\npreference c, so that larger c encourages more feature sharing\\r\\nto reduce total computational cost.\\r\\n3.3.2\\r\\n\\r\\nCross-task Adaptation\\r\\n\\r\\nThe architecture sampled by the edge hypernet h contains\\r\\nedges that have not been observed during the anchor net training. These are denoted as cross-task edges since they connect\\r\\n\\r\\n∆γil )\\r\\n\\r\\nxli −µli\\r\\nσil\\r\\n\\r\\n+ (βil + ∆βil ) by predicting the perturbations\\r\\n\\r\\nto the parameters: {∆βil , ∆γil }0≤i≤N,0≤l<L = h̄(r, c; θ),\\r\\nwhere γil and βil are the original affine parameters, and µli\\r\\nand σil denote the batch statistics of the node input xli . This\\r\\nmodulation primarily affects the preferences with two or\\r\\nmore dominant tasks, where cross-task connections occur.\\r\\n\\r\\n4. Experiments\\r\\nIn this section, we demonstrate the ability of our framework to dynamically search for efficient architectures for\\r\\nmulti-task learning. We show that our framework achieves\\r\\nflexibility between two extremes of the accuracy-efficiency\\r\\ntrade-off, allowing a better control within a single model.\\r\\nExtensive experiments indicate that the predicted network\\r\\nstructures match well with the input preferences, in terms of\\r\\nboth resource usage and task performance.\\r\\n\\r\\n4.1. Evaluation Criteria\\r\\nUniformity. To measure controllability with respect to task\\r\\npreferences, we utilize uniformity [35] which quantifies how\\r\\nwell the vector of task losses L = [L1 , . . . , LN ] is aligned\\r\\nwith the given preference. Specifically, for the loss vector\\r\\nL corresponding to the architecture for task preference r,\\r\\nuniformity is defined as µr = 1 − DKL (L̂ ∥ 1/N ), where\\r\\nr L\\r\\nL̂(j) = P j rijLi . This arises from the fact that, ideally, rj ∝\\r\\ni\\r\\n1/Lj , which in turn implies r1 L1 = r2 L2 · · · = rN LN .\\r\\nHypervolume. Using the trained controller, we are able to\\r\\napproximate the trade-off curve among the different tasks\\r\\nin the loss space. To evaluate the quality of this curve we\\r\\n\\r\\n\\x0c\\n\\nutilize hypervolume (HV) [60] – a popular metric in the\\r\\nmulti-objective optimization literature to compare different\\r\\nsets of solutions approximating the Pareto front [13]. It\\r\\nmeasures the volume in the loss space of points dominated\\r\\nby a solution in the evaluated set. Since this volume is\\r\\nunbounded, hypervolume measures the volume in a rectangle\\r\\ndefined by the solutions and a selected reference point. More\\r\\ndetails can be found in the appendix.\\r\\nComputational Resource. We measure the computational\\r\\ncost using the memory of the activated nodes in the anchor\\r\\nnet and the GFLOPs, which approximates the time spent\\r\\nin the forward pass. We also report the computational cost\\r\\nof the hypernets to take into account their overheads. We\\r\\ndiscuss more on the model size in the appendix.\\r\\n\\r\\n4.2. Datasets\\r\\nWe evaluate the performance of our approach using three\\r\\nmulti-task datasets, namely PASCAL-Context [37] and\\r\\nNYU-v2 [48], and CIFAR-100 [25]. The PASCAL-Context\\r\\ndataset is used for joint semantic segmentation, human parts\\r\\nsegmentation and saliency estimation, as well as these three\\r\\ntasks together with surface normal estimation, and edge detection as in [5]. The NYU-v2 dataset comprises images of\\r\\nindoor scenes, fully labeled for semantic segmentation, depth\\r\\nestimation and surface normal estimation. For CIFAR-100,\\r\\nwe split the dataset into 20 five-way classification tasks [41].\\r\\n\\r\\n4.3. Baselines\\r\\nWe compare our framework with both static and dynamic\\r\\nnetworks. Static networks include Single-task networks,\\r\\nwhere we train each task separately using a task-specific\\r\\nbackbone, and Multi-task networks, in which all tasks share\\r\\nthe backbone but have separate task-specific heads at the end.\\r\\nThese multi-task networks are trained separately for different\\r\\npreferences and thus, training time scales linearly with the\\r\\nnumber of preferences. We use this to contrast the training\\r\\ntime of our framework. The single-task networks demonstrate the anchor net performance. We also compare our\\r\\narchitectures with two multi-task NAS methods, LTB [27]\\r\\nand BMTAS [5], which use the same tree-structured search\\r\\nspace to perform NAS, but are static. The dynamic networks include Pareto Hypernetworks (PHN) [38], which\\r\\npredicts only the weights of a shared backbone network conditioned on a task preference vector using hypernetworks,\\r\\nand PHN-BN, a variation of PHN which predicts only the\\r\\nnormalization parameters similar to our weight hypernet.\\r\\nImplementation details are presented in the appendix.\\r\\n\\r\\n4.4. Comparison with Baselines\\r\\nControllable resource usage. We visualize the variation\\r\\nin computational cost with respect to different task and resource usage preferences in Figure 5. We adopt the ratio\\r\\nof the size of the predicted architecture to the size of the\\r\\n\\r\\nOurs\\r\\n\\r\\nT2\\r\\n\\r\\n<latexit sha1_base64=\"E+K72hNm9lVnGocZCyBa+qa2aM4=\">AAAB7HicbVBNSwMxEJ2tX7V+VT16CRbBU9mVol6EohePFdy20C4lm862odnskmSFUvobvHhQxKs/yJv/xrTdg7Y+SHi8N8PMvDAVXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKmTTDH0WSIS1Q6pRsEl+oYbge1UIY1Dga1wdDfzW0+oNE/koxmnGMR0IHnEGTVW8tmNW3V75Yr95yCrxMtJBXI0euWvbj9hWYzSMEG17nhuaoIJVYYzgdNSN9OYUjaiA+xYKmmMOpjMl52SM6v0SZQo+6Qhc/V3x4TGWo/j0FbG1Az1sjcT//M6mYmugwmXaWZQssWgKBPEJGR2OelzhcyIsSWUKW53JWxIFWXG5lOyIXjLJ6+S5kXVu6zWHmqV+m0eRxFO4BTOwYMrqMM9NMAHBhye4RXeHOm8OO/Ox6K04OQ9x/AHzucPk4qN4w==</latexit>\\r\\n\\r\\nConventional\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nc = 0.0\\r\\n\\r\\nc = 0.3\\r\\n<latexit sha1_base64=\"V3Z/XBx7PSGaLaa6ezPstIwVENg=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0i0qBeh6MVjBdMW2lA22027dHcTdjdCCf0NXjwo4tUf5M1/47bNQasPBh7vzTAzL0o508bzvpzSyura+kZ5s7K1vbO7V90/aOkkU4QGJOGJ6kRYU84kDQwznHZSRbGIOG1H49uZ336kSrNEPphJSkOBh5LFjGBjpYBce+55v1rzXG8O9Jf4BalBgWa/+tkbJCQTVBrCsdZd30tNmGNlGOF0WullmqaYjPGQdi2VWFAd5vNjp+jEKgMUJ8qWNGiu/pzIsdB6IiLbKbAZ6WVvJv7ndTMTX4U5k2lmqCSLRXHGkUnQ7HM0YIoSwyeWYKKYvRWREVaYGJtPxYbgL7/8l7TOXP/Crd/Xa42bIo4yHMExnIIPl9CAO2hCAAQYPMELvDrSeXbenPdFa8kpZg7hF5yPb5gWjeY=</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT1\\r\\n\\r\\nT3\\r\\n\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nT1\\r\\n\\r\\nT3\\r\\n\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nc = 0.6\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"fBJIsDnXst0iooGo1R6DIZ5+LvA=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0hEqheh6MVjBdMW2lA22027dLMbdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpRypo3nfTultfWNza3ydmVnd2//oHp41NIyU4QGRHKpOhHWlDNBA8MMp51UUZxEnLaj8d3Mbz9RpZkUj2aS0jDBQ8FiRrCxUkBuPLfer9Y815sDrRK/IDUo0OxXv3oDSbKECkM41rrre6kJc6wMI5xOK71M0xSTMR7SrqUCJ1SH+fzYKTqzygDFUtkSBs3V3xM5TrSeJJHtTLAZ6WVvJv7ndTMTX4c5E2lmqCCLRXHGkZFo9jkaMEWJ4RNLMFHM3orICCtMjM2nYkPwl19eJa0L16+7lw+XtcZtEUcZTuAUzsGHK2jAPTQhAAIMnuEV3hzhvDjvzseiteQUM8fwB87nD5yijek=</latexit>\\r\\n\\r\\nc = 1.0\\r\\n<latexit sha1_base64=\"H0RhIjR0liAXE9TO7lC0A2LloR4=\">AAAB7HicbVBNS8NAEJ2tX7V+VT16WSyCp5BIUS9C0YvHCqYttKFstpt26WYTdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpgKro3rfqPS2vrG5lZ5u7Kzu7d/UD08aukkU5T5NBGJ6oREM8El8w03gnVSxUgcCtYOx3czv/3ElOaJfDSTlAUxGUoecUqMlXx64zluv1pzHXcOvEq8gtSgQLNf/eoNEprFTBoqiNZdz01NkBNlOBVsWullmqWEjsmQdS2VJGY6yOfHTvGZVQY4SpQtafBc/T2Rk1jrSRzazpiYkV72ZuJ/Xjcz0XWQc5lmhkm6WBRlApsEzz7HA64YNWJiCaGK21sxHRFFqLH5VGwI3vLLq6R14XiXTv2hXmvcFnGU4QRO4Rw8uIIG3EMTfKDA4Rle4Q1J9ILe0ceitYSKmWP4A/T5A5UQjeQ=</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nT3\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nT3\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nT3\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nFigure 5. Resource usage on NYU-v2. We visualize resource\\r\\nusage by plotting the proportion of parameters active in the anchor\\r\\nnet versus the task preference. The three vertices represent the\\r\\ntask preferences with non-zero importance to only one task, while\\r\\nareas in the middle correspond to more dense preferences. As c\\r\\nincreases, the predicted networks grow progressively smaller in the\\r\\ndense regions. On the other hand, conventional dynamic networks\\r\\nfor MTL always have a constant resource usage (T1 :semantic seg.,\\r\\nT2 :surface normal, T3 :depth).\\r\\n\\r\\ntotal anchor net as the criterion for evaluating computational\\r\\ncost. Compared to conventional dynamic networks that only\\r\\nadjust weights with a fixed computational cost (right), our\\r\\nframework (left) enables control over the total cost via a\\r\\ncost preference c. Resource usage peaks at the center of the\\r\\ncontour, when more tasks are active, and falls down gradually as we move towards the corners, where task preferences\\r\\nare heavily skewed. Furthermore, the average resource usage decreases as c is increased, indicating the ability of the\\r\\ncontroller to incorporate resource constraints.\\r\\nMulti-task performance. We demonstrate the overall multitask performance in Tables 1-4 on four different settings\\r\\n(PASCAL-Context 5-task, PASCAL-Context 3-task, NYUv2 3-task, CIFAR-100 20-task). In all cases, we report hypervolume (reference point mentioned below heading) and\\r\\nuniformity averaged across 20 task preference vectors r,\\r\\nsampled uniformly from SN . Inference network cost is calculated similarly over 1000 preference vectors. These are\\r\\nshown for two choices of c ∈ {0, 1} to highlight the two\\r\\nextreme cases of resource usage.\\r\\nOur framework achieves higher values in both hypervolume and uniformity compared to the existing dynamic\\r\\nmodels (PHN and PHN-BN) in all four settings. While the\\r\\nhigh hypervolume reinforces the efficacy of tree-structured\\r\\nmodels in solving multi-task problems, the uniformity values consolidate architectural change as an effective approach\\r\\ntowards modeling task trade-offs. This is accompanied by\\r\\nincreased average computational cost, indicated by inference parameter count. As discussed above, this is due to the\\r\\nflexible architecture over preferences, where actual cost will\\r\\ndiffer for each preference, e.g., reaching the cost of PHN-BN\\r\\nfor extremely skewed preferences (Figure 5). Compared to\\r\\nSingle-Task, the proposed controller is able to find effective architectures (as indicated by the hypervolume) which\\r\\n\\r\\n\\x0c\\n\\nInference\\r\\nParams.↓\\r\\n\\r\\nGFLOPs↓\\r\\n\\r\\nControl\\r\\nParams.\\r\\n\\r\\nSingle-Task\\r\\n\\r\\n81.56\\r\\n\\r\\n-\\r\\n\\r\\n9.84M\\r\\n\\r\\n16.17\\r\\n\\r\\n-\\r\\n\\r\\nPHN\\r\\nPHN-BN\\r\\n\\r\\n42.61\\r\\n72.27\\r\\n\\r\\n0.72\\r\\n0.69\\r\\n\\r\\n2.15M\\r\\n2.15M\\r\\n\\r\\n6.28\\r\\n6.28\\r\\n\\r\\n21.50M\\r\\n3.63M\\r\\n\\r\\nOurs w/o adaptation, c=0.0\\r\\nOurs w/o adaptation, c=1.0\\r\\nOurs, c=0.0\\r\\nOurs, c=1.0\\r\\n\\r\\n47.73\\r\\n30.91\\r\\n75.52\\r\\n73.20\\r\\n\\r\\n0.84\\r\\n0.86\\r\\n0.76\\r\\n0.79\\r\\n\\r\\n3.34M\\r\\n2.75M\\r\\n3.34M\\r\\n2.75M\\r\\n\\r\\n7.21\\r\\n6.81\\r\\n7.21\\r\\n6.81\\r\\n\\r\\n0.06M\\r\\n0.06M\\r\\n15.32M\\r\\n15.32M\\r\\n\\r\\n4.2\\r\\n\\r\\n12\\r\\n\\r\\n10\\r\\n\\r\\nMethod\\r\\n\\r\\nHV.↑\\r\\n[3, 3, 3]\\r\\n\\r\\nUnif.↑\\r\\n\\r\\nInference\\r\\nParams.↓\\r\\n\\r\\nGFLOPs↓\\r\\n\\r\\nControl\\r\\nParams.\\r\\n\\r\\nSingle-Task\\r\\n\\r\\n4.31\\r\\n\\r\\n-\\r\\n\\r\\n5.91M\\r\\n\\r\\n9.75\\r\\n\\r\\n-\\r\\n\\r\\nPHN\\r\\nPHN-BN\\r\\n\\r\\n1.97\\r\\n3.92\\r\\n\\r\\n0.74\\r\\n0.79\\r\\n\\r\\n2.06M\\r\\n2.06M\\r\\n\\r\\n4.81\\r\\n4.81\\r\\n\\r\\n21.10M\\r\\n3.32M\\r\\n\\r\\nOurs w/o adaptation, c=0.0\\r\\nOurs w/o adaptation, c=1.0\\r\\nOurs, c=0.0\\r\\nOurs, c=1.0\\r\\n\\r\\n3.56\\r\\n3.35\\r\\n4.26\\r\\n4.25\\r\\n\\r\\n0.92\\r\\n0.91\\r\\n0.82\\r\\n0.82\\r\\n\\r\\n3.15M\\r\\n2.86M\\r\\n3.15M\\r\\n2.86M\\r\\n\\r\\n5.52\\r\\n5.07\\r\\n5.52\\r\\n5.07\\r\\n\\r\\n0.03M\\r\\n0.03M\\r\\n9.25M\\r\\n9.25M\\r\\n\\r\\n20\\r\\n40\\r\\nTime (hours)\\r\\nc = 0.0\\r\\n\\r\\n3.6\\r\\n\\r\\nc = 1.0\\r\\n\\r\\n60\\r\\n\\r\\n20\\r\\n40\\r\\nTime (hours)\\r\\n\\r\\nc = 0.0 w/o adaptation\\r\\n\\r\\nc = 1.0 w/o adaptation\\r\\n\\r\\nSemantic Segmentation\\r\\n\\r\\n12.83\\r\\n\\r\\n-\\r\\n\\r\\n64.47M\\r\\n\\r\\n58.78\\r\\n\\r\\n-\\r\\n\\r\\nPHN\\r\\nPHN-BN\\r\\n\\r\\n2.36\\r\\n11.72\\r\\n\\r\\n0.75\\r\\n0.73\\r\\n\\r\\n21.59M\\r\\n21.59M\\r\\n\\r\\n21.02\\r\\n21.02\\r\\n\\r\\n21.04M\\r\\n2.23M\\r\\n\\r\\nOurs w/o adaptation, c=0.0\\r\\nOurs w/o adaptation, c=1.0\\r\\nOurs, c=0.0\\r\\nOurs, c=1.0\\r\\n\\r\\n12.42\\r\\n9.53\\r\\n13.43\\r\\n13.08\\r\\n\\r\\n0.82\\r\\n0.84\\r\\n0.76\\r\\n0.78\\r\\n\\r\\n41.06M\\r\\n34.68M\\r\\n41.06M\\r\\n34.68M\\r\\n\\r\\n29.04\\r\\n25.98\\r\\n29.04\\r\\n25.98\\r\\n\\r\\n0.03M\\r\\n0.03M\\r\\n5.72M\\r\\n5.72M\\r\\n\\r\\nMethod\\r\\n\\r\\nHV.↑\\r\\n[1, 1, . . . , 1]\\r\\n\\r\\nUnif.↑\\r\\n\\r\\nInference\\r\\nParams.↓\\r\\n\\r\\nGFLOPs↓\\r\\n\\r\\nControl\\r\\nParams.\\r\\n\\r\\nSingle-Task\\r\\n\\r\\n0.009\\r\\n\\r\\n-\\r\\n\\r\\n36.18M\\r\\n\\r\\n348.79\\r\\n\\r\\n-\\r\\n\\r\\nPHN\\r\\nPHN-BN\\r\\n\\r\\n0.002\\r\\n0.007\\r\\n\\r\\n0.54\\r\\n0.49\\r\\n\\r\\n16.35M\\r\\n16.35M\\r\\n\\r\\n73.13\\r\\n73.13\\r\\n\\r\\n11.03M\\r\\n0.31M\\r\\n\\r\\nOurs w/o adaptation, c=0.0\\r\\nOurs w/o adaptation, c=1.0\\r\\nOurs, c=0.0\\r\\nOurs, c=1.0\\r\\n\\r\\n0.003\\r\\n0.001\\r\\n0.010\\r\\n0.009\\r\\n\\r\\n0.58\\r\\n0.53\\r\\n0.54\\r\\n0.49\\r\\n\\r\\n31.86M\\r\\n31.37M\\r\\n31.86M\\r\\n31.37M\\r\\n\\r\\n174.36\\r\\n129.23\\r\\n174.36\\r\\n129.23\\r\\n\\r\\n0.34M\\r\\n0.34M\\r\\n3.10M\\r\\n3.10M\\r\\n\\r\\nTable 4. Evaluation on CIFAR-100.\\r\\n\\r\\nperform nearly at par with a smaller memory footprint (as\\r\\nindicated by the average inference network parameter count).\\r\\nNotably, in the NYU-v2 3-task and CIFAR-100 settings, the\\r\\nability to find effective architectures enables the model to\\r\\noutperform single-task networks, demonstrating the benefit of sharing features among related tasks via architectural\\r\\nchange. In addition, our framework enjoys flexibility between two extreme cases, i.e. Single-Task (highest accuracy\\r\\nwith lowest inference efficiency) and dynamic models with\\r\\nshared backbone (lowest accuracy with highest inference\\r\\nefficiency), spanning a range of trade-offs for different c values. The range of HV is larger when task-specific features\\r\\nare useful, compared to when the compact architecture already achieves higher HV than the Single-Task (Tables 3,4).\\r\\n“Control Params.” is the cost of the hypernets. Note that this\\r\\noverhead will materialize only when the preference changes\\r\\nand does not have any effect on the task inference time.\\r\\nEffect of cross-task adaptation In Tables 1-4, “Ours w/o\\r\\nadaptation” denotes the model without weight hypernet. As\\r\\nindicated by larger HVs, cross-task adaptation improves the\\r\\nperformance without affecting the inference time. A trend\\r\\n\\r\\nmulti-task\\r\\n\\r\\n4.0\\r\\n\\r\\n2.5\\r\\n2.0\\r\\n1.5\\r\\n\\r\\n3.0\\r\\n2.0\\r\\n\\r\\n0.2 0.4 0.6 0.8\\r\\nTask preference\\r\\n\\r\\n1\\r\\n\\r\\n0.2 0.4 0.6 0.8\\r\\nTask preference\\r\\n\\r\\n1\\r\\n\\r\\nDepth\\r\\nc\\r\\nc\\r\\nc\\r\\nc\\r\\n\\r\\n2.5\\r\\n\\r\\nTable 3. Evaluation on NYU-v2.\\r\\n\\r\\n60\\r\\n\\r\\nSurface Normals\\r\\n\\r\\n3.0\\r\\nLoss\\r\\n\\r\\nGFLOPs↓\\r\\n\\r\\nControl\\r\\nParams.\\r\\n\\r\\nLoss\\r\\n\\r\\nUnif.↑\\r\\n\\r\\nInference\\r\\nParams.↓\\r\\n\\r\\nLoss\\r\\n\\r\\nSingle-Task\\r\\n\\r\\n3.8\\r\\n\\r\\nFigure 6. Comparison with preference-specific multi-task networks. For static multi-task models, each value is computed by\\r\\nevaluating a subset of preferences, with the shaded area marking\\r\\nthe variance across selected subsets. Our framework achieves high\\r\\nhypervolume significantly faster with a single model.\\r\\n\\r\\nTable 2. Evaluation on PASCAL-Context (3 tasks).\\r\\nMethod\\r\\n\\r\\n4.0\\r\\n\\r\\n3.4\\r\\n\\r\\nTable 1. Evaluation on PASCAL-Context (5 tasks).\\r\\n\\r\\nHV.↑\\r\\n[4, 4, 4]\\r\\n\\r\\nPascal-Context (3 tasks)\\r\\n\\r\\nNYU-v2\\r\\n14\\r\\nHypervolume\\r\\n\\r\\nUnif.↑\\r\\n\\r\\nHypervolume\\r\\n\\r\\nHV.↑\\r\\n[3, 3, . . . , 3]\\r\\n\\r\\nMethod\\r\\n\\r\\n2.0\\r\\n1.5\\r\\n0.2 0.4 0.6 0.8\\r\\nTask preference\\r\\n\\r\\n=\\r\\n=\\r\\n=\\r\\n=\\r\\n\\r\\n0.0 w/o adaptation\\r\\n1.0 w/o adaptation\\r\\n0.0\\r\\n1.0\\r\\n\\r\\n1\\r\\n\\r\\nFigure 7. Marginal evaluation tasks on NYU-v2\\r\\n\\r\\nthat persists across all the settings is the slight drop in uniformity that accompanies the adapted models in comparison\\r\\nto the unadapted ones. This is due to the propensity of the\\r\\nweight hypernet to improve task performance as much as\\r\\npossible while keeping the preferences intact. This leads\\r\\nto improved performance even in the low-priority tasks at\\r\\nthe expense of lower uniformity. Note that our primary factor of controllability is through architectural changes which\\r\\nremains unaffected by the weight hypernet.\\r\\nTraining efficiency. In contrast to dynamic networks, static\\r\\nmulti-task networks require multiple models to be trained,\\r\\ncorresponding to different task preferences, to approximate\\r\\nthe trade-off curve. As a result, these methods have a clear\\r\\ntrade-off between their performance and their training time.\\r\\nTo analyze this trade-off, we plot hypervolume vs. training\\r\\ntime for our framework when compared to training multiple static models in Figure 6. We trained 20 multi-task\\r\\nmodels with different preferences sampled uniformly, and at\\r\\nthe inference time we selected subsets of various sizes and\\r\\ncomputed their hypervolume. The shaded area in Figure 6\\r\\nreflects the variance over different selections of task preference subsets. This empirically shows that our approach\\r\\nrequires shorter training time to achieve similar hypervolume\\r\\ncompared to the static multi-task networks.\\r\\n\\r\\n\\x0c\\n\\nT1 ↑\\r\\n\\r\\nT2 ↑\\r\\n\\r\\nT3 ↑\\r\\n\\r\\nAvg ∆T (%) ↑\\r\\n\\r\\n# Params (%) ↓\\r\\n\\r\\nSingle-Task\\r\\n\\r\\n64.11\\r\\n\\r\\n58.41\\r\\n\\r\\n65.17\\r\\n\\r\\n-\\r\\n\\r\\n-\\r\\n\\r\\nLTB\\r\\nBMTAS\\r\\n\\r\\n61.84\\r\\n62.79\\r\\n\\r\\n59.41\\r\\n58.41\\r\\n\\r\\n64.18\\r\\n64.74\\r\\n\\r\\n-1.12\\r\\n-0.93\\r\\n\\r\\n-35.0\\r\\n-48.9\\r\\n\\r\\nOurs, c=0.0\\r\\nOurs† , c=0.0\\r\\nOurs, c=1.0\\r\\nOurs† , c=1.0\\r\\n\\r\\n63.60\\r\\n62.34\\r\\n63.12\\r\\n61.91\\r\\n\\r\\n59.41\\r\\n58.60\\r\\n58.93\\r\\n58.71\\r\\n\\r\\n64.94\\r\\n65.17\\r\\n64.93\\r\\n65.01\\r\\n\\r\\n+0.18\\r\\n-0.81\\r\\n-0.34\\r\\n-1.05\\r\\n\\r\\n-35.2\\r\\n-35.2\\r\\n-40.8\\r\\n-40.8\\r\\n\\r\\nTable 5. Architecture evaluation on PASCAL-Context (3 tasks).\\r\\nWe report the mean intersection over union for T1 : Semantic seg.,\\r\\nT2 : Parts seg., and T3 : Saliency. Presence of † indicates that\\r\\nwe train the networks initialized from ImageNet weights, while its\\r\\nabsence indicates training from anchor net weights.\\r\\n\\r\\n4.5. Analysis\\r\\nArchitecture evaluation. We study the effectiveness of the\\r\\narchitectures predicted by the edge hypernet by comparing\\r\\nthem with those predicted by LTB [15] and BMTAS [5]. We\\r\\nchoose the architecture predicted for a uniform task preference and, similar to LTB, we retrain it for a fair comparison.\\r\\nWe evaluate the performance in terms of relative drop in\\r\\nperformance across tasks and number of parameters with\\r\\nrespect to the single task baseline. Despite not being directly\\r\\ntrained for NAS, our framework is able to output architectures which perform at par with LTB (Table 5). More results\\r\\non the Pascal-Context dataset and visualization of dynamic\\r\\narchitectures can be found in the appendix.\\r\\nTask controllability. In Figure 7 we visualize the task controllability for our framework by plotting the test loss at\\r\\ndifferent values of task preference for that specific task,\\r\\nmarginalized over preference values of the other tasks. As\\r\\nexpected, increasing the preference for a task gradually leads\\r\\nto a decrease in the loss value. Furthermore, increasing c\\r\\nleads to higher loss values due to smaller predicted architectures. The effect of the weight hypernet is also evident, as\\r\\nshown by the lower loss values obtained on using it on top\\r\\nof the edge hypernet (w/o adaptation).\\r\\n\\r\\n4.6. Ablation Study\\r\\nImpact of inactive loss. Removing Linact leads to loss of\\r\\ncontrollability with the edge hypernet predominantly predicting the full original anchor net, with minimal branching,\\r\\nleading to high resource usage and poor uniformity (Table 6).\\r\\nImpact of weighting factors. Removing the two branching\\r\\nweights, L−l\\r\\nL and A, in the active loss, we make three key\\r\\nobservations in Table 6: 1) average resource usage increases,\\r\\n2) uniformity drops due to poor alignment between architectures and preferences, with larger architectures incorrectly\\r\\npredicted for skewed preferences, which ideally require less\\r\\nresources, 3) hypervolume remains almost constant across\\r\\ndifferent c indicating poor cost control. Resource usage plots\\r\\nare presented in the appendix.\\r\\nAnalysis of task threshold. We compare the effect of vary-\\r\\n\\r\\nHV.↑\\r\\n\\r\\nMethod\\r\\n\\r\\nUnif.↑\\r\\n\\r\\n#Inference Params↓\\r\\n\\r\\nc = 0.0\\r\\n\\r\\nc = 1.0\\r\\n\\r\\nc = 0.0\\r\\n\\r\\nc = 1.0\\r\\n\\r\\nc = 0.0\\r\\n\\r\\nc = 1.0\\r\\n\\r\\nOurs\\r\\n\\r\\n13.43\\r\\n\\r\\n13.08\\r\\n\\r\\n0.76\\r\\n\\r\\n0.78\\r\\n\\r\\n41.06M\\r\\n\\r\\n34.68M\\r\\n\\r\\nno Linactive\\r\\nno layer weighting\\r\\nno task affinity A\\r\\nno task dichotomy\\r\\n\\r\\n12.81\\r\\n12.69\\r\\n12.53\\r\\n12.57\\r\\n\\r\\n12.73\\r\\n12.21\\r\\n12.35\\r\\n11.20\\r\\n\\r\\n0.49\\r\\n0.51\\r\\n0.51\\r\\n0.60\\r\\n\\r\\n0.51\\r\\n0.53\\r\\n0.52\\r\\n0.63\\r\\n\\r\\n61.75M\\r\\n46.21M\\r\\n45.73M\\r\\n56.73M\\r\\n\\r\\n54.78M\\r\\n41.33M\\r\\n42.55M\\r\\n45.17M\\r\\n\\r\\nTable 6. Ablation study on NYU-v2\\r\\nHypervolume\\r\\n\\r\\nMethod\\r\\n\\r\\n12\\r\\n10\\r\\n8\\r\\n\\r\\nτ = 0.1\\r\\nτ = 0.3\\r\\n0.2\\r\\n\\r\\nτ = 0.2\\r\\nτ = 0.5\\r\\n0.4\\r\\n\\r\\n0.6\\r\\n\\r\\n0.8\\r\\n\\r\\n1\\r\\n\\r\\nc\\r\\n\\r\\nFigure 8. Varying thresholds on NYU-v2\\r\\n\\r\\ning the threshold τ in Figure 8. Increasing the value beyond\\r\\n1/N (∼ 0.3) leads to loss of controllability as indicated by\\r\\nthe constant hypervolume across different values of c. This\\r\\nis due to the inability to account for uniform preferences. On\\r\\nthe other hand, choosing values below this threshold leads\\r\\nto comparable performance. Additional explanations are\\r\\nprovided in the appendix.\\r\\nTask classification. We analyse the importance of the induced task dichotomy by considering all tasks as active. This\\r\\nleads to: 1) high overall resource usage, and 2) poor controllability, especially at low values of c, as shown in Table 6.\\r\\nResource usage plots are presented in the appendix.\\r\\n\\r\\n5. Conclusion\\r\\nWe present a new framework for dynamic resource allocation in multi-task networks. We design a controller using hypernets to dynamically predict both network architecture and\\r\\nweights to match user-defined task trade-offs and resource\\r\\nconstraints. In contrast to current dynamic MTL methods\\r\\nwhich work with a fixed model, our formulation allows the\\r\\nflexibility in controlling the total compute cost and matches\\r\\nthe task preference better. We show the effectiveness of our\\r\\napproach on four multi-task settings, attaining diverse and\\r\\nefficient architectures across a wide range of preferences.\\r\\nLimitations and future work. Our framework searches\\r\\nsolely over network width and thus, the compute cost is\\r\\nlower bounded by network depth. One possible solution is\\r\\nto extend the search space to allow skip connections within\\r\\nand across streams to allow variable depth. Also, scalability\\r\\ncould be an issue as the required memory for the anchor\\r\\nnet is proportional to the number of tasks. Our future work\\r\\nwill address these issues by reducing the dependency on the\\r\\nanchor net initialization.\\r\\nAcknowledgements. This work was a part of Dripta S. Raychaudhuri’s internship at NEC Labs America. This work was\\r\\nalso partially supported by the NRI grant 2021-67022-33453\\r\\nand the NSF grant 1724341.\\r\\n\\r\\n\\x0c\\n\\nReferences\\r\\n[1] Youhei Akimoto, Shinichi Shirakawa, Nozomu Yoshinari,\\r\\nKento Uchida, Shota Saito, and Kouhei Nishida. Adaptive\\r\\nstochastic natural gradient method for one-shot neural architecture search. In International Conference on Machine\\r\\nLearning, pages 171–180. PMLR, 2019. 3\\r\\n[2] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay\\r\\nVasudevan, and Quoc Le. Understanding and simplifying\\r\\none-shot architecture search. In International Conference on\\r\\nMachine Learning, pages 550–559. PMLR, 2018. 3, 4\\r\\n[3] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh\\r\\nSaligrama. Adaptive neural networks for efficient inference.\\r\\nIn International Conference on Machine Learning, pages\\r\\n527–536. PMLR, 2017. 2\\r\\n[4] Andrew Brock, Theodore Lim, James M Ritchie, and Nick\\r\\nWeston. Smash: one-shot model architecture search through\\r\\nhypernetworks. arXiv preprint arXiv:1708.05344, 2017. 2, 3\\r\\n[5] David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van Gool. Automated search for resourceefficient branched multi-task networks. arXiv preprint\\r\\narXiv:2008.10292, 2020. 2, 6, 8, 14\\r\\n[6] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct\\r\\nneural architecture search on target task and hardware. arXiv\\r\\npreprint arXiv:1812.00332, 2018. 3\\r\\n[7] Rich Caruana. Multitask learning. Machine learning,\\r\\n28(1):41–75, 1997. 1, 2\\r\\n[8] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\\r\\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\\r\\nseparable convolution for semantic image segmentation. In\\r\\nProceedings of the European Conference on Computer Vision,\\r\\npages 801–818, 2018. 14\\r\\n[9] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew\\r\\nRabinovich. Gradnorm: Gradient normalization for adaptive\\r\\nloss balancing in deep multitask networks. In International\\r\\nConference on Machine Learning, pages 794–803. PMLR,\\r\\n2018. 2\\r\\n[10] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong,\\r\\nHenrik Kretzschmar, Yuning Chai, and Dragomir Anguelov.\\r\\nJust pick a sign: Optimizing deep multitask models with\\r\\ngradient sign dropout. In Advances in Neural Information\\r\\nProcessing Systems, 2020. 2\\r\\n[11] Alexey Dosovitskiy and Josip Djolonga. You only train once:\\r\\nLoss-conditional training of deep networks. In International\\r\\nConference on Learning Representations, 2019. 3\\r\\n[12] Kshitij Dwivedi and Gemma Roig. Representation similarity\\r\\nanalysis for efficient task taxonomy & transfer learning. In\\r\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12387–12396, 2019. 5,\\r\\n12\\r\\n[13] Mark Fleischer. The measure of pareto optima applications to\\r\\nmulti-objective metaheuristics. In International Conference\\r\\non Evolutionary Multi-Criterion Optimization, pages 519–\\r\\n533. Springer, 2003. 6\\r\\n[14] Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan L\\r\\nYuille. Nddr-cnn: Layerwise feature fusing in multi-task cnns\\r\\nby neural discriminative dimensionality reduction. In Pro-\\r\\n\\r\\n[15]\\r\\n\\r\\n[16]\\r\\n\\r\\n[17]\\r\\n[18]\\r\\n\\r\\n[19]\\r\\n\\r\\n[20]\\r\\n\\r\\n[21]\\r\\n\\r\\n[22]\\r\\n\\r\\n[23]\\r\\n\\r\\n[24]\\r\\n\\r\\n[25]\\r\\n[26]\\r\\n\\r\\n[27]\\r\\n\\r\\n[28]\\r\\n\\r\\n[29]\\r\\n\\r\\nceedings of the IEEE/CVF Conference on Computer Vision\\r\\nand Pattern Recognition, pages 3205–3214, 2019. 2\\r\\nPengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning\\r\\nto branch for multi-task learning. In International Conference\\r\\non Machine Learning, 2020. 1, 2, 3, 4, 8\\r\\nZichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,\\r\\nZechun Liu, Yichen Wei, and Jian Sun. Single path oneshot neural architecture search with uniform sampling. In\\r\\nProceedings of the European Conference on Computer Vision,\\r\\npages 544–560. Springer, 2020. 3\\r\\nDavid Ha, Andrew Dai, and Quoc V Le. Hypernetworks.\\r\\narXiv preprint arXiv:1609.09106, 2016. 1, 2, 3, 4, 14\\r\\nYizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui\\r\\nWang, and Yulin Wang. Dynamic neural networks: A survey.\\r\\narXiv preprint arXiv:2102.04906, 2021. 2\\r\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\r\\nDeep residual learning for image recognition. In Proceedings\\r\\nof the IEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition, pages 770–778, 2016. 14\\r\\nGao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens\\r\\nvan der Maaten, and Kilian Q Weinberger. Multi-scale dense\\r\\nnetworks for resource efficient image classification. arXiv\\r\\npreprint arXiv:1703.09844, 2017. 2\\r\\nEric Jang, Shixiang Gu, and Ben Poole. Categorical\\r\\nreparameterization with gumbel-softmax. arXiv preprint\\r\\narXiv:1611.01144, 2016. 5\\r\\nXu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V\\r\\nGool. Dynamic filter networks. Advances in Neural Information Processing Systems, 29:667–675, 2016. 2\\r\\nAlex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\\r\\nlearning using uncertainty to weigh losses for scene geometry\\r\\nand semantics. In Proceedings of the IEEE/CVF Conference\\r\\non Computer Vision and Pattern Recognition, pages 7482–\\r\\n7491, 2018. 2\\r\\nIasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision\\r\\nusing diverse datasets and limited memory. In Proceedings of\\r\\nthe IEE/CVFE Conference on Computer Vision and Pattern\\r\\nRecognition, pages 6129–6138, 2017. 2\\r\\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\\r\\nlayers of features from tiny images. 2009. 2, 6, 14\\r\\nChanglin Li, Guangrun Wang, Bing Wang, Xiaodan Liang,\\r\\nZhihui Li, and Xiaojun Chang. Dynamic slimmable network.\\r\\nIn Proceedings of the IEEE/CVF Conference on Computer\\r\\nVision and Pattern Recognition, pages 8607–8617, 2021. 2, 3\\r\\nYanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu\\r\\nZhang, Xingang Wang, and Jian Sun. Learning dynamic\\r\\nrouting for semantic segmentation. In Proceedings of the\\r\\nIEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition, pages 8553–8562, 2020. 2, 3, 6\\r\\nXi Lin, Zhiyuan Yang, Qingfu Zhang, and Sam Kwong.\\r\\nControllable pareto multi-task learning. arXiv preprint\\r\\narXiv:2010.06313, 2020. 1, 2, 3\\r\\nXi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and\\r\\nSam Kwong. Pareto multi-task learning. Advances in Neural\\r\\nInformation Processing Systems, 32:12060–12070, 2019. 2\\r\\n\\r\\n\\x0c\\n\\n[30] Gidi Littwin and Lior Wolf. Deep meta functionals for shape\\r\\nrepresentation. In Proceedings of the IEEE/CVF International\\r\\nConference on Computer Vision, pages 1824–1833, 2019. 2\\r\\n[31] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference\\r\\non Learning Representations, 2019. 3\\r\\n[32] Lanlan Liu and Jia Deng. Dynamic deep neural networks:\\r\\nOptimizing accuracy-efficiency trade-offs by selective execution. In Proceedings of the AAAI Conference on Artificial\\r\\nIntelligence, 2018. 2, 3\\r\\n[33] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S\\r\\nYu. Learning multiple tasks with multilinear relationship\\r\\nnetworks. arXiv preprint arXiv:1506.02117, 2015. 2\\r\\n[34] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficient multi-task\\r\\nfine-tuning for transformers via shared hypernetworks. arXiv\\r\\npreprint arXiv:2106.04489, 2021. 5\\r\\n[35] Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient descent with controlled\\r\\nascent in pareto optimization. In International Conference on\\r\\nMachine Learning, pages 6597–6607. PMLR, 2020. 2, 5\\r\\n[36] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning.\\r\\nIn Proceedings of the IEEE/CVF Conference on Computer\\r\\nVision and Pattern Recognition, pages 3994–4003, 2016. 2\\r\\n[37] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\\r\\nCho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\\r\\nAlan Yuille. The role of context for object detection and\\r\\nsemantic segmentation in the wild. In Proceedings of the\\r\\nIEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition, 2014. 2, 6, 14\\r\\n[38] Aviv Navon, Aviv Shamsian, Gal Chechik, and Ethan Fetaya.\\r\\nLearning the pareto front with hypernetworks. In International Conference on Learning Representations, 2021. 1, 2,\\r\\n3, 6, 14\\r\\n[39] Augustus Odena, Dieterich Lawson, and Christopher Olah.\\r\\nChanging model behavior at test-time using reinforcement\\r\\nlearning. arXiv preprint arXiv:1702.07780, 2017. 2, 3\\r\\n[40] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix:\\r\\nMonotonic value function factorisation for deep multi-agent\\r\\nreinforcement learning. In International Conference on Machine Learning, pages 4295–4304. PMLR, 2018. 2\\r\\n[41] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer.\\r\\nRouting networks: Adaptive selection of non-linear functions\\r\\nfor multi-task learning. arXiv preprint arXiv:1711.01239,\\r\\n2017. 6\\r\\n[42] Sebastian Ruder. An overview of multi-task learning in deep\\r\\nneural networks. arXiv preprint arXiv:1706.05098, 2017. 1,\\r\\n2, 3\\r\\n[43] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and\\r\\nAnders Søgaard. Latent multi-task architecture learning. In\\r\\nProceedings of the AAAI Conference on Artificial Intelligence,\\r\\n2019. 2\\r\\n[44] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\\r\\nresiduals and linear bottlenecks. In Proceedings of the\\r\\n\\r\\n[45]\\r\\n\\r\\n[46]\\r\\n\\r\\n[47]\\r\\n\\r\\n[48]\\r\\n\\r\\n[49]\\r\\n\\r\\n[50]\\r\\n\\r\\n[51]\\r\\n\\r\\n[52]\\r\\n\\r\\n[53]\\r\\n\\r\\n[54]\\r\\n\\r\\n[55]\\r\\n\\r\\n[56]\\r\\n\\r\\n[57]\\r\\n\\r\\n[58]\\r\\n\\r\\nIEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition, pages 4510–4520, 2018. 14\\r\\nElad Sarafian, Shai Keynan, and Sarit Kraus. Recomposing\\r\\nthe reinforcement learning building blocks with hypernetworks. In International Conference on Machine Learning,\\r\\npages 9301–9312. PMLR, 2021. 2\\r\\nJürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural\\r\\nComputation, 4(1):131–139, 1992. 2\\r\\nOzan Sener and Vladlen Koltun. Multi-task learning as multiobjective optimization. In Advances in Neural Information\\r\\nProcessing Systems, 2018. 2\\r\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\\r\\nFergus. Indoor segmentation and support inference from\\r\\nrgbd images. In Proceedings of the European Conference on\\r\\nComputer Vision, 2012. 2, 6, 14\\r\\nEmma Strubell, Ananya Ganesh, and Andrew McCallum.\\r\\nEnergy and policy considerations for deep learning in nlp.\\r\\narXiv preprint arXiv:1906.02243, 2019. 15\\r\\nXimeng Sun, Rameswar Panda, Rogerio Feris, and Kate\\r\\nSaenko. Adashare: Learning what to share for efficient deep\\r\\nmulti-task learning. arXiv preprint arXiv:1911.12423, 2019.\\r\\n2\\r\\nSimon Vandenhende, Stamatios Georgoulis, Bert De Brabandere, and Luc Van Gool. Branched multi-task networks: deciding what layers to share. arXiv preprint arXiv:1904.02920,\\r\\n2019. 4\\r\\nAndreas Veit and Serge Belongie. Convolutional networks\\r\\nwith adaptive inference graphs. In Proceedings of the European Conference on Computer Vision, pages 3–18, 2018.\\r\\n2\\r\\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation\\r\\nby entropy minimization. arXiv preprint arXiv:2006.10726,\\r\\n2020. 3, 5\\r\\nXin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E\\r\\nGonzalez. Skipnet: Learning dynamic routing in convolutional networks. In Proceedings of the European Conference\\r\\non Computer Vision, pages 409–424, 2018. 2\\r\\nBichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\\r\\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing\\r\\nJia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In\\r\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10734–10742, 2019. 3\\r\\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine,\\r\\nKarol Hausman, and Chelsea Finn. Gradient surgery for multitask learning. In Advances in Neural Information Processing\\r\\nSystems, 2020. 2\\r\\nZhihang Yuan, Bingzhe Wu, Guangyu Sun, Zheng Liang, Shiwan Zhao, and Weichen Bi. S2dnas: Transforming static cnn\\r\\nmodel for dynamic inference via neural architecture search.\\r\\nIn Proceedings of the European Conference on Computer\\r\\nVision, 2020. 2, 3\\r\\nAmir R Zamir, Alexander Sax, , William B Shen, Leonidas\\r\\nGuibas, Jitendra Malik, and Silvio Savarese. Taskonomy:\\r\\nDisentangling task transfer learning. In Proceedings of\\r\\n\\r\\n\\x0c\\n\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\r\\nRecognition, 2018. 1\\r\\n\\r\\nc = 0.0\\r\\n\\r\\nc = 0.3\\r\\n\\r\\n<latexit sha1_base64=\"E+K72hNm9lVnGocZCyBa+qa2aM4=\">AAAB7HicbVBNSwMxEJ2tX7V+VT16CRbBU9mVol6EohePFdy20C4lm862odnskmSFUvobvHhQxKs/yJv/xrTdg7Y+SHi8N8PMvDAVXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKmTTDH0WSIS1Q6pRsEl+oYbge1UIY1Dga1wdDfzW0+oNE/koxmnGMR0IHnEGTVW8tmNW3V75Yr95yCrxMtJBXI0euWvbj9hWYzSMEG17nhuaoIJVYYzgdNSN9OYUjaiA+xYKmmMOpjMl52SM6v0SZQo+6Qhc/V3x4TGWo/j0FbG1Az1sjcT//M6mYmugwmXaWZQssWgKBPEJGR2OelzhcyIsSWUKW53JWxIFWXG5lOyIXjLJ6+S5kXVu6zWHmqV+m0eRxFO4BTOwYMrqMM9NMAHBhye4RXeHOm8OO/Ox6K04OQ9x/AHzucPk4qN4w==</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"V3Z/XBx7PSGaLaa6ezPstIwVENg=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0i0qBeh6MVjBdMW2lA22027dHcTdjdCCf0NXjwo4tUf5M1/47bNQasPBh7vzTAzL0o508bzvpzSyura+kZ5s7K1vbO7V90/aOkkU4QGJOGJ6kRYU84kDQwznHZSRbGIOG1H49uZ336kSrNEPphJSkOBh5LFjGBjpYBce+55v1rzXG8O9Jf4BalBgWa/+tkbJCQTVBrCsdZd30tNmGNlGOF0WullmqaYjPGQdi2VWFAd5vNjp+jEKgMUJ8qWNGiu/pzIsdB6IiLbKbAZ6WVvJv7ndTMTX4U5k2lmqCSLRXHGkUnQ7HM0YIoSwyeWYKKYvRWREVaYGJtPxYbgL7/8l7TOXP/Crd/Xa42bIo4yHMExnIIPl9CAO2hCAAQYPMELvDrSeXbenPdFa8kpZg7hF5yPb5gWjeY=</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\n[59] Yu Zhang and Qiang Yang. A survey on multi-task learning.\\r\\nIEEE Transactions on Knowledge and Data Engineering,\\r\\n2021. 2\\r\\n[60] Eckart Zitzler and Lothar Thiele. Multiobjective evolutionary\\r\\nalgorithms: a comparative case study and the strength pareto\\r\\napproach. IEEE transactions on Evolutionary Computation,\\r\\n3(4):257–271, 1999. 6\\r\\n\\r\\nT3\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nc = 0.6\\r\\n\\r\\nT3\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nc = 1.0\\r\\n<latexit sha1_base64=\"H0RhIjR0liAXE9TO7lC0A2LloR4=\">AAAB7HicbVBNS8NAEJ2tX7V+VT16WSyCp5BIUS9C0YvHCqYttKFstpt26WYTdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpgKro3rfqPS2vrG5lZ5u7Kzu7d/UD08aukkU5T5NBGJ6oREM8El8w03gnVSxUgcCtYOx3czv/3ElOaJfDSTlAUxGUoecUqMlXx64zluv1pzHXcOvEq8gtSgQLNf/eoNEprFTBoqiNZdz01NkBNlOBVsWullmqWEjsmQdS2VJGY6yOfHTvGZVQY4SpQtafBc/T2Rk1jrSRzazpiYkV72ZuJ/Xjcz0XWQc5lmhkm6WBRlApsEzz7HA64YNWJiCaGK21sxHRFFqLH5VGwI3vLLq6R14XiXTv2hXmvcFnGU4QRO4Rw8uIIG3EMTfKDA4Rle4Q1J9ILe0ceitYSKmWP4A/T5A5UQjeQ=</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"fBJIsDnXst0iooGo1R6DIZ5+LvA=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0hEqheh6MVjBdMW2lA22027dLMbdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpRypo3nfTultfWNza3ydmVnd2//oHp41NIyU4QGRHKpOhHWlDNBA8MMp51UUZxEnLaj8d3Mbz9RpZkUj2aS0jDBQ8FiRrCxUkBuPLfer9Y815sDrRK/IDUo0OxXv3oDSbKECkM41rrre6kJc6wMI5xOK71M0xSTMR7SrqUCJ1SH+fzYKTqzygDFUtkSBs3V3xM5TrSeJJHtTLAZ6WVvJv7ndTMTX4c5E2lmqCCLRXHGkZFo9jkaMEWJ4RNLMFHM3orICCtMjM2nYkPwl19eJa0L16+7lw+XtcZtEUcZTuAUzsGHK2jAPTQhAAIMnuEV3hzhvDjvzseiteQUM8fwB87nD5yijek=</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nAppendix\\r\\nT3\\r\\n\\r\\nT1\\r\\n\\r\\nIn Figures 9-12, we plot the resource usage across different preferences on the NYU-v2 dataset to analyze the\\r\\neffect of different parts of our framework. The primary observation in each case is the lack of proper controllability.\\r\\nOn removing Linact or task dichotomy (Figures 9,12), the\\r\\noverall resource usage increases across all preferences. On\\r\\nthe other hand, removing the active loss branching weights\\r\\n(Figures 10,11) leads to a misallocation of resources - more\\r\\ncompute cost is allocated to skewed resources than the dense\\r\\nones.\\r\\n\\r\\nc = 0.0\\r\\n\\r\\n<latexit sha1_base64=\"V3Z/XBx7PSGaLaa6ezPstIwVENg=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0i0qBeh6MVjBdMW2lA22027dHcTdjdCCf0NXjwo4tUf5M1/47bNQasPBh7vzTAzL0o508bzvpzSyura+kZ5s7K1vbO7V90/aOkkU4QGJOGJ6kRYU84kDQwznHZSRbGIOG1H49uZ336kSrNEPphJSkOBh5LFjGBjpYBce+55v1rzXG8O9Jf4BalBgWa/+tkbJCQTVBrCsdZd30tNmGNlGOF0WullmqaYjPGQdi2VWFAd5vNjp+jEKgMUJ8qWNGiu/pzIsdB6IiLbKbAZ6WVvJv7ndTMTX4U5k2lmqCSLRXHGkUnQ7HM0YIoSwyeWYKKYvRWREVaYGJtPxYbgL7/8l7TOXP/Crd/Xa42bIo4yHMExnIIPl9CAO2hCAAQYPMELvDrSeXbenPdFa8kpZg7hF5yPb5gWjeY=</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT3\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nc = 0.6\\r\\n<latexit sha1_base64=\"V3Z/XBx7PSGaLaa6ezPstIwVENg=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0i0qBeh6MVjBdMW2lA22027dHcTdjdCCf0NXjwo4tUf5M1/47bNQasPBh7vzTAzL0o508bzvpzSyura+kZ5s7K1vbO7V90/aOkkU4QGJOGJ6kRYU84kDQwznHZSRbGIOG1H49uZ336kSrNEPphJSkOBh5LFjGBjpYBce+55v1rzXG8O9Jf4BalBgWa/+tkbJCQTVBrCsdZd30tNmGNlGOF0WullmqaYjPGQdi2VWFAd5vNjp+jEKgMUJ8qWNGiu/pzIsdB6IiLbKbAZ6WVvJv7ndTMTX4U5k2lmqCSLRXHGkUnQ7HM0YIoSwyeWYKKYvRWREVaYGJtPxYbgL7/8l7TOXP/Crd/Xa42bIo4yHMExnIIPl9CAO2hCAAQYPMELvDrSeXbenPdFa8kpZg7hF5yPb5gWjeY=</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nc = 0.6\\r\\n\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT3\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nT3\\r\\n\\r\\nT3\\r\\n\\r\\nT1\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nT3\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nc = 1.0\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"H0RhIjR0liAXE9TO7lC0A2LloR4=\">AAAB7HicbVBNS8NAEJ2tX7V+VT16WSyCp5BIUS9C0YvHCqYttKFstpt26WYTdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpgKro3rfqPS2vrG5lZ5u7Kzu7d/UD08aukkU5T5NBGJ6oREM8El8w03gnVSxUgcCtYOx3czv/3ElOaJfDSTlAUxGUoecUqMlXx64zluv1pzHXcOvEq8gtSgQLNf/eoNEprFTBoqiNZdz01NkBNlOBVsWullmqWEjsmQdS2VJGY6yOfHTvGZVQY4SpQtafBc/T2Rk1jrSRzazpiYkV72ZuJ/Xjcz0XWQc5lmhkm6WBRlApsEzz7HA64YNWJiCaGK21sxHRFFqLH5VGwI3vLLq6R14XiXTv2hXmvcFnGU4QRO4Rw8uIIG3EMTfKDA4Rle4Q1J9ILe0ceitYSKmWP4A/T5A5UQjeQ=</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nFigure 11. Resource usage on removing A\\r\\nc = 0.0\\r\\n\\r\\nc = 0.3\\r\\n\\r\\n<latexit sha1_base64=\"E+K72hNm9lVnGocZCyBa+qa2aM4=\">AAAB7HicbVBNSwMxEJ2tX7V+VT16CRbBU9mVol6EohePFdy20C4lm862odnskmSFUvobvHhQxKs/yJv/xrTdg7Y+SHi8N8PMvDAVXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKmTTDH0WSIS1Q6pRsEl+oYbge1UIY1Dga1wdDfzW0+oNE/koxmnGMR0IHnEGTVW8tmNW3V75Yr95yCrxMtJBXI0euWvbj9hWYzSMEG17nhuaoIJVYYzgdNSN9OYUjaiA+xYKmmMOpjMl52SM6v0SZQo+6Qhc/V3x4TGWo/j0FbG1Az1sjcT//M6mYmugwmXaWZQssWgKBPEJGR2OelzhcyIsSWUKW53JWxIFWXG5lOyIXjLJ6+S5kXVu6zWHmqV+m0eRxFO4BTOwYMrqMM9NMAHBhye4RXeHOm8OO/Ox6K04OQ9x/AHzucPk4qN4w==</latexit>\\r\\n\\r\\nT1\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nT2\\r\\n\\r\\n<latexit sha1_base64=\"fBJIsDnXst0iooGo1R6DIZ5+LvA=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0hEqheh6MVjBdMW2lA22027dLMbdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpRypo3nfTultfWNza3ydmVnd2//oHp41NIyU4QGRHKpOhHWlDNBA8MMp51UUZxEnLaj8d3Mbz9RpZkUj2aS0jDBQ8FiRrCxUkBuPLfer9Y815sDrRK/IDUo0OxXv3oDSbKECkM41rrre6kJc6wMI5xOK71M0xSTMR7SrqUCJ1SH+fzYKTqzygDFUtkSBs3V3xM5TrSeJJHtTLAZ6WVvJv7ndTMTX4c5E2lmqCCLRXHGkZFo9jkaMEWJ4RNLMFHM3orICCtMjM2nYkPwl19eJa0L16+7lw+XtcZtEUcZTuAUzsGHK2jAPTQhAAIMnuEV3hzhvDjvzseiteQUM8fwB87nD5yijek=</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nc = 1.0\\r\\n\\r\\nT2\\r\\n\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nT3\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"H0RhIjR0liAXE9TO7lC0A2LloR4=\">AAAB7HicbVBNS8NAEJ2tX7V+VT16WSyCp5BIUS9C0YvHCqYttKFstpt26WYTdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpgKro3rfqPS2vrG5lZ5u7Kzu7d/UD08aukkU5T5NBGJ6oREM8El8w03gnVSxUgcCtYOx3czv/3ElOaJfDSTlAUxGUoecUqMlXx64zluv1pzHXcOvEq8gtSgQLNf/eoNEprFTBoqiNZdz01NkBNlOBVsWullmqWEjsmQdS2VJGY6yOfHTvGZVQY4SpQtafBc/T2Rk1jrSRzazpiYkV72ZuJ/Xjcz0XWQc5lmhkm6WBRlApsEzz7HA64YNWJiCaGK21sxHRFFqLH5VGwI3vLLq6R14XiXTv2hXmvcFnGU4QRO4Rw8uIIG3EMTfKDA4Rle4Q1J9ILe0ceitYSKmWP4A/T5A5UQjeQ=</latexit>\\r\\n\\r\\nT1\\r\\nT1\\r\\n\\r\\nL−l\\r\\nL\\r\\n\\r\\nc = 0.3\\r\\n\\r\\n<latexit sha1_base64=\"E+K72hNm9lVnGocZCyBa+qa2aM4=\">AAAB7HicbVBNSwMxEJ2tX7V+VT16CRbBU9mVol6EohePFdy20C4lm862odnskmSFUvobvHhQxKs/yJv/xrTdg7Y+SHi8N8PMvDAVXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKmTTDH0WSIS1Q6pRsEl+oYbge1UIY1Dga1wdDfzW0+oNE/koxmnGMR0IHnEGTVW8tmNW3V75Yr95yCrxMtJBXI0euWvbj9hWYzSMEG17nhuaoIJVYYzgdNSN9OYUjaiA+xYKmmMOpjMl52SM6v0SZQo+6Qhc/V3x4TGWo/j0FbG1Az1sjcT//M6mYmugwmXaWZQssWgKBPEJGR2OelzhcyIsSWUKW53JWxIFWXG5lOyIXjLJ6+S5kXVu6zWHmqV+m0eRxFO4BTOwYMrqMM9NMAHBhye4RXeHOm8OO/Ox6K04OQ9x/AHzucPk4qN4w==</latexit>\\r\\n\\r\\nc = 0.3\\r\\n\\r\\n<latexit sha1_base64=\"E+K72hNm9lVnGocZCyBa+qa2aM4=\">AAAB7HicbVBNSwMxEJ2tX7V+VT16CRbBU9mVol6EohePFdy20C4lm862odnskmSFUvobvHhQxKs/yJv/xrTdg7Y+SHi8N8PMvDAVXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKmTTDH0WSIS1Q6pRsEl+oYbge1UIY1Dga1wdDfzW0+oNE/koxmnGMR0IHnEGTVW8tmNW3V75Yr95yCrxMtJBXI0euWvbj9hWYzSMEG17nhuaoIJVYYzgdNSN9OYUjaiA+xYKmmMOpjMl52SM6v0SZQo+6Qhc/V3x4TGWo/j0FbG1Az1sjcT//M6mYmugwmXaWZQssWgKBPEJGR2OelzhcyIsSWUKW53JWxIFWXG5lOyIXjLJ6+S5kXVu6zWHmqV+m0eRxFO4BTOwYMrqMM9NMAHBhye4RXeHOm8OO/Ox6K04OQ9x/AHzucPk4qN4w==</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nFigure 10. Resource usage on removing\\r\\n\\r\\n<latexit sha1_base64=\"fBJIsDnXst0iooGo1R6DIZ5+LvA=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0hEqheh6MVjBdMW2lA22027dLMbdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpRypo3nfTultfWNza3ydmVnd2//oHp41NIyU4QGRHKpOhHWlDNBA8MMp51UUZxEnLaj8d3Mbz9RpZkUj2aS0jDBQ8FiRrCxUkBuPLfer9Y815sDrRK/IDUo0OxXv3oDSbKECkM41rrre6kJc6wMI5xOK71M0xSTMR7SrqUCJ1SH+fzYKTqzygDFUtkSBs3V3xM5TrSeJJHtTLAZ6WVvJv7ndTMTX4c5E2lmqCCLRXHGkZFo9jkaMEWJ4RNLMFHM3orICCtMjM2nYkPwl19eJa0L16+7lw+XtcZtEUcZTuAUzsGHK2jAPTQhAAIMnuEV3hzhvDjvzseiteQUM8fwB87nD5yijek=</latexit>\\r\\n\\r\\nc = 0.0\\r\\n\\r\\nT3\\r\\n\\r\\nT1\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nA. Resource Usage Plots\\r\\n\\r\\nT3\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"V3Z/XBx7PSGaLaa6ezPstIwVENg=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0i0qBeh6MVjBdMW2lA22027dHcTdjdCCf0NXjwo4tUf5M1/47bNQasPBh7vzTAzL0o508bzvpzSyura+kZ5s7K1vbO7V90/aOkkU4QGJOGJ6kRYU84kDQwznHZSRbGIOG1H49uZ336kSrNEPphJSkOBh5LFjGBjpYBce+55v1rzXG8O9Jf4BalBgWa/+tkbJCQTVBrCsdZd30tNmGNlGOF0WullmqaYjPGQdi2VWFAd5vNjp+jEKgMUJ8qWNGiu/pzIsdB6IiLbKbAZ6WVvJv7ndTMTX4U5k2lmqCSLRXHGkUnQ7HM0YIoSwyeWYKKYvRWREVaYGJtPxYbgL7/8l7TOXP/Crd/Xa42bIo4yHMExnIIPl9CAO2hCAAQYPMELvDrSeXbenPdFa8kpZg7hF5yPb5gWjeY=</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT3\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nFigure 9. Resource usage on removing Linact\\r\\n\\r\\nT3\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nc = 0.6\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nT3\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nc = 1.0\\r\\n\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"H0RhIjR0liAXE9TO7lC0A2LloR4=\">AAAB7HicbVBNS8NAEJ2tX7V+VT16WSyCp5BIUS9C0YvHCqYttKFstpt26WYTdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpgKro3rfqPS2vrG5lZ5u7Kzu7d/UD08aukkU5T5NBGJ6oREM8El8w03gnVSxUgcCtYOx3czv/3ElOaJfDSTlAUxGUoecUqMlXx64zluv1pzHXcOvEq8gtSgQLNf/eoNEprFTBoqiNZdz01NkBNlOBVsWullmqWEjsmQdS2VJGY6yOfHTvGZVQY4SpQtafBc/T2Rk1jrSRzazpiYkV72ZuJ/Xjcz0XWQc5lmhkm6WBRlApsEzz7HA64YNWJiCaGK21sxHRFFqLH5VGwI3vLLq6R14XiXTv2hXmvcFnGU4QRO4Rw8uIIG3EMTfKDA4Rle4Q1J9ILe0ceitYSKmWP4A/T5A5UQjeQ=</latexit>\\r\\n\\r\\n<latexit sha1_base64=\"fBJIsDnXst0iooGo1R6DIZ5+LvA=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0hEqheh6MVjBdMW2lA22027dLMbdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpRypo3nfTultfWNza3ydmVnd2//oHp41NIyU4QGRHKpOhHWlDNBA8MMp51UUZxEnLaj8d3Mbz9RpZkUj2aS0jDBQ8FiRrCxUkBuPLfer9Y815sDrRK/IDUo0OxXv3oDSbKECkM41rrre6kJc6wMI5xOK71M0xSTMR7SrqUCJ1SH+fzYKTqzygDFUtkSBs3V3xM5TrSeJJHtTLAZ6WVvJv7ndTMTX4c5E2lmqCCLRXHGkZFo9jkaMEWJ4RNLMFHM3orICCtMjM2nYkPwl19eJa0L16+7lw+XtcZtEUcZTuAUzsGHK2jAPTQhAAIMnuEV3hzhvDjvzseiteQUM8fwB87nD5yijek=</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nT2\\r\\n<latexit sha1_base64=\"9hz6uGECkpnnkvr/I/BhGJJkneM=\">AAAB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyUoi6LblxW6ENoh5JJM21oJhmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck9OEHOmjet+O4WNza3tneJuaW//4PCofHzS0TJRhLaJ5FI9BlhTzgRtG2Y4fYwVxVHAaTeY3GV+d0qVZlK0zCymfoRHgoWMYGMlvx9hMyaYp635oDYoV9yquwBaJ15OKpCjOSh/9YeSJBEVhnCsdc9zY+OnWBlGOJ2X+ommMSYTPKI9SwWOqPbTReg5urDKEIVS2ScMWqi/N1IcaT2LAjuZhdSrXib+5/USE974KRNxYqggy0NhwpGRKGsADZmixPCZJZgoZrMiMsYKE2N7KtkSvNUvr5NOrepdVesP9UrjNq+jCGdwDpfgwTU04B6a0AYCT/AMr/DmTJ0X5935WI4WnHznFP7A+fwBv0OSGA==</latexit>\\r\\n\\r\\nB. Additional Results on Controllability\\r\\nTrade-off curves. In Figures 13 and 14, we visualize the\\r\\ntask controllability on pairs of tasks while keeping the preference on the rest fixed to zero. Compared to conventional\\r\\ndynamic models, our framework achieves a much larger\\r\\ndynamic range in terms of performance.\\r\\nMarginal evaluation. We present the marginal evaluation\\r\\nof task controllability on the PASCAL-Context 3 task learning setting in Figure 15.\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nT3\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nT1\\r\\n<latexit sha1_base64=\"XUXURlvbA4P3PuHoO83CatY5ipc=\">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV+oJ2KJk004ZmMmOSKZSh3+HGhSJu/Rh3/o2ZdhbaeiBwOOde7snxY8G1cZxvVNjY3NreKe6W9vYPDo/KxydtHSWKshaNRKS6PtFMcMlahhvBurFiJPQF6/iT+8zvTJnSPJJNM4uZF5KR5AGnxFjJ64fEjCkRaXM+cAflilN1FsDrxM1JBXI0BuWv/jCiScikoYJo3XOd2HgpUYZTwealfqJZTOiEjFjPUklCpr10EXqOL6wyxEGk7JMGL9TfGykJtZ6Fvp3MQupVLxP/83qJCW69lMs4MUzS5aEgEdhEOGsAD7li1IiZJYQqbrNiOiaKUGN7KtkS3NUvr5P2VdW9rtYea5X6XV5HEc7gHC7BhRuowwM0oAUUnuAZXuENTdELekcfy9ECyndO4Q/Q5w+9v5IX</latexit>\\r\\n\\r\\nT3\\r\\n<latexit sha1_base64=\"jes7E4VsxRzy0J2VgU42qqwWSHg=\">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBFclRkt6rLoxmWFvqAdSibNtKGZZEwyhTL0O9y4UMStH+POvzHTzkJbDwQO59zLPTlBzJk2rvvtrK1vbG5tF3aKu3v7B4elo+OWlokitEkkl6oTYE05E7RpmOG0EyuKo4DTdjC+z/z2hCrNpGiYaUz9CA8FCxnBxkp+L8JmRDBPG7P+Vb9UdivuHGiVeDkpQ456v/TVG0iSRFQYwrHWXc+NjZ9iZRjhdFbsJZrGmIzxkHYtFTii2k/noWfo3CoDFEplnzBorv7eSHGk9TQK7GQWUi97mfif101MeOunTMSJoYIsDoUJR0airAE0YIoSw6eWYKKYzYrICCtMjO2paEvwlr+8SlqXFe+6Un2slmt3eR0FOIUzuAAPbqAGD1CHJhB4gmd4hTdn4rw4787HYnTNyXdO4A+czx/Ax5IZ</latexit>\\r\\n\\r\\nFigure 12. Resource usage on removing task dichotomy\\r\\n\\r\\n\\x0c\\n\\n2.2\\r\\n\\r\\nc=0.0 w/o adaptation\\r\\nc=0.0\\r\\nPHN-BN\\r\\n\\r\\n2.6\\r\\n\\r\\n4.0\\r\\n2.0\\r\\nLoss: Depth\\r\\n\\r\\nLoss: Depth\\r\\n\\r\\n2.4\\r\\n2.2\\r\\n2.0\\r\\n1.8\\r\\n\\r\\nLoss: Surface Normals\\r\\n\\r\\n2.8\\r\\n\\r\\n1.8\\r\\n1.6\\r\\n\\r\\n1.6\\r\\n\\r\\n3.5\\r\\n3.0\\r\\n2.5\\r\\n\\r\\n1.4\\r\\n\\r\\n1.4\\r\\n2.0\\r\\n\\r\\n2.5\\r\\n\\r\\n3.0\\r\\n3.5\\r\\nLoss: Surface Normals\\r\\n\\r\\n4.0\\r\\n\\r\\n1.6\\r\\n\\r\\n(a) Surface Normal/Depth\\r\\n\\r\\n1.8\\r\\n2.0\\r\\n2.2\\r\\n2.4\\r\\nLoss: Semantic Segmentation\\r\\n\\r\\n2.0\\r\\n\\r\\n2.6\\r\\n\\r\\n(b) Semantic Seg./Depth\\r\\n\\r\\n1.5\\r\\n\\r\\n2.0\\r\\n2.5\\r\\n3.0\\r\\nLoss: Semantic Segmentation\\r\\n\\r\\n3.5\\r\\n\\r\\n(c) Semantic Seg./Surface Normal\\r\\n\\r\\nFigure 13. Trade-off curves on NYU-v2: Visualization of performance trade-off between pairs of tasks.\\r\\n2.5\\r\\n\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n\\r\\n1.4\\r\\n\\r\\n2.0\\r\\n\\r\\nLoss: Saliency\\r\\n\\r\\nLoss: Human Parts\\r\\n\\r\\n1.4\\r\\nLoss: Saliency\\r\\n\\r\\n1.6\\r\\n\\r\\nc=0.0 w/o adaptation\\r\\nc=0.0\\r\\nPHN-BN\\r\\n\\r\\n1.6\\r\\n\\r\\n1.5\\r\\n1.0\\r\\n\\r\\n0.6\\r\\n0.4\\r\\n0.6\\r\\n\\r\\n0.8\\r\\n1.0\\r\\n1.2\\r\\n1.4\\r\\n1.6\\r\\nLoss: Semantic Segmentation\\r\\n\\r\\n0.5\\r\\n0.25\\r\\n\\r\\n1.8\\r\\n\\r\\n(a) Human Parts/Saliency\\r\\n\\r\\n1.2\\r\\n1.0\\r\\n0.8\\r\\n0.6\\r\\n0.4\\r\\n\\r\\n0.50\\r\\n\\r\\n0.75 1.00 1.25 1.50 1.75\\r\\nLoss: Semantic Segmentation\\r\\n\\r\\n2.00\\r\\n\\r\\n(b) Semantic Seg./Human Parts\\r\\n\\r\\n2.25\\r\\n\\r\\n0.25\\r\\n\\r\\n0.50\\r\\n\\r\\n0.75 1.00 1.25 1.50 1.75\\r\\nLoss: Semantic Segmentation\\r\\n\\r\\n2.00\\r\\n\\r\\n(c) Semantic Seg./Saliency\\r\\n\\r\\nFigure 14. Trade-off curves on PASCAL-Context (3 tasks): Visualization of performance trade-off between pairs of tasks.\\r\\nSemantic Segmentation\\r\\n1.0\\r\\n0.5\\r\\n\\r\\nC. Architecture Evaluation\\r\\n\\r\\nHuman Parts\\r\\n\\r\\n1.5\\r\\nLoss\\r\\n\\r\\nLoss\\r\\n\\r\\n1.5\\r\\n\\r\\n1.0\\r\\n0.5\\r\\n\\r\\n0.2 0.4 0.6 0.8\\r\\nTask preference\\r\\n\\r\\n1\\r\\n\\r\\n0.2 0.4 0.6 0.8\\r\\nTask preference\\r\\n\\r\\n1\\r\\n\\r\\nSaliency\\r\\nLoss\\r\\n\\r\\n1.5\\r\\n\\r\\nc\\r\\nc\\r\\nc\\r\\nc\\r\\n\\r\\n1.0\\r\\n0.5\\r\\n0.2 0.4 0.6 0.8\\r\\nTask preference\\r\\n\\r\\n=\\r\\n=\\r\\n=\\r\\n=\\r\\n\\r\\n0.0 w/o adaptation\\r\\n1.0 w/o adaptation\\r\\n0.0\\r\\n1.0\\r\\n\\r\\n1\\r\\n\\r\\nFigure 15. Marginal evaluation on PASCAL-Context (3 tasks)\\r\\nMethod\\r\\n\\r\\nT1 ↑\\r\\n\\r\\nT2 ↑\\r\\n\\r\\nT3 ↑\\r\\n\\r\\nT4 ↓\\r\\n\\r\\nT5 ↓\\r\\n\\r\\nAvg ∆T (%) ↑\\r\\n\\r\\nSingle-Task\\r\\n\\r\\n63.88\\r\\n\\r\\n58.08\\r\\n\\r\\n64.92\\r\\n\\r\\n13.95\\r\\n\\r\\n0.018\\r\\n\\r\\n-\\r\\n\\r\\n# Params (%) ↓\\r\\n-\\r\\n\\r\\nLTB\\r\\nBMTAS\\r\\n\\r\\n59.45\\r\\n63.27\\r\\n\\r\\n56.48\\r\\n59.32\\r\\n\\r\\n65.29\\r\\n64.52\\r\\n\\r\\n14.19\\r\\n13.87\\r\\n\\r\\n0.018\\r\\n0.018\\r\\n\\r\\n-3.61\\r\\n+0.37\\r\\n\\r\\n-36.3\\r\\n-53.8\\r\\n\\r\\nOurs, c=0.0\\r\\nOurs† , c=0.0\\r\\nOurs, c=1.0\\r\\nOurs† , c=1.0\\r\\n\\r\\n61.64\\r\\n60.03\\r\\n61.15\\r\\n59.74\\r\\n\\r\\n56.95\\r\\n56.91\\r\\n56.85\\r\\n56.66\\r\\n\\r\\n64.55\\r\\n64.59\\r\\n64.67\\r\\n64.62\\r\\n\\r\\n13.72\\r\\n13.95\\r\\n13.74\\r\\n13.98\\r\\n\\r\\n0.018\\r\\n0.018\\r\\n0.018\\r\\n0.018\\r\\n\\r\\n-1.45\\r\\n-2.84\\r\\n-1.76\\r\\n-3.20\\r\\n\\r\\n-48.5\\r\\n-48.5\\r\\n-50.1\\r\\n-50.1\\r\\n\\r\\nTable 7. Architecture evaluation on PASCAL-Context (5 tasks).\\r\\nWe report the mean intersection over union for T1 : Semantic seg.,\\r\\nT2 : Parts seg., and T3 : Saliency. We report mean error in angle\\r\\nfor T4 : Surface normal and mean loss for T5 : Edge. Presence\\r\\nof † indicates that we train the networks from ImageNet weights,\\r\\nwhile its absence indicates training from anchor net weights.\\r\\n\\r\\nFigure 16 illustrates the architectures predicted by the\\r\\nedge hypernet for uniform task preference on the NYU-v2\\r\\n3-task setting. As we increase c, the model size decreases\\r\\nvia increased sharing of the low-level features. We also visualize the architectures for skewed preferences in Figure 17.\\r\\nThis leads to architectures which are predominantly a singlestream network, with the selected stream corresponding to\\r\\nthe primary task. In all cases, Task 1 denotes semantic\\r\\nsegmentation, Task 2 denotes surface normals estimation,\\r\\nand Task 3 denotes depth estimation. We report an additional evaluation of the predicted architectures on the PascalContext dataset in Table 7. We choose the architecture predicted for a uniform task preference and, similar to LTB, we\\r\\nretrain it for a fair comparison.\\r\\n\\r\\nD. Calculation of Task Affinity\\r\\nWe adopt Representational Similarity Analysis\\r\\n(RSA) [12] to obtain the task affinity scores from the anchor\\r\\nnet F. First, using a random subset of K instances from the\\r\\ntraining set, we extract the features for each of these data\\r\\npoints for every task. Let us denote the extracted feature\\r\\nmap for instance k at layer l for task i as f[i,l,k] . Using these\\r\\nfeature maps, we compute the feature similarity tensor Sl at\\r\\neach layer, of dimensions N × K × K, as follows,\\r\\n  \\\\mathbf {S}_l \\\\left (i,k,k\\'\\\\right ) = \\\\frac {\\\\langle \\\\bm {f}_{[i,l,k]},\\\\bm {f}_{[i,l,k\\']}\\\\rangle }{\\\\|\\\\bm {f}_{[i,l,k]}\\\\|\\\\|\\\\bm {f}_{[i,l,k\\']}\\\\|}. \\r\\n\\r\\n(7)\\r\\n\\r\\n\\x0c\\n\\nc = 0.0\\r\\n<latexit sha1_base64=\"E+K72hNm9lVnGocZCyBa+qa2aM4=\">AAAB7HicbVBNSwMxEJ2tX7V+VT16CRbBU9mVol6EohePFdy20C4lm862odnskmSFUvobvHhQxKs/yJv/xrTdg7Y+SHi8N8PMvDAVXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKmTTDH0WSIS1Q6pRsEl+oYbge1UIY1Dga1wdDfzW0+oNE/koxmnGMR0IHnEGTVW8tmNW3V75Yr95yCrxMtJBXI0euWvbj9hWYzSMEG17nhuaoIJVYYzgdNSN9OYUjaiA+xYKmmMOpjMl52SM6v0SZQo+6Qhc/V3x4TGWo/j0FbG1Az1sjcT//M6mYmugwmXaWZQssWgKBPEJGR2OelzhcyIsSWUKW53JWxIFWXG5lOyIXjLJ6+S5kXVu6zWHmqV+m0eRxFO4BTOwYMrqMM9NMAHBhye4RXeHOm8OO/Ox6K04OQ9x/AHzucPk4qN4w==</latexit>\\r\\n\\r\\nTask 1\\r\\n\\r\\nx\\r\\n\\r\\nTask 3\\r\\n\\r\\n<latexit sha1_base64=\"khnBQ7PuF5ECJ3Lc4XrNtQGQPvM=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae0Y8lk0jY0kwxJRi1D/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJYs60cd1vp7Cyura+UdwsbW3v7O6V9w9aWiaK0CaRXKpOgDXlTNCmYYbTTqwojgJO28H4OvPbD1RpJsWdmcTUj/BQsAEj2FjpvhdIHupJZK/0adovV9yqOwNaJl5OKpCj0S9/9UJJkogKQzjWuuu5sfFTrAwjnE5LvUTTGJMxHtKupQJHVPvpLPUUnVglRAOp7BEGzdTfGymOdBbNTkbYjPSil4n/ed3EDC79lIk4MVSQ+UODhCMjUVYBCpmixPCJJZgoZrMiMsIKE2OLKtkSvMUvL5PWWdU7r9Zua5X6VV5HEY7gGE7Bgwuoww00oAkEFDzDK7w5j86L8+58zEcLTr5zCH/gfP4AVjqTEg==</latexit>\\r\\n\\r\\nTask 2\\r\\n\\r\\nc = 1.0\\r\\n<latexit sha1_base64=\"H0RhIjR0liAXE9TO7lC0A2LloR4=\">AAAB7HicbVBNS8NAEJ2tX7V+VT16WSyCp5BIUS9C0YvHCqYttKFstpt26WYTdjdCCf0NXjwo4tUf5M1/47bNQVsfDDzem2FmXpgKro3rfqPS2vrG5lZ5u7Kzu7d/UD08aukkU5T5NBGJ6oREM8El8w03gnVSxUgcCtYOx3czv/3ElOaJfDSTlAUxGUoecUqMlXx64zluv1pzHXcOvEq8gtSgQLNf/eoNEprFTBoqiNZdz01NkBNlOBVsWullmqWEjsmQdS2VJGY6yOfHTvGZVQY4SpQtafBc/T2Rk1jrSRzazpiYkV72ZuJ/Xjcz0XWQc5lmhkm6WBRlApsEzz7HA64YNWJiCaGK21sxHRFFqLH5VGwI3vLLq6R14XiXTv2hXmvcFnGU4QRO4Rw8uIIG3EMTfKDA4Rle4Q1J9ILe0ceitYSKmWP4A/T5A5UQjeQ=</latexit>\\r\\n\\r\\nTask 1\\r\\n\\r\\nx\\r\\n\\r\\nTask 3\\r\\n\\r\\n<latexit sha1_base64=\"khnBQ7PuF5ECJ3Lc4XrNtQGQPvM=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae0Y8lk0jY0kwxJRi1D/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJYs60cd1vp7Cyura+UdwsbW3v7O6V9w9aWiaK0CaRXKpOgDXlTNCmYYbTTqwojgJO28H4OvPbD1RpJsWdmcTUj/BQsAEj2FjpvhdIHupJZK/0adovV9yqOwNaJl5OKpCj0S9/9UJJkogKQzjWuuu5sfFTrAwjnE5LvUTTGJMxHtKupQJHVPvpLPUUnVglRAOp7BEGzdTfGymOdBbNTkbYjPSil4n/ed3EDC79lIk4MVSQ+UODhCMjUVYBCpmixPCJJZgoZrMiMsIKE2OLKtkSvMUvL5PWWdU7r9Zua5X6VV5HEY7gGE7Bgwuoww00oAkEFDzDK7w5j86L8+58zEcLTr5zCH/gfP4AVjqTEg==</latexit>\\r\\n\\r\\nTask 2\\r\\n\\r\\nFigure 16. Predicted architectures on NYU-v2 for uniform task preference.\\r\\n\\r\\nx\\r\\n\\r\\nTask 1\\r\\n\\r\\n<latexit sha1_base64=\"khnBQ7PuF5ECJ3Lc4XrNtQGQPvM=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae0Y8lk0jY0kwxJRi1D/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJYs60cd1vp7Cyura+UdwsbW3v7O6V9w9aWiaK0CaRXKpOgDXlTNCmYYbTTqwojgJO28H4OvPbD1RpJsWdmcTUj/BQsAEj2FjpvhdIHupJZK/0adovV9yqOwNaJl5OKpCj0S9/9UJJkogKQzjWuuu5sfFTrAwjnE5LvUTTGJMxHtKupQJHVPvpLPUUnVglRAOp7BEGzdTfGymOdBbNTkbYjPSil4n/ed3EDC79lIk4MVSQ+UODhCMjUVYBCpmixPCJJZgoZrMiMsIKE2OLKtkSvMUvL5PWWdU7r9Zua5X6VV5HEY7gGE7Bgwuoww00oAkEFDzDK7w5j86L8+58zEcLTr5zCH/gfP4AVjqTEg==</latexit>\\r\\n\\r\\nTask 2\\r\\n<latexit sha1_base64=\"Czb46SLafA0hQAklNc5Be8dg494=\">AAACAXicbVDLSgMxFM3UV62vUTeCm2ARXJQyI0XdCEU3LivYVpgOJZPJtKGZZEgyQhnqxl9x40IRt/6FO//GTDsLbT0QcjjnXu69J0gYVdpxvq3S0vLK6lp5vbKxubW9Y+/udZRIJSZtLJiQ9wFShFFO2ppqRu4TSVAcMNINRte5330gUlHB7/Q4IX6MBpxGFCNtpL590AsEC9U4Nl8mJ/ASem7NqTl+3646dWcKuEjcglRBgVbf/uqFAqcx4RozpJTnOon2MyQ1xYxMKr1UkQThERoQz1COYqL8bHrBBB4bJYSRkOZxDafq744MxSpf0lTGSA/VvJeL/3leqqMLP6M8STXheDYoShnUAuZxwJBKgjUbG4KwpGZXiIdIIqxNaBUTgjt/8iLpnNbds3rjtlFtXhVxlMEhOAInwAXnoAluQAu0AQaP4Bm8gjfryXqx3q2PWWnJKnr2wR9Ynz+ympW/</latexit>\\r\\n\\r\\nr = [1, 0, 0]\\r\\n\\r\\nTask 3\\r\\n\\r\\nx\\r\\n\\r\\nTask 2\\r\\n\\r\\n<latexit sha1_base64=\"khnBQ7PuF5ECJ3Lc4XrNtQGQPvM=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae0Y8lk0jY0kwxJRi1D/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJYs60cd1vp7Cyura+UdwsbW3v7O6V9w9aWiaK0CaRXKpOgDXlTNCmYYbTTqwojgJO28H4OvPbD1RpJsWdmcTUj/BQsAEj2FjpvhdIHupJZK/0adovV9yqOwNaJl5OKpCj0S9/9UJJkogKQzjWuuu5sfFTrAwjnE5LvUTTGJMxHtKupQJHVPvpLPUUnVglRAOp7BEGzdTfGymOdBbNTkbYjPSil4n/ed3EDC79lIk4MVSQ+UODhCMjUVYBCpmixPCJJZgoZrMiMsIKE2OLKtkSvMUvL5PWWdU7r9Zua5X6VV5HEY7gGE7Bgwuoww00oAkEFDzDK7w5j86L8+58zEcLTr5zCH/gfP4AVjqTEg==</latexit>\\r\\n\\r\\nTask 1\\r\\n<latexit sha1_base64=\"2/phTbSiYmy/UqFgHCU0soI7QKk=\">AAACAXicbVDLSgMxFM3UV62vUTeCm2ARXJQyI0XdCEU3LivYVpgOJZPJtKGZZEgyQhnqxl9x40IRt/6FO//GTDsLbT0QcjjnXu69J0gYVdpxvq3S0vLK6lp5vbKxubW9Y+/udZRIJSZtLJiQ9wFShFFO2ppqRu4TSVAcMNINRte5330gUlHB7/Q4IX6MBpxGFCNtpL590AsEC9U4Nl8mJ/ASek7NrTl+3646dWcKuEjcglRBgVbf/uqFAqcx4RozpJTnOon2MyQ1xYxMKr1UkQThERoQz1COYqL8bHrBBB4bJYSRkOZxDafq744MxSpf0lTGSA/VvJeL/3leqqMLP6M8STXheDYoShnUAuZxwJBKgjUbG4KwpGZXiIdIIqxNaBUTgjt/8iLpnNbds3rjtlFtXhVxlMEhOAInwAXnoAluQAu0AQaP4Bm8gjfryXqx3q2PWWnJKnr2wR9Ynz+ymJW/</latexit>\\r\\n\\r\\nr = [0, 1, 0]\\r\\n\\r\\nTask 3\\r\\n\\r\\nx\\r\\n\\r\\nTask 3\\r\\n\\r\\n<latexit sha1_base64=\"khnBQ7PuF5ECJ3Lc4XrNtQGQPvM=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFNy4r2Ae0Y8lk0jY0kwxJRi1D/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJYs60cd1vp7Cyura+UdwsbW3v7O6V9w9aWiaK0CaRXKpOgDXlTNCmYYbTTqwojgJO28H4OvPbD1RpJsWdmcTUj/BQsAEj2FjpvhdIHupJZK/0adovV9yqOwNaJl5OKpCj0S9/9UJJkogKQzjWuuu5sfFTrAwjnE5LvUTTGJMxHtKupQJHVPvpLPUUnVglRAOp7BEGzdTfGymOdBbNTkbYjPSil4n/ed3EDC79lIk4MVSQ+UODhCMjUVYBCpmixPCJJZgoZrMiMsIKE2OLKtkSvMUvL5PWWdU7r9Zua5X6VV5HEY7gGE7Bgwuoww00oAkEFDzDK7w5j86L8+58zEcLTr5zCH/gfP4AVjqTEg==</latexit>\\r\\n\\r\\nTask 1\\r\\n<latexit sha1_base64=\"keskYqF0whac9dsMdraBh+7/yYQ=\">AAACAXicbVDLSgMxFM3UV62vUTeCm2ARXJQyI0XdCEU3LivYVpgOJZPJtKGZZEgyQhnqxl9x40IRt/6FO//GTDsLbT0QcjjnXu69J0gYVdpxvq3S0vLK6lp5vbKxubW9Y+/udZRIJSZtLJiQ9wFShFFO2ppqRu4TSVAcMNINRte5330gUlHB7/Q4IX6MBpxGFCNtpL590AsEC9U4Nl8mJ/ASek7Nqbl+3646dWcKuEjcglRBgVbf/uqFAqcx4RozpJTnOon2MyQ1xYxMKr1UkQThERoQz1COYqL8bHrBBB4bJYSRkOZxDafq744MxSpf0lTGSA/VvJeL/3leqqMLP6M8STXheDYoShnUAuZxwJBKgjUbG4KwpGZXiIdIIqxNaBUTgjt/8iLpnNbds3rjtlFtXhVxlMEhOAInwAXnoAluQAu0AQaP4Bm8gjfryXqx3q2PWWnJKnr2wR9Ynz+ylpW/</latexit>\\r\\n\\r\\nr = [0, 0, 1]\\r\\n\\r\\nTask 2\\r\\n\\r\\n  D_l(i,j) = \\\\|\\\\mathbf {S}_l \\\\left (i,:,:\\\\right )-\\\\mathbf {S}_l \\\\left (j,:,:\\\\right )\\\\|_{F}. \\r\\n\\r\\n(8)\\r\\n\\r\\nSe\\r\\ng\\r\\nrts\\r\\nPa\\r\\n\\r\\nSe\\r\\nma\\r\\n\\r\\ncti\\r\\nc\\r\\n\\r\\nSe\\r\\n\\r\\ng.\\r\\n\\r\\nThe task dissimilarity matrix Dl at layer l, of dimensions\\r\\nN × N , is calculated as follows,\\r\\n\\r\\n.\\r\\nSa\\r\\nlie\\r\\nnc\\r\\ny\\r\\nSu\\r\\nrfa\\r\\nce\\r\\nNo\\r\\nrm\\r\\nals\\r\\nEd\\r\\nge\\r\\n\\r\\nFigure 17. Predicted architectures on NYU-v2 for preferences focusing on a single task.\\r\\n\\r\\n1.0\\r\\n\\r\\nSemactic Seg.\\r\\n\\r\\nThe rows of Dl are normalized separately between [0, 1] to\\r\\nget the scaled task dissimilarity matrix D̂l . The task affinity\\r\\nmatrix is subsequently calculated as Al (i, j) = 1 − D̂l (i, j).\\r\\nThe factor A(i, j) is obtained by taking the mean\\r\\nP of the\\r\\naffinity scores across layers, i.e., A(i, j) = L1 l Al (i, j).\\r\\nFigure 18 presents an example of A for the PASCAL-Context\\r\\ndataset.\\r\\n\\r\\n0.8\\r\\nParts Seg.\\r\\n0.6\\r\\nSaliency\\r\\n0.4\\r\\nSurface Normals\\r\\n0.2\\r\\n\\r\\nE. Calculation of Puse\\r\\nPuse (l, i) represents the probability that the node i at\\r\\nlayer l is present in the tree structure that is sampled by the\\r\\nedge hypernet. We follow a dynamic programming approach\\r\\nto calculate this value. Denoting the ith node in the lth layer\\r\\n\\r\\nEdge\\r\\n\\r\\nFigure 18. Task affinity for PASCAL-Context 5-task setting\\r\\n\\r\\nas (l, i), we can define the marginal probability that the node\\r\\n\\r\\n\\x0c\\n\\n(l + 1, k) samples the node (l, i) as its parent as νkl (i). Thus,\\r\\nthe probability that the node (l, i) is used in the sampled tree\\r\\nstructure is\\r\\n  \\\\label {eq:p_use} P_{use}(l,i) = 1 - \\\\prod _{k}\\\\{ 1 - P_{use}(l+1,k) \\\\cdot \\\\nu ^{l}_k(i) \\\\}, \\r\\n\\r\\n(9)\\r\\n\\r\\nwhere Puse (L, i) = 1 for all i ∈ [N ].\\r\\nNote that the actual path chosen from hypernet can be\\r\\ndifferent from the ones whose α is penalized in (3). We also\\r\\ntested an alternative, which applies dynamic programming\\r\\nsimilarly as in (9), to penalize exact activated path. Since the\\r\\nperformance gain was minor, we adopt (3) for simplicity.\\r\\n\\r\\nF. Pseudo Code\\r\\nAlgorithm 1 Training controllable dynamic multi-task architecture.\\r\\n1: Initialize F with single-task weights\\r\\n\\\\triangleright  Anchor net\\r\\n2: while h not converged do\\r\\n\\\\triangleright  Edge hypernet\\r\\n3:\\r\\nr ∼ Dir(η), c ∼ Unif(0, 1)\\r\\n4:\\r\\nSample batch {(x, y1 , . . . , yN )}\\r\\n5:\\r\\nα̂ = h(r, c; ϕ)\\r\\n6:\\r\\nCalculate Ω(r, c, α̂) P\\r\\n7:\\r\\nCalculate Ltask (r) = i ri Li (x, yi , F, α̂)\\r\\n8:\\r\\nUpdate ϕ by back-propagating Ltask + Ω\\r\\n9: while h̄ not converged do\\r\\n\\\\triangleright  Weight hypernet\\r\\n10:\\r\\nr ∼ Dir(η), c ∼ Unif(0, 1)\\r\\n11:\\r\\nSample batch {(x, y1 , . . . , yN )}\\r\\n12:\\r\\nα̂ = h(r, c; ϕ)\\r\\n13:\\r\\nβ̂, γ̂ = h̄(r, c; ϕ̄)\\r\\nP\\r\\n14:\\r\\nCalculate Ltask (r) = i ri Li (x, yi , F, α̂, β̂, γ̂)\\r\\n15:\\r\\nUpdate ϕ̄ by back-propagating Ltask\\r\\n\\r\\nG. Implementation Details\\r\\nHypernet architecture. The edge hypernet is constructed\\r\\nusing an MLP with two hidden layers (dimension 100) and\\r\\nL linear heads, (dimension N × N ) which output the flattened branching distribution parameters at each layer. For\\r\\nthe weight hypernet, we use a similar MLP with three hidden\\r\\nlayers (dimension 100) and generate the normalization parameters using linear heads. In both cases we use learnable\\r\\nembeddings for each task (ei ) and the cost (ec ). Given the\\r\\nuser preference\\r\\nP (r, c), the preference embedding is calculated as p = i ri ei + cec . This embedding p is then used\\r\\nas input the MLP. The preference dimension is set to 32\\r\\nin all experiments. PHN-BN and PHN [38] use a similar\\r\\narchitecture to the weight hypernet, with PHN employing an\\r\\nadditional chunking [17] embedding for scalability.\\r\\nTask loss scaling. Due to the different scales of the various\\r\\ntask losses, we first weight the loss terms with a factor wi\\r\\n\\r\\nη\\r\\n\\r\\n0.1\\r\\n\\r\\n0.2\\r\\n\\r\\n0.5\\r\\n\\r\\n1.0\\r\\n\\r\\nc=0.0\\r\\nc=1.0\\r\\n\\r\\n4.20\\r\\n4.18\\r\\n\\r\\n4.26\\r\\n4.25\\r\\n\\r\\n4.26\\r\\n4.25\\r\\n\\r\\n4.26\\r\\n4.21\\r\\n\\r\\nTable 8. Varying parameterization of Dirichlet distribution.\\r\\n\\r\\nbefore applying linear scalarization with respect to r. This\\r\\nensures that the relative task importance is\\r\\nPnot skewed by\\r\\nthe different loss scales. Thus, Ltask (r) = i ri wi Li .\\r\\nAnchor net architecture. The anchor net comprises N\\r\\nsingle-task networks, with each stream corresponding to a\\r\\nparticular task. For experiments on dense prediction tasks,\\r\\nwe use the DeepLabv3+ architecture [8] for each task. The\\r\\nMobileNetV2 [44] backbone is used for experiments on\\r\\nPASCAL-Context [37], while the ResNet-34 [19] backbone\\r\\nis used for experiments on NYU-v2 [48]. For CIFAR100 [25] experiments we use the ResNet-9 1 architecture\\r\\nwith linear task heads.\\r\\nHyperparameters. For experiments on CIFAR-100 we use\\r\\nλA = 0.2 and λI = 0.02. For the rest, we use λA = 1 and\\r\\nλI = 0.1. The task scaling weights are given below:\\r\\n1. NYU-v2:\\r\\n• Semantic segmentation: 1\\r\\n• Surface normals: 10\\r\\n• Depth: 3\\r\\n2. PASCAL-Context:\\r\\n• Semantic segmentation: 1\\r\\n• Human parts segmentation: 2\\r\\n• Saliency: 5\\r\\n• Surface normals: 10\\r\\n• Edge: 50\\r\\nFor CIFAR-100, all tasks are equally weighted.\\r\\nThe threshold τ is set to 0.02 for CIFAR-100, 0.2\\r\\nfor NYU-v2 and PASCAL-Context (3 task), and 0.1 for\\r\\nPASCAL-Context (5 task).\\r\\nTraining. Hypernetworks are trained using Adam for 30K\\r\\nsteps with a learning rate of 1e−3, reduced by a factor of\\r\\n0.3 every 14K steps. Temperature ζ is initialized to 5 and\\r\\nis decayed by 0.97 every 300 steps. Single-task networks\\r\\nfor dense-prediction tasks are trained in accordance to [5].\\r\\nFor CIFAR, we use Adam with a learning rate of 1e−3 and\\r\\nweight decay of 1e−5 for 75 epochs.\\r\\nPreference sampling. During training we sample preferences (r, c) from the distribution P(r,c) = Pr Pc , where Pr\\r\\nis defined as a Dirichlet distribution of order N with parameter η = [η1 , η2 , . . . , ηN ] (ηi > 0) and Pc is defined as a standard uniform distribution Unif(0, 1). Following [38], we set\\r\\nη = (0.2, 0.2, . . . , 0.2) for all our experiments. As shown\\r\\nin Table 8, varying this parameter on PASCAL-Context (3\\r\\ntasks) does not have any significant impact on the hypervolume.\\r\\n1 https://github.com/davidcpage/cifar10-fast\\r\\n\\r\\n\\x0c\\n\\nH. Effect of τ\\r\\n\\r\\nK. Potential Negative Societal Impact\\r\\n\\r\\nSetting τ ≫ N1 leads to virtually all tasks being treated\\r\\nas inactive. This results in lack of control since there is no\\r\\nactive task to be tied to the resource control c. Also, there\\r\\nis no compression since the inactive loss searches for active\\r\\ntasks to share nodes with. Consequently, this mostly leads to\\r\\na fully branched out network which explains the flat curve in\\r\\nFigure 8. When τ = N1 + ϵ (ϵ ≥ 0), both losses are in effect,\\r\\nwhich results in proper controllability, but possible ignorance\\r\\nof the uniform (or near uniform) preferences explains the\\r\\nslightly poorer performance. In our experiments, τ = 0.6/N\\r\\nworks well across all datasets.\\r\\n\\r\\nThe requirement of heavy computation makes neural architecture search (NAS) environmentally unfriendly. As\\r\\npointed out in [49], the CO2 emissions from a NAS process\\r\\ncan be comparable to that from 5 cars’ lifetime. Since our\\r\\nframework is fundamentally a NAS method for multi-task\\r\\nlearning, it shares these drawbacks. However, due to the\\r\\ndynamic nature of our method, it saves energy in the long\\r\\nrun by allowing a single network to emulate different models\\r\\ncorresponding to various preferences.\\r\\n\\r\\nI. Computational Resource\\r\\nController overhead. The usage of hypernetworks leads\\r\\nto additional resource usage which we term as controller\\r\\noverhead. In comparison to PHN-BN, while we incur a\\r\\nlarger overhead due to the prediction of a higher number of\\r\\nnormalization parameters, the effect is minimal due to the\\r\\ncontroller being active infrequently only during preference\\r\\nchange. This overhead has no impact on inference.\\r\\nModel size. In this work, we consider inference time as a\\r\\nmeasure of computational resource. We believe fast inference matters in many practical scenarios, whereas reasonable\\r\\namount of increase in memory overhead is relatively easy\\r\\nto handle. This justifies our design choice to introduce the\\r\\nanchor net.\\r\\n\\r\\nJ. Hypervolume\\r\\nGiven a point set S ⊂ RN and a reference point p ∈ RN\\r\\nin the loss space, the hypervolume of the set S is defined as\\r\\nthe size of the region dominated by S and bounded above by\\r\\np,\\r\\n  \\\\textrm {HV}(S) = \\\\lambda \\\\left (\\\\{a\\\\in \\\\mathbb {R}^N|\\\\exists b\\\\in S:b\\\\preceq a \\\\textrm { and } b\\\\preceq p\\\\}\\\\right ), \\r\\n(10)\\r\\nwhere λ is the Lebesgue measure. Figure 19 shows an example in two-dimensions.\\r\\n\\r\\n<latexit sha1_base64=\"mcbuxDXTW9QgPzCOHFrdFyHodyI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOlZtIvV9yqOwdZJV5OKpCj0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrqaQRaj+bHzolZ1YZkDBWtqQhc/X3REYjrSdRYDsjakZ62ZuJ/3nd1ITXfsZlkhqUbLEoTAUxMZl9TQZcITNiYgllittbCRtRRZmx2ZRsCN7yy6ukfVH1Lqu1Zq1Sv8njKMIJnMI5eHAFdbiDBrSAAcIzvMKb8+i8OO/Ox6K14OQzx/AHzucP3QeM/Q==</latexit>\\r\\n\\r\\np\\r\\n\\r\\nFigure 19. Hypervolume calculation. The shaded area represents\\r\\nthe hypervolume obtained by the point set comprising the red points,\\r\\nwith respect to the reference green point p.\\r\\n\\r\\n\\x0c']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the text into one string\n",
    "\n",
    "array_pdf_text=[]\n",
    "def pdf_to_text():\n",
    "    for i in array_link:\n",
    "        array_pdf_text.append(\"\\n\\n\".join(load_pdf(i[21:])))\n",
    "    return array_pdf_text\n",
    "pdf_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc82c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Minghao.Chen', 'Fangyun.Wei', 'Chong.Li', 'Deng.Cai']\n",
      "Frame-wise Action Representations for Long Videos via\r\n",
      "Sequence Contrastive Learning\r\n",
      "\r\n",
      "arXiv:2203.14957v1 [cs.CV] 28 Mar 2022\r\n",
      "\r\n",
      "Minghao Chen1∗ Fangyun Wei2† Chong Li2 Deng Cai1\r\n",
      "1\r\n",
      "State Key Lab of CAD&CG, College of Computer Science, Zhejiang University\r\n",
      "2\r\n",
      "Microsoft Research Asia\r\n",
      "minghaochen01@gmail.com\r\n",
      "\r\n",
      "{fawe, chol}@microsoft.com\r\n",
      "\r\n",
      "dengcai@cad.zju.edu.cn\r\n",
      "\r\n",
      "Abstract\r\n",
      "Prior works on action representation learning mainly focus on designing various architectures to extract the global\r\n",
      "representations for short video clips. In contrast, many\r\n",
      "practical applications such as video alignment have strong\r\n",
      "demand for learning dense representations for long videos.\r\n",
      "In this paper, we introduce a novel contrastive action representation learning (CARL) framework to learn frame-wise\r\n",
      "action representations, especially for long videos, in a selfsupervised manner. Concretely, we introduce a simple yet\r\n",
      "efficient video encoder that considers spatio-temporal context to extract frame-wise representations. Inspired by the\r\n",
      "recent progress of self-supervised learning, we present a\r\n",
      "novel sequence contrastive loss (SCL) applied on two correlated views obtained through a series of spatio-temporal\r\n",
      "data augmentations. SCL optimizes the embedding space by\r\n",
      "minimizing the KL-divergence between the sequence similarity of two augmented views and a prior Gaussian distribution of timestamp distance. Experiments on FineGym,\r\n",
      "PennAction and Pouring datasets show that our method\r\n",
      "outperforms previous state-of-the-art by a large margin\r\n",
      "for downstream fine-grained action classification. Surprisingly, although without training on paired videos, our\r\n",
      "approach also shows outstanding performance on video\r\n",
      "alignment and fine-grained frame retrieval tasks. Code\r\n",
      "and models are available at https://github.com/\r\n",
      "minghchen/CARL_code.\r\n",
      "\r\n",
      "Query\r\n",
      "\r\n",
      "Top-3 Retrieved Results\r\n",
      "\r\n",
      "(a) Fine-grained frame retrieval on FineGym dataset.\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "Hand touches\r\n",
      "bottle\r\n",
      "\r\n",
      "Liquid starts\r\n",
      "exiting\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "Pouring\r\n",
      "complete\r\n",
      "\r\n",
      "Bottle back on\r\n",
      "table\r\n",
      "\r\n",
      "(b) Phase boundary detection on Pouring dataset.\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "(c) Temporal video alignment on PennAction dataset.\r\n",
      "\r\n",
      "Figure 1. Multiple applications of our frame-wise representation\r\n",
      "learning on various datasets: (a) Fine-grained frame retrieval on\r\n",
      "FineGym [37]. (b) Phase boundary detection on Pouring [36]. (c)\r\n",
      "Temporal video alignment on PennAction [50]. As shown in the\r\n",
      "figures, the representations obtained through our method (CARL)\r\n",
      "are invariant to the appearance, viewpoint and background.\r\n",
      "\r\n",
      "1. Introduction\r\n",
      "In the last few years, deep learning for video understanding [1, 9, 17, 33, 39, 41, 44, 48] has achieved great success\r\n",
      "on video classification task [9, 19, 40]. Networks such as\r\n",
      "I3D [9] and SlowFast [17] always take short video clips\r\n",
      "(e.g., 32 frames or 64 frames) as input and extract global\r\n",
      "*Accomplished during Minghao Chen’s internship at MSRA.\r\n",
      "† Corresponding author.\r\n",
      "\r\n",
      "representations to predict the action category. In contrast,\r\n",
      "many practical applications, e.g., sign language translation [4, 5, 13], robotic imitation learning [29, 36], action\r\n",
      "alignment [6, 21, 23] and phase classification [16, 27, 37, 50]\r\n",
      "require algorithms having ability to model long videos with\r\n",
      "hundreds of frames and extract frame-wise representations\r\n",
      "rather than the global features (Fig. 1).\r\n",
      "Previous methods [27, 35, 37] have made an effort to\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "learn frame-wise representations via supervised learning,\r\n",
      "where sub-actions or phase boundaries are annotated. However, it is time-consuming and even impractical to manually\r\n",
      "label each frame and exact action boundaries [21] on largescale datasets, which hinders the generalization of models trained with fully supervised learning in realistic scenarios. To reduce the dependency of labeled data, some\r\n",
      "methods such as TCC [16], LAV [23] and GTA [21] explored weakly-supervised learning by using either cycleconsistency loss [16] or soft dynamic time warping [21,23].\r\n",
      "All these methods rely on video-level annotations and the\r\n",
      "training is conducted on the paired videos describing the\r\n",
      "same action. This setting obstructs them from applying on\r\n",
      "more generic video datasets where no labels are available.\r\n",
      "The goal of this work is to learn frame-wise representations with spatio-temporal context information for long\r\n",
      "videos in a self-supervised manner. Inspired by the recent\r\n",
      "progress of contrastive representation learning [8, 11, 12,\r\n",
      "20], we present a novel framework named contrastive action\r\n",
      "representation learning (CARL) to achieve our goal. We assume no labels are available during training, and videos in\r\n",
      "both training and testing sets have long durations (hundreds\r\n",
      "of frames). Moreover, we do not rely on video pairs of the\r\n",
      "same action for training. Thus it is practical to scale up our\r\n",
      "training set with less cost.\r\n",
      "Modeling long videos with hundreds of frames is challenging. It is non-trivial to directly use off-the-shelf backbones designed for short video clip classification, since our\r\n",
      "task is to extract frame-wise representations for long videos.\r\n",
      "In our work, we present a simple yet efficient video encoder\r\n",
      "that consists of a 2D network to encode spatial information\r\n",
      "per frame and a Transformer [42] encoder to model temporal interaction. The frame-wise features are then used for\r\n",
      "representation learning.\r\n",
      "Recently, SimCLR [11] uses instance discrimination [46] as the pretext task and introduces a contrastive\r\n",
      "loss named NT-Xent, which maximizes the agreement between two augmented views of the same data. In their implementation, all instances other than the positive reference\r\n",
      "are considered as negatives. Unlike image data, videos provide more abundant instances (each frame is regarded as\r\n",
      "an instance), and the neighboring frames have high semantic similarities. Directly regarding these frames as negatives may hurt the learning. To avoid this issue, we present\r\n",
      "a novel sequence contrastive loss (SCL), which optimizes\r\n",
      "the embedding space by minimizing the KL-divergence\r\n",
      "between the sequence similarity of two augmented video\r\n",
      "views and a prior Gaussian distribution.\r\n",
      "The main contributions of this paper are summarized as\r\n",
      "follows:\r\n",
      "• We propose a novel framework named contrastive action representation learning (CARL) to learn framewise action representations with spatio-temporal con-\r\n",
      "\r\n",
      "text information for long videos in a self-supervised\r\n",
      "manner. Our method does not rely on any data annotations and has no assumptions on datasets.\r\n",
      "• We introduce a Transformer-based network to efficiently encode long videos and a novel sequence contrastive loss (SCL) for representation learning. Meanwhile, a series of spatio-temporal data augmentations\r\n",
      "are designed to increase the variety of training data.\r\n",
      "• Our framework outperforms the state-of-the-art methods by a large margin on multiple tasks across different datasets. For example, under the linear evaluation protocol on FineGym [37] dataset, our framework\r\n",
      "achieves 41.75% accuracy, which is +13.94% higher\r\n",
      "than the existing best method GTA [21]. On PennAction [50] dataset, our method achieves 91.67% for\r\n",
      "fine-grained classification, 99.1% for Kendall’s Tau,\r\n",
      "and 90.58% top-5 accuracy for fine-grained frame retrieval, which all surpass the existing best methods.\r\n",
      "\r\n",
      "2. Related Works\r\n",
      "Conventional Action Recognition. Various challenging\r\n",
      "video datasets [9, 25, 32, 38, 40] have been constructed to\r\n",
      "reason deeply about diverse scenes and situations. These\r\n",
      "datasets provide labels of high-level concepts or detailed\r\n",
      "physical aspects for short videos or trimmed clips. To tackle\r\n",
      "video recognition, large amounts of architectures have been\r\n",
      "proposed [1, 3, 9, 17, 33, 39, 41, 43, 44]. Most networks are\r\n",
      "based on 3D Convolution layers and combined with the\r\n",
      "techniques in image recognition [9, 17, 41], e.g., residual\r\n",
      "connections [24] and ImageNet pre-training [14]. Some\r\n",
      "works [33, 44] find that 3D ConvNets have insufficient receptive fields and become the bottleneck of the computational budget.\r\n",
      "Recently, Transformers [42] achieved great success in\r\n",
      "the field of computer vision, e.g., ViT [15] and DETR [7].\r\n",
      "There are also several works that extend Transformers to\r\n",
      "video recognition, such as TimeSformer [3] and ViViT [1].\r\n",
      "Due to the strong capacity of Transformers and the global\r\n",
      "receptive field, these methods have become new stateof-the-art. Combining 2D backbones and Transformers,\r\n",
      "VTN [33] can efficiently process long video sequences.\r\n",
      "However, these architecture are all designed for video classification and predict one global class for a video.\r\n",
      "Fine-grained Action Recognition. There are also some\r\n",
      "datasets [27, 35, 37, 50] that investigate fine-grained action\r\n",
      "recognition. They decompose an action into some action\r\n",
      "units, sub-actions, or phases. As a result, each video contains multiple simple stages, e.g., wash the cucumber, peel\r\n",
      "the cucumber, place the cucumber, take a knife, and make\r\n",
      "a slice in preparing cucumber [35]. However, these finelevel labels are more expensive to collect, resulting in a\r\n",
      "limited size of these datasets. GTA [21] argues that these\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "View 1\r\n",
      "\r\n",
      "Frame-wise Representations for View 1\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "Random Sampling\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "Long Video Sequence\r\n",
      "\r\n",
      "Temporal\r\n",
      "Random\r\n",
      "Crop\r\n",
      "\r\n",
      "Spatial\r\n",
      "Aug\r\n",
      "\r\n",
      "Random Sampling\r\n",
      "\r\n",
      "Common\r\n",
      "Frames\r\n",
      "\r\n",
      "FVE\r\n",
      "Projection\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "Sequence\r\n",
      "Contrastive\r\n",
      "Loss\r\n",
      "\r\n",
      "Frame-wise Representations for View 2\r\n",
      "\r\n",
      "View 2\r\n",
      "\r\n",
      "Data Preprocessing\r\n",
      "\r\n",
      "Representation Learning\r\n",
      "\r\n",
      "Figure 2. Overview of our framework (CARL). Two augmented views are constructed from a training video through a series of spatiotemporal data augmentations. The frame-level video encoder (FVE) and the projection head are optimized by minimizing the proposed\r\n",
      "sequence contrastive loss (SCL) between two views.\r\n",
      "\r\n",
      "boundary of manual annotations are subjective. Therefore,\r\n",
      "self-supervised learning for fine-level representations is a\r\n",
      "promising direction.\r\n",
      "Self-supervised Learning in Videos. Previous methods of\r\n",
      "self-supervised learning in videos construct pretext tasks,\r\n",
      "including inferring the future [22], discriminating shuffled\r\n",
      "frames [31] and predicting speed [2]. There are also some\r\n",
      "alignment-based methods, where a pair of videos are trained\r\n",
      "with cycle-consistent loss [16] or soft dynamic time warping (DTW) [10, 21, 23]. Recently, the contrastive learning\r\n",
      "methods [11, 12, 20, 45] based on instance discrimination\r\n",
      "have shown superior performance on 2D image tasks. Some\r\n",
      "works [18, 26, 34, 36, 49] also use this contrastive loss for\r\n",
      "video representation learning. They treat different frames\r\n",
      "in a video [26, 36, 49] or different clips [18, 34] in other\r\n",
      "videos as negative samples. Different from these methods,\r\n",
      "our goal is fine-grained temporal understanding of videos\r\n",
      "and we treat a long sequence of frames as input data. The\r\n",
      "most relevant work to ours is [28], which utilizes 3D human keypoints for self-supervised acton discovery in long\r\n",
      "kinematic videos.\r\n",
      "\r\n",
      "is named data preprocessing. Then we feed two augmented\r\n",
      "views into our frame-level video encoder (FVE) to extract\r\n",
      "dense representations. Following SimCLR [11], FVE is appended with a small projection network which is a twolayer MLP for obtaining latent embeddings. Due to the\r\n",
      "fact that temporally adjacent frames are highly correlated,\r\n",
      "we assume that the similarity distribution between two augmented views should follow a prior Gaussian distribution.\r\n",
      "Based on the assumption, we propose a novel sequence contrastive loss (SCL) to optimize frame-wise representations\r\n",
      "in the embedding space.\r\n",
      "\r\n",
      "3. Method\r\n",
      "\r\n",
      "Concretely, for a training video V with S frames, we\r\n",
      "aim to construct two augmented videos with T frames independently through a series spatio-temporal data augmentations. For temporal data augmentation, we first perform\r\n",
      "temporal random crop on V to generate two randomly\r\n",
      "cropped clips with the length of [T, αT ] frames, where α\r\n",
      "is a hyper-parameter controlling maximum crop size. During this process, we guarantee at least β percent of overlapped frames existing between two clips. Then we randomly sample T frames for each video sequence, and obtain V 1 = {v 1i | 1 ≤ i ≤ T }, and V 2 = {v 2i | 1 ≤ i ≤ T },\r\n",
      "where v 1i and v 2i represent i-th frame from V 1 and V 2 , respectively. We set T = 240 by default. For the videos with\r\n",
      "less than T frames, empty frames are padded before cropping. Finally, we apply several temporal-consistent spatial\r\n",
      "data augmentations, including random resize and crop, horizontal flip, random color distortions, and random Gaussian\r\n",
      "blur, on V 1 and V 2 independently.\r\n",
      "\r\n",
      "In this section, we introduce a novel framework named\r\n",
      "contrastive action representation learning (CARL) to learn\r\n",
      "frame-wise action representations in a self-supervised manner. In particular, our framework is designed to model\r\n",
      "long video sequences by considering spatio-temporal context. We first present an overview of the proposed framework in Section 3.1. Then we introduce the details of view\r\n",
      "construction and data augmentation in Section 3.2. Next,\r\n",
      "we describe our frame-level video encoder in Section 3.3.\r\n",
      "Finally, the proposed sequence contrastive loss (SCL) and\r\n",
      "its design principles are introduced in Section 3.4.\r\n",
      "\r\n",
      "3.1. Overview\r\n",
      "Figure 2 displays an overview of our framework. We first\r\n",
      "construct two augmented views for an input video through\r\n",
      "a series of spatio-temporal data augmentations. This step\r\n",
      "\r\n",
      "3.2. View Construction\r\n",
      "We first introduce the view construction step of our\r\n",
      "method, as shown in the ‘data preprocessing’ part in Figure 2. Data augmentation is crucial to avoid trivial solutions\r\n",
      "in self-supervised learning [11, 12]. Different from prior\r\n",
      "methods designed for image data which only require spatial augmentations, we introduce a series of spatio-temporal\r\n",
      "data augmentations to further increase the variety of videos.\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Prior Gaussian Distribution of\r\n",
      "Timestamp Distance between 𝑠𝑖1 and S 2\r\n",
      "\r\n",
      "Frame-wise Representations\r\n",
      "\r\n",
      "𝑇 × 128\r\n",
      "\r\n",
      "Timeline of\r\n",
      "the Video\r\n",
      "\r\n",
      "Linear\r\n",
      "\r\n",
      "𝑠𝑖1\r\n",
      "\r\n",
      "𝑇 × 256\r\n",
      "\r\n",
      "Transformer Encoder\r\n",
      "𝑇 × 256\r\n",
      "\r\n",
      "Positional\r\n",
      "Encoding\r\n",
      "\r\n",
      "Sample Frames\r\n",
      "𝑽1\r\n",
      "\r\n",
      "ResNet-50\r\n",
      "\r\n",
      "𝑆2\r\n",
      "\r\n",
      "…\r\n",
      "𝑽2\r\n",
      "\r\n",
      "𝑣𝑖1\r\n",
      "\r\n",
      "Transformation\r\n",
      "𝑇 × 2048\r\n",
      "\r\n",
      "…\r\n",
      "\r\n",
      "FVE + Projection\r\n",
      "𝒁1\r\n",
      "\r\n",
      "𝒁2\r\n",
      "\r\n",
      "𝑧𝑖1\r\n",
      "Cosine Similarity (𝑧𝑖1 , 𝒁2 )\r\n",
      "\r\n",
      "𝑇 × 224 ×224 ×3\r\n",
      "Input Video\r\n",
      "\r\n",
      "Figure 3. Architecture of the proposed frame-level video encoder\r\n",
      "(FVE). The input is a long video with T frames and the outputs\r\n",
      "are frame-wise representations. ResNet-50 is pre-trained on ImageNet. We freeze the first four residual blocks of ResNet-50 and\r\n",
      "only finetune the last block.\r\n",
      "\r\n",
      "3.3. Frame-level Video Encoder\r\n",
      "It is non-trivial to directly apply video classification\r\n",
      "backbones [9, 17, 41] to model long video sequences with\r\n",
      "hundreds of frames due to the huge computational cost.\r\n",
      "TCC [16] presents a video encoder that combines 2D\r\n",
      "ResNet and 3D Convolution to generate frame-wise features. However, stacking too many 3D Convolutional layers\r\n",
      "leads to unaffordable computational costs. As a result, this\r\n",
      "kind of design may have limited receptive fields to capture\r\n",
      "temporal context. Recently, Transformers [42] achieved\r\n",
      "great progress in computer vision [7, 15]. Transformers utilize the attention mechanism to solve sequence-to-sequence\r\n",
      "tasks while handling long-range dependencies with ease. In\r\n",
      "our network implementation, we adopt the Transformer encoder as an alternative to model temporal context.\r\n",
      "Figure 3 shows our frame-level video encoder (FVE). To\r\n",
      "seek the tradeoff between representation performance and\r\n",
      "inference speed, we first use a 2D network, e.g., ResNet50 [24], along temporal dimension to extract spatial features\r\n",
      "for the RGB video sequence of size T × 224 × 224 × 3.\r\n",
      "Then a transformation block that consists of two fully connected layers with batch normalization and ReLU is applied to project the spatial features to the intermediate embeddings of size T × 256. Following common practice,\r\n",
      "we add the sine-cosine positional encoding [42] on top of\r\n",
      "the intermediate embeddings to encode the order information. Next, the encoded embeddings are fed into the 3-layer\r\n",
      "Transformer encoder to model temporal context. At last, a\r\n",
      "linear layer is adopted to obtain the final frame-wise representations H ∈ RT ×128 . We use hi (1 ≤ i ≤ T ) to denote\r\n",
      "the representation of i-th frame.\r\n",
      "\r\n",
      "Sequence Contrastive\r\n",
      "Loss of 𝐿1𝑖\r\n",
      "\r\n",
      "Figure 4. Illustration of the proposed sequence contrastive loss.\r\n",
      "We use the loss computation of v 1i ∈ V 1 as the example. We\r\n",
      "first compute a prior Gaussian distribution of timestamp distance\r\n",
      "(s1i −s21 , · · · , s1i −s2T ). Then the embedding similarity distribution\r\n",
      "between z 1i and Z 2 is calculated. We minimize the KL-divergence\r\n",
      "of two distributions in the embedding space.\r\n",
      "\r\n",
      "The 2D ResNet-50 network is pre-trained on ImageNet [14]. Considering the limited computational budget,\r\n",
      "we freeze the first four residual blocks since they already\r\n",
      "learned favorable low-level visual representations by pretraining. This simple design ensures that our network can\r\n",
      "be trained and tested on videos with more than 500 frames.\r\n",
      "VTN [33] adopt a similar hybrid Transformer-based network to perform video classification. They use the [CLS]\r\n",
      "token to generate a global feature, while our network is designed to extract frame-wise representations by considering\r\n",
      "the spatio-temporal context. In addition, our network explores modeling much more prolonged video sequences.\r\n",
      "\r\n",
      "3.4. Sequence Contrastive Loss\r\n",
      "SimCLR [11] introduces a contrastive loss named NTXent by maximizing agreement between augmented views\r\n",
      "of the same instance.\r\n",
      "Unlike self-supervised learning for images, videos provide abundant sequential information, which is a vital supervisory signal. For typical instance discrimination, all\r\n",
      "instances other than the positive reference are considered\r\n",
      "as negatives. However, the neighboring frames around the\r\n",
      "reference frame are highly correlated. Directly regarding\r\n",
      "these frames as negatives may hurt the learning. Learning\r\n",
      "principles should be carefully designed to avoid this issue.\r\n",
      "To optimize frame-wise representations, we propose a novel\r\n",
      "sequence contrastive loss (SCL) which minimizes the KLdivergence between the embedding similarity of two augmented views and the prior Gaussian distribution, as shown\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "in Figure 4.\r\n",
      "Concretely, following SimCLR, we use a small projection network g(·) which is a two-layer MLP to project\r\n",
      "frame-wise representations H encoded by the proposed\r\n",
      "FVE to the latent embeddings Z = g(H). Let Z 1 =\r\n",
      "{z 1i | 1 ≤ i ≤ T } and Z 2 = {z 2i | 1 ≤ i ≤ T } denote the latent embeddings of V 1 and V 2 , where z 1i and\r\n",
      "z 2i represent the latent embedding of i-th frame in V 1 and\r\n",
      "V 2 respectively. Let S 1 = {s1i | 1 ≤ i ≤ T } denote timestamp vector of V 1 , where s1i is the corresponding raw video\r\n",
      "timestamp of the i-th frame in V 1 (see Figure 4). In the\r\n",
      "same way, we can define S 2 = {s2i | 1 ≤ i ≤ T }.\r\n",
      "Given the i-th reference frame in V 1 and its corresponding latent embedding z 1i , due to the fact that temporally\r\n",
      "adjacent frames are more highly correlated than those faraway ones, we assume the embedding similarity between\r\n",
      "z 1i and Z 2 = {z 2i | 1 ≤ i ≤ T } should follow a prior\r\n",
      "Gaussian distribution of timestamp distance between s1i and\r\n",
      "S 2 = {s2i | 1 ≤ i ≤ T }. This assumption motivates\r\n",
      "us to use KL-divergence to optimize the embedding space.\r\n",
      "Specifically, let sim(u, v) = u> v/kukkvk denote cosine\r\n",
      "x2\r\n",
      "similarity, and G(x) = σ√12π exp(− 2σ\r\n",
      "2 ) denote the Gaussian function, where σ 2 is the variance. We formulate the\r\n",
      "loss of i-th reference frame in V 1 as follows:\r\n",
      "\r\n",
      "PennAction Dataset. Videos in this dataset show humans doing different kinds of sports or exercise. Following TCC [16], we use 13 actions of PennAction dataset. In\r\n",
      "total, there are 1140 videos for training and 966 videos for\r\n",
      "testing. Each action set has 40-134 videos for training and\r\n",
      "42-116 videos for testing. We obtain per-frame labels from\r\n",
      "LAV [23]. The video frames are from 18 to 663.\r\n",
      "FineGym Dataset. FineGym is a recent large-scale finegrained action recognition dataset that requires representation learning methods to distinguish different sub-actions\r\n",
      "within the same video. We chunk the original YouTube\r\n",
      "videos according to the action boundaries so that each\r\n",
      "trimmed video data only describes a single action type\r\n",
      "(Floor Exercise, Balance Beam, Uneven Bars, or VaultWomen). Finally, we obtained 3182 videos for training and\r\n",
      "1442 videos for testing. The video frames vary from 140\r\n",
      "to 5153. FineGym provides two data splits according to the\r\n",
      "category number, namely FineGym99 with 99 sub-action\r\n",
      "classes and FineGym288 with 288 sub-action classes.\r\n",
      "Pouring Dataset. In this dataset, videos record the process of hand pouring water from one object to another. The\r\n",
      "phase labels (5 phase classes) are obtained from TCC [16].\r\n",
      "Following TCC [16], we use 70 videos for training and 14\r\n",
      "videos for testing. The video frames are from 186 to 797.\r\n",
      "\r\n",
      "T\r\n",
      "X\r\n",
      "exp(sim(z 1i , z 2j )/τ )\r\n",
      "=−\r\n",
      "wij log PT\r\n",
      ",\r\n",
      "1\r\n",
      "2\r\n",
      "k=1 exp(sim(z i , z k )/τ )\r\n",
      "j=1\r\n",
      "\r\n",
      "Evaluation Metrics. For each dataset, We first optimize\r\n",
      "our network on the training set, without using any labels,\r\n",
      "and then use the following four metrics to evaluate the\r\n",
      "frame-wise representations:\r\n",
      "\r\n",
      "L1i\r\n",
      "\r\n",
      "G(s1i − s2j )\r\n",
      ",\r\n",
      "wij = PT\r\n",
      "1\r\n",
      "2\r\n",
      "k=1 G(si − sk )\r\n",
      "\r\n",
      "(1)\r\n",
      "\r\n",
      "(2)\r\n",
      "\r\n",
      "where wij is the normalized Gaussian weight and τ is the\r\n",
      "temperature parameter. Then the overall loss for V 1 can be\r\n",
      "computed across all frames:\r\n",
      "T\r\n",
      "1X 1\r\n",
      "L .\r\n",
      "L =\r\n",
      "T i=1 i\r\n",
      "1\r\n",
      "\r\n",
      "(3)\r\n",
      "\r\n",
      "Similarly, we can calculate the loss L2 for V 2 . Our sequence contrastive loss is defined as LSCL = L1 + L2 .\r\n",
      "Noticeably, our loss does not rely on frame-to-frame correspondence between V 1 and V 2 , which supports the diversity of spatial-temporal data augmentation.\r\n",
      "\r\n",
      "• Phase Classification (or Fine-grained Action Classification) [16] is the averaged per-frame classification\r\n",
      "accuracy on testing set. Before testing, we fix the network and train a linear classifier by using per-frame\r\n",
      "labels (phase class or sub-action category) of the training set.\r\n",
      "• Phase Progression [16] measures the representation\r\n",
      "ability to predict the phase progress. We fix the\r\n",
      "network and train a linear regressor to predict the\r\n",
      "phase progression values (timestamp distance between\r\n",
      "a query frame and phase boundaries) for all frames.\r\n",
      "Then it is computed as the average R-squared measure.\r\n",
      "\r\n",
      "4.1. Datasets and Metrics\r\n",
      "\r\n",
      "• Kendall’s Tau [16] is calculated over every pair of testing videos by sampling two frames in the first video\r\n",
      "and retrieving the corresponding nearest frames in the\r\n",
      "second video, and checking whether their orders are\r\n",
      "shuffled. It measures how well-aligned two sequences\r\n",
      "are in time. No more training or finetuning is needed.\r\n",
      "\r\n",
      "We use three video datasets, namely PennAction [50],\r\n",
      "FineGym [37] and Pouring [36] to evaluate the performance\r\n",
      "of our method. We compare our method with sate-of-thearts on all three datasets. Unless otherwise specified, all\r\n",
      "ablation studies on conducted on PennAction dataset.\r\n",
      "\r\n",
      "• Average Precision@K [23] is computed as how many\r\n",
      "frames in the retrieved K frames have the same phase\r\n",
      "labels as the query frame. It measures the fine-grained\r\n",
      "frame retrieval accuracy. K = 5, 10, 15 are evaluated.\r\n",
      "No more training or finetuning is needed.\r\n",
      "\r\n",
      "4. Experiments\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Training Strategy\r\n",
      "\r\n",
      "Annotation\r\n",
      "\r\n",
      "Classification\r\n",
      "\r\n",
      "Progress\r\n",
      "\r\n",
      "τ\r\n",
      "\r\n",
      "TCC [16]\r\n",
      "LAV [23]\r\n",
      "\r\n",
      "Per-action\r\n",
      "\r\n",
      "Weakly\r\n",
      "\r\n",
      "81.35\r\n",
      "84.25\r\n",
      "\r\n",
      "0.664\r\n",
      "0.661\r\n",
      "\r\n",
      "0.701\r\n",
      "0.805\r\n",
      "\r\n",
      "TCC [16]\r\n",
      "LAV [23]\r\n",
      "GTA [21]\r\n",
      "\r\n",
      "Joint\r\n",
      "\r\n",
      "Weakly\r\n",
      "\r\n",
      "74.39\r\n",
      "78.68\r\n",
      "-\r\n",
      "\r\n",
      "0.591\r\n",
      "0.625\r\n",
      "0.789\r\n",
      "\r\n",
      "0.641\r\n",
      "0.684\r\n",
      "0.748\r\n",
      "\r\n",
      "SaL [31]\r\n",
      "TCN [36]\r\n",
      "Ours\r\n",
      "\r\n",
      "Joint\r\n",
      "\r\n",
      "None\r\n",
      "\r\n",
      "68.15\r\n",
      "68.09\r\n",
      "93.07\r\n",
      "\r\n",
      "0.390\r\n",
      "0.383\r\n",
      "0.918\r\n",
      "\r\n",
      "0.474\r\n",
      "0.542\r\n",
      "0.985\r\n",
      "\r\n",
      "Method\r\n",
      "\r\n",
      "Table 1. Comparison with state-of-the-art methods on PennAction, using various evaluation metrics: Phase Classification (Classification),\r\n",
      "Phase Progression (Progress) and Kendall’s Tau (τ ). The top row results are from per-action models, i.e., separate models are trained for\r\n",
      "different actions. The results in middle and bottom row are obtained from training a single model for all actions.\r\n",
      "\r\n",
      "Following [16, 23, 36], Phase Classification, Phase\r\n",
      "Progression and Kendall’s Tau are evaluated on Pouring\r\n",
      "dataset. For PennAction, all four metrics are evaluated\r\n",
      "within each action category, and the final results are averaged across the 13 action categories. Following [21], we use\r\n",
      "Fine-grained Action Classification to evaluate our method\r\n",
      "on FineGym dataset.\r\n",
      "\r\n",
      "4.2. Implementation Details\r\n",
      "In our network, we adopt ResNet-50 [24] pre-trained by\r\n",
      "BYOL [20] as frame-wise spatial encoder. Unless otherwise specified, we use a 3-layer Transformer encoder [42]\r\n",
      "with 256 hidden size and 8 heads to model temporal context. We train the model using Adam optimizer with learning rate 10−4 and weight decay 10−5 . We decay the learning rate with cosine decay schedule without restarts [30]. In\r\n",
      "our loss, we set σ 2 = 10 and τ = 0.1 as default. Following SimCLR [11], random image cropping, horizontal flipping, random color distortions, and random Gaussian blur\r\n",
      "are employed as the spatial augmentations. For our temporal data augmentations described in Section 3.2, we set\r\n",
      "hyper-parameters α = 1.5 and β = 20%. The video batch\r\n",
      "size is set as 4 (8 views), and our model is trained on 4\r\n",
      "Nvidia V100 GPUs for 300 epochs. During training, we\r\n",
      "sample T = 240 frames for Pouring and FineGym, T = 80\r\n",
      "frames for PennAction. During testing, we feed the whole\r\n",
      "video into the model at once, without any temporal downsampling. We L2-normalize the frame-wise representations\r\n",
      "for evaluation.\r\n",
      "\r\n",
      "4.3. Main Results\r\n",
      "Results on PennAction Dataset. In Table 1, our method\r\n",
      "is compared with state-of-the-art methods on PennAction.\r\n",
      "TCC [16] and LAV [23] train a separate model for each action (‘Per-action’ in the table), which results in 13 expert\r\n",
      "models for 13 action classes correspondingly. In contrast,\r\n",
      "we train only one model for all 13 action classes (‘Joint’\r\n",
      "in the table). Noticeably, our approach not only outper-\r\n",
      "\r\n",
      "Method\r\n",
      "\r\n",
      "AP@5\r\n",
      "\r\n",
      "AP@10\r\n",
      "\r\n",
      "AP@15\r\n",
      "\r\n",
      "TCN [36]\r\n",
      "TCC [16]\r\n",
      "LAV [23]\r\n",
      "\r\n",
      "77.84\r\n",
      "76.74\r\n",
      "79.13\r\n",
      "\r\n",
      "77.51\r\n",
      "76.27\r\n",
      "78.98\r\n",
      "\r\n",
      "77.28\r\n",
      "75.88\r\n",
      "78.90\r\n",
      "\r\n",
      "Ours\r\n",
      "\r\n",
      "92.28\r\n",
      "\r\n",
      "92.10\r\n",
      "\r\n",
      "91.82\r\n",
      "\r\n",
      "Table 2. Fine-grained frame retrieval results on PennAction.\r\n",
      "\r\n",
      "Method\r\n",
      "\r\n",
      "FineGym99\r\n",
      "\r\n",
      "FineGym288\r\n",
      "\r\n",
      "3\r\n",
      "\r\n",
      "D TW [10]\r\n",
      "SpeedNet [2]\r\n",
      "TCN [36]\r\n",
      "SaL [31]\r\n",
      "TCC [16]\r\n",
      "GTA [21]\r\n",
      "\r\n",
      "15.28\r\n",
      "16.86\r\n",
      "20.02\r\n",
      "21.45\r\n",
      "25.18\r\n",
      "27.81\r\n",
      "\r\n",
      "14.07\r\n",
      "15.57\r\n",
      "17.11\r\n",
      "19.58\r\n",
      "20.82\r\n",
      "24.16\r\n",
      "\r\n",
      "Ours\r\n",
      "\r\n",
      "41.75\r\n",
      "\r\n",
      "35.23\r\n",
      "\r\n",
      "Table 3. Comparison with state-of-the-art methods on FineGym,\r\n",
      "under the evaluation of Fine-grained Action Classification.\r\n",
      "\r\n",
      "forms the methods using joint training, but also outperforms\r\n",
      "the methods adopting per-action training strategy by a large\r\n",
      "margin under different evaluation metrics. In Table 2, we\r\n",
      "report the results under the Average Precision@K metric,\r\n",
      "which measures the performance of fine-grained frame retrieval. Surprisingly, although our model is not trained on\r\n",
      "paired data, it can successfully find frames with similar semantics from other videos. For all AP@K, our method is at\r\n",
      "least +11% better than previous methods.\r\n",
      "Results on FineGym Dataset. Table 3 summarizes the\r\n",
      "experimental results of Fine-grained Action Classification\r\n",
      "on FineGym99 and FineGym288. Our method outperforms the other self-supervised [2, 31, 36] and weakly\r\n",
      "supervised [10, 16, 21] methods. The performance of\r\n",
      "our method surpasses the previous state-of-the-art method\r\n",
      "GTA [21] by +13.94% on FineGym99 and +11.07% on FineGym288. The weakly supervised methods, i.e., TCC [16],\r\n",
      "D3 TW [10] and GTA [21], assume there exists an optimal\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Classification\r\n",
      "\r\n",
      "Progress\r\n",
      "\r\n",
      "τ\r\n",
      "\r\n",
      "#Layers\r\n",
      "\r\n",
      "Classification\r\n",
      "\r\n",
      "Progress\r\n",
      "\r\n",
      "τ\r\n",
      "\r\n",
      "TCN [36]\r\n",
      "TCC [16]\r\n",
      "LAV [23]\r\n",
      "\r\n",
      "89.53\r\n",
      "91.53\r\n",
      "92.84\r\n",
      "\r\n",
      "0.804\r\n",
      "0.837\r\n",
      "0.805\r\n",
      "\r\n",
      "0.852\r\n",
      "0.864\r\n",
      "0.856\r\n",
      "\r\n",
      "Ours\r\n",
      "\r\n",
      "93.73\r\n",
      "\r\n",
      "0.935\r\n",
      "\r\n",
      "0.992\r\n",
      "\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n",
      "\r\n",
      "92.15\r\n",
      "92.61\r\n",
      "93.07\r\n",
      "92.81\r\n",
      "\r\n",
      "0.909\r\n",
      "0.913\r\n",
      "0.918\r\n",
      "0.910\r\n",
      "\r\n",
      "0.985\r\n",
      "0.990\r\n",
      "0.985\r\n",
      "0.990\r\n",
      "\r\n",
      "Method\r\n",
      "\r\n",
      "Table 4. Comparison with state-of-the-art methods on Pouring.\r\n",
      "\r\n",
      "Classification\r\n",
      "\r\n",
      "Progress\r\n",
      "\r\n",
      "τ\r\n",
      "\r\n",
      "ResNet-50 only\r\n",
      "ResNet-50+C3D\r\n",
      "\r\n",
      "68.63\r\n",
      "83.96\r\n",
      "\r\n",
      "0.296\r\n",
      "0.705\r\n",
      "\r\n",
      "0.440\r\n",
      "0.778\r\n",
      "\r\n",
      "ResNet-50+\r\n",
      "Transformer\r\n",
      "\r\n",
      "93.07\r\n",
      "\r\n",
      "0.918\r\n",
      "\r\n",
      "0.985\r\n",
      "\r\n",
      "Architecture\r\n",
      "\r\n",
      "Table 6. Study on the effects of using different number of layers\r\n",
      "in Transformer encoder.\r\n",
      "\r\n",
      "Learnable Blocks\r\n",
      "None\r\n",
      "Block5\r\n",
      "Block4+Block5\r\n",
      "\r\n",
      "Classification\r\n",
      "\r\n",
      "Progress\r\n",
      "\r\n",
      "τ\r\n",
      "\r\n",
      "90.63\r\n",
      "93.07\r\n",
      "92.98\r\n",
      "\r\n",
      "0.907\r\n",
      "0.918\r\n",
      "0.919\r\n",
      "\r\n",
      "0.994\r\n",
      "0.985\r\n",
      "0.989\r\n",
      "\r\n",
      "Table 7. Ablation study on learnable blocks of ResNet-50.\r\n",
      "\r\n",
      "Table 5. Ablation study on different architectures.\r\n",
      "\r\n",
      "alignment between two videos from the training set. However, for FineGym dataset, even in two videos describing\r\n",
      "the same action, the set and order of sub-actions may differ.\r\n",
      "Therefore, the alignment found by these methods can be incorrect, which impedes learning. The great improvement\r\n",
      "verifies the effectiveness of our framework.\r\n",
      "Results on Pouring Dataset. As shown in Table 4, our\r\n",
      "method also achieves the best performance on a relatively\r\n",
      "small dataset, Pouring. These results further demonstrate\r\n",
      "the great generalization ability of our approach.\r\n",
      "Visualization Results. We present the visualization of finegrained frame retrieval and video alignment in Section A.\r\n",
      "\r\n",
      "4.4. Ablation Study\r\n",
      "In this section, we perform multiple experiments to analyze the different components of our framework. Unless\r\n",
      "otherwise specified, experiments are conducted on the PennAction dataset.\r\n",
      "Network Architecture. In Table 5, we investigate the network architecture. ‘ResNet-50+Transformer’ denotes our\r\n",
      "default frame-level video encoder introduced in Section 3.3.\r\n",
      "‘ResNet-50 only’ means we remove the Transformer encoder in our network, and only use 2D ResNet-50 and\r\n",
      "linear transformation layers to extract representations per\r\n",
      "frame. ‘ResNet-50+C3D’ represents that two 3D convolutional layers [41] are added on top of the ResNet-50 before the spatial pooling, which is the same as the model\r\n",
      "adopted in TCC [16] and LAV [23]. These models are all\r\n",
      "trained with the proposed sequence contrastive loss. Our\r\n",
      "default network outperforms the other two networks, which\r\n",
      "attributes to the long-range dependency modeling ability of\r\n",
      "Transformers.\r\n",
      "Layer Number of Transformer Encoder. Table 6 shows\r\n",
      "studies using different numbers of layers in Transformers.\r\n",
      "We find that Phase Classification increases with more lay-\r\n",
      "\r\n",
      "Classification\r\n",
      "\r\n",
      "Progress\r\n",
      "\r\n",
      "τ\r\n",
      "\r\n",
      "TCN†\r\n",
      "TCC†\r\n",
      "\r\n",
      "86.31\r\n",
      "86.35\r\n",
      "\r\n",
      "0.898\r\n",
      "0.899\r\n",
      "\r\n",
      "0.832\r\n",
      "0.980\r\n",
      "\r\n",
      "Ours\r\n",
      "\r\n",
      "93.07\r\n",
      "\r\n",
      "0.918\r\n",
      "\r\n",
      "0.985\r\n",
      "\r\n",
      "Method\r\n",
      "\r\n",
      "Table 8. Applying our network to TCN and TCC. † denotes we reimplement the method and replace the network with ours. “Contrastive baseline” uses the corresponding frame at the other view\r\n",
      "as the positive sample.\r\n",
      "\r\n",
      "ers. However, Phase Progression slightly drops when there\r\n",
      "are too many layers. We use 3 layers by default.\r\n",
      "Training Different Blocks of ResNet. In our implementation, ResNet-50 is pre-trained on ImageNet. In Table 7, we\r\n",
      "study the effects of finetuning different blocks of ResNet50. The standard ResNet contains 5 blocks, namely Block1Block5. ‘None’ denotes that all layers of ResNet are frozen.\r\n",
      "‘Block5’ denotes we freeze the first four residual blocks\r\n",
      "of ResNet and only make the last residual block learnable,\r\n",
      "which is our default setting. Similarly, ‘Block4+Block5’\r\n",
      "means we freeze the first three blocks and only train the last\r\n",
      "two blocks. Table 7 shows that encoding dataset-related\r\n",
      "spatial information is important (‘None’ vs. ‘Block5’),\r\n",
      "and training more blocks does not lead to improvement\r\n",
      "(‘Block5’ vs. ‘Block4+Block5’).\r\n",
      "Applying Our Network to Other Methods. We study\r\n",
      "whether our frame-level video encoder (FVE) introduced\r\n",
      "in Section 3.3 can boost the performances of TCC [16] and\r\n",
      "TCN [36]. We replace the C3D-based network with ours.\r\n",
      "Table 8 shows the results. We find that the proposed network can dramatically improve the performance of their\r\n",
      "methods (compared with the results in Table 3). In addition, our method still keeps a large performance gain, which\r\n",
      "attributes to the proposed sequence contrastive loss.\r\n",
      "Hyper-parameters of Sequence Contrastive Loss. We\r\n",
      "study the hyper-parameters, i.e., temperature parameter τ\r\n",
      "and Gaussian variance σ 2 in our sequence contrastive loss\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Hyper-parameters\r\n",
      "\r\n",
      "Classification\r\n",
      "\r\n",
      "Progress\r\n",
      "\r\n",
      "τ\r\n",
      "\r\n",
      "α\r\n",
      "\r\n",
      "Sampling\r\n",
      "\r\n",
      "β (%)\r\n",
      "\r\n",
      "FineGym99\r\n",
      "\r\n",
      "τ =0.1, σ 2 =1\r\n",
      "τ =0.1, σ 2 =25\r\n",
      "\r\n",
      "92.95\r\n",
      "92.03\r\n",
      "\r\n",
      "0.903\r\n",
      "0.922\r\n",
      "\r\n",
      "0.963\r\n",
      "0.993\r\n",
      "\r\n",
      "Random\r\n",
      "\r\n",
      "20\r\n",
      "\r\n",
      "τ =1.0, σ 2 =10\r\n",
      "τ =0.3, σ 2 =10\r\n",
      "τ =0.1, σ 2 =10\r\n",
      "\r\n",
      "91.57\r\n",
      "92.13\r\n",
      "93.07\r\n",
      "\r\n",
      "0.889\r\n",
      "0.903\r\n",
      "0.918\r\n",
      "\r\n",
      "0.993\r\n",
      "0.992\r\n",
      "0.985\r\n",
      "\r\n",
      "0\r\n",
      "1.5\r\n",
      "1\r\n",
      "\r\n",
      "36.72\r\n",
      "41.75\r\n",
      "39.03\r\n",
      "\r\n",
      "1.5\r\n",
      "\r\n",
      "Even\r\n",
      "\r\n",
      "20\r\n",
      "\r\n",
      "38.44\r\n",
      "\r\n",
      "Random\r\n",
      "\r\n",
      "0\r\n",
      "20\r\n",
      "50\r\n",
      "80\r\n",
      "100\r\n",
      "\r\n",
      "38.15\r\n",
      "41.75\r\n",
      "39.14\r\n",
      "37.94\r\n",
      "35.53\r\n",
      "\r\n",
      "Table 9. Ablation study on Gaussian variance σ 2 and the temperature τ in sequence contrastive loss.\r\n",
      "\r\n",
      "(see Eq. 2). The variance σ 2 of the prior Gaussian distribution controls how the adjacent frames are semantically similar to the reference frame, on the assumption. As Table 9\r\n",
      "shows, too small variance (σ 2 = 1) or too large variance\r\n",
      "(σ 2 = 25) degrades the performance. We use σ 2 = 10 by\r\n",
      "default. In addition, we observe an appropriate temperature\r\n",
      "(τ = 0.1) facilitates the learning from hard negatives, which\r\n",
      "is consistent with the conclusion in SimCLR [11].\r\n",
      "Study on Different Temporal Data Augmentations. We\r\n",
      "study the different temporal data augmentations described\r\n",
      "in Section 3.2, including maximum crop size α, overlap\r\n",
      "ratio β between views, and different sampling strategies,\r\n",
      "namely random sampling and even sampling. Table 10\r\n",
      "shows the results. From the table, we can see that the performance drops dramatically when we crop the video with a\r\n",
      "fixed length (α = 1). The performance also decreases when\r\n",
      "we perform even sampling on the cropped clips. As described in Section 3.4, our sequence contrastive loss does\r\n",
      "not rely on frame-to-frame correspondence between two\r\n",
      "augmented views. Experimentally, constructing two views\r\n",
      "with β = 100% percent of overlapped frames degrades the\r\n",
      "performance, since the variety of augmented data decreases.\r\n",
      "In addition, we also observe the performance drops when\r\n",
      "two views are constructed independently (β = 0% ). The\r\n",
      "reason is that in this setting, the training may bring the representations of temporally distant frames closer, which hinders the optimization.\r\n",
      "Number of Training Frames and Linear Evaluation Under Different Data Protocols. As described in Section 3.2,\r\n",
      "our network takes augmented views with T frames as input. We study the effects of different frame numbers T on\r\n",
      "FineGym99. Table 11 shows the results. We observe that\r\n",
      "taking long sequences as input is essential for frame-wise\r\n",
      "representation learning. However, a too large frame number degrades the performance. We thus set T = 240 by default. We also conduct linear evaluation under different data\r\n",
      "protocols. Concretely, we use 10%, 50% and 100% labeled\r\n",
      "data to train the linear classifier. Compared with the supervised model (all layers are learnable), our method achieves\r\n",
      "better performance when the labeled data is limited (10%\r\n",
      "data protocol).\r\n",
      "\r\n",
      "1.5\r\n",
      "\r\n",
      "Table 10. Ablation study on hyper-parameters of temporal data\r\n",
      "augmentations. Effects of maximum crop size α, overlap ratio β\r\n",
      "and random sampling strategy are studied. The experiments are\r\n",
      "conducted on FineGym99 dataset.\r\n",
      "\r\n",
      "% of Labeled Data →\r\n",
      "\r\n",
      "10\r\n",
      "\r\n",
      "Number of training frames:\r\n",
      "80\r\n",
      "27.10\r\n",
      "160\r\n",
      "30.28\r\n",
      "33.53\r\n",
      "240\r\n",
      "480\r\n",
      "31.46\r\n",
      "Supervised\r\n",
      "\r\n",
      "24.51\r\n",
      "\r\n",
      "50\r\n",
      "\r\n",
      "100\r\n",
      "\r\n",
      "32.78\r\n",
      "36.46\r\n",
      "39.89\r\n",
      "37.92\r\n",
      "\r\n",
      "34.02\r\n",
      "38.06\r\n",
      "41.75\r\n",
      "39.45\r\n",
      "\r\n",
      "48.75\r\n",
      "\r\n",
      "60.37\r\n",
      "\r\n",
      "Table 11. Ablation studies on number of training frames under\r\n",
      "different data protocols. Study is conducted on FineGym99 Finegrained Action Classification task. ‘Supervised’ means all layers\r\n",
      "are trained with supervised learning.\r\n",
      "\r\n",
      "5. Conclusion\r\n",
      "In this paper, we present a novel framework named\r\n",
      "contrastive action representation learning (CARL) to learn\r\n",
      "frame-wise action representations, especially for long\r\n",
      "videos, in a self-supervised manner. To model long videos\r\n",
      "with hundreds of frames, we introduce a simple yet efficient\r\n",
      "network named frame-level video encoder (FVE), which\r\n",
      "considers spatio-temporal context during training. In addition, we propose a novel sequence contrastive loss (SCL)\r\n",
      "for frame-wise representation learning. SCL optimizes the\r\n",
      "embedding space by minimizing the KL-divergence between the sequence similarity of two augmented views\r\n",
      "and a prior Gaussian distribution. Experiments on various\r\n",
      "datasets and tasks show effectiveness and generalization of\r\n",
      "our method.\r\n",
      "\r\n",
      "Acknowledgments\r\n",
      "This work was supported in part by The National\r\n",
      "Key Research and Development Program of China (Grant\r\n",
      "Nos: 2018AAA0101400), in part by The National Nature Science Foundation of China (Grant Nos: 62036009,\r\n",
      "61936006), in part by Innovation Capability Support Program of Shaanxi (Program No. 2021TD-05).\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Figure 5. Visualization of video alignment on FineGym dataset.\r\n",
      "Please refer to video demos in our supplementary materials for\r\n",
      "more visualization results.\r\n",
      "\r\n",
      "Query\r\n",
      "\r\n",
      "Top-5 Retrieved Frames\r\n",
      "\r\n",
      "Figure 7. Visualization of fine-grained frame retrieval on FineGym\r\n",
      "datast by using our method.\r\n",
      "\r\n",
      "give video demos in our supplementary materials.\r\n",
      "(a) Pouring dataset.\r\n",
      "\r\n",
      "(b) PennAction dataset.\r\n",
      "\r\n",
      "Figure 6. We randomly select two videos recording the same process (or action) from Pouring (or PennAction) dataset and compute the similarity matrix for frame-wise representations extracted\r\n",
      "by our method. The similarities are normalized for better visualization.\r\n",
      "\r\n",
      "A. More Results\r\n",
      "In this section, we show visualization results of video\r\n",
      "alignment and fine-grained frame retrieval.\r\n",
      "\r\n",
      "A.1. Video Alignment\r\n",
      "Given two videos recording the similar action or process,\r\n",
      "the goal of video alignment is to find the temporal correspondence between them. Firstly, we use our framework\r\n",
      "to extract the frame-wise representations for two randomly\r\n",
      "selected videos. Then we compute the cosine similarities\r\n",
      "between the frame-wise representations of two videos and\r\n",
      "utilize the famous dynamic time warping (DTW) algorithm\r\n",
      "on the similarity matrix to find the best temporal alignment.\r\n",
      "Figure 5 shows an example from FineGym test set. Please\r\n",
      "refer to video demos in our supplementary materials for\r\n",
      "more visualization results.\r\n",
      "We also randomly select two videos recording the same\r\n",
      "process (or action) from Pouring (or PennAction) dataset,\r\n",
      "and similarly, we can compute the similarity matrix which is\r\n",
      "rendered as a heatmap in Figure 6. We observe that the diagonal is highlighted, which means our approach find the favorable alignment between two correlated videos. We also\r\n",
      "\r\n",
      "A.2. Fine-grained Frame Retrieval\r\n",
      "In Figure 7, we present the visualization results of finegrained frame retrieval on FineGym dataset. To be specific, we feed the video containing the query frames into\r\n",
      "our CARL framework to generate query features, and similarly, we can extract frame-wise features for the rest videos\r\n",
      "in the test set. We simply compute the cosine similarity between query features and frame-wise features from candidate videos to obtain top-5 retrieved frames as shown in Figure 7. The retrieved frames have similar semantics with the\r\n",
      "query frame, though the appearances, the camera views, and\r\n",
      "the backgrounds are different, which suggests our method is\r\n",
      "robust to these factors.\r\n",
      "\r\n",
      "A.3. Action Localization\r\n",
      "To show the potential of our method on large datasets and\r\n",
      "more downstream tasks, we optimize the frame-wise features via our self-supervised method on ActivityNet [25].\r\n",
      "Then we use G-TAD [47] on the top of the features (without fine-tuning) to perform temporal action localization. As\r\n",
      "shown in Table 12, we use mAP(%) at {0.5, 0.75, 0.95}\r\n",
      "tIoU thresholds and the average mAP across 10 tIoU levels for evaluation. In contrast to the supervised two-stream\r\n",
      "model [41], our method does not need any video labels\r\n",
      "while achieving better performance.\r\n",
      "\r\n",
      "A.4. Compare with Contrastive Baseline\r\n",
      "We compare our SCL with the contrastive baseline which\r\n",
      "only uses the corresponding frame in the other view as\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "Method\r\n",
      "G-TAD w. 2stream\r\n",
      "G-TAD w. ours\r\n",
      "\r\n",
      "0.5\r\n",
      "\r\n",
      "0.75\r\n",
      "\r\n",
      "0.95\r\n",
      "\r\n",
      "Average\r\n",
      "\r\n",
      "50.36\r\n",
      "51.22\r\n",
      "\r\n",
      "34.60\r\n",
      "35.19\r\n",
      "\r\n",
      "9.02\r\n",
      "8.54\r\n",
      "\r\n",
      "34.09\r\n",
      "34.46\r\n",
      "\r\n",
      "[6]\r\n",
      "\r\n",
      "Table 12. Temporal action localization on ActivityNet v1.3.\r\n",
      "\r\n",
      "Method\r\n",
      "Contrastive baseline\r\n",
      "SCL (ours)\r\n",
      "\r\n",
      "Classification\r\n",
      "\r\n",
      "Progress\r\n",
      "\r\n",
      "τ\r\n",
      "\r\n",
      "88.05\r\n",
      "93.07\r\n",
      "\r\n",
      "0.898\r\n",
      "0.918\r\n",
      "\r\n",
      "0.891\r\n",
      "0.985\r\n",
      "\r\n",
      "Table 13. Compare our SCL with contrastive baseline, which uses\r\n",
      "the corresponding frame in the other view as the positive sample.\r\n",
      "\r\n",
      "Training Dataset\r\n",
      "K400\r\n",
      "K400 → PennAction\r\n",
      "\r\n",
      "Classification\r\n",
      "\r\n",
      "Progress\r\n",
      "\r\n",
      "τ\r\n",
      "\r\n",
      "91.9\r\n",
      "93.9\r\n",
      "\r\n",
      "0.903\r\n",
      "0.908\r\n",
      "\r\n",
      "0.949\r\n",
      "0.977\r\n",
      "\r\n",
      "[7]\r\n",
      "\r\n",
      "[8]\r\n",
      "\r\n",
      "[9]\r\n",
      "\r\n",
      "[10]\r\n",
      "\r\n",
      "[11]\r\n",
      "Table 14. Our CARL pre-trained on Kinetics-400 shows outstanding transfer ability on PennAction. Fine-tuning the pre-trained\r\n",
      "model on PennAction further boosts the performance.\r\n",
      "[12]\r\n",
      "\r\n",
      "the positive sample and ignores temporal adjacent frames.\r\n",
      "As Table 13 shows, our SCL can more efficiently employ\r\n",
      "the sequential information and thus achieves better performance.\r\n",
      "\r\n",
      "A.5. Kinetics-400 Pre-training\r\n",
      "To show our method can benefit from large-scale datasets\r\n",
      "without any labels, we train our CARL on Kinetics-400 [9].\r\n",
      "As Table 14 shows, the frame-wise representations trained\r\n",
      "on Kinetics-400 shows outstanding generalization on PennAction dataset. Moreover, fine-tuning the pre-trained model\r\n",
      "on PennAction by using our CARL further boosts the performance, e.g., + 2% classification improvement.\r\n",
      "\r\n",
      "[13]\r\n",
      "\r\n",
      "[14]\r\n",
      "\r\n",
      "[15]\r\n",
      "\r\n",
      "[16]\r\n",
      "\r\n",
      "References\r\n",
      "[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\r\n",
      "Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. ArXiv, 2021. 1, 2\r\n",
      "[2] Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri,\r\n",
      "William T. Freeman, Michael Rubinstein, Michal Irani, and\r\n",
      "Tali Dekel. Speednet: Learning the speediness in videos. In\r\n",
      "CVPR, 2020. 3, 6\r\n",
      "[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\r\n",
      "space-time attention all you need for video understanding?\r\n",
      "ArXiv, 2021. 2\r\n",
      "[4] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. Neural sign language translation. In CVPR, 2018. 1\r\n",
      "[5] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and\r\n",
      "Richard Bowden. Sign language transformers: Joint end-\r\n",
      "\r\n",
      "[17]\r\n",
      "\r\n",
      "[18]\r\n",
      "\r\n",
      "[19]\r\n",
      "\r\n",
      "[20]\r\n",
      "\r\n",
      "to-end sign language recognition and translation. In CVPR,\r\n",
      "2020. 1\r\n",
      "Kaidi Cao, Jingwei Ji, Zhangjie Cao, C. Chang, and\r\n",
      "Juan Carlos Niebles. Few-shot video classification via temporal alignment. In CVPR, 2020. 1\r\n",
      "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\r\n",
      "Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. ArXiv, 2020. 2, 4\r\n",
      "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning\r\n",
      "of visual features by contrasting cluster assignments. ArXiv,\r\n",
      "2020. 2\r\n",
      "João Carreira and Andrew Zisserman. Quo vadis, action\r\n",
      "recognition? a new model and the kinetics dataset. In CVPR,\r\n",
      "2017. 1, 2, 4, 10\r\n",
      "C. Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and\r\n",
      "Juan Carlos Niebles. D3tw: Discriminative differentiable\r\n",
      "dynamic time warping for weakly supervised action alignment and segmentation. In CVPR, 2019. 3, 6\r\n",
      "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3, 4, 6,\r\n",
      "8\r\n",
      "Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming\r\n",
      "He. Improved baselines with momentum contrastive learning. ArXiv, 2020. 2, 3\r\n",
      "Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and\r\n",
      "Stephen Lin. A simple multi-modality transfer learning baseline for sign language translation. arXiv preprint\r\n",
      "arXiv:2203.04287, 2022. 1\r\n",
      "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\r\n",
      "and Fei-Fei Li. Imagenet: A large-scale hierarchical image\r\n",
      "database. In CVPR, 2009. 2, 4\r\n",
      "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\r\n",
      "Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\r\n",
      "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\r\n",
      "worth 16x16 words: Transformers for image recognition at\r\n",
      "scale. In ICLR, 2021. 2, 4\r\n",
      "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre\r\n",
      "Sermanet, and Andrew Zisserman.\r\n",
      "Temporal cycleconsistency learning. In CVPR, 2019. 1, 2, 3, 4, 5, 6, 7\r\n",
      "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\r\n",
      "Kaiming He. Slowfast networks for video recognition. In\r\n",
      "ICCV, 2019. 1, 2, 4\r\n",
      "Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross B. Girshick, and Kaiming He. A large-scale study on unsupervised\r\n",
      "spatiotemporal representation learning. In CVPR, 2021. 3\r\n",
      "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\r\n",
      "Valentin Haenel, Ingo Fründ, Peter N. Yianilos, Moritz\r\n",
      "Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax,\r\n",
      "and Roland Memisevic. The “something something” video\r\n",
      "database for learning and evaluating visual common sense.\r\n",
      "In ICCV, 2017. 1\r\n",
      "Jean-Bastien Grill, Florian Strub, Florent Altch’e, Corentin\r\n",
      "Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Do-\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "[21]\r\n",
      "\r\n",
      "[22]\r\n",
      "\r\n",
      "[23]\r\n",
      "\r\n",
      "[24]\r\n",
      "\r\n",
      "[25]\r\n",
      "\r\n",
      "[26]\r\n",
      "\r\n",
      "[27]\r\n",
      "\r\n",
      "[28]\r\n",
      "\r\n",
      "[29]\r\n",
      "\r\n",
      "[30]\r\n",
      "[31]\r\n",
      "\r\n",
      "[32]\r\n",
      "\r\n",
      "[33]\r\n",
      "[34]\r\n",
      "\r\n",
      "[35]\r\n",
      "\r\n",
      "[36]\r\n",
      "\r\n",
      "ersch, Bernardo Ávila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi\r\n",
      "Munos, and Michal Valko. Bootstrap your own latent: A new\r\n",
      "approach to self-supervised learning. ArXiv, 2020. 2, 3, 6\r\n",
      "Isma Hadji, Konstantinos G. Derpanis, and Allan D. Jepson.\r\n",
      "Representation learning via global temporal alignment and\r\n",
      "cycle-consistency. In CVPR, 2021. 1, 2, 3, 6\r\n",
      "Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In ICCVW,\r\n",
      "2019. 3\r\n",
      "Sanjay Haresh, Sateesh Kumar, Huseyin Coskun,\r\n",
      "Shahram Najam Syed, Andrey Konin, M. Zeeshan Zia,\r\n",
      "and Quoc-Huy Tran. Learning by aligning videos in time.\r\n",
      "In CVPR, 2021. 1, 2, 3, 5, 6, 7\r\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\r\n",
      "Deep residual learning for image recognition. In CVPR,\r\n",
      "2016. 2, 4, 6\r\n",
      "Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\r\n",
      "and Juan Carlos Niebles. Activitynet: A large-scale video\r\n",
      "benchmark for human activity understanding. In CVPR,\r\n",
      "2015. 2, 9\r\n",
      "Haofei Kuang, Yi Zhu, Zhi Zhang, Xinyu Li, Joseph Tighe,\r\n",
      "Sören Schwertfeger, C. Stachniss, and Mu Li. Video contrastive learning with global context. ArXiv, 2021. 3\r\n",
      "Hilde Kuehne, Ali Bilgin Arslan, and Thomas Serre. The\r\n",
      "language of actions: Recovering the syntax and semantics of\r\n",
      "goal-directed human activities. In CVPR, 2014. 1, 2\r\n",
      "Kenneth Li, Xiao Sun, Zhirong Wu, Fangyun Wei, and\r\n",
      "Stephen Lin. Towards tokenized human dynamics representation. arXiv preprint arXiv:2111.11433, 2021. 3\r\n",
      "Yuxuan Liu, Abhishek Gupta, P. Abbeel, and Sergey Levine.\r\n",
      "Imitation from observation: Learning to imitate behaviors\r\n",
      "from raw video via context translation. 2018 IEEE International Conference on Robotics and Automation (ICRA),\r\n",
      "2018. 1\r\n",
      "Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\r\n",
      "descent with warm restarts. arXiv: Learning, 2017. 6\r\n",
      "Ishan Misra, C. Lawrence Zitnick, and Martial Hebert. Shuffle and learn: Unsupervised learning using temporal order\r\n",
      "verification. In ECCV, 2016. 3, 6\r\n",
      "Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa M. Brown,\r\n",
      "Quanfu Fan, Dan Gutfreund, Carl Vondrick, and Aude Oliva.\r\n",
      "Moments in time dataset: One million videos for event understanding. PAMI, 2020. 2\r\n",
      "Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. ArXiv, 2021. 1, 2, 4\r\n",
      "Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang,\r\n",
      "H. Wang, Serge J. Belongie, and Yin Cui. Spatiotemporal\r\n",
      "contrastive video representation learning. In CVPR, 2021. 3\r\n",
      "Marcus Rohrbach, Anna Rohrbach, Michaela Regneri,\r\n",
      "Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, and\r\n",
      "Bernt Schiele. Recognizing fine-grained and composite activities using hand-centric features and script data. In IJCV,\r\n",
      "2015. 1, 2\r\n",
      "Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine\r\n",
      "Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Timecontrastive networks: Self-supervised learning from video.\r\n",
      "\r\n",
      "[37]\r\n",
      "\r\n",
      "[38]\r\n",
      "\r\n",
      "[39]\r\n",
      "\r\n",
      "[40]\r\n",
      "\r\n",
      "[41]\r\n",
      "\r\n",
      "[42]\r\n",
      "\r\n",
      "[43]\r\n",
      "\r\n",
      "[44]\r\n",
      "[45]\r\n",
      "\r\n",
      "[46]\r\n",
      "\r\n",
      "[47]\r\n",
      "\r\n",
      "[48]\r\n",
      "\r\n",
      "[49]\r\n",
      "\r\n",
      "[50]\r\n",
      "\r\n",
      "2018 IEEE International Conference on Robotics and Automation (ICRA), 2018. 1, 3, 5, 6, 7\r\n",
      "Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A\r\n",
      "hierarchical video dataset for fine-grained action understanding. In CVPR, 2020. 1, 2, 5\r\n",
      "Gunnar A. Sigurdsson, Gül Varol, X. Wang, Ali Farhadi,\r\n",
      "Ivan Laptev, and Abhinav Gupta. Hollywood in homes:\r\n",
      "Crowdsourcing data collection for activity understanding. In\r\n",
      "ECCV, 2016. 2\r\n",
      "Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In\r\n",
      "NIPS, 2014. 1, 2\r\n",
      "Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\r\n",
      "Ucf101: A dataset of 101 human actions classes from videos\r\n",
      "in the wild. ArXiv, 2012. 1, 2\r\n",
      "Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features\r\n",
      "with 3d convolutional networks. In ICCV, 2015. 1, 2, 4, 7, 9\r\n",
      "Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\r\n",
      "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\r\n",
      "Illia Polosukhin. Attention is all you need. In NIPS, 2017.\r\n",
      "2, 4, 6\r\n",
      "Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\r\n",
      "Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment\r\n",
      "networks for action recognition in videos. PAMI, 2019. 2\r\n",
      "X. Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming\r\n",
      "He. Non-local neural networks. In CVPR, 2018. 1, 2\r\n",
      "Fangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen\r\n",
      "Lin. Aligning pretraining for detection via object-level contrastive learning. Advances in Neural Information Processing Systems, 34, 2021. 3\r\n",
      "Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\r\n",
      "Unsupervised feature learning via non-parametric instance\r\n",
      "discrimination. In CVPR, 2018. 2\r\n",
      "Mengmeng Xu, Chen Zhao, David S. Rojas, Ali Thabet, and\r\n",
      "Bernard Ghanem. G-tad: Sub-graph localization for temporal action detection. In CVPR, 2020. 9\r\n",
      "Yinghao Xu, Fangyun Wei, Xiao Sun, Ceyuan Yang, Yujun Shen, Bo Dai, Bolei Zhou, and Stephen Lin. Crossmodel pseudo-labeling for semi-supervised action recognition. arXiv preprint arXiv:2112.09690, 2021. 1\r\n",
      "Ting Yao, Yiheng Zhang, Zhaofan Qiu, Yingwei Pan, and\r\n",
      "Tao Mei. Seco: Exploring sequence supervision for unsupervised representation learning. In AAAI, 2021. 3\r\n",
      "Weiyu Zhang, Menglong Zhu, and Konstantinos G. Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In ICCV, 2013.\r\n",
      "1, 2, 5\r\n",
      "\r\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "\n",
    "print(array_authors[0])\n",
    "print(array_pdf_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1220700",
   "metadata": {},
   "source": [
    "### Extract all word after the term: \"References\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e6468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frame-wise Action Representations for Long Videos via\\r\\nSequence Contrastive Learning\\r\\n\\r\\narXiv:2203.14957v1 [cs.CV] 28 Mar 2022\\r\\n\\r\\nMinghao Chen1∗ Fangyun Wei2† Chong Li2 Deng Cai1\\r\\n1\\r\\nState Key Lab of CAD&CG, College of Computer Science, Zhejiang University\\r\\n2\\r\\nMicrosoft Research Asia\\r\\nminghaochen01@gmail.com\\r\\n\\r\\n{fawe, chol}@microsoft.com\\r\\n\\r\\ndengcai@cad.zju.edu.cn\\r\\n\\r\\nAbstract\\r\\nPrior works on action representation learning mainly focus on designing various architectures to extract the global\\r\\nrepresentations for short video clips. In contrast, many\\r\\npractical applications such as video alignment have strong\\r\\ndemand for learning dense representations for long videos.\\r\\nIn this paper, we introduce a novel contrastive action representation learning (CARL) framework to learn frame-wise\\r\\naction representations, especially for long videos, in a selfsupervised manner. Concretely, we introduce a simple yet\\r\\nefficient video encoder that considers spatio-temporal context to extract frame-wise representations. Inspired by the\\r\\nrecent progress of self-supervised learning, we present a\\r\\nnovel sequence contrastive loss (SCL) applied on two correlated views obtained through a series of spatio-temporal\\r\\ndata augmentations. SCL optimizes the embedding space by\\r\\nminimizing the KL-divergence between the sequence similarity of two augmented views and a prior Gaussian distribution of timestamp distance. Experiments on FineGym,\\r\\nPennAction and Pouring datasets show that our method\\r\\noutperforms previous state-of-the-art by a large margin\\r\\nfor downstream fine-grained action classification. Surprisingly, although without training on paired videos, our\\r\\napproach also shows outstanding performance on video\\r\\nalignment and fine-grained frame retrieval tasks. Code\\r\\nand models are available at https://github.com/\\r\\nminghchen/CARL_code.\\r\\n\\r\\nQuery\\r\\n\\r\\nTop-3 Retrieved Results\\r\\n\\r\\n(a) Fine-grained frame retrieval on FineGym dataset.\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\nHand touches\\r\\nbottle\\r\\n\\r\\nLiquid starts\\r\\nexiting\\r\\n\\r\\n…\\r\\n\\r\\nPouring\\r\\ncomplete\\r\\n\\r\\nBottle back on\\r\\ntable\\r\\n\\r\\n(b) Phase boundary detection on Pouring dataset.\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n(c) Temporal video alignment on PennAction dataset.\\r\\n\\r\\nFigure 1. Multiple applications of our frame-wise representation\\r\\nlearning on various datasets: (a) Fine-grained frame retrieval on\\r\\nFineGym [37]. (b) Phase boundary detection on Pouring [36]. (c)\\r\\nTemporal video alignment on PennAction [50]. As shown in the\\r\\nfigures, the representations obtained through our method (CARL)\\r\\nare invariant to the appearance, viewpoint and background.\\r\\n\\r\\n1. Introduction\\r\\nIn the last few years, deep learning for video understanding [1, 9, 17, 33, 39, 41, 44, 48] has achieved great success\\r\\non video classification task [9, 19, 40]. Networks such as\\r\\nI3D [9] and SlowFast [17] always take short video clips\\r\\n(e.g., 32 frames or 64 frames) as input and extract global\\r\\n*Accomplished during Minghao Chen’s internship at MSRA.\\r\\n† Corresponding author.\\r\\n\\r\\nrepresentations to predict the action category. In contrast,\\r\\nmany practical applications, e.g., sign language translation [4, 5, 13], robotic imitation learning [29, 36], action\\r\\nalignment [6, 21, 23] and phase classification [16, 27, 37, 50]\\r\\nrequire algorithms having ability to model long videos with\\r\\nhundreds of frames and extract frame-wise representations\\r\\nrather than the global features (Fig. 1).\\r\\nPrevious methods [27, 35, 37] have made an effort to\\r\\n\\r\\n\\x0c\\n\\nlearn frame-wise representations via supervised learning,\\r\\nwhere sub-actions or phase boundaries are annotated. However, it is time-consuming and even impractical to manually\\r\\nlabel each frame and exact action boundaries [21] on largescale datasets, which hinders the generalization of models trained with fully supervised learning in realistic scenarios. To reduce the dependency of labeled data, some\\r\\nmethods such as TCC [16], LAV [23] and GTA [21] explored weakly-supervised learning by using either cycleconsistency loss [16] or soft dynamic time warping [21,23].\\r\\nAll these methods rely on video-level annotations and the\\r\\ntraining is conducted on the paired videos describing the\\r\\nsame action. This setting obstructs them from applying on\\r\\nmore generic video datasets where no labels are available.\\r\\nThe goal of this work is to learn frame-wise representations with spatio-temporal context information for long\\r\\nvideos in a self-supervised manner. Inspired by the recent\\r\\nprogress of contrastive representation learning [8, 11, 12,\\r\\n20], we present a novel framework named contrastive action\\r\\nrepresentation learning (CARL) to achieve our goal. We assume no labels are available during training, and videos in\\r\\nboth training and testing sets have long durations (hundreds\\r\\nof frames). Moreover, we do not rely on video pairs of the\\r\\nsame action for training. Thus it is practical to scale up our\\r\\ntraining set with less cost.\\r\\nModeling long videos with hundreds of frames is challenging. It is non-trivial to directly use off-the-shelf backbones designed for short video clip classification, since our\\r\\ntask is to extract frame-wise representations for long videos.\\r\\nIn our work, we present a simple yet efficient video encoder\\r\\nthat consists of a 2D network to encode spatial information\\r\\nper frame and a Transformer [42] encoder to model temporal interaction. The frame-wise features are then used for\\r\\nrepresentation learning.\\r\\nRecently, SimCLR [11] uses instance discrimination [46] as the pretext task and introduces a contrastive\\r\\nloss named NT-Xent, which maximizes the agreement between two augmented views of the same data. In their implementation, all instances other than the positive reference\\r\\nare considered as negatives. Unlike image data, videos provide more abundant instances (each frame is regarded as\\r\\nan instance), and the neighboring frames have high semantic similarities. Directly regarding these frames as negatives may hurt the learning. To avoid this issue, we present\\r\\na novel sequence contrastive loss (SCL), which optimizes\\r\\nthe embedding space by minimizing the KL-divergence\\r\\nbetween the sequence similarity of two augmented video\\r\\nviews and a prior Gaussian distribution.\\r\\nThe main contributions of this paper are summarized as\\r\\nfollows:\\r\\n• We propose a novel framework named contrastive action representation learning (CARL) to learn framewise action representations with spatio-temporal con-\\r\\n\\r\\ntext information for long videos in a self-supervised\\r\\nmanner. Our method does not rely on any data annotations and has no assumptions on datasets.\\r\\n• We introduce a Transformer-based network to efficiently encode long videos and a novel sequence contrastive loss (SCL) for representation learning. Meanwhile, a series of spatio-temporal data augmentations\\r\\nare designed to increase the variety of training data.\\r\\n• Our framework outperforms the state-of-the-art methods by a large margin on multiple tasks across different datasets. For example, under the linear evaluation protocol on FineGym [37] dataset, our framework\\r\\nachieves 41.75% accuracy, which is +13.94% higher\\r\\nthan the existing best method GTA [21]. On PennAction [50] dataset, our method achieves 91.67% for\\r\\nfine-grained classification, 99.1% for Kendall’s Tau,\\r\\nand 90.58% top-5 accuracy for fine-grained frame retrieval, which all surpass the existing best methods.\\r\\n\\r\\n2. Related Works\\r\\nConventional Action Recognition. Various challenging\\r\\nvideo datasets [9, 25, 32, 38, 40] have been constructed to\\r\\nreason deeply about diverse scenes and situations. These\\r\\ndatasets provide labels of high-level concepts or detailed\\r\\nphysical aspects for short videos or trimmed clips. To tackle\\r\\nvideo recognition, large amounts of architectures have been\\r\\nproposed [1, 3, 9, 17, 33, 39, 41, 43, 44]. Most networks are\\r\\nbased on 3D Convolution layers and combined with the\\r\\ntechniques in image recognition [9, 17, 41], e.g., residual\\r\\nconnections [24] and ImageNet pre-training [14]. Some\\r\\nworks [33, 44] find that 3D ConvNets have insufficient receptive fields and become the bottleneck of the computational budget.\\r\\nRecently, Transformers [42] achieved great success in\\r\\nthe field of computer vision, e.g., ViT [15] and DETR [7].\\r\\nThere are also several works that extend Transformers to\\r\\nvideo recognition, such as TimeSformer [3] and ViViT [1].\\r\\nDue to the strong capacity of Transformers and the global\\r\\nreceptive field, these methods have become new stateof-the-art. Combining 2D backbones and Transformers,\\r\\nVTN [33] can efficiently process long video sequences.\\r\\nHowever, these architecture are all designed for video classification and predict one global class for a video.\\r\\nFine-grained Action Recognition. There are also some\\r\\ndatasets [27, 35, 37, 50] that investigate fine-grained action\\r\\nrecognition. They decompose an action into some action\\r\\nunits, sub-actions, or phases. As a result, each video contains multiple simple stages, e.g., wash the cucumber, peel\\r\\nthe cucumber, place the cucumber, take a knife, and make\\r\\na slice in preparing cucumber [35]. However, these finelevel labels are more expensive to collect, resulting in a\\r\\nlimited size of these datasets. GTA [21] argues that these\\r\\n\\r\\n\\x0c\\n\\nView 1\\r\\n\\r\\nFrame-wise Representations for View 1\\r\\n\\r\\n…\\r\\n\\r\\nRandom Sampling\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\n…\\r\\n\\r\\nLong Video Sequence\\r\\n\\r\\nTemporal\\r\\nRandom\\r\\nCrop\\r\\n\\r\\nSpatial\\r\\nAug\\r\\n\\r\\nRandom Sampling\\r\\n\\r\\nCommon\\r\\nFrames\\r\\n\\r\\nFVE\\r\\nProjection\\r\\n\\r\\n…\\r\\n\\r\\nSequence\\r\\nContrastive\\r\\nLoss\\r\\n\\r\\nFrame-wise Representations for View 2\\r\\n\\r\\nView 2\\r\\n\\r\\nData Preprocessing\\r\\n\\r\\nRepresentation Learning\\r\\n\\r\\nFigure 2. Overview of our framework (CARL). Two augmented views are constructed from a training video through a series of spatiotemporal data augmentations. The frame-level video encoder (FVE) and the projection head are optimized by minimizing the proposed\\r\\nsequence contrastive loss (SCL) between two views.\\r\\n\\r\\nboundary of manual annotations are subjective. Therefore,\\r\\nself-supervised learning for fine-level representations is a\\r\\npromising direction.\\r\\nSelf-supervised Learning in Videos. Previous methods of\\r\\nself-supervised learning in videos construct pretext tasks,\\r\\nincluding inferring the future [22], discriminating shuffled\\r\\nframes [31] and predicting speed [2]. There are also some\\r\\nalignment-based methods, where a pair of videos are trained\\r\\nwith cycle-consistent loss [16] or soft dynamic time warping (DTW) [10, 21, 23]. Recently, the contrastive learning\\r\\nmethods [11, 12, 20, 45] based on instance discrimination\\r\\nhave shown superior performance on 2D image tasks. Some\\r\\nworks [18, 26, 34, 36, 49] also use this contrastive loss for\\r\\nvideo representation learning. They treat different frames\\r\\nin a video [26, 36, 49] or different clips [18, 34] in other\\r\\nvideos as negative samples. Different from these methods,\\r\\nour goal is fine-grained temporal understanding of videos\\r\\nand we treat a long sequence of frames as input data. The\\r\\nmost relevant work to ours is [28], which utilizes 3D human keypoints for self-supervised acton discovery in long\\r\\nkinematic videos.\\r\\n\\r\\nis named data preprocessing. Then we feed two augmented\\r\\nviews into our frame-level video encoder (FVE) to extract\\r\\ndense representations. Following SimCLR [11], FVE is appended with a small projection network which is a twolayer MLP for obtaining latent embeddings. Due to the\\r\\nfact that temporally adjacent frames are highly correlated,\\r\\nwe assume that the similarity distribution between two augmented views should follow a prior Gaussian distribution.\\r\\nBased on the assumption, we propose a novel sequence contrastive loss (SCL) to optimize frame-wise representations\\r\\nin the embedding space.\\r\\n\\r\\n3. Method\\r\\n\\r\\nConcretely, for a training video V with S frames, we\\r\\naim to construct two augmented videos with T frames independently through a series spatio-temporal data augmentations. For temporal data augmentation, we first perform\\r\\ntemporal random crop on V to generate two randomly\\r\\ncropped clips with the length of [T, αT ] frames, where α\\r\\nis a hyper-parameter controlling maximum crop size. During this process, we guarantee at least β percent of overlapped frames existing between two clips. Then we randomly sample T frames for each video sequence, and obtain V 1 = {v 1i | 1 ≤ i ≤ T }, and V 2 = {v 2i | 1 ≤ i ≤ T },\\r\\nwhere v 1i and v 2i represent i-th frame from V 1 and V 2 , respectively. We set T = 240 by default. For the videos with\\r\\nless than T frames, empty frames are padded before cropping. Finally, we apply several temporal-consistent spatial\\r\\ndata augmentations, including random resize and crop, horizontal flip, random color distortions, and random Gaussian\\r\\nblur, on V 1 and V 2 independently.\\r\\n\\r\\nIn this section, we introduce a novel framework named\\r\\ncontrastive action representation learning (CARL) to learn\\r\\nframe-wise action representations in a self-supervised manner. In particular, our framework is designed to model\\r\\nlong video sequences by considering spatio-temporal context. We first present an overview of the proposed framework in Section 3.1. Then we introduce the details of view\\r\\nconstruction and data augmentation in Section 3.2. Next,\\r\\nwe describe our frame-level video encoder in Section 3.3.\\r\\nFinally, the proposed sequence contrastive loss (SCL) and\\r\\nits design principles are introduced in Section 3.4.\\r\\n\\r\\n3.1. Overview\\r\\nFigure 2 displays an overview of our framework. We first\\r\\nconstruct two augmented views for an input video through\\r\\na series of spatio-temporal data augmentations. This step\\r\\n\\r\\n3.2. View Construction\\r\\nWe first introduce the view construction step of our\\r\\nmethod, as shown in the ‘data preprocessing’ part in Figure 2. Data augmentation is crucial to avoid trivial solutions\\r\\nin self-supervised learning [11, 12]. Different from prior\\r\\nmethods designed for image data which only require spatial augmentations, we introduce a series of spatio-temporal\\r\\ndata augmentations to further increase the variety of videos.\\r\\n\\r\\n\\x0c\\n\\nPrior Gaussian Distribution of\\r\\nTimestamp Distance between 𝑠𝑖1 and S 2\\r\\n\\r\\nFrame-wise Representations\\r\\n\\r\\n𝑇 × 128\\r\\n\\r\\nTimeline of\\r\\nthe Video\\r\\n\\r\\nLinear\\r\\n\\r\\n𝑠𝑖1\\r\\n\\r\\n𝑇 × 256\\r\\n\\r\\nTransformer Encoder\\r\\n𝑇 × 256\\r\\n\\r\\nPositional\\r\\nEncoding\\r\\n\\r\\nSample Frames\\r\\n𝑽1\\r\\n\\r\\nResNet-50\\r\\n\\r\\n𝑆2\\r\\n\\r\\n…\\r\\n𝑽2\\r\\n\\r\\n𝑣𝑖1\\r\\n\\r\\nTransformation\\r\\n𝑇 × 2048\\r\\n\\r\\n…\\r\\n\\r\\nFVE + Projection\\r\\n𝒁1\\r\\n\\r\\n𝒁2\\r\\n\\r\\n𝑧𝑖1\\r\\nCosine Similarity (𝑧𝑖1 , 𝒁2 )\\r\\n\\r\\n𝑇 × 224 ×224 ×3\\r\\nInput Video\\r\\n\\r\\nFigure 3. Architecture of the proposed frame-level video encoder\\r\\n(FVE). The input is a long video with T frames and the outputs\\r\\nare frame-wise representations. ResNet-50 is pre-trained on ImageNet. We freeze the first four residual blocks of ResNet-50 and\\r\\nonly finetune the last block.\\r\\n\\r\\n3.3. Frame-level Video Encoder\\r\\nIt is non-trivial to directly apply video classification\\r\\nbackbones [9, 17, 41] to model long video sequences with\\r\\nhundreds of frames due to the huge computational cost.\\r\\nTCC [16] presents a video encoder that combines 2D\\r\\nResNet and 3D Convolution to generate frame-wise features. However, stacking too many 3D Convolutional layers\\r\\nleads to unaffordable computational costs. As a result, this\\r\\nkind of design may have limited receptive fields to capture\\r\\ntemporal context. Recently, Transformers [42] achieved\\r\\ngreat progress in computer vision [7, 15]. Transformers utilize the attention mechanism to solve sequence-to-sequence\\r\\ntasks while handling long-range dependencies with ease. In\\r\\nour network implementation, we adopt the Transformer encoder as an alternative to model temporal context.\\r\\nFigure 3 shows our frame-level video encoder (FVE). To\\r\\nseek the tradeoff between representation performance and\\r\\ninference speed, we first use a 2D network, e.g., ResNet50 [24], along temporal dimension to extract spatial features\\r\\nfor the RGB video sequence of size T × 224 × 224 × 3.\\r\\nThen a transformation block that consists of two fully connected layers with batch normalization and ReLU is applied to project the spatial features to the intermediate embeddings of size T × 256. Following common practice,\\r\\nwe add the sine-cosine positional encoding [42] on top of\\r\\nthe intermediate embeddings to encode the order information. Next, the encoded embeddings are fed into the 3-layer\\r\\nTransformer encoder to model temporal context. At last, a\\r\\nlinear layer is adopted to obtain the final frame-wise representations H ∈ RT ×128 . We use hi (1 ≤ i ≤ T ) to denote\\r\\nthe representation of i-th frame.\\r\\n\\r\\nSequence Contrastive\\r\\nLoss of 𝐿1𝑖\\r\\n\\r\\nFigure 4. Illustration of the proposed sequence contrastive loss.\\r\\nWe use the loss computation of v 1i ∈ V 1 as the example. We\\r\\nfirst compute a prior Gaussian distribution of timestamp distance\\r\\n(s1i −s21 , · · · , s1i −s2T ). Then the embedding similarity distribution\\r\\nbetween z 1i and Z 2 is calculated. We minimize the KL-divergence\\r\\nof two distributions in the embedding space.\\r\\n\\r\\nThe 2D ResNet-50 network is pre-trained on ImageNet [14]. Considering the limited computational budget,\\r\\nwe freeze the first four residual blocks since they already\\r\\nlearned favorable low-level visual representations by pretraining. This simple design ensures that our network can\\r\\nbe trained and tested on videos with more than 500 frames.\\r\\nVTN [33] adopt a similar hybrid Transformer-based network to perform video classification. They use the [CLS]\\r\\ntoken to generate a global feature, while our network is designed to extract frame-wise representations by considering\\r\\nthe spatio-temporal context. In addition, our network explores modeling much more prolonged video sequences.\\r\\n\\r\\n3.4. Sequence Contrastive Loss\\r\\nSimCLR [11] introduces a contrastive loss named NTXent by maximizing agreement between augmented views\\r\\nof the same instance.\\r\\nUnlike self-supervised learning for images, videos provide abundant sequential information, which is a vital supervisory signal. For typical instance discrimination, all\\r\\ninstances other than the positive reference are considered\\r\\nas negatives. However, the neighboring frames around the\\r\\nreference frame are highly correlated. Directly regarding\\r\\nthese frames as negatives may hurt the learning. Learning\\r\\nprinciples should be carefully designed to avoid this issue.\\r\\nTo optimize frame-wise representations, we propose a novel\\r\\nsequence contrastive loss (SCL) which minimizes the KLdivergence between the embedding similarity of two augmented views and the prior Gaussian distribution, as shown\\r\\n\\r\\n\\x0c\\n\\nin Figure 4.\\r\\nConcretely, following SimCLR, we use a small projection network g(·) which is a two-layer MLP to project\\r\\nframe-wise representations H encoded by the proposed\\r\\nFVE to the latent embeddings Z = g(H). Let Z 1 =\\r\\n{z 1i | 1 ≤ i ≤ T } and Z 2 = {z 2i | 1 ≤ i ≤ T } denote the latent embeddings of V 1 and V 2 , where z 1i and\\r\\nz 2i represent the latent embedding of i-th frame in V 1 and\\r\\nV 2 respectively. Let S 1 = {s1i | 1 ≤ i ≤ T } denote timestamp vector of V 1 , where s1i is the corresponding raw video\\r\\ntimestamp of the i-th frame in V 1 (see Figure 4). In the\\r\\nsame way, we can define S 2 = {s2i | 1 ≤ i ≤ T }.\\r\\nGiven the i-th reference frame in V 1 and its corresponding latent embedding z 1i , due to the fact that temporally\\r\\nadjacent frames are more highly correlated than those faraway ones, we assume the embedding similarity between\\r\\nz 1i and Z 2 = {z 2i | 1 ≤ i ≤ T } should follow a prior\\r\\nGaussian distribution of timestamp distance between s1i and\\r\\nS 2 = {s2i | 1 ≤ i ≤ T }. This assumption motivates\\r\\nus to use KL-divergence to optimize the embedding space.\\r\\nSpecifically, let sim(u, v) = u> v/kukkvk denote cosine\\r\\nx2\\r\\nsimilarity, and G(x) = σ√12π exp(− 2σ\\r\\n2 ) denote the Gaussian function, where σ 2 is the variance. We formulate the\\r\\nloss of i-th reference frame in V 1 as follows:\\r\\n\\r\\nPennAction Dataset. Videos in this dataset show humans doing different kinds of sports or exercise. Following TCC [16], we use 13 actions of PennAction dataset. In\\r\\ntotal, there are 1140 videos for training and 966 videos for\\r\\ntesting. Each action set has 40-134 videos for training and\\r\\n42-116 videos for testing. We obtain per-frame labels from\\r\\nLAV [23]. The video frames are from 18 to 663.\\r\\nFineGym Dataset. FineGym is a recent large-scale finegrained action recognition dataset that requires representation learning methods to distinguish different sub-actions\\r\\nwithin the same video. We chunk the original YouTube\\r\\nvideos according to the action boundaries so that each\\r\\ntrimmed video data only describes a single action type\\r\\n(Floor Exercise, Balance Beam, Uneven Bars, or VaultWomen). Finally, we obtained 3182 videos for training and\\r\\n1442 videos for testing. The video frames vary from 140\\r\\nto 5153. FineGym provides two data splits according to the\\r\\ncategory number, namely FineGym99 with 99 sub-action\\r\\nclasses and FineGym288 with 288 sub-action classes.\\r\\nPouring Dataset. In this dataset, videos record the process of hand pouring water from one object to another. The\\r\\nphase labels (5 phase classes) are obtained from TCC [16].\\r\\nFollowing TCC [16], we use 70 videos for training and 14\\r\\nvideos for testing. The video frames are from 186 to 797.\\r\\n\\r\\nT\\r\\nX\\r\\nexp(sim(z 1i , z 2j )/τ )\\r\\n=−\\r\\nwij log PT\\r\\n,\\r\\n1\\r\\n2\\r\\nk=1 exp(sim(z i , z k )/τ )\\r\\nj=1\\r\\n\\r\\nEvaluation Metrics. For each dataset, We first optimize\\r\\nour network on the training set, without using any labels,\\r\\nand then use the following four metrics to evaluate the\\r\\nframe-wise representations:\\r\\n\\r\\nL1i\\r\\n\\r\\nG(s1i − s2j )\\r\\n,\\r\\nwij = PT\\r\\n1\\r\\n2\\r\\nk=1 G(si − sk )\\r\\n\\r\\n(1)\\r\\n\\r\\n(2)\\r\\n\\r\\nwhere wij is the normalized Gaussian weight and τ is the\\r\\ntemperature parameter. Then the overall loss for V 1 can be\\r\\ncomputed across all frames:\\r\\nT\\r\\n1X 1\\r\\nL .\\r\\nL =\\r\\nT i=1 i\\r\\n1\\r\\n\\r\\n(3)\\r\\n\\r\\nSimilarly, we can calculate the loss L2 for V 2 . Our sequence contrastive loss is defined as LSCL = L1 + L2 .\\r\\nNoticeably, our loss does not rely on frame-to-frame correspondence between V 1 and V 2 , which supports the diversity of spatial-temporal data augmentation.\\r\\n\\r\\n• Phase Classification (or Fine-grained Action Classification) [16] is the averaged per-frame classification\\r\\naccuracy on testing set. Before testing, we fix the network and train a linear classifier by using per-frame\\r\\nlabels (phase class or sub-action category) of the training set.\\r\\n• Phase Progression [16] measures the representation\\r\\nability to predict the phase progress. We fix the\\r\\nnetwork and train a linear regressor to predict the\\r\\nphase progression values (timestamp distance between\\r\\na query frame and phase boundaries) for all frames.\\r\\nThen it is computed as the average R-squared measure.\\r\\n\\r\\n4.1. Datasets and Metrics\\r\\n\\r\\n• Kendall’s Tau [16] is calculated over every pair of testing videos by sampling two frames in the first video\\r\\nand retrieving the corresponding nearest frames in the\\r\\nsecond video, and checking whether their orders are\\r\\nshuffled. It measures how well-aligned two sequences\\r\\nare in time. No more training or finetuning is needed.\\r\\n\\r\\nWe use three video datasets, namely PennAction [50],\\r\\nFineGym [37] and Pouring [36] to evaluate the performance\\r\\nof our method. We compare our method with sate-of-thearts on all three datasets. Unless otherwise specified, all\\r\\nablation studies on conducted on PennAction dataset.\\r\\n\\r\\n• Average Precision@K [23] is computed as how many\\r\\nframes in the retrieved K frames have the same phase\\r\\nlabels as the query frame. It measures the fine-grained\\r\\nframe retrieval accuracy. K = 5, 10, 15 are evaluated.\\r\\nNo more training or finetuning is needed.\\r\\n\\r\\n4. Experiments\\r\\n\\r\\n\\x0c\\n\\nTraining Strategy\\r\\n\\r\\nAnnotation\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nTCC [16]\\r\\nLAV [23]\\r\\n\\r\\nPer-action\\r\\n\\r\\nWeakly\\r\\n\\r\\n81.35\\r\\n84.25\\r\\n\\r\\n0.664\\r\\n0.661\\r\\n\\r\\n0.701\\r\\n0.805\\r\\n\\r\\nTCC [16]\\r\\nLAV [23]\\r\\nGTA [21]\\r\\n\\r\\nJoint\\r\\n\\r\\nWeakly\\r\\n\\r\\n74.39\\r\\n78.68\\r\\n-\\r\\n\\r\\n0.591\\r\\n0.625\\r\\n0.789\\r\\n\\r\\n0.641\\r\\n0.684\\r\\n0.748\\r\\n\\r\\nSaL [31]\\r\\nTCN [36]\\r\\nOurs\\r\\n\\r\\nJoint\\r\\n\\r\\nNone\\r\\n\\r\\n68.15\\r\\n68.09\\r\\n93.07\\r\\n\\r\\n0.390\\r\\n0.383\\r\\n0.918\\r\\n\\r\\n0.474\\r\\n0.542\\r\\n0.985\\r\\n\\r\\nMethod\\r\\n\\r\\nTable 1. Comparison with state-of-the-art methods on PennAction, using various evaluation metrics: Phase Classification (Classification),\\r\\nPhase Progression (Progress) and Kendall’s Tau (τ ). The top row results are from per-action models, i.e., separate models are trained for\\r\\ndifferent actions. The results in middle and bottom row are obtained from training a single model for all actions.\\r\\n\\r\\nFollowing [16, 23, 36], Phase Classification, Phase\\r\\nProgression and Kendall’s Tau are evaluated on Pouring\\r\\ndataset. For PennAction, all four metrics are evaluated\\r\\nwithin each action category, and the final results are averaged across the 13 action categories. Following [21], we use\\r\\nFine-grained Action Classification to evaluate our method\\r\\non FineGym dataset.\\r\\n\\r\\n4.2. Implementation Details\\r\\nIn our network, we adopt ResNet-50 [24] pre-trained by\\r\\nBYOL [20] as frame-wise spatial encoder. Unless otherwise specified, we use a 3-layer Transformer encoder [42]\\r\\nwith 256 hidden size and 8 heads to model temporal context. We train the model using Adam optimizer with learning rate 10−4 and weight decay 10−5 . We decay the learning rate with cosine decay schedule without restarts [30]. In\\r\\nour loss, we set σ 2 = 10 and τ = 0.1 as default. Following SimCLR [11], random image cropping, horizontal flipping, random color distortions, and random Gaussian blur\\r\\nare employed as the spatial augmentations. For our temporal data augmentations described in Section 3.2, we set\\r\\nhyper-parameters α = 1.5 and β = 20%. The video batch\\r\\nsize is set as 4 (8 views), and our model is trained on 4\\r\\nNvidia V100 GPUs for 300 epochs. During training, we\\r\\nsample T = 240 frames for Pouring and FineGym, T = 80\\r\\nframes for PennAction. During testing, we feed the whole\\r\\nvideo into the model at once, without any temporal downsampling. We L2-normalize the frame-wise representations\\r\\nfor evaluation.\\r\\n\\r\\n4.3. Main Results\\r\\nResults on PennAction Dataset. In Table 1, our method\\r\\nis compared with state-of-the-art methods on PennAction.\\r\\nTCC [16] and LAV [23] train a separate model for each action (‘Per-action’ in the table), which results in 13 expert\\r\\nmodels for 13 action classes correspondingly. In contrast,\\r\\nwe train only one model for all 13 action classes (‘Joint’\\r\\nin the table). Noticeably, our approach not only outper-\\r\\n\\r\\nMethod\\r\\n\\r\\nAP@5\\r\\n\\r\\nAP@10\\r\\n\\r\\nAP@15\\r\\n\\r\\nTCN [36]\\r\\nTCC [16]\\r\\nLAV [23]\\r\\n\\r\\n77.84\\r\\n76.74\\r\\n79.13\\r\\n\\r\\n77.51\\r\\n76.27\\r\\n78.98\\r\\n\\r\\n77.28\\r\\n75.88\\r\\n78.90\\r\\n\\r\\nOurs\\r\\n\\r\\n92.28\\r\\n\\r\\n92.10\\r\\n\\r\\n91.82\\r\\n\\r\\nTable 2. Fine-grained frame retrieval results on PennAction.\\r\\n\\r\\nMethod\\r\\n\\r\\nFineGym99\\r\\n\\r\\nFineGym288\\r\\n\\r\\n3\\r\\n\\r\\nD TW [10]\\r\\nSpeedNet [2]\\r\\nTCN [36]\\r\\nSaL [31]\\r\\nTCC [16]\\r\\nGTA [21]\\r\\n\\r\\n15.28\\r\\n16.86\\r\\n20.02\\r\\n21.45\\r\\n25.18\\r\\n27.81\\r\\n\\r\\n14.07\\r\\n15.57\\r\\n17.11\\r\\n19.58\\r\\n20.82\\r\\n24.16\\r\\n\\r\\nOurs\\r\\n\\r\\n41.75\\r\\n\\r\\n35.23\\r\\n\\r\\nTable 3. Comparison with state-of-the-art methods on FineGym,\\r\\nunder the evaluation of Fine-grained Action Classification.\\r\\n\\r\\nforms the methods using joint training, but also outperforms\\r\\nthe methods adopting per-action training strategy by a large\\r\\nmargin under different evaluation metrics. In Table 2, we\\r\\nreport the results under the Average Precision@K metric,\\r\\nwhich measures the performance of fine-grained frame retrieval. Surprisingly, although our model is not trained on\\r\\npaired data, it can successfully find frames with similar semantics from other videos. For all AP@K, our method is at\\r\\nleast +11% better than previous methods.\\r\\nResults on FineGym Dataset. Table 3 summarizes the\\r\\nexperimental results of Fine-grained Action Classification\\r\\non FineGym99 and FineGym288. Our method outperforms the other self-supervised [2, 31, 36] and weakly\\r\\nsupervised [10, 16, 21] methods. The performance of\\r\\nour method surpasses the previous state-of-the-art method\\r\\nGTA [21] by +13.94% on FineGym99 and +11.07% on FineGym288. The weakly supervised methods, i.e., TCC [16],\\r\\nD3 TW [10] and GTA [21], assume there exists an optimal\\r\\n\\r\\n\\x0c\\n\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\n#Layers\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nTCN [36]\\r\\nTCC [16]\\r\\nLAV [23]\\r\\n\\r\\n89.53\\r\\n91.53\\r\\n92.84\\r\\n\\r\\n0.804\\r\\n0.837\\r\\n0.805\\r\\n\\r\\n0.852\\r\\n0.864\\r\\n0.856\\r\\n\\r\\nOurs\\r\\n\\r\\n93.73\\r\\n\\r\\n0.935\\r\\n\\r\\n0.992\\r\\n\\r\\n1\\r\\n2\\r\\n3\\r\\n4\\r\\n\\r\\n92.15\\r\\n92.61\\r\\n93.07\\r\\n92.81\\r\\n\\r\\n0.909\\r\\n0.913\\r\\n0.918\\r\\n0.910\\r\\n\\r\\n0.985\\r\\n0.990\\r\\n0.985\\r\\n0.990\\r\\n\\r\\nMethod\\r\\n\\r\\nTable 4. Comparison with state-of-the-art methods on Pouring.\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nResNet-50 only\\r\\nResNet-50+C3D\\r\\n\\r\\n68.63\\r\\n83.96\\r\\n\\r\\n0.296\\r\\n0.705\\r\\n\\r\\n0.440\\r\\n0.778\\r\\n\\r\\nResNet-50+\\r\\nTransformer\\r\\n\\r\\n93.07\\r\\n\\r\\n0.918\\r\\n\\r\\n0.985\\r\\n\\r\\nArchitecture\\r\\n\\r\\nTable 6. Study on the effects of using different number of layers\\r\\nin Transformer encoder.\\r\\n\\r\\nLearnable Blocks\\r\\nNone\\r\\nBlock5\\r\\nBlock4+Block5\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\n90.63\\r\\n93.07\\r\\n92.98\\r\\n\\r\\n0.907\\r\\n0.918\\r\\n0.919\\r\\n\\r\\n0.994\\r\\n0.985\\r\\n0.989\\r\\n\\r\\nTable 7. Ablation study on learnable blocks of ResNet-50.\\r\\n\\r\\nTable 5. Ablation study on different architectures.\\r\\n\\r\\nalignment between two videos from the training set. However, for FineGym dataset, even in two videos describing\\r\\nthe same action, the set and order of sub-actions may differ.\\r\\nTherefore, the alignment found by these methods can be incorrect, which impedes learning. The great improvement\\r\\nverifies the effectiveness of our framework.\\r\\nResults on Pouring Dataset. As shown in Table 4, our\\r\\nmethod also achieves the best performance on a relatively\\r\\nsmall dataset, Pouring. These results further demonstrate\\r\\nthe great generalization ability of our approach.\\r\\nVisualization Results. We present the visualization of finegrained frame retrieval and video alignment in Section A.\\r\\n\\r\\n4.4. Ablation Study\\r\\nIn this section, we perform multiple experiments to analyze the different components of our framework. Unless\\r\\notherwise specified, experiments are conducted on the PennAction dataset.\\r\\nNetwork Architecture. In Table 5, we investigate the network architecture. ‘ResNet-50+Transformer’ denotes our\\r\\ndefault frame-level video encoder introduced in Section 3.3.\\r\\n‘ResNet-50 only’ means we remove the Transformer encoder in our network, and only use 2D ResNet-50 and\\r\\nlinear transformation layers to extract representations per\\r\\nframe. ‘ResNet-50+C3D’ represents that two 3D convolutional layers [41] are added on top of the ResNet-50 before the spatial pooling, which is the same as the model\\r\\nadopted in TCC [16] and LAV [23]. These models are all\\r\\ntrained with the proposed sequence contrastive loss. Our\\r\\ndefault network outperforms the other two networks, which\\r\\nattributes to the long-range dependency modeling ability of\\r\\nTransformers.\\r\\nLayer Number of Transformer Encoder. Table 6 shows\\r\\nstudies using different numbers of layers in Transformers.\\r\\nWe find that Phase Classification increases with more lay-\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nTCN†\\r\\nTCC†\\r\\n\\r\\n86.31\\r\\n86.35\\r\\n\\r\\n0.898\\r\\n0.899\\r\\n\\r\\n0.832\\r\\n0.980\\r\\n\\r\\nOurs\\r\\n\\r\\n93.07\\r\\n\\r\\n0.918\\r\\n\\r\\n0.985\\r\\n\\r\\nMethod\\r\\n\\r\\nTable 8. Applying our network to TCN and TCC. † denotes we reimplement the method and replace the network with ours. “Contrastive baseline” uses the corresponding frame at the other view\\r\\nas the positive sample.\\r\\n\\r\\ners. However, Phase Progression slightly drops when there\\r\\nare too many layers. We use 3 layers by default.\\r\\nTraining Different Blocks of ResNet. In our implementation, ResNet-50 is pre-trained on ImageNet. In Table 7, we\\r\\nstudy the effects of finetuning different blocks of ResNet50. The standard ResNet contains 5 blocks, namely Block1Block5. ‘None’ denotes that all layers of ResNet are frozen.\\r\\n‘Block5’ denotes we freeze the first four residual blocks\\r\\nof ResNet and only make the last residual block learnable,\\r\\nwhich is our default setting. Similarly, ‘Block4+Block5’\\r\\nmeans we freeze the first three blocks and only train the last\\r\\ntwo blocks. Table 7 shows that encoding dataset-related\\r\\nspatial information is important (‘None’ vs. ‘Block5’),\\r\\nand training more blocks does not lead to improvement\\r\\n(‘Block5’ vs. ‘Block4+Block5’).\\r\\nApplying Our Network to Other Methods. We study\\r\\nwhether our frame-level video encoder (FVE) introduced\\r\\nin Section 3.3 can boost the performances of TCC [16] and\\r\\nTCN [36]. We replace the C3D-based network with ours.\\r\\nTable 8 shows the results. We find that the proposed network can dramatically improve the performance of their\\r\\nmethods (compared with the results in Table 3). In addition, our method still keeps a large performance gain, which\\r\\nattributes to the proposed sequence contrastive loss.\\r\\nHyper-parameters of Sequence Contrastive Loss. We\\r\\nstudy the hyper-parameters, i.e., temperature parameter τ\\r\\nand Gaussian variance σ 2 in our sequence contrastive loss\\r\\n\\r\\n\\x0c\\n\\nHyper-parameters\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\nα\\r\\n\\r\\nSampling\\r\\n\\r\\nβ (%)\\r\\n\\r\\nFineGym99\\r\\n\\r\\nτ =0.1, σ 2 =1\\r\\nτ =0.1, σ 2 =25\\r\\n\\r\\n92.95\\r\\n92.03\\r\\n\\r\\n0.903\\r\\n0.922\\r\\n\\r\\n0.963\\r\\n0.993\\r\\n\\r\\nRandom\\r\\n\\r\\n20\\r\\n\\r\\nτ =1.0, σ 2 =10\\r\\nτ =0.3, σ 2 =10\\r\\nτ =0.1, σ 2 =10\\r\\n\\r\\n91.57\\r\\n92.13\\r\\n93.07\\r\\n\\r\\n0.889\\r\\n0.903\\r\\n0.918\\r\\n\\r\\n0.993\\r\\n0.992\\r\\n0.985\\r\\n\\r\\n0\\r\\n1.5\\r\\n1\\r\\n\\r\\n36.72\\r\\n41.75\\r\\n39.03\\r\\n\\r\\n1.5\\r\\n\\r\\nEven\\r\\n\\r\\n20\\r\\n\\r\\n38.44\\r\\n\\r\\nRandom\\r\\n\\r\\n0\\r\\n20\\r\\n50\\r\\n80\\r\\n100\\r\\n\\r\\n38.15\\r\\n41.75\\r\\n39.14\\r\\n37.94\\r\\n35.53\\r\\n\\r\\nTable 9. Ablation study on Gaussian variance σ 2 and the temperature τ in sequence contrastive loss.\\r\\n\\r\\n(see Eq. 2). The variance σ 2 of the prior Gaussian distribution controls how the adjacent frames are semantically similar to the reference frame, on the assumption. As Table 9\\r\\nshows, too small variance (σ 2 = 1) or too large variance\\r\\n(σ 2 = 25) degrades the performance. We use σ 2 = 10 by\\r\\ndefault. In addition, we observe an appropriate temperature\\r\\n(τ = 0.1) facilitates the learning from hard negatives, which\\r\\nis consistent with the conclusion in SimCLR [11].\\r\\nStudy on Different Temporal Data Augmentations. We\\r\\nstudy the different temporal data augmentations described\\r\\nin Section 3.2, including maximum crop size α, overlap\\r\\nratio β between views, and different sampling strategies,\\r\\nnamely random sampling and even sampling. Table 10\\r\\nshows the results. From the table, we can see that the performance drops dramatically when we crop the video with a\\r\\nfixed length (α = 1). The performance also decreases when\\r\\nwe perform even sampling on the cropped clips. As described in Section 3.4, our sequence contrastive loss does\\r\\nnot rely on frame-to-frame correspondence between two\\r\\naugmented views. Experimentally, constructing two views\\r\\nwith β = 100% percent of overlapped frames degrades the\\r\\nperformance, since the variety of augmented data decreases.\\r\\nIn addition, we also observe the performance drops when\\r\\ntwo views are constructed independently (β = 0% ). The\\r\\nreason is that in this setting, the training may bring the representations of temporally distant frames closer, which hinders the optimization.\\r\\nNumber of Training Frames and Linear Evaluation Under Different Data Protocols. As described in Section 3.2,\\r\\nour network takes augmented views with T frames as input. We study the effects of different frame numbers T on\\r\\nFineGym99. Table 11 shows the results. We observe that\\r\\ntaking long sequences as input is essential for frame-wise\\r\\nrepresentation learning. However, a too large frame number degrades the performance. We thus set T = 240 by default. We also conduct linear evaluation under different data\\r\\nprotocols. Concretely, we use 10%, 50% and 100% labeled\\r\\ndata to train the linear classifier. Compared with the supervised model (all layers are learnable), our method achieves\\r\\nbetter performance when the labeled data is limited (10%\\r\\ndata protocol).\\r\\n\\r\\n1.5\\r\\n\\r\\nTable 10. Ablation study on hyper-parameters of temporal data\\r\\naugmentations. Effects of maximum crop size α, overlap ratio β\\r\\nand random sampling strategy are studied. The experiments are\\r\\nconducted on FineGym99 dataset.\\r\\n\\r\\n% of Labeled Data →\\r\\n\\r\\n10\\r\\n\\r\\nNumber of training frames:\\r\\n80\\r\\n27.10\\r\\n160\\r\\n30.28\\r\\n33.53\\r\\n240\\r\\n480\\r\\n31.46\\r\\nSupervised\\r\\n\\r\\n24.51\\r\\n\\r\\n50\\r\\n\\r\\n100\\r\\n\\r\\n32.78\\r\\n36.46\\r\\n39.89\\r\\n37.92\\r\\n\\r\\n34.02\\r\\n38.06\\r\\n41.75\\r\\n39.45\\r\\n\\r\\n48.75\\r\\n\\r\\n60.37\\r\\n\\r\\nTable 11. Ablation studies on number of training frames under\\r\\ndifferent data protocols. Study is conducted on FineGym99 Finegrained Action Classification task. ‘Supervised’ means all layers\\r\\nare trained with supervised learning.\\r\\n\\r\\n5. Conclusion\\r\\nIn this paper, we present a novel framework named\\r\\ncontrastive action representation learning (CARL) to learn\\r\\nframe-wise action representations, especially for long\\r\\nvideos, in a self-supervised manner. To model long videos\\r\\nwith hundreds of frames, we introduce a simple yet efficient\\r\\nnetwork named frame-level video encoder (FVE), which\\r\\nconsiders spatio-temporal context during training. In addition, we propose a novel sequence contrastive loss (SCL)\\r\\nfor frame-wise representation learning. SCL optimizes the\\r\\nembedding space by minimizing the KL-divergence between the sequence similarity of two augmented views\\r\\nand a prior Gaussian distribution. Experiments on various\\r\\ndatasets and tasks show effectiveness and generalization of\\r\\nour method.\\r\\n\\r\\nAcknowledgments\\r\\nThis work was supported in part by The National\\r\\nKey Research and Development Program of China (Grant\\r\\nNos: 2018AAA0101400), in part by The National Nature Science Foundation of China (Grant Nos: 62036009,\\r\\n61936006), in part by Innovation Capability Support Program of Shaanxi (Program No. 2021TD-05).\\r\\n\\r\\n\\x0c\\n\\nFigure 5. Visualization of video alignment on FineGym dataset.\\r\\nPlease refer to video demos in our supplementary materials for\\r\\nmore visualization results.\\r\\n\\r\\nQuery\\r\\n\\r\\nTop-5 Retrieved Frames\\r\\n\\r\\nFigure 7. Visualization of fine-grained frame retrieval on FineGym\\r\\ndatast by using our method.\\r\\n\\r\\ngive video demos in our supplementary materials.\\r\\n(a) Pouring dataset.\\r\\n\\r\\n(b) PennAction dataset.\\r\\n\\r\\nFigure 6. We randomly select two videos recording the same process (or action) from Pouring (or PennAction) dataset and compute the similarity matrix for frame-wise representations extracted\\r\\nby our method. The similarities are normalized for better visualization.\\r\\n\\r\\nA. More Results\\r\\nIn this section, we show visualization results of video\\r\\nalignment and fine-grained frame retrieval.\\r\\n\\r\\nA.1. Video Alignment\\r\\nGiven two videos recording the similar action or process,\\r\\nthe goal of video alignment is to find the temporal correspondence between them. Firstly, we use our framework\\r\\nto extract the frame-wise representations for two randomly\\r\\nselected videos. Then we compute the cosine similarities\\r\\nbetween the frame-wise representations of two videos and\\r\\nutilize the famous dynamic time warping (DTW) algorithm\\r\\non the similarity matrix to find the best temporal alignment.\\r\\nFigure 5 shows an example from FineGym test set. Please\\r\\nrefer to video demos in our supplementary materials for\\r\\nmore visualization results.\\r\\nWe also randomly select two videos recording the same\\r\\nprocess (or action) from Pouring (or PennAction) dataset,\\r\\nand similarly, we can compute the similarity matrix which is\\r\\nrendered as a heatmap in Figure 6. We observe that the diagonal is highlighted, which means our approach find the favorable alignment between two correlated videos. We also\\r\\n\\r\\nA.2. Fine-grained Frame Retrieval\\r\\nIn Figure 7, we present the visualization results of finegrained frame retrieval on FineGym dataset. To be specific, we feed the video containing the query frames into\\r\\nour CARL framework to generate query features, and similarly, we can extract frame-wise features for the rest videos\\r\\nin the test set. We simply compute the cosine similarity between query features and frame-wise features from candidate videos to obtain top-5 retrieved frames as shown in Figure 7. The retrieved frames have similar semantics with the\\r\\nquery frame, though the appearances, the camera views, and\\r\\nthe backgrounds are different, which suggests our method is\\r\\nrobust to these factors.\\r\\n\\r\\nA.3. Action Localization\\r\\nTo show the potential of our method on large datasets and\\r\\nmore downstream tasks, we optimize the frame-wise features via our self-supervised method on ActivityNet [25].\\r\\nThen we use G-TAD [47] on the top of the features (without fine-tuning) to perform temporal action localization. As\\r\\nshown in Table 12, we use mAP(%) at {0.5, 0.75, 0.95}\\r\\ntIoU thresholds and the average mAP across 10 tIoU levels for evaluation. In contrast to the supervised two-stream\\r\\nmodel [41], our method does not need any video labels\\r\\nwhile achieving better performance.\\r\\n\\r\\nA.4. Compare with Contrastive Baseline\\r\\nWe compare our SCL with the contrastive baseline which\\r\\nonly uses the corresponding frame in the other view as\\r\\n\\r\\n\\x0c\\n\\nMethod\\r\\nG-TAD w. 2stream\\r\\nG-TAD w. ours\\r\\n\\r\\n0.5\\r\\n\\r\\n0.75\\r\\n\\r\\n0.95\\r\\n\\r\\nAverage\\r\\n\\r\\n50.36\\r\\n51.22\\r\\n\\r\\n34.60\\r\\n35.19\\r\\n\\r\\n9.02\\r\\n8.54\\r\\n\\r\\n34.09\\r\\n34.46\\r\\n\\r\\n[6]\\r\\n\\r\\nTable 12. Temporal action localization on ActivityNet v1.3.\\r\\n\\r\\nMethod\\r\\nContrastive baseline\\r\\nSCL (ours)\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\n88.05\\r\\n93.07\\r\\n\\r\\n0.898\\r\\n0.918\\r\\n\\r\\n0.891\\r\\n0.985\\r\\n\\r\\nTable 13. Compare our SCL with contrastive baseline, which uses\\r\\nthe corresponding frame in the other view as the positive sample.\\r\\n\\r\\nTraining Dataset\\r\\nK400\\r\\nK400 → PennAction\\r\\n\\r\\nClassification\\r\\n\\r\\nProgress\\r\\n\\r\\nτ\\r\\n\\r\\n91.9\\r\\n93.9\\r\\n\\r\\n0.903\\r\\n0.908\\r\\n\\r\\n0.949\\r\\n0.977\\r\\n\\r\\n[7]\\r\\n\\r\\n[8]\\r\\n\\r\\n[9]\\r\\n\\r\\n[10]\\r\\n\\r\\n[11]\\r\\nTable 14. Our CARL pre-trained on Kinetics-400 shows outstanding transfer ability on PennAction. Fine-tuning the pre-trained\\r\\nmodel on PennAction further boosts the performance.\\r\\n[12]\\r\\n\\r\\nthe positive sample and ignores temporal adjacent frames.\\r\\nAs Table 13 shows, our SCL can more efficiently employ\\r\\nthe sequential information and thus achieves better performance.\\r\\n\\r\\nA.5. Kinetics-400 Pre-training\\r\\nTo show our method can benefit from large-scale datasets\\r\\nwithout any labels, we train our CARL on Kinetics-400 [9].\\r\\nAs Table 14 shows, the frame-wise representations trained\\r\\non Kinetics-400 shows outstanding generalization on PennAction dataset. Moreover, fine-tuning the pre-trained model\\r\\non PennAction by using our CARL further boosts the performance, e.g., + 2% classification improvement.\\r\\n\\r\\n[13]\\r\\n\\r\\n[14]\\r\\n\\r\\n[15]\\r\\n\\r\\n[16]\\r\\n\\r\\nReferences\\r\\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\\r\\nSun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. ArXiv, 2021. 1, 2\\r\\n[2] Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri,\\r\\nWilliam T. Freeman, Michael Rubinstein, Michal Irani, and\\r\\nTali Dekel. Speednet: Learning the speediness in videos. In\\r\\nCVPR, 2020. 3, 6\\r\\n[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\\r\\nspace-time attention all you need for video understanding?\\r\\nArXiv, 2021. 2\\r\\n[4] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. Neural sign language translation. In CVPR, 2018. 1\\r\\n[5] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and\\r\\nRichard Bowden. Sign language transformers: Joint end-\\r\\n\\r\\n[17]\\r\\n\\r\\n[18]\\r\\n\\r\\n[19]\\r\\n\\r\\n[20]\\r\\n\\r\\nto-end sign language recognition and translation. In CVPR,\\r\\n2020. 1\\r\\nKaidi Cao, Jingwei Ji, Zhangjie Cao, C. Chang, and\\r\\nJuan Carlos Niebles. Few-shot video classification via temporal alignment. In CVPR, 2020. 1\\r\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\r\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. ArXiv, 2020. 2, 4\\r\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning\\r\\nof visual features by contrasting cluster assignments. ArXiv,\\r\\n2020. 2\\r\\nJoão Carreira and Andrew Zisserman. Quo vadis, action\\r\\nrecognition? a new model and the kinetics dataset. In CVPR,\\r\\n2017. 1, 2, 4, 10\\r\\nC. Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and\\r\\nJuan Carlos Niebles. D3tw: Discriminative differentiable\\r\\ndynamic time warping for weakly supervised action alignment and segmentation. In CVPR, 2019. 3, 6\\r\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3, 4, 6,\\r\\n8\\r\\nXinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming\\r\\nHe. Improved baselines with momentum contrastive learning. ArXiv, 2020. 2, 3\\r\\nYutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and\\r\\nStephen Lin. A simple multi-modality transfer learning baseline for sign language translation. arXiv preprint\\r\\narXiv:2203.04287, 2022. 1\\r\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\r\\nand Fei-Fei Li. Imagenet: A large-scale hierarchical image\\r\\ndatabase. In CVPR, 2009. 2, 4\\r\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\r\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\r\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\r\\nworth 16x16 words: Transformers for image recognition at\\r\\nscale. In ICLR, 2021. 2, 4\\r\\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre\\r\\nSermanet, and Andrew Zisserman.\\r\\nTemporal cycleconsistency learning. In CVPR, 2019. 1, 2, 3, 4, 5, 6, 7\\r\\nChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\\r\\nKaiming He. Slowfast networks for video recognition. In\\r\\nICCV, 2019. 1, 2, 4\\r\\nChristoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross B. Girshick, and Kaiming He. A large-scale study on unsupervised\\r\\nspatiotemporal representation learning. In CVPR, 2021. 3\\r\\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\\r\\nValentin Haenel, Ingo Fründ, Peter N. Yianilos, Moritz\\r\\nMueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax,\\r\\nand Roland Memisevic. The “something something” video\\r\\ndatabase for learning and evaluating visual common sense.\\r\\nIn ICCV, 2017. 1\\r\\nJean-Bastien Grill, Florian Strub, Florent Altch’e, Corentin\\r\\nTallec, Pierre H. Richemond, Elena Buchatskaya, Carl Do-\\r\\n\\r\\n\\x0c\\n\\n[21]\\r\\n\\r\\n[22]\\r\\n\\r\\n[23]\\r\\n\\r\\n[24]\\r\\n\\r\\n[25]\\r\\n\\r\\n[26]\\r\\n\\r\\n[27]\\r\\n\\r\\n[28]\\r\\n\\r\\n[29]\\r\\n\\r\\n[30]\\r\\n[31]\\r\\n\\r\\n[32]\\r\\n\\r\\n[33]\\r\\n[34]\\r\\n\\r\\n[35]\\r\\n\\r\\n[36]\\r\\n\\r\\nersch, Bernardo Ávila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi\\r\\nMunos, and Michal Valko. Bootstrap your own latent: A new\\r\\napproach to self-supervised learning. ArXiv, 2020. 2, 3, 6\\r\\nIsma Hadji, Konstantinos G. Derpanis, and Allan D. Jepson.\\r\\nRepresentation learning via global temporal alignment and\\r\\ncycle-consistency. In CVPR, 2021. 1, 2, 3, 6\\r\\nTengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In ICCVW,\\r\\n2019. 3\\r\\nSanjay Haresh, Sateesh Kumar, Huseyin Coskun,\\r\\nShahram Najam Syed, Andrey Konin, M. Zeeshan Zia,\\r\\nand Quoc-Huy Tran. Learning by aligning videos in time.\\r\\nIn CVPR, 2021. 1, 2, 3, 5, 6, 7\\r\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\r\\nDeep residual learning for image recognition. In CVPR,\\r\\n2016. 2, 4, 6\\r\\nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\\r\\nand Juan Carlos Niebles. Activitynet: A large-scale video\\r\\nbenchmark for human activity understanding. In CVPR,\\r\\n2015. 2, 9\\r\\nHaofei Kuang, Yi Zhu, Zhi Zhang, Xinyu Li, Joseph Tighe,\\r\\nSören Schwertfeger, C. Stachniss, and Mu Li. Video contrastive learning with global context. ArXiv, 2021. 3\\r\\nHilde Kuehne, Ali Bilgin Arslan, and Thomas Serre. The\\r\\nlanguage of actions: Recovering the syntax and semantics of\\r\\ngoal-directed human activities. In CVPR, 2014. 1, 2\\r\\nKenneth Li, Xiao Sun, Zhirong Wu, Fangyun Wei, and\\r\\nStephen Lin. Towards tokenized human dynamics representation. arXiv preprint arXiv:2111.11433, 2021. 3\\r\\nYuxuan Liu, Abhishek Gupta, P. Abbeel, and Sergey Levine.\\r\\nImitation from observation: Learning to imitate behaviors\\r\\nfrom raw video via context translation. 2018 IEEE International Conference on Robotics and Automation (ICRA),\\r\\n2018. 1\\r\\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\\r\\ndescent with warm restarts. arXiv: Learning, 2017. 6\\r\\nIshan Misra, C. Lawrence Zitnick, and Martial Hebert. Shuffle and learn: Unsupervised learning using temporal order\\r\\nverification. In ECCV, 2016. 3, 6\\r\\nMathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa M. Brown,\\r\\nQuanfu Fan, Dan Gutfreund, Carl Vondrick, and Aude Oliva.\\r\\nMoments in time dataset: One million videos for event understanding. PAMI, 2020. 2\\r\\nDaniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. ArXiv, 2021. 1, 2, 4\\r\\nRui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang,\\r\\nH. Wang, Serge J. Belongie, and Yin Cui. Spatiotemporal\\r\\ncontrastive video representation learning. In CVPR, 2021. 3\\r\\nMarcus Rohrbach, Anna Rohrbach, Michaela Regneri,\\r\\nSikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, and\\r\\nBernt Schiele. Recognizing fine-grained and composite activities using hand-centric features and script data. In IJCV,\\r\\n2015. 1, 2\\r\\nPierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine\\r\\nHsu, Eric Jang, Stefan Schaal, and Sergey Levine. Timecontrastive networks: Self-supervised learning from video.\\r\\n\\r\\n[37]\\r\\n\\r\\n[38]\\r\\n\\r\\n[39]\\r\\n\\r\\n[40]\\r\\n\\r\\n[41]\\r\\n\\r\\n[42]\\r\\n\\r\\n[43]\\r\\n\\r\\n[44]\\r\\n[45]\\r\\n\\r\\n[46]\\r\\n\\r\\n[47]\\r\\n\\r\\n[48]\\r\\n\\r\\n[49]\\r\\n\\r\\n[50]\\r\\n\\r\\n2018 IEEE International Conference on Robotics and Automation (ICRA), 2018. 1, 3, 5, 6, 7\\r\\nDian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A\\r\\nhierarchical video dataset for fine-grained action understanding. In CVPR, 2020. 1, 2, 5\\r\\nGunnar A. Sigurdsson, Gül Varol, X. Wang, Ali Farhadi,\\r\\nIvan Laptev, and Abhinav Gupta. Hollywood in homes:\\r\\nCrowdsourcing data collection for activity understanding. In\\r\\nECCV, 2016. 2\\r\\nKaren Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In\\r\\nNIPS, 2014. 1, 2\\r\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\\r\\nUcf101: A dataset of 101 human actions classes from videos\\r\\nin the wild. ArXiv, 2012. 1, 2\\r\\nDu Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features\\r\\nwith 3d convolutional networks. In ICCV, 2015. 1, 2, 4, 7, 9\\r\\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\\r\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\\r\\nIllia Polosukhin. Attention is all you need. In NIPS, 2017.\\r\\n2, 4, 6\\r\\nLimin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\\r\\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\\r\\nnetworks for action recognition in videos. PAMI, 2019. 2\\r\\nX. Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming\\r\\nHe. Non-local neural networks. In CVPR, 2018. 1, 2\\r\\nFangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen\\r\\nLin. Aligning pretraining for detection via object-level contrastive learning. Advances in Neural Information Processing Systems, 34, 2021. 3\\r\\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\r\\nUnsupervised feature learning via non-parametric instance\\r\\ndiscrimination. In CVPR, 2018. 2\\r\\nMengmeng Xu, Chen Zhao, David S. Rojas, Ali Thabet, and\\r\\nBernard Ghanem. G-tad: Sub-graph localization for temporal action detection. In CVPR, 2020. 9\\r\\nYinghao Xu, Fangyun Wei, Xiao Sun, Ceyuan Yang, Yujun Shen, Bo Dai, Bolei Zhou, and Stephen Lin. Crossmodel pseudo-labeling for semi-supervised action recognition. arXiv preprint arXiv:2112.09690, 2021. 1\\r\\nTing Yao, Yiheng Zhang, Zhaofan Qiu, Yingwei Pan, and\\r\\nTao Mei. Seco: Exploring sequence supervision for unsupervised representation learning. In AAAI, 2021. 3\\r\\nWeiyu Zhang, Menglong Zhu, and Konstantinos G. Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In ICCV, 2013.\\r\\n1, 2, 5\\r\\n\\r\\n\\x0c'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with one pdf first\n",
    "\n",
    "mypdftext=array_pdf_text[0]\n",
    "mypdftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a99fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\r\n",
      "Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. ArXiv, 2021. 1, 2\r\n",
      "[2] Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri,\r\n",
      "William T. Freeman, Michael Rubinstein, Michal Irani, and\r\n",
      "Tali Dekel. Speednet: Learning the speediness in videos. In\r\n",
      "CVPR, 2020. 3, 6\r\n",
      "[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\r\n",
      "space-time attention all you need for video understanding?\r\n",
      "ArXiv, 2021. 2\r\n",
      "[4] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. Neural sign language translation. In CVPR, 2018. 1\r\n",
      "[5] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and\r\n",
      "Richard Bowden. Sign language transformers: Joint end-\r\n",
      "\r\n",
      "[17]\r\n",
      "\r\n",
      "[18]\r\n",
      "\r\n",
      "[19]\r\n",
      "\r\n",
      "[20]\r\n",
      "\r\n",
      "to-end sign language recognition and translation. In CVPR,\r\n",
      "2020. 1\r\n",
      "Kaidi Cao, Jingwei Ji, Zhangjie Cao, C. Chang, and\r\n",
      "Juan Carlos Niebles. Few-shot video classification via temporal alignment. In CVPR, 2020. 1\r\n",
      "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\r\n",
      "Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. ArXiv, 2020. 2, 4\r\n",
      "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning\r\n",
      "of visual features by contrasting cluster assignments. ArXiv,\r\n",
      "2020. 2\r\n",
      "João Carreira and Andrew Zisserman. Quo vadis, action\r\n",
      "recognition? a new model and the kinetics dataset. In CVPR,\r\n",
      "2017. 1, 2, 4, 10\r\n",
      "C. Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and\r\n",
      "Juan Carlos Niebles. D3tw: Discriminative differentiable\r\n",
      "dynamic time warping for weakly supervised action alignment and segmentation. In CVPR, 2019. 3, 6\r\n",
      "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3, 4, 6,\r\n",
      "8\r\n",
      "Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming\r\n",
      "He. Improved baselines with momentum contrastive learning. ArXiv, 2020. 2, 3\r\n",
      "Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and\r\n",
      "Stephen Lin. A simple multi-modality transfer learning baseline for sign language translation. arXiv preprint\r\n",
      "arXiv:2203.04287, 2022. 1\r\n",
      "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\r\n",
      "and Fei-Fei Li. Imagenet: A large-scale hierarchical image\r\n",
      "database. In CVPR, 2009. 2, 4\r\n",
      "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\r\n",
      "Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\r\n",
      "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\r\n",
      "worth 16x16 words: Transformers for image recognition at\r\n",
      "scale. In ICLR, 2021. 2, 4\r\n",
      "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre\r\n",
      "Sermanet, and Andrew Zisserman.\r\n",
      "Temporal cycleconsistency learning. In CVPR, 2019. 1, 2, 3, 4, 5, 6, 7\r\n",
      "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\r\n",
      "Kaiming He. Slowfast networks for video recognition. In\r\n",
      "ICCV, 2019. 1, 2, 4\r\n",
      "Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross B. Girshick, and Kaiming He. A large-scale study on unsupervised\r\n",
      "spatiotemporal representation learning. In CVPR, 2021. 3\r\n",
      "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\r\n",
      "Valentin Haenel, Ingo Fründ, Peter N. Yianilos, Moritz\r\n",
      "Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax,\r\n",
      "and Roland Memisevic. The “something something” video\r\n",
      "database for learning and evaluating visual common sense.\r\n",
      "In ICCV, 2017. 1\r\n",
      "Jean-Bastien Grill, Florian Strub, Florent Altch’e, Corentin\r\n",
      "Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Do-\r\n",
      "\r\n",
      "\f",
      "\n",
      "\n",
      "[21]\r\n",
      "\r\n",
      "[22]\r\n",
      "\r\n",
      "[23]\r\n",
      "\r\n",
      "[24]\r\n",
      "\r\n",
      "[25]\r\n",
      "\r\n",
      "[26]\r\n",
      "\r\n",
      "[27]\r\n",
      "\r\n",
      "[28]\r\n",
      "\r\n",
      "[29]\r\n",
      "\r\n",
      "[30]\r\n",
      "[31]\r\n",
      "\r\n",
      "[32]\r\n",
      "\r\n",
      "[33]\r\n",
      "[34]\r\n",
      "\r\n",
      "[35]\r\n",
      "\r\n",
      "[36]\r\n",
      "\r\n",
      "ersch, Bernardo Ávila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi\r\n",
      "Munos, and Michal Valko. Bootstrap your own latent: A new\r\n",
      "approach to self-supervised learning. ArXiv, 2020. 2, 3, 6\r\n",
      "Isma Hadji, Konstantinos G. Derpanis, and Allan D. Jepson.\r\n",
      "Representation learning via global temporal alignment and\r\n",
      "cycle-consistency. In CVPR, 2021. 1, 2, 3, 6\r\n",
      "Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In ICCVW,\r\n",
      "2019. 3\r\n",
      "Sanjay Haresh, Sateesh Kumar, Huseyin Coskun,\r\n",
      "Shahram Najam Syed, Andrey Konin, M. Zeeshan Zia,\r\n",
      "and Quoc-Huy Tran. Learning by aligning videos in time.\r\n",
      "In CVPR, 2021. 1, 2, 3, 5, 6, 7\r\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\r\n",
      "Deep residual learning for image recognition. In CVPR,\r\n",
      "2016. 2, 4, 6\r\n",
      "Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\r\n",
      "and Juan Carlos Niebles. Activitynet: A large-scale video\r\n",
      "benchmark for human activity understanding. In CVPR,\r\n",
      "2015. 2, 9\r\n",
      "Haofei Kuang, Yi Zhu, Zhi Zhang, Xinyu Li, Joseph Tighe,\r\n",
      "Sören Schwertfeger, C. Stachniss, and Mu Li. Video contrastive learning with global context. ArXiv, 2021. 3\r\n",
      "Hilde Kuehne, Ali Bilgin Arslan, and Thomas Serre. The\r\n",
      "language of actions: Recovering the syntax and semantics of\r\n",
      "goal-directed human activities. In CVPR, 2014. 1, 2\r\n",
      "Kenneth Li, Xiao Sun, Zhirong Wu, Fangyun Wei, and\r\n",
      "Stephen Lin. Towards tokenized human dynamics representation. arXiv preprint arXiv:2111.11433, 2021. 3\r\n",
      "Yuxuan Liu, Abhishek Gupta, P. Abbeel, and Sergey Levine.\r\n",
      "Imitation from observation: Learning to imitate behaviors\r\n",
      "from raw video via context translation. 2018 IEEE International Conference on Robotics and Automation (ICRA),\r\n",
      "2018. 1\r\n",
      "Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\r\n",
      "descent with warm restarts. arXiv: Learning, 2017. 6\r\n",
      "Ishan Misra, C. Lawrence Zitnick, and Martial Hebert. Shuffle and learn: Unsupervised learning using temporal order\r\n",
      "verification. In ECCV, 2016. 3, 6\r\n",
      "Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa M. Brown,\r\n",
      "Quanfu Fan, Dan Gutfreund, Carl Vondrick, and Aude Oliva.\r\n",
      "Moments in time dataset: One million videos for event understanding. PAMI, 2020. 2\r\n",
      "Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. ArXiv, 2021. 1, 2, 4\r\n",
      "Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang,\r\n",
      "H. Wang, Serge J. Belongie, and Yin Cui. Spatiotemporal\r\n",
      "contrastive video representation learning. In CVPR, 2021. 3\r\n",
      "Marcus Rohrbach, Anna Rohrbach, Michaela Regneri,\r\n",
      "Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, and\r\n",
      "Bernt Schiele. Recognizing fine-grained and composite activities using hand-centric features and script data. In IJCV,\r\n",
      "2015. 1, 2\r\n",
      "Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine\r\n",
      "Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Timecontrastive networks: Self-supervised learning from video.\r\n",
      "\r\n",
      "[37]\r\n",
      "\r\n",
      "[38]\r\n",
      "\r\n",
      "[39]\r\n",
      "\r\n",
      "[40]\r\n",
      "\r\n",
      "[41]\r\n",
      "\r\n",
      "[42]\r\n",
      "\r\n",
      "[43]\r\n",
      "\r\n",
      "[44]\r\n",
      "[45]\r\n",
      "\r\n",
      "[46]\r\n",
      "\r\n",
      "[47]\r\n",
      "\r\n",
      "[48]\r\n",
      "\r\n",
      "[49]\r\n",
      "\r\n",
      "[50]\r\n",
      "\r\n",
      "2018 IEEE International Conference on Robotics and Automation (ICRA), 2018. 1, 3, 5, 6, 7\r\n",
      "Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A\r\n",
      "hierarchical video dataset for fine-grained action understanding. In CVPR, 2020. 1, 2, 5\r\n",
      "Gunnar A. Sigurdsson, Gül Varol, X. Wang, Ali Farhadi,\r\n",
      "Ivan Laptev, and Abhinav Gupta. Hollywood in homes:\r\n",
      "Crowdsourcing data collection for activity understanding. In\r\n",
      "ECCV, 2016. 2\r\n",
      "Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In\r\n",
      "NIPS, 2014. 1, 2\r\n",
      "Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\r\n",
      "Ucf101: A dataset of 101 human actions classes from videos\r\n",
      "in the wild. ArXiv, 2012. 1, 2\r\n",
      "Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features\r\n",
      "with 3d convolutional networks. In ICCV, 2015. 1, 2, 4, 7, 9\r\n",
      "Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\r\n",
      "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\r\n",
      "Illia Polosukhin. Attention is all you need. In NIPS, 2017.\r\n",
      "2, 4, 6\r\n",
      "Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\r\n",
      "Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment\r\n",
      "networks for action recognition in videos. PAMI, 2019. 2\r\n",
      "X. Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming\r\n",
      "He. Non-local neural networks. In CVPR, 2018. 1, 2\r\n",
      "Fangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen\r\n",
      "Lin. Aligning pretraining for detection via object-level contrastive learning. Advances in Neural Information Processing Systems, 34, 2021. 3\r\n",
      "Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\r\n",
      "Unsupervised feature learning via non-parametric instance\r\n",
      "discrimination. In CVPR, 2018. 2\r\n",
      "Mengmeng Xu, Chen Zhao, David S. Rojas, Ali Thabet, and\r\n",
      "Bernard Ghanem. G-tad: Sub-graph localization for temporal action detection. In CVPR, 2020. 9\r\n",
      "Yinghao Xu, Fangyun Wei, Xiao Sun, Ceyuan Yang, Yujun Shen, Bo Dai, Bolei Zhou, and Stephen Lin. Crossmodel pseudo-labeling for semi-supervised action recognition. arXiv preprint arXiv:2112.09690, 2021. 1\r\n",
      "Ting Yao, Yiheng Zhang, Zhaofan Qiu, Yingwei Pan, and\r\n",
      "Tao Mei. Seco: Exploring sequence supervision for unsupervised representation learning. In AAAI, 2021. 3\r\n",
      "Weiyu Zhang, Menglong Zhu, and Konstantinos G. Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In ICCV, 2013.\r\n",
      "1, 2, 5\r\n",
      "\r\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#First filter to have only the References from a pdf\n",
    "\n",
    "def after_references(mypdftext): \n",
    "    keyword1 = 'References'\n",
    "    keyword2 = 'REFERENCES'\n",
    "    keyword3 = 'R EFERENCES'\n",
    "    keyword4 = 'Reference'\n",
    "    keyword5='[1]' \n",
    "\n",
    "    if keyword1 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword1)\n",
    "    elif keyword2 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword2)\n",
    "    elif keyword3 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword3)\n",
    "    elif keyword4 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword4)\n",
    "    elif keyword5 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword5)\n",
    "    else:\n",
    "        after_keyword = mypdftext[:10000]\n",
    "    return after_keyword\n",
    "\n",
    "#All references in a variable\n",
    "\n",
    "references=after_references(mypdftext)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439d489",
   "metadata": {},
   "source": [
    "## Preprocess to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e325730d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First cleaning\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns): \n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s) \n",
    "        return s\n",
    "\n",
    "replacer=RegexpReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e46e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess a text before using IA\n",
    "\n",
    "def preprocess_text(test):\n",
    "\n",
    "    #Removing Numbers\n",
    "    test=re.sub(r'\\d+','',test)\n",
    "\n",
    "    #Removing Letter alone\n",
    "    test = re.sub(r'\\b\\w\\b', ' ', test)\n",
    "\n",
    "    #Removing white spaces\n",
    "    test=test.strip()\n",
    "    \n",
    "    #Replacer replace\n",
    "    text_replaced = replacer.replace(test)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text_replaced)\n",
    "\n",
    "    #Tokenize words\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = tokenizer.tokenize(sentences[i])\n",
    "\n",
    "    #Remove stop words\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stops=set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [word for word in sentences[i] if word not in stops]\n",
    "\n",
    "    #Lemmatize\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer_output=WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            sentences[i][j] = lemmatizer_output.lemmatize(sentences[i][j])\n",
    "\n",
    "\n",
    "    #Join the words back into a sentence.\n",
    "    a=[' '.join(s) for s in sentences]\n",
    "    b=', '.join(a)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c077ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anurag Arnab Mostafa Dehghani Georg Heigold Chen Sun Mario Lucic Cordelia Schmid, Vivit video vision transformer, ArXiv, Sagie Benaim Ariel Ephrat Oran Lang Inbar Mosseri William, Freeman Michael Rubinstein Michal Irani Tali Dekel, Speednet Learning speediness video, In CVPR, Gedas Bertasius Heng Wang Lorenzo Torresani, Is space time attention need video understanding, ArXiv, Necati Cihan Camgoz Simon Hadfield Oscar Koller Hermann Ney Richard Bowden, Neural sign language translation, In CVPR, Necati Cihan Camgoz Oscar Koller Simon Hadfield Richard Bowden, Sign language transformer Joint end end sign language recognition translation, In CVPR, Kaidi Cao Jingwei Ji Zhangjie Cao, Chang Juan Carlos Niebles, Few shot video classification via temporal alignment, In CVPR, Nicolas Carion Francisco Massa Gabriel Synnaeve Nicolas Usunier Alexander Kirillov Sergey Zagoruyko, End toend object detection transformer, ArXiv, Mathilde Caron Ishan Misra Julien Mairal Priya Goyal Piotr Bojanowski Armand Joulin, Unsupervised learning visual feature contrasting cluster assignment, ArXiv, Joa Carreira Andrew Zisserman, Quo vadis action recognition, new model kinetics dataset, In CVPR, , Chang De An Huang Yanan Sui Li Fei Fei Juan Carlos Niebles, Dtw Discriminative differentiable dynamic time warping weakly supervised action alignment segmentation, In CVPR, Ting Chen Simon Kornblith Mohammad Norouzi Geoffrey, Hinton, simple framework contrastive learning visual representation, In ICML, Xinlei Chen Haoqi Fan Ross, Girshick Kaiming He, Improved baseline momentum contrastive learning, ArXiv, Yutong Chen Fangyun Wei Xiao Sun Zhirong Wu Stephen Lin, simple multi modality transfer learning baseline sign language translation, arXiv preprint arXiv, Jia Deng Wei Dong Richard Socher Li Jia Li Kai Li Fei Fei Li, Imagenet large scale hierarchical image database, In CVPR, Alexey Dosovitskiy Lucas Beyer Alexander Kolesnikov Dirk Weissenborn Xiaohua Zhai Thomas Unterthiner Mostafa Dehghani Matthias Minderer Georg Heigold Sylvain Gelly Jakob Uszkoreit Neil Houlsby, An image worth word Transformers image recognition scale, In ICLR, Debidatta Dwibedi Yusuf Aytar Jonathan Tompson Pierre Sermanet Andrew Zisserman, Temporal cycleconsistency learning, In CVPR, Christoph Feichtenhofer Haoqi Fan Jitendra Malik Kaiming He, Slowfast network video recognition, In ICCV, Christoph Feichtenhofer Haoqi Fan Bo Xiong Ross, Girshick Kaiming He, large scale study unsupervised spatiotemporal representation learning, In CVPR, Raghav Goyal Samira Ebrahimi Kahou Vincent Michalski Joanna Materzynska Susanne Westphal Heuna Kim Valentin Haenel Ingo Fru nd Peter, Yianilos Moritz Mueller Freitag Florian Hoppe Christian Thurau Ingo Bax Roland Memisevic, The something something video database learning evaluating visual common sense, In ICCV, Jean Bastien Grill Florian Strub Florent Altch Corentin Tallec Pierre, Richemond Elena Buchatskaya Carl Do ersch Bernardo vila Pires Zhaohan Daniel Guo Mohammad Gheshlaghi Azar Bilal Piot Koray Kavukcuoglu Re mi Munos Michal Valko, Bootstrap latent new approach self supervised learning, ArXiv, Isma Hadji Konstantinos, Derpanis Allan, Jepson, Representation learning via global temporal alignment cycle consistency, In CVPR, Tengda Han Weidi Xie Andrew Zisserman, Video representation learning dense predictive coding, In ICCVW, Sanjay Haresh Sateesh Kumar Huseyin Coskun Shahram Najam Syed Andrey Konin, Zeeshan Zia Quoc Huy Tran, Learning aligning video time, In CVPR, Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun, Deep residual learning image recognition, In CVPR, Fabian Caba Heilbron Victor Escorcia Bernard Ghanem Juan Carlos Niebles, Activitynet large scale video benchmark human activity understanding, In CVPR, Haofei Kuang Yi Zhu Zhi Zhang Xinyu Li Joseph Tighe So ren Schwertfeger, Stachniss Mu Li, Video contrastive learning global context, ArXiv, Hilde Kuehne Ali Bilgin Arslan Thomas Serre, The language action Recovering syntax semantics goal directed human activity, In CVPR, Kenneth Li Xiao Sun Zhirong Wu Fangyun Wei Stephen Lin, Towards tokenized human dynamic representation, arXiv preprint arXiv, Yuxuan Liu Abhishek Gupta, Abbeel Sergey Levine, Imitation observation Learning imitate behavior raw video via context translation, IEEE International Conference Robotics Automation ICRA, Ilya Loshchilov Frank Hutter, Sgdr Stochastic gradient descent warm restarts, arXiv Learning, Ishan Misra, Lawrence Zitnick Martial Hebert, Shuffle learn Unsupervised learning using temporal order verification, In ECCV, Mathew Monfort Bolei Zhou Sarah Adel Bargal Alex Andonian Tom Yan Kandan Ramakrishnan Lisa, Brown Quanfu Fan Dan Gutfreund Carl Vondrick Aude Oliva, Moments time dataset One million video event understanding, PAMI, Daniel Neimark Omri Bar Maya Zohar Dotan Asselmann, Video transformer network, ArXiv, Rui Qian Tianjian Meng Boqing Gong Ming Hsuan Yang, Wang Serge, Belongie Yin Cui, Spatiotemporal contrastive video representation learning, In CVPR, Marcus Rohrbach Anna Rohrbach Michaela Regneri Sikandar Amin Mykhaylo Andriluka Manfred Pinkal Bernt Schiele, Recognizing fine grained composite activity using hand centric feature script data, In IJCV, Pierre Sermanet Corey Lynch Yevgen Chebotar Jasmine Hsu Eric Jang Stefan Schaal Sergey Levine, Timecontrastive network Self supervised learning video, IEEE International Conference Robotics Automation ICRA, Dian Shao Yue Zhao Bo Dai Dahua Lin, Finegym hierarchical video dataset fine grained action understanding, In CVPR, Gunnar, Sigurdsson Gu Varol, Wang Ali Farhadi Ivan Laptev Abhinav Gupta, Hollywood home Crowdsourcing data collection activity understanding, In ECCV, Karen Simonyan Andrew Zisserman, Two stream convolutional network action recognition video, In NIPS, Khurram Soomro Amir Roshan Zamir Mubarak Shah, Ucf dataset human action class video wild, ArXiv, Du Tran Lubomir, Bourdev Rob Fergus Lorenzo Torresani Manohar Paluri, Learning spatiotemporal feature convolutional network, In ICCV, Ashish Vaswani Noam, Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan, Gomez Lukasz Kaiser Illia Polosukhin, Attention need, In NIPS, Limin Wang Yuanjun Xiong Zhe Wang Yu Qiao Dahua Lin Xiaoou Tang Luc Van Gool, Temporal segment network action recognition video, PAMI, , Wang Ross, Girshick Abhinav Gupta Kaiming He, Non local neural network, In CVPR, Fangyun Wei Yue Gao Zhirong Wu Han Hu Stephen Lin, Aligning pretraining detection via object level contrastive learning, Advances Neural Information Processing Systems, Zhirong Wu Yuanjun Xiong Stella Yu Dahua Lin, Unsupervised feature learning via non parametric instance discrimination, In CVPR, Mengmeng Xu Chen Zhao David, Rojas Ali Thabet Bernard Ghanem, tad Sub graph localization temporal action detection, In CVPR, Yinghao Xu Fangyun Wei Xiao Sun Ceyuan Yang Yujun Shen Bo Dai Bolei Zhou Stephen Lin, Crossmodel pseudo labeling semi supervised action recognition, arXiv preprint arXiv, Ting Yao Yiheng Zhang Zhaofan Qiu Yingwei Pan Tao Mei, Seco Exploring sequence supervision unsupervised representation learning, In AAAI, Weiyu Zhang Menglong Zhu Konstantinos, Derpanis, From actemes action strongly supervised representation detailed action understanding, In ICCV, \n"
     ]
    }
   ],
   "source": [
    "references_clean= preprocess_text(references)\n",
    "print(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5af5e9",
   "metadata": {},
   "source": [
    "## Get_human_names Algorithme using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa296d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    \n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    for person in person_list:\n",
    "        person_split = person.split(\" \")\n",
    "        for name in person_split:\n",
    "            if wordnet.synsets(name):\n",
    "                if(name in person):\n",
    "                    person_names.remove(person)\n",
    "                    break\n",
    "    return person_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864d30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 48 and without 160\n",
      "Ex: Gedas Bertasius Heng Wang Lorenzo Torresani\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "\n",
    "print(\"Len with preprocess\",len(get_human_names(references_clean)), \"and without\",len(get_human_names(references)))\n",
    "print(\"Ex:\",get_human_names(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8dd9ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gedas Bertasius Heng Wang Lorenzo Torresani',\n",
       " 'Richard Bowden',\n",
       " 'Mathilde Caron Ishan Misra Julien Mairal Priya Goyal Piotr Bojanowski Armand Joulin',\n",
       " 'Joa Carreira',\n",
       " 'Raghav Goyal Samira Ebrahimi Kahou Vincent Michalski Joanna Materzynska Susanne Westphal Heuna',\n",
       " 'Kim Valentin Haenel Ingo Fru',\n",
       " 'Pires Zhaohan',\n",
       " 'Munos Michal Valko',\n",
       " 'Derpanis Allan',\n",
       " 'Sanjay Haresh Sateesh Kumar Huseyin Coskun Shahram Najam Syed Andrey Konin',\n",
       " 'Zeeshan Zia Quoc Huy Tran',\n",
       " 'Yuxuan Liu Abhishek Gupta',\n",
       " 'Abbeel Sergey Levine',\n",
       " 'Ishan Misra',\n",
       " 'Khurram Soomro Amir Roshan Zamir Mubarak Shah',\n",
       " 'Du Tran Lubomir',\n",
       " 'Ashish Vaswani Noam',\n",
       " 'Shazeer Niki Parmar Jakob Uszkoreit',\n",
       " 'Girshick Abhinav Gupta',\n",
       " 'Weiyu Zhang Menglong',\n",
       " 'Zhu Konstantinos',\n",
       " 'Anurag Arnab',\n",
       " 'Mostafa Dehghani',\n",
       " 'Georg Heigold',\n",
       " 'Mario Lucic',\n",
       " 'Sagie Benaim',\n",
       " 'Ariel Ephrat',\n",
       " 'Inbar Mosseri',\n",
       " 'Gedas Bertasius',\n",
       " 'Heng Wang',\n",
       " 'Lorenzo Torresani',\n",
       " 'Necati Cihan Camgoz',\n",
       " 'Zhangjie Cao',\n",
       " 'Nicolas Carion',\n",
       " 'Francisco Massa',\n",
       " 'Nicolas Usunier',\n",
       " 'Sergey Zagoruyko',\n",
       " 'Julien Mairal',\n",
       " 'Priya Goyal',\n",
       " 'Piotr Bojanowski',\n",
       " 'Geoffrey E. Hinton',\n",
       " 'Wei Dong',\n",
       " 'Richard Socher',\n",
       " 'Alexey Dosovitskiy',\n",
       " 'Xiaohua Zhai',\n",
       " 'Matthias Minderer',\n",
       " 'Jakob Uszkoreit',\n",
       " 'Neil Houlsby',\n",
       " 'Yusuf Aytar',\n",
       " 'Bo Xiong',\n",
       " 'Raghav Goyal',\n",
       " 'Samira Ebrahimi Kahou',\n",
       " 'Vincent Michalski',\n",
       " 'Joanna Materzynska',\n",
       " 'Susanne Westphal',\n",
       " 'Heuna Kim',\n",
       " 'Valentin Haenel',\n",
       " 'Ingo Fründ',\n",
       " 'Moritz Mueller-Freitag',\n",
       " 'Florian Hoppe',\n",
       " 'Ingo Bax',\n",
       " 'Florian Strub',\n",
       " 'Florent Altch',\n",
       " 'Corentin Tallec',\n",
       " 'Elena Buchatskaya',\n",
       " 'Bilal Piot',\n",
       " 'Koray Kavukcuoglu',\n",
       " 'Michal Valko',\n",
       " 'Konstantinos G. Derpanis',\n",
       " 'Allan D. Jepson',\n",
       " 'Weidi Xie',\n",
       " 'Sanjay Haresh',\n",
       " 'Sateesh Kumar',\n",
       " 'Huseyin Coskun',\n",
       " 'Shahram Najam Syed',\n",
       " 'Andrey Konin',\n",
       " 'Zeeshan Zia',\n",
       " 'Xiangyu Zhang',\n",
       " 'Shaoqing Ren',\n",
       " 'Haofei Kuang',\n",
       " 'Zhi Zhang',\n",
       " 'Hilde Kuehne',\n",
       " 'Abhishek Gupta',\n",
       " 'Sergey Levine',\n",
       " 'Mathew Monfort',\n",
       " 'Alex Andonian',\n",
       " 'Kandan Ramakrishnan',\n",
       " 'Dan Gutfreund',\n",
       " 'Carl Vondrick',\n",
       " 'Aude Oliva',\n",
       " 'Dotan Asselmann',\n",
       " 'Tianjian Meng',\n",
       " 'Marcus Rohrbach',\n",
       " 'Michaela Regneri',\n",
       " 'Sikandar Amin',\n",
       " 'Mykhaylo Andriluka',\n",
       " 'Manfred Pinkal',\n",
       " 'Bernt Schiele',\n",
       " 'Yevgen Chebotar',\n",
       " 'Eric Jang',\n",
       " 'Stefan Schaal',\n",
       " 'Dian Shao',\n",
       " 'Bo Dai',\n",
       " 'Ivan Laptev',\n",
       " 'Abhinav Gupta',\n",
       " 'Khurram Soomro',\n",
       " 'Lubomir D. Bourdev',\n",
       " 'Manohar Paluri',\n",
       " 'Noam M. Shazeer',\n",
       " 'Niki Parmar',\n",
       " 'Aidan N. Gomez',\n",
       " 'Illia Polosukhin',\n",
       " 'Limin Wang',\n",
       " 'Yuanjun Xiong',\n",
       " 'Zhe Wang',\n",
       " 'Yu Qiao',\n",
       " 'Ali Thabet',\n",
       " 'Yinghao Xu',\n",
       " 'Yujun Shen',\n",
       " 'Yiheng Zhang',\n",
       " 'Zhaofan Qiu',\n",
       " 'Weiyu Zhang',\n",
       " 'Menglong Zhu',\n",
       " 'Necati Cihan Camgoz Simon Hadfield Oscar Koller Hermann Ney',\n",
       " 'Alexander Kirillov Sergey Zagoruyko',\n",
       " 'Xinlei Chen Haoqi Fan Ross',\n",
       " 'Alexander Kolesnikov Dirk Weissenborn Xiaohua Zhai Thomas Unterthiner Mostafa Dehghani Matthias Minderer Georg Heigold Sylvain Gelly Jakob Uszkoreit Neil Houlsby',\n",
       " 'Jean Bastien Grill Florian Strub Florent Altch Corentin Tallec Pierre',\n",
       " 'Tengda Han Weidi Xie',\n",
       " 'Stachniss Mu Li',\n",
       " 'Sgdr Stochastic',\n",
       " 'Rui Qian Tianjian Meng Boqing Gong',\n",
       " 'Pierre Sermanet Corey Lynch Yevgen Chebotar Jasmine Hsu Eric Jang Stefan Schaal Sergey Levine',\n",
       " 'Karen Simonyan',\n",
       " 'Limin Wang Yuanjun Xiong Zhe Wang Yu Qiao Dahua Lin Xiaoou Tang Luc Van Gool',\n",
       " 'Rojas Ali Thabet Bernard Ghanem',\n",
       " 'Arnab Mostafa Dehghani Georg Heigold Chen Sun Mario Lucic Cordelia Schmid',\n",
       " 'Speednet Learning',\n",
       " 'Chang Juan Carlos Niebles',\n",
       " 'Andrew Zisserman',\n",
       " 'Chen Simon Kornblith Mohammad Norouzi Geoffrey',\n",
       " 'Alexey Dosovitskiy Lucas Beyer',\n",
       " 'Christoph Feichtenhofer Haoqi Fan Jitendra Malik',\n",
       " 'Richemond Elena Buchatskaya Carl Do',\n",
       " 'Xiangyu Zhang Shaoqing Ren Jian Sun',\n",
       " 'Zhu Zhi Zhang Xinyu Li Joseph Tighe So',\n",
       " 'Ilya Loshchilov Frank Hutter',\n",
       " 'Brown Quanfu Fan Dan Gutfreund Carl Vondrick Aude Oliva',\n",
       " 'Hsuan Yang',\n",
       " 'Dian Shao Yue',\n",
       " 'Wang Ali Farhadi Ivan Laptev Abhinav Gupta',\n",
       " 'Gomez Lukasz Kaiser Illia Polosukhin',\n",
       " 'Fangyun Wei Yue Gao Zhirong Wu Han Hu Stephen Lin',\n",
       " 'Mengmeng Xu Chen Zhao David']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the result\n",
    "get_human_names(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fcb76",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "656d73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "\n",
    "def nlp_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    PROPN=[token.lemma_ for token in doc if token.pos_ == \"PROPN\"]\n",
    "    \n",
    "#     Remove duplicate\n",
    "    PROPN = list(dict.fromkeys(PROPN))\n",
    "#     Remove word with first letter as lowercase \n",
    "    for word in PROPN:\n",
    "        if word[0].islower():\n",
    "            PROPN.remove(word)\n",
    "    return PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4230cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453\n"
     ]
    }
   ],
   "source": [
    "final_names_prep_spacy=nlp_entities(references_clean)\n",
    "final_names_spacy=nlp_entities(references)\n",
    "print(len(final_names_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d123b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An other function with more accuracy\n",
    "\n",
    "def extract(text:str) :\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    doc = spacy_nlp(text.strip())\n",
    "    named_entities = []\n",
    "    \n",
    "    for i in doc.ents:\n",
    "        entry = str(i.lemma_).lower()\n",
    "        text = text.replace(str(i).lower(), \"\")\n",
    "        if i.label_ in [\"ART\", \"EVE\", \"NAT\", \"PERSON\"]:\n",
    "            named_entities.append(entry.title().replace(\" \", \"_\").replace(\"\\n\",\"_\"))\n",
    "        named_entities = list(dict.fromkeys(named_entities))\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f8b14fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 136 and without 196\n",
      "Ex: Anurag_Arnab_Mostafa_Dehghani_Georg\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "\n",
    "print(\"Len with preprocess\",len(extract(references_clean)), \"and without\",len(extract(references)))\n",
    "print(\"Ex:\",extract(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "167b9906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anurag_Arnab_Mostafa_Dehghani_Georg',\n",
       " 'Ariel_Ephrat',\n",
       " 'Oran_Lang',\n",
       " 'Mosseri_William',\n",
       " 'Freeman_Michael_Rubinstein_Michal_Irani_Tali_Dekel',\n",
       " 'Gedas_Bertasius',\n",
       " 'Wang_Lorenzo_Torresani',\n",
       " 'Cihan_Camgoz',\n",
       " 'Simon_Hadfield_Oscar',\n",
       " 'Hermann_Ney',\n",
       " 'Richard_Bowden',\n",
       " 'Oscar_Koller',\n",
       " 'Simon_Hadfield_Richard_Bowden',\n",
       " 'Kaidi_Cao_Jingwei',\n",
       " 'Ji_Zhangjie_Cao',\n",
       " 'Chang_Juan_Carlos_Niebles',\n",
       " 'Alexander_Kirillov_Sergey_Zagoruyko',\n",
       " 'Mathilde_Caron',\n",
       " 'Joa_Carreira_Andrew_Zisserman',\n",
       " 'Quo',\n",
       " 'Chang_De_An_Huang',\n",
       " 'Sui_Li_Fei_Fei_Juan_Carlos_Niebles',\n",
       " 'Ting_Chen_Simon_Kornblith_Mohammad_Norouzi_Geoffrey',\n",
       " 'Xinlei_Chen',\n",
       " 'Haoqi_Fan_Ross',\n",
       " 'Girshick_Kaiming',\n",
       " 'Yutong_Chen_Fangyun',\n",
       " 'Wei_Xiao',\n",
       " 'Stephen_Lin',\n",
       " 'Jia_Deng',\n",
       " 'Wei_Dong',\n",
       " 'Richard_Socher',\n",
       " 'Li_Jia',\n",
       " 'Li_Kai_Li_Fei_Fei_Li',\n",
       " 'Alexey_Dosovitskiy_Lucas',\n",
       " 'Alexander_Kolesnikov',\n",
       " 'Weissenborn_Xiaohua',\n",
       " 'Thomas_Unterthiner_Mostafa_Dehghani_Matthias_Minderer_Georg',\n",
       " 'Neil_Houlsby',\n",
       " 'Andrew_Zisserman',\n",
       " 'Christoph_Feichtenhofer',\n",
       " 'Jitendra_Malik',\n",
       " 'Xiong_Ross',\n",
       " 'Raghav_Goyal_Samira_Ebrahimi_Kahou',\n",
       " 'Vincent_Michalski',\n",
       " 'Joanna',\n",
       " 'Susanne_Westphal',\n",
       " 'Heuna_Kim',\n",
       " 'Valentin_Haenel_Ingo_Fru',\n",
       " 'Yianilos_Moritz_Mueller',\n",
       " 'Roland_Memisevic',\n",
       " 'Jean_Bastien_Grill',\n",
       " 'Corentin_Tallec_Pierre',\n",
       " 'Richemond_Elena_Buchatskaya_Carl_Do',\n",
       " 'Daniel_Guo_Mohammad_Gheshlaghi_Azar',\n",
       " 'Isma_Hadji_Konstantinos',\n",
       " 'Derpanis_Allan',\n",
       " 'Sanjay_Haresh',\n",
       " 'Andrey_Konin',\n",
       " 'Zeeshan_Zia_Quoc_Huy_Tran',\n",
       " 'Kaiming_He_Xiangyu_Zhang',\n",
       " 'Jian_Sun',\n",
       " 'Caba_Heilbron',\n",
       " 'Escorcia_Bernard',\n",
       " 'Juan_Carlos_Niebles',\n",
       " 'Haofei_Kuang',\n",
       " 'Yi_Zhu_Zhi_Zhang_Xinyu_Li',\n",
       " 'Joseph_Tighe',\n",
       " 'Stachniss_Mu_Li',\n",
       " 'Hilde_Kuehne',\n",
       " 'Thomas_Serre',\n",
       " 'Kenneth_Li',\n",
       " 'Wu_Fangyun',\n",
       " 'Wei_Stephen_Lin',\n",
       " 'Yuxuan_Liu_Abhishek_Gupta',\n",
       " 'Abbeel_Sergey_Levine',\n",
       " 'Frank_Hutter',\n",
       " 'Sgdr_Stochastic',\n",
       " 'Ishan_Misra',\n",
       " 'Lawrence_Zitnick_Martial_Hebert',\n",
       " 'Monfort_Bolei',\n",
       " 'Alex_Andonian',\n",
       " 'Tom_Yan',\n",
       " 'Kandan_Ramakrishnan_Lisa',\n",
       " 'Dan_Gutfreund',\n",
       " 'Carl_Vondrick_Aude_Oliva',\n",
       " 'Daniel_Neimark_Omri_Bar_Maya_Zohar_Dotan_Asselmann',\n",
       " 'Arxiv',\n",
       " 'Rui_Qian',\n",
       " 'Meng_Boqing',\n",
       " 'Hsuan_Yang',\n",
       " 'Wang_Serge',\n",
       " 'Belongie_Yin_Cui',\n",
       " 'Anna_Rohrbach',\n",
       " 'Regneri_Sikandar',\n",
       " 'Amin_Mykhaylo',\n",
       " 'Pierre_Sermanet',\n",
       " 'Corey_Lynch',\n",
       " 'Hsu_Eric_Jang',\n",
       " 'Stefan_Schaal_Sergey_Levine',\n",
       " 'Zhao_Bo_Dai_Dahua_Lin',\n",
       " 'Sigurdsson_Gu_Varol',\n",
       " 'Wang_Ali_Farhadi',\n",
       " 'Laptev_Abhinav_Gupta',\n",
       " 'Karen_Simonyan_Andrew_Zisserman',\n",
       " 'Amir_Roshan_Zamir_Mubarak_Shah',\n",
       " 'Du_Tran_Lubomir',\n",
       " 'Bourdev_Rob_Fergus',\n",
       " 'Torresani_Manohar_Paluri',\n",
       " 'Ashish_Vaswani_Noam',\n",
       " 'Parmar_Jakob',\n",
       " 'Jones_Aidan',\n",
       " 'Gomez_Lukasz_Kaiser',\n",
       " 'Illia_Polosukhin',\n",
       " 'Limin_Wang',\n",
       " 'Yuanjun_Xiong',\n",
       " 'Wang_Yu',\n",
       " 'Qiao_Dahua_Lin',\n",
       " 'Tang_Luc_Van_Gool',\n",
       " 'Wang_Ross',\n",
       " 'Girshick_Abhinav_Gupta_Kaiming',\n",
       " 'Fangyun_Wei_Yue_Gao_Zhirong',\n",
       " 'Wu_Han',\n",
       " 'Hu_Stephen_Lin',\n",
       " 'Wu_Yuanjun',\n",
       " 'Stella_Yu_Dahua_Lin',\n",
       " 'Mengmeng_Xu',\n",
       " 'Chen_Zhao_David',\n",
       " 'Rojas_Ali_Thabet_Bernard_Ghanem',\n",
       " 'Yinghao_Xu_Fangyun',\n",
       " 'Yang_Yujun',\n",
       " 'Dai_Bolei_Zhou',\n",
       " 'Ting_Yao_Yiheng_Zhang',\n",
       " 'Pan_Tao_Mei',\n",
       " 'Weiyu_Zhang',\n",
       " 'Menglong_Zhu_Konstantinos']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Result\n",
    "\n",
    "extract(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dbd17",
   "metadata": {},
   "source": [
    "## TextBlob : TextBlob est une bibliothèque python et propose une API simple pour accéder à ses méthodes et effectuer des tâches NLP de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86155dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_names_TextBlob= TextBlob(references_clean)\n",
    "PROPN=[words for words, tag in final_names_TextBlob.tags if tag == \"NNP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cc696d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 662\n",
      "Ex: Anurag\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess\n",
    "final_names_TextBlob=PROPN\n",
    "#With preprocess\n",
    "print(\"Len with preprocess\",len(final_names_TextBlob))\n",
    "print(\"Ex:\",final_names_TextBlob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60bbf0",
   "metadata": {},
   "source": [
    "Also efficence but not useful here because spacy hasa better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f8eab",
   "metadata": {},
   "source": [
    "## API using NLTK package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccb418",
   "metadata": {},
   "source": [
    "### Service Docker (de cette API):\n",
    "\n",
    "Go in the main folder and type to build the image:\n",
    "\n",
    "    $ docker build -t pfr .\n",
    "\n",
    "Then run the container:\n",
    "\n",
    "    $ docker run -d --name mycontainer -p 80:80 pfr\n",
    "\n",
    "Interactive API docs: \n",
    "\n",
    "Now you can go to http://127.0.0.1/docs and try the API to analyse a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73cd41c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Names entities in text from NLTK ': ['Celtes', 'Wisigoths', 'Burgondes', 'Normands', 'Alamans', 'Francs', 'Aux']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "text=\"Aux groupes présents depuis le Paléolithique et le Néolithique, sont venues s'ajouter, à l'Âge du bronze et à l'Âge du fer, des vagues successives de Celtes, puis au iiie siècle de peuples germains (Francs, Wisigoths, Alamans, Burgondes) et au ixe siècle de scandinaves appelés Normands.\"\n",
    "\n",
    "response = requests.get(f'http://127.0.0.1/NLTK/NER?text={text}')\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4aa18ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dian Shao Yue Zhao Bo Dai Dahua Lin', 'Temporal', 'Gedas Bertasius Heng Wang Lorenzo Torresani', 'Joa Carreira Andrew Zisserman', 'Girshick', 'Yutong Chen Fangyun Wei Xiao Sun Zhirong Wu Stephen Lin', 'PAMI', 'Imitation', 'Yianilos Moritz Mueller Freitag Florian Hoppe Christian Thurau Ingo Bax Roland Memisevic', 'Sigurdsson Gu Varol', 'Karen Simonyan Andrew Zisserman', 'Necati Cihan Camgoz Simon Hadfield Oscar Koller Hermann Ney Richard Bowden', 'Isma Hadji Konstantinos', 'arXiv Learning', 'Mengmeng Xu Chen Zhao David', 'Brown Quanfu Fan Dan Gutfreund Carl Vondrick Aude Oliva', 'Crossmodel', 'AAAI', 'Chen Simon Kornblith Mohammad Norouzi Geoffrey', 'Xinlei Chen Haoqi Fan Ross', 'Daniel Neimark Omri Bar Maya Zohar Dotan Asselmann', 'Limin Wang Yuanjun Xiong Zhe Wang Yu Qiao Dahua Lin Xiaoou Tang Luc Van Gool', 'ICCVW', 'Ucf', 'Rui Qian Tianjian Meng Boqing Gong Ming Hsuan Yang', 'IEEE International Conference Robotics Automation ICRA', 'ArXiv', 'Timecontrastive', 'Jean Bastien Grill Florian Strub', 'Munos Michal Valko', 'Mathew Monfort Bolei Zhou Sarah Adel Bargal Alex Andonian Tom Yan Kandan Ramakrishnan Lisa', 'Shuffle', 'Anurag Arnab Mostafa Dehghani Georg Heigold Chen Sun Mario Lucic Cordelia Schmid', 'Wang Ali Farhadi Ivan Laptev Abhinav Gupta', 'Chang Juan Carlos Niebles', 'Yuxuan Liu Abhishek Gupta', 'Chang De An Huang Yanan Sui Li Fei Fei Juan Carlos Niebles', 'Jepson', 'Deep', 'Marcus Rohrbach Anna Rohrbach Michaela Regneri Sikandar Amin Mykhaylo Andriluka Manfred Pinkal Bernt Schiele', 'Sgdr Stochastic', 'Zeeshan Zia Quoc Huy Tran', 'Dtw Discriminative', 'Kenneth Li Xiao Sun Zhirong Wu Fangyun Wei Stephen Lin', 'ICML', 'ICLR', 'Zhirong Wu Yuanjun Xiong Stella Yu Dahua Lin', 'Mathilde Caron Ishan Misra Julien Mairal Priya Goyal Piotr Bojanowski Armand Joulin', 'NIPS', 'Debidatta Dwibedi Yusuf Aytar Jonathan Tompson Pierre Sermanet Andrew Zisserman', 'Bourdev Rob Fergus Lorenzo Torresani Manohar Paluri', 'arXiv', 'Gomez Lukasz Kaiser Illia Polosukhin', 'Towards', 'Hollywood', 'Speednet Learning', 'Weiyu Zhang Menglong Zhu Konstantinos', 'Derpanis Allan', 'Quo', 'Belongie Yin Cui', 'Christoph Feichtenhofer Haoqi Fan Bo Xiong Ross', 'Finegym', 'Sign', 'Wang Ross', 'CVPR', 'Stachniss Mu Li', 'Alexey Dosovitskiy Lucas Beyer Alexander Kolesnikov Dirk Weissenborn Xiaohua Zhai Thomas Unterthiner Mostafa Dehghani Matthias Minderer Georg Heigold Sylvain Gelly Jakob Uszkoreit Neil Houlsby', 'Christoph Feichtenhofer Haoqi Fan Jitendra Malik', 'Seco', 'Imagenet', 'Ilya Loshchilov Frank Hutter', 'IJCV', 'ECCV', 'Xiangyu Zhang Shaoqing Ren Jian Sun', 'Rojas Ali Thabet Bernard Ghanem', 'Freeman Michael Rubinstein Michal Irani Tali Dekel', 'ICCV', 'Richemond Elena Buchatskaya Carl Do', 'Yao Yiheng Zhang Zhaofan Qiu Yingwei Pan Tao Mei', 'Video', 'Haofei Kuang Yi Zhu Zhi Zhang Xinyu Li Joseph Tighe So', 'Lawrence Zitnick Martial Hebert', 'From', 'Derpanis', 'Fangyun Wei Yue Gao Zhirong Wu Han Hu Stephen Lin', 'Fabian Caba Heilbron', 'Raghav Goyal Samira Ebrahimi Kahou', 'Nicolas Carion Francisco Massa Gabriel Synnaeve Nicolas Usunier Alexander Kirillov Sergey Zagoruyko', 'Girshick Abhinav Gupta', 'Joint', 'Abbeel Sergey Levine', 'Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan', 'Hilde Kuehne Ali Bilgin Arslan Thomas Serre', 'Bootstrap', 'Khurram Soomro Amir Roshan Zamir Mubarak Shah', 'Neural', 'Bernardo', 'Pierre Sermanet Corey Lynch Yevgen Chebotar Jasmine Hsu Eric Jang Stefan Schaal Sergey Levine', 'Sagie Benaim Ariel Ephrat Oran Lang Inbar Mosseri William', 'Pires Zhaohan Daniel Guo Mohammad Gheshlaghi Azar Bilal Piot Koray Kavukcuoglu Re', 'Wang Serge', 'Self', 'Kaidi Cao Jingwei Ji Zhangjie Cao', 'Improved', 'Jia Deng Wei Dong Richard Socher Li Jia Li Kai Li Fei Fei Li', 'Ashish Vaswani Noam', 'Ishan Misra', 'Necati Cihan Camgoz Oscar Koller Simon Hadfield Richard Bowden', 'Tengda Han Weidi Xie Andrew Zisserman', 'Moments', 'Gunnar', 'Yinghao Xu Fangyun Wei Xiao Sun Ceyuan Yang Yujun Shen Bo Dai Bolei Zhou Stephen Lin', 'Non', 'Hinton', 'Advances Neural Information Processing Systems', 'Sanjay Haresh Sateesh Kumar Huseyin Coskun Shahram Najam Syed Andrey Konin', 'Few', 'Vivit', 'Slowfast']\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(f'http://127.0.0.1/NLTK/NER?text={references_clean}')\n",
    "result_API=response.json()\n",
    "result_API=print(result_API['Names entities in text from NLTK '])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb67165",
   "metadata": {},
   "source": [
    "## Apply Spacy functions on all pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b585bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding all the pdf text in UTF-8\n",
    "\n",
    "elements=[]\n",
    "for i in array_pdf_text:\n",
    "    elements.append(i.encode(\"utf-8\"))\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4129a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All function in one\n",
    "\n",
    "entities=[]\n",
    "def main():\n",
    "    for text in elements:\n",
    "        temp=after_references(str(text))\n",
    "        temp=preprocess_text(temp)\n",
    "        temp=extract(temp)\n",
    "        entities.append(temp)\n",
    "    return entities\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897bff1",
   "metadata": {},
   "source": [
    "## Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164621c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready import *\n",
    "\n",
    "onto_path.append(\"owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto = Ontology(\"http://test.org/onto.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class References(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class quoted_by(Property):\n",
    "    ontolgy = onto\n",
    "    domain = [References]\n",
    "    range = [Author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.sax.saxutils import escape\n",
    "\n",
    "def invalid_xml_remove(char):\n",
    "    \"\"\"Tracks illegal unicode characters\"\"\"\n",
    "    #http://stackoverflow.com/questions/1707890\n",
    "    # /fast-way-to-filter-illegal-xml-unicode-chars-in-python\n",
    "    illegal_unichrs = [ (0x00, 0x08), (0x0B, 0x1F), (0x7F, 0x84), (0x86, 0x9F),\n",
    "                    (0xD800, 0xDFFF), (0xFDD0, 0xFDDF), (0xFFFE, 0xFFFF),\n",
    "                    (0x1FFFE, 0x1FFFF), (0x2FFFE, 0x2FFFF), (0x3FFFE, 0x3FFFF),\n",
    "                    (0x4FFFE, 0x4FFFF), (0x5FFFE, 0x5FFFF), (0x6FFFE, 0x6FFFF),\n",
    "                    (0x7FFFE, 0x7FFFF), (0x8FFFE, 0x8FFFF), (0x9FFFE, 0x9FFFF),\n",
    "                    (0xAFFFE, 0xAFFFF), (0xBFFFE, 0xBFFFF), (0xCFFFE, 0xCFFFF),\n",
    "                    (0xDFFFE, 0xDFFFF), (0xEFFFE, 0xEFFFF), (0xFFFFE, 0xFFFFF),\n",
    "                    (0x10FFFE, 0x10FFFF) ]\n",
    "\n",
    "    illegal_ranges = [f\"{chr(low)}-{chr(high)}\"\n",
    "                  for (low, high) in illegal_unichrs\n",
    "                  if low < sys.maxunicode]\n",
    "\n",
    "    illegal_xml_re = re.compile(f'[{\"\".join(illegal_ranges)}]')\n",
    "    if illegal_xml_re.search(char) is not None:\n",
    "        #Replace with space\n",
    "        return ''\n",
    "    else:\n",
    "        return char\n",
    "\n",
    "def clean_char(char):\n",
    "    \"\"\"\n",
    "    Function for remove invalid XML characters from\n",
    "    incoming data.\n",
    "    \"\"\"\n",
    "    #Get rid of the ctrl characters first.\n",
    "    #http://stackoverflow.com/questions/1833873/python-regex-escape-characters\n",
    "    char = re.sub('\\x1b[^m]*m', '', char)\n",
    "    #Clean up invalid xml\n",
    "    char = invalid_xml_remove(char)\n",
    "    replacements = [\n",
    "        ('\\u201c', '\\\"'),\n",
    "        ('\\u0141', '\\\"'),\n",
    "        ('\\u201d', '\\\"'),\n",
    "        (\"\\u001B\", ''), #http://www.fileformat.info/info/unicode/char/1b/index.htm\n",
    "        (\"\\u0019\", ''), #http://www.fileformat.info/info/unicode/char/19/index.htm\n",
    "        (\"\\u0016\", ''), #http://www.fileformat.info/info/unicode/char/16/index.htm\n",
    "        (\"\\u001C\", ''), #http://www.fileformat.info/info/unicode/char/1c/index.htm\n",
    "        (\"\\u0003\", ''), #http://www.utf8-chartable.de/unicode-utf8-table.pl?utf8=0x\n",
    "        (\"\\u000C\", ''),\n",
    "        (\"\\u03b1\", ''),\n",
    "        (\"u\\u039C\", ''),\n",
    "        (\"\\u03C3\", ''),\n",
    "        (\"\\u0141\", ''),\n",
    "        (\"\\u0308\", ''),\n",
    "        (\"\\u2032\", ''),\n",
    "        (\"\\u03b8\", '')\n",
    "\n",
    "    \n",
    "    ]\n",
    "    for rep, new_char in replacements:\n",
    "        if char == rep:\n",
    "            return new_char\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_value(text: str):\n",
    "        \"\"\"Escape the illegal characters for an ontology property\"\"\"\n",
    "        if text is None:\n",
    "            return None\n",
    "        # function to escape XML character data\n",
    "        text = escape(text)\n",
    "        text = text.replace('\\n', '')\n",
    "        text = text.replace('\\r', '')\n",
    "        text = text.replace('\\f', '')\n",
    "        text = text.replace('\\b', '')\n",
    "        text = text.replace('\"', '')\n",
    "        text = text.replace('[', '')\n",
    "        text = text.replace(']', '')\n",
    "        text = text.replace('{', '')\n",
    "        text = text.replace('}', '')\n",
    "        text = text.replace('#', '')\n",
    "        text = text.replace('|', '')\n",
    "        text = text.replace(' ', '_')\n",
    "        text = clean_char(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(array_authors)):\n",
    "    for j in range(len(array_authors[i])):\n",
    "        aut_num = Author(escape_value(array_authors[i][j]))\n",
    "        for z in range(len(entities[i])):\n",
    "            ref_num = References(escape_value(entities[i][z]))\n",
    "            ref_num.quoted_by.append(aut_num)\n",
    "            print(ref_num.quoted_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e8710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "onto.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472064be",
   "metadata": {},
   "source": [
    "### To conlude, we will change the xml encoding to UTF-8 otherwise, Protégé may not read the file ont.owl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
