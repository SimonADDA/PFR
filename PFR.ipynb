{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126a7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfminer.six\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install nameparser\n",
    "# !pip install pdfminer\n",
    "# !pip install refextract\n",
    "# !pip install pdfx\n",
    "# !pip install -U textblob\n",
    "# !pip install owlready\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2ac646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "* Owlready * Creating new ontology owl <http://www.w3.org/2002/07/owl>.\n",
      "* Owlready * Creating new ontology 22-rdf-syntax-ns <http://www.w3.org/1999/02/22-rdf-syntax-ns>.\n",
      "* Owlready * Creating new ontology rdf-schema <http://www.w3.org/2000/01/rdf-schema>.\n",
      "* Owlready * Creating new ontology XMLSchema <http://www.w3.org/2001/XMLSchema>.\n",
      "* Owlready * Creating new ontology anonymous <http://anonymous>.\n",
      "* Owlready * Creating new ontology owlready_ontology <http://www.lesfleursdunormal.fr/static/_downloads/owlready_ontology.owl>.\n",
      "* Owlready *     ...loading ontology owlready_ontology from C:\\Users\\Admin\\anaconda3\\envs\\PFR1\\lib\\site-packages\\owlready\\owlready_ontology.owl...\n"
     ]
    }
   ],
   "source": [
    "#Import of librairies\n",
    "\n",
    "#ARVIX\n",
    "import arxiv\n",
    "\n",
    "#PDFTEXT\n",
    "import urllib.request\n",
    "# import pdftotext\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "\n",
    "# Prevent future/deprecation warnings from showing in output\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#Local\n",
    "import os\n",
    "import requests as r\n",
    "import sys\n",
    "\n",
    "#AI\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk.tag.stanford as st\n",
    "from nltk.tag.stanford import StanfordNERTagger \n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "import boto3\n",
    "\n",
    "from owlready import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf6f98",
   "metadata": {},
   "source": [
    "## Download pdf file from Arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63be17ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203.16533v1\n",
      "2203.16531v1\n",
      "2203.16530v1\n",
      "2203.16529v1\n",
      "2203.16528v1\n"
     ]
    }
   ],
   "source": [
    "#You can choose here how many pdf you want. Here 5 because it is enough to see the result\n",
    "max_results=5\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = \"Computer Science & AI\",\n",
    "    max_results = max_results,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "for result in search.results():\n",
    "    print(result.pdf_url[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b3ba48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link:  ['http://arxiv.org/pdf/2203.16533v1', 'http://arxiv.org/pdf/2203.16531v1', 'http://arxiv.org/pdf/2203.16530v1', 'http://arxiv.org/pdf/2203.16529v1', 'http://arxiv.org/pdf/2203.16528v1']\n",
      "Authors:  [['Dengpan.Fu', 'Dongdong.Chen', 'Hao.Yang', 'Jianmin.Bao', 'Lu.Yuan', 'Lei.Zhang', 'Houqiang.Li', 'Fang.Wen', 'Dong.Chen'], ['Shengyi.Qian', 'Linyi.Jin', 'Chris.Rockwell', 'Siyi.Chen', 'David.F..Fouhey'], ['Yuliang.Zou', 'Zizhao.Zhang', 'Chun.Liang.Li', 'Han.Zhang', 'Tomas.Pfister', 'Jia.Bin.Huang'], ['Jiahui.Lei', 'Kostas.Daniilidis'], ['Osman.Erman.Okman', 'Mehmet.Gorkem.Ulkar', 'Gulnur.Selda.Uyanik']]\n",
      "Title:  ['Large-Scale Pre-training for Person Re-identification with Noisy Labels', 'Understanding 3D Object Articulation in Internet Videos', 'Learning Instance-Specific Adaptation for Cross-Domain Segmentation', 'CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism', 'L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors']\n"
     ]
    }
   ],
   "source": [
    "#Store in array all informations on pdf (title, authors and links)\n",
    "\n",
    "array_link=[]\n",
    "array_authors=[]\n",
    "array_title=[]\n",
    "\n",
    "for result in search.results():\n",
    "    array_link.append(result.pdf_url)\n",
    "    array_title.append(result.title)\n",
    "    temp=result.authors\n",
    "    array_authors.append([re.sub(\"[^A-Za-z0-9]\",\".\",str(i)) for i in temp])\n",
    "    \n",
    "    #Download pdf as link.pdf\n",
    "    result.download_pdf(dirpath=\".\\Download\", filename=f'{result.pdf_url[21:]}.pdf')\n",
    "    \n",
    "print(\"Link: \",array_link)\n",
    "print(\"Authors: \",array_authors)\n",
    "print(\"Title: \",array_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a5ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(value):\n",
    "    with open(f'Download/{value}.pdf', \"rb\") as f:\n",
    "        text = extract_text(f)\n",
    "        return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c06a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Large-Scale Pre-training for Person Re-identification with Noisy Labels\\n\\nDengpan Fu1 Dongdong Chen3 Hao Yang2\\nLei Zhang4 Houqiang Li1\\n\\nLu Yuan3\\n\\nJianmin Bao2*\\n\\nFang Wen 2 Dong Chen2\\n\\n1University of Science and Technology of China\\n\\nfdpan@mail.ustc.edu.cn\\n\\n2Microsoft Research, 3Microsoft Cloud AI, 4IDEA\\nlihq@ustc.edu.cn\\n\\ncddlyf@gmail.com\\n\\n{jianbao,haya,luyuan,fangwen,doch}@microsoft.com, leizhang@idea.edu.cn\\n\\nAbstract\\n\\nThis paper aims to address the problem of pre-training\\nfor person re-identification (Re-ID) with noisy labels. To\\nsetup the pre-training task, we apply a simple online multi-\\nobject tracking system on raw videos of an existing un-\\nlabeled Re-ID dataset “LUPerson” and build the Noisy\\nLabeled variant called “LUPerson-NL”. Since theses ID\\nlabels automatically derived from tracklets inevitably con-\\ntain noises, we develop a large-scale Pre-training frame-\\nwork utilizing Noisy Labels (PNL), which consists of three\\nlearning modules: supervised Re-ID learning, prototype-\\nbased contrastive learning, and label-guided contrastive\\nlearning. In principle, joint learning of these three mod-\\nules not only clusters similar examples to one prototype,\\nbut also rectifies noisy labels based on the prototype as-\\nsignment. We demonstrate that learning directly from raw\\nvideos is a promising alternative for pre-training, which\\nutilizes spatial and temporal correlations as weak super-\\nvision. This simple pre-training task provides a scalable\\nway to learn SOTA Re-ID representations from scratch on\\n“LUPerson-NL” without bells and whistles. For example,\\nby applying on the same supervised Re-ID method MGN,\\nour pre-trained model improves the mAP over the unsu-\\npervised pre-training counterpart by 5.7%, 2.2%, 2.3% on\\nCUHK03, DukeMTMC, and MSMT17 respectively. Under\\nthe small-scale or few-shot setting, the performance gain is\\neven more significant, suggesting a better transferability of\\nthe learned representation. Code is available at https:\\n//github.com/DengpanFu/LUPerson-NL.\\n\\n1. Introduction\\n\\nA large high-quality labeled dataset for person re-\\nidentification (Re-ID) is labor intensive and costly to cre-\\nate. Existing fully labeled datasets [25, 52, 58, 61] for per-\\nson Re-ID are all of limited scale and diversity compared\\nto other vision tasks. Therefore, model pre-training be-\\n\\n*Corresponding author.\\n\\n(a) Market1501 with MGN\\n\\n(b) Market1501 with IDE\\n\\n(c) DukeMTMC with MGN\\n\\n(d) DukeMTMC with IDE\\n\\nFigure 1. Comparing person Re-ID performances of three pre-\\ntrained models on two methods (IDE [59] and MGN [51]). Re-\\nsults are reported on Market1501 and DukeMTC, with different\\nscales under the small-scale setting. IN.sup. refers to the model\\nsupervised pre-trained on ImageNet, LUP.unsup. is the model un-\\nsupervised pre-trained on LUPserson, and LUPnl.pnl. is the model\\npre-trained on our LUPerson-NL dataset using our proposed PNL.\\n\\ncomes a crucial approach to achieve good Re-ID perfor-\\nmance. However, due to the lack of large-scale Re-ID\\ndataset, most previous methods simply use the models pre-\\ntrained on the crowd-labeled ImageNet dataset, resulting in\\na limited improvement because of the big domain gap be-\\ntween generic images in ImageNet and person-focused im-\\nages desired by the Re-ID task. To mitigate this problem,\\nthe recent work [12] has demonstrated that unsupervised\\npre-training on a web-scale unlabeled Re-ID image dataset\\n“LUPerson” (sub-sampled from massive streeview videos)\\nsurpasses that of pre-training on ImageNet.\\n\\nIn this paper, our hypothesis is that scalable ReID pre-\\ntraining methods that learn directly from raw videos can\\ngenerate better representations. To verify it, we propose the\\nnoisy labels guided person Re-ID pre-training, which lever-\\n\\n\\x0cages the spatial and temporal correlations in videos as weak\\nsupervision. This supervision is nearly cost-free, and can be\\nachieved by the tracklets of a person over time derived from\\nany multi-object tracking algorithm, such as [56]. In par-\\nticular, we track each person in consecutive video frames,\\nand automatically assign the tracked persons in the same\\ntracklet to the same Re-ID label and vice versa. Enabled by\\nthe large amounts of raw videos in LUPerson [12], publicly\\navailable data of this form on the internet, we create a new\\nvariant named “LUPerson-NL” with derived pseudo Re-ID\\nlabels from tracklets for pre-training with noisy labels. This\\nvariant totally consists of 10M person images from 21K\\nscenes with noisy labels of about 430K identities.\\n\\nWe demonstrate that contrastive pre-training of Re-ID is\\nan effective method of learning from this weak supervision\\nat large scale. This new Pre-training framework utilizing\\nNoisy Labels (PNL) composes three learning modules: (1)\\na simple supervised learning module directly learns from\\nRe-ID labels through classification; (2) a prototype-based\\ncontrastive learning module helps cluster instances to the\\nprototype which is dynamically updated by moving aver-\\naging the centroids of instance features, and progressively\\nrectify the noisy labels based on the prototype assignment.\\nand (3) a label-guided contrastive learning module utilizes\\nthe rectified labels subsequently as the guidance. In contrast\\nto the vanilla momentum contrastive learning [7,12,19] that\\ntreats only features from the same instance as positive sam-\\nples, our label-guided contrastive learning uses the rectified\\nlabels to distinguish positive and negative samples accord-\\ningly, leading to a better performance. In principle, joint\\nlearning of these three modules make the consistency be-\\ntween the prototype assignment from instances and the high\\nconfident (rectified) labels, as possible as it can.\\n\\nThe experiments show that our PNL model achieves re-\\nmarkable improvements on various person Re-ID bench-\\nmarks. Figure 1 indicates that the performance gain from\\nour pre-trained models is consistent on different scales of\\ntraining data. For example, upon the strong MGN [51]\\nimproves the mAP by\\nbaseline, our pre-trained model\\n4.4%, 4.9% on Market1501 and DukeMTMC over the Im-\\nageNet supervised one, and 0.9%, 2.2% over the unsuper-\\nvised pre-training baseline [12]. Moreover, the gains are\\neven larger under the small-scale and few-shot settings,\\nwhere the labeled Re-ID data are extremely limited. To the\\nbest of our knowledge, we are the first to show that large-\\nscale noisy label guided pre-training can significantly ben-\\nefit person Re-ID task.\\n\\nOur key contributions can be summarized as follows:\\n\\n• We propose noisy label guided pre-training for person Re-\\nID, which incorporates supervised learning, prototype-\\nbased contrastive learning, label-guided contrastive learn-\\ning and noisy label rectification to a unified framework.\\n• We construct a large-scale noisy labeled person Re-ID\\n\\ndataset “LUPerson-NL” as a new variant of “LUPerson”.\\nIt is by far the largest noisy labeled person Re-ID dataset\\nwithout any human labeling effort.\\n\\n• Our models pre-trained on LUPerson-NL push the state-\\nof-the-art results on various public benchmarks to a new\\nlimit without bells and whistles.\\n\\n2. Related Work\\nSupervised Person Re-ID. Most studies of person Re-ID\\nemploy supervised learning. Some [6, 21, 55] introduce a\\nhard triplet loss on the global feature, ensuring a closer fea-\\nture distance for the same identity, while some [45, 59, 60]\\nimpose classification loss to learn a global feature from\\nthe whole image. There are also some other works that\\nlearn part-based local features with separate classification\\nlosses. For example, Suh et al. [46] presented part-aligned\\nbi-linear representations and Sun et al. [48] represented fea-\\ntures as horizontal strips. Recent approaches investigate\\nlearning invariant features concerning views [34], resolu-\\ntions [31], poses [32], domains [22,23], or exploiting group-\\nwise losses [36] or temporal information [18, 27] to im-\\nprove performance. The more advantageous results on pub-\\nlic benchmarks are achieved by MGN [51], which learns\\nboth global and local features with multiple losses. In [40],\\nQian et al further demonstrated the potential of generating\\ncross-view images for person re-indentification conditioned\\non normalized poses. In this paper, we focus on model pre-\\ntraining, and our pre-trained models can be applied to these\\nrepresentative methods and boost their performance.\\nUnsupervised Person Re-ID. To alleviate the lack of pre-\\ncise annotations, some works resort to unsupervised train-\\ning on unlabeled datasets. For example, MMCL [49] for-\\nmulates unsupervised person Re-ID as a multi-label classi-\\nfication to progressively seek true labels. BUC [33] jointly\\noptimizes the network and the sample relationship with\\na bottom-up hierarchical clustering. MMT [14] collabo-\\nratively trains two networks to refine both hard and soft\\npseudo labels. SpCL [15] designs a hybrid memory to\\nunify the representations for clustering and instance-wise\\ncontrastive learning. Both MMT [14] and SpCL [15] rely\\non explicit clustering of features from the whole training\\nset, making them quite inefficient on large datasets like\\nMSMT17. Since the appearance ambiguity is difficult to\\naddress without supervision, these unsupervised methods\\nhave limited performance. One alternative to address this\\nissue is introducing model pre-training on large scale data.\\nInspired by the success of self-supervised representation\\nlearning [4,5,7,17,19,28,53], Fu et al. [12] proposed a large\\nscale unlabeled Re-ID dataset, LUPerson, and illustrated\\nthe effectiveness of its unsupervised pre-trained models. In\\nthis work, we further try to make use of noisy labels from\\nvideo tracklets to improve the pre-training quality through\\nlarge-scale weakly-supervised pre-training.\\nWeakly Supervised Person Re-ID. Several approaches\\n\\n\\x0calso employ weak supervision in person Re-ID training.\\nInstead of requiring bounding boxes within each frame,\\nMeng et al. [38] rely on precise video-level labels, which\\nreduces annotation cost but still need manual efforts to la-\\nbel videos. On the contrary, we resort to noisy labels that\\ncan be automatically generated from tracklets on a much\\nlarger scale. Some [8, 29, 50] also leverage tracklets to su-\\npervise the training of Re-ID tasks. But unlike these ap-\\nproaches, we are proposing a large-scale pre-training strat-\\negy for person Re-ID, by both building a new very large-\\nscale dataset and devising a new pre-training framework:\\nthe new dataset, LUPerson-NL, is even larger than LU-\\nPerson [12] and has large amount of noisy Re-ID labels;\\nThe new framework, PNL, combines supervised learning,\\nlabel-guided contrastive learning and prototype based con-\\ntrastive learning to exploit the knowledge under large-scale\\nnoise labels. Most importantly, our pre-trained models have\\ndemonstrated remarkable performance and generalization\\nability, helping achieve state-of-the-art results superior to\\nall existing methods on public person Re-ID benchmarks.\\n3. LUPerson-NL: LUPerson With Noisy Labels\\nSupervised models based on deep networks are always\\ndata-hungry, but the labeled data they rely on are expen-\\nsive to acquire.\\nIt is a tremendous issue for person Re-\\nID task, since the human labelers need to check across\\nmultiple views to ensure the correctness of Re-ID labels.\\nThe data shortage is partially alleviated by a recently pub-\\nlished dataset, LUPerson [12], a dataset of unlabeled per-\\nson images with a significantly larger scale than previous\\nperson Re-ID datasets. Unsupervised pre-trained models\\n[12] on LUPerson have demonstrated remarkable effective-\\nness without utilizing additional manual annotations, which\\narouses our curiosity: can we further improve the perfor-\\nmance of pre-training directly by utilizing temporal cor-\\nrelation as weak supervision? To verify this, we build a\\nnew variant of LUPerson on top of the raw videos from\\nLUPerson and assign label to each person image with au-\\ntomatically generated tracklet. We name it LUPerson-NL\\nIt consists of 10M\\nwith NL standing for Noisy Labels.\\nimages with about 430K identities collected from 21K\\nscenes. To our best knowledge, this is the largest person\\nRe-ID dataset constructed without human labelling by far.\\nOur LUPerson-NL will be released for scientific research\\nonly, while any usage for other purpose is forbidden.\\n3.1. Constructing LUPerson-NL\\n\\nWe utilize the off-the-shelf tracking algorithm [56] 1 to\\ndetect persons and extract person tracklets from the same\\nraw videos of [12]. We assign each tracklet with a unique\\nclass label. The detection is not perfect: e.g. the bounding\\nboxes may only cover partial bodies without heads or upper\\nparts. Human pose estimation [47] is thus appended that\\n\\n1FairMOT: https://github.com/ifzhang/FairMOT\\n\\nFigure 2.\\n(X, Y ) indicates Y % of identities each has less than X images.\\n\\nIdentity distribution of LUPerson-NL. A curve point\\n\\nFigure 3. Besides the correctly labeled identities as shown by (a),\\nthere are two types of labeling errors in LUPerson-NL. Noise-I:\\nsame person labeled as different identities, e.g. D, E and F shown\\nin (b). Noise-II: different persons labeled as the same identity,\\ne.g. G shown in (c).\\n\\nhelps filter out imperfect boxes by predicting landmarks.\\n\\nWe track every person in the video frame by frame. In\\norder to guarantee both the sufficiency and diversity, we\\nadopt the following strategy:\\ni) We first remove the per-\\nson identities that appear in too few frames, i.e. no more\\nthan 200; ii) Within the tracklet of each identity, we then\\nperform sampling with a rate of one image per 20 frames\\nto reduce the number of duplicated images. Thus we can\\nmake sure that there would be at least 10 images associat-\\ning to each identity. Through this filtering procedure, we\\nhave collected 10, 683, 716 images of 433, 997 identities in\\ntotal. They belong to 21, 697 videos which are less than\\nthe videos that [12] uses, due to our extra filtering strat-\\negy for more reliable identity labels. Thus, LUPerson-NL\\nis very different from LUPerson, as it adopts very different\\nsampling and post-processing strategies, not to mention the\\nnoisy labels driven from the spatial-temporal information.\\n3.2. Properties of LUPerson-NL\\n\\nLUPerson-NL is advantageous in following aspects:\\nLarge amount of images and identities. We detail the\\nstatistics of existing popular person Re-ID datasets in Table\\n1. As we can see, the proposed LUPerson-NL, with over\\n10M images and 433K noisy labeled identities, is the sec-\\nond largest among the listed. Indeed, SYSU30K has more\\nimages, but it extracts images from only 1K TV program\\nvideos frame by frame, making it less competitive in vari-\\nability and less compatible in practice, the pre-training per-\\nformance comparison can be found at supplementary mate-\\nrials. Besides, LUPerson-NL was constructed without hu-\\nman labeling effort, making it more suitable to scale-up.\\n\\n102030405060708090#ofpersonimages0255075100%ofidentities𝐴𝐵𝐶(a) Correctly labeled identities(b) Noise-I(c) Noise-II𝐷𝐸𝐹𝐺\\x0cDatasets\\nVIPeR [16]\\nGRID [35]\\nCUHK03 [30]\\nMarket [58]\\nAirport [25]\\nDukeMTMC [61]\\nMSMT17 [52]\\nSYSU30K [50]\\nLUPerson [12]\\nLUPerson-NL\\n\\n#scene\\n#images\\n2\\n1,264\\n8\\n1,275\\n2\\n14, 096\\n6\\n32, 668\\n6\\n39, 902\\n8\\n36, 411\\n15\\n126, 441\\n1,000\\n29,606,918\\n46, 260\\n4, 180, 243\\n10, 683, 716 21, 697 ≃ 433, 997\\n\\n#persons\\n632\\n1,025\\n1, 467\\n1, 501\\n9, 651\\n1, 852\\n4, 101\\n30,508\\n> 200k\\n\\nlabeled environment\\n\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\n\\n-\\nsubway\\ncampus\\ncampus\\nairport\\ncampus\\ncampus\\n\\nweakly TV program\\n\\nno\\nnoisy\\n\\nvary\\nvary\\n\\ncamera view\\nfixed\\nfixed\\nfixed\\nfixed\\nfixed\\nfixed\\nfixed\\ndynamic\\ndynamic\\ndynamic\\n\\ndetector\\nhand\\nhand\\nDPM [11]+hand\\nDPM [11]+hand\\nACF [10]\\nHand\\nFasterRCNN [42]\\nYOLOv2\\nYOLOv5\\nFairMOT [56]\\n\\ncrop size\\n128 × 48\\nvary\\nvary\\n128 × 64\\n128 × 64\\nvary\\nvary\\nvary\\nvary\\nvary\\n\\nTable 1. Comparing statistics among existing popular Re-ID datasets. LUPerson-NL is by far the largest Re-ID dataset with better diversity\\nwithout human labeling effort. SYSU30K is partly annotated by human annotator.\\n\\n4. PNL: Pre-training with Noisy Labels for\\n\\nPerson Re-ID\\n\\nBased on the new LUPerson-NL dataset with large scale\\nnoisy labels, we devise a novel Pretraining framework with\\nNoisy Labels for person Re-ID, namely PNL.\\n\\nDenote all\\n\\nthe data samples from LUPerson-NL as\\n{(xi, yi)}n\\ni=1, with n being the size of the dataset, xi a\\nperson image and yi ∈ {1, . . . , K} its associated identity\\nlabel. Here K represents the number of all identities that\\nare recorded in LUPerson-NL.\\n\\nInspired by recent methods [4, 5, 7, 17, 19, 28], our PNL\\nframework adopts Siamese networks that have been fully in-\\nvestigated for contrastive representation learning. As shown\\nby Figure 4, given an input person image xi, we first per-\\nform two randomly selected augmentations (T, T ′), pro-\\nducing two augmented images ( ˜xi, ˜x′\\ni). We feed one of\\nthem, ˜xi, into an encoder Eq to get a query feature qi;\\nwhile the other one, ˜x′\\ni, is fed into another encoder Ek\\nto get a key feature ki. Following [19], we design Ek to\\nbe a momentum version of Eq, i.e. the two encoders Ek\\nand Eq share the same network structure, but with different\\nweights. The weights in Ek are exponential moving aver-\\nages of the weights in Eq. During training, weights of Ek\\nare refreshed through a momentum update from Eq. And\\nthe detailed algorithm can be found at supplementary mate-\\nrials.\\n4.1. Supervised Classification\\nSince the raw labels {yi}n\\n\\ni=1 in LUPerson-NL contain\\nlots of noises as illustrated in previous section, they have\\nto be rectified during training. Let ˆyi be the rectified label\\nof image xi. As long as ˆyi is given, it would be intuitive\\nthat we train classification based on the corrected label ˆyi.\\nIn particular, we would append a classifier to transform the\\nfeature from Eq into probabilities pi ∈ RK with K being\\nthe number of classes. Then we impose a classification loss\\n\\nLi\\n\\nce = − log(pi[ˆyi]).\\n\\n(1)\\n\\nHowever, the acquisition of ˆyi is not straight-forward.\\nWe resort to prototypes, the moving averaged centroids of\\nfeatures from training instances, to accomplish this task.\\n\\nFigure 4. The overview of our PNL framework.\\nIt comprises\\na supervised classification module, a prototype based contrastive\\nlearning module, and a label-guided contrastive learning module.\\n\\nBalanced distribution of identities. We illustrate the cu-\\nmulative percentage of identities with respect to the number\\nof their corresponding person images as a curve in Figure 2.\\nA point (X, Y ) on the curve represents that there are in total\\nY % identities in LUPerson-NL, that each of them has less\\nthan X images. It can be observed that: i) about 75% of all\\nthe identities in LUPerson-NL have a person image num-\\nber within [10, 25]; ii) the percentage of identities that have\\nmore than 50 person images each, occupy only a very small\\nportion of about 6.4% (27, 767/433, 997) in LUPerson-NL.\\nThese observations all show that our LUPerson-NL is well\\nbalanced in terms of identity distribution, making it a suit-\\nable dataset for person Re-ID tasks.\\n\\nIn spite of our dedicatedly designed tracking and filter-\\ning strategies as proposed in Sec 3.1, the identity labels we\\nobtained can never be very accurate due to the technical up-\\nper bounds of current tracking methods. Figure 3 visualizes\\nthe two noise types in LUPerson-NL that are caused by dif-\\nferent labeling errors, which are Noise-I, where the same\\nperson is split into different tracklets and is mistaken as dif-\\nferent persons; and Noise-II, that different persons are\\nrecognized as the same person.\\n\\nfc𝒙!𝑇𝑇\"#𝒙!#𝒙!\"𝐸#𝐸$𝑦!&𝑦!𝒌!!,𝒌!\",…,𝒌!#$𝑦!!,$𝑦!\",…,$𝑦!#&𝑦!prototypesqueue𝒒!𝒄\",𝒄#,…,𝒄$Label Guided Contrastive LossClassificationLoss𝒌!Label Update&𝑦!Prototype BasedContrastive Loss𝒑!update prototypesenqueuemomentum updatew. gradientw/o. gradient𝒔!\\x0c(2)\\n\\n(3)\\n\\n4.2. Label Rectification with Prototypes\\n\\nAs depicted by Figure 4, we maintain prototypes as a dic-\\ntionary of feature vectors {c1, c2, . . . , cK}, where K is the\\nnumber of identities, ck ∈ Rd is a prototype representing a\\nclass-wise feature centroid. In each training step, we would\\nfirst evaluate the similarity score sk\\ni between the query fea-\\nture qi and each of the current prototypes ck by\\n\\nsk\\ni =\\n\\nexp(qi · ck/τ )\\nk=1 exp(qi · ck/τ )\\n\\n.\\n\\n(cid:80)K\\n\\nLet pi be the classification probability given by the clas-\\nsifier with weights updated in the previous step. The rec-\\ntified label ˆyi for this step is then generated by combining\\nboth the prototype scores si = {sk\\nk=1 and the classifica-\\ntion probability pi as\\n\\ni }K\\n\\nli =\\n\\n(pi + si),\\n\\n1\\n2\\n(cid:40)\\n\\nˆyi =\\n\\narg maxj lj\\nyi\\n\\ni\\n\\nif maxj lj\\notherwise.\\n\\ni > T ,\\n\\nHere we compute a soft pseudo label li and convert it to a\\nhard one ˆyi based on a threshold T . If the highest score in li\\nis larger than T , the corresponding class would be selected\\nas ˆyi, otherwise the original raw label yi would be kept.\\n\\n4.3. Prototype Based Contrastive Learning\\n\\nThe newly rectified label ˆyi can then be used to super-\\nvise the cross-entropy loss Li\\nce for classification as formu-\\nlated by Equation 1. Besides, it also helps train prototypes\\nck in return. In specific, we propose a prototype based con-\\ntrastive loss Li\\npro to constrain that the feature of each sam-\\nple should be closer to the prototype it belongs to. We for-\\nmulate the loss as\\n\\nLi\\n\\npro = −log\\n\\nexp(qi · cˆyi/τ )\\nj=1 exp(qi · cj/τ )\\n\\n,\\n\\n(cid:80)K\\n\\n(4)\\n\\nwith qi being the query feature from Eq, τ being a hyper-\\n\\nparameter representing temperature.\\n\\nAll the prototypes are maintained as a dictionary, with\\n\\nstep-wise updates following a momentum mechanism as\\ncˆyi = mcˆyi + (1 − m)qi.\\n\\n(5)\\n\\n4.4. Label-Guided Contrastive Learning\\n\\nInstance-wise contrastive learning proved to be very ef-\\nfective in self-supervised learning [4, 5, 7, 17, 19]. It learns\\ninstance-level feature discrimination by encouraging simi-\\nlarity among features from the same instance, while pro-\\nmoting dissimilarity between features from different in-\\nstances. The instance-wise contrastive loss is given by\\n\\nLi\\n\\nic = −log\\n\\nexp(qi · k+\\ni /τ ) + (cid:80)M\\n\\ni /τ )\\nj=1 exp(qi · k−\\n\\nj /τ )\\n\\n, (6)\\n\\nexp(qi · k+\\n\\nwith qi being the query feature of current instance i.\\nk+\\ni (= ki) is the positive key feature generated from the\\nmomentum encoder Ek. It is marked positive since it shares\\n∗ ∈ Rd, on the contrary, are\\nthe same instance with qi. k−\\nthe rest features stored in a queue that represent negative\\nsamples. The queue has a size of M . At the end of each\\ntraining step, the queue would be updated by en-queuing\\nthe new key feature and de-queuing the oldest one.\\n\\nSuch instance-level contrastive learning is far from per-\\nfect, as it neglects the relationships among different in-\\nstances. For example, even though two instances depict the\\nsame person, it would still strengthen the gap between their\\nInstead, we propose a label guided contrastive\\nfeatures.\\nlearning module, making use of the rectified labels ˆyi to\\nensure more reasonable grouping of contrastive pairs.\\n\\nWe redesign the queue to additionally record labels ˆyi.\\nRepresented by Q = [(kjt, ˆyjt)]M\\nt=1, our new queue accepts\\nnot only a key feature ki but also its rectified label ˆyi during\\nupdate. These newly recorded labels help better distinguish\\npositive and negative pairs. Let P(i) be the new set of posi-\\ntive features and N (i) the new set of negative features: fea-\\ntures in P(i) share the same rectified label with the current\\ninstance i while features in N (i) do not. Our label guided\\ncontrastive loss can be given by\\n\\nLi\\n\\nlgc =\\n\\n−1\\n|P(i)|\\n\\nlog\\n\\nexp\\n\\n(cid:17)\\n\\n(cid:16) qi·k+\\nτ\\n\\n(cid:80)\\nk+∈P(i)\\n(cid:16) qi·k+\\nτ\\n\\n(cid:17)\\n\\n(cid:17) ,\\n\\n(cid:16) qi·k−\\nτ\\n\\n+ (cid:80)\\nk−∈N (i)\\n\\nexp\\n\\n(cid:80)\\nk+∈P(i)\\n\\nexp\\n\\n(7)\\n\\nwith\\n\\nP(i) = {kjt|ˆyjt = ˆyi, ∀(kjt, ˆyjt) ∈ Q} ∪ {ki},\\nN (i) = {kjt|ˆyjt ̸= ˆyi, ∀(kjt, ˆyjt) ∈ Q},\\n\\n(8)\\n\\nwhere ki and ˆyi are the key feature and the rectified label of\\nthe current instance i.\\n\\nFinally we combine all the components above to pre-\\n\\ntrain models on LUPerson-NL with the following loss\\nce + λproLi\\n\\npro + λlgcLi\\n\\nLi = Li\\n\\nlgc.\\n\\n(9)\\n\\nWe set λpro = λlgc = 1 during training.\\n\\n5. Experiments\\n\\n5.1. Implementation\\n\\nHyper-parameter settings. We set the hyper-parameters\\nτ = 0.1 and T = 0.8. The momentum m for updating\\nboth the momentum encoder Ek and the prototypes is set\\nto 0.999. More hyper-parameters exploration and training\\ndetails can be found at supplementary materials.\\nDataset and protocol. We conduct extensive experiments\\non four popular person Re-ID datasets: CUHK03, Market,\\n\\n\\x0cpre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\npre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\nTrip [21]\\n45.2/63.8\\n55.5/61.2\\n62.6/67.6\\n69.1/73.1\\n\\nTrip [21]\\n65.2/80.7\\n65.4/81.1\\n69.8/83.1\\n71.0/84.7\\n\\nIDE [59]\\n50.6/55.9\\n52.5/57.7\\n57.6/62.3\\n68.3/73.5\\n\\nIDE [59]\\n62.8/80.8\\n63.4/81.6\\n65.9/82.2\\n70.3/85.0\\n\\nMGN [51]\\n70.5/71.2\\n67.1/67.0\\n74.7/75.4\\n80.4/80.9\\n\\nMGN [51]\\n79.4/89.0\\n79.5/89.1\\n82.1/91.0\\n84.3/92.0\\n\\npre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\npre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\nTrip [21]\\n76.2/89.7\\n75.1/88.5\\n79.8/71.5\\n81.2/91.4\\n\\nTrip [21]\\n34.3/54.8\\n34.4/55.4\\n36.6/57.1\\n41.4/61.6\\n\\nIDE [59]\\n74.1/90.2\\n74.5/89.3\\n77.9/91.0\\n82.4/92.8\\n\\nIDE [59]\\n36.2/66.2\\n37.6/67.3\\n39.8/68.9\\n44.0/72.0\\n\\nMGN [51]\\n87.5/95.1\\n88.2/95.3\\n91.0/96.4\\n91.9/96.6\\n\\nMGN [51]\\n63.7/85.1\\n62.7/84.3\\n65.7/85.5\\n68.0/86.0\\n\\n(a) CUHK03\\n\\n(b) Market1501\\n\\n(c) DukeMTMC\\n\\n(d) MSMT17\\n\\nTable 2. Comparing three supervised Re-ID baselines using different pre-trained models. “IN sup.”/“IN unsup.” indicates model that is\\nsupervisely/unsupervisely pre-trained on ImageNet; “LUP unsup.” is the model unsupervisely pre-trained on LUPerson; “LUPnl pnl.“\\nrefers to the model that pre-trained on LUPerson-NL using our PNL framework. All results are shown in mAP/cmc1.\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\n10%\\n53.1/76.9\\n58.4/81.7\\n64.6/85.5\\n72.4/88.8\\n\\n30%\\n75.2/90.8\\n76.6/91.9\\n81.9/93.7\\n85.2/94.2\\n\\n10%\\n45.1/65.3\\n48.1/66.9\\n53.5/72.0\\n60.6/75.8\\n\\n30%\\n64.7/80.2\\n65.8/80.2\\n69.4/81.9\\n74.5/86.3\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\n10%\\n23.2/50.2\\n22.6/48.8\\n25.5/51.1\\n28.2/51.1\\n\\n30%\\n41.9/70.8\\n40.4/68.7\\n44.6/71.4\\n47.7/71.2\\n\\nsmall-scale\\n50%\\n81.5/93.5\\n82.0/94.1\\n85.8/94.9\\n88.3/95.5\\n\\nsmall-scale\\n50%\\n71.8/84.6\\n72.5/84.4\\n75.6/86.7\\n78.8/88.3\\n\\nsmall-scale\\n50%\\n50.3/76.9\\n49.0/75.0\\n53.0/77.7\\n55.5/77.2\\n\\n70%\\n84.8/94.5\\n85.4/94.5\\n88.8/95.9\\n90.1/96.2\\n\\n90%\\n86.9/95.2\\n87.4/95.5\\n90.5/96.4\\n91.3/96.4\\n\\n10%\\n21.1/41.8\\n18.6/36.1\\n26.4/47.5\\n42.0/61.6\\n\\n30%\\n68.1/87.6\\n69.3/87.8\\n78.3/92.1\\n83.7/94.0\\n\\n(a) Market1501\\n\\n70%\\n75.5/86.8\\n76.3/86.9\\n78.9/88.2\\n81.6/89.5\\n\\n90%\\n78.0/88.3\\n78.5/88.7\\n81.1/90.0\\n83.3/91.2\\n\\n10%\\n31.5/47.1\\n32.4/48.0\\n35.8/50.2\\n52.2/64.1\\n\\n30%\\n65.4/79.8\\n65.3/80.2\\n72.3/83.8\\n77.7/87.9\\n\\n(b) DukeMTMC\\n\\n70%\\n56.9/81.2\\n55.7/79.9\\n59.5/81.8\\n61.6/81.8\\n\\n90%\\n61.9/84.2\\n60.9/83.0\\n63.7/85.0\\n66.1/84.8\\n\\n10%\\n14.7/34.1\\n13.2/29.2\\n17.0/36.0\\n24.5/42.7\\n\\n30%\\n44.5/71.1\\n41.4/67.1\\n49.0/73.6\\n53.2/74.4\\n\\nfew-shot\\n50%\\n80.2/92.8\\n78.3/90.9\\n84.2/93.9\\n88.1/95.2\\n\\nfew-shot\\n50%\\n73.9/85.7\\n73.7/85.1\\n77.7/87.4\\n81.1/89.6\\n\\nfew-shot\\n50%\\n56.2/79.5\\n53.3/77.6\\n57.4/80.5\\n62.2/81.0\\n\\n70%\\n84.2/94.0\\n84.4/94.1\\n88.4/95.5\\n90.5/96.3\\n\\n90%\\n86.7/94.6\\n87.1/95.2\\n90.4/96.3\\n91.6/96.4\\n\\n70%\\n77.2/87.8\\n77.7/87.8\\n80.8/89.2\\n83.2/91.1\\n\\n90%\\n79.1/88.8\\n79.4/89.0\\n82.0/90.6\\n84.1/91.3\\n\\n70%\\n60.9/82.8\\n59.1/81.5\\n62.9/83.5\\n65.8/83.8\\n\\n90%\\n63.4/84.5\\n62.4/83.8\\n65.0/85.1\\n67.4/85.3\\n\\n(c) MSMT17\\nTable 3. Comparing pre-trained models on three labeled Re-ID datasets, under the small-scale setting and the few-shot setting, with\\ndifferent usable data percentages. “LUPnl pnl.” is our model pre-trained on LUPerson-NL using PNL. Results are shown in mAP/cmc1.\\n\\nDukeMTMC and MSMT17. We adopt their official set-\\ntings, except CUHK03 where its labeled counterpart with\\nnew protocols proposed in [62] is used. We follow the stan-\\ndard evaluation metrics: the mean Average Precision (mAP)\\nand the Cumulated Matching Characteristics top-1 (cmc1).\\n5.2. Improving Supervised Re-ID\\n\\nTo evaluate our pre-trained model based on LUPerson-\\nNL with respect to supervised person Re-ID tasks. we\\nconduct experiments using three representative supervised\\nRe-ID baselines with different pre-training models. These\\nbaseline methods include two simpler approaches driven\\nonly by the triplet loss (Trip [21]) or the classification loss\\n(IDE [59]), as well as a stronger and more complex method\\nMGN [51] that use both triplet and classification losses.\\n\\n{“IN”, “LUP”, “LUPnl”} represent ImageNet [43], LU-\\nPerson [12] and our LUPerson-NL respectively; while the\\n{“sup.”, “unsup.”, “pnl.”} stand for the {“supervised”, “un-\\nsupervised”, and “pretrain with noisy label”} pre-training\\nmethods. e.g. the “LUPnl pnl.” in the bottom rows of Ta-\\nble 2 all refer to our model, which is pre-trained on our\\nLUPerson-NL dataset using our PNL framework.\\n\\nFrom Table 2 we can see, for all of the three base-\\nline methods, our pre-trained model improves their perfor-\\nmances greatly on the four popular person Re-ID datasets.\\nSpecifically, the improvements are at least 5.7%, 0.9%,\\n1.2% and 2.3% in terms of mAP on CUHK03, Market1501,\\nDukeMTMC and MSMT17 respectively.\\n\\nWe report results in Table 2, where the abbreviations\\n\\nNote that even though the performance of the baseline\\n\\n\\x0cMGN on Market1501 has been extremely high, our model\\nstill brings considerable improvement over it. The other\\nway around, our pre-trained models obtain more signifi-\\ncant improvements on relatively weak methods (Trip and\\nIDE), unveiling that model initialization plays a critical part\\nin person Re-ID training.\\n\\nOur noisy label guided pre-training models are also sig-\\nnificantly advantageous over the previous “LUPerson un-\\nsup“ models, which emphasizes the superiority of our PNL\\nframework and our LUPerson-NL dataset.\\n\\n5.3. Improving Unsupervised Re-ID Methods\\n\\nOur pre-trained model can also benefit unsupervised per-\\nson Re-ID methods. Based on the state-of-the-art unsuper-\\nvised method SpCL [15], we explore different pre-training\\nmodels utilizing two settings proposed by SpCL: the pure\\nunsupervised learning (USL) and the unsupervised domain\\nadaptation (UDA). Results in Table 4 illustrate that our pre-\\ntrained model outperforms the others in all UDA tasks, as\\nwell as the USL task on DukeMTMC dataset. In the USL\\ntask on Market1501, we achieve the second best scores\\nslightly lower than the LUPerson model [12].\\n\\n5.4. Comparison on Small-scale and Few-shot\\n\\nFollowing the same protocols proposed by [12], we con-\\nduct experiments under two small data settings: the small-\\nscale setting and the few-shot setting. The small-scale set-\\nting restricts the percentage of usable identities, while the\\nfew-shot setting restricts the percentage of usable person\\nimages each identity has. Under both settings, we vary\\nthe usable data percentages of three popular datasets from\\n10% ∼ 100%. We compare different pre-trained models\\nunder these settings with MGN as the baseline method. The\\nresults shown in Table 3 verify the consistent improvements\\nbrought by our model on all the datasets under both settings.\\nBesides, the results in Table 3 show that the gains of\\nour pre-trained models are even larger under a more lim-\\nited amount of labeled data. For example, under the “small-\\nscale” setting, our model outperforms “LUPerson unsup”\\nby 7.8%, 7.1% and 2.7% on Market1501, DukeMTMC and\\nMSMT17 respectively with 10% identities. The improve-\\nments rise to 15.6%, 16.4% and 6.5% under the “few-shot”\\nsetting with 10% person images.\\n\\nMost importantly, our pre-trained “LUPnl pnl” model\\nhelps achieve advantageous results with a mAP of 72.4 and\\na cmc1 of 88.8, using only 10% labeled data from the Mar-\\nket1501 training set. The task is really challenging, con-\\nsidering that the training set composes only 1, 170 images\\nbelonging to 75 identities; while evaluations are performed\\non a much larger testing set with 19, 281 images belong-\\ning to 750 identities. We consider these results extremely\\nappealing as they demonstrate the strong potential of our\\npre-trained models in real-world applications.\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\nUSL\\n\\nM\\n72.4/87.8\\n72.9/88.6\\n76.2/90.2\\n75.6/89.3\\n\\nD\\n64.9/80.3\\n62.6/78.8\\n67.1/81.6\\n68.1/82.0\\n\\nUDA\\n\\nD → M\\n76.4/90.1\\n77.1/90.6\\n79.2/91.7\\n80.7/92.2\\n\\nM → D\\n67.9/82.3\\n66.3/81.6\\n69.1/83.2\\n72.2/84.9\\n\\nTable 4. Performances of different pre-trained models on the un-\\nsupervised Re-ID method SpCL [15] under two unsupervised task\\nsettings: the pure unsupervised learning (USL) and the unsuper-\\nvised domain adaptation (UDA). Here M and D refer to the Mar-\\nket1501 dataset and the DukeMTMC dataset respectively.\\n\\nmethod\\nMSMT17\\n\\nSupCont [26]\\n66.5/84.7\\n\\nLUP [12]\\n65.3/84.0\\n\\nPNL(ours)\\n68.0/86.0\\n\\n20%\\n\\nTable 5. Performance comparison for different pre-training meth-\\nods on LUPerson-NL dataset.\\n# ce ic pro lgc\\n1 ✓\\n✓\\n2\\n3 ✓ ✓\\n4 ✓\\n5 ✓\\n6 ✓ ✓ ✓\\n7 ✓\\n\\n40%\\n32.0/56.1 45.0/69.5 62.7/83.0\\n34.5/59.5 47.9/72.6 65.3/84.0\\n37.6/62.6 49.6/73.5 66.5/84.7\\n35.7/59.1 48.5/72.4 65.8/84.1\\n✓ 38.5/63.0 50.9/74.5 67.1/85.2\\n39.0/63.4 51.7/74.4 67.4/85.4\\n✓ ✓ 39.6/63.7 51.9/75.0 68.0/86.0\\n\\n100%\\n\\n✓\\n\\nTable 6. Ablating components of PNL on MSMT with data per-\\ncentages 20%, 40% and 100% under the small scale setting. ce:\\nsupervised classification; ic:\\ninstance-wise contrastive learning;\\npro: prototypes for both prototype-based contrastive learning and\\nlabel rectification; lgc: label-guided contrastive learning.\\n\\n5.5. Comparison with other pre-training methods\\n\\nWe compare our proposed PNL with some other popu-\\nlar pre-training methods in Table 5. LUP [12] is a varient\\nof MoCoV2 for person Re-ID based on unsupervised con-\\nstrastive learning, while SupCont [26] considers both su-\\npervised and constrastive learning. Our PNL outperforms\\nall these rep-resentative pre-training methods, indicating the\\nsuperiority of our proposed method.\\n5.6. Ablation Study\\n\\nWe also investigate the effectiveness of each designed\\ncomponent in PNL through ablation experiments. Results\\nshown by Table 6 illustrate the efficacy of our proposed\\ncomponents. We have the following observations: i) Train-\\ning with an instance-wise contrastive loss Li\\nic (row 2) with-\\nout using any labels leads to even better performance than\\ntraining with a classification loss Li\\nce (row 1) that utilizes\\nthe labels from LUPerson-NL, implying that the noisy la-\\nbels in LUPerson-NL would misguide representation learn-\\nii) Jointly training\\ning if directly adopted as supervision.\\nwith both losses Li\\nic (row 3) improves over us-\\ning only one loss (row 1, row 2), suggesting that learning\\ninstance-wise discriminative representations complements\\niii) The prototypes which contribute to\\nlabel supervision.\\n\\nce and Li\\n\\n\\x0csetting\\n\\nMSMT17\\n\\nce+pro\\n\\nce+pro+lgc\\n\\nw/o. lc\\n64.8/83.4\\n\\nw. lc\\n65.8/84.1\\n\\nw/o. lc\\n66.7/85.0\\n\\nw. lc\\n68.0/86.0\\n\\nTable 7. Ablating the label correction. lc: “label correction”.\\n\\nMethod\\nMGN† [51] (2018)\\nBOT [37] (2019)\\nDSA [57] (2019)\\nAuto [41] (2019)\\nABDNet [3] (2019)\\nSCAL [2] (2019)\\nMHN [1] (2019)\\nBDB [9] (2019)\\nSONA [54] (2019)\\nGCP [39] (2020)\\nSAN [24] (2020)\\nISP [63] (2020)\\nGASM [20] (2020)\\nESNET [44] (2020)\\nLUP [12](2020)\\nOurs+MGN\\nOurs+BDB\\n\\nCUHK03 Market1501 DukeMTMC MSMT17\\n63.7/85.1\\n70.5/71.2\\n-\\n-\\n-\\n75.2/78.9\\n52.5/78.2\\n73.0/77.9\\n60.8/82.3\\n-\\n-\\n72.3/74.8\\n-\\n72.4/77.2\\n-\\n76.7/79.4\\n-\\n79.2/81.8\\n-\\n75.6/77.9\\n55.7/79.2\\n76.4/80.1\\n-\\n74.1/76.5\\n52.5/79.5\\n-\\n57.3/80.5\\n-\\n65.7/85.5\\n79.6/81.9*\\n68.0/86.0\\n80.4/80.9\\n82.3/84.7\\n53.4/79.0\\n\\n79.4/89.0\\n76.4/86.4\\n74.3/86.2\\n-\\n78.6/89.0\\n79.6/89.0\\n77.2/89.1\\n76.0/89.0\\n78.3/89.4\\n78.6/87.9\\n75.5/87.9\\n80.0/89.6\\n74.4/88.3\\n78.7/88.5\\n82.1/91.0\\n84.3/92.0\\n79.0/89.2\\n\\n87.5/95.1\\n85.9/94.5\\n87.6/95.7\\n85.1/94.5\\n88.3/95.6\\n89.3/95.8\\n85.0/95.1\\n86.7/95.3\\n88.8/95.6\\n88.9/95.2\\n88.0/96.1\\n88.6/95.3\\n84.7/95.3\\n88.6/95.7\\n91.0/96.4\\n91.9/96.6\\n88.4/95.4\\n\\nTable 8. Comparison with the state of the art. Numbers of MGN†\\ncome from a re-implementation based on FastReID, which are\\neven better than the original. Numbers of PNL marked by * are ob-\\ntained on BDB, the rest without the * mark are obtained on MGN.\\nWe show best scores in bold and the second scores underlined.\\n\\n5.8. Comparison with State-of-the-Art Methods\\n\\nWe compare our results with current state-of-the-art\\nmethods on four public benchmarks. We don’t apply any\\npost-processing techniques such as IIA [13] and RR [62].\\nTo ensure fairness, we adopt ResNet50 as our backbone and\\ndoes not compare with methods that rely on stronger back-\\nbones (results with stronger backbones e.g. ResNet101 can\\nbe found at supplementary materials). Results in Table 8\\nverify the remarkable advantage brought by our pre-trained\\nmodels. Without bells and whistles, we achieve state-of-\\nthe-art performance on all four benchmarks, outperforming\\nthe second with clear margins.\\n\\n6. Conclusion\\n\\nIn this paper, we demonstrate that large-scale Re-ID\\nrepresentation can be directly learned from massive raw\\nvideos by leveraging the spatial and temporal information.\\nWe not only build a large-scale noisy labeled person Re-\\nID dataset LUPerson-NL based on tracklets of raw videos\\nfrom LUPerson without using manual annotations, but also\\ndesign a novel weakly supervised pretraining framework\\nPNL comprising different learning modules including su-\\npervised learning, prototypes-based learning, label-guided\\ncontrastive learning and label rectification. Equipped with\\nour pre-trained models, we push existing benchmark results\\nto a new limit, which outperforms unsupervised pre-trained\\nmodels and ImageNet supervised pre-trained models by a\\n\\n(a) Correction for Noise-I\\n\\n(b) Correction for Noise-II\\n\\nFigure 5. Visualizing the label correction functionality of our PNL\\nframework with respect to the two noise types from LUPerson-\\nNL. Person images in the same rectangle indicate that they are\\nrecognized as the same identity. The right-hand similarity matrices\\nare calculated based on the image features all learned using our\\nPNL framework with label correction.\\n\\nboth prototype-based contrastive learning and the label cor-\\nrection, are very important under various settings, as veri-\\nfied by comparing row 1 with row 4; row 3 with row 6; and\\nrow 5 with row 7. iv) Our label-guided contrastive learning\\ncomponent consistently outperforms the vanilla instance-\\nwise contrastive learning under various settings, as verified\\nby comparing row 3 with row 5, and row 6 with row 7. v)\\nCombining all our components (supervised classification,\\nprototypes and label-guided contrastive learning) together\\nleads to the best performance as shown by row 7.\\n5.7. Label Correction\\n\\nOur PNL can indeed correct noisy labels. We demon-\\nstrate two typical examples in Figure 5 visualizing the la-\\nbel corrections with respect to the two kinds of noises. As\\nwe can see, in Figure 5a the same person are marked as\\nthree different persons in LUPerson-NL due to Noise-I\\nin labels. After our PNL pre-training, these three track-\\nlets are merged together since their trained features are very\\nclose, as verified by the right-hand similarity matrix. In Fig-\\nure 5b different persons are labeled as the same identity\\nin LUPerson-NL due to Noise-II in labels. After PNL\\ntraining, these mis-labeled person identities are all correctly\\nre-grouped into two identities, which can also be reflected\\nby the right-hand similarity matrix.\\n\\nwe also ablate the label correction module in Table 7\\nwith different settings, and observe it can improve the per-\\nIt also validates the importance of combining\\nformance.\\nlabel rectification with label-guided contrastive learning to-\\ngether, where more accurate positive/negative pairs can be\\nleveraged.\\n\\nBeforeAfter2103876954Similarity MatrixFinal ID=179919BeforeAfter2103876954Similarity MatrixFinal ID=419043Final ID=73264\\x0clarge margin.\\nAcknowledgement. This work is partially supported by\\nthe National Natural Science Foundation of China (NSFC,\\n61836011).\\n\\nReferences\\n\\n[1] Binghui Chen, Weihong Deng, and Jiani Hu. Mixed high-\\norder attention network for person re-identification. In Pro-\\nceedings of the IEEE International Conference on Computer\\nVision, pages 371–381, 2019. 8\\n\\n[2] Guangyi Chen, Chunze Lin, Liangliang Ren, Jiwen Lu, and\\nSelf-critical attention learning for person re-\\nJie Zhou.\\nIn Proceedings of the IEEE International\\nidentification.\\nConference on Computer Vision, pages 9637–9646, 2019. 8\\n[3] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang\\nChen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-\\nIn Pro-\\nnet: Attentive but diverse person re-identification.\\nceedings of the IEEE International Conference on Computer\\nVision, pages 8351–8361, 2019. 8\\n\\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. arXiv preprint arXiv:2002.05709,\\n2020. 2, 4, 5\\n\\n[5] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\\nNorouzi, and Geoffrey Hinton. Big self-supervised mod-\\narXiv preprint\\nels are strong semi-supervised learners.\\narXiv:2006.10029, 2020. 2, 4, 5\\n\\n[6] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi\\nHuang. Beyond triplet loss: a deep quadruplet network for\\nIn Proceedings of the IEEE con-\\nperson re-identification.\\nference on computer vision and pattern recognition, pages\\n403–412, 2017. 2\\n\\n[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\\nImproved baselines with momentum contrastive learning.\\narXiv preprint arXiv:2003.04297, 2020. 2, 4, 5\\n\\n[8] Zhirui Chen, Jianheng Li, and Wei-Shi Zheng. Weakly su-\\npervised tracklet person re-identification by deep feature-\\narXiv preprint arXiv:1910.14333,\\nwise mutual learning.\\n2019. 3\\n\\n[9] Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu,\\nand Ping Tan. Batch dropblock network for person re-\\nidentification and beyond. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pages 3691–3701,\\n2019. 8\\n\\n[10] Piotr Doll´ar, Ron Appel, Serge Belongie, and Pietro Per-\\nIEEE\\nona. Fast feature pyramids for object detection.\\ntransactions on pattern analysis and machine intelligence,\\n36(8):1532–1545, 2014. 4\\n\\n[11] Pedro F Felzenszwalb, Ross B Girshick, David McAllester,\\nand Deva Ramanan. Object detection with discriminatively\\nIEEE transactions on pattern\\ntrained part-based models.\\nanalysis and machine intelligence, 32(9):1627–1645, 2009.\\n4\\n\\nings of the IEEE conference on computer vision and pattern\\nrecognition, 2021. 1, 2, 3, 4, 6, 7, 8\\n\\n[13] Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen,\\nJianmin Bao, Gang Hua, and Houqiang Li. Improving person\\nre-identification with iterative impression aggregation. IEEE\\nTransactions on Image Processing, 29:9559–9571, 2020. 8\\n[14] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-\\nteaching: Pseudo label refinery for unsupervised domain\\nadaptation on person re-identification. In International Con-\\nference on Learning Representations, 2019. 2\\n\\n[15] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hong-\\nsheng Li. Self-paced contrastive learning with hybrid mem-\\nory for domain adaptive object re-id. In Advances in Neural\\nInformation Processing Systems, 2020. 2, 7\\n\\n[16] Douglas Gray and Hai Tao. Viewpoint invariant pedes-\\ntrian recognition with an ensemble of localized features. In\\nEuropean conference on computer vision, pages 262–275.\\nSpringer, 2008. 4\\n\\n[17] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\\nnew approach to self-supervised learning. arXiv preprint\\narXiv:2006.07733, 2020. 2, 4, 5\\n\\n[18] Xinqian Gu, Bingpeng Ma, Hong Chang, Shiguang Shan,\\nand Xilin Chen. Temporal knowledge propagation for image-\\nIn Proceedings of the\\nto-video person re-identification.\\nIEEE/CVF International Conference on Computer Vision,\\npages 9647–9656, 2019. 2\\n\\n[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\nGirshick. Momentum contrast for unsupervised visual rep-\\nresentation learning. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n9729–9738, 2020. 2, 4, 5\\n\\n[20] Lingxiao He and Wu Liu. Guided saliency feature learning\\nfor person re-identification in crowded scenes. In European\\nConference on Computer Vision, pages 357–373. Springer,\\n2020. 8\\n\\n[21] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-\\nfense of the triplet loss for person re-identification. arXiv\\npreprint arXiv:1703.07737, 2017. 2, 6\\n\\n[22] Yan Huang, Qiang Wu, JingSong Xu, and Yi Zhong. Sbsgan:\\nSuppression of inter-domain background shift for person re-\\nidentification. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision, pages 9527–9536, 2019. 2\\n[23] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li\\nZhang. Style normalization and restitution for generalizable\\nIn Proceedings of the IEEE/CVF\\nperson re-identification.\\nConference on Computer Vision and Pattern Recognition,\\npages 3143–3152, 2020. 2\\n\\n[24] Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, and\\nZhibo Chen. Semantics-aligned representation learning for\\nperson re-identification. In AAAI, pages 11173–11180, 2020.\\n8\\n\\n[12] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu\\nYuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsu-\\npervised pre-training for person re-identification. Proceed-\\n\\n[25] Srikrishna Karanam, Mengran Gou, Ziyan Wu, Angels\\nRates-Borras, Octavia Camps, and Richard J Radke. A\\ncomprehensive evaluation and benchmark for person re-\\n\\n\\x0cidentification: Features, metrics, and datasets. arXiv preprint\\narXiv:1605.09653, 2(3):5, 2016. 1, 4\\n\\n[26] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\\nDilip Krishnan. Supervised contrastive learning. Advances\\nin Neural Information Processing Systems, 33, 2020. 7\\n[27] Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, and Shiliang\\nZhang. Global-local temporal representations for video per-\\nson re-identification. In Proceedings of the IEEE/CVF Inter-\\nnational Conference on Computer Vision, pages 3958–3967,\\n2019. 2\\n\\n[28] Junnan Li, Caiming Xiong, and Steven Hoi. Mopro: Webly\\nsupervised learning with momentum prototypes. In Interna-\\ntional Conference on Learning Representations, 2020. 2, 4\\n[29] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsuper-\\nvised tracklet person re-identification. IEEE transactions on\\npattern analysis and machine intelligence, 42(7):1770–1782,\\n2019. 3\\n\\n[30] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-\\nreid: Deep filter pairing neural network for person re-\\nIn Proceedings of the IEEE conference on\\nidentification.\\ncomputer vision and pattern recognition, pages 152–159,\\n2014. 4\\n\\n[31] Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, and\\nYu-Chiang Frank Wang. Recover and identify: A generative\\ndual model for cross-resolution person re-identification. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 8090–8099, 2019. 2\\n\\n[32] Yu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank\\nWang. Cross-dataset person re-identification via unsuper-\\nvised pose disentanglement and adaptation. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vi-\\nsion, pages 7919–7929, 2019. 2\\n\\n[33] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi\\nYang. A bottom-up clustering approach to unsupervised per-\\nIn Proceedings of the AAAI Confer-\\nson re-identification.\\nence on Artificial Intelligence, volume 33, pages 8738–8745,\\n2019. 2\\n\\n[34] Fangyi Liu and Lei Zhang. View confusion feature learning\\nfor person re-identification. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 6639–\\n6648, 2019. 2\\n\\n[35] Chen Change Loy, Chunxiao Liu, and Shaogang Gong. Per-\\nson re-identification by manifold ranking. In 2013 IEEE In-\\nternational Conference on Image Processing, pages 3567–\\n3571. IEEE, 2013. 4\\n\\n[36] Chuanchen Luo, Yuntao Chen, Naiyan Wang, and Zhaoxi-\\nang Zhang. Spectral feature transformation for person re-\\nidentification. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision, pages 4976–4985, 2019. 2\\n[37] Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao,\\nShenqi Lai, and Jianyang Gu. A strong baseline and batch\\nnormalization neck for deep person re-identification. IEEE\\nTransactions on Multimedia, 2019. 8\\n\\n[38] Jingke Meng, Sheng Wu, and Wei-Shi Zheng. Weakly su-\\nIn Proceedings of the\\npervised person re-identification.\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 760–769, 2019. 3\\n\\n[39] Hyunjong Park and Bumsub Ham. Relation network for per-\\nson re-identification. In Proceedings of the AAAI Conference\\non Artificial Intelligence, volume 34, pages 11839–11847,\\n2020. 8\\n\\n[40] Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie\\nQiu, Yang Wu, Yu-Gang Jiang, and Xiangyang Xue. Pose-\\nnormalized image generation for person re-identification. In\\nProceedings of the European conference on computer vision\\n(ECCV), pages 650–667, 2018. 2\\n\\n[41] Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, and Yi\\nYang. Auto-reid: Searching for a part-aware convnet for\\nperson re-identification. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pages 3750–3759,\\n2019. 8\\n\\n[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. In Advances in neural information pro-\\ncessing systems, pages 91–99, 2015. 4\\n\\n[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, et al.\\nImagenet large\\nscale visual recognition challenge. International journal of\\ncomputer vision, 115(3):211–252, 2015. 6\\n\\n[44] Dong Shen, Shuai Zhao, Jinming Hu, Hao Feng, Deng Cai,\\nand Xiaofei He. Es-net: Erasing salient parts to learn more\\nin re-identification. IEEE Transactions on Image Processing,\\n2020. 8\\n\\n[45] Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen,\\nand Xiaogang Wang. Person re-identification with deep\\nsimilarity-guided graph neural network. In Proceedings of\\nthe European conference on computer vision (ECCV), pages\\n486–504, 2018. 2\\n\\n[46] Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Ky-\\noung Mu Lee. Part-aligned bilinear representations for per-\\nson re-identification. In Proceedings of the European Con-\\nference on Computer Vision (ECCV), pages 402–419, 2018.\\n2\\n\\n[47] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\\nhigh-resolution representation learning for human pose esti-\\nmation. In CVPR, 2019. 3\\n\\n[48] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\\nWang. Beyond part models: Person retrieval with refined\\npart pooling (and a strong convolutional baseline). In ECCV,\\n2018. 2\\n\\n[49] Dongkai Wang and Shiliang Zhang. Unsupervised person re-\\nidentification via multi-label classification. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 10981–10990, 2020. 2\\n\\n[50] Guangrun Wang, Guangcong Wang, Xujie Zhang, Jianhuang\\nLai, Zhengtao Yu, and Liang Lin. Weakly supervised per-\\nson re-id: Differentiable graphical learning and a new bench-\\nmark. IEEE Transactions on Neural Networks and Learning\\nSystems, 32(5):2142–2156, 2020. 3, 4\\n\\n[51] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\\nZhou. Learning discriminative features with multiple granu-\\nlarities for person re-identification. In 2018 ACM Multime-\\ndia Conference on Multimedia Conference, pages 274–282.\\nACM, 2018. 1, 2, 6, 8\\n\\n\\x0c[52] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\\nPerson transfer gan to bridge domain gap for person re-\\nIn Proceedings of the IEEE Conference on\\nidentification.\\nComputer Vision and Pattern Recognition, pages 79–88,\\n2018. 1, 4\\n\\n[53] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\nUnsupervised feature learning via non-parametric instance\\nIn Proceedings of the IEEE Conference\\ndiscrimination.\\non Computer Vision and Pattern Recognition, pages 3733–\\n3742, 2018. 2\\n\\n[54] Bryan Ning Xia, Yuan Gong, Yizhe Zhang, and Christian\\nPoellabauer. Second-order non-local attention networks for\\nperson re-identification. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pages 3760–3769,\\n2019. 8\\n\\n[55] Ye Yuan, Wuyang Chen, Yang Yang, and Zhangyang Wang.\\nIn defense of the triplet loss again: Learning robust person\\nre-identification with fast approximated triplet loss and label\\ndistillation. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition Workshops, pages\\n354–355, 2020. 2\\n\\n[56] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\\nand Wenyu Liu. Fairmot: On the fairness of detection and\\nre-identification in multiple object tracking. arXiv e-prints,\\npages arXiv–2004, 2020. 2, 3, 4\\n\\n[57] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo\\nChen. Densely semantically aligned person re-identification.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 667–676, 2019. 8\\n\\n[58] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\\ndong Wang, and Qi Tian. Scalable person re-identification:\\nA benchmark. 2015 IEEE International Conference on Com-\\nputer Vision (ICCV), pages 1116–1124, 2015. 1, 4\\n\\n[59] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan\\nChandraker, Yi Yang, and Qi Tian. Person re-identification\\nin the wild. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition, pages 1367–1376,\\n2017. 1, 2, 6\\n\\n[60] Zhedong Zheng, Liang Zheng, and Yi Yang. A discrimi-\\nnatively learned cnn embedding for person reidentification.\\nACM Transactions on Multimedia Computing, Communica-\\ntions, and Applications (TOMM), 14(1):1–20, 2017. 2\\n[61] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-\\nples generated by gan improve the person re-identification\\nbaseline in vitro. In Proceedings of the IEEE International\\nConference on Computer Vision, 2017. 1, 4\\n\\n[62] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-\\nranking person re-identification with k-reciprocal encoding.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 1318–1327, 2017. 6, 8\\n[63] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao\\nWang. Identity-guided human semantic parsing for person\\nre-identification. ECCV, 2020. 8\\n\\n\\x0cIn this material, we will 1) show demo cases for scene\\nchanging for specific person in LUPerson-NL, 2) share the\\ntraining details for PNL, 3) provide the detailed algorithm\\ntable for PNL, 4) compare models pre-trained on SYSU30K\\nand LUPerson-NL 5) analyze the hyper parameters of our\\nPNL, 6) demonstrate results of our method using a stronger\\nbackbones, 7) explore the impact of pre-training image\\nscale, and 8) list more detailed results for our “small-scale”\\nand “few-shot” experiments. All these experiments are un-\\nder MGN settings.\\n\\nA. Scene changing in LUPerson-NL\\n\\nOur LUPerson-NL are driven from street view videos,\\nFigure 6 shows a demo person in our LUPerson-NL. As we\\ncan see, our LUPerson-NL is able to cover multiple scenes,\\nsince the videos we use have lots of moving cameras and\\nmoving persons, and only traklets with ≥ 200 frames are\\nselected. It is approximate close to the real person Re-ID\\nscenario.\\n\\nC. Algorithm for PNL\\n\\nAlgorithm 1 shows the procedure of training PNL, we\\ntrain our framework for 90 epochs, and apply label correc-\\ntion from 10 epochs. Once the rectification begins, we keep\\nrectifying for every iteration.\\n\\nAlgorithm 1: PNL algorithm.\\n\\nInput: total epochs N , correction start epochs Ns,\\nprototype feature vectors {⃗c1, ⃗c2, . . . , ⃗cK}, where\\nK is the number of identities, temperature τ ,\\nthreshold T , momentum m, encoder network\\nEq(·), classifier h(·), momentum encoder Ek(·),\\nloss weight λpro and λlgc.\\nfor epoch = 1 : N do\\n\\n{(⃗xi, yi)}b\\nfor i ∈ {1, ..., b} do\\n\\ni=1 sampled from data loader.\\n\\n˜⃗xi = aug1(⃗xi), ˜⃗x′\\ni = aug2(⃗xi)\\n⃗qi = Eq(˜⃗xi), ⃗ki = Ek(˜⃗x′\\ni)\\n⃗pi = h(⃗qi)\\nk=1, sk\\ni }K\\n⃗si = {sk\\n⃗li = (⃗pi + ⃗si)/2\\nif epoch > Ns and maxk lj\\n\\n(cid:80)K\\n\\ni = exp( ⃗qi·⃗ck/τ )\\n\\nk=1 exp(⃗qi·⃗ck/τ )\\n\\ni > T then\\n\\nˆyi = arg maxj\\n\\n⃗lj\\ni\\n\\nelse\\n\\nˆyi = ⃗yi\\n\\nLi\\nLi\\nLi\\n\\nce = − log(⃗pi[ˆyi])\\npro = −log\\nlgc =\\n\\n(cid:80)K\\n\\nexp(⃗qi·⃗c ˆyi /τ )\\nj=1 exp(⃗qi·⃗cj /τ )\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n⃗qi·⃗k+\\nτ\\n\\nexp\\n\\n(cid:80)\\nX\\n⃗qi·⃗k+\\nτ\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n−1\\n|P(i)| log\\n\\n⃗qi·⃗k−\\nτ\\n\\nexp\\n\\n+(cid:80)\\nexp\\n⃗k−∈N (i)\\n\\n(cid:80)\\n⃗k+ ∈P(i)\\n⃗cˆyi = m⃗cˆyi + (1 − m)⃗qi\\nce + λproLi\\nLi = Li\\nupdate networks Eq, h to minimize Li\\nupdate networks Eq, h to minimize Li\\nupdate momentum encoder Ek.\\n\\npro + λlgcLi\\n\\nlgc\\n\\nD. Compare with SYSU30K\\n\\nAs show in Table 9, we compare the pre-trained mod-\\nels between LUPerson-NL and SYSU30K. We pre-train\\nPNL on both LUPerson-NL and SYSU30K for 40 epochs\\nwith same experiment settings for a fair comparison. The\\nperformance of LUPerson-NL pre-training is much better\\nthan SYSU30K pre-training, showing the superiority of our\\nLUPerson-NL, and also suggesting that a large number of\\nimages with limited diversity does not bring more represen-\\ntative representation, but large-scale images with diversity\\ndoes.\\n\\nFigure 6. Scene changing for a specific person in LUPerson-NL.\\n\\nB. Training details\\n\\nDuring training, all\\n\\nimages are resized to 256 ×\\n128 and pass through the same augmentations veri-\\nfied in [12], which are Random Resized Crop, Hor-\\nizontal Flip, Normalization, Random Gaussian Blur,\\nSpecifi-\\nRandom Gray Scale and Random Erasing.\\ncally, the images are normalized with mean and std of\\n[0.3452, 0.3070, 0.3114], [0.2633, 0.2500, 0.2480], which\\nare calculated from all images in LUPerson-NL. We train\\nour model on 8 × V 100 GPUs for 90 epochs with a batch\\nsize of 1, 536. The initial learning rate is set to 0.4 with\\na step-wise decay by 0.1 for every 40 epochs. The opti-\\nmizer is SGD with a momentum of 0.9 and a weight de-\\ncay of 0.0001. We set the hyper-parameters τ = 0.1 and\\nT = 0.8. The momentum m for updating both the momen-\\ntum encoder Ek and the prototypes is set to 0.999. We de-\\nsign a large queue with a size of 65, 536 to increase the oc-\\ncurrence of positive samples for the label guided contrastive\\nlearning. The label correction according to Equation 3 starts\\nfrom the 10-th epoch. The deployment of the label guided\\ncontrastive loss Li\\n\\nlgc starts from the 15-th epoch.\\n\\n\\x0cE.1. Temperature Factor τ\\n\\nTable 13. Comparison for different pre-training data scale\\n\\nDataset\\nMSMT17\\n\\nLUPerson-NL\\n66.1/84.6\\n\\nSYSU30K\\n55.2/76.7\\n\\nTable 9. Comparison of applying our PNL on both LUPerson-NL\\nand SYSU30K.\\n\\nτ\\n\\n0.05\\n0.07\\n0.1\\n0.2\\n0.3\\n\\nDukeMTMC\\n\\nMSMT17\\n\\n40%\\n76.1/87.2\\n76.4/87.5\\n77.0/87.3\\n75.8/86.8\\n74.7/86.2\\n\\n100%\\n83.5/91.4\\n83.6/91.4\\n84.3/92.0\\n83.4/91.0\\n82.7/90.6\\n\\n40%\\n49.8/73.3\\n50.7/74.1\\n51.9/74.9\\n50.1/73.7\\n48.6/72.0\\n\\n100%\\n65.4/83.8\\n67.2/85.3\\n68.0/86.0\\n66.4/85.0\\n65.5/83.7\\n\\nTable 10. Performances under different τ values on DukeMTMC\\nand MSMT17 with data percentages 40% and 100% under the\\nsmall scale setting. The threshold is set as T = 0.8. The best\\nscores are in bold and the second ones are underlined.\\n\\nE. Hyper Parameter Analysis\\n\\nIn our PNL, there are two key hyper-parameters: tem-\\nperature factor τ and the threshold for correction T . Here,\\nwe provide the analysis of these two parameters.\\n\\nTable 10 shows the performance comparison with differ-\\nent τ values with a fixed label correction threshold T = 0.8.\\nAs we can see, the setting τ = 0.1 achieves the best re-\\nsults on both DukeMTMC and MSMT17, while τ = 0.07\\nachieves the second best. When we use a larger τ , the\\nperformance drops rapidly. It may be because Re-ID is a\\nmore fine-grained task, larger τ will cause smaller inter-\\nclass variations and make positive samples too close to neg-\\native samples. In all the experiments, we set τ = 0.1.\\n\\nE.2. Threshold T\\n\\nTable 11 shows the results with different label correc-\\ntion threshold values T . As we can see, the performance\\nis relatively stable to different T values varying in a large\\nrange of 0.6 ∼ 0.8. However, if T is too small or too large,\\nthe performance drops rapidly. For the former case, labels\\nare easier to be modified, which may cause wrong rectifi-\\ncations, while for the latter case, the label noises become\\nharder to be corrected, which also has consistently negative\\neffects on the performance. In all the experiments, we set\\nT = 0.8.\\n\\nF. Results for stronger backbones\\n\\nWe train our PNL using two stronger backbones\\nResNet101 and ResNet152, and report the results in Ta-\\nble 12. As we can see, the stronger ResNet bring more\\nsuperior performances. These results also outperform the\\n\\nT\\n\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n\\nDukeMTMC\\n\\nMSMT17\\n\\n40%\\n76.1/86.5\\n77.1/87.7\\n77.0/87.5\\n77.0/87.3\\n75.7/86.4\\n\\n100%\\n83.3/91.0\\n84.1/91.6\\n84.0/91.8\\n84.3/92.0\\n83.0/90.8\\n\\n40%\\n51.1/74.6\\n52.3/75.5\\n51.9/75.6\\n51.9/75.0\\n50.9/74.3\\n\\n100%\\n67.5/85.2\\n68.1/85.7\\n68.2/85.8\\n68.0/86.0\\n67.2/85.0\\n\\nTable 11. Performances under different T values on DukeMTMC\\nand MSMT17 with data percentages 40% and 100% under the\\nsmall scale setting. The temperature factor is set as τ = 0.1. The\\nbest scores are in bold and the second ones are underlined.\\n\\nArch CUHK03 Market1501 DukeMTMC MSMT17\\n68.0/86.0\\nR50\\n80.4/80.9\\n70.8/87.1\\nR101 80.5/81.2\\n71.6/87.5\\nR152 80.6/81.2\\n\\n84.3/92.0\\n85.5/92.8\\n85.6/92.4\\n\\n91.9/96.6\\n92.5/96.9\\n92.7/96.8\\n\\nTable 12. Results with different ResNet backbones. R50, R101\\nand R152 stand for ResNet50, ResNet101 and ResNet152 respec-\\ntively.\\n\\nScale\\nMSMT17\\n\\n10%\\n57.4/79.2\\n\\n30%\\n62.2/82.1\\n\\n100%\\n68.0/86.0\\n\\nscores reported in Table 8 of our main submission. Most\\nimportantly, we are the FIRST to obtain a mAP score on\\nMSMT17 that is larger than 70 without any post-processing\\nfor convolutional network.\\n\\nG. Pre-training data scales\\n\\nWe study the impact of pre-training data scale. Specif-\\nically, we involve various percentages (10%,30%,100%\\npseudo based) of LUPerson-NL into pre-training and then\\nevaluate the finetuing performance on the target datasets.\\nAs shown in Table 13, the learned representation is much\\nstronger with the increase of the pre-training data scale, in-\\ndicating the necessity of building large-scale dataset, and\\nour LUPerson-NL is very important.\\n\\nH. More results for small-scale and few-shot\\n\\nTo complement Table 3 in the main text, we provide\\nmore detailed results under “small scale” and “few shot”\\nsettings in Table 14. As we can see, our weakly pre-trained\\nmodels are consistently better than other pre-trained mod-\\nels. Our advantage is much larger with less training data,\\nsuggesting the potential practical value of our pre-trained\\nmodels for real-world person ReID applications.\\n\\n\\x0cscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\n10%\\n75\\n1,170\\n53.1/76.9\\n58.4/81.7\\n64.6/85.5\\n72.4/88.8\\n\\n10%\\n751\\n1,293\\n21.1/41.8\\n18.6/36.1\\n26.4/47.5\\n42.0/61.6\\n\\n10%\\n70\\n1,670\\n45.1/65.3\\n48.1/66.9\\n53.5/72.0\\n60.6/75.8\\n\\n10%\\n702\\n1,679\\n31.5/47.1\\n32.4/48.0\\n35.8/50.2\\n52.2/64.1\\n\\n10%\\n104\\n3,659\\n23.2/50.2\\n22.6/48.8\\n25.5/51.1\\n28.2/51.1\\n\\n10%\\n1,041\\n3,262\\n14.7/34.1\\n13.2/29.2\\n17.0/36.0\\n24.5/42.7\\n\\n20%\\n150\\n2,643\\n67.7/86.8\\n70.2/89.1\\n76.9/92.1\\n81.7/93.2\\n\\n20%\\n751\\n2,587\\n53.2/75.1\\n56.5/77.5\\n63.5/83.0\\n75.7/89.1\\n\\n20%\\n140\\n3,192\\n58.3/75.4\\n60.6/76.6\\n65.0/78.9\\n70.5/83.3\\n\\n20%\\n702\\n3,321\\n56.2/72.1\\n56.4/72.2\\n61.0/74.9\\n71.7/82.5\\n\\n20%\\n208\\n6,471\\n34.6/64.0\\n32.7/60.9\\n36.0/62.6\\n39.6/63.7\\n\\n20%\\n1,041\\n6,524\\n35.6/61.4\\n33.5/58.6\\n37.4/61.4\\n45.6/67.2\\n\\n30%\\n225\\n3,962\\n75.2/90.8\\n76.6/91.9\\n81.9/93.7\\n85.2/94.2\\n\\n30%\\n751\\n3,880\\n68.1/87.6\\n69.3/87.8\\n78.3/92.1\\n83.7/94.0\\n\\n30%\\n210\\n5,530\\n64.7/80.2\\n65.8/80.2\\n69.4/81.9\\n74.5/86.3\\n\\n30%\\n702\\n4,938\\n65.4/79.8\\n65.3/80.2\\n72.3/83.8\\n77.7/87.9\\n\\n30%\\n312\\n9,787\\n41.9/70.8\\n40.4/68.7\\n44.6/71.4\\n47.7/71.2\\n\\n30%\\n1,041\\n9,786\\n44.5/71.1\\n41.4/67.1\\n49.0/73.6\\n53.2/74.4\\n\\n40%\\n300\\n5,226\\n79.1/92.5\\n80.0/93.0\\n84.1/94.4\\n87.3/95.1\\n\\n50%\\n375\\n6,408\\n81.5/93.5\\n82.0/94.1\\n85.8/94.9\\n88.3/95.5\\n\\n60%\\n450\\n7,814\\n81.5/93.5\\n83.7/94.3\\n87.8/95.8\\n89.6/96.0\\n\\n(a) Market1501 small-scale\\n40%\\n751\\n5,174\\n75.4/90.4\\n78.8/88.3\\n80.3/92.7\\n86.0/94.3\\n\\n50%\\n751\\n6,468\\n80.2/92.8\\n78.3/90.9\\n84.2/93.9\\n88.1/95.2\\n\\n60%\\n751\\n7,758\\n83.0/93.6\\n81.7/93.3\\n86.7/94.7\\n89.8/95.8\\n\\n(b) Market1501 few-shot\\n\\n40%\\n280\\n6,924\\n68.5/83.0\\n69.5/82.9\\n72.8/84.7\\n77.0/87.3\\n\\n50%\\n351\\n8,723\\n71.8/84.6\\n72.5/84.4\\n75.6/86.7\\n78.8/88.3\\n\\n60%\\n421\\n10,197\\n74.1/85.6\\n75.0/86.2\\n77.6/87.1\\n80.5/89.2\\n\\n(c) DukeMTMC small-scale\\n40%\\n702\\n6,599\\n71.0/83.9\\n70.2/83.4\\n75.2/86.8\\n79.3/88.1\\n\\n50%\\n702\\n8,278\\n73.9/85.7\\n73.7/85.1\\n77.7/87.4\\n81.1/89.6\\n\\n60%\\n702\\n9,923\\n75.8/86.6\\n75.8/86.7\\n79.4/88.4\\n82.3/90.2\\n\\n(d) DukeMTMC few-shot\\n40%\\n416\\n13,006\\n46.7/74.5\\n45.1/72.2\\n49.2/74.9\\n51.9/74.9\\n\\n50%\\n520\\n15,917\\n50.3/76.9\\n49.0/75.0\\n53.0/77.7\\n55.5/77.2\\n\\n60%\\n624\\n19,672\\n53.9/79.4\\n52.7/78.0\\n56.4/79.7\\n59.1/80.1\\n\\n(e) MSMT17 small-scale\\n\\n40%\\n1,041\\n13,048\\n52.0/76.9\\n47.7/72.7\\n53.9/78.5\\n58.6/78.8\\n\\n50%\\n1,041\\n16,310\\n56.2/79.5\\n53.3/77.6\\n57.4/80.5\\n62.2/81.0\\n\\n60%\\n1,041\\n19,572\\n58.8/81.7\\n56.5/79.6\\n60.0/82.1\\n64.1/82.6\\n\\n(f) MSMT17 few-shot\\n\\n70%\\n525\\n9,120\\n84.8/94.5\\n85.4/94.5\\n88.8/95.9\\n90.1/96.2\\n\\n70%\\n751\\n9,055\\n84.2/94.0\\n84.4/94.1\\n88.4/95.5\\n90.5/96.3\\n\\n70%\\n491\\n11,939\\n75.5/86.8\\n76.3/86.9\\n78.9/88.2\\n81.6/89.5\\n\\n70%\\n702\\n11,564\\n77.2/87.8\\n77.7/88.2\\n80.8/89.2\\n83.2/91.1\\n\\n70%\\n728\\n22,680\\n56.9/81.2\\n55.7/79.9\\n59.5/81.8\\n61.6/81.8\\n\\n70%\\n1,041\\n22,834\\n60.9/82.8\\n59.1/81.5\\n62.9/83.5\\n65.8/83.8\\n\\n80%\\n600\\n11,417\\n85.9/95.2\\n86.4/95.0\\n89.8/96.2\\n90.9/96.4\\n\\n80%\\n751\\n10,348\\n86.3/94.7\\n86.4/95.0\\n89.8/96.0\\n91.2/96.4\\n\\n80%\\n561\\n13,500\\n76.8/87.3\\n77.4/87.3\\n80.2/89.2\\n82.9/90.6\\n\\n80%\\n702\\n13,201\\n78.3/88.6\\n78.7/88.7\\n81.7/90.3\\n84.0/91.6\\n\\n80%\\n832\\n26,335\\n59.6/82.4\\n58.6/82.0\\n61.9/83.6\\n64.2/83.3\\n\\n80%\\n1,041\\n26,096\\n62.5/84.2\\n60.9/82.3\\n64.2/84.5\\n67.2/84.7\\n\\n90%\\n675\\n11,727\\n86.9/95.2\\n87.4/95.5\\n90.5/96.4\\n91.3/96.4\\n\\n90%\\n751\\n11,642\\n86.7/94.6\\n87.1/95.2\\n90.4/96.3\\n91.6/96.4\\n\\n90%\\n631\\n15,111\\n78.0/88.3\\n78.5/88.7\\n81.1/90.0\\n83.3/91.2\\n\\n90%\\n702\\n14,860\\n79.1/88.8\\n79.4/89.0\\n82.0/90.6\\n84.1/91.3\\n\\n90%\\n936\\n29,529\\n61.9/84.2\\n60.9/83.0\\n63.7/85.0\\n66.1/84.8\\n\\n90%\\n1,041\\n29,358\\n63.4/84.5\\n62.4/83.8\\n65.0/85.1\\n67.4/85.3\\n\\n100%\\n751\\n12,936\\n87.5/95.1\\n88.2/95.3\\n91.0/96.4\\n91.9/96.6\\n\\n100%\\n751\\n12,936\\n87.5/95.1\\n88.2/95.3\\n91.0/96.4\\n91.9/96.6\\n\\n100%\\n702\\n16,522\\n79.4/89.0\\n79.5/89.1\\n82.1/91.0\\n84.3/92.0\\n\\n100%\\n702\\n16,522\\n79.4/89.0\\n79.5/89.1\\n82.1/91.0\\n84.3/92.0\\n\\n100%\\n1,041\\n32,621\\n63.7/85.1\\n62.7/84.3\\n65.7/85.5\\n68.0/86.0\\n\\n100%\\n1,041\\n32,621\\n63.7/85.1\\n62.7/84.3\\n65.7/85.5\\n68.0/86.0\\n\\nTable 14. Performance for small-scale and few-shot setting with MGN method for Market1501, DukeMTMC and MSMT17. “IN sup.” and\\n“IN unsup.” refer to supervised and unsupervised pre-trained model on ImageNet, “LUP unsup.” refers to unsupervised pre-trained model\\non LUPerson, “LUPws wsp.“ refers to our model pre-trained on LUPerson-WS using WSP. The first number is mAP and second is cmc1.\\n\\n\\x0c',\n",
       " 'Understanding 3D Object Articulation in Internet Videos\\n\\nShengyi Qian\\n\\nLinyi Jin\\n\\nChris Rockwell\\n\\nSiyi Chen\\n\\nDavid F. Fouhey\\n\\nUniversity of Michigan\\n{syqian,jinlinyi,cnris,siyich,fouhey}@umich.edu\\nhttps://jasonqsy.github.io/Articulation3D\\n\\nFigure 1. Given an ordinary video, our system produces a 3D planar representation of the observed articulation. The 3D renderings\\nillustrate how the microwave (in Pink) can be articulated in 3D space. We also show the predicted rotation axis using a Blue arrow.\\n\\nAbstract\\n\\nWe propose to investigate detecting and characterizing\\nthe 3D planar articulation of objects from ordinary RGB\\nvideos. While seemingly easy for humans, this problem\\nposes many challenges for computers. Our approach is\\nbased on a top-down detection system that finds planes that\\ncan be articulated. This approach is followed by optimizing\\nfor a 3D plane that explains a sequence of detected articu-\\nlations. We show that this system can be trained on a com-\\nbination of videos and 3D scan datasets. When tested on\\na dataset of challenging Internet videos and the Charades\\ndataset, our approach obtains strong performance.\\n\\n1. Introduction\\n\\nHow would you make sense of Figure 1? Behind the set\\nof RGB pixels that make up the video is a real 3D trans-\\nformation consisting of a 3D planar door rotating about an\\naxis. The goal of this paper is to give the same ability to\\ncomputers. We focus on planar articulation taking the form\\nof a rotation or translation along an axis. This special case\\nof articulation is ubiquitous in human scenes and under-\\nstanding it lets a system understand objects ranging from\\nrefrigerators and drawers to closets and cabinets. While we\\noften learn about these shapes and articulations with phys-\\nical embodiment [53], we have no difficulty understanding\\nthem from video cues alone, for instance while watching a\\n\\nmovie or seeing another person perform an action. We for-\\nmalize this ability for computers as recognizing and charac-\\nterizing a class-agnostic planar articulation via a 3D planar\\nsegment, articulation type (rotation or translation), 3D ar-\\nticulation axis, and articulation angle.\\n\\nThis problem is beyond the current state of the art in\\nscene understanding since it requires reconciling single\\nimage 3D understanding with dynamic 3D understanding.\\nWhile there has been substantial work on 3D reconstruction\\nfrom a single image [5,10,13,65], including work dedicated\\nto planes [34], these works focus on reconstructing static\\nscenes. On the other hand, while there has been work under-\\nstanding articulation, these works often require the place-\\nment of tags for tracking [37, 43], a complete 3D model or\\ndepth sensor [21, 31, 41], or successful 3D human recon-\\nstruction [69]. Moreover, making progress is challenging\\nbecause of data. Unsupervised approaches based on motion\\nanalysis [44, 54] require something to track, which breaks\\nin realistic data since many human-made articulated ob-\\njects are untextured (e.g., refrigerators) or transparent (e.g.,\\novens). While supervised approaches [31, 40, 41] can per-\\nhaps bypass tracking features, they seemingly require ac-\\ncess to large amounts of RGBD data of interactions. For\\nnow, this data does not exist, and training on synthetic data\\ncan fall short when tested on real data (as our experiments\\nempirically demonstrate).\\n\\nWe overcome these challenges with a learning-based ap-\\nproach that combines both detection and 3D optimization\\n\\n1\\n\\n\\x0cand is trained with supervision from multiple sources (Sec-\\ntion 4). The foundation of our approach is a top-down de-\\ntection approach that recognizes articulation axes and types\\nand 3D planes; this approach’s outputs are processed with\\nan optimization method that seeks to explain the per-frame\\nresults in terms of a single coherent 3D articulation.\\n\\nVia this model, we show that one can build an under-\\nstanding of 3D object dynamics via a mix of 2D supervision\\non Internet videos of objects undergoing articulation as well\\nas 3D supervision on existing 3D datasets that do not de-\\npict articulations. To provide 2D supervision, we introduce\\n(Section 3) a new set of 9447 Creative Commons Internet\\nvideos. These videos depict articulation with a variety of\\nobjects as well as negative samples and come with sparse\\nframe annotations of articulation boxes, axes, and surface\\nnormals that can be used for training and evaluating planar\\narticulation models.\\n\\nOur experiments (Section 5) evaluate how well our ap-\\nproach can recognize and characterize articulation. We\\nevaluate on our new dataset of videos as well as the Cha-\\nrades [51] dataset. We compare with a variety of alter-\\nnate approaches, including bottom-up signals like optical\\nflow [56] and changes in surface normals [4], training on\\nsynthetic data [68], as well as systems that analyze human-\\nobject interaction [69]. Our approach outperforms these\\napproaches on our data, often even when the baselines are\\ngiven access to ground-truth location of articulation.\\n\\nOur primary contributions include:\\n\\n(1) The new task\\nof detecting 3D object articulation on unconstrained ordi-\\nnary RGB videos without requiring RGBD video at train-\\ning time; (2) A dataset of Internet videos, with sparse frame\\nannotations of articulation boxes, axes, and surface normals\\nthat can be used for training and evaluating planar articu-\\nlation models; (3) A top-down detection network and op-\\ntimization to tackle this problem, which has strong perfor-\\nmance on the Internet video dataset and Charades.\\n\\n2. Related Work\\n\\nOur paper proposes to extract 3D models of articulation\\nfrom ordinary RGB videos. This problem lies at the inter-\\nsection of 3D vision, learning from videos, and touches on\\nrobotics applications. We note that there are specialized ap-\\nproaches for understanding general articulation (e.g., non-\\nrigid structure from motion [58]) as well as for understand-\\ning specialized motion models (e.g., for a full human 3D\\nmesh models [73] or quadrupeds [29]) or for understanding\\nmore general transformations [20, 63]. Our work focuses\\non understanding the articulation of general objects whose\\narticulated pieces can be represented by a 3D plane rotating\\nor translating.\\n\\nDue to the ubiquitous nature of articulated objects, the\\ntask of understanding them has long been an interest across\\nall of artificial intelligence. In vision, the understanding of\\n\\nthe motion of rigid objects undergoing transformations was\\none of the early successes of computer vision [25, 57, 62].\\nUnfortunately, these early works rely on reliable motion\\ntracks, which is made difficult by the textureless or re-\\nflective nature of many indoor planes (e.g., refrigerator\\ndoors). Our top-down detector gives 3D planes that can\\nhelp provide correspondence between frames where corre-\\nspondence is challenging.\\n\\nMore recent work in robotics has used the value of 3D\\nand integrated it into their modeling approaches [6,9,39,44,\\n54]; however their approaches often use an RGBD sensor,\\nunlike our use of ordinary RGB sensors. This dependence\\non RGBD has been carried forward to the most recent work\\nthat uses deep learning frameworks [1, 21, 31, 36, 66, 68].\\nIn fact, some methods require full 3D models [41], which\\nis typically unavailable in real world 3D scans. [40] by Mo\\net al. can be run on 2D images as long as the point cloud\\nencoder is replaced with a RGB encoder, but its 2D im-\\nages contain a single object without any background, in-\\nstead of challenging Internet videos. While there has been\\nincreasing amounts of work aimed at virtual articulated ob-\\njects [55, 68], simultaneously achieving scale and quality\\nis challenging. For instance ReplicaCAD [55] has only 92\\nobjects. In contrast, our approach works at test time on stan-\\ndard RGB videos by bringing its own 3D via a learned de-\\ntector [34] trained on RGBD data [7].\\n\\nWhile our outputs are 3D planar regions, our approach\\nis deeply connected to the task of understanding human-\\nobject interactions. In these works [3, 14, 50], the goal is\\nto recognize the relationship between humans and the ob-\\njects they interact with. The interactions that we study are\\ncaused by these humans, and so we use an approach that can\\npredict human-object interactions [50] to help identify the\\ndata we train our systems on. The most related work in this\\narea is [69], which aims to jointly understand dynamic 3D\\nhuman-object interactions in 3D. This work, however as-\\nsumes that the object CAD model is known once the articu-\\nlated object is detected, which we do not need. Our method\\nalso works with articulation videos that are more varied in\\nviewpoint and perspective. A more thorough understand-\\ning of the joint relationship between articulated objects and\\nhuman-object interaction, akin to early work [11, 28], is be-\\nyond the scope of this work, but of future value.\\n\\nWe solve the problem of describing 3D articulation by\\nproducing 3D planar models. This uses advances in 3D\\nfrom a single image.\\nIn particular, we build on PlaneR-\\nCNN [34], which is part of a growing body of works aimed\\nat extracting planes from single images [35, 70, 71]. These\\nplanes have advantages for the articulation reasoning since\\nthey offer a compact representation to track and describe.\\nWhile we use plane recognition, the plane is just one com-\\nponent of our output (along with rotation axes) and we ana-\\nlyze our output in a video with temporal optimization.\\n\\n2\\n\\n\\x0c3. Dataset\\n\\nOne critical component of our approach is accurate 2D\\nannotations of articulation occurring in RGB data. We show\\nthat these 2D annotations can be combined with existing\\nRGBD data and the right method to build systems that un-\\nderstand 3D articulation on video data. We next describe\\nhow we collect a dataset of articulations. Our goals are to\\nhave a large collection of annotations of object box, artic-\\nulation type, and axis. Rather than directly look for exam-\\nples of people articulating objects, we follow the data-first\\napproach of [8, 12, 50, 72], namely to gather data containing\\nmany related activities and then analyze and annotate it.\\n\\nData Collection. Our pipeline generates a set of candidate\\nclips to be annotated from a collection of candidate videos\\nvia an automatic pipeline that aims to eliminate frames that\\nare easy to see as not depicting articulation. We begin with\\ncandidate videos that are found by variants of searches for\\na set of 10 objects among Creative Commons videos on\\nYouTube. Within these videos, we find stationary contin-\\nuous shots in these videos with homographies [16] fit on\\nORB [49] features. Many of these clips cannot depict in-\\nteraction since they do not contain any people or do not\\ncontain the objects of interest. We filter by responses by\\na hand detector [50] trained on 100K+ frames of Internet\\ndata, as well as object detectors trained on COCO [33] and\\nLVIS [15]. These filtering steps maximize the use of an-\\nnotator time by eliminating clear negatives, and generate a\\nlarge number of candidate clips.\\n\\nWith a collection of candidate clips of interest, we then\\nturn to manual annotation. For a given clip, we hire an\\nannotation company to annotate frames sparsely (every 10\\nframes) within the clip. They annotate: (box) a box around\\nthe articulated plane and its type, if it exists; and (axis) the\\nprojection of the articulation axis, framed as a line segmen-\\ntation annotation problem. This results in a set of 19411\\nframes, containing 19411 boxes around articulating planes\\nwith 13508 rotation axes and 2755 translation axes, as well\\nas 39411 negative frames. The number of articulation axes\\nis not equal to the number of boxes, since some articulation\\naxes are outside the image. We provide training, validation,\\nand test splits based on uploader, leading to 7845/601/1001\\nvideos in the train/val/test split. A more complete descrip-\\ntion of our annotation pipeline appears in the supplement.\\n\\nWe collect two additional annotations. For the test set,\\nwe also annotate the surface normal of the plane follow-\\ning [4], so we can evaluate how well our model can learn 3D\\nproperties. To show generalization, we also collect the same\\nannotations except surface normals on the Charades [51]\\ndataset.\\n\\nData Availability and Ethics. Our data consists of videos\\nthat users uploaded publicly and chose to share as Creative\\nCommons data. These do not involve interaction with hu-\\n\\nmans or private data. We filtered obviously offensive con-\\ntent, videos depicting children, and cartoons. Examples\\nappear throughout the paper; screenshots of annotation in-\\nstructions and details appear in the supplement.\\n\\n4. Approach\\n\\nThe goal of our approach is to detect and characterize\\nplanar articulation in an unseen RGB video clip. These\\narticulations are an important special case that are ubiqui-\\ntous in human scenes. As shown in Figure 2, we propose a\\n3D Articulation Detection Network (3DADN) to solve the\\ntask. As output, the 3DADN produces the type of motion\\n(rotation or translation), a bounding box around where the\\nmotion is located, the 2D location of the rotation or transla-\\ntion axis, and the 3D location of the articulated plane. The\\n3DADN’s output is followed by post-processing to find a\\nconsistent explanation over the whole video.\\n\\n4.1. 3D Articulation Detection Network\\n\\nThe 3DADN processes each frame independently.\\nIts\\noutput consists of: a segment mask Mi; plane parameters\\nπi = [ni, oi] giving the plane equation πT\\ni [x, y, z, −1] = 0\\n(where ni is the the plane normal with ||ni||2 = 1 and oi\\nis the plane offset); a projected rotation or translation axis\\nai = [θ, p] which is the projection of the 3D articulation\\naxis; and articulation type.\\n\\nWe use a top-down approach to detect this representa-\\ntion, which we train on RGB videos that depict articulation\\nwithout 3D information as well as RGBD images with 3D\\ninformation that do not depict articulation. Our backbone is\\na Faster R-CNN [47] style network that first detects bound-\\ning boxes for the articulating objects and classifies them into\\ntwo classes (rotation and translation). These boxes provide\\nROI-pooled features that are passed into detection heads\\nthat predict our outputs (Mi, πi, ai). Our heads and losses\\nfor Mi follow the common practice of Mask R-CNN [17].\\nWe describe ai and πi below.\\nParameterizing Rotation and Translation Axis. We\\nmodel the projected articulation axis as a 2D line in the\\nimage. This projected axis is the projection of the 3D ar-\\nticulation axis (e.g., the hinge of a door). We describe the\\nprojected axis with the normal form of the line, x cos(θ) +\\ny sin(θ) = p where p ≥ 0 is the distance from the box to\\nthe center and θ is the inclination of the normal of the axis\\nin pixel coordinates. Since a translation corresponds to a di-\\nrection/family of lines as opposed to a line, we define p = 0\\nfor translation arbitrarily.\\n\\nThe articulation head contains two independent branches\\nfor predicting the rotation and translation axes. We handle\\nthe circularity of the prediction of θ by lifting predictions\\nand ground-truth for the angle to the 2D unit circle; since\\nthe line is 180-degree-ambiguous (i.e., θ + π is the same as\\n\\n3\\n\\n\\x0cFigure 2. Overview of our approach. (a) Given an ordinary video clip, we first apply our 3D Articulation Detection Network (3DADN) to\\ndetect 3D planes can be articulated for each frame. (b) We then apply temporal optimization to fit the articulation model. Final results are\\ndemonstrated in both 2D image and 3D rendering.\\n\\nθ), we predict a 2D vector [sin(2θ), cos(2θ)]. The resulting\\nnetwork thus predicts a 3D vector containing θ and p, which\\nwe supervise with a L1 loss.\\n\\nParameterizing Plane Parameters. Following a line of\\nwork on predicting planes in images, we use a 3D plane [35]\\nto represent the 3D locations of the articulated objects, since\\nmany common articulated objects like doors, refrigerators,\\nand microwaves can be modeled as planes and because past\\nliterature [22, 23, 34] suggests that R-CNN-style networks\\nare adept at predicting plane representations.\\n\\nA 3D plane is represented by plane parameters πi =\\n[ni, oi] giving the plane equation πT\\ni [x, y, z, −1] = 0 With\\ncamera intrinsics, planes can be recovered in 3D and with a\\nmask, this plane can be converted to a plane segment. Fol-\\nlowing [23, 34], we extend R-CNN by adding a plane head\\nwhich directly regresses the normal of the plane. A depth\\nhead is used to predict depth of the image. The depth is only\\nused to calculate the offset value of the plane. We supervise\\nwith L2 loss for the plane normal regression and L1 loss for\\nthe depth regression.\\n\\nTraining. There is no dataset that is non-synthetic and\\nlarge enough to train a 3DADN directly: the 3DADN needs\\nboth realistic interactions and 3D information. However,\\nwe can train the 3DADN in parts. In the first stage, we train\\nthe backbone, RPN, and axis heads directly on our Inter-\\nnet video training set, which has boxes and axes. We then\\nfreeze the backbone, RPN, and axis heads and fine-tune the\\nmask and plane head on a modified version of ScanNet [7].\\nIn particular, we found that humans often occlude the\\nobjects they articulate and models that had not seen humans\\nin training produced worse qualitative results. We therefore\\ncomposited humans from SURREAL [60] into the scenes.\\nWe randomly sample 98,235 ScanNet images, select a syn-\\nIn\\nthetic human and render it on ScanNet backgrounds.\\n\\ntraining, we do not change the ground truth, pretending\\nthe ground truth plane is partially occluded by humans and\\ntraining our model to identify them.\\n\\nMeanwhile, we found that the order of training the heads\\nwas crucial. Planes in ScanNet [7] are defined geometri-\\ncally, and so unopened doors often merge with walls; sim-\\nilarly, ScanNet [7] does not contain transitional moments\\nduring which planes are articulating. Thus, RPNs trained\\non ScanNet [7] perform poorly on articulation videos. In-\\nstead, it is important to train the RPN on our Internet videos,\\nfreeze the backbone, and only rely on ScanNet to train plane\\nparameters and masks, which are unavailable in Internet\\nvideos. During inference we keep the ScanNet camera since\\nour data does not have camera intrinsics.\\nImplementation Details. Full architectural details of our\\nis im-\\napproach are in the supplemental. Our model\\nplemented using Detectron2 [67]. The backbone uses\\nResNet50-FPN [32] pretrained on COCO [33].\\n\\n4.2. Temporal Optimization\\n\\n, π(t)\\ni\\n\\nAfter the 3DADN provides per-frame estimates of artic-\\nulations, we perform temporal optimization to find a sin-\\ngle explanation for the detections across frames. We are\\ngiven a sequence of detections indexed by a time of the form\\n[M(t)\\n, a(t)\\n]. We aim to find a single consistent expla-\\ni\\ni\\nnation for these detections.\\nTracking. Optimizing requires a sequence of planes to op-\\ntimize over. We match box i with the box in the next frame\\naccording to pairwise intersection over union (IoU). Box\\ni at t matches box j = arg maxj′ IoU(M(t)\\n) at\\ntime t + 1; we then track greedily to get a sequence. We\\nsubsequently drop the subscripts for clarity.\\nArticulation Model Fitting. Given a sequence of detec-\\ntions, we find a consistent explanation via a RANSAC-like\\n\\n, M(t+1)\\nj′\\n\\ni\\n\\n4\\n\\n][Per-frameDetectionPlaneMaskPlaneParamsArticulationAxisPredictionsVideo ClipPerframeDetection CNN2D boxand axis3D planesand axisTemporal Optimization……Across frames\\x0cInput\\n\\nPred1\\n\\nPred2\\n\\nPred3\\n\\nPred4\\n\\nPred 3D\\n\\nFigure 3. Predictions on Internet videos. For each example, we show the input (left), detected 2D planes and how they will be articulated\\nusing the predicted articulation axes and surface normals (middle). We also show 3D renderings to illustrate how these common objects\\nare articulated in the 3D space (right). The predicted rotation axis is shown as the Blue arrow, and translation axis is the Pink arrow.\\n\\napproach. We begin with a hypothesis of a plane segment\\nπ and articulation axis a, which we obtain by selecting out-\\nput on a reference frame. Along with an assumed camera\\nintrinsics K, the plane parameters let us lift the plane seg-\\nment and axis to 3D, producing 3D plane segment Π and\\n3D axis A. Then, for each frame t, we solve for an artic-\\nulation degree α(t) maximizing the reprojection agreement\\nwith the predicted mask at time t. Let us define the repro-\\njection score as\\n\\nr(α, t) = IoU\\n\\n(cid:16)\\n\\n(cid:17)\\nM(t), K [Rα, tα] Π\\n\\n,\\n\\n(1)\\n\\nwhere RA,α and tA,α are α steps over the rotation and\\ntranslation for axis A. We then solve for α(t) by solving\\narg maxα r(α), which gives a per-frame angle using grid\\nsearch. We detect articulation by calculating how well the\\nrotation degree α(t) can be explained as a linear function\\nof t (i.e., that there is constant motion). Since many scenes\\nare not constant motion, we have loose thresholds: we con-\\nsider R2 ≥ 0.4 and slope k > 0.1 to be an articulation. We\\nexclude hypotheses where all r(α(t), t) < 0.5.\\n\\n5. Experiments\\n\\nWe have introduced a method that can infer 3D articu-\\nlation in Section 4. In the experiments, we aim to answer\\nthe following questions: (1) how well can one detect 3D\\narticulating objects from ordinary videos; (2) how well do\\nalternate approaches to the problem do?\\n\\n5.1. Experimental Setup\\n\\nWe first describe the setup of our experiments. Our\\nmethod aims to look at an ordinary RGB video and infer\\ninformation about an articulated plane on a object in 3D,\\nincluding: whether the object is articulating, its extent, and\\nthe projection of its rotation or translation axis. We there-\\nfore evaluate our approach on two challenging datasets, us-\\ning metrics that capture various aspects of a 3D plane artic-\\nulating in 3D.\\n\\nDatasets: We validate our approach on both Internet videos\\n(described in Section 3) and the Charades dataset [52]. We\\nuse Charades for cross-dataset evaluations. We focus on\\nCharades videos that are opening objects (doors, refrigera-\\n\\n5\\n\\n\\x0cFlow+Normal\\n\\nSAPIEN\\n\\nSAPIEN w/ gtbox\\n\\nD3D-HOI\\n\\nOurs\\n\\nGT\\n\\nFigure 4. We compare our approach with four baselines. See detailed discussions in the text. We show translation in Pink and rotation in\\nBlue, except D3D-HOI which uses a different detector.\\n\\ntors, etc.), and annotate 2491 frames across 479 videos; we\\nalso randomly sample 479 negative videos containing 4401\\nnegative frames. Our Charades annotation process is simi-\\nlar to Internet videos, with the exceptions that: we annotate\\nonly rotations as Charades contains few translation articu-\\nlations; and we do not annotate surface normals.\\n\\nEvaluation Criteria: Evaluation of our approach is non-\\ntrivial, since our assumed input (RGB videos) precludes\\nmeasuring outputs quantitatively in 3D. We therefore eval-\\nuate our approach on a series of subsets of the problem. We\\nstress from the start though that these metrics are what can\\nbe measured (due to the use of RGB inputs), as opposed to\\nthe full rich output.\\nArticulation Recognition: We first independently evaluate\\nthe ability to detect whether someone is articulating this ob-\\nject at a point in time. We frame this as a binary prediction\\nproblem. This is surprisingly difficult in real scenes because\\nobjects are typically partially occluded by humans when hu-\\nmans articulate them, and because humans often touch a ar-\\nticulated objects (e.g., cleaning the surface) without open-\\ning it. We use AUROC to measure the performance.\\nArticulation Description: We next evaluate the ability of\\na system to detect the articulated object, corresponding ar-\\nticulation type (rotation/translation), axes, and surface nor-\\nmals. We follow other approaches [23, 30, 42, 45, 59] that\\nreconstruct the scene factored into components and treat it\\nas a 3D detection problem, evaluated using average preci-\\nsion (AP). We define error metrics as follows: (Bounding\\n\\nbox) IoU, thresholded as 0.5. We find the normal COCO\\nAP, which measures IoU up to 0.95, to be too strict be-\\ncause the precise boundaries of articulating parts are often\\noccluded by people and hard to annotate. (Axes) EA-score\\nfrom the semantic line detection literature [74]. This metric\\nhandles a number of edge cases; we use 0.5 as the thresh-\\nold as recommended by [74]. (Surface normal) mean angle\\nerror, thresholded at 30◦, following [10, 64]. A prediction\\nis a true positive only if all errors are lower than our thresh-\\nolds. We calculate the precision-recall curve based on that\\nand report AP for varying combinations of metrics.\\n\\nBaselines: Prior approaches for articulation detection have\\nfocused on robots, synthetic datasets, and real-world RGBD\\nscans. These are different from our setting for two reasons.\\nFirst, videos of people articulating objects show a noisy\\nbackground with a person interacting with and occluding\\nthe object, as opposed to an isolated articulated object in a\\nsimulator. Second, RGB videos do not have depth, which is\\noften a requirement of existing articulation models. For ex-\\nample [31] requires depth, and while they show results on\\nreal-world depth scans, their RGBD scans only contain a\\nstatic object without humans. We propose to compare with\\nthe following methods.\\n\\n3DADN + SAPIEN [68] Data: To test whether we can solve\\nthe problem just by training on synthetic data, we create a\\nsynthetic data-based method where we train our 3DADN\\nsystem on synthetic data. We render a synthetic dataset us-\\n\\n6\\n\\n\\x0cTable 1. We report AUROC for Articulation Recognition, as well as AP for Articulation Description. To separate out difficulties in\\ndetecting articulation and characterizing its parameters, we assist Flow+Normal and 3DADN+SAPIEN with ground truth bounding box\\nand denote it as gtbox. 3DADN+SAPIEN cannot detect most objects without the help of gtbox.\\n\\nMethods\\n\\nFlow [56] + Normal [4]\\nFlow [56] + Normal [4]\\nD3D-HOI [69] Upper Bound\\n3DADN + SAPIEN [68]\\nOurs\\n\\n✗\\n✔\\n✗\\n✔\\n✗\\n\\nRecog.\\ngtbox AUROC bbox\\n\\nRotation\\n\\nTranslation\\n\\nbbox+axis\\n\\nbbox+axis+normal\\n\\nbbox\\n\\nbbox+axis\\n\\nbbox+axis+normal\\n\\n68.5\\n-\\n62.7\\n-\\n76.6\\n\\n7.7\\n-\\n28.8\\n-\\n61.3\\n\\n0.3\\n3.0\\n19.7\\n16.8\\n30.4\\n\\n0.0\\n0.3\\nn/a\\n1.40\\n17.2\\n\\n0.3\\n-\\n4.70\\n-\\n34.0\\n\\n0.0\\n1.4\\n4.7\\n15.1\\n27.1\\n\\n0.0\\n0.7\\nn/a\\n0.40\\n17.9\\n\\ning SAPIEN [68] by randomly sampling and driving 3D\\nobjects. We filtered 1053 objects of 18 categories with\\nmovable planes from PartNet-Mobility Dataset [68], such\\nas doors and laptops. We render frames with the objects\\narticulated, with location parameters picked to give plau-\\nsible scenes, and extract the information needed to train\\n3DADN. Without a background, the detection problem be-\\ncomes trivial, so to mimic real 3D scenes, we blend the\\nrenderings with random ScanNet [7] images as the back-\\nground and render synthetic humans from SURREAL [60].\\nFor fair comparison, we use the same ScanNet+SURREAL\\nimages used to train our system’s plane parameter head.\\nWhen evaluated on SAPIEN data, this approach performs\\nwell and obtains an AP of (bbox) 60.3, (bbox+rot) 64.1,\\n(bbox+rot+normal) 41.0.\\n\\nBottom-up Optical Flow [56] and Surface Normal\\nChange [4] (Flow+Normal): To test whether the data can\\nbe solved by the use of fairly simple cues, we construct\\na baseline that uses Optical Flow [56] (since articulating\\nobjects tend to cause movement) and Surface Normals [4]\\n(since rotating planes change their orientation). Both flow\\nand normals provide a H × W map that can be analyzed.\\nWe also use the output of a human segmentation system [19]\\nthat was trained on multiple datasets and mask normal and\\nflow magnitude maps wherever it improves performance.\\nGiven these maps, we recognize the presence of articulation\\nvia logistic regression on a feature vector consisting of the\\nfraction of pixels above multiple thresholds; we recognize\\nbounding boxes via thresholding and finding the tightest en-\\nclosing box; we estimate rotation axis as perpendicular to\\nthe mean flow change in the bounding box (flow tends to\\nincrease away from hinges); we find translation axis using\\nmean flow direction in the box; we find articulation normal\\nusing mean predicted normals in the box. Throughout, we\\nuse the optimal option of surface normals and flow; this hy-\\nbrid system performs substantially better than either flow or\\nnormals alone.\\n\\nBaselines with + GT Box: To separate out difficulties in\\ndetecting articulation and characterizing its parameters, we\\nalso experiment with giving baselines ground-truth bound-\\ning box information about the articulating object. This gives\\n\\nan upper-bound on performance.\\n\\nD3D-HOI [69] Upper Bound: We compare with D3D-\\nHOI since it accepts RGB video as input and detects how\\nhumans articulate objects. A direct comparison with D3D-\\nHOI is challenging since it only works when EFT [24] re-\\nconstructs 3D human poses and Pointrend [26] detects the\\nobjects that are assumed to articulate and correct CAD mod-\\nels are chosen for the object. However, EFT does not work\\nwell on the dataset due to truncated or multiple humans on\\nInternet videos [27, 48]. We therefore report upper-bounds\\non the performance. We assume it predicts the ground\\ntruth bounding box, when EFT mask and pseudo ground\\ntruth 2D human segmentation mask [18] has IoU > 0.5 and\\nPointRend [26] produces a mask on articulated objects with\\nconfidence > 0.7.\\nOurs: This is our proposed method. It includes both the per-\\nframe approach described in Section 4.1 and the optimiza-\\ntion approach of Section 4.2. We note that this approach\\nalso produces outputs that are not being quantitatively mea-\\nsured, such as a 3D plane articulating in 3D. These are qual-\\nitatively shown in Figures 1 and 3.\\n\\n5.2. Results\\n\\nWe first show qualitative results in Figure 3. On chal-\\nlenging Internet videos, our approach usually detects and\\nrecovers the 3D articulated plane regardless of categories.\\n\\nIn Figure 4, we compare our approach with four base-\\nlines visually. Flow can occasionally locate articulation\\n(third row), but in most cases, flow is not localized to only\\nthe object articulating (e.g. camera movement, top row).\\nTraining purely on SAPIEN [68] data has difficulty detect-\\ning articulated objects in Internet videos, even if we show\\nall detected objects with confidence score > 0.1. It learns\\nsome information of articulation axes when we assist it with\\nground truth bounding boxes. D3D-HOI [69] relies on both\\nEFT [24] to detect humans and PointRend [26] to detect ob-\\njects. However, EFT has diffculty predicting 3D humans on\\nInternet videos.\\n\\nQuantitative Results. We evaluate the approach quan-\\ntitatively on the three tasks in Table 1. Our approach\\nsubstantially outperforms the alternate methods. While\\n\\n7\\n\\n\\x0cPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nFigure 5. Qualitative results on Charades dataset. Without fine-\\ntuning on Charades data, our model obtains strong performance\\non detecting and characterizing 3D articulation.\\n\\nTable 2. Evaluation on Charades dataset [52]. We only report rota-\\ntion AP since Charades does not have sufficient translation motion.\\n\\nMethods\\n\\nFlow [56] + Normal [4]\\nFlow [56] + Normal [4]\\nD3D-HOI Upper Bound\\n3DADN + SAPIEN [68]\\nOurs\\n\\nRecog.\\ngtbox AUROC\\n\\nRotation\\n\\nbbox\\n\\nbbox+axis\\n\\n✗\\n✔\\n✗\\n✔\\n✗\\n\\n53.7\\n-\\n55.9\\n-\\n58.4\\n\\n3.1\\n-\\n14.9\\n-\\n12.0\\n\\n0.2\\n4.2\\n13.7\\n1.54\\n12.8\\n\\nstatistically-combined bottom-up cues [4, 56] do better than\\nchance at predicting the presence of an articulation, they\\nare substantially worse than the proposed approach and\\nfail to obtain sensible bounding boxes. Even when given\\nthis method fails to obtain good\\nthe ground-truth box,\\naxes. Due to the frequency of truncated humans in Internet\\nvideos [27, 48], D3D-HOI [69]’s performance upper-bound\\nis substantially lower than our method’s performance. The\\ndetection system when trained on synthetic data from [68]\\nfails on our system; when given a good bounding box, syn-\\nthetic training data obtains reasonable, but inferior numbers\\nand poor accuracy in predicting normals.\\n\\nAblations – Optimization. Our optimization produces\\nmodest gains in recognition accuracy and axis localization\\nin 2D: It improves recognition AUROC from 74.0 to 76.6,\\nrotation AP from 16.6 to 17.2 and translation AP from 14.3\\nto 17.9. This small gain is understandable because the\\nevaluation is per-frame and the optimization mainly seeks\\nto make the predictions more consistent.\\nIf we quantify\\nthe consistency in the results before and after optimization,\\nwe find that the EAScore [74] between tracked predicted\\nframes increases from 0.69 (before optimization) to 0.96\\n(after optimization).\\n\\n5.3. Generalization Results\\n\\nWe next test our trained models without fine-tuning on\\nCharades [51]. We show results in Figure 5. Our approach\\ntypically generates reasonable estimations. We find that the\\nvideo quality and resolution of Charades is lower relative to\\nour videos, with many dark or blurry videos.\\n\\nWe also show quantitative evaluations in Table 2. Here,\\nour performance is slightly diminished. However, we sub-\\n\\nFigure 6. Typical failure modes.\\n(1) Ambiguity of articulation\\ntype; (2) Axis outside of the frame or ambiguity of articulation\\naxis location due to symmetry; (3) Object has complex motions (a\\nperson moving an object while articulating it; the rotation axis is\\noutside of the articulating surface).\\n\\nstantially outperform the baselines. We are only marginally\\noutperformed by D3D-HOI upper bound, which assumes\\nperfect performance so long as the data can be obtained.\\n\\n5.4. Limitations and Failure Modes\\n\\nWe finally discuss our limitations and typical failure\\nmodes in Figure 6. We find some examples are particu-\\nlarly challenging: (1) Column 1: some images may contain\\nhard examples where the axis types are hard to figure out.\\n(2) Column 2: the axis is outside of the image frame or its\\nlocation is ambiguous due to symmetry or occlusion. (3)\\nColumn 3: the object has complex dynamics or dual axes;\\nfor example, a person moving a laptop while opening it or\\nthe cabinet has multiple joints.\\n\\n6. Conclusion\\n\\nWe have demonstrated our approach’s ability to detect\\nand characterize 3D planar articulation of objects from or-\\ndinary videos. Future work includes combining 3D shape\\nreconstruction with the articulation detection pipeline.\\n\\nOur approach can have positive impacts by helping build\\nsmart robots that are able to understand and manipulate ar-\\nticulated objects. On the other hand, our approach may be\\nuseful for surveillance activities. Moreover, our network is\\ntrained on Internet videos and deep networks may amplify\\nbiases in the data.\\n\\nAcknowledgments This work was supported by the\\nDARPA Machine Common Sense Program and Toyota Re-\\nsearch Institute. Toyota Research Institute (“TRI”) provided\\nfunds to assist the authors with their research but this article\\nsolely reflects the opinions and conclusions of its authors\\nand not TRI or any other Toyota entity. We thank Dandan\\nShan, Jiaqi Geng, Sarah Jabbour and Ruiyu Li for their help\\nwith data collection, Mohamed El Banani for his help of\\nblender, Fanbo Xiang for his help of SAPIEN, as well as\\nYichen Yang and Ziyang Chen for their help of Figure 2.\\nWe also thank Justin Johnson, Jiteng Mu, Tiange Luo and\\nMax Smith for helpful discussions.\\n\\n8\\n\\n\\x0cReferences\\n\\n[1] Dhruv Batra, Angel X Chang, Sonia Chernova, Andrew J\\nDavison, Jia Deng, Vladlen Koltun, Sergey Levine, Jiten-\\ndra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Re-\\narrangement: A challenge for embodied ai. arXiv preprint\\narXiv:2011.01975, 2020. 2\\n\\n[2] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\\nHalber, Matthias Niessner, Manolis Savva, Shuran Song,\\nAndy Zeng, and Yinda Zhang. Matterport3d: Learning\\narXiv preprint\\nfrom rgb-d data in indoor environments.\\narXiv:1709.06158, 2017. 12\\n\\n[3] Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and\\nJia Deng. Hico: A benchmark for recognizing human-object\\ninteractions in images. In ICCV, 2015. 2\\n\\n[4] Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima,\\nMax Hamilton, and Jia Deng. Oasis: A large-scale dataset\\nfor single image 3d in the wild. In CVPR, 2020. 2, 3, 7, 8,\\n13\\n\\n[5] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\\nChen, and Silvio Savarese. 3D-R2N2: A unified approach\\nfor single and multi-view 3d object reconstruction. In ECCV,\\n2016. 1\\n\\n[6] Cristina Garcia Cifuentes, Jan Issac, Manuel W¨uthrich, Ste-\\nfan Schaal, and Jeannette Bohg. Probabilistic articulated\\nIEEE Robotics\\nreal-time tracking for robot manipulation.\\nand Automation Letters, 2(2):577–584, 2016. 2\\n\\n[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\\nber, Thomas Funkhouser, and Matthias Nießner. Scannet:\\nRichly-annotated 3d reconstructions of indoor scenes.\\nIn\\nCVPR, 2017. 2, 4, 7, 12, 16\\n\\n[8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\\nSanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide\\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and\\nMichael Wray. Scaling egocentric vision: The epic-kitchens\\ndataset. In ECCV, 2018. 3\\n\\n[9] Karthik Desingh, Shiyang Lu, Anthony Opipari, and\\nOdest Chadwicke Jenkins. Factored pose estimation of ar-\\nticulated objects using efficient nonparametric belief propa-\\ngation. In 2019 International Conference on Robotics and\\nAutomation (ICRA), pages 7221–7227. IEEE, 2019. 2\\n[10] David Eigen and Rob Fergus. Predicting depth, surface nor-\\nmals and semantic labels with a common multi-scale convo-\\nlutional architecture. In ICCV, 2015. 1, 6\\n\\n[11] David F. Fouhey, Vincent Delaitre, Abhinav Gupta,\\nAlexei A. Efros, Ivan Laptev, and Josef Sivic. People watch-\\ning: Human actions as a cue for single-view geometry. In\\nECCV, 2012. 2\\n\\n[15] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\\ndataset for large vocabulary instance segmentation. In CVPR,\\n2019. 3, 12\\n\\n[16] R. I. Hartley and A. Zisserman. Multiple View Geometry\\nin Computer Vision. Cambridge University Press, ISBN:\\n0521540518, second edition, 2004. 3, 12\\n\\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-\\n\\nshick. Mask r-cnn. In ICCV, 2017. 3\\n\\n[18] Vladimir\\n\\nIglovikov.\\n\\nBinary segmentation of peo-\\nple. https://github.com/ternaus/people_\\nsegmentation, 2020. 7\\n\\n[19] Vladimir Iglovikov and Alexey Shvets. Ternausnet: U-net\\nwith vgg11 encoder pre-trained on imagenet for image seg-\\nmentation. arXiv preprint arXiv:1801.05746, 2018. 7\\n[20] Phillip Isola, Joseph J. Lim, and Edward H. Adelson. Dis-\\ncovering states and transformations in image collections. In\\nCVPR, 2015. 2\\n\\n[21] Ajinkya Jain, Rudolf Lioutikov, Caleb Chuck, and Scott\\nScrewnet: Category-independent articulation\\nNiekum.\\nmodel estimation from depth images using screw theory.\\narXiv preprint arXiv:2008.10518, 2020. 1, 2\\n\\n[22] Ziyu Jiang, Buyu Liu, Samuel Schulter, Zhangyang Wang,\\nand Manmohan Chandraker. Peek-a-boo: Occlusion reason-\\ning in indoor scenes with plane representations. In CVPR,\\n2020. 4\\n\\n[23] Linyi Jin, Shengyi Qian, Andrew Owens, and David F.\\nFouhey. Planar surface reconstruction from sparse views. In\\nICCV, 2021. 4, 6, 12, 13\\n\\n[24] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-\\nemplar fine-tuning for 3d human model fitting towards\\narXiv preprint\\nin-the-wild 3d human pose estimation.\\narXiv:2004.03686, 2020. 7\\n\\n[25] Ken-ichi Kanatani. Motion segmentation by subspace sepa-\\n\\nration and model selection. In ICCV, 2001. 2\\n\\n[26] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-\\nIn\\n\\nImage segmentation as rendering.\\n\\nshick. Pointrend:\\nCVPR, 2020. 7\\n\\n[27] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,\\nand Michael J Black. Pare: Part attention regressor for 3d\\nhuman body estimation. In ICCV, 2021. 7, 8\\n\\n[28] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh Sax-\\nena. Learning human activities and object affordances from\\nRGB-D videos. The International Journal of Robotics Re-\\nsearch, 32(8):951–970, 2013. 2\\n\\n[29] Nilesh Kulkarni, Abhinav Gupta, David Fouhey, and Shub-\\nham Tulsiani. Articulation-aware canonical surface map-\\nping. In CVPR, 2020. 2\\n\\n[12] David F. Fouhey, Weicheng Kuo, Alexei A. Efros, and Jiten-\\ndra Malik. From lifestyle VLOGs to everyday interactions.\\nIn CVPR, 2018. 3\\n\\n[30] Nilesh Kulkarni, Ishan Misra, Shubham Tulsiani, and Abhi-\\nnav Gupta. 3D-RelNet: Joint object and relational network\\nfor 3D prediction. In ICCV, 2019. 6\\n\\n[13] R. Girdhar, D.F. Fouhey, M. Rodriguez, and A. Gupta.\\nLearning a predictable and generative vector representation\\nfor objects. In ECCV, 2016. 1\\n\\n[31] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn\\nAbbott, and Shuran Song. Category-level articulated object\\npose estimation. In CVPR, 2020. 1, 2, 6\\n\\n[14] Georgia Gkioxari, Ross Girshick, Piotr Dollar, and Kaiming\\nHe. Detecting and recognizing human-object interactions. In\\nCVPR, 2018. 2\\n\\n[32] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyramid\\nnetworks for object detection. In CVPR, 2017. 4\\n\\n9\\n\\n\\x0c[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C. Lawrence\\nZitnick. Microsoft coco: Common objects in context.\\nIn\\nECCV, 2014. 3, 4, 12\\n\\n[34] Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, and\\nJan Kautz. PlaneRCNN: 3D plane detection and reconstruc-\\ntion from a single image. In CVPR, 2019. 1, 2, 4\\n\\n[35] Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, and Ya-\\nsutaka Furukawa. Planenet: Piece-wise planar reconstruc-\\ntion from a single rgb image. In CVPR, 2018. 2, 4\\n\\n[36] Liu Liu, Han Xue, Wenqiang Xu, Haoyuan Fu, and Cewu\\nLu. Towards real-world category-level articulation pose es-\\ntimation. IEEE Transactions on Image Processing, 2022. 2\\n[37] Yizhou Liu, Fusheng Zha, Lining Sun, Jingxuan Li, Man-\\ntian Li, and Xin Wang. Learning articulated constraints from\\na one-shot demonstration for robot manipulation planning.\\nIEEE Access, 7:172584–172596, 2019. 1\\n\\n[38] David G Lowe. Distinctive image features from scale-\\nInternational journal of computer vi-\\n\\ninvariant keypoints.\\nsion, 60(2):91–110, 2004. 12\\n\\n[39] Frank Michel, Alexander Krull,\\n\\nEric Brachmann,\\nMichael Ying Yang, Stefan Gumhold, and Carsten Rother.\\nPose estimation of kinematic chain instances via object\\ncoordinate regression. In BMVC, 2015. 2\\n\\n[40] Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav\\nGupta, and Shubham Tulsiani. Where2act: From pixels to\\nactions for articulated 3d objects. In ICCV, 2021. 1, 2\\n[41] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille,\\nNuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning\\ndisentangled signed distance functions for articulated shape\\nrepresentation. In ICCV, 2021. 1, 2\\n\\n[42] Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian\\nChang, and Jian Jun Zhang. Total3dunderstanding: Joint lay-\\nout, object pose and mesh reconstruction for indoor scenes\\nfrom a single image. In CVPR, 2020. 6\\n\\n[43] Claudia P´erez-D’Arpino and Julie A Shah. C-learn: Learn-\\ning geometric constraints from demonstrations for multi-step\\nmanipulation in shared autonomy. In ICRA, 2017. 1\\n\\n[44] Sudeep Pillai, Matthew R Walter, and Seth Teller. Learning\\narticulated motions from visual demonstration. In RSS, 2014.\\n1, 2\\n\\n[45] Shengyi Qian, Linyi Jin, and David F. Fouhey. Associa-\\ntive3d: Volumetric reconstruction from sparse views.\\nIn\\nECCV, 2020. 6\\n\\n[46] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\\nlor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\\nGkioxari. Accelerating 3d deep learning with pytorch3d.\\narXiv:2007.08501, 2020. 12\\n\\n[47] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. In Advances in neural information pro-\\ncessing systems, pages 91–99, 2015. 3\\n\\n[48] Chris Rockwell and David F Fouhey. Full-body awareness\\n\\nfrom partial observations. In ECCV, 2020. 7, 8\\n\\n[50] Dandan Shan, Jiaqi Geng, Michelle Shu, and David Fouhey.\\nUnderstanding human hands in contact at internet scale. In\\nCVPR, 2020. 2, 3, 12\\n\\n[51] Gunnar A. Sigurdsson, G¨ul Varol, Xiaolong Wang, Ali\\nFarhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in\\nhomes: Crowdsourcing data collection for activity under-\\nstanding. In ECCV, 2016. 2, 3, 8\\n\\n[52] Gunnar A Sigurdsson, G¨ul Varol, Xiaolong Wang, Ali\\nFarhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in\\nhomes: Crowdsourcing data collection for activity under-\\nstanding. In ECCV, 2016. 5, 8, 12\\n\\n[53] Linda Smith and Michael Gasser. The development of em-\\nbodied cognition: Six lessons from babies. Artificial life,\\n11(1-2):13–29, 2005. 1\\n\\n[54] J¨urgen Sturm, Cyrill Stachniss, and Wolfram Burgard. A\\nprobabilistic framework for learning kinematic models of ar-\\nticulated objects. Journal of Artificial Intelligence Research,\\n41:477–526, 2011. 1, 2\\n\\n[55] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans,\\nYili Zhao, John Turner, Noah Maestre, Mustafa Mukadam,\\nDevendra Chaplot, Oleksandr Maksymets, et al. Habitat 2.0:\\nTraining home assistants to rearrange their habitat. arXiv\\npreprint arXiv:2106.14405, 2021. 2\\n\\n[56] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\\n\\ntransforms for optical flow. In ECCV, 2020. 2, 7, 8\\n\\n[57] Carlo Tomasi and Takeo Kanade. Shape and motion from im-\\nage streams under orthography: a factorization method. In-\\nternational journal of computer vision, 9(2):137–154, 1992.\\n2\\n\\n[58] Lorenzo Torresani, Aaron Hertzmann, and Chris Bregler.\\nNonrigid structure-from-motion: Estimating shape and mo-\\ntion with hierarchical priors. IEEE transactions on pattern\\nanalysis and machine intelligence, 30(5):878–892, 2008. 2\\n\\n[59] Shubham Tulsiani, Saurabh Gupta, David F Fouhey,\\nAlexei A Efros, and Jitendra Malik. Factoring shape, pose,\\nand layout from the 2D image of a 3D scene. In CVPR, 2018.\\n6\\n\\n[60] G¨ul Varol, Javier Romero, Xavier Martin, Naureen Mah-\\nmood, Michael J. Black, Ivan Laptev, and Cordelia Schmid.\\nLearning from synthetic humans. In CVPR, 2017. 4, 7\\n[61] Gul Varol, Javier Romero, Xavier Martin, Naureen Mah-\\nmood, Michael J Black, Ivan Laptev, and Cordelia Schmid.\\nLearning from synthetic humans. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 109–117, 2017. 16\\n\\n[62] John YA Wang and Edward H Adelson. Representing mov-\\ning images with layers. IEEE transactions on image process-\\ning, 3(5):625–638, 1994. 2\\n\\n[63] Xiaolong Wang, Ali Farhadi, and Abhinav Gupta. Actions ˜\\n\\ntransformations. In CVPR, 2016. 2\\n\\n[64] Xiaolong Wang, David Fouhey, and Abhinav Gupta. Design-\\ning deep networks for surface normal estimation. In CVPR,\\n2015. 6\\n\\n[49] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary\\nBradski. Orb: An efficient alternative to sift or surf. In ICCV,\\n2011. 3, 12\\n\\n[65] Xiaolong Wang, David F. Fouhey, and Abhinav Gupta. De-\\nsigning deep networks for surface normal estimation.\\nIn\\nCVPR, 2015. 1\\n\\n10\\n\\n\\x0c[66] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qin-\\nping Zhao, and Kai Xu. Shape2motion: Joint analysis of\\nmotion parts and attributes from 3d shapes. In CVPR, 2019.\\n2\\n\\n[67] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\\nLo, and Ross Girshick. Detectron2. https://github.\\ncom/facebookresearch/detectron2, 2019. 4, 12\\n[68] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao\\nZhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,\\nHe Wang, et al. Sapien: A simulated part-based interactive\\nenvironment. In CVPR, 2020. 2, 6, 7, 8, 16\\n\\n[69] Xiang Xu, Hanbyul Joo, Greg Mori, and Manolis Savva.\\ninteractions from\\n\\nD3d-hoi: Dynamic 3d human-object\\nvideos. arXiv preprint arXiv:2108.08420, 2021. 1, 2, 7, 8\\n\\n[70] Fengting Yang and Zihan Zhou. Recovering 3d planes from\\na single image via convolutional neural networks. In ECCV,\\n2018. 2\\n\\n[71] Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, and\\nShenghua Gao. Single-image piece-wise planar 3d recon-\\nstruction via associative embedding. In CVPR, 2019. 2\\n[72] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\\nFrom recognition to cognition: Visual commonsense reason-\\ning. In CVPR, 2019. 3\\n\\n[73] Jason Y. Zhang, Panna Felsen, Angjoo Kanazawa, and Jiten-\\ndra Malik. Predicting 3d human dynamics from video. In\\nICCV, 2019. 2\\n\\n[74] Kai Zhao, Qi Han, Chang-Bin Zhang, Jun Xu, and Ming-\\nMing Cheng. Deep hough transform for semantic line detec-\\ntion. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 2021. 6, 8\\n\\n11\\n\\n\\x0cA. Implementation Details\\n\\nDetector. Our network architecture is shown in Table 3.\\nWe use Detectron2 [67] to implement our articulation de-\\ntection and take the codebase from SparsePlanes [23]. Our\\narticulation head predicts rotation and translation axis sep-\\narately. Each of the branch takes the RoI feature from the\\nbackbone and uses four convolutional layers with 256 chan-\\nnels and two linear layers to regress the axes. The rotation\\nbranch predicts a three dimensional vector and the transla-\\ntion branch predicts a two dimensional vector. While they\\ntrain the model on Matterport3D [2], we start from COCO\\npretraining and train the model on our Internet videos. The\\ntraining is run on a single RTX 2080Ti.\\n\\nTemporal Optimization. For tracking, we use 0.5 as our\\nIoU threshold. For articulation model fitting, we use the\\nScanNet camera intrinsics as our assumed camera intrisics,\\nsince the plane and depth heads of the model is trained on\\nScanNet [7]. We use PyTorch to implement the temporal\\noptimization. For 3D transformations we use PyTorch3D\\n[46] so that it is compatible. The optimization is parallel\\nand runs on 8 GTX 1080Ti gpus of an internal cluster to\\nsave inference time and it can be run on a single gpu.\\n\\nB. Data Collection Pipeline\\n\\nOur semi-automatic data collection consists of an auto-\\nmatic pipeline to download and filter Internet videos to re-\\nmove clear negatives, and a manual annotation to label ar-\\nticulated objects. We also discuss how we filter Charades\\ndataset [52] since it involves slightly different steps.\\n\\nB.1. Filtering Internet videos\\n\\nYoutube queries. We start with 10 initial common articu-\\nlated objects in our daily life: door, laptop, oven, refrigera-\\ntor, washing machine, dishwasher, microwave oven, drawer,\\ncabinet and box. Using the combination of words, we make\\na list of queries for each initial category, e.g. “best laundry\\ntips”, following 100DOH [50]. To improve the number of\\nvideos we can find on the Internet and the diversity of the\\ndataset, we also translate the queries into Chinese, Japanese,\\nHindi and Spanish. We search these queries on Youtube and\\ndownload related Creative Commons videos.\\n\\nConverting videos to shots. Within these videos, we find\\nstationary continuous shots by fitting homographies [16] on\\nORB [49] features. In practice, we find ORB [49] are much\\nfaster and slightly more robust than SIFT [38] features, so\\nit saves a lot of computing time.\\n\\nFiltering videos based on interaction. A lot of station-\\nary shots does not contain any people or objects of interest.\\nWe further filter out video shots based on a hand interac-\\ntion detector trained on 100K+ frames of Internet data [50].\\n\\nIn practice, we find it works well and we believe it is be-\\ncause [50] is also trained on Internet data. For each video\\nshot, we run the hand interaction detector on frames evenly\\nsampled every 1 second. We remove video shots which do\\nnot have hand interactions at all.\\n\\nFiltering categories of interests. We further filter object\\nof interests by an object detector trained on COCO [33] and\\nLVIS [15]. We use the pretrained model of Faster R-CNN\\n(X101-FPN, 3x) from Detectron2 [67]. For categories that\\nCOCO does not have annotation (e.g. washing machines),\\nwe use LVIS since it has much more categories. Especially,\\nLVIS does not have annotations for doors, and we use door-\\nknob instead.\\n\\nB.2. Annotating Articulated Objects\\n\\nFinally, we use annotate articulated objects using crowd-\\nsourcing. We split video shots into 3s clips, where the fps\\nis 30. Therefore, there are 90 frames per clip.\\n\\nWe use Hive 1 as our data annotation platform. We in-\\nclude the screenshot and estimated hourly pay for each step,\\nsince these steps are separate from each other. The hourly\\npay is a rough estimation since we only have limited worker\\nstatistics provided by Hive and we do not manage it our-\\nselves.\\n\\nRecognizing articulated clips. The first step is to judge\\nif the video clips have objects which are being articulated.\\nThis is a binary classification question. We show 9 key\\nframes of the video shot (sample every 10 frames) and ask\\nworkers to classify. The screenshot is shown in Figure 7.\\nWe pay $0.015 for each clip, each clip is annotated by about\\n2 workers (which is managed by Hive based on consensus\\nand out of our control), and we estimate they can annotate\\n8 clips per minute. Therefore, the estimated hourly pay is\\n0.015 · 8 · 60/2 = $3.6.\\n\\nAnnotating bounding boxes of articulated objects. After\\nlabelling positive video clips, we annotate bounding boxes\\nof objects which are being articulated. We also ask workers\\nto specify the object is being rotated or translated. We only\\nannotate on 9 key frames of the video clip, since consecutive\\nframes tend to be similar. The screenshot is shown in Figure\\n8. We pay $0.14 for each clip, each clip is annotated by\\nabout 2 workers, and we estimate they can annotate 1.5 clips\\nper minute. Therefore, the estimated hourly pay is 0.14·1.5·\\n60/2 = $6.3.\\n\\nAnnotating rotation axis. For objects which are annotated\\nto be rotated, we annotate their rotation axes. We ask work-\\ners to draw a line to represent the 2D rotation axis. The\\nscreenshot is shown in Figure 9. We pay $0.04 for each\\nclip, each clip is annotated by about 2 workers, and we es-\\n\\n1https://thehive.ai/\\n\\n12\\n\\n\\x0cTable 3. Overall architecture for our proposed network. The backbone, RPN and plane branches are identical to [23]. The RPN predicts a\\nbounding box for each of A anchors in the input feature map. C is the number of categories (here = 2 for rotation and translation). We use\\nclass agnostic mask because the mask head is trained on ScanNet. TConv is a transpose convolution with stride 2. ReLU is used between\\nall Linear, Conv and TConv operations. Depth branch uses Conv and Deconv layers to generate a depthmap with the same resolution as\\nthe input image.\\n\\nIndex\\n\\nInputs Operation\\n\\nInputs\\n(1)\\n(2)\\n\\nInput Image\\nBackbone: ResNet50-FPN\\nRPN\\n(2),(3) RoIAlign\\n\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n(7)\\n(8)\\n(9)\\n(10)\\n\\n(4)\\n(4)\\n(4)\\n(4)\\n(4)\\n(2)\\n\\nBox: 2×downsample, Flatten, Linear(7 × 7 × 256 → 1024), Linear(1024 → 5C)\\nMask: 4 × Conv(256 → 256, 3 × 3), TConv(256 → 256, 2 × 2, 2), Conv(256 → 1 × 1)\\nNormal: 4 × Conv(256 → 256, 3 × 3), Linear(14 × 14 × 256 → 1024), Linear(1024 → 3)\\nRotation Axis: 4 × Conv(256 → 256, 3 × 3), Linear(14 × 14 × 256 → 1024), Linear(1024 → 3)\\nTranslation Axis: 4 × Conv(256 → 256, 3 × 3), Linear(14 × 14 × 256 → 1024), Linear(1024 → 2)\\nDepth\\n\\nOutput shape\\n\\nH × W × 3\\nh × w × 256\\nh × w × A × 4\\n14 × 14 × 256\\nC × 5\\n1 × 28 × 28\\n1 × 3\\nC × 3\\nC × 2\\nH × W × 1\\n\\nFigure 7. The screenshot of recognize articulated clips.\\n\\ntimate they can annotate 4 axes per minute. Therefore, the\\nestimated hourly pay is 0.04 · 4 · 60/2 = $4.8.\\n\\nAnnotating translation axis. For objects which are anno-\\ntated to be translated, we annotate their translation direc-\\ntion. We also ask workers to draw a line to represent the\\n2D rotation direction. However, since translation is only\\nrelated to the angle of the line and does not need the line\\noffset, we draw a circle at the center of the bounding box\\nand ask workers to start there. The screenshot is shown in\\nFigure 10. The estimated hourly pay is $4.8, which is the\\nsame as annotating rotation axis since it is defined as the\\nsame task “line segment” on Hive.\\n\\nAnnotating surface normals. All bounding boxes and ar-\\nticulation axes are annotated in 2D. However, in this paper,\\nwe are interested in 3D object articulation. Thus, for the\\ntest set, we also annotate the o annotate the surface normal\\nof the plane following [4], so we can evaluate how well our\\nmodel can learn 3D properties. Since the annotation of sur-\\nface normals are not available on Hive and we only need\\nsurface normals on the test set for evaluation purpose, we\\ndo all surface normals annotations ourselves. The screen-\\nshot is show in Figure 11.\\n\\nFinally, we postprocess the dataset to make sure the\\ndataset does not have offensive content, cartoons, and any\\nvideos depicting children. Since the distribution is unbal-\\n\\n13\\n\\n\\x0cFigure 8. The screenshot of annotating bounding boxes.\\n\\nFigure 9. The screenshot of annotating rotation axes.\\n\\nanced and negative examples are much more than positive\\nones, we only sample a negative clip with hand interaction\\nfrom the same video shot of positive clips.\\n\\nB.3. Annotating Charades Dataset\\n\\nWe test generalization of our method on Charades with-\\nout additional training. Data is prepared and annotated us-\\ning the following process on the original Charades test set.\\n\\nThe Charades test set contains 1863 videos of people\\n\\n14\\n\\n\\x0cFigure 10. The screenshot of annotating translation axes.\\n\\nFigure 11. The screenshot of annotating surface normals.\\n\\nacting out designated scripts. Videos are typically short at\\naround 30 seconds. The dataset contains script informa-\\ntion and additional annotation, which can be used to filter\\nvideos that are highly likely to contain articulation. After\\n\\ninitial filtering, we split filtered videos into clips of a pre-\\ndefined length. These clips are then annotated as to whether\\nthey contain articulation, for articulation bounding box lo-\\ncation (if applicable), and articulation angle (if applicable).\\n\\n15\\n\\n\\x0cFigure 12. Random examples from Sapien renderings.\\n\\nAnnotation is performed using a similar setup to YouTube\\nvia Hive. Because Charades is small, we use all clips which\\nHive workers have labeled as containing articulation.\\nFiltering criteria. Charades contains videos with acting\\ninformation, which we use to perform filtering. Each video\\nhas corresponding categorical actions that can be used to\\nfind dense articulation instances. These categorical actions\\nfall into 157 categories such as ”Holding some clothes”\\nor ”Opening a bag”, and are annotated on a one-tenth of\\na second basis. For example, one given video may have\\ntwo corresponding actions ”Holding some clothes” and\\n”Opening a bag”, which correspond to seconds 1.2 - 11.7\\nand 12.3 - 18.3, correspondingly. We selecct video clips\\norresponding to eight categorical actions for our dataset:\\n”Opening a door”, ”Closing a door”, ”Opening a laptop”,\\n”Closing a laptop”, ”Opening a closet/cabinet”, ”Closing\\na closet/cabinet”, ”Opening a refrigerator”, and ”Closing a\\nrefrigerator”.\\nGathering filtered clips. To find corresponding video\\nclips, we first calculate the middle of each action – e.g. for\\nthe 1.2 - 11.7 interval this would be 6.45. Next, we select\\na 7.5 seconds both greater than and less than the middle of\\nthe action. This is subject to beginning and ending of video,\\nand we remove overlapping clips. In our example, ”Hold-\\ning some clothes” has a middle of 6.45 seconds, so the clip\\nwould begin at 0 seconds and end at 13.95 seconds. The\\nsecond clip has a middle of 15.3, and cannot overlap with\\nthe first, so would start at 13.95 seconds and end at 22.8.\\nThis totals 649 filtered clips of <= 15 seconds.\\nSplitting video clips. Given a set of video clips which cor-\\nrespond to the selected categorical actions, we next split\\n\\n16\\n\\nclips into 3 second clips to be used for a standardized articu-\\nlation framework. Any clip less than 3 seconds is truncated.\\nThis results in 2232 3-second mini-clips.\\n\\nC. Rendering Sapien data\\n\\nIn our experiments, we test whether the model trained\\non synthetic data transfer to Internet videos using Sapien\\nrenderings [68]. Here we provide additional details about\\nrendering and example images. Examples of our renderings\\nare shown in Figure 12.\\n\\nTo generate these results, we first randomly sam-\\nple 3D objects with articulation. We filtered 1053 ob-\\njects of 18 caterories with movable planes from PartNet-\\nMobility Dataset [68]. 18 categories are: “Box”, “Dish-\\nwasher”, “Display”, “Door”, “FoldingChair”, “Laptop”,\\n“Microwave”, “Oven”, “Phone”, “Refrigerator”, “Safe”,\\n“StorageFurniture”, “Suitcase”, “Table”, “Toilet”, “Trash-\\nCan”, “WashingMachine”, and “Window”. They have sig-\\nnificant overlapping with our queries to generate Internet\\nvideos, since these objects are common objects which can\\nbe articulated. We control its rotation or translation and ren-\\nder ground truth depth, surface normal, mask, motion type\\nand 3D rotation axis. The outputs are object articulation\\nvideos without backgrounds.\\n\\nTo mimic real 3D scenes, we blend random backgrounds.\\nOtherwise the detection problem becomes trivial. We use\\nScanNet [7] images with synthetic humans [61] used to train\\nour approach, to ensure the fair comparison.\\n\\n\\x0c',\n",
       " 'Learning Instance-Specific Adaptation\\nfor Cross-Domain Segmentation\\n\\nYuliang Zou1 Zizhao Zhang2 Chun-Liang Li2 Han Zhang3\\nTomas Pfister2 Jia-Bin Huang4\\n\\n1Virginia Tech 2Google Cloud AI\\n3Google Brain 4University of Maryland, College Park\\n\\nn\\n\\ni\\na\\nm\\no\\nd\\n-\\ns\\ns\\no\\nr\\nC\\n\\nn\\no\\ni\\nt\\na\\nt\\nn\\ne\\nm\\ng\\ne\\ns\\n\\nc\\ni\\nt\\nn\\na\\nm\\ne\\nS\\n\\nc\\ni\\nt\\np\\no\\nn\\na\\nP\\n\\n)\\nl\\na\\ne\\nr\\n\\n→\\nn\\ny\\ns\\n(\\n\\n)\\ng\\no\\nf\\n\\n→\\nn\\nu\\ns\\n(\\n\\nTarget domain input\\n\\nPre-trained\\n\\nOurs\\n\\nFig. 1: Cross-domain segmentation. Models trained with the standard recipe on\\nsource domain data perform poorly on unseen target domains. On the contrary, the\\nproposed method significantly improves upon off-the-shelf pre-trained models, without\\naccessing the target domain at training time or parameter optimization at test-time.\\n\\nAbstract. We propose a test-time adaptation method for cross-domain\\nimage segmentation. Our method is simple: Given a new unseen instance\\nat test time, we adapt a pre-trained model by conducting instance-specific\\nBatchNorm (statistics) calibration. Our approach has two core compo-\\nnents. First, we replace the manually designed BatchNorm calibration rule\\nwith a learnable module. Second, we leverage strong data augmentation\\nto simulate random domain shifts for learning the calibration rule. In con-\\ntrast to existing domain adaptation methods, our method does not require\\naccessing the target domain data at training time or conducting compu-\\ntationally expensive test-time model training/optimization. Equipping\\nour method with models trained by standard recipes achieves significant\\nimprovement, comparing favorably with several state-of-the-art domain\\ngeneralization and one-shot unsupervised domain adaptation approaches.\\n\\nPredictionGround truthLossSunny imagesTraining timeTest time𝜃A segmentation modelPredictionImages from other domainsA segmentation model\\x0cCombining our method with the domain generalization methods further\\nimproves performance, reaching a new state of the art.\\n\\n2\\n\\nY. Zou et al.\\n\\n1 Introduction\\n\\nDeep neural networks have shown impressive results in many computer vision\\napplications. However, these models suffer from inevitable performance drops\\nwhen deployed in out-of-distribution environments due to domain shift [3]. For\\nexample, segmentation models trained on sunny images may perform poorly on\\nfoggy or rainy scenes [10]. Improving the cross-domain performance of deep vision\\nmodels has thus received considerable attention in recent years.\\n\\nDomain adaptation. One straightforward approach for reducing the domain\\nshift is to collect diverse labeled data in the target domain of interest for supervised\\nfine-tuning. However, collecting sufficient annotated data in the target domain\\ncould be expensive or infeasible (e.g., in continuously changing environments).\\nThis is particularly challenging for many dense prediction tasks such as image\\nsegmentation as it requires dense (pixel-wise) labels. Unsupervised domain\\nadaptation (UDA) [18,33,53,56] is an alternative route for reducing the domain\\ngap by using unlabeled target data. However, UDA methods require accessing\\ntarget domain data for model training before deployment. Such assumptions may\\nnot hold as we are not able to anticipate what scenarios the model would encounter\\n(e.g., different weather conditions) and therefore cannot collect the unlabeled\\ndata accordingly. One-shot UDA [4,32] relaxes the constraint by requiring only\\none target example for model training. The model can thus use the first example\\nencountered in the unseen target domain as the training example. However, the\\nadaptation procedure often requires thousands of training steps [32], hindering its\\napplicability as a plug-and-play module when deployed on new target domains.\\nThese UDA methods also require access to source data during the adaptation\\nprocess, which may be unrealistic at test time.\\n\\n[1,28,35,63] overcomes the above limitations\\nDomain generalization (DG)\\nby learning invariant representations using multiple source domains to improve\\nmodel robustness on unseen or continuously changing environments. Recent\\napproaches [41,62] relax the constraint by requiring one single source domain\\nonly. However, as Dubey et al. [15] points out, the optimal model learned from\\ntraining domains may be far from being optimal for an unseen target domain.\\n\\nTest-time adaptation approaches have been proposed to tackle exactly the\\nsame problem. These methods can be roughly categorized into two groups: 1)\\noptimizing model parameters at test time with a proxy task [2,51], prediction\\npseudo-label [30], or entropy regularizations [57], and 2) BatchNorm calibra-\\ntion [23,37,48]. These approaches can be applied to update models along with\\nobserving each target test data, thus observing the entire test distribution. Al-\\nternatively, they can be used to create an instance-specific model for each test\\nexample individually. Despite the flexibility, the optimization-based methods all\\nrequire time-consuming backprop computation to update model parameters.\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n3\\n\\nOur work. In this paper, we present a simple test-time adaptation method for\\ncross-domain segmentation (Figure 1). Building upon BatchNorm calibration\\nmethods [37,48], we propose to learn instance-specific calibration rules using\\nstrong data augmentations to simulate various pseudo source domains. Our\\napproach offers several advantages. First, compared with existing work [37,48]\\nwith manually determined calibration rules that require time-consuming grid\\nsearches and may not transfer to different models, our approach is data-driven and\\ninstance-specific. Second, unlike other test-time adaptation methods [2,30,51,57],\\nour work does not involve expensive gradient-based optimization for updating\\nmodel parameters at test time. Third, our method learns to calibrate BatchNorm\\nstatistics with one single instance (i.e., without accessing to a batch of samples).1\\nWe validate our proposed methods on cross-domain semantic and panoptic\\nsegmentation tasks on several benchmarks. Our experiments show a sizable boost\\nover existing adaptation methods.\\nContributions. In summary, we make the following contributions:\\n\\n– We propose a simple instance-specific test-time adaptation method and show\\nits applicability to off-the-shelf segmentation models containing BatchNorm.\\n– We conduct a detailed ablation study and analysis to validate our design\\nchoices in semantic segmentation. Applying the optimal configuration to the\\nmore complex panoptic segmentation task leads to promising performance.\\n– When combined with the models pre-trained by standard recipes, our method\\ncompares favorably with state-of-the-art one-shot UDA methods and domain\\ngeneralizing semantic segmentation methods. Our approach can also be\\ncombined with existing DG methods to improve the performance further.\\n\\n2 Related Work\\n\\nDomain adaptation. Models trained on one (source) domain often suffers from\\na severe performance drop when processing samples from unseen (target) domains.\\nDomain adaptation methods aim to mitigate this issue by adapting a pre-trained\\nmodel using samples from target domains. Unsupervised domain adaptation\\n(UDA) methods show promising results by leveraging unlabeled target data. These\\nUDA techniques include 1) domain invariant learning, 2) generative models, and\\n3) self-training. Domain invariant learning methods learn invariant features for\\nthe source and target domains by imposing an adversarial loss [18,33,53,56],\\nminimizing the domain distribution distance (e.g., MMD) [31,54] or correlation\\ndistance [50]. Applying data augmentation with generative models can also reduce\\ndomain gap using image-to-image translation [5,49,64], style transfer [16], or\\nhybrid methods that integrate with domain invariance learning methods [8,22].\\nSelf-training methods [65,66] select confident/reliable target data predictions and\\nconvert them into pseudo labels. These methods then iterate the fine-tuning and\\npseudo-labeling procedures until convergence.\\n\\n1 As we validated in Table 3, even examples come for the same test distribution,\\n\\nbatch-wise calibration may be sub-optimal when the batch size is small.\\n\\n\\x0c4\\n\\nY. Zou et al.\\n\\nWhile we have observed remarkable progress in UDA, pre-collected target\\ndomain data requirement makes it less practical. Recently, one-shot UDA meth-\\nods [4,32] have been proposed to tackle this problem. Instead of training on\\nmany unlabeled target data, these approaches require only one unlabeled target\\ndata. However, these methods require time-consuming offline training before\\ndeploying on the target domain. In contrast, our proposed method efficiently\\nadapts the model by calibrating BatchNorm on each target example on the fly,\\nwithout offline training on each target domain separately. Models trained with\\nour method can be easily applied to many different unseen domains.\\n\\nDomain generalization. Instead of adapting models to using target domain\\ndata, domain generalization [1,28,35,63] aims to train a model on source domains\\nthat are generalizable to unseen target domains by encouraging the networks to\\nlearn domain-invariant representations. However, these approaches require multi-\\nple source domains for training, which poses additional challenges in (labeled)\\ndata collection and restricts their feasibility in practical usage. To mitigate the\\ndata collection issues, single domain generalization [41] trains models on one\\nsingle source domain only by either exploiting strong data augmentation strate-\\ngies [41,55,62] to diversify the source domain training data, or performing feature\\nwhitening operations or normalization [10,17,24,40] to remove domain-specific\\ninformation during training. Similar to these methods, our method also exploits\\ndata augmentation to diversify one single source domain data. However, instead\\nof enforcing models to learn domain-invariant features, we encourage models to\\ncalibrate BatchNorm on each unseen target data at test time, by training them\\non diverse pseudo domains generated with strong data augmentation strategies\\nin training time. Our proposed method can also complement (single) domain\\ngeneralization approaches to improve the performance further.\\n\\nTest-time adaptation. Depending on the use of online training/optimization,\\ntest-time adaptation methods can be divided into two groups. First, optimization-\\nfree test-time adaptation methods mostly focus on calibrating the running statis-\\ntics inside BatchNorm layers [23,26,36,37,48] because these feature statistics carry\\ndomain-specific information [29]. However, these methods either directly replace\\nthe running statistics with current input batch statistics or mix the running\\nstatistics and current input batch statistics with a pre-defined calibration rule.\\nIn contrast, we propose to learn the instance-specific BatchNorm calibration rule\\nfrom source domain data. Second, test-time optimization methods adapt the\\nmodel parameters using a training objective such as entropy minimization [57],\\npseudo-labeling [30], or self-supervised proxy tasks [11,51]. Our experiments show\\nthat integrating our method with test-time optimization boosts performance.\\n\\n3 Learning Instance-Specific BatchNorm Calibration\\n\\nOur method applies to off-the-shelf pre-trained segmentation models containing\\nBatchNorm layers [25], a reasonable assumption in most modern CNN models.\\nIn section 3.1, we first review BatchNorm and recent test-time calibration tech-\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n5\\n\\nFig. 2: Overview.\\n(a) At training time, we learn the BatchNorm calibration rule\\n(equation 8) by training only the newly initialized parameters on the strongly-augmented\\nsource domain data; (b) At test time, we conduct instance-specific BatchNorm calibration\\nusing the learned calibration rule. Note that our method does not perform test-time\\ntraining or optimization, and thus the model parameters are fixed after training.\\n\\nniques. We then introduce our method (unconditional and conditional BatchNorm\\ncalibration) in section 3.2.\\n\\n3.1 Background\\n\\nA brief review of BatchNorm. BatchNorm has been empirically shown\\nto stabilize model training and improve model convergence speed [58], making\\nit an essential component in most modern CNN models. The inputs to each\\nBatchNorm layer are CNN features x ∈ RB×C×H×W , where B denotes the batch\\nsize, C denotes the number of feature channels, H × W denotes the spatial size.\\nBatchNorm conducts normalization followed by an affine transformation on the\\ninputs x to get outputs y ∈ RB×C×H×W\\n\\ny =\\n\\n√\\n\\n× γ + β,\\n\\nx − µ\\nσ2 + ϵ\\n\\nwhere ϵ is a small constant for numerical stability, γ ∈ RC and β ∈ RC are learn-\\nable parameters. We generalize the tensor operations by assuming broadcasting\\nwhen the dimensions are not exactly the same. Note that the definition of µ and\\nσ2 differs in training and test time. In training time, µ and σ2 are set as the\\ninput batch statistics.2\\n\\nAt test time, µ and σ2 are set to population statistics µpop, σ2\\n\\npop, accumulated\\n\\nin training time using exponential moving averaging\\n\\nµB = mean(x, axis = (B, H, W ))\\nσ2\\nB = var(x, axis = (B, H, W ))\\n\\nµpop,t = (1 − α) × µpop,t−1 + α × µB\\npop,t−1 + α × σ2\\npop,t = (1 − α) × σ2\\nσ2\\nB\\n\\n2 Following the “NamedTensor” practice [44], this computes the statistics over the B,\\n\\nH, W dimensions and return vectors with dimension C.\\n\\n(1)\\n\\n(2)\\n\\n(3)\\n\\n(4)\\n\\n(5)\\n\\nPredictionGround truthSource domain dataLossLearning BatchNorm calibration ruleStrongly-augmentedPredictionTarget domain dataCalibrate BatchNorm w/ the learned rule(a) Training time(b) Test timePre-trained model(parameters fixed)Pre-trained model(parameters fixed)\\x0c6\\n\\nY. Zou et al.\\n\\nwhere α is a scalar called momentum, and the default value is 0.1 (in PyTorch\\nconvention). Note that the population statistics update happens during training\\nin every feed-forward step.\\n\\nManual BatchNorm calibration. Despite the empirical success in in-domain\\ntesting, models with BatchNorm layers suffer from a significant performance\\ndrop when testing on out-of-distribution data. One potential reason is that\\nthe population statistics within the BatchNorm layers carry domain-specific\\ninformation [29], and thus these statistics are not suitable for normalizing inputs\\nfrom a different domain. Recent studies [29,48,57] show that, by calibrating the\\npopulation statistics with input statistics, the cross-domain performance can be\\nsignificantly improved:\\n\\ny =\\n\\nx − ((1 − m) × µpop + m × µins)\\n(cid:113)(cid:0)(1 − m) × σ2\\n(cid:1) + ϵ\\n\\npop + m × σ2\\n\\nins\\n\\n× γ + β,\\n\\n(6)\\n\\nwhere m indicates calibration strength, each method has a different empirically\\nspecified value, µins ∈ RB×C and σ2\\nins ∈ RB×C indicate instance mean and\\nvariance. Note that the above calibration step happens in every BatchNorm layer,\\nand thus the input features in later BatchNorm layers will be increasingly more\\ncalibrated.\\n\\nIn our study (Table 1(a)), we show that simply setting calibration strength m\\nas the default momentum value 0.1 can improve overall cross-domain performance.\\nHowever, we find several potential issues. First, the calibration strength m\\nis specified empirically, requiring a grid search to obtain the optimal value.\\nNevertheless, the optimal value for one setting might not well transfer to other\\nsettings with different pre-trained models or target domains of interest. Second,\\nthe calibration strength m is a scalar. However, different feature channels may\\nencode different semantic information [59]. Therefore, we may use different\\ncalibration strengths for different feature channels. Third, calibrating the mean\\nand variance with the same strength leads to sub-optimal results.\\n\\n3.2 The proposed method\\n\\nLearning to calibrate BatchNorm (InstCal-U). To address the aforemen-\\ntioned issues, we propose to learn the calibration strengths during training instead\\nof manually specifying them at test time. For simplicity, we define the following\\nfunction\\n\\nfc(a, b, m) = (⃗1 − m) × a + m × b\\n\\nThe proposed calibration and normalization process can thus be written as\\n\\n(7)\\n\\n(8)\\n\\ny =\\n\\nx − fc (µpop, µins, mµ)\\n(cid:113)\\n(cid:1) + ϵ\\n\\n(cid:0)σ2\\n\\npop, σ2\\n\\nins, mσ\\n\\nfc\\n\\n× γ + β,\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n7\\n\\nwhere mµ ∈ RC and mσ ∈ RC are two learnable parameters. More specifically,\\nwe initialize two learnable parameters mµ and mσ with the default momentum\\nvalue 0.1. We provide a detailed PyTorch implementation of this module in the\\nsupplementary material.\\n\\nGiven an off-the-shelf model, we convert all BatchNorm layers into the instance-\\nspecific calibrated format in equation 8. Note that we only train the newly\\ninitialized calibration parameters mµ and mσ, and we keep the other learnable\\nparameters (including γ and β) fixed.\\n\\nUsing training data in the source domain, we train parameters mµ and mσ\\non a diverse set of domains. Our intuition is that, by exposing the model to\\ndiverse (simulated) domains, we implicitly constrain the learnable calibration\\nparameters mµ and mσ to be robust and invariant to unseen target domains.\\nHowever, since we only have one single source domain, we need to generate\\nmultiple pseudo domains based on the source domain. Instead of adopting complex\\ngenerative models to generate pseudo domains, we find that applying appropriate\\nstrong data augmentation during training leads to promising results. We explore\\nthree different augmentation strategies: RandAugment [13], AugMix [21], and\\nDeepAugment [20], and empirically find that DeepAugment performs the best\\n(Table 1(b)). The details of augmentations are in supplementary materials.\\nLearning to conditionally calibrate BatchNorm (InstCal-C). While the\\ngoal is to learn the BatchNorm calibration parameters so that the models can\\nadapt to unseen domains at test time, the learnable parameters mµ and mσ\\nare fixed after training. We propose an optional module to enable conditional\\ncalibration to increase the flexibility.\\n\\nInstead of directly learning the parameters mµ and mσ, we propose to learn\\na set of parameters mµ,i and mσ,i for mean and variance, respectively. These\\nparameters can be viewed as the basis of calibration rules. We will use two\\nlightweight MLPs (one for each statistic) to predict the coefficients to combine\\nthe basis to get the actual calibration strength for each test example, given the\\nconcatenation of instance and population statistics. Take mµ as an example, the\\ncomputation step can be written as follows\\n\\n{cµ,i}K\\n\\n1 = Softmax (gµ (Concat(µpop, µins)))\\n\\nmµ =\\n\\ncµ,imµ,i\\n\\nK\\n(cid:88)\\n\\ni\\n\\n(9)\\n\\n(10)\\n\\nwhere cµ,i is a scalar, Concat(·) is the channel-wise concatenation operation,\\nand gµ(·) is a small 2-layer MLP. The computation of mσ is similar. We provide a\\ndetailed PyTorch implementation of this module in the supplementary material.\\nThanks to the redesign, we further increase the learnable instance-specific\\nBatchNorm calibration flexibility by setting the calibration rule to be conditional\\non the input features. As a result, the calibration rule is now dynamically changing\\naccording to different test target samples, while the inference process is still done\\nwithin one forward pass. As shown in section 4, in general, the performance\\nof instance-specific calibration (InstCal-C) improves upon the unconditional\\n\\n\\x0c8\\n\\nY. Zou et al.\\n\\ncalibration (InstCal-U) on synthetic-to-real settings where significant domain\\nshifts exist.\\n\\n4 Experimental Results\\n\\nWe mainly validate and analyze our method using the semantic segmentation\\ntasks. In section 4.8, we also apply the proposed method to panoptic segmentation\\nand observe promising results.\\n\\n4.1 Experimental setup\\n\\nWe conduct experiments on the public semantic segmentation benchmarks:\\nGTA5 [42], SYNTHIA [43], Cityscapes [12], BDD100k [60], Mapillary [38], and\\nWildDash2 [61] datasets. The GTA5 and SYNTHIA datasets are synthetic, while\\nthe others are real-world datasets. For both synthetic datasets, we split the data\\nfollowing Chen et al. [7]. For the WildDash2 dataset, we only evaluate the 19\\nclasses overlapping with Cityscapes and ignore the remaining classes. We evaluate\\nmodel performance using the standard mean intersection-over-union (mIoU)\\nmetric. We provide the implementation details in the supplementary material.\\nWe will release our source code and pre-trained models for reproducibility.\\n\\n4.2 Ablation study\\n\\nWe use GTA5 as the source domain for ablation experiments and Cityscapes\\nas the unseen target domain. We use the DeepLabv2 model with a ResNet-101\\nbackbone.\\n\\nTable 1: Ablation study. We show results from a DeepLabv2 model with a ResNet-\\n101 backbone. We train models on the GTA5 dataset and treat the Cityscapes dataset\\nas the unseen target domain for evaluation.\\n\\n(a) Calibration parameters to learn\\n\\n(b) Different augmentations\\n\\nStrategy\\n\\nmIoU (%)\\n\\nPre-trained\\nm = 0.1, fixed\\nm ∈ R\\nm ∈ RC\\nmµ ∈ RC, mσ ∈ RC\\n\\n35.7\\n40.1\\n\\n39.8\\n41.1\\n41.5\\n\\nAugmentation\\n\\nmIoU (%)\\n\\nDefault\\nAugMix [21]\\nRandAugment [13]\\nDeepAugment [20]\\n\\n35.7\\n35.9\\n37.9\\n31.7\\n\\nAugmentation\\n\\nInstCal-U InstCal-C\\n\\nDefault\\nAugMix [21]\\nRandAugment [13]\\nDeepAugment [20]\\n\\n39.7\\n40.6\\n41.1\\n41.5\\n\\n40.9\\n41.3\\n40.0\\n42.2\\n\\n#basis mIoU (%)\\n\\n2\\n4\\n8\\n16\\n\\n40.9\\n41.6\\n42.2\\n40.7\\n\\n(c) Not enough to pre-train with strong aug.\\n\\n(d) Number of basis for InstCal-C\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n9\\n\\nWhat calibration parameters should we learn? We first conduct experi-\\nments to study what calibration parameters should be learned. As shown in Ta-\\nble 1(a), suppose we directly learn a scalar parameter shared by the mean and\\nvariance. The performance is worse than using a default value of calibration\\nstrength (0.1) to calibrate BatchNorm. Learning a vector parameter works much\\nbetter than a single scalar and outperforms the baseline calibration. Separating\\nthe learned vector for mean and variance leads to further improved performance.\\nWhich data augmentation strategy should we use? As we mentioned\\nin section 3.2, since we only require one source domain for model training, we\\nneed to use strong data augmentation to simulate a diverse set of training domains.\\nIn this experiment, we study the impact of data augmentation methods.\\n\\nTable 1(b) shows that using the default weak augmentation, e.g., random\\nscaling, cropping, the performance is even worse than the default baseline. While\\nRandAugment [13] and AugMix [21] work well for InstCal-U or InstCal-C sepa-\\nrately, these two augmentation strategies do not work well in both variants. Our\\nresults show that DeepAugment [20] achieves the best overall performance. We\\nthus adopt DeepAugment as our default strong data augmentation strategy.\\nIn the\\nPre-training with strong data augmentation is not sufficient.\\nprevious study, we show that the selection of strong data augmentation is critical.\\nOne may wonder if pre-training with strong data augmentation without the\\nproposed adaptation (InstCal-U and InstCal-C) is sufficient for performance\\nimprovement. Table 1(c) shows that pre-training models using strong data\\naugmentations do not achieve the models trained with our proposed adaptation\\nmethods. AugMix [21] and RandAugment [13] can improve the performance over\\nthe baseline with standard weak augmentation, but not as significant as using\\nthem in InstCal-U or InstCal-C. If we directly use DeepAugment [20] for model\\npre-training, the performance even drops significantly. The results suggest that\\nit is necessary to apply strong data augmentation, but we need to use them\\nin the InstCal-U/InstCal-C training stage instead of simply using them during\\npre-training.\\nNumber of basis for conditional calibration. In Table 1(d), we study the\\nimpact of number of basis (K in equation 9) for InstCal-C. Using eight basis\\nleads to the best results among several options.\\n\\n4.3 Comparison with other test-time adaptation methods\\n\\nWe conduct experiments using the DeepLabv2 model with a ResNet-101 back-\\nbone. We first construct a baseline using a default value (m = 0.1) to cali-\\nbrate BatchNorm statistics. We compare with one optimization-based approach\\n(TENT [57]) and two BatchNorm calibration based methods (AdaptiveBN [48]\\nand PT-BN [37]). We use the same protocols to separately conduct test-time\\nadaptation on multiple unseen target domains. (i.e., setting test batch size to 1\\nand adapting to each test example individually).\\n\\nNote that AdaptiveBN [48], PT-BN [37], and our baseline share the same\\nformulation (equation 8) but using different m values. As shown in Table 2,\\n\\n\\x0c10\\n\\nY. Zou et al.\\n\\n(a) Cityscape\\n\\n(b) BDD100k\\n\\n(c) Mapillary\\n\\n(d) WildDash2\\n\\nFig. 3: Manually set calibration strength m. We show results from a DeepLabv2\\nmodel with a ResNet-101 backbone. The source domain is the GTA5 dataset.\\n\\nwhile both the simple baseline and AdaptiveBN [48] show improved results,\\nPT-BN [37] even hurts the pre-trained performance in many cases. TENT [57]\\nalso shows strong results in some of the test settings, but with the price of\\nsignificantly increased computation time. In contrast, the proposed InstCal-U\\nand InstCal-C outperform these test-time adaptation methods in most settings.\\nWe also note that InstCal-U performs better in real-world cross-domain settings,\\nwhile InstCal-C achieves more promising results in synthetic-to-real settings.\\n\\nIn addition to setting the calibration strength to the default value (0.1), we\\nalso experiment with different values. We try from 0.0 to 1.0 with a step size of\\n0.1, and visualize the results in Figure 3. As we can see, using scalar as strength\\nto calibrate BatchNorm is highly sensitive to selecting the values. On the contrary,\\nthe proposed InstCal-U and InstCal-C consistently perform well across unseen\\ntarget domains.\\n\\nTable 2: Generalizing across multiple domains. We show results from DeepLabv2\\nmodels with a ResNet-101 backbone. The baseline uses calibration strength m = 0.1.\\n“C” indicates Cityscapes, “B” indicates BDD100k, “M” indicates Mapillary, and “W”\\nindicates WildDash2. The best performance is in bold and the second best is underlined.\\n\\nSource: GTA5\\n\\nSource: Cityscapes\\n\\nMethod\\n\\nC\\n\\nB M W Avg. B M W Avg.\\n\\nPre-trained\\n\\n35.7 32.9 41.1 27.4 34.3 41.2 49.5 33.9 41.5\\n\\nBaseline (m = 0.1) 40.1 37.4 45.3 32.0 38.7 42.8 51.9 37.6 44.1\\n39.0 36.3 44.3 30.8 37.6 43.0 52.4 37.2 44.2\\nAdaptiveBN [48]\\n33.9 34.3 40.5 27.8 34.1 34.4 39.1 28.8 34.1\\nPT-BN [37]\\n38.1 37.8 44.7 32.5 38.3 44.2 52.3 36.8 44.4\\nTENT [57]\\nInstCal-U (Ours)\\n41.5 39.4 46.0 34.4 40.3 45.1 52.2 40.3 45.9\\nInstCal-C (Ours) 42.2 40.2 46.8 35.3 41.1 44.3 51.5 39.3 45.0\\n\\n4.4 Analysis\\n\\nFor the following studies, we use DeepLabv2 models with a ResNet-101 backbone.\\nImprovement on in-domain performance. We test if our models can im-\\nprove the performance for source domain. We do so by evaluating the trained\\n\\n0.00.10.20.30.40.50.60.70.80.91.0Calibration strength3436384042mIoU (%)InstCal-UInstCal-CBaseline (m=0.1)0.00.10.20.30.40.50.60.70.80.91.0Calibration strength3334353637383940mIoU (%)InstCal-UInstCal-CBaseline (m=0.1)0.00.10.20.30.40.50.60.70.80.91.0Calibration strength41424344454647mIoU (%)InstCal-UInstCal-CBaseline (m=0.1)0.00.10.20.30.40.50.60.70.80.91.0Calibration strength272829303132333435mIoU (%)InstCal-UInstCal-CBaseline (m=0.1)\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n11\\n\\nmodel on the test split of the source data. We report in Table 3(a) that our\\nlearned BatchNorm calibration (both unconditional and conditional) still acheive\\nsizable performance gain.\\n\\nTable 3: Analysis. Results are from DeepLabv2 models with a ResNet-101 backbone.\\n\\n(a) In-domain performance\\n\\nMethod\\n\\nGTA5 Cityscapes\\n\\nPre-trained 69.1\\nInstCal-U 70.3\\nInstCal-C 70.5\\n(c) Model calibration\\n\\n66.1\\n66.6\\n66.8\\n\\n(b) Input batch statistics\\n16\\n1\\n\\n8\\n\\n4\\n\\n2\\n\\nBatch size\\n\\nBaseline (m = 1) 40.1 39.8 39.7 39.7 39.6\\n41.5 41.2 40.9 40.8 40.7\\nInstCal-U\\n42.2 41.8 41.5 41.5 41.4\\nInstCal-C\\n(d) Test-time optimization\\n\\nMethod\\n\\nPre-trained\\nTENT [57]\\n\\nInstCal-U\\nInstCal-U + entropy min. [57]\\n\\nInstCal-C\\nInstCal-C + entropy min. [57]\\n\\nmIoU (%)\\n\\n35.7\\n38.1\\n\\n41.5\\n44.1\\n\\n42.2\\n44.2\\n\\nInput batch statistics v.s. instance statistics As mentioned in section 3.2,\\nwe compute input instance statistics for each test example, instead of computing\\nthe batch statistics for mixing statistics across different examples within a mini-\\nbatch. We validate this design choice by replacing instance statistics with batch\\nstatics using different batch sizes during test time. Table 3(b) shows that the\\nperformance drops as we increase the batch size, even though the test examples\\ncome from the same target distribution. We conjecture the performance will\\nworsen if the mini-batch contains test examples from multiple target domains.\\nThus, we stick to using the input instance statistics.\\nImprovement on model calibration. We compute the expected calibration\\nerror (ECE) [19] for the pre-trained model, baseline update (m = 0.1), PT-BN [37],\\nand the proposed InstCal-U/InstCal-C. As shown in Table 3(c), calibrating\\nBatchNorm statistics indeed reduces model calibration error.\\nCompatible with test-time optimization. We incorporate the optimization-\\nbased method, TENT [57], into our methods, by optimizing the prediction entropy\\nat test-time. Following TENT [57], we only optimize the weight γ and bias β in\\nBatchNorm layers. Moreover, we conduct instance-specific adaptation. Table 3(d)\\nshows the complementary nature of these two strategies.\\nRunning time. We test the inference speed on Cityscapes on a single V100 GPU.\\nThe pre-trained model takes 39 ms to process each testing sample (1024×512\\nresolution). The BatchNorm calibration method [37] induce a 60 ms overhead.\\nOur method increases the inference time by 58 ms (for InstCal-U) and 149 ms\\n(InstCal-C).\\n\\nGTA5 -> CS101214161820ECE (%)Pre-trainedBaseline (m=0.1)PT-BNInstCal-U (Ours)InstCal-C (Ours)\\x0c12\\n\\nY. Zou et al.\\n\\n4.5 Comparison with one-shot unsupervised domain adaptation\\n\\nThis section compares the proposed method with recent state-of-the-art one-\\nshot UDA methods. One-shot UDA methods adapt source domain pre-trained\\nmodels on one single unlabeled target example offline. In contrast, InstCal-\\nU and InstCal-C adapt pre-trained models on the fly on each test example\\nindividually. Conceptually, one-shot UDA methods and InstCal-U/InstCal-C\\nuse the same amount of data for adaptation. However, one-shot UDA methods\\nusually require time-consuming offline training, and thus it is impossible to\\nadapt models on each target example separately. So these methods only adapt\\nthe models using one single unlabeled (training) example and then deploy the\\nadapted model at test-time without adaptation. As shown in Table 4, simply\\naugmenting pre-trained models with InstCal-U/InstCal-C, compares favorably\\nwith recent one-shot UDA methods and even outperforms the state of the arts\\nby a large margin in Synthia→Cityscapes setting.\\n\\nTable 4: Comparison with one-shot unsupervised domain adaptation. All\\nresults are from modified DeepLabv2 models (specific for domain adaptation). The best\\nperformance is in bold and the second best is underlined.\\n\\nGTA5→Cityscapes\\n\\nSynthia→Cityscapes\\n\\nmIoU\\n\\nmIoU (13-class) mIoU (16-class)\\n\\nMethod\\n\\nCLAN [33]\\nAdvEnt [56]\\nCBST [65]\\nOST [4]\\nASM [32]\\n\\nSource-only pre-trained\\n+ InstCal-U (Ours)\\n+ InstCal-C (Ours)\\n\\n37.7\\n36.1\\n37.1\\n42.3\\n43.2\\n\\n36.2\\n42.4\\n42.2\\n\\n40.4\\n39.9\\n38.5\\n42.8\\n40.7\\n\\n36.2\\n43.5\\n44.1\\n\\n-\\n-\\n-\\n-\\n34.6\\n\\n31.6\\n37.7\\n38.1\\n\\n4.6 Comparison with domain generalizing segmentation\\n\\nThis section compares our InstCal-U/InstCal-C with recent domain generalizing\\n(DG) semantic segmentation approaches. We use the DeepLabv3+ model with a\\nResNet-50 backbone. As shown in Table 5, upgrading non-DG pre-trained weak\\nmodels with InstCal-U/InstCal-C compares favorably with these strong domain\\ngeneralizing segmentation methods across different testing settings. Our method\\neven outperforms all the methods except ISW [10] by a large margin.\\n\\nNote that our method and these domain generalizing methods complement\\neach other. Thus, we can also incorporate our methods on top of these domain\\ngeneralizing segmentation methods. As shown in Figure 4, the proposed method\\nconsistently improves the performance of these methods. Our method can even\\nimprove the strong ISW [10] approach and achieve a new state of the art.\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n13\\n\\nTable 5: Comparison with state-of-the-art domain generalizing semantic\\nsegmentation methods. We show results from a DeepLabv3+ model with a ResNet-\\n50 backbone. “C” indicates Cityscapes, “B” indicates BDD100k, and “M” indicates\\nMapillary. The best performance is in bold and the second best is underlined.\\n\\nSource: GTA5\\n\\nSource: Cityscapes\\n\\nMethod\\n\\nC\\n\\nB M Avg. B M Avg.\\n\\nSW [40]\\nIBN-Net [39]\\nIterNorm [24]\\nISW [10]\\n\\n52.2\\n29.9 27.5 29.7 29.0 48.5 55.8\\n52.8\\n33.9 32.3 37.8 34.7 48.6 57.0\\n31.8 32.7 33.9 32.8 49.2 56.3\\n52.8\\n36.6 35.2 40.3 37.4 50.7 58.6 54.7\\n\\n49.3\\nNon-DG pre-trained 29.6 25.7 28.5 27.9 46.1 52.5\\n+ InstCal-U (Ours) 39.8 32.9 38.6 37.1 51.1 58.5 54.8\\n54.1\\n+ InstCal-C (Ours) 40.3 32.9 38.7 37.3 50.5 57.7\\n\\n(a) GTA5→Cityscapes\\n\\n(b) GTA5→BDD100k\\n\\n(c) GTA5→Mapillary\\n\\nFig. 4: Combining domain generalization with our method. In addition to the\\nmodel pre-trained with the standard recipe (non-DG, labeled as “Default”), we choose\\nthree DG methods: SW [40], IBN-Net [39], and ISW [10]. All methods use DeepLabv3+\\nmodels with a ResNet-50 backbone, trained on the GTA5 dataset.\\n\\n4.7 Backbone network agnostic\\n\\nIn previous sections, we have shown the proposed method can improve pre-trained\\nmodel performance on multiple unseen target domains. However, we only conduct\\nexperiments using the ResNet backbones. In this section, we use the DeepLabv3+\\nmodel with ShuffleNetV2 [34] and MobileNetV2 [47] as backbones, to demonstrate\\nour methods is network-agnostic. As shown in Table 6, the proposed methods\\ncan also improve pre-trained models with these backbones by a large margin,\\noutperforming the recent state of the arts.\\n\\n4.8 Panoptic segmentation results\\n\\nIn this section, we directly apply InstCal-U/InstCal-C to an even more challenging\\ntask, panoptic segmentation [27]. We start with the off-the-shelf models from\\nPanoptic-DeepLab [9] and train these models on the Cityscapes dataset. We test\\nthe models on Foggy Cityscapes [46], which inserts synthetic fog into the original\\nCityscapes clear images with three strength levels (0.005, 0.01, and 0.02). We\\nadopt panoptic quality (PQ), mean intersection-over-union (mIoU), and mean\\n\\nDefaultSWIBN-NetISW283032343638404244mIoU (%)Pre-trained+ InstCal-U+ InstCal-CDefaultSWIBN-NetISW2426283032343638mIoU (%)Pre-trained+ InstCal-U+ InstCal-CDefaultSWIBN-NetISW283032343638404244mIoU (%)Pre-trained+ InstCal-U+ InstCal-C\\x0c14\\n\\nY. Zou et al.\\n\\nTable 6: The proposed module is backbone network agnostic. We show results\\nfrom a DeepLabv3+ model with ShuffleNetV2 and MobileNetV2 as backbones. These\\nmodels are trained on the GTA5 dataset. “C” indicates Cityscapes, “B” indicates\\nBDD100k, and “M” indicates Mapillary. The best performance is in bold and the\\nsecond best is underlined.\\n\\nShuffleNetV2\\n\\nMobileNetV2\\n\\nMethod\\n\\nIBN-Net [39]\\nISW [10]\\n\\nC\\n\\nB M Avg. C\\n\\nB M Avg.\\n\\n27.1 31.8 34.9 31.3 30.1 27.7 27.1 28.3\\n31.0 32.1 35.3 32.8 30.9 30.1 30.7 30.6\\n\\nNon-DG pre-trained 25.7 22.1 28.3 25.4 27.1 27.5 27.3 27.3\\n+ InstCal-U (Ours) 35.8 31.1 36.4 34.4 37.2 31.2 34.5 34.3\\n+ InstCal-C (Ours) 35.9 30.8 35.4 34.0 37.8 30.0 33.9 33.9\\n\\naverage precision (mAP) as the evaluation metrics. As shown in Table 7, InstCal-\\nU/InstCal-C greatly improves off-the-shelf Panoptic-DeepLab performance on\\nout-of-distribution foggy scenes by a large margin, validating the proposed method\\nis universally applicable to different image segmentation tasks without further\\ntuning. We also provide visual results in Figure 1 and supplementary material.\\n\\nTable 7: Panoptic segmentation results. We show results on two Panoptic-DeepLab\\nmodel variants (w/ and w/o depthwise separable convolution). The best performance is\\nin bold and the second best is underlined.\\n\\n0.005\\n\\nSynthetic fog strength\\n0.01\\n\\n0.02\\n\\nMethod\\n\\nw/ DSConv PQ mIoU mAP PQ mIoU mAP PQ mIoU mAP\\n\\nPre-trained\\n+ InstCal-U\\n+ InstCal-C\\n\\nPre-trained\\n+ InstCal-U\\n+ InstCal-C\\n\\n×\\n×\\n×\\n\\n✓\\n✓\\n✓\\n\\n53.3 72.2 25.3 45.0 64.9 18.8 32.6 52.8 11.6\\n56.6 75.7 28.9 51.1 71.9 24.3 42.8 64.5 18.5\\n56.6 75.7 28.5 51.2 71.9 24.4 42.4 64.8 18.0\\n\\n53.0 73.2 24.5 45.3 66.5 18.3 33.1 54.8 11.5\\n55.5 76.0 27.2 49.1 71.3 21.8 40.4 63.2 16.2\\n55.6 76.3 27.6 48.9 71.6 22.4 40.9 64.1 16.8\\n\\n5 Discussions\\n\\nThis paper proposes a simple learning-based test-time adaptation method for\\ncross-domain segmentation. The proposed method is learned to perform instance-\\nspecific BatchNorm calibration during training, without time-consuming test-\\ntime parameter optimization. As a result, our method is efficient and effective,\\ndemonstrating competitive performance across multiple cross-domain image\\nsegmentation settings.\\nLimitations. Currently, we conduct calibration for every BatchNorm layers.\\nIt will be interesting to study which layer is more important and thus only\\ncalibrate specific layers to increase inference speed. And it will be interesting\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n15\\n\\nto extend our method to other normalization layers (e.g., LayerNorm for Vision\\nTransformers [14]) and other challenging tasks. We leave these as future work.\\n\\nAcknowledgement\\n\\nWe thank Sayna Ebrahimi, Shih-Yang Su, and Yun-Chun Chen for their valuable\\ncomments. Y. Zou and J.-B. Huang were supported in part by NSF under Grant\\nNo. (#1755785).\\n\\nReferences\\n\\n1. Balaji, Y., Sankaranarayanan, S., Chellappa, R.: Metareg: Towards domain gener-\\n\\nalization using meta-regularization. In: NeurIPS (2018) 2, 4\\n\\n2. Bartler, A., B¨uhler, A., Wiewel, F., D¨obler, M., Yang, B.: Mt3: Meta test-time\\ntraining for self-supervised test-time adaption. arXiv preprint arXiv:2103.16201\\n(2021) 2, 3\\n\\n3. Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., Vaughan, J.W.: A\\ntheory of learning from different domains. Machine learning 79(1), 151–175 (2010)\\n2\\n\\n4. Benaim, S., Wolf, L.: One-shot unsupervised cross domain translation. In: NeurIPS\\n\\n(2018) 2, 4, 12\\n\\n5. Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D.: Unsupervised\\npixel-level domain adaptation with generative adversarial networks. In: CVPR\\n(2017) 3\\n\\n6. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:\\nSemantic image segmentation with deep convolutional nets, atrous convolution,\\nand fully connected crfs. TPAMI (2017) 18\\n\\n7. Chen, M., Xue, H., Cai, D.: Domain adaptation for semantic segmentation with\\n\\nmaximum squares loss. In: ICCV (2019) 8, 18\\n\\n8. Chen, Y.C., Lin, Y.Y., Yang, M.H., Huang, J.B.: Crdoco: Pixel-level domain transfer\\n\\nwith cross-domain consistency. In: CVPR (2019) 3\\n\\n9. Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.:\\nPanoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic\\nsegmentation. In: CVPR (2020) 13, 19\\n\\n10. Choi, S., Jung, S., Yun, H., Kim, J.T., Kim, S., Choo, J.: Robustnet: Improving\\ndomain generalization in urban-scene segmentation via instance selective whitening.\\nIn: CVPR (2021) 2, 4, 12, 13, 14, 18\\n\\n11. Cohen, T., Shulman, N., Morgenstern, H., Mechrez, R., Farhan, E.: Self-supervised\\ndynamic networks for covariate shift robustness. arXiv preprint arXiv:2006.03952\\n(2020) 4\\n\\n12. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\\nFranke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\\nunderstanding. In: CVPR (2016) 8\\n\\n13. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated\\ndata augmentation with a reduced search space. In: CVPR Workshop (2020) 7, 8,\\n9, 19\\n\\n14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth\\n16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 15\\n\\n\\x0c16\\n\\nY. Zou et al.\\n\\n15. Dubey, A., Ramanathan, V., Pentland, A., Mahajan, D.: Adaptive methods for\\n\\nreal-world domain generalization. In: CVPR (2021) 2\\n\\n16. Dundar, A., Liu, M.Y., Wang, T.C., Zedlewski, J., Kautz, J.: Domain stylization:\\nA strong, simple baseline for synthetic to real image domain adaptation. arXiv\\npreprint arXiv:1807.09384 (2018) 3\\n\\n17. Fan, X., Wang, Q., Ke, J., Yang, F., Gong, B., Zhou, M.: Adversarially adaptive\\n\\nnormalization for single domain generalization. In: CVPR (2021) 4\\n\\n18. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F.,\\nMarchand, M., Lempitsky, V.: Domain-adversarial training of neural networks.\\nJMLR 17(1), 2096–2030 (2016) 2, 3\\n\\n19. Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural\\n\\nnetworks. In: ICML (2017) 11\\n\\n20. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai,\\nR., Zhu, T., Parajuli, S., Guo, M., et al.: The many faces of robustness: A critical\\nanalysis of out-of-distribution generalization. In: ICCV (2021) 7, 8, 9, 19\\n\\n21. Hendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.:\\nAugmix: A simple data processing method to improve robustness and uncertainty.\\nIn: ICLR (2020) 7, 8, 9, 19\\n\\n22. Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A., Darrell,\\nT.: Cycada: Cycle-consistent adversarial domain adaptation. In: ICML (2018) 3\\n23. Hu, X., Uzunbas, G., Chen, S., Wang, R., Shah, A., Nevatia, R., Lim, S.N.:\\nMixnorm: Test-time adaptation through online normalization estimation. arXiv\\npreprint arXiv:2110.11478 (2021) 2, 4\\n\\n24. Huang, L., Zhou, Y., Zhu, F., Liu, L., Shao, L.: Iterative normalization: Beyond\\n\\nstandardization towards efficient whitening. In: CVPR (2019) 4, 13\\n\\n25. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\\n\\nreducing internal covariate shift. In: ICML (2015) 4\\n\\n26. Khurana, A., Paul, S., Rai, P., Biswas, S., Aggarwal, G.: Sita: Single image test-time\\n\\nadaptation. arXiv preprint arXiv:2112.02355 (2021) 4\\n\\n27. Kirillov, A., He, K., Girshick, R., Rother, C., Doll´ar, P.: Panoptic segmentation. In:\\n\\nCVPR (2019) 13\\n\\n28. Li, D., Zhang, J., Yang, Y., Liu, C., Song, Y.Z., Hospedales, T.M.: Episodic training\\n\\nfor domain generalization. In: ICCV (2019) 2, 4\\n\\n29. Li, Y., Wang, N., Shi, J., Liu, J., Hou, X.: Revisiting batch normalization for\\n\\npractical domain adaptation. In: ICLR Workshop (2017) 4, 6\\n\\n30. Liang, J., Hu, D., Feng, J.: Do we really need to access the source data? source\\nhypothesis transfer for unsupervised domain adaptation. In: ICML (2020) 2, 3, 4\\n31. Long, M., Cao, Y., Wang, J., Jordan, M.: Learning transferable features with deep\\n\\nadaptation networks. In: ICML (2015) 3\\n\\n32. Luo, Y., Liu, P., Guan, T., Yu, J., Yang, Y.: Adversarial style mining for one-shot\\n\\nunsupervised domain adaptation. In: NeurIPS (2020) 2, 4, 12\\n\\n33. Luo, Y., Zheng, L., Guan, T., Yu, J., Yang, Y.: Taking a closer look at domain\\nshift: Category-level adversaries for semantics consistent domain adaptation. In:\\nCVPR (2019) 2, 3, 12\\n\\n34. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines for\\n\\nefficient cnn architecture design. In: ECCV (2018) 13\\n\\n35. Matsuura, T., Harada, T.: Domain generalization using a mixture of multiple latent\\n\\ndomains. In: AAAI (2020) 2, 4\\n\\n36. Mirza, M.J., Micorek, J., Possegger, H., Bischof, H.: The norm must go on: Dynamic\\nunsupervised domain adaptation by normalization. arXiv preprint arXiv:2112.00463\\n(2021) 4\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n17\\n\\n37. Nado, Z., Padhy, S., Sculley, D., D’Amour, A., Lakshminarayanan, B., Snoek, J.:\\nEvaluating prediction-time batch normalization for robustness under covariate shift.\\narXiv preprint arXiv:2006.10963 (2020) 2, 3, 4, 9, 10, 11\\n\\n38. Neuhold, G., Ollmann, T., Rota Bulo, S., Kontschieder, P.: The mapillary vistas\\n\\ndataset for semantic understanding of street scenes. In: ICCV (2017) 8\\n\\n39. Pan, X., Luo, P., Shi, J., Tang, X.: Two at once: Enhancing learning and general-\\n\\nization capacities via ibn-net. In: ECCV (2018) 13, 14\\n\\n40. Pan, X., Zhan, X., Shi, J., Tang, X., Luo, P.: Switchable whitening for deep\\n\\nrepresentation learning. In: ICCV (2019) 4, 13\\n\\n41. Qiao, F., Zhao, L., Peng, X.: Learning to learn single domain generalization. In:\\n\\nCVPR (2020) 2, 4\\n\\n42. Richter, S.R., Vineet, V., Roth, S., Koltun, V.: Playing for data: Ground truth\\n\\nfrom computer games. In: ECCV (2016) 8\\n\\n43. Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia dataset:\\nA large collection of synthetic images for semantic segmentation of urban scenes.\\nIn: CVPR (2016) 8\\n44. Rush, A.: Tensor\\nNamedTensor 5\\n\\nhttp://nlp.seas.harvard.edu/\\n\\nconsidered\\n\\nharmful,\\n\\n45. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpa-\\nthy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recognition\\nchallenge. IJCV 115(3), 211–252 (2015) 18\\n\\n46. Sakaridis, C., Dai, D., Van Gool, L.: Semantic foggy scene understanding with\\n\\nsynthetic data. IJCV 126(9), 973–992 (Sep 2018) 13\\n\\n47. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted\\n\\nresiduals and linear bottlenecks. In: CVPR (2018) 13\\n\\n48. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., Bethge, M.: Im-\\nproving robustness against common corruptions by covariate shift adaptation. In:\\nNeurIPS (2020) 2, 3, 4, 6, 9, 10\\n\\n49. Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., Webb, R.: Learning\\nfrom simulated and unsupervised images through adversarial training. In: CVPR\\n(2017) 3\\n\\n50. Sun, B., Saenko, K.: Deep coral: Correlation alignment for deep domain adaptation.\\n\\nIn: ECCV (2016) 3\\n\\n51. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., Hardt, M.: Test-time training with\\nself-supervision for generalization under distribution shifts. In: ICML (2020) 2, 3, 4\\n52. Theis, L., Shi, W., Cunningham, A., Husz´ar, F.: Lossy image compression with\\n\\ncompressive autoencoders. In: ICLR (2017) 19\\n\\n53. Tzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial discriminative domain\\n\\nadaptation. In: CVPR (2017) 2, 3\\n\\n54. Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T.: Deep domain confusion:\\nMaximizing for domain invariance. arXiv preprint arXiv:1412.3474 (2014) 3\\n55. Volpi, R., Namkoong, H., Sener, O., Duchi, J.C., Murino, V., Savarese, S.: Gener-\\nalizing to unseen domains via adversarial data augmentation. In: NeurIPS (2018)\\n4\\n\\n56. Vu, T.H., Jain, H., Bucher, M., Cord, M., P´erez, P.: Advent: Adversarial entropy\\nminimization for domain adaptation in semantic segmentation. In: CVPR (2019) 2,\\n3, 12\\n\\n57. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., Darrell, T.: Tent: Fully test-time\\n\\nadaptation by entropy minimization. In: ICLR (2021) 2, 3, 4, 6, 9, 10, 11\\n\\n58. Wu, Y., Johnson, J.: Rethinking” batch” in batchnorm. arXiv preprint\\n\\narXiv:2105.07576 (2021) 5\\n\\n\\x0c18\\n\\nY. Zou et al.\\n\\n59. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., Lipson, H.: Understanding neural\\n\\nnetworks through deep visualization. In: ICML Workshop (2014) 6\\n\\n60. Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell,\\nT.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In:\\nCVPR (2020) 8\\n\\n61. Zendel, O., Honauer, K., Murschitz, M., Steininger, D., Dominguez, G.F.: Wilddash\\n\\n- creating hazard-aware benchmarks. In: ECCV (2018) 8\\n\\n62. Zhao, L., Liu, T., Peng, X., Metaxas, D.: Maximum-entropy adversarial data\\naugmentation for improved generalization and robustness. In: NeurIPS (2020) 2, 4\\n63. Zhao, S., Gong, M., Liu, T., Fu, H., Tao, D.: Domain generalization via entropy\\n\\nregularization. In: NeurIPS (2020) 2, 4\\n\\n64. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\\n\\nusing cycle-consistent adversarial networks. In: ICCV (2017) 3\\n\\n65. Zou, Y., Yu, Z., Kumar, B., Wang, J.: Unsupervised domain adaptation for semantic\\n\\nsegmentation via class-balanced self-training. In: ECCV (2018) 3, 12\\n\\n66. Zou, Y., Yu, Z., Liu, X., Kumar, B., Wang, J.: Confidence regularized self-training.\\n\\nIn: ICCV (2019) 3\\n\\nSupplementary Matarial\\n\\nIn this supplementary document, we provide additional experimental results and\\ndetails to complement the main manuscript. First, we provide the implementation\\ndetails of our experiments. Second, we describe the three data augmentation\\nstrategies we explore. Lastly, we show qualitative results of different cross-domain\\nsegmentation settings.\\n\\nA Implementation details\\n\\nWe conduct all the experiments using the PyTorch framework with one single\\nV100 GPU.\\nSemantic segmentation. We start with a DeepLabv2 model [6] with a ResNet-\\n101 backbone pre-trained on ImageNet [45]. We follow Chen et al. [7] to pre-train\\nthe source-only model. In our proposed learning stage, we freeze the model\\nparameters except for the proposed module and train the model with the SGD\\noptimizer with momentum 0.9 and weight decay 5×10−4. We use a learning rate\\nof 2.5×10−3 for InstCal-U and a learning rate of 2.5×10−2 for InstCal-C. We\\nadopt the polynomial learning rate decay schedule as in Chen et al. [7] and set\\nthe total number of training iterations to 80,000. We set the batch size to one.\\nTo compare with recent domain generalizing semantic segmentation methods,\\nwe adopt the implementation and off-the-shelf models from RobustNet [10]. We\\ndo not conduct pre-training and directly train these off-the-shelf models using\\nthe proposed method. For these DeepLabv3+ models, we set the learning rate as\\n2.5×10−3 and reduce the batch size for single GPU training (i.e., 8 for ResNet-50,\\n4 for ShuffleNetV2 and MobileNetV2). The other training hyper-parameters are\\nkept the same. Please refer to Choi et al. [10] for more details.\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n19\\n\\nPanoptic segmentation. We adopt off-the-shelf models from Panoptic-DeepLab [9],\\nimplemented in the PyTorch Detectron2 codebase. We use a learning rate of\\n2.5×10−4. We set the batch size to 4. The other training hyper-parameters are\\nkept the same. Please refer to the PyTorch implementation for more details.3\\n\\nB Strong data augmentation strategies\\n\\nWe provide details for the three strong data augmentation strategies we adopted.\\nRandAugment [13]. RandAugment samples m operations from a pre-defined\\nlist of image augmentation operations and composes them to form the final\\naugmentation for each input data. In this paper, we set m = 2, and we only\\nadopt the color-related augmentation operations in the pre-defined list: Identity,\\nAutoContrast, Invert, Equalize, Solarize, Posterize, Color, Brightness, Sharpness.\\nAugMix [21]. AugMix is proposed to improve model robustness and uncertainty\\nestimation. AugMix constructs three augmentation paths with cascaded one, two,\\nand three augmentation operations for each input data. The three augmented\\ndata are then linearly combined, where the combination weights are sampled from\\na Dirichlet distribution. Finally, the original and augmented images are linearly\\ncombined to construct the final augmented image, where the combination weight\\nis sampled from a Beta distribution. We adopt all the augmentation operations,\\nincluding geometric transforms, and we use the original annotation as the ground\\ntruth for the final augmented image. We only use the augmentation but not the\\nJensen-Shannon divergence consistency loss described in Hendrycks et al. [21].\\nDeepAugment [20]. Instead of composing basic image augmentation operators\\nto construct a strong data augmentation, DeepAugment transforms the input\\nimage by feeding it into an image-to-image translation network and randomly\\nperturbing the intermediate feature representations. There are three options\\nprovided in the official implementation. We adopt the CAE [52] variant, which is\\ninitially used for the image compression task.\\n\\nC Qualitative results\\n\\nWe visualize model prediction results in Figure 5 and Figure 6. For cross-domain\\nsemantic segmentation, the proposed method consistently improves over different\\nsettings. For cross-domain panoptic segmentation, we can see that the proposed\\nmethod dramatically improves the background segmentation, such as trees.\\n\\n3 https://github.com/facebookresearch/detectron2/tree/main/projects/\\n\\nPanoptic-DeepLab\\n\\n\\x0c20\\n\\nY. Zou et al.\\n\\nInput\\n\\nGround truth\\n\\nPre-trained\\n\\nInstCal-U\\n\\nInstCal-C\\n\\nFig. 5: Qualitative results of semantic segmentation. Models are trained on the\\nGTA5 dataset. We treat the Cityscapes, BDD100k, Mapillary, and WildDash2 datasets\\nas the unseen target domains.\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n21\\n\\nInput\\n\\nPre-trained\\n\\nInstCal-U\\n\\nInstCal-C\\n\\nFig. 6: Qualitative results of panoptic segmentation. Models are trained on the\\nclean Cityscapes dataset and tested on the Foggy Cityscapes dataset.\\n\\n\\x0c',\n",
       " 'CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic\\nSurface Representation via Neural Homeomorphism\\n\\nJiahui Lei Kostas Daniilidis\\nUniversity of Pennsylvania\\n\\nhttps://www.cis.upenn.edu/˜leijh/projects/cadex\\n\\nFigure 1. We model the deformable surface through a learned canonical shape (the middle shape) and factorize the deformation (orange\\narrow) by learnable continuous bijective canonical maps (green bidirectional arrows) that provide the cycle consistency and topology\\npreservation. Visual results are from the test set on dynamic animals, human bodies and articulated objects.\\n\\nAbstract\\n\\nWhile neural representations for static 3D shapes are\\nwidely studied, representations for deformable surfaces\\nare limited to be template-dependent or to lack efficiency.\\nWe introduce Canonical Deformation Coordinate Space\\n(CaDeX), a unified representation of both shape and non-\\nrigid motion. Our key insight is the factorization of the de-\\nformation between frames by continuous bijective canon-\\nical maps (homeomorphisms) and their inverses that go\\nthrough a learned canonical shape. Our novel deforma-\\ntion representation and its implementation are simple, ef-\\nficient, and guarantee cycle consistency, topology preser-\\nvation, and, if needed, volume conservation. Our mod-\\nelling of the learned canonical shapes provides a flexible\\nand stable space for shape prior learning. We demonstrate\\nstate-of-the-art performance in modelling a wide range of\\ndeformable geometries: human bodies, animal bodies, and\\narticulated objects.\\n\\n1. Introduction\\n\\nHumans perceive, interact, and learn in a continuously\\nchanging real world. One of our key perceptual capabil-\\nities is the modeling of a dynamic 3D world. Such geo-\\nmetric intelligence requires sufficiently general neural rep-\\nresentations that can model different dynamic geometries\\nin 4D sequences to facilitate solving robotics [55], com-\\nputer vision [29], and graphics [44] tasks. Unlike the widely\\nstudied 3D neural representations, a dynamic representa-\\ntion has to be able to associate (for example, finding corre-\\n\\nspondence) and aggregate (for example, reconstruction and\\ntexturing) information across the deformation states of the\\nworld. Directly extending a successful static 3D represen-\\ntation (for example, [33]) to each deformed frame leads to\\nlow efficiency [36], and the inability to model the infor-\\nmation flow across frames, which is critical when solving\\nill-posed problems as in [44]. Our desired dynamic rep-\\nresentation needs to simultaneously represent a global sur-\\nface (canonical/reference shape) across all frames and the\\nconsistent deformation (correspondence/flow/motion) be-\\ntween any frame pair (Fig. 1), so that we can recover the dy-\\nnamic geometry by reconstructing only one reference sur-\\nface and generating the rest of the deformed surfaces by us-\\ning the consistent deformation representation as well as as-\\nsociate and aggregate information across frames (Fig. 2A).\\n\\nThe majority of dynamic representations that satisfy the\\nabove desired properties are model-based and rely on para-\\nmetric models for specific categories like human bodies [1,\\n31] (Fig. 2B), faces [4, 30], or hands [47]. On the contrary,\\nrecent model-free methods like the implicit flow [36, 50]\\n(Fig. 2C) apply one universal 4D representation but model\\nthe canonical shape in an ad hoc chosen frame [36, 50] that\\ncomplicates the shape prior. Alternatively, the choice of an\\napproximate mean/neutral shape [58] as the canonical shape\\ncan limit the shape expressibility. Modeling of the defor-\\nmation is done by either MLPs [50, 58] that ignore the real\\nworld deformation properties, or by ODEs [36] that are in-\\nefficient for space deformation, or by an optimized embed-\\nded graph [6] or Atlas [2] that are sequence specific.\\n\\nIn this work, we introduce a novel and general archi-\\n\\n1\\n\\n\\x0ctecture and representation that enable a competitive re-\\nconstruction of every frame and the recovery of consis-\\ntent correspondence across frames. Our approach is rooted\\nin the factorization of deformation (Sec. 3.1).\\nIf we as-\\nsume that the topology does not change during deformation,\\nall deformed surfaces of one instance can be regarded as\\nequivalent through continuous bijective mappings (home-\\nomorphisms). This allows us to factorize the deforma-\\ntion between two frames by the composition of two con-\\ntinuous invertible functions such that one maps the source\\nframe into a common 3D Canonical Deformation Coordi-\\nnate Space (CaDeX) while another maps it back to the des-\\ntination frame. Such a factorization and its implementa-\\ntion (Sec. 3.2) is novel, simple, and efficient (compared to\\nODEs [36]) while it guarantees cycle consistency, topol-\\nogy preservation, and, if necessary, volume conservation\\n(Sec.3.3). The canonical shape embedded in the CaDeX\\ncan be regarded as the representative element, while the\\nassociated invertible mappings that transform between de-\\nformed frames and the CaDeX are the canonical maps.\\nTherefore, we model the reference surface directly in the\\nCaDeX via an implicit field [33] (Sec. 3.4), which can be\\noptimized together with the canonical maps during training.\\nIn summary, our main contributions are: (1) A novel\\ngeneral representation and architecture for dynamic sur-\\nfaces that jointly solve the canonical shape and consistent\\ndeformation problems. (2) Learnable continuous bijective\\ncanonical maps and canonical shapes that jointly factorize\\nthe shape deformation, and are novel, simple, efficient, and\\nguarantee cycle consistency and topology preservation. (3)\\nA novel solution to the dynamic surface reconstruction and\\ncorrespondence tasks given sparse point clouds or depth\\nviews based on the proposed representation. (4) We demon-\\nstrate state-of-the-art performance on modelling different\\ndeformable categories: Human bodies [5], Animals [57]\\nand Articulated Objects [53].\\n\\n2. Related Work\\n\\nProposed neural representations for static 3D geome-\\ntry [9, 11, 17, 18, 20, 26, 33, 34, 37, 40, 43, 56] are promising,\\nbut most of them do not involve modeling of deformations.\\nA few recent approaches represent or process 3D shapes via\\ndeformation [13, 22, 23, 25, 59], but they focus on static 3D\\nshape collections that do not meet the requirements (e.g, ef-\\nficiency) for processing 4D data. We will focus our related\\nwork on dynamic representations of deformable geometry.\\nModel-Based Dynamic Representation: Many success-\\nful 3D parametric models for specific shape categories have\\nbeen introduced, for example, the morphable model [4] and\\nFLAME [30] for faces, SCAPE [1] and SMPL [31] for hu-\\nman bodies, and MANO [47] for hands . These model-\\nbased representations (Fig. 2B) suffer from limited expres-\\nsivity, which can be mitigated by neural networks. Net-\\n\\nFigure 2. (A) Problem definition: A list, or a set of deformed sur-\\nfaces S = {Si} of one instance should be represented by 1.) one\\ncanonical 3D surface U (in the green box) and 2.) the consistent\\ndeformation between surfaces (yellow arrows); (B) Model-based\\nmethods: Si is obtained through the skinning function (green ar-\\nrows) from the template mesh [31]; (C) Implicit-flow methods:\\nthe first frame serves as the reference shape and the deformation is\\nmodeled by Neural-ODEs [36] or MLPs [50].\\n\\nworks can express detailed template shapes based on tem-\\nplate meshes [32, 38, 48, 52] or skeletons [12, 27, 51], and\\ncan learn more detailed skinning functions [8,48,52] or for-\\nward deformations [38]. However, they rely on the strong\\nassumption of canonicalization through the pose, skeleton,\\nand template mesh, which makes them limited to specific\\ncategories, and insufficient for modeling the rich dynamic\\n3D world. Our method does not rely on any hard-wired\\ntemplate mesh or skeleton, and the same architecture is uni-\\nversal for all shape categories.\\nModel-Free Dynamic Representation: Recent works [6,\\n24, 36, 50] extend the success of static 3D representa-\\ntions [9, 33, 40] to 4D by modeling the deformation be-\\ntween frames. Fig. 2C illustrates how the two closest works\\nto ours, O-Flow [36] and LPDC [50], are related to our\\nproblem formulation. First, our method differs from O-\\nFlow [36] and LPDC [50] in the representation of the space\\ndeformation. We represent the deformation through a novel\\ncanonical map factorization that is efficient and guarantees\\nreal world properties based on conditional neural home-\\nomorphisms [14, 15], while O-Flow [36] uses a Neural-\\nODE [7] that also guarantees the production of a well-\\nbehaved deformation (see [22] for details) but with higher\\ncomputational complexity than ours. LPDC [50] replaces\\nthe Neural-ODE [7] by a Multilayer Perceptron (MLP) to\\nlearn correspondences in parallel. However, the MLP de-\\nformation [20, 41, 44, 50] has difficulty to model a home-\\nomorphism or express real world deformation properties.\\nNote that both O-Flow [36] and LPDC [50] compute the\\nreference surface in the first frame, which turns out to be a\\nrandom choice, since the shape can be in an arbitrary defor-\\nmation state in the first frame. Our reference shape is mod-\\neled in the learned canonical space induced by the canonical\\nmap, which is more stable and can be optimized (Fig. 1).\\n\\nI3DMM [58] learns a near neutral/mean canonical tem-\\nplate from human head scans, which limits its expressive\\nability. CASPR [46] and Garment Nets [10] learn a canon-\\nicalization of deformable objects, but rely on the ground\\n\\n2\\n\\n\\x0ctruth canonical coordinate supervision, which is often inac-\\ncessible. Other neural dynamic representations include the\\nlearned embedded graph [6] first proposed in [49] and para-\\nmetric atlases [2]. Beyond 4D data, A-SDF [35] models the\\ngeneral articulated objects with a specially designed disen-\\ntanglement network, but it cannot model correspondence.\\nInstead, our method achieves stronger disentanglement by\\nexplicitly modeling the deformation.\\nInvertible Networks for 3D representation: Many\\nworks [3, 7, 14, 15, 19, 28, 39] have been proposed to con-\\nstruct invertible networks for generative models. In 3D deep\\nlearning, Neural-ODE [7] is widely used as a good model\\nof deformation [22, 23, 25, 36] or transformation of point\\ncloud [56]. ShapeFlow [25] learns a “Hub-and-spoke” sur-\\nface deformation for 3D shape collection via ODEs, but is\\ninefficient when applied to the 4D data since every frame\\nneeds to be lifted to the “hub” through integration. Besides\\nODEs, I-ResNet [3] is used in [21] to build invertible defor-\\nmation for shape editing. Our method is inspired by Neural-\\nParts [42] where Real-NVP [15] is used to model the defor-\\nmation from a sphere primitive to a local part. While we\\nalso use Real-NVP [15] for its simplicity and efficiency, we\\nhave two distinct differences compared to [42]: Our canon-\\nical shape is a learned implicit surface instead of a fixed\\nsphere that can only capture local parts; we use the inverse\\nof the Real-NVP to close the factorization cycle, while [42]\\nuses the inverse in a complementary training path.\\n\\n3. Method\\n\\nAs in Fig. 2A, given a sequence1 of point cloud observa-\\ntions of one deforming instance, our goal is to reconstruct\\na sequence of surfaces. Instead of directly solving a corre-\\nspondence map between two frames during reconstruction,\\nwe propose an architecture (Fig. 3) where the interframe\\ncorrespondence is computed via a pivot canonical shape.\\nWe will call the map between a surface in any deformed\\nframe and the canonical shape a canonical map.\\n\\n3.1. CaDeX and Canonical Map\\n\\nLet us denote [xi, yi, zi] ∈ R3 as the 3D coordinates\\nof the input 3D space2, in which a deformed surface Si at\\ntime ti is embedded. Consider a continuous bijective map-\\nping (homeomorphism) Hi : R3 (cid:55)→ R3 at time ti that maps\\neach deformed coordinate to its global (shared over differ-\\nent time frames) 3D coordinate [u, v, w] = Hi([xi, yi, zi]).\\nNote that [u, v, w] has no index of time and can be seen\\nas a globally consistent indicator of each correspondence\\ntrajectory across time in the input 3D space. Hence, we\\nname [u, v, w] the canonical deformation coordinates of\\nthe position [xi, yi, zi] at time ti and call the uvw 3D space\\n\\n1Or a set, but for conciseness, we will only refer to the sequence.\\n2The superscript i refers to the time index.\\n\\nthe Canonical Deformation Coordinate Space (CaDeX)\\nof the sequence S = {Si}. The homeomorphisms Ht that\\ntransform [xt, yt, zt] to [u, v, w] are called canonical maps.\\nSince CaDeX is globally shared across time, we model the\\ncanonical shape (surface) U directly in CaDeX instead of\\nselecting an input frame as is the case in [36, 50]. Taking\\nadvantage of neural fields [54], we model U as a level set of\\nan occupancy field [33]:\\n\\nU = { [u, v, w] | OccField([u, v, w]) = l } ,\\n\\n(1)\\n\\nwhere l is the surface level. Using the inverse of each canon-\\nical map Hi at time ti, we can directly obtain each deformed\\nsurface at time ti in the input 3D space as:\\n\\nSi = (cid:8) H−1\\n\\ni\\n\\n([u, v, w]) | ∀[u, v, w] ∈ U (cid:9) .\\n\\n(2)\\n\\nThe correspondence/deformation Fij that associates any\\ncoordinate (for both surface and non-surface points) from\\nthe 3D space at time ti to the 3D space at time tj can be\\nfactorized by the canonical maps as:\\n\\n[xj, yj, zj] = Fij([xi, yi, zi]) = H−1\\n\\nj ◦ Hi([xi, yi, zi]).\\n(3)\\nNote that Ht must be invertible; otherwise, the above defor-\\nmation function cannot be defined. By now, any surface that\\nis topologically isomorphic to the deformable instance sat-\\nisfies the above definitions, leading to infinitely many valid\\ncanonical shapes and maps. In the following, we will opti-\\nmize the canonical shapes and maps predicted by the archi-\\ntecture in Fig. 3 subject to the priors from the dataset.\\n\\n3.2. Canonical Map Implementation\\n\\nNeural Homeomorphism One key technique of our im-\\nplementation is an efficient way to parameterize and learn\\nthe homeomorphism between coordinate spaces. Unfor-\\ntunately, the widely used Neural-ODEs [7] do not meet\\nour efficiency requirements since a full integration would\\nhave to be applied to every frame.\\nInspired by [42], we\\nutilize the Conditional Real-NVP [15] (Real-valued Non-\\nVolume Preserving) or the NICE [14] (Nonlinear Indepen-\\ndent Component Estimation) normalizing flow implemen-\\ntations to learn the homeomorphism. Taking the more gen-\\neral NVP [15] as an example (Fig. 3-C), with the network\\nbeing a stack of Coupling Blocks [15], we apply NVP to 3D\\ncoordinates. During initialization, each block is randomly\\nassigned an input split pattern; for example, a block always\\nsplits [x, y, z] to [x, y] and [z]. Given a condition latent code\\nc, each block takes in 3D coordinates [x, y, z] and outputs\\nthe transformed coordinates [x′, y′, z′] by changing one part\\nbased on the other part in the input coordinate split:\\n\\n[x′, y′, z′] = [x, y, z exp(sθ(x, y|c)) + tθ(x, y|c)]\\n\\n(4)\\n\\nwhere sθ(·|c) : R2 (cid:55)→ R and tθ(·|c) : R2 (cid:55)→ R are scale\\nand translation predicted by any network conditioned on c.\\n\\n3\\n\\n\\x0cFigure 3. (A) Canonical Map (Sec. 3.2): A sequence (or a set) of input point clouds is first sent to the Deformation Encoder generating\\ndeformation embeddings ci for each frame. Then the canonical map H can transform any coordinate (e.g. a yellow point in the point cloud,\\na blue query position for implicit field or a purple source point for correspondence) from any deformed frame to the canonical coordinate\\nvia H conditioning on the corresponding deformation embedding. The correspondence prediction (right bottom) can be obtained by\\ndirectly mapping back the canonical coordinate through H −1. (B) Canonical Shape Encoder-Decoder (Sec. 3.4): All input multi-frame\\npoint clouds are first transformed to the canonical space via H and are directly unioned to aggregate a canonical observation. The global\\ngeometry embedding g (unique across frames) is encoded via a PointNet [45] ϕ, and the occupancy value for the canonical coordinate of a\\nquery position at ti (blue point) is predicted through a standard OccNet [33] ψ. During training, the occupancy is supervised by LR, and\\nthe correspondence can be optionally supervised by LC (Sec. 3.5). (C) The Real-NVP [15] invertible architecture of H (Sec. 3.2).\\n\\nSuch a block models a bijection since the inverse can be\\nimmediately derived as:\\n\\n[x, y, z] =\\n\\n(cid:20)\\nx′, y′,\\n\\nz′ − tθ(x′, y′|c)\\nexp(sθ(x′, y′|c))\\n\\n(cid:21)\\n\\n.\\n\\n(5)\\n\\nTherefore, the whole stack of blocks is invertible.\\nIf the\\nactivation functions in each block are continuous, then the\\nwhole network models a homeomorphism. NICE [14] is\\nsimply removing the scale freedom from the NVP block, i.e:\\nsθ(·|c) ≡ 0. Note that the inverse of NVP and NICE is as\\nsimple as the forward, which induces our desired efficiency\\nand simplicity, and enables using the definition in Eq.3.\\nH architecture Note that in Eq. 2, each deformed sur-\\nface Si has a different canonical map Hi that associates Si\\nwith U . We implement them with the conditional real-NVP\\nor NICE, denoted by H (noncalligraphic). Given the vec-\\ntor ci that encodes the deformation information at time ti\\nsuch that Hi(·) ≡ H(· ; ci), where the network H is shared\\nacross different time frames. The canonical deformation co-\\nordinates are predicted as (Fig. 3-A, boxes marked with H):\\n\\n[u, v, w] = H([xi, yi, zi] ; ci).\\n\\n(6)\\n\\nNote that on the right side of Eq. 6, the input coordinates\\nand the deformation embedding have the index ti since\\nthey come from each deformed frame. However, after ap-\\nplication of the canonical map, the coordinates on the left\\nside are independent of the index because there is only one\\nglobal CaDeX for this sequence. Finally, the correspon-\\ndence/deformation between two deformed frames (Eq. 3)\\ncan be implemented as:\\n\\n[ˆxj, ˆyj, ˆzj] = H −1 (cid:0)H([xi, yi, zi] ; ci) ; cj\\n\\n(cid:1)\\n\\n(7)\\n\\nwhere [ˆxj, ˆyj, ˆzj] is the mapped position at time tj of the\\noriginal position [xi, yi, zi] at time ti. Regarding the choice\\nof H, Real-NVP [15] can provide more flexible defor-\\nmation since it has one more degree of freedom (scale);\\nNICE [14] guarantees volume conservation (Sec. 3.3) that\\nresults in a more regularized deformation.\\nDeformation Encoder To obtain the per-frame deforma-\\ntion embedding ci that is used as the condition of H, we\\ndemonstrate two kinds of inputs and three encoder types\\n(Fig. 3-A, orange box). One direct approach is to employ a\\nPointNet that summarizes the deformation code separately\\nper frame (PF). If the input is a sequence of point clouds,\\n\\n4\\n\\n\\x0cwe can alternatively use the ST-PointNet variant proposed\\nin [50] to get the deformation code (ST). The ST encoder\\nprocesses the 4D coordinates and applies the pooling spa-\\ntially and temporally. If the input is a set without order, we\\ndevelop a 2-phase PointNet to obtain a global set deforma-\\ntion code (SET), and then use a 1-D code query network\\nto output the deformation embedding ci taking the query\\narticulation angle and the global deformation code as in-\\nput. Since these are not our main contributions, we refer the\\nreader to the supplementary for details on these encoders.\\n\\n3.3. Properties of the Canonical Map\\n\\nThe novel factorization and its implementation induce\\nthe following desired properties of real world deformation:\\nCycle consistency: The deformation/correspondence be-\\ntween deformed frames predicted by our factorization\\n(Eq. 3, 7) is cycle consistent (path-invariant). The reason\\nis that every canonical map maps any deformed frame in\\nthe sequence (or set) to the global CaDeX of this sequence\\n(or set), and the canonical maps are invertible:\\n\\nFjk ◦ Fij = H−1\\n\\nk ◦ Hj ◦ H−1\\n\\nj ◦ Hi = H−1\\n\\nk ◦ Hi = Fik.\\n\\n(8)\\nTopology preserving deformation: Since our factorization\\n(Eq. 3, 7) is a composition of two homeomorphisms, the\\ninduced deformation function is thus a homeomorphism as\\nwell, and therefore never changes the surface topology.\\nVolume conservation (NICE): If H is implemented by\\nNICE [14], then the predicted deformation preserves the\\nvolume of the geometry, which can be proved by the fact\\nthat the determinant of the Jacobian of every coupling block\\nin NICE [14] is 1 (see Supp. for more details).\\nContinuous deformation if c is continuous: Some appli-\\ncations require the sequence S = {S} to be dense on time\\naxis, for example, modeling continuous deformation across\\ntime [36]. In this case, the deformation codes c become a\\nfunction of time c(t). Since all activation functions we are\\nusing in the canonical map are continuous, it is obvious that\\nif c(t) is continuous, then the predicted deformation in Eq. 7\\nmust be continuous across t.\\n\\n3.4. Representing Canonical Shape\\n\\nGeometry Encoder We represent the canonical shape in\\nthe CaDeX by a standard Occupancy Network [33]. The\\ncanonical map brings additional benefits for encoding the\\nglobal geometry embedding (Fig. 3-B). Denote the ob-\\nj, yi\\nserved point cloud at time ti as Xi = { [xi\\nj] | j =\\n0, 1, . . . , Ni }3, where [xi\\nj, zi\\nj] is the 3D coordinate of\\neach point in the point cloud. The observations from dif-\\nferent ti’s are partial, noisy, and not aligned. We overcome\\nsuch irregularity by using the same canonical map (Sec.3.2)\\n\\nj, yi\\n\\nj, zi\\n\\n3Note here the superscripts are still the index of the time, the subscripts\\n\\nare the index of the points in the cloud.\\n\\nto obtain a canonical aggregated observation. Given the de-\\nformation embedding ci per-frame, the canonical observa-\\ntions are merged via set union as:\\n\\n¯X =\\n\\n(cid:91)\\n\\nti\\n\\n{ H([xi\\n\\nj, yi\\n\\nj, zi\\n\\nj] ; ci) | ∀[xi\\n\\nj, yi\\n\\nj, zi\\n\\nj] ∈ Xi }.\\n\\n(9)\\n\\nThe global geometry embedding g of the sequence S is en-\\ncoded by a PointNet ϕ: g = ϕ( ¯X).\\nGeometry Decoder Given the global geometry embed-\\nding g, we obtain the canonical shape encoded in g via an\\noccupancy network [33] that takes g as well as the query po-\\nsition [u, v, w] in the CaDeX as input, and predicts the oc-\\ncupancy in the CaDeX: ˆo = ψ([u, v, w]; g), where the de-\\ncoder ψ is an MLP. However, the ground truth ([u, v, w], o∗)\\nsupervision pair is unavailable in the CaDeX since the\\ncanonical shape is not known in advance and is learned dur-\\ning training. Available types of supervision are the query-\\noccupancy pairs ([xi, yi, zi], oi∗) in each deformed coordi-\\nnate space where the deformed surface Si is embedded at\\neach time ti. Therefore, we predict the occupancy field of\\nany deformed frame through the canonical map via Eq. 6:\\n\\nˆo = ψ (cid:0)H([xi, yi, zi] ; ci) ; g(cid:1)\\n\\n(10)\\n\\n3.5. Losses, Training, Inference\\n\\nOur model is fully differentiable and is trained end-to-\\nend. Following Eq. 10, the main loss function is the recon-\\nstruction loss in each deformed frame:\\n\\nLR =\\n\\n1\\nT\\n\\nT\\n(cid:88)\\n\\nMi(cid:88)\\n\\n1\\nMi\\n\\ni=1\\n\\nj=1\\n\\nBCE (cid:2)ψ (cid:0)H(pi\\n\\nj; ci); g(cid:1) , oi∗\\n\\nj\\n\\n(cid:3) (11)\\n\\nwhere T is the total number of frames that have occupancy\\nfield supervision and Mi is the number of queried positions\\nat each frame. We denote by pi\\nj the jth query position in\\nframe ti, and by oi∗\\nthe corresponding ground truth occu-\\nj\\npancy state. Optionally, if the ground truth correspondence\\npairs are given, we can utilize them as a supervision signal\\nvia Eq. 7. The additional correspondence loss reads:\\n\\nLC =\\n\\n1\\n|Q|\\n\\n(cid:88)\\n\\n(cid:13)\\n(cid:13)H −1 (cid:0)H(pi\\n(cid:13)\\n\\nk; ci); cj\\n\\n(cid:1) − pj\\n\\nk\\n\\n(cid:13)\\n(cid:13)\\n(cid:13)l\\n\\n(12)\\n\\n(pi\\n\\nk,pj\\n\\nk)∈Q\\n\\nwhere Q is the set of ground truth correspondence pairs: pi\\nk\\nis the source position (Fig. 3 purple coordinate) in frame ti\\nand pj\\nk is the ground truth corresponding position in frame\\ntj; and k is the index of all supervision pairs. We denote by l\\nthe order of the error norm. Note that the cycle consistency\\nguaranteed by our method (Sec. 3.3) does not depend on\\nLC. The overall loss function is L = wRLR + wCLC,\\nwhere wC can be zero if no correspondence supervision is\\nprovided. Note that there is no loss directly applied to the\\n\\n5\\n\\n\\x0cpredicted canonical deformation coordinates [u, v, w]. This\\ngives the maximum freedom to the canonical shape to form\\na pattern that helps the prediction accuracy. All patterns of\\nthe canonical shape emerge automatically during training\\n(see the Supplement for an additional discussion).\\n\\nDuring training, our model\\n\\nis trained directly from\\nscratch with the mandatory reconstruction loss (Eq. 11). For\\nefficiency, at each training iteration, we randomly select a\\nsubset of frames in the input sequence and supervise the\\noccupancy prediction. If the ground-truth correspondence\\nsupervision is also provided, we predict the corresponding\\nposition of surface points in the first frame for every other\\nframe and minimize the correspondence loss in Eq. 12.\\n\\nDuring inference, our model generates all surfaces of a\\nsequence in parallel after a single marching cubes mesh ex-\\ntraction. Directly marching the CaDeX is intractable since\\nit is learned. However, by using Eq. 10 as a query function,\\nwe can extract the mesh (V0, E0) in the first frame, which\\nis equivalent to marching the CaDeX given the canoni-\\ncal map. The equivalent canonical mesh in the CaDeX is\\n(Vc, Ec) = (H(V0; c0), E0). Then any mesh in other frames\\ncan be extracted as:\\n\\n(Vi, Ei) = (H −1(Vc; ci), Ec).\\n\\n(13)\\n\\nNote that all meshes above share the same connectivity E0,\\nso the mesh correspondence is produced. Eq. 13 can be\\nimplemented in batch to achieve better efficiency.\\n\\n4. Results\\n\\nTo demonstrate CaDeX as a general and expressive rep-\\nresentation, we investigate the performance in modeling\\nthree distinct categories: human bodies (Sec. 4.1), animals\\n(Sec. 4.2) and articulated objects (Sec. 4.3). Finally, we ex-\\namine the effectiveness of our design choice in Sec. 4.4.\\nMetrics: To measure our performance for shape and cor-\\nrespondence modeling, we follow the paradigm of [36, 50]\\nand use the same metrics: evaluating the reconstruction ac-\\ncuracy using the IoU and Chamfer Distance, and the motion\\naccuracy by correspondence l2-distance error.\\nBaselines: We compare with the closest model-free dy-\\nnamic representations. The main baselines described in\\nSec. 2 are: O-Flow [36] and LPDC [50] for sequence in-\\nputs and A-SDF [35] for articulated object set inputs.\\n\\n4.1. Modeling Dynamic Human Bodies\\n\\nWe first demonstrate the power of modeling the dynamic\\nhuman body across time. We use the same experiment\\nsetup, dataset, and split as [24, 36, 50]. The data are gen-\\nerated from D-FAUST [5], a real 4D human scan dataset.\\nFollowing the setting of [36], the input is a randomly sam-\\npled sparse point cloud trajectory (300 points) of 17 frames\\nevenly sampled across time. The ground-truth occupancy\\n\\nMethod\\n\\nSeen Individual\\nCD↓\\n\\nIoU↑\\n\\nCorr↓\\n\\nUnseen Individual\\nCD↓\\nIoU↑\\n\\nCorr↓\\n\\nPSGN-4D [16]\\nONet-4D [33]\\nO-Flow [36]\\nLCR [24]\\nLCR-F [24]\\nOurs\\n\\n-\\n\\n0.108\\n77.9% 0.084\\n79.9% 0.073\\n81.8% 0.068\\n81.5% 0.068\\n85.5% 0.056\\n\\n3.234\\n-\\n0.122\\n-\\n-\\n0.100\\n\\n-\\n\\n0.127\\n66.6% 0.140\\n69.6% 0.095\\n68.2% 0.100\\n69.9% 0.094\\n75.4% 0.074\\n\\n3.041\\n-\\n0.149\\n-\\n-\\n0.126\\n\\nTable 1. Results on D-FAUST [5] human bodies, trained without\\ncorrespondence supervision.\\n\\nMethod\\n\\nSeen Individual\\nCD↓\\n\\nIoU↑\\n\\nCorr↓\\n\\nUnseen Individual\\nCD↓\\nIoU↑\\n\\nCorr↓\\n\\nPSGN-4D [16]\\nO-Flow [36]\\nLPDC [50]\\nOurs(NICE)\\nOurs(ST)\\nOurs(PF)\\n\\n-\\n\\n0.101\\n81.5% 0.065\\n84.9% 0.055\\n85.4% 0.051\\n86.7% 0.046\\n89.1% 0.039\\n\\n0.102\\n0.094\\n0.080\\n0.082\\n0.077\\n0.070\\n\\n-\\n\\n0.119\\n72.3% 0.084\\n76.2% 0.071\\n75.6% 0.070\\n78.1% 0.063\\n80.7% 0.055\\n\\n0.131\\n0.117\\n0.098\\n0.104\\n0.095\\n0.087\\n\\nTable 2. Results on D-FAUST [5] human bodies, trained with cor-\\nrespondence supervision.\\n\\nfield as well as the optional surface point correspondence\\nare provided. Our default model is configured by using\\nthe ST-encoder (Sec. 3.2) and an NVP homeomorphism.\\nThe following tables and sections assume such a configu-\\nration if not otherwise specified. We test the performance\\nof the PF-encoder and the NICE homeomorphism variants\\nas well. The experiments are divided into training without\\ncorrespondence (Tab. 1) and training with correspondence\\n(Tab. 2) tracks for fair comparison between methods. The\\ntesting set has two difficulty levels: unseen motion and un-\\nseen individuals [36].\\n\\nQuantitative comparisons in Tab. 1, 2 indicate that our\\nmethod outperforms state-of-the-art methods by a signifi-\\ncant margin. The qualitative comparison in Fig. 4 shows the\\nadvantage of our method in capturing fast moving parts and\\nshape details (marked with red). We attribute such improve-\\nments to two main reasons: First, our factorization of the\\ndeformation and its implementation provide a strong regu-\\nlarization that other approaches like O-flow [36] can only\\nachieve with an ODE integration. Additionally, supervising\\na per-frame implicit reconstruction in our model is equiva-\\nlent to the dense cross-frame reconstruction supervision in\\n[50]. Second, our shape prior is stored in the learned canon-\\nical space (marked green in Fig. 4), which is relatively sta-\\nble across different sequences as shown in the figure. When\\ntraining without correspondence (Tab. 1), our method can\\nlearn the correspondence implicitly and reach a similar re-\\nconstruction performance as [50] in Tab. 2, which is trained\\nwith dense parallel correspondence supervision. Compar-\\ning the different configurations of our method in Tab. 2, the\\nNICE [14] version has a performance drop since the defor-\\nmation is strongly regularized to conserve the volume, but\\n\\n6\\n\\n\\x0cFigure 4. Left: Human body modeling (Sec. 4.1); Right: Animal body modeling (Sec. 4.2). The left top figure marked in the green box is\\nour canonical shape, the first input is not displayed. The colors of the meshes encode the correspondence. More results are in the the Supp.\\n\\nInput\\n\\nMehtod\\n\\nSeen individual\\nCD↓\\n\\nIoU↑\\n\\nCorr↓\\n\\nUnseen individual\\nCD↓\\nIoU↑\\n\\nCorr↓\\n\\nPCL\\n\\nDep\\n\\nO-Flow [36]\\nLPDC [50]\\nOurs\\n\\nO-Flow [36]\\nLPDC [50]\\nOurs\\n\\n70.6% 0.104\\n72.4% 0.085\\n80.3% 0.061\\n\\n63.0% 0.131\\n58.4% 0.160\\n71.1% 0.094\\n\\n0.204\\n0.162\\n0.133\\n\\n0.250\\n0.249\\n0.186\\n\\n57.3% 0.175\\n59.4% 0.149\\n64.7% 0.127\\n\\n49.0% 0.228\\n45.8% 0.261\\n55.7% 0.175\\n\\n0.285\\n0.262\\n0.239\\n\\n0.374\\n0.388\\n0.301\\n\\nTable 3. Results on DeformingThings4D [57] animal bodies, PCL\\nand Dep correspond to the input types.\\n\\nthis is achieved by freezing half of the capacity (scale free-\\ndom). We note that the naive per-frame encoder (PF) works\\nbetter compared to the spatial-temporal encoder [50] (ST).\\nA potential reason is that the per-frame encoder provides a\\nhigher canonicalization level since when deciding the de-\\nformation code ci, no information from other frames can be\\nconsidered, so the PF encoder might avoid overfitting.\\n\\n4.2. Modeling Dynamic Animals\\n\\nWe experiment with a more challenging setting: mod-\\neling different categories of animals with one model. We\\ngenerate the same supervision types as Sec. 4.1 based on\\nthe DeformingThings4D-Animals [57] dataset (DT4D-A).\\nWe use 17 animal categories and generate 2 types of input\\nobservations: Sparse point cloud input as Sec. 4.1 as well\\nas the monocular depth video input from a randomly posed\\nstatic camera. We assume that the camera view point esti-\\nmation problem is solved so all partial observations live in\\none global world frame. All models are trained across all\\nanimal categories. We refer the reader to the supplementary\\nmaterial for more details. Such a setting is more challenging\\nbecause animals have both large shape and motion variance\\nacross categories. Additionally, the models are required to\\naggregate information across time and hallucinate the miss-\\ning parts in depth observation inputs. Quantitative results\\nof both the sparse point cloud input and the depth input in\\nTab. 3 as well as the qualitative results in Fig. 4 indicate that\\nour method outperforms state-of-the-art methods in these\\nchallenging settings. In addition to the reasons mentioned\\nin Sec. 4.1, the improvement when predicting from depth\\n\\nInput Method\\n\\nIoU↑\\n\\nCD↓\\n\\nCorr↓\\n\\nt(s)\\n\\nθ (deg)\\n\\nPCL\\n\\nDep\\n\\nA-SDF [35]\\nLPDC [50]\\nOurs\\n\\nA-SDF [35]\\nLPDC [50]\\nOurs\\n\\n55.2% 0.127\\n49.2% 0.171\\n58.9% 0.118\\n\\n53.9% 0.127\\n46.4% 0.195\\n56.4% 0.116\\n\\n-\\n0.230\\n0.160\\n\\n-\\n0.269\\n0.161\\n\\n3.44\\n0.53\\n1.12\\n\\n3.65\\n0.54\\n1.26\\n\\n3.38\\n3.00\\n2.75\\n\\n5.06\\n4.85\\n4.34\\n\\nTable 4. Results on Shape2Motion [35, 53] articulated objects,\\nPCL and Dep correspond to the input types. The average perfor-\\nmance across 7 categories is reported, we refer the readers to our\\nsupplementary for the full table. t is the surface generation average\\ntime and θ is the average angle prediction error.\\n\\nobservation can be attributed to our design of the canoni-\\ncal observation encoder (Sec. 3.4) that explicitly aggregates\\nobservations in the CaDeX.\\n\\n4.3. Modeling Articulated Objects\\n\\nWe extend CaDeX from modeling the 4D nonrigid sur-\\nface sequence to representing semi-nonrigid articulated ob-\\nject sets. We generate the dataset and inputs as in Sec. 4.2\\nfrom [35] based on Shape2Motion [53], which contains 7\\ncategories of articulated objects with 1 or 2 deformable\\nangles. We configure the model with the SET encoder\\n(Sec. 3.2) that produces the global dynamic code and then\\nuse the articulation angle to query the deformation code for\\neach frame (for details, see the Supplement). During train-\\ning, we input the sparse point cloud of 4 randomly sampled\\ndeformed frames of one object and then use the ground-\\ntruth angle to query per-frame deformation codes; finally,\\nthe model predicts the occupancy field for 4 seen (input)\\nframes and 4 unseen frames. We supervise both LR and\\nLC. For completeness, we also predict the articulation an-\\ngles of the input frames by a small head in the encoder\\nand supervise them. Each category is trained separately for\\nall methods. Note that A-SDF [35] demonstrates the auto-\\ndecoder setup, but it only solves half of our problem without\\ncorrespondence. Simultaneously solving the shape and the\\ncorrespondence leads to difficulties when applying an auto-\\ndecoder with optimization during testing, so we leave this\\nas a future direction. For fair comparison, we adapt A-SDF\\n\\n7\\n\\n\\x0cIoU↑\\n\\nCD↓\\n\\nCorr↓\\n\\nFull\\nMLP\\nNo G-Enc\\n\\n66.5% 0.128\\n61.9% 0.161\\n63.4% 0.141\\n\\n0.223\\n0.303\\n0.216\\n\\nt(s)\\n\\n1.8\\n20.5\\n1.7\\n\\nTable 5. Ablation study, t is the average surface generation time.\\n\\nto extract meshes in this version. Second, we remove the\\ngeometry encoder in the canonical space and obtain the\\nglobal geometry embedding via a latent fusion using the\\nST-encoder [50]. We demonstrate the performance of the\\ndeer subcategory from DT4D-A [57] with point cloud in-\\nputs (Sec. 4.2). Tab. 5 shows the performance difference,\\nwhere we observe a significant performance decrease as\\nwell as longer inference times when using MLP instead of\\nhomeomorphisms. Additionally, we observe the drop in\\nreconstruction accuracy when removing the geometry en-\\ncoder in the canonical space (Sec. 3.4). We present more\\ndetails in the supplementary material.\\n\\n5. Limitations\\n\\nOur method guarantees several desirable properties and\\nachieves state-of-the-art performance on a wide range of\\nshapes, but still has limitations that need future exploration.\\nAlthough we can produce continuous deformation across\\ntime if c(t) is continuous, the continuity of c is not guaran-\\nteed in the ST-encoder [50] that we use. Therefore, when\\nthe input undergoes a large discontinuity, we do observe a\\ntrembling in the output of both LPDC [50] and our method.\\nAnother issue is that although our method preserves the\\ntopology, sometimes the real world deformation also results\\nin topology changes. Future work can explore how to selec-\\ntively preserve or alter the topology. Finally, it is currently\\nnontrivial to adapt our method in an auto-decoder frame-\\nwork [35, 40] since it requires simultaneously optimizing\\nthe canonical map (deformation) and the canonical shape\\nduring testing, which future work can explore.\\n\\n6. Conclusion\\n\\nWe introduced a novel and general representation for dy-\\nnamic surface reconstruction and correspondence. Our key\\ninsight is the factorization of the deformation by continuous\\nbijective canonical maps through a learned canonical shape.\\nWe prove that our representation guarantees cycle consis-\\ntency and topology preservation, as well as (if desired) vol-\\nume conservation. Extensive experiments on reconstructing\\nhumans, animals, and articulated objects demonstrate the\\neffectiveness and versatility of our approach. We believe\\nthat CaDeX enables more possibilities for future research\\non modeling and learning from our dynamic real world.\\nAcknowledgement: The authors appreciate the support of the fol-\\nlowing grants: ARL MURI W911NF-20-1-0080, NSF TRIPODS\\n1934960, NSF CPS 2038873, ARL DCIST CRA W911NF-17-2-\\n0181, and ONR N00014-17-1-2093.\\n\\nFigure 5. Articulated objects modeling (Sec. 4.3) with 7 distinct\\ncategories. The left-top figure marked in the green box is our\\ncanonical shape, the four small figures next to it are the inputs.\\nThe first row is the reconstruction of an observed deformation an-\\ngle and the second is for an unobserved angle. Note that A-SDF\\nhas no correspondence so is not colored.\\n\\nwith a similar encoder as our model and adapt the decoder\\nto predict the occupancy. We also compare with LPDC [50]\\nwhich is also adapted to use a similar encoder as ours.\\n\\nTab. 4 summarizes the average performance across 7 ob-\\nject categories while Fig. 5 presents the qualitative com-\\nparison. Both of them show our state-of-the-art perfor-\\nmance on modeling general articulated objects. We produce\\nan accurate reconstruction, while providing the correspon-\\ndence prediction that A-SDF [35] can not predict. Thus, the\\nmarching cube is needed for each frame in [35] and results\\nin a longer inference time as shown in Tab. 4. Note that our\\nmethod preserves the topology when deforming the objects\\n(Fig. 5 oven) while [35] does not have such guarantees. This\\nis the main reason that our method has a performance drop\\non eyeglasses category since the dataset contains many un-\\nrealistic deformations where the legs of the eyeglasses get\\ncrossed. Additionally, our method models more details in\\nthe moving parts (e.g, the inner side of the refrigerator door\\nin Fig. 5) due to the learned canonical space, which provides\\na stable container for the shape prior.\\n\\n4.4. Ablation Study\\n\\nWe show the effectiveness of our design as the following:\\nFirst, we replace the invertible canonical map with a one-\\nway MLP that maps the deformed coordinates to the canon-\\nical space (such setting is similar to [13, 58, 59]). Since\\nthe mapping is one-way, we supervise the correspondence\\nby enforcing the consistency in the canonical space. Ev-\\nery frame needs a separate application of marching cubes\\n\\n8\\n\\n\\x0cReferences\\n\\n[1] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-\\nbastian Thrun, Jim Rodgers, and James Davis. Scape: shape\\ncompletion and animation of people. In ACM SIGGRAPH\\n2005 Papers, pages 408–416. 2005. 1, 2\\n\\n[2] Jan Bednarik, Vladimir G Kim, Siddhartha Chaudhuri,\\nShaifali Parashar, Mathieu Salzmann, Pascal Fua, and Noam\\nAigerman. Temporally-coherent surface reconstruction via\\nmetric-consistent atlases. arXiv preprint arXiv:2104.06950,\\n2021. 1, 3\\n\\n[3] Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Du-\\nvenaud, and J¨orn-Henrik Jacobsen. Invertible residual net-\\nworks. In International Conference on Machine Learning,\\npages 573–582. PMLR, 2019. 3\\n\\n[4] Volker Blanz and Thomas Vetter. A morphable model for\\nIn Proceedings of the 26th an-\\nthe synthesis of 3d faces.\\nnual conference on Computer graphics and interactive tech-\\nniques, pages 187–194, 1999. 1, 2\\n\\n[5] Federica Bogo, Javier Romero, Gerard Pons-Moll, and\\nMichael J. Black. Dynamic FAUST: Registering human bod-\\nies in motion. In IEEE Conf. on Computer Vision and Pattern\\nRecognition (CVPR), July 2017. 2, 6\\n\\n[6] Aljaz Bozic, Pablo Palafox, Michael Zollhofer, Justus Thies,\\nAngela Dai, and Matthias Nießner. Neural deformation\\ngraphs for globally-consistent non-rigid reconstruction.\\nIn\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition, pages 1450–1459, 2021. 1, 2,\\n3\\n\\n[7] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and\\nDavid Duvenaud. Neural ordinary differential equations.\\narXiv preprint arXiv:1806.07366, 2018. 2, 3\\n\\n[8] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,\\nand Andreas Geiger. Snarf: Differentiable forward skin-\\nning for animating non-rigid neural implicit shapes. arXiv\\npreprint arXiv:2104.03953, 2021. 2\\n\\n[9] Zhiqin Chen and Hao Zhang. Learning implicit fields for\\ngenerative shape modeling. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 5939–5948, 2019. 2\\n\\n[10] Cheng Chi and Shuran Song. Garmentnets: Category-level\\npose estimation for garments via canonical space shape com-\\npletion. arXiv preprint arXiv:2104.05177, 2021. 2\\n\\n[11] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll.\\nImplicit functions in feature space for 3d shape reconstruc-\\ntion and completion. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n6970–6981, 2020. 2\\n\\n[12] Boyang Deng, John P Lewis, Timothy Jeruzalski, Gerard\\nPons-Moll, Geoffrey Hinton, Mohammad Norouzi, and An-\\ndrea Tagliasacchi. Nasa neural articulated shape approxi-\\nIn Computer Vision–ECCV 2020: 16th European\\nmation.\\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\\nings, Part VII 16, pages 612–628. Springer, 2020. 2\\n\\n[13] Yu Deng, Jiaolong Yang, and Xin Tong. Deformed implicit\\nfield: Modeling 3d shapes with learned dense correspon-\\ndence. In Proceedings of the IEEE/CVF Conference on Com-\\n\\nputer Vision and Pattern Recognition, pages 10286–10296,\\n2021. 2, 8\\n\\n[14] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:\\narXiv\\n\\nNon-linear independent components estimation.\\npreprint arXiv:1410.8516, 2014. 2, 3, 4, 5, 6\\n\\n[15] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-\\narXiv preprint\\n\\ngio. Density estimation using real nvp.\\narXiv:1605.08803, 2016. 2, 3, 4\\n\\n[16] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set\\ngeneration network for 3d object reconstruction from a single\\nimage. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 605–613, 2017. 6\\n[17] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,\\nand Thomas Funkhouser. Local deep implicit functions for\\nIn Proceedings of the IEEE/CVF Conference\\n3d shape.\\non Computer Vision and Pattern Recognition, pages 4857–\\n4866, 2020. 2\\n\\n[18] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,\\nand Thomas A Funkhouser. Deep structured implicit func-\\ntions. 2019. 2\\n\\n[19] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo\\nLarochelle. Made: Masked autoencoder for distribution es-\\ntimation. In International Conference on Machine Learning,\\npages 881–889. PMLR, 2015. 3\\n\\n[20] Thibault Groueix, Matthew Fisher, Vladimir G Kim,\\nBryan C Russell, and Mathieu Aubry. A papier-mˆach´e ap-\\nproach to learning 3d surface generation. In Proceedings of\\nthe IEEE conference on computer vision and pattern recog-\\nnition, pages 216–224, 2018. 2\\n\\n[21] Bharath Hariharan Guandao Yang, Serge Belongie and\\nVladlen Koltun. Geometry processing with neural fields. Ad-\\nvances in Neural Information Processing Systems, 33, 2021.\\n3\\n\\n[22] Kunal Gupta. Neural Mesh Flow: 3D Manifold Mesh Gen-\\neration via Diffeomorphic Flows. University of California,\\nSan Diego, 2020. 2, 3\\n\\n[23] Jingwei Huang, Chiyu Max Jiang, Baiqiang Leng, Bin\\nWang, and Leonidas Guibas. Meshode: A robust and\\nscalable framework for mesh deformation. arXiv preprint\\narXiv:2005.11617, 2020. 2, 3\\n\\n[24] Boyan Jiang, Yinda Zhang, Xingkui Wei, Xiangyang Xue,\\nLearning compositional representation\\nand Yanwei Fu.\\nIn Proceedings of the\\nfor 4d captures with neural ode.\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 5340–5350, 2021. 2, 6\\n\\n[25] Chiyu Jiang, Jingwei Huang, Andrea Tagliasacchi, Leonidas\\nGuibas, et al. Shapeflow: Learnable deformations among 3d\\nshapes. arXiv preprint arXiv:2006.07982, 2020. 2, 3\\n[26] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei\\nHuang, Matthias Nießner, Thomas Funkhouser, et al. Local\\nimplicit grid representations for 3d scenes. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 6001–6010, 2020. 2\\n\\n[27] Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar\\nHilliges, and Siyu Tang. A skeleton-driven neural occu-\\npancy representation for articulated hands. arXiv preprint\\narXiv:2109.11399, 2021. 2\\n\\n9\\n\\n\\x0c[28] Diederik P Kingma and Prafulla Dhariwal. Glow: Gener-\\native flow with invertible 1x1 convolutions. arXiv preprint\\narXiv:1807.03039, 2018. 3\\n\\n[29] Zihang Lai, Sifei Liu, Alexei A Efros, and Xiaolong\\nWang. Video autoencoder: self-supervised disentanglement\\nIn Proceedings of the\\nof static 3d structure and motion.\\nIEEE/CVF International Conference on Computer Vision,\\npages 9730–9740, 2021. 1\\n\\n[30] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier\\nRomero. Learning a model of facial shape and expression\\nfrom 4d scans. ACM Trans. Graph., 36(6):194–1, 2017. 1, 2\\n[31] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\\nmulti-person linear model. ACM Trans. Graphics (Proc.\\nSIGGRAPH Asia), 34(6):248:1–248:16, Oct. 2015. 1, 2\\n[32] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,\\nGerard Pons-Moll, Siyu Tang, and Michael J Black. Learn-\\ning to dress 3d people in generative clothing. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, pages 6469–6478, 2020. 2\\n\\n[33] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\\nLearning 3d reconstruction in function space. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, pages 4460–4470, 2019. 1, 2, 3, 4, 5, 6\\n[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\nRepresenting scenes as neural radiance fields for view syn-\\nIn European conference on computer vision, pages\\nthesis.\\n405–421. Springer, 2020. 2\\n\\n[35] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille,\\nNuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning\\ndisentangled signed distance functions for articulated shape\\nrepresentation. arXiv preprint arXiv:2104.07645, 2021. 3,\\n6, 7, 8\\n\\n[36] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\\nAndreas Geiger. Occupancy flow: 4d reconstruction by\\nlearning particle dynamics. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 5379–\\n5389, 2019. 1, 2, 3, 5, 6, 7\\n\\n[37] Michael Oechsle, Songyou Peng, and Andreas Geiger.\\nimplicit surfaces and radi-\\nUnisurf: Unifying neural\\nance fields for multi-view reconstruction. arXiv preprint\\narXiv:2104.10078, 2021. 2\\n\\n[38] Pablo Palafox, Aljaˇz Boˇziˇc, Justus Thies, Matthias Nießner,\\nand Angela Dai. Npms: Neural parametric models for 3d\\ndeformable shapes. arXiv preprint arXiv:2104.00702, 2021.\\n2\\n\\n[39] George Papamakarios, Theo Pavlakou, and Iain Murray.\\nMasked autoregressive flow for density estimation. arXiv\\npreprint arXiv:1705.07057, 2017. 3\\n\\n[40] Jeong Joon Park, Peter Florence, Julian Straub, Richard\\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\\ntinuous signed distance functions for shape representation.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 165–174, 2019. 2, 8\\n\\n[41] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\\n\\nMartin-Brualla. Deformable neural radiance fields. arXiv\\npreprint arXiv:2011.12948, 2020. 2\\n\\n[42] Despoina Paschalidou, Angelos Katharopoulos, Andreas\\nGeiger, and Sanja Fidler. Neural parts: Learning expres-\\nsive 3d shape abstractions with invertible neural networks.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 3204–3215, 2021. 3\\n\\n[43] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\\nPollefeys, and Andreas Geiger. Convolutional occupancy\\nnetworks. In Computer Vision–ECCV 2020: 16th European\\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\\nings, Part III 16, pages 523–540. Springer, 2020. 2\\n\\n[44] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields\\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n10318–10327, 2021. 1, 2\\n\\n[45] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\\nPointnet: Deep learning on point sets for 3d classification\\nIn Proceedings of the IEEE conference\\nand segmentation.\\non computer vision and pattern recognition, pages 652–660,\\n2017. 4\\n\\n[46] Davis Rempe, Tolga Birdal, Yongheng Zhao, Zan Gojcic,\\nSrinath Sridhar, and Leonidas J Guibas. Caspr: Learning\\ncanonical spatiotemporal point cloud representations. arXiv\\npreprint arXiv:2008.02792, 2020. 2\\n\\n[47] Javier Romero, Dimitrios Tzionas, and Michael J. Black.\\nEmbodied hands: Modeling and capturing hands and bod-\\nies together. ACM Transactions on Graphics, (Proc. SIG-\\nGRAPH Asia), 36(6), Nov. 2017. 1, 2\\n\\n[48] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J\\nBlack. Scanimate: Weakly supervised learning of skinned\\nIn Proceedings of the IEEE/CVF\\nclothed avatar networks.\\nConference on Computer Vision and Pattern Recognition,\\npages 2886–2897, 2021. 2\\n\\n[49] Robert W Sumner, Johannes Schmid, and Mark Pauly. Em-\\nbedded deformation for shape manipulation. In ACM SIG-\\nGRAPH 2007 papers, pages 80–es. 2007. 3\\n\\n[50] Jiapeng Tang, Dan Xu, Kui Jia, and Lei Zhang. Learning par-\\nallel dense correspondence from spatio-temporal descriptors\\nfor efficient and robust 4d reconstruction. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 6022–6031, 2021. 1, 2, 3, 5, 6, 7, 8\\n[51] Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, and Ger-\\nard Pons-Moll. Neural-gif: Neural generalized implicit\\nfunctions for animating people in clothing. arXiv preprint\\narXiv:2108.08807, 2021. 2\\n\\n[52] Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas\\nGeiger, and Siyu Tang. Metaavatar: Learning animat-\\nable clothed human models from few depth images. arXiv\\npreprint arXiv:2106.11944, 2021. 2\\n\\n[53] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qin-\\nping Zhao, and Kai Xu. Shape2motion: Joint analysis of\\nmotion parts and attributes from 3d shapes. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, pages 8876–8884, 2019. 2, 7\\n\\n10\\n\\n\\x0c[54] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,\\nShiqin Yan, Numair Khan, Federico Tombari, James Tomp-\\nkin, Vincent Sitzmann, and Srinath Sridhar.\\nNeural\\narXiv preprint\\nfields in visual computing and beyond.\\narXiv:2111.11426, 2021. 3\\n\\n[55] Zhenjia Xu, Zhanpeng He, Jiajun Wu, and Shuran Song.\\nLearning 3d dynamic scene representations for robot manip-\\nulation. arXiv preprint arXiv:2011.01968, 2020. 1\\n\\n[56] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\\nBelongie, and Bharath Hariharan. Pointflow: 3d point cloud\\ngeneration with continuous normalizing flows. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision, pages 4541–4550, 2019. 2, 3\\n\\n[57] Takafumi Taketomi Yang Li, Hikari Takehara, Bo Zheng,\\nand Matthias Nießner. 4dcomplete: Non-rigid motion es-\\narXiv preprint\\ntimation beyond the observable surface.\\narXiv:2105.01905, 2021. 2, 7, 8\\n\\n[58] Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-\\nPeter Seidel, Mohamed Elgharib, Daniel Cremers, and\\ni3dmm: Deep implicit 3d morphable\\nChristian Theobalt.\\nIn Proceedings of the IEEE/CVF\\nmodel of human heads.\\nConference on Computer Vision and Pattern Recognition,\\npages 12803–12813, 2021. 1, 2, 8\\n\\n[59] Zerong Zheng, Tao Yu, Qionghai Dai, and Yebin Liu. Deep\\nimplicit templates for 3d shape representation. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 1429–1439, 2021. 2, 8\\n\\n11\\n\\n\\x0c',\n",
       " 'L3U-net: Low-Latency Lightweight U-net Based Image\\nSegmentation Model for Parallel CNN Processors\\n\\nOsman Erman Okman∗\\nMehmet Gorkem Ulkar∗\\nGulnur Selda Uyanik\\nerman.okman@analog.com\\ngorkem.ulkar@analog.com\\nselda.uyanik@analog.com\\nAnalog Devices Inc.\\nIstanbul, Turkey\\n\\n2\\n2\\n0\\n2\\n \\nr\\na\\n\\nM\\n \\n0\\n3\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n8\\n2\\n5\\n6\\n1\\n.\\n3\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nABSTRACT\\nIn this research, we propose a tiny image segmentation model,\\nL3U-net, that works on low-resource edge devices in real-time. We\\nintroduce a data folding technique that reduces inference latency by\\nleveraging the parallel convolutional layer processing capability of\\nthe CNN accelerators. We also deploy the proposed model to such\\na device, MAX78000, and the results show that L3U-net achieves\\nmore than 90% accuracy over two different segmentation datasets\\nwith 10 fps.\\n\\nKEYWORDS\\nneural networks, image segmentation, low latency, edge cnn pro-\\ncessor\\n\\nACM Reference Format:\\nOsman Erman Okman, Mehmet Gorkem Ulkar, and Gulnur Selda Uyanik.\\n2022. L3U-net: Low-Latency Lightweight U-net Based Image Segmenta-\\ntion Model for Parallel CNN Processors. In Proceedings of tinyML Research\\nSymposium (tinyML Research Symposium’22). ACM, New York, NY, USA,\\n6 pages.\\n\\n1 INTRODUCTION\\nSemantic segmentation is one of the fundamental computer vi-\\nsion tasks that assign each pixel of an image with a predefined\\nlabel. This task is effectively utilized for many applications like\\nvideo surveillance, autonomous vehicle guidance, robotics, and\\nbiomedical image analysis. For most of these applications, real-time\\nprocessing with high-resolution inputs and the low energy con-\\nsumption is critical as many of these applications are designed to be\\na part of battery-powered devices like drones or personal gadgets.\\nThis makes TinyML research essential to develop applications that\\ncan run on hardware using mWs of power, KBs of RAM, and less\\nthan KBs of flash [27].\\n\\nWith the advance of deep learning (DL), the performance of\\nsemantic segmentation models has been improved significantly\\n\\n∗Both authors contributed equally to this research.\\n\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\ntinyML Research Symposium’22, March 2022, San Jose, CA\\n© 2022 Copyright held by the owner/author(s).\\n\\nand now very accurate results are possible. There are various ap-\\nproaches in the literature for this task that differ in particular as-\\npects such as network architecture, cost function, training strate-\\ngies, training data, etc. [25]. The initial approaches adopt fully\\nconvolutional networks (FCN) [19, 21] where the global context\\ninformation cannot be efficiently used. To add more contextual\\ninformation to FCN approaches, Conditional Random Fields (CRFs)\\nand Markov Random Fields (MRFs) are integrated back to the DL\\napproaches. Liu et al. proposed a CNN model that enables MRFs in\\na single forward pass [20].\\n\\nOne of the most popular image segmentation approaches uses\\nconvolutional encoder-decoder model architectures. Noh et al. [26]\\nproposed to use transposed convolution (deconvolution) layers to\\nreconstruct the segmentation map from the VGG 16-layer network\\nand present promising results on PASCAL VOC 2012 dataset. U-net\\n[28] and V-Net [24] are popular auto encode-based approaches.\\nThese architectures are different from [26] as there are residual\\nconnections between specified encoding and decoding layers. Vari-\\nous modified U-net models have also been used for many different\\napplications like 3D image segmentation [9] or land use determina-\\ntion [8, 32, 35]. U-net is still a prevalent approach, and many recent\\nstudies propose modifications like adding dense skip connections\\n[14] or proposing attention-based U-net architectures [12, 16] for\\ndifferent applications. Weng et al. [34] proposed a neural architec-\\nture search (NAS) approach to search highly accurate U-net-based\\nmodels for medical image segmentation effectively.\\n\\nDeepLab [4] family models are another group of commonly pre-\\nferred semantic segmentation DL models that effectively utilize\\ndilated (a.k.a. atrous) convolutions. This operator controls the res-\\nolution at which the features are computed in the DL models. By\\nincreasing the dilation parameter, the field of the view of the utilized\\nkernels is increased to deal with larger context effectively without\\nincreasing the number of model parameters. Further improvements\\nare proposed to this model [5–7]. Finally, DeepLab v3+, which com-\\nbines cascaded and parallel dilated convolution modules and uses\\nan auto-encoder approach, achieved an 89% mean intersection over\\nunion (mIoU) score.\\n\\nA study [25] compares many other approaches like attention-\\nbased, generative or recurrent neural networks, as well as other\\npopular approaches like RefineNet [17]. Nevertheless, these meth-\\nods are not deployable to low-power, low-memory edge devices\\ndue to their high architectural and computational requirements.\\n\\n\\x0ctinyML Research Symposium’22, March 2022, San Jose, CA\\n\\nOkman and Ulkar, et al.\\n\\nMost of them cannot achieve real-time inference speeds. Besides,\\nthe existing studies on TinyML semantic segmentation are min-\\nimal. [23] proposes to use a new convolutional module, efficient\\nspatial pyramid (ESP), efficient computation, memory, and power. A\\nhuman-machine collaborative design strategy is used in [18] to cre-\\nate networks with customized module-level macro architecture and\\nmicro-architecture designs specifically for semantic segmentation\\ntasks. A modified dilated convolution approach is also proposed in\\n[10], where many layers are optimized for this task, and the back-\\nbone is changed with MobileNetv2 [29]. Despite these approaches\\nreducing the computational load and testing on edge, they are not\\napplied on TinyML hardware. Bahl et al. proposed a lightweight\\nencoder-decoder model with depthwise separable convolutions and\\ndecreasing memory usage by removing skip connections between\\nencoder and decoder [2]. The model is also realized on an FPGA,\\nand its results are given on very low-resolution satellite images.\\nAnother recent work demonstrates an attention condenser neural\\nnetwork, AttendSeg, and its results on CamVid dataset [33], which\\nis stated as deployable to a tiny edge device.\\n\\nThis study introduces a very lightweight U-net-based encoder-\\ndecoder model to solve the semantic segmentation problems on tiny\\nbattery-powered hardware. A data reforming (i.e., folding) approach\\nis also proposed to be used for hardware composed of parallel\\nCNN processor units. This strategy also enables the hardware to\\nhandle higher spatial resolution images. The complete design is\\nimplemented on a low power edge device, MAX78000 [1], which is\\nshown as the most energy-efficient CNN inference hardware in the\\nmarket [11, 22, 31]. To summarize, our contributions are:\\n\\n• A fully CNN very lightweight modified U-net model for\\n\\nsemantic segmentation problems,\\n\\n• A data folding approach that enables higher resolution through-\\n\\nput for parallel CNN processing hardware architectures,\\n• An implementation of the proposed approach on such a\\ndevice, MAX78000, and presentation of the results in terms\\nof accuracy, speed, and energy consumption.\\n\\nThe remainder of this paper is organized as follows: In Section\\n2, we present the proposed data folding technique. The proposed\\nnetwork architecture is detailed in Section 3. The model deployment\\nplatform is described in Section 4. We evaluate the proposed models\\nin Section 5.3 trained with the datasets presented in Section 5.1.\\nThe training approach and parameters are presented in Section 5.2.\\nFinally, Section 6 summarizes the results.\\n\\n2 DATA FOLDING TECHNIQUE\\nInference latency plays a critical role for neural network accelera-\\ntors at the edge since it directly affects those resource-constrained\\ndevices’ frame-per-second (FPS) performance metrics. The convo-\\nlution operation is parallelizable in the channel domain, and this\\nfeature has already been utilized in neural network accelerators\\n[1]. The initial layers of CNN networks generally have the highest\\nspatial resolution and spatial size, whereas they have the lowest\\nchannel counts compared to deeper layers of the network. These\\ninitial layers induce a significant proportion of the entire network’s\\nlatency since only a small number of cores can be used to process\\nthe small number of input channels that are big in spatial size.\\n\\nHowever, the spatial resolutions decrease for most network archi-\\ntectures, and the channel counts increase as the network proceeds\\nover the following layers. Therefore, these middle layers are more\\nsuitable for parallel processing. U-net is an example of such net-\\nworks, and the highest spatial resolution is in the initial layers. In\\nthe original U-net architecture, the input image consists of only\\none channel, and its size is 572x572 [28]. Suppose U-net is deployed\\nin a neural network accelerator. In that case, only one processor\\ncore out of many will be active and busy processing the big input,\\nwhich generates a significant amount of all network latency.\\n\\nTo optimize the latency of the initial layers and to distribute the\\nuneven processor loads to many processor cores more evenly, we\\npropose a novel technique, data folding. Applying this technique\\nand performing a regular convolution with stride=1 is mathemat-\\nically equivalent to strided convolutions; however, the latency is\\nreduced by distributing the processing load to the parallel cores.\\nThe proposed data folding technique works by creating downsam-\\npled versions of the input data by first shifting in height and width,\\nthen combining those downsampled versions of the original data\\nin the channel domain. Although the spatial resolution appears\\nto shrink, all of the original information is fed to the network by\\nincreasing the channel count. Consequently, the data becomes more\\nsuitable for parallel processing since more channels mean more\\nprocessing cores can be utilized for the same amount of data pro-\\ncessing. Likewise, the processing latency drops as each processor\\nneeds to handle smaller data due to lowered resolution after folding.\\nIn Fig. 1, the folding of input data of 3x8x8 size into 48x2x2 is illus-\\ntrated, where the shapes are given as channel size x height x width\\nformat (CHW). As shown, the channels of the folded data are the\\ndownsampled versions of the original channels. In this example, the\\nfolding factor, 𝛼, is assumed to be 4. The downsampled data from\\neach channel with sampling offsets are placed through the channels\\nof the new kernel, where the sampling offsets vary between 0 to\\n𝛼 − 1 in both height and width axes. In Fig 1, blue channels show\\nthe downsampled data with sampling offset is equal to 0 for both\\nheight and width. On the other hand, yellow channels illustrate\\nthe downsampled data with sampling offset is equal to 0 for the\\nheight but 1 for the width. Yellow channels follow the blue channels\\nstacked in the channel axis. Likewise, other downsampled versions\\nwith increasing offset values are stacked further. Figure 2 shows an\\nexample of a folded image when 𝛼 is 2.\\n\\nSince the data in the neighbor pixels are folded into channels, the\\nsame data portion can be processed with a smaller 2d convolutional\\nkernel after the folding. In Fig. 3 A, the data with a shape 𝑁𝑐ℎ ∗ 𝑁ℎ ∗\\n𝑁𝑤 is shown. The 2d convolutional kernel with a side length equal\\nto 𝛼 ∗ 𝑘 is illustrated with a yellow square prism in the same figure.\\n𝑁𝑤\\nAfter folding, the 3d data matrix takes the shape of 𝛼 2𝑁𝑐ℎ ∗\\n𝛼 .\\nThe data in the slice of the kernel highlighted in yellow in Fig. 3 A\\nis now placed in another square prism. The side length of this prism\\nis now 𝑘 instead of 𝛼 ∗𝑘 as seen in Fig. 3 B. Therefore, the same data\\ncan be accessed with a smaller convolutional kernel after folding.\\nData folding not only changes the kernel lengths, but also the stride\\nparameter. Since the spatial dimensions shrink after folding, any\\nstep in these shrunk dimensions corresponds to larger steps in\\nthe original data tensor. Numerically, a convolution stride of 𝑠 in\\nthe folded tensor corresponds to a stride of 𝛼 ∗ 𝑠 in the original\\n\\n𝑁ℎ\\n𝛼 ∗\\n\\n\\x0cL3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors\\n\\ntinyML Research Symposium’22, March 2022, San Jose, CA\\n\\nFigure 1: Visualization of the proposed data folding tech-\\nnique\\n\\nFigure 3: 2D convolution operation before and after folding\\n\\nFigure 2: Example for input image folding for 𝛼=2. The orig-\\ninal image is 3x352x352 and the folded image is 12x176x176.\\n\\nunfolded tensor as seen in Fig. 3 D and C. Thus, data folding is a data\\nreshaping operation that when a 2d convolution operation follows\\nit, it becomes equivalent to a convolution operation on the original\\ntensor with a larger kernel and a larger stride. As mentioned above,\\ndata folding allows distributing the convolution processing load\\nto a larger number of parallel processors. In Fig. 4, the number of\\nmultiplications done by each parallel processor for a 2d convolution\\noperation on 352x352 data is shown. The red bar shows the case\\nwhere there is no data folding. The yellow bar shows the case with\\nfolding and the folding factor, 𝛼, is equal to 2. Lastly, the turquoise\\nbar shows the case for 𝛼 = 4. The input data is assumed to have\\nthree channels, e.g., RGB channels of an image, and the processor\\nis assumed to have at least 48 parallel cores. It is noteworthy that as\\n𝛼 doubles, the processor load on each active processor is quartered\\nif enough idle processors exist. The folding factor must be chosen\\nbased on the number of parallel cores and the desired stride on the\\noriginal tensor. Higher folding factors results in greater steps in the\\nshift of the convolutional kernel in the original tensor.\\n\\n3 PROPOSED L3U-NET ARCHITECTURE\\nThe U-net model [28] is a CNN model that has a condensing path\\nto obtain the context and a symmetrical expanding path to provide\\naccurate localization. There are connections between these paths\\nat different resolutions in the U-net model to allow information\\nshared through the network. To achieve low latency and energy\\n\\nFigure 4: Distribution of convolution operations per proces-\\nsor with regard to the number of input data channels.\\n\\nwhile providing high performance in edge devices, we propose a\\nmodel that combines the data folding technique with a small version\\nof the U-net architecture.\\n\\nBecause of the data reshaping limitations of the chosen deploy-\\nment platform, we utilize data folding only at the beginning layer\\nof the proposed architecture. To reverse the folding action and\\ncreate the segmentation map with the same resolution of the in-\\nput image, data unfolding is applied at the end of the network.\\nThis operation is simply a tensor reshaping. In Fig. 5, the proposed\\nUL-net architecture is illustrated. In the figure, the input image\\nhas three channels, and its size is 352x352. With data folding, a\\n48x88x88 shaped input tensor is obtained. In this spatial resolution,\\nthe architecture sequentially contains three Conv2d layers with\\n1x1 kernel and one Conv2d layer with 3x3 kernel. In the spatial\\n\\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263641020304050607080901001101201301401501601701801902002102202302402502602702802903003103203303403503603703803904004104204304404504604704804905005105205305405505605705805906006106206306401533371050330370100200300400500600700800900100011001200130014001500160017001800190020002100220023002400250026002700280029003000310032003300340035003600370038003900400041004200430044004500460047004800490050005100520053005400550056005700580059006000610062006300640053710050033003700263438206034038020060034003800263438206034038020060034003800153337105033037010050033003700153337105033037010050033003700Channel1Channel2Channel3Downsampled Image1Original ImageFolded Image Shape: 48 x 2 x 2(Shape: 3 x 8 x 8) α=2ABCDNumber of processors31248No folding, kernel = 12x12, stride=4Number of multiplications per processor69,696278,7841,115,136, kernel=3x3, stride=1, kernel=6x6, stride=2\\x0ctinyML Research Symposium’22, March 2022, San Jose, CA\\n\\nOkman and Ulkar, et al.\\n\\nresolution contraction path, there are three Conv2d 3x3 layers with\\na 2x2 MaxPool preceding each of them. After the contraction path,\\nthree ConvTranspose2d operations take place in the the expand-\\ning path. Between the contraction and expanding paths at each\\nresolution, skip layers and channel-wise concatenations exist to\\nfacilitate information exchange and gradient propagation. After the\\nresolution is increased back to 88x88, two Conv2d layers with 3x3\\nkernel and four Conv2d layers with 1x1 kernel are utilized to create\\nthe segmentation map.\\n\\nAll of the Conv2d layers contain batch normalization (Batch-\\nNorm), and before quantization-aware training (QAT) starts, the\\nBatchNorm layers are fused into the weights and biases of the\\npreceding Conv2d layers. The total number of parameters of the\\nproposed architecture is 278,176 for four-class segmentation and\\n277,004 for two-class segmentation.\\n\\n4 DEPLOYMENT PLATFORM\\nThe proposed model is intended for deployment on CNN inference\\nengines that work at the edge. As an example deployment platform,\\nAnalog Devices MAX78000 is chosen in this study. MAX78000 is an\\nUltra-Low-Power Arm Cortex-M4 processor with FPU-based MCU\\nwith CNN accelerator that targets battery-powered neural network-\\nbased applications at the edge [1]. MAX78000 has a weight memory\\nof 442KB, and 1, 2, 4, or 8-bit weights are supported. As layers,\\nit supports both Conv1D and Conv2D as convolution operations,\\n2D transposed convolution layer, linear layer, pooling layers, and\\nelement-wise operations. MAX78000 has 64 parallel processors.\\nEach can only read from specific segments of the data memory\\nbut can write to the entire data memory. The CNN parallelization\\nis performed at the input channels. Hence each input channel or\\ngroup of input channels are processed by a different processor. The\\nproposed data folding method leverages this feature by assigning\\nthe same load to a higher number of channels which results in an\\nincrease of parallelism.\\n\\n5 EXPERIMENTS\\n5.1 Dataset\\nThis study conducts experiments on two different datasets, and\\nwe present two possible edge applications. The first dataset, the\\nCambridge-driving Labeled Video Database (CamVid), provides\\nground truth labels that associate each pixel with one of 32 se-\\nmantic classes [3]. Many researchers use this common dataset to\\nunderstand complex street views, enabling different applications,\\nespecially for driving safety and navigation. We used three semantic\\nclasses for our application: ‘Building’, ‘Sky’, and ‘Tree’, where the\\nremaining classes fall into the class ‘Other’. The CamVid dataset\\nincludes 601 960x720 color images, where 370 of them are used\\nfor training, and the rest are used for testing. Due to the memory\\nlimitations of the deployment platform, MAX78000, all images are\\ncropped into intersecting 352x352 images. For the training set, the\\nintersection ratio is kept around 40 percent to populate the training\\nset sufficiently. For the test set, it is set to one percent to avoid\\nbiasing the accuracy results due to the duplicated pixels in the test\\nset. Further, only the images with at least one of the selected three\\n\\nTable 1: Sample implementation results of proposed ap-\\nproach\\n\\nDataset\\n\\nPixel-to-Pixel\\nAccuracy (%)\\n\\nLatency\\n(ms)\\n\\nEnergy/Inf.\\n(mJ)\\n\\nCamVid\\n\\nAISegment\\n\\n91.05\\n\\n99.19\\n\\n95.1\\n\\n90.3\\n\\n7.3\\n\\n6.9\\n\\nmIoU\\n(%)\\n\\n84.24\\n\\n98.09\\n\\nsemantic classes are selected; the resulting set contains 4428 train-\\ning samples and 1392 test samples. Finally, all images are folded\\nwith 𝛼=4 as explained in Section 2 to form images of size 48x88x88.\\nThe second dataset, AISegment [13] is a human portrait segmen-\\ntation dataset. The dataset includes 34,427 human portrait color\\nimages with a resolution of 600x800. The respective masks of the\\nportrait images are provided in RGBA format. The alpha channel\\nis either zero or one and is used for labeling each pixel with the\\n‘Background’ or ‘Portrait’ label. The dataset training-test split ra-\\ntio is selected as 90% − 10%. The label distribution in both sets\\nis around 43% − 57% for the ‘Portrait’ and ‘Background’ classes,\\nrespectively. In the data augmentation phase, three overlapping\\nimages and corresponding matting images of size (600x600) are\\ncropped from the original image, sliding in the y-direction. Each of\\nthe cropped images is then re-scaled to size (352x352). As a result\\nof this cropping operation, the total number of images increases to\\n103,281. Finally, all images are folded with 𝛼=4 to form images of\\nsize 48x88x88.\\n\\n5.2 Training\\nTo be deployed on MAX78000, the model needs to be quantized to\\nat most to 8-bit integers. Since simple post-quantization methods\\ncause performance degradation [31], QAT is employed in this study.\\nThe QAT approach implements the fake (a.k.a. simulated) quantiza-\\ntion approach of [15]. When training L3U-net, an Adam optimizer is\\nused with an initial learning rate of 0.001. For the CamVid dataset,\\nthe training takes 100 epochs, and QAT starts at epoch 40. The\\nlearning rate is halved two times at the 20th and 80th epochs. For\\nthe AISegment dataset, the training takes 200 epochs, QAT starts\\nat epoch 150, and the learning rate is halved three times at the 50th,\\n100th and 140th epochs. In this study, all weights are quantized\\nto 8-bits, and batchnorm parameters are fused into convolutional\\nlayers before the QAT starts.\\n\\n5.3 Results\\nThe results of the proposed approach for the two selected datasets\\nare summarized in Table 1.\\n\\nFig. 6 shows the results of L3U-net for three sample images.\\nAs seen from these samples, the model accurately segments the\\nregions, except for very tiny structures. For the four-class CamVid\\ndataset, our approach provides better accuracy than many of the\\nedge approaches in the literature AttendSeg [33] or EdgeSegNet\\n[18]. As these works give their results for 32-class segmentation, it is\\nnot possible to compare the results in terms of accuracy, only model\\nsize and complexity of the operations. Our approach has 4x and 30x\\nfewer parameters; 10x and 100x fewer MAC operations [18, 33]. In\\naddition, with the proposed data folding approach, we were able to\\n\\n\\x0cL3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors\\n\\ntinyML Research Symposium’22, March 2022, San Jose, CA\\n\\nFigure 5: Proposed tiny U-net model architecture\\n\\nrun the model on a battery-powered edge device, MAX78000, with\\nan inference speed up to 10 fps.\\n\\nThe model output for the AISegment dataset is the binary class\\nlabel of each pixel, and Fig. 7 includes a model input image and\\nremoved background by L3U-net model. A reference model, [30],\\nprovides 98% mIoU for 224x224 image with a floating-point model\\nthat has 8x more parameters than ours. In that sense, L3U-net is a\\nmore effective approach requiring less memory and less computa-\\ntion for larger input data.\\n\\nFigure 6: Sample model outputs for CamVid dataset.\\n\\nFigure 7: Sample model output for portrait segmentation.\\n\\nLastly, we provide energy measurements of the L3U-net model\\non MAX78000. According to the results, with an AA alkaline bat-\\ntery, it is possible to calculate over 1.5 million inferences using the\\nproposed model and setup. To the best of our knowledge, no other\\nstudy gives these values for semantic segmentation at the edge; so\\nwe provide these results for future studies.\\n\\n56ConvTranspose2dConv2d: 3x3ReLUMaxPool: 2x2BatchNormInOut11x1122x2244x4488x88concat828285611256112562856288166448Conv2d: 1x1ReLUBatchNormfolding646432486464646443352x352Conv2d: 1x1ReLUBatchNormConv2d: 1x1ReLUBatchNormConv2d: 3x3ReLUBatchNormConv2d: 3x3ReLUMaxPool: 2x2BatchNormConv2d: 3x3ReLUMaxPool: 2x2BatchNormConv2d: 3x3ReLUBatchNormConvTranspose2dconcatConv2d: 3x3ReLUBatchNormConvTranspose2dconcatConv2d: 3x3ReLUBatchNormConv2d: 3x3ReLUBatchNormConv2d: 1x1ReLUBatchNormConv2d: 1x1ReLUBatchNormConv2d: 1x1BatchNormConv2d: 1x1BatchNormunfolding8\\x0ctinyML Research Symposium’22, March 2022, San Jose, CA\\n\\nOkman and Ulkar, et al.\\n\\n6 CONCLUSION\\nWe presented L3U-net , a lightweight U-net-based fully CNN model\\nwhich is also deployed to a tiny edge device, MAX78000. We also\\nproposed a data folding approach that reduces the latency and en-\\nables processing of higher resolution input images for a multi-core\\nCNN inference engine, MAX78000. The experiments performed\\nshow that L3U-net is capable of providing 91% and 99% accuracy\\nfor four- and two-class segmentation examples at a speed of 10 fps.\\n\\nACKNOWLEDGMENTS\\nThe authors would like to thank Robert Muchsel, Brian Rush, and\\nother members of the AI Development Group at Analog Devices\\nthat contributed to this work.\\n\\nREFERENCES\\n[1] Analog Devices 2021. Artificial Intelligence Microcontroller with Ultra-Low-Power\\nConvolutional Neural Network Accelerator. Analog Devices. https://datasheets.\\nmaximintegrated.com/en/ds/MAX78000.pdf Rev. 1.\\n\\n[2] Gaetan Bahl, Lionel Daniel, Matthieu Moretti, and Florent Lafarge. 2019. Low-\\nPower Neural Networks for Semantic Segmentation of Satellite Images. In Pro-\\nceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\\nWorkshops.\\n\\n[3] Gabriel J. Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. 2008.\\nSegmentation and Recognition Using Structure from Motion Point Clouds. In\\nECCV (1). 44–57.\\n\\n[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and\\nAlan L Yuille. 2014. Semantic image segmentation with deep convolutional nets\\nand fully connected crfs. arXiv preprint arXiv:1412.7062 (2014).\\n\\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and\\nAlan L. Yuille. 2018. DeepLab: Semantic Image Segmentation with Deep\\nConvolutional Nets, Atrous Convolution, and Fully Connected CRFs.\\nIEEE\\nTransactions on Pattern Analysis and Machine Intelligence 40, 4 (2018), 834–848.\\nhttps://doi.org/10.1109/TPAMI.2017.2699184\\n\\n[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. 2017.\\nRethinking atrous convolution for semantic image segmentation. arXiv preprint\\narXiv:1706.05587 (2017).\\n\\n[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig\\nAdam. 2018. Encoder-Decoder with Atrous Separable Convolution for Semantic\\nImage Segmentation. In Proceedings of the European Conference on Computer\\nVision (ECCV).\\n\\n[8] Zhengquan Chu, Tian Tian, Ruyi Feng, and Lizhe Wang. 2019.\\n\\nSea-Land\\nSegmentation With Res-UNet And Fully Connected CRF. In IGARSS 2019 -\\n2019 IEEE International Geoscience and Remote Sensing Symposium. 3840–3843.\\nhttps://doi.org/10.1109/IGARSS.2019.8900625\\n\\n[9] Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf\\nRonneberger. 2016. 3D U-Net: learning dense volumetric segmentation from\\nsparse annotation. In International conference on medical image computing and\\ncomputer-assisted intervention. Springer, 424–432.\\n\\n[10] Taha Emara, Hossam E. Abd El Munim, and Hazem M. Abbas. 2019. LiteSeg: A\\nNovel Lightweight ConvNet for Semantic Segmentation. In 2019 Digital Image\\nComputing: Techniques and Applications (DICTA). 1–7. https://doi.org/10.1109/\\nDICTA47822.2019.8945975\\n\\n[11] Marco Giordano and Michele Magno. 2021. A Battery-Free Long-Range Wireless\\nSmart Camera for Face Recognition. In Proceedings of the 19th ACM Conference on\\nEmbedded Networked Sensor Systems (Coimbra, Portugal) (SenSys ’21). Association\\nfor Computing Machinery, New York, NY, USA, 594–595. https://doi.org/10.\\n1145/3485730.3493367\\n\\n[12] Changlu Guo, Márton Szemenyei, Yugen Yi, Wenle Wang, Buer Chen, and\\nChangqi Fan. 2021. SA-UNet: Spatial Attention U-Net for Retinal Vessel Seg-\\nmentation. In 2020 25th International Conference on Pattern Recognition (ICPR).\\n1236–1242. https://doi.org/10.1109/ICPR48806.2021.9413346\\n\\n[13] Laurent H. 2018. AISegment.com - Matting Human Datasets. https://www.kaggle.\\ncom/laurentmih/aisegmentcom-matting-human-datasets. Accessed: 2021-12-10.\\n[14] Huimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu, Qiaowei Zhang, Yutaro\\nIwamoto, Xianhua Han, Yen-Wei Chen, and Jian Wu. 2020. UNet 3+: A Full-\\nScale Connected UNet for Medical Image Segmentation. In ICASSP 2020 - 2020\\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\\n1055–1059. https://doi.org/10.1109/ICASSP40776.2020.9053405\\n\\n[15] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew\\nHoward, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and\\nTraining of Neural Networks for Efficient Integer-Arithmetic-Only Inference. In\\n\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR).\\n\\n[16] Sheng Lian, Zhiming Luo, Zhun Zhong, Xiang Lin, Songzhi Su, and Shaozi\\nLi. 2018. Attention guided U-Net for accurate iris segmentation. Journal of\\nVisual Communication and Image Representation 56 (2018), 296–304.\\nhttps:\\n//doi.org/10.1016/j.jvcir.2018.10.001\\n\\n[17] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. 2017. RefineNet:\\nMulti-Path Refinement Networks for High-Resolution Semantic Segmentation.\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR).\\n\\n[18] Zhong Qiu Lin, Brendan Chwyl, and Alexander Wong. 2019. Edgesegnet: A\\ncompact network for semantic segmentation. arXiv preprint arXiv:1905.04222\\n(2019).\\n\\n[19] Wei Liu, Andrew Rabinovich, and Alexander C. Berg. 2015. ParseNet: Looking\\nWider to See Better. CoRR abs/1506.04579 (2015). arXiv:1506.04579 http://arxiv.\\norg/abs/1506.04579\\n\\n[20] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen-Change Loy, and Xiaoou Tang. 2015.\\nSemantic Image Segmentation via Deep Parsing Network. In Proceedings of the\\nIEEE International Conference on Computer Vision (ICCV).\\n\\n[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully Convolutional\\nNetworks for Semantic Segmentation. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR).\\n\\n[22] Michele Magno. 2021. A Battery-Free Long-Range Wireless Smart Camera for\\nFace Detection: An accurate benchmark of novel Edge AI platforms and milliwatt\\nmicrocontrollers. https://www.tinyml.org/event/emea-2021 TinyML Emea 2021\\nTalk.\\n\\n[23] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, and Hannaneh\\nHajishirzi. 2018. ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for\\nSemantic Segmentation. In Proceedings of the European Conference on Computer\\nVision (ECCV).\\n\\n[24] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. 2016. V-Net: Fully\\nConvolutional Neural Networks for Volumetric Medical Image Segmentation.\\nIn 2016 Fourth International Conference on 3D Vision (3DV). 565–571. https:\\n//doi.org/10.1109/3DV.2016.79\\n\\n[25] Shervin Minaee, Yuri Y. Boykov, Fatih Porikli, Antonio J Plaza, Nasser Kehtar-\\nnavaz, and Demetri Terzopoulos. 2021. Image Segmentation Using Deep Learning:\\nA Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021),\\n1–1. https://doi.org/10.1109/TPAMI.2021.3059968\\n\\n[26] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. 2015. Learning Deconvolu-\\ntion Network for Semantic Segmentation. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV).\\n\\n[27] Partha Pratim Ray. 2021. A Review on TinyML: State-of-the-art and Prospects.\\nJournal of King Saud University - Computer and Information Sciences (2021).\\nhttps://doi.org/10.1016/j.jksuci.2021.11.019\\n\\n[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional\\nNetworks for Biomedical Image Segmentation. In Medical Image Computing and\\nComputer-Assisted Intervention – MICCAI 2015, Nassir Navab, Joachim Horneg-\\nger, William M. Wells, and Alejandro F. Frangi (Eds.). Springer International\\nPublishing, Cham, 234–241.\\n\\n[29] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-\\nChieh Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR).\\n\\n[30] A. Sathyan. 2021. https://github.com/anilsathyan7/Portrait-Segmentation.\\n[31] Mehmet Gorkem Ulkar and Osman Erman Okman. 2021. Ultra-Low Power\\n\\nKeyword Spotting at the Edge. arXiv:2111.04988 [cs.SD]\\n\\n[32] Priit Ulmas and Innar Liiv. 2020. Segmentation of satellite imagery using u-net\\nmodels for land cover classification. arXiv preprint arXiv:2003.02899 (2020).\\n[33] Xiaoyu Wen, Mahmoud Famouri, Andrew Hryniowski, and Alexander Wong.\\n2021. AttendSeg: A Tiny Attention Condenser Neural Network for Semantic\\nSegmentation on the Edge. arXiv preprint arXiv:2104.14623 (2021).\\n\\n[34] Yu Weng, Tianbao Zhou, Yujie Li, and Xiaoyu Qiu. 2019. NAS-Unet: Neural\\nIEEE Access 7 (2019),\\n\\nArchitecture Search for Medical Image Segmentation.\\n44247–44257. https://doi.org/10.1109/ACCESS.2019.2908991\\n\\n[35] Pengbin Zhang, Yinghai Ke, Zhenxin Zhang, Mingli Wang, Peng Li, and\\nShuangyue Zhang. 2018. Urban Land Use and Land Cover Classification Us-\\ning Novel Deep Learning Models Based on High Spatial Resolution Satellite\\nImagery. Sensors 18, 11 (2018). https://doi.org/10.3390/s18113717\\n\\n\\x0c']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the text into one string\n",
    "\n",
    "array_pdf_text=[]\n",
    "def pdf_to_text():\n",
    "    for i in array_link:\n",
    "        array_pdf_text.append((load_pdf(i[21:])))\n",
    "    return array_pdf_text\n",
    "pdf_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87320f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dengpan.Fu', 'Dongdong.Chen', 'Hao.Yang', 'Jianmin.Bao', 'Lu.Yuan', 'Lei.Zhang', 'Houqiang.Li', 'Fang.Wen', 'Dong.Chen']\n",
      "Large-Scale Pre-training for Person Re-identification with Noisy Labels\n",
      "\n",
      "Dengpan Fu1 Dongdong Chen3 Hao Yang2\n",
      "Lei Zhang4 Houqiang Li1\n",
      "\n",
      "Lu Yuan3\n",
      "\n",
      "Jianmin Bao2*\n",
      "\n",
      "Fang Wen 2 Dong Chen2\n",
      "\n",
      "1University of Science and Technology of China\n",
      "\n",
      "fdpan@mail.ustc.edu.cn\n",
      "\n",
      "2Microsoft Research, 3Microsoft Cloud AI, 4IDEA\n",
      "lihq@ustc.edu.cn\n",
      "\n",
      "cddlyf@gmail.com\n",
      "\n",
      "{jianbao,haya,luyuan,fangwen,doch}@microsoft.com, leizhang@idea.edu.cn\n",
      "\n",
      "Abstract\n",
      "\n",
      "This paper aims to address the problem of pre-training\n",
      "for person re-identification (Re-ID) with noisy labels. To\n",
      "setup the pre-training task, we apply a simple online multi-\n",
      "object tracking system on raw videos of an existing un-\n",
      "labeled Re-ID dataset “LUPerson” and build the Noisy\n",
      "Labeled variant called “LUPerson-NL”. Since theses ID\n",
      "labels automatically derived from tracklets inevitably con-\n",
      "tain noises, we develop a large-scale Pre-training frame-\n",
      "work utilizing Noisy Labels (PNL), which consists of three\n",
      "learning modules: supervised Re-ID learning, prototype-\n",
      "based contrastive learning, and label-guided contrastive\n",
      "learning. In principle, joint learning of these three mod-\n",
      "ules not only clusters similar examples to one prototype,\n",
      "but also rectifies noisy labels based on the prototype as-\n",
      "signment. We demonstrate that learning directly from raw\n",
      "videos is a promising alternative for pre-training, which\n",
      "utilizes spatial and temporal correlations as weak super-\n",
      "vision. This simple pre-training task provides a scalable\n",
      "way to learn SOTA Re-ID representations from scratch on\n",
      "“LUPerson-NL” without bells and whistles. For example,\n",
      "by applying on the same supervised Re-ID method MGN,\n",
      "our pre-trained model improves the mAP over the unsu-\n",
      "pervised pre-training counterpart by 5.7%, 2.2%, 2.3% on\n",
      "CUHK03, DukeMTMC, and MSMT17 respectively. Under\n",
      "the small-scale or few-shot setting, the performance gain is\n",
      "even more significant, suggesting a better transferability of\n",
      "the learned representation. Code is available at https:\n",
      "//github.com/DengpanFu/LUPerson-NL.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "A large high-quality labeled dataset for person re-\n",
      "identification (Re-ID) is labor intensive and costly to cre-\n",
      "ate. Existing fully labeled datasets [25, 52, 58, 61] for per-\n",
      "son Re-ID are all of limited scale and diversity compared\n",
      "to other vision tasks. Therefore, model pre-training be-\n",
      "\n",
      "*Corresponding author.\n",
      "\n",
      "(a) Market1501 with MGN\n",
      "\n",
      "(b) Market1501 with IDE\n",
      "\n",
      "(c) DukeMTMC with MGN\n",
      "\n",
      "(d) DukeMTMC with IDE\n",
      "\n",
      "Figure 1. Comparing person Re-ID performances of three pre-\n",
      "trained models on two methods (IDE [59] and MGN [51]). Re-\n",
      "sults are reported on Market1501 and DukeMTC, with different\n",
      "scales under the small-scale setting. IN.sup. refers to the model\n",
      "supervised pre-trained on ImageNet, LUP.unsup. is the model un-\n",
      "supervised pre-trained on LUPserson, and LUPnl.pnl. is the model\n",
      "pre-trained on our LUPerson-NL dataset using our proposed PNL.\n",
      "\n",
      "comes a crucial approach to achieve good Re-ID perfor-\n",
      "mance. However, due to the lack of large-scale Re-ID\n",
      "dataset, most previous methods simply use the models pre-\n",
      "trained on the crowd-labeled ImageNet dataset, resulting in\n",
      "a limited improvement because of the big domain gap be-\n",
      "tween generic images in ImageNet and person-focused im-\n",
      "ages desired by the Re-ID task. To mitigate this problem,\n",
      "the recent work [12] has demonstrated that unsupervised\n",
      "pre-training on a web-scale unlabeled Re-ID image dataset\n",
      "“LUPerson” (sub-sampled from massive streeview videos)\n",
      "surpasses that of pre-training on ImageNet.\n",
      "\n",
      "In this paper, our hypothesis is that scalable ReID pre-\n",
      "training methods that learn directly from raw videos can\n",
      "generate better representations. To verify it, we propose the\n",
      "noisy labels guided person Re-ID pre-training, which lever-\n",
      "\n",
      "\f",
      "ages the spatial and temporal correlations in videos as weak\n",
      "supervision. This supervision is nearly cost-free, and can be\n",
      "achieved by the tracklets of a person over time derived from\n",
      "any multi-object tracking algorithm, such as [56]. In par-\n",
      "ticular, we track each person in consecutive video frames,\n",
      "and automatically assign the tracked persons in the same\n",
      "tracklet to the same Re-ID label and vice versa. Enabled by\n",
      "the large amounts of raw videos in LUPerson [12], publicly\n",
      "available data of this form on the internet, we create a new\n",
      "variant named “LUPerson-NL” with derived pseudo Re-ID\n",
      "labels from tracklets for pre-training with noisy labels. This\n",
      "variant totally consists of 10M person images from 21K\n",
      "scenes with noisy labels of about 430K identities.\n",
      "\n",
      "We demonstrate that contrastive pre-training of Re-ID is\n",
      "an effective method of learning from this weak supervision\n",
      "at large scale. This new Pre-training framework utilizing\n",
      "Noisy Labels (PNL) composes three learning modules: (1)\n",
      "a simple supervised learning module directly learns from\n",
      "Re-ID labels through classification; (2) a prototype-based\n",
      "contrastive learning module helps cluster instances to the\n",
      "prototype which is dynamically updated by moving aver-\n",
      "aging the centroids of instance features, and progressively\n",
      "rectify the noisy labels based on the prototype assignment.\n",
      "and (3) a label-guided contrastive learning module utilizes\n",
      "the rectified labels subsequently as the guidance. In contrast\n",
      "to the vanilla momentum contrastive learning [7,12,19] that\n",
      "treats only features from the same instance as positive sam-\n",
      "ples, our label-guided contrastive learning uses the rectified\n",
      "labels to distinguish positive and negative samples accord-\n",
      "ingly, leading to a better performance. In principle, joint\n",
      "learning of these three modules make the consistency be-\n",
      "tween the prototype assignment from instances and the high\n",
      "confident (rectified) labels, as possible as it can.\n",
      "\n",
      "The experiments show that our PNL model achieves re-\n",
      "markable improvements on various person Re-ID bench-\n",
      "marks. Figure 1 indicates that the performance gain from\n",
      "our pre-trained models is consistent on different scales of\n",
      "training data. For example, upon the strong MGN [51]\n",
      "improves the mAP by\n",
      "baseline, our pre-trained model\n",
      "4.4%, 4.9% on Market1501 and DukeMTMC over the Im-\n",
      "ageNet supervised one, and 0.9%, 2.2% over the unsuper-\n",
      "vised pre-training baseline [12]. Moreover, the gains are\n",
      "even larger under the small-scale and few-shot settings,\n",
      "where the labeled Re-ID data are extremely limited. To the\n",
      "best of our knowledge, we are the first to show that large-\n",
      "scale noisy label guided pre-training can significantly ben-\n",
      "efit person Re-ID task.\n",
      "\n",
      "Our key contributions can be summarized as follows:\n",
      "\n",
      "• We propose noisy label guided pre-training for person Re-\n",
      "ID, which incorporates supervised learning, prototype-\n",
      "based contrastive learning, label-guided contrastive learn-\n",
      "ing and noisy label rectification to a unified framework.\n",
      "• We construct a large-scale noisy labeled person Re-ID\n",
      "\n",
      "dataset “LUPerson-NL” as a new variant of “LUPerson”.\n",
      "It is by far the largest noisy labeled person Re-ID dataset\n",
      "without any human labeling effort.\n",
      "\n",
      "• Our models pre-trained on LUPerson-NL push the state-\n",
      "of-the-art results on various public benchmarks to a new\n",
      "limit without bells and whistles.\n",
      "\n",
      "2. Related Work\n",
      "Supervised Person Re-ID. Most studies of person Re-ID\n",
      "employ supervised learning. Some [6, 21, 55] introduce a\n",
      "hard triplet loss on the global feature, ensuring a closer fea-\n",
      "ture distance for the same identity, while some [45, 59, 60]\n",
      "impose classification loss to learn a global feature from\n",
      "the whole image. There are also some other works that\n",
      "learn part-based local features with separate classification\n",
      "losses. For example, Suh et al. [46] presented part-aligned\n",
      "bi-linear representations and Sun et al. [48] represented fea-\n",
      "tures as horizontal strips. Recent approaches investigate\n",
      "learning invariant features concerning views [34], resolu-\n",
      "tions [31], poses [32], domains [22,23], or exploiting group-\n",
      "wise losses [36] or temporal information [18, 27] to im-\n",
      "prove performance. The more advantageous results on pub-\n",
      "lic benchmarks are achieved by MGN [51], which learns\n",
      "both global and local features with multiple losses. In [40],\n",
      "Qian et al further demonstrated the potential of generating\n",
      "cross-view images for person re-indentification conditioned\n",
      "on normalized poses. In this paper, we focus on model pre-\n",
      "training, and our pre-trained models can be applied to these\n",
      "representative methods and boost their performance.\n",
      "Unsupervised Person Re-ID. To alleviate the lack of pre-\n",
      "cise annotations, some works resort to unsupervised train-\n",
      "ing on unlabeled datasets. For example, MMCL [49] for-\n",
      "mulates unsupervised person Re-ID as a multi-label classi-\n",
      "fication to progressively seek true labels. BUC [33] jointly\n",
      "optimizes the network and the sample relationship with\n",
      "a bottom-up hierarchical clustering. MMT [14] collabo-\n",
      "ratively trains two networks to refine both hard and soft\n",
      "pseudo labels. SpCL [15] designs a hybrid memory to\n",
      "unify the representations for clustering and instance-wise\n",
      "contrastive learning. Both MMT [14] and SpCL [15] rely\n",
      "on explicit clustering of features from the whole training\n",
      "set, making them quite inefficient on large datasets like\n",
      "MSMT17. Since the appearance ambiguity is difficult to\n",
      "address without supervision, these unsupervised methods\n",
      "have limited performance. One alternative to address this\n",
      "issue is introducing model pre-training on large scale data.\n",
      "Inspired by the success of self-supervised representation\n",
      "learning [4,5,7,17,19,28,53], Fu et al. [12] proposed a large\n",
      "scale unlabeled Re-ID dataset, LUPerson, and illustrated\n",
      "the effectiveness of its unsupervised pre-trained models. In\n",
      "this work, we further try to make use of noisy labels from\n",
      "video tracklets to improve the pre-training quality through\n",
      "large-scale weakly-supervised pre-training.\n",
      "Weakly Supervised Person Re-ID. Several approaches\n",
      "\n",
      "\f",
      "also employ weak supervision in person Re-ID training.\n",
      "Instead of requiring bounding boxes within each frame,\n",
      "Meng et al. [38] rely on precise video-level labels, which\n",
      "reduces annotation cost but still need manual efforts to la-\n",
      "bel videos. On the contrary, we resort to noisy labels that\n",
      "can be automatically generated from tracklets on a much\n",
      "larger scale. Some [8, 29, 50] also leverage tracklets to su-\n",
      "pervise the training of Re-ID tasks. But unlike these ap-\n",
      "proaches, we are proposing a large-scale pre-training strat-\n",
      "egy for person Re-ID, by both building a new very large-\n",
      "scale dataset and devising a new pre-training framework:\n",
      "the new dataset, LUPerson-NL, is even larger than LU-\n",
      "Person [12] and has large amount of noisy Re-ID labels;\n",
      "The new framework, PNL, combines supervised learning,\n",
      "label-guided contrastive learning and prototype based con-\n",
      "trastive learning to exploit the knowledge under large-scale\n",
      "noise labels. Most importantly, our pre-trained models have\n",
      "demonstrated remarkable performance and generalization\n",
      "ability, helping achieve state-of-the-art results superior to\n",
      "all existing methods on public person Re-ID benchmarks.\n",
      "3. LUPerson-NL: LUPerson With Noisy Labels\n",
      "Supervised models based on deep networks are always\n",
      "data-hungry, but the labeled data they rely on are expen-\n",
      "sive to acquire.\n",
      "It is a tremendous issue for person Re-\n",
      "ID task, since the human labelers need to check across\n",
      "multiple views to ensure the correctness of Re-ID labels.\n",
      "The data shortage is partially alleviated by a recently pub-\n",
      "lished dataset, LUPerson [12], a dataset of unlabeled per-\n",
      "son images with a significantly larger scale than previous\n",
      "person Re-ID datasets. Unsupervised pre-trained models\n",
      "[12] on LUPerson have demonstrated remarkable effective-\n",
      "ness without utilizing additional manual annotations, which\n",
      "arouses our curiosity: can we further improve the perfor-\n",
      "mance of pre-training directly by utilizing temporal cor-\n",
      "relation as weak supervision? To verify this, we build a\n",
      "new variant of LUPerson on top of the raw videos from\n",
      "LUPerson and assign label to each person image with au-\n",
      "tomatically generated tracklet. We name it LUPerson-NL\n",
      "It consists of 10M\n",
      "with NL standing for Noisy Labels.\n",
      "images with about 430K identities collected from 21K\n",
      "scenes. To our best knowledge, this is the largest person\n",
      "Re-ID dataset constructed without human labelling by far.\n",
      "Our LUPerson-NL will be released for scientific research\n",
      "only, while any usage for other purpose is forbidden.\n",
      "3.1. Constructing LUPerson-NL\n",
      "\n",
      "We utilize the off-the-shelf tracking algorithm [56] 1 to\n",
      "detect persons and extract person tracklets from the same\n",
      "raw videos of [12]. We assign each tracklet with a unique\n",
      "class label. The detection is not perfect: e.g. the bounding\n",
      "boxes may only cover partial bodies without heads or upper\n",
      "parts. Human pose estimation [47] is thus appended that\n",
      "\n",
      "1FairMOT: https://github.com/ifzhang/FairMOT\n",
      "\n",
      "Figure 2.\n",
      "(X, Y ) indicates Y % of identities each has less than X images.\n",
      "\n",
      "Identity distribution of LUPerson-NL. A curve point\n",
      "\n",
      "Figure 3. Besides the correctly labeled identities as shown by (a),\n",
      "there are two types of labeling errors in LUPerson-NL. Noise-I:\n",
      "same person labeled as different identities, e.g. D, E and F shown\n",
      "in (b). Noise-II: different persons labeled as the same identity,\n",
      "e.g. G shown in (c).\n",
      "\n",
      "helps filter out imperfect boxes by predicting landmarks.\n",
      "\n",
      "We track every person in the video frame by frame. In\n",
      "order to guarantee both the sufficiency and diversity, we\n",
      "adopt the following strategy:\n",
      "i) We first remove the per-\n",
      "son identities that appear in too few frames, i.e. no more\n",
      "than 200; ii) Within the tracklet of each identity, we then\n",
      "perform sampling with a rate of one image per 20 frames\n",
      "to reduce the number of duplicated images. Thus we can\n",
      "make sure that there would be at least 10 images associat-\n",
      "ing to each identity. Through this filtering procedure, we\n",
      "have collected 10, 683, 716 images of 433, 997 identities in\n",
      "total. They belong to 21, 697 videos which are less than\n",
      "the videos that [12] uses, due to our extra filtering strat-\n",
      "egy for more reliable identity labels. Thus, LUPerson-NL\n",
      "is very different from LUPerson, as it adopts very different\n",
      "sampling and post-processing strategies, not to mention the\n",
      "noisy labels driven from the spatial-temporal information.\n",
      "3.2. Properties of LUPerson-NL\n",
      "\n",
      "LUPerson-NL is advantageous in following aspects:\n",
      "Large amount of images and identities. We detail the\n",
      "statistics of existing popular person Re-ID datasets in Table\n",
      "1. As we can see, the proposed LUPerson-NL, with over\n",
      "10M images and 433K noisy labeled identities, is the sec-\n",
      "ond largest among the listed. Indeed, SYSU30K has more\n",
      "images, but it extracts images from only 1K TV program\n",
      "videos frame by frame, making it less competitive in vari-\n",
      "ability and less compatible in practice, the pre-training per-\n",
      "formance comparison can be found at supplementary mate-\n",
      "rials. Besides, LUPerson-NL was constructed without hu-\n",
      "man labeling effort, making it more suitable to scale-up.\n",
      "\n",
      "102030405060708090#ofpersonimages0255075100%ofidentities𝐴𝐵𝐶(a) Correctly labeled identities(b) Noise-I(c) Noise-II𝐷𝐸𝐹𝐺\f",
      "Datasets\n",
      "VIPeR [16]\n",
      "GRID [35]\n",
      "CUHK03 [30]\n",
      "Market [58]\n",
      "Airport [25]\n",
      "DukeMTMC [61]\n",
      "MSMT17 [52]\n",
      "SYSU30K [50]\n",
      "LUPerson [12]\n",
      "LUPerson-NL\n",
      "\n",
      "#scene\n",
      "#images\n",
      "2\n",
      "1,264\n",
      "8\n",
      "1,275\n",
      "2\n",
      "14, 096\n",
      "6\n",
      "32, 668\n",
      "6\n",
      "39, 902\n",
      "8\n",
      "36, 411\n",
      "15\n",
      "126, 441\n",
      "1,000\n",
      "29,606,918\n",
      "46, 260\n",
      "4, 180, 243\n",
      "10, 683, 716 21, 697 ≃ 433, 997\n",
      "\n",
      "#persons\n",
      "632\n",
      "1,025\n",
      "1, 467\n",
      "1, 501\n",
      "9, 651\n",
      "1, 852\n",
      "4, 101\n",
      "30,508\n",
      "> 200k\n",
      "\n",
      "labeled environment\n",
      "\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "\n",
      "-\n",
      "subway\n",
      "campus\n",
      "campus\n",
      "airport\n",
      "campus\n",
      "campus\n",
      "\n",
      "weakly TV program\n",
      "\n",
      "no\n",
      "noisy\n",
      "\n",
      "vary\n",
      "vary\n",
      "\n",
      "camera view\n",
      "fixed\n",
      "fixed\n",
      "fixed\n",
      "fixed\n",
      "fixed\n",
      "fixed\n",
      "fixed\n",
      "dynamic\n",
      "dynamic\n",
      "dynamic\n",
      "\n",
      "detector\n",
      "hand\n",
      "hand\n",
      "DPM [11]+hand\n",
      "DPM [11]+hand\n",
      "ACF [10]\n",
      "Hand\n",
      "FasterRCNN [42]\n",
      "YOLOv2\n",
      "YOLOv5\n",
      "FairMOT [56]\n",
      "\n",
      "crop size\n",
      "128 × 48\n",
      "vary\n",
      "vary\n",
      "128 × 64\n",
      "128 × 64\n",
      "vary\n",
      "vary\n",
      "vary\n",
      "vary\n",
      "vary\n",
      "\n",
      "Table 1. Comparing statistics among existing popular Re-ID datasets. LUPerson-NL is by far the largest Re-ID dataset with better diversity\n",
      "without human labeling effort. SYSU30K is partly annotated by human annotator.\n",
      "\n",
      "4. PNL: Pre-training with Noisy Labels for\n",
      "\n",
      "Person Re-ID\n",
      "\n",
      "Based on the new LUPerson-NL dataset with large scale\n",
      "noisy labels, we devise a novel Pretraining framework with\n",
      "Noisy Labels for person Re-ID, namely PNL.\n",
      "\n",
      "Denote all\n",
      "\n",
      "the data samples from LUPerson-NL as\n",
      "{(xi, yi)}n\n",
      "i=1, with n being the size of the dataset, xi a\n",
      "person image and yi ∈ {1, . . . , K} its associated identity\n",
      "label. Here K represents the number of all identities that\n",
      "are recorded in LUPerson-NL.\n",
      "\n",
      "Inspired by recent methods [4, 5, 7, 17, 19, 28], our PNL\n",
      "framework adopts Siamese networks that have been fully in-\n",
      "vestigated for contrastive representation learning. As shown\n",
      "by Figure 4, given an input person image xi, we first per-\n",
      "form two randomly selected augmentations (T, T ′), pro-\n",
      "ducing two augmented images ( ˜xi, ˜x′\n",
      "i). We feed one of\n",
      "them, ˜xi, into an encoder Eq to get a query feature qi;\n",
      "while the other one, ˜x′\n",
      "i, is fed into another encoder Ek\n",
      "to get a key feature ki. Following [19], we design Ek to\n",
      "be a momentum version of Eq, i.e. the two encoders Ek\n",
      "and Eq share the same network structure, but with different\n",
      "weights. The weights in Ek are exponential moving aver-\n",
      "ages of the weights in Eq. During training, weights of Ek\n",
      "are refreshed through a momentum update from Eq. And\n",
      "the detailed algorithm can be found at supplementary mate-\n",
      "rials.\n",
      "4.1. Supervised Classification\n",
      "Since the raw labels {yi}n\n",
      "\n",
      "i=1 in LUPerson-NL contain\n",
      "lots of noises as illustrated in previous section, they have\n",
      "to be rectified during training. Let ˆyi be the rectified label\n",
      "of image xi. As long as ˆyi is given, it would be intuitive\n",
      "that we train classification based on the corrected label ˆyi.\n",
      "In particular, we would append a classifier to transform the\n",
      "feature from Eq into probabilities pi ∈ RK with K being\n",
      "the number of classes. Then we impose a classification loss\n",
      "\n",
      "Li\n",
      "\n",
      "ce = − log(pi[ˆyi]).\n",
      "\n",
      "(1)\n",
      "\n",
      "However, the acquisition of ˆyi is not straight-forward.\n",
      "We resort to prototypes, the moving averaged centroids of\n",
      "features from training instances, to accomplish this task.\n",
      "\n",
      "Figure 4. The overview of our PNL framework.\n",
      "It comprises\n",
      "a supervised classification module, a prototype based contrastive\n",
      "learning module, and a label-guided contrastive learning module.\n",
      "\n",
      "Balanced distribution of identities. We illustrate the cu-\n",
      "mulative percentage of identities with respect to the number\n",
      "of their corresponding person images as a curve in Figure 2.\n",
      "A point (X, Y ) on the curve represents that there are in total\n",
      "Y % identities in LUPerson-NL, that each of them has less\n",
      "than X images. It can be observed that: i) about 75% of all\n",
      "the identities in LUPerson-NL have a person image num-\n",
      "ber within [10, 25]; ii) the percentage of identities that have\n",
      "more than 50 person images each, occupy only a very small\n",
      "portion of about 6.4% (27, 767/433, 997) in LUPerson-NL.\n",
      "These observations all show that our LUPerson-NL is well\n",
      "balanced in terms of identity distribution, making it a suit-\n",
      "able dataset for person Re-ID tasks.\n",
      "\n",
      "In spite of our dedicatedly designed tracking and filter-\n",
      "ing strategies as proposed in Sec 3.1, the identity labels we\n",
      "obtained can never be very accurate due to the technical up-\n",
      "per bounds of current tracking methods. Figure 3 visualizes\n",
      "the two noise types in LUPerson-NL that are caused by dif-\n",
      "ferent labeling errors, which are Noise-I, where the same\n",
      "person is split into different tracklets and is mistaken as dif-\n",
      "ferent persons; and Noise-II, that different persons are\n",
      "recognized as the same person.\n",
      "\n",
      "fc𝒙!𝑇𝑇\"#𝒙!#𝒙!\"𝐸#𝐸$𝑦!&𝑦!𝒌!!,𝒌!\",…,𝒌!#$𝑦!!,$𝑦!\",…,$𝑦!#&𝑦!prototypesqueue𝒒!𝒄\",𝒄#,…,𝒄$Label Guided Contrastive LossClassificationLoss𝒌!Label Update&𝑦!Prototype BasedContrastive Loss𝒑!update prototypesenqueuemomentum updatew. gradientw/o. gradient𝒔!\f",
      "(2)\n",
      "\n",
      "(3)\n",
      "\n",
      "4.2. Label Rectification with Prototypes\n",
      "\n",
      "As depicted by Figure 4, we maintain prototypes as a dic-\n",
      "tionary of feature vectors {c1, c2, . . . , cK}, where K is the\n",
      "number of identities, ck ∈ Rd is a prototype representing a\n",
      "class-wise feature centroid. In each training step, we would\n",
      "first evaluate the similarity score sk\n",
      "i between the query fea-\n",
      "ture qi and each of the current prototypes ck by\n",
      "\n",
      "sk\n",
      "i =\n",
      "\n",
      "exp(qi · ck/τ )\n",
      "k=1 exp(qi · ck/τ )\n",
      "\n",
      ".\n",
      "\n",
      "(cid:80)K\n",
      "\n",
      "Let pi be the classification probability given by the clas-\n",
      "sifier with weights updated in the previous step. The rec-\n",
      "tified label ˆyi for this step is then generated by combining\n",
      "both the prototype scores si = {sk\n",
      "k=1 and the classifica-\n",
      "tion probability pi as\n",
      "\n",
      "i }K\n",
      "\n",
      "li =\n",
      "\n",
      "(pi + si),\n",
      "\n",
      "1\n",
      "2\n",
      "(cid:40)\n",
      "\n",
      "ˆyi =\n",
      "\n",
      "arg maxj lj\n",
      "yi\n",
      "\n",
      "i\n",
      "\n",
      "if maxj lj\n",
      "otherwise.\n",
      "\n",
      "i > T ,\n",
      "\n",
      "Here we compute a soft pseudo label li and convert it to a\n",
      "hard one ˆyi based on a threshold T . If the highest score in li\n",
      "is larger than T , the corresponding class would be selected\n",
      "as ˆyi, otherwise the original raw label yi would be kept.\n",
      "\n",
      "4.3. Prototype Based Contrastive Learning\n",
      "\n",
      "The newly rectified label ˆyi can then be used to super-\n",
      "vise the cross-entropy loss Li\n",
      "ce for classification as formu-\n",
      "lated by Equation 1. Besides, it also helps train prototypes\n",
      "ck in return. In specific, we propose a prototype based con-\n",
      "trastive loss Li\n",
      "pro to constrain that the feature of each sam-\n",
      "ple should be closer to the prototype it belongs to. We for-\n",
      "mulate the loss as\n",
      "\n",
      "Li\n",
      "\n",
      "pro = −log\n",
      "\n",
      "exp(qi · cˆyi/τ )\n",
      "j=1 exp(qi · cj/τ )\n",
      "\n",
      ",\n",
      "\n",
      "(cid:80)K\n",
      "\n",
      "(4)\n",
      "\n",
      "with qi being the query feature from Eq, τ being a hyper-\n",
      "\n",
      "parameter representing temperature.\n",
      "\n",
      "All the prototypes are maintained as a dictionary, with\n",
      "\n",
      "step-wise updates following a momentum mechanism as\n",
      "cˆyi = mcˆyi + (1 − m)qi.\n",
      "\n",
      "(5)\n",
      "\n",
      "4.4. Label-Guided Contrastive Learning\n",
      "\n",
      "Instance-wise contrastive learning proved to be very ef-\n",
      "fective in self-supervised learning [4, 5, 7, 17, 19]. It learns\n",
      "instance-level feature discrimination by encouraging simi-\n",
      "larity among features from the same instance, while pro-\n",
      "moting dissimilarity between features from different in-\n",
      "stances. The instance-wise contrastive loss is given by\n",
      "\n",
      "Li\n",
      "\n",
      "ic = −log\n",
      "\n",
      "exp(qi · k+\n",
      "i /τ ) + (cid:80)M\n",
      "\n",
      "i /τ )\n",
      "j=1 exp(qi · k−\n",
      "\n",
      "j /τ )\n",
      "\n",
      ", (6)\n",
      "\n",
      "exp(qi · k+\n",
      "\n",
      "with qi being the query feature of current instance i.\n",
      "k+\n",
      "i (= ki) is the positive key feature generated from the\n",
      "momentum encoder Ek. It is marked positive since it shares\n",
      "∗ ∈ Rd, on the contrary, are\n",
      "the same instance with qi. k−\n",
      "the rest features stored in a queue that represent negative\n",
      "samples. The queue has a size of M . At the end of each\n",
      "training step, the queue would be updated by en-queuing\n",
      "the new key feature and de-queuing the oldest one.\n",
      "\n",
      "Such instance-level contrastive learning is far from per-\n",
      "fect, as it neglects the relationships among different in-\n",
      "stances. For example, even though two instances depict the\n",
      "same person, it would still strengthen the gap between their\n",
      "Instead, we propose a label guided contrastive\n",
      "features.\n",
      "learning module, making use of the rectified labels ˆyi to\n",
      "ensure more reasonable grouping of contrastive pairs.\n",
      "\n",
      "We redesign the queue to additionally record labels ˆyi.\n",
      "Represented by Q = [(kjt, ˆyjt)]M\n",
      "t=1, our new queue accepts\n",
      "not only a key feature ki but also its rectified label ˆyi during\n",
      "update. These newly recorded labels help better distinguish\n",
      "positive and negative pairs. Let P(i) be the new set of posi-\n",
      "tive features and N (i) the new set of negative features: fea-\n",
      "tures in P(i) share the same rectified label with the current\n",
      "instance i while features in N (i) do not. Our label guided\n",
      "contrastive loss can be given by\n",
      "\n",
      "Li\n",
      "\n",
      "lgc =\n",
      "\n",
      "−1\n",
      "|P(i)|\n",
      "\n",
      "log\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(cid:16) qi·k+\n",
      "τ\n",
      "\n",
      "(cid:80)\n",
      "k+∈P(i)\n",
      "(cid:16) qi·k+\n",
      "τ\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(cid:17) ,\n",
      "\n",
      "(cid:16) qi·k−\n",
      "τ\n",
      "\n",
      "+ (cid:80)\n",
      "k−∈N (i)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:80)\n",
      "k+∈P(i)\n",
      "\n",
      "exp\n",
      "\n",
      "(7)\n",
      "\n",
      "with\n",
      "\n",
      "P(i) = {kjt|ˆyjt = ˆyi, ∀(kjt, ˆyjt) ∈ Q} ∪ {ki},\n",
      "N (i) = {kjt|ˆyjt ̸= ˆyi, ∀(kjt, ˆyjt) ∈ Q},\n",
      "\n",
      "(8)\n",
      "\n",
      "where ki and ˆyi are the key feature and the rectified label of\n",
      "the current instance i.\n",
      "\n",
      "Finally we combine all the components above to pre-\n",
      "\n",
      "train models on LUPerson-NL with the following loss\n",
      "ce + λproLi\n",
      "\n",
      "pro + λlgcLi\n",
      "\n",
      "Li = Li\n",
      "\n",
      "lgc.\n",
      "\n",
      "(9)\n",
      "\n",
      "We set λpro = λlgc = 1 during training.\n",
      "\n",
      "5. Experiments\n",
      "\n",
      "5.1. Implementation\n",
      "\n",
      "Hyper-parameter settings. We set the hyper-parameters\n",
      "τ = 0.1 and T = 0.8. The momentum m for updating\n",
      "both the momentum encoder Ek and the prototypes is set\n",
      "to 0.999. More hyper-parameters exploration and training\n",
      "details can be found at supplementary materials.\n",
      "Dataset and protocol. We conduct extensive experiments\n",
      "on four popular person Re-ID datasets: CUHK03, Market,\n",
      "\n",
      "\f",
      "pre-train\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPnl pnl.\n",
      "\n",
      "pre-train\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPnl pnl.\n",
      "\n",
      "Trip [21]\n",
      "45.2/63.8\n",
      "55.5/61.2\n",
      "62.6/67.6\n",
      "69.1/73.1\n",
      "\n",
      "Trip [21]\n",
      "65.2/80.7\n",
      "65.4/81.1\n",
      "69.8/83.1\n",
      "71.0/84.7\n",
      "\n",
      "IDE [59]\n",
      "50.6/55.9\n",
      "52.5/57.7\n",
      "57.6/62.3\n",
      "68.3/73.5\n",
      "\n",
      "IDE [59]\n",
      "62.8/80.8\n",
      "63.4/81.6\n",
      "65.9/82.2\n",
      "70.3/85.0\n",
      "\n",
      "MGN [51]\n",
      "70.5/71.2\n",
      "67.1/67.0\n",
      "74.7/75.4\n",
      "80.4/80.9\n",
      "\n",
      "MGN [51]\n",
      "79.4/89.0\n",
      "79.5/89.1\n",
      "82.1/91.0\n",
      "84.3/92.0\n",
      "\n",
      "pre-train\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPnl pnl.\n",
      "\n",
      "pre-train\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPnl pnl.\n",
      "\n",
      "Trip [21]\n",
      "76.2/89.7\n",
      "75.1/88.5\n",
      "79.8/71.5\n",
      "81.2/91.4\n",
      "\n",
      "Trip [21]\n",
      "34.3/54.8\n",
      "34.4/55.4\n",
      "36.6/57.1\n",
      "41.4/61.6\n",
      "\n",
      "IDE [59]\n",
      "74.1/90.2\n",
      "74.5/89.3\n",
      "77.9/91.0\n",
      "82.4/92.8\n",
      "\n",
      "IDE [59]\n",
      "36.2/66.2\n",
      "37.6/67.3\n",
      "39.8/68.9\n",
      "44.0/72.0\n",
      "\n",
      "MGN [51]\n",
      "87.5/95.1\n",
      "88.2/95.3\n",
      "91.0/96.4\n",
      "91.9/96.6\n",
      "\n",
      "MGN [51]\n",
      "63.7/85.1\n",
      "62.7/84.3\n",
      "65.7/85.5\n",
      "68.0/86.0\n",
      "\n",
      "(a) CUHK03\n",
      "\n",
      "(b) Market1501\n",
      "\n",
      "(c) DukeMTMC\n",
      "\n",
      "(d) MSMT17\n",
      "\n",
      "Table 2. Comparing three supervised Re-ID baselines using different pre-trained models. “IN sup.”/“IN unsup.” indicates model that is\n",
      "supervisely/unsupervisely pre-trained on ImageNet; “LUP unsup.” is the model unsupervisely pre-trained on LUPerson; “LUPnl pnl.“\n",
      "refers to the model that pre-trained on LUPerson-NL using our PNL framework. All results are shown in mAP/cmc1.\n",
      "\n",
      "pre-train\n",
      "\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPnl pnl.\n",
      "\n",
      "pre-train\n",
      "\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPnl pnl.\n",
      "\n",
      "10%\n",
      "53.1/76.9\n",
      "58.4/81.7\n",
      "64.6/85.5\n",
      "72.4/88.8\n",
      "\n",
      "30%\n",
      "75.2/90.8\n",
      "76.6/91.9\n",
      "81.9/93.7\n",
      "85.2/94.2\n",
      "\n",
      "10%\n",
      "45.1/65.3\n",
      "48.1/66.9\n",
      "53.5/72.0\n",
      "60.6/75.8\n",
      "\n",
      "30%\n",
      "64.7/80.2\n",
      "65.8/80.2\n",
      "69.4/81.9\n",
      "74.5/86.3\n",
      "\n",
      "pre-train\n",
      "\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPnl pnl.\n",
      "\n",
      "10%\n",
      "23.2/50.2\n",
      "22.6/48.8\n",
      "25.5/51.1\n",
      "28.2/51.1\n",
      "\n",
      "30%\n",
      "41.9/70.8\n",
      "40.4/68.7\n",
      "44.6/71.4\n",
      "47.7/71.2\n",
      "\n",
      "small-scale\n",
      "50%\n",
      "81.5/93.5\n",
      "82.0/94.1\n",
      "85.8/94.9\n",
      "88.3/95.5\n",
      "\n",
      "small-scale\n",
      "50%\n",
      "71.8/84.6\n",
      "72.5/84.4\n",
      "75.6/86.7\n",
      "78.8/88.3\n",
      "\n",
      "small-scale\n",
      "50%\n",
      "50.3/76.9\n",
      "49.0/75.0\n",
      "53.0/77.7\n",
      "55.5/77.2\n",
      "\n",
      "70%\n",
      "84.8/94.5\n",
      "85.4/94.5\n",
      "88.8/95.9\n",
      "90.1/96.2\n",
      "\n",
      "90%\n",
      "86.9/95.2\n",
      "87.4/95.5\n",
      "90.5/96.4\n",
      "91.3/96.4\n",
      "\n",
      "10%\n",
      "21.1/41.8\n",
      "18.6/36.1\n",
      "26.4/47.5\n",
      "42.0/61.6\n",
      "\n",
      "30%\n",
      "68.1/87.6\n",
      "69.3/87.8\n",
      "78.3/92.1\n",
      "83.7/94.0\n",
      "\n",
      "(a) Market1501\n",
      "\n",
      "70%\n",
      "75.5/86.8\n",
      "76.3/86.9\n",
      "78.9/88.2\n",
      "81.6/89.5\n",
      "\n",
      "90%\n",
      "78.0/88.3\n",
      "78.5/88.7\n",
      "81.1/90.0\n",
      "83.3/91.2\n",
      "\n",
      "10%\n",
      "31.5/47.1\n",
      "32.4/48.0\n",
      "35.8/50.2\n",
      "52.2/64.1\n",
      "\n",
      "30%\n",
      "65.4/79.8\n",
      "65.3/80.2\n",
      "72.3/83.8\n",
      "77.7/87.9\n",
      "\n",
      "(b) DukeMTMC\n",
      "\n",
      "70%\n",
      "56.9/81.2\n",
      "55.7/79.9\n",
      "59.5/81.8\n",
      "61.6/81.8\n",
      "\n",
      "90%\n",
      "61.9/84.2\n",
      "60.9/83.0\n",
      "63.7/85.0\n",
      "66.1/84.8\n",
      "\n",
      "10%\n",
      "14.7/34.1\n",
      "13.2/29.2\n",
      "17.0/36.0\n",
      "24.5/42.7\n",
      "\n",
      "30%\n",
      "44.5/71.1\n",
      "41.4/67.1\n",
      "49.0/73.6\n",
      "53.2/74.4\n",
      "\n",
      "few-shot\n",
      "50%\n",
      "80.2/92.8\n",
      "78.3/90.9\n",
      "84.2/93.9\n",
      "88.1/95.2\n",
      "\n",
      "few-shot\n",
      "50%\n",
      "73.9/85.7\n",
      "73.7/85.1\n",
      "77.7/87.4\n",
      "81.1/89.6\n",
      "\n",
      "few-shot\n",
      "50%\n",
      "56.2/79.5\n",
      "53.3/77.6\n",
      "57.4/80.5\n",
      "62.2/81.0\n",
      "\n",
      "70%\n",
      "84.2/94.0\n",
      "84.4/94.1\n",
      "88.4/95.5\n",
      "90.5/96.3\n",
      "\n",
      "90%\n",
      "86.7/94.6\n",
      "87.1/95.2\n",
      "90.4/96.3\n",
      "91.6/96.4\n",
      "\n",
      "70%\n",
      "77.2/87.8\n",
      "77.7/87.8\n",
      "80.8/89.2\n",
      "83.2/91.1\n",
      "\n",
      "90%\n",
      "79.1/88.8\n",
      "79.4/89.0\n",
      "82.0/90.6\n",
      "84.1/91.3\n",
      "\n",
      "70%\n",
      "60.9/82.8\n",
      "59.1/81.5\n",
      "62.9/83.5\n",
      "65.8/83.8\n",
      "\n",
      "90%\n",
      "63.4/84.5\n",
      "62.4/83.8\n",
      "65.0/85.1\n",
      "67.4/85.3\n",
      "\n",
      "(c) MSMT17\n",
      "Table 3. Comparing pre-trained models on three labeled Re-ID datasets, under the small-scale setting and the few-shot setting, with\n",
      "different usable data percentages. “LUPnl pnl.” is our model pre-trained on LUPerson-NL using PNL. Results are shown in mAP/cmc1.\n",
      "\n",
      "DukeMTMC and MSMT17. We adopt their official set-\n",
      "tings, except CUHK03 where its labeled counterpart with\n",
      "new protocols proposed in [62] is used. We follow the stan-\n",
      "dard evaluation metrics: the mean Average Precision (mAP)\n",
      "and the Cumulated Matching Characteristics top-1 (cmc1).\n",
      "5.2. Improving Supervised Re-ID\n",
      "\n",
      "To evaluate our pre-trained model based on LUPerson-\n",
      "NL with respect to supervised person Re-ID tasks. we\n",
      "conduct experiments using three representative supervised\n",
      "Re-ID baselines with different pre-training models. These\n",
      "baseline methods include two simpler approaches driven\n",
      "only by the triplet loss (Trip [21]) or the classification loss\n",
      "(IDE [59]), as well as a stronger and more complex method\n",
      "MGN [51] that use both triplet and classification losses.\n",
      "\n",
      "{“IN”, “LUP”, “LUPnl”} represent ImageNet [43], LU-\n",
      "Person [12] and our LUPerson-NL respectively; while the\n",
      "{“sup.”, “unsup.”, “pnl.”} stand for the {“supervised”, “un-\n",
      "supervised”, and “pretrain with noisy label”} pre-training\n",
      "methods. e.g. the “LUPnl pnl.” in the bottom rows of Ta-\n",
      "ble 2 all refer to our model, which is pre-trained on our\n",
      "LUPerson-NL dataset using our PNL framework.\n",
      "\n",
      "From Table 2 we can see, for all of the three base-\n",
      "line methods, our pre-trained model improves their perfor-\n",
      "mances greatly on the four popular person Re-ID datasets.\n",
      "Specifically, the improvements are at least 5.7%, 0.9%,\n",
      "1.2% and 2.3% in terms of mAP on CUHK03, Market1501,\n",
      "DukeMTMC and MSMT17 respectively.\n",
      "\n",
      "We report results in Table 2, where the abbreviations\n",
      "\n",
      "Note that even though the performance of the baseline\n",
      "\n",
      "\f",
      "MGN on Market1501 has been extremely high, our model\n",
      "still brings considerable improvement over it. The other\n",
      "way around, our pre-trained models obtain more signifi-\n",
      "cant improvements on relatively weak methods (Trip and\n",
      "IDE), unveiling that model initialization plays a critical part\n",
      "in person Re-ID training.\n",
      "\n",
      "Our noisy label guided pre-training models are also sig-\n",
      "nificantly advantageous over the previous “LUPerson un-\n",
      "sup“ models, which emphasizes the superiority of our PNL\n",
      "framework and our LUPerson-NL dataset.\n",
      "\n",
      "5.3. Improving Unsupervised Re-ID Methods\n",
      "\n",
      "Our pre-trained model can also benefit unsupervised per-\n",
      "son Re-ID methods. Based on the state-of-the-art unsuper-\n",
      "vised method SpCL [15], we explore different pre-training\n",
      "models utilizing two settings proposed by SpCL: the pure\n",
      "unsupervised learning (USL) and the unsupervised domain\n",
      "adaptation (UDA). Results in Table 4 illustrate that our pre-\n",
      "trained model outperforms the others in all UDA tasks, as\n",
      "well as the USL task on DukeMTMC dataset. In the USL\n",
      "task on Market1501, we achieve the second best scores\n",
      "slightly lower than the LUPerson model [12].\n",
      "\n",
      "5.4. Comparison on Small-scale and Few-shot\n",
      "\n",
      "Following the same protocols proposed by [12], we con-\n",
      "duct experiments under two small data settings: the small-\n",
      "scale setting and the few-shot setting. The small-scale set-\n",
      "ting restricts the percentage of usable identities, while the\n",
      "few-shot setting restricts the percentage of usable person\n",
      "images each identity has. Under both settings, we vary\n",
      "the usable data percentages of three popular datasets from\n",
      "10% ∼ 100%. We compare different pre-trained models\n",
      "under these settings with MGN as the baseline method. The\n",
      "results shown in Table 3 verify the consistent improvements\n",
      "brought by our model on all the datasets under both settings.\n",
      "Besides, the results in Table 3 show that the gains of\n",
      "our pre-trained models are even larger under a more lim-\n",
      "ited amount of labeled data. For example, under the “small-\n",
      "scale” setting, our model outperforms “LUPerson unsup”\n",
      "by 7.8%, 7.1% and 2.7% on Market1501, DukeMTMC and\n",
      "MSMT17 respectively with 10% identities. The improve-\n",
      "ments rise to 15.6%, 16.4% and 6.5% under the “few-shot”\n",
      "setting with 10% person images.\n",
      "\n",
      "Most importantly, our pre-trained “LUPnl pnl” model\n",
      "helps achieve advantageous results with a mAP of 72.4 and\n",
      "a cmc1 of 88.8, using only 10% labeled data from the Mar-\n",
      "ket1501 training set. The task is really challenging, con-\n",
      "sidering that the training set composes only 1, 170 images\n",
      "belonging to 75 identities; while evaluations are performed\n",
      "on a much larger testing set with 19, 281 images belong-\n",
      "ing to 750 identities. We consider these results extremely\n",
      "appealing as they demonstrate the strong potential of our\n",
      "pre-trained models in real-world applications.\n",
      "\n",
      "pre-train\n",
      "\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPnl pnl.\n",
      "\n",
      "USL\n",
      "\n",
      "M\n",
      "72.4/87.8\n",
      "72.9/88.6\n",
      "76.2/90.2\n",
      "75.6/89.3\n",
      "\n",
      "D\n",
      "64.9/80.3\n",
      "62.6/78.8\n",
      "67.1/81.6\n",
      "68.1/82.0\n",
      "\n",
      "UDA\n",
      "\n",
      "D → M\n",
      "76.4/90.1\n",
      "77.1/90.6\n",
      "79.2/91.7\n",
      "80.7/92.2\n",
      "\n",
      "M → D\n",
      "67.9/82.3\n",
      "66.3/81.6\n",
      "69.1/83.2\n",
      "72.2/84.9\n",
      "\n",
      "Table 4. Performances of different pre-trained models on the un-\n",
      "supervised Re-ID method SpCL [15] under two unsupervised task\n",
      "settings: the pure unsupervised learning (USL) and the unsuper-\n",
      "vised domain adaptation (UDA). Here M and D refer to the Mar-\n",
      "ket1501 dataset and the DukeMTMC dataset respectively.\n",
      "\n",
      "method\n",
      "MSMT17\n",
      "\n",
      "SupCont [26]\n",
      "66.5/84.7\n",
      "\n",
      "LUP [12]\n",
      "65.3/84.0\n",
      "\n",
      "PNL(ours)\n",
      "68.0/86.0\n",
      "\n",
      "20%\n",
      "\n",
      "Table 5. Performance comparison for different pre-training meth-\n",
      "ods on LUPerson-NL dataset.\n",
      "# ce ic pro lgc\n",
      "1 ✓\n",
      "✓\n",
      "2\n",
      "3 ✓ ✓\n",
      "4 ✓\n",
      "5 ✓\n",
      "6 ✓ ✓ ✓\n",
      "7 ✓\n",
      "\n",
      "40%\n",
      "32.0/56.1 45.0/69.5 62.7/83.0\n",
      "34.5/59.5 47.9/72.6 65.3/84.0\n",
      "37.6/62.6 49.6/73.5 66.5/84.7\n",
      "35.7/59.1 48.5/72.4 65.8/84.1\n",
      "✓ 38.5/63.0 50.9/74.5 67.1/85.2\n",
      "39.0/63.4 51.7/74.4 67.4/85.4\n",
      "✓ ✓ 39.6/63.7 51.9/75.0 68.0/86.0\n",
      "\n",
      "100%\n",
      "\n",
      "✓\n",
      "\n",
      "Table 6. Ablating components of PNL on MSMT with data per-\n",
      "centages 20%, 40% and 100% under the small scale setting. ce:\n",
      "supervised classification; ic:\n",
      "instance-wise contrastive learning;\n",
      "pro: prototypes for both prototype-based contrastive learning and\n",
      "label rectification; lgc: label-guided contrastive learning.\n",
      "\n",
      "5.5. Comparison with other pre-training methods\n",
      "\n",
      "We compare our proposed PNL with some other popu-\n",
      "lar pre-training methods in Table 5. LUP [12] is a varient\n",
      "of MoCoV2 for person Re-ID based on unsupervised con-\n",
      "strastive learning, while SupCont [26] considers both su-\n",
      "pervised and constrastive learning. Our PNL outperforms\n",
      "all these rep-resentative pre-training methods, indicating the\n",
      "superiority of our proposed method.\n",
      "5.6. Ablation Study\n",
      "\n",
      "We also investigate the effectiveness of each designed\n",
      "component in PNL through ablation experiments. Results\n",
      "shown by Table 6 illustrate the efficacy of our proposed\n",
      "components. We have the following observations: i) Train-\n",
      "ing with an instance-wise contrastive loss Li\n",
      "ic (row 2) with-\n",
      "out using any labels leads to even better performance than\n",
      "training with a classification loss Li\n",
      "ce (row 1) that utilizes\n",
      "the labels from LUPerson-NL, implying that the noisy la-\n",
      "bels in LUPerson-NL would misguide representation learn-\n",
      "ii) Jointly training\n",
      "ing if directly adopted as supervision.\n",
      "with both losses Li\n",
      "ic (row 3) improves over us-\n",
      "ing only one loss (row 1, row 2), suggesting that learning\n",
      "instance-wise discriminative representations complements\n",
      "iii) The prototypes which contribute to\n",
      "label supervision.\n",
      "\n",
      "ce and Li\n",
      "\n",
      "\f",
      "setting\n",
      "\n",
      "MSMT17\n",
      "\n",
      "ce+pro\n",
      "\n",
      "ce+pro+lgc\n",
      "\n",
      "w/o. lc\n",
      "64.8/83.4\n",
      "\n",
      "w. lc\n",
      "65.8/84.1\n",
      "\n",
      "w/o. lc\n",
      "66.7/85.0\n",
      "\n",
      "w. lc\n",
      "68.0/86.0\n",
      "\n",
      "Table 7. Ablating the label correction. lc: “label correction”.\n",
      "\n",
      "Method\n",
      "MGN† [51] (2018)\n",
      "BOT [37] (2019)\n",
      "DSA [57] (2019)\n",
      "Auto [41] (2019)\n",
      "ABDNet [3] (2019)\n",
      "SCAL [2] (2019)\n",
      "MHN [1] (2019)\n",
      "BDB [9] (2019)\n",
      "SONA [54] (2019)\n",
      "GCP [39] (2020)\n",
      "SAN [24] (2020)\n",
      "ISP [63] (2020)\n",
      "GASM [20] (2020)\n",
      "ESNET [44] (2020)\n",
      "LUP [12](2020)\n",
      "Ours+MGN\n",
      "Ours+BDB\n",
      "\n",
      "CUHK03 Market1501 DukeMTMC MSMT17\n",
      "63.7/85.1\n",
      "70.5/71.2\n",
      "-\n",
      "-\n",
      "-\n",
      "75.2/78.9\n",
      "52.5/78.2\n",
      "73.0/77.9\n",
      "60.8/82.3\n",
      "-\n",
      "-\n",
      "72.3/74.8\n",
      "-\n",
      "72.4/77.2\n",
      "-\n",
      "76.7/79.4\n",
      "-\n",
      "79.2/81.8\n",
      "-\n",
      "75.6/77.9\n",
      "55.7/79.2\n",
      "76.4/80.1\n",
      "-\n",
      "74.1/76.5\n",
      "52.5/79.5\n",
      "-\n",
      "57.3/80.5\n",
      "-\n",
      "65.7/85.5\n",
      "79.6/81.9*\n",
      "68.0/86.0\n",
      "80.4/80.9\n",
      "82.3/84.7\n",
      "53.4/79.0\n",
      "\n",
      "79.4/89.0\n",
      "76.4/86.4\n",
      "74.3/86.2\n",
      "-\n",
      "78.6/89.0\n",
      "79.6/89.0\n",
      "77.2/89.1\n",
      "76.0/89.0\n",
      "78.3/89.4\n",
      "78.6/87.9\n",
      "75.5/87.9\n",
      "80.0/89.6\n",
      "74.4/88.3\n",
      "78.7/88.5\n",
      "82.1/91.0\n",
      "84.3/92.0\n",
      "79.0/89.2\n",
      "\n",
      "87.5/95.1\n",
      "85.9/94.5\n",
      "87.6/95.7\n",
      "85.1/94.5\n",
      "88.3/95.6\n",
      "89.3/95.8\n",
      "85.0/95.1\n",
      "86.7/95.3\n",
      "88.8/95.6\n",
      "88.9/95.2\n",
      "88.0/96.1\n",
      "88.6/95.3\n",
      "84.7/95.3\n",
      "88.6/95.7\n",
      "91.0/96.4\n",
      "91.9/96.6\n",
      "88.4/95.4\n",
      "\n",
      "Table 8. Comparison with the state of the art. Numbers of MGN†\n",
      "come from a re-implementation based on FastReID, which are\n",
      "even better than the original. Numbers of PNL marked by * are ob-\n",
      "tained on BDB, the rest without the * mark are obtained on MGN.\n",
      "We show best scores in bold and the second scores underlined.\n",
      "\n",
      "5.8. Comparison with State-of-the-Art Methods\n",
      "\n",
      "We compare our results with current state-of-the-art\n",
      "methods on four public benchmarks. We don’t apply any\n",
      "post-processing techniques such as IIA [13] and RR [62].\n",
      "To ensure fairness, we adopt ResNet50 as our backbone and\n",
      "does not compare with methods that rely on stronger back-\n",
      "bones (results with stronger backbones e.g. ResNet101 can\n",
      "be found at supplementary materials). Results in Table 8\n",
      "verify the remarkable advantage brought by our pre-trained\n",
      "models. Without bells and whistles, we achieve state-of-\n",
      "the-art performance on all four benchmarks, outperforming\n",
      "the second with clear margins.\n",
      "\n",
      "6. Conclusion\n",
      "\n",
      "In this paper, we demonstrate that large-scale Re-ID\n",
      "representation can be directly learned from massive raw\n",
      "videos by leveraging the spatial and temporal information.\n",
      "We not only build a large-scale noisy labeled person Re-\n",
      "ID dataset LUPerson-NL based on tracklets of raw videos\n",
      "from LUPerson without using manual annotations, but also\n",
      "design a novel weakly supervised pretraining framework\n",
      "PNL comprising different learning modules including su-\n",
      "pervised learning, prototypes-based learning, label-guided\n",
      "contrastive learning and label rectification. Equipped with\n",
      "our pre-trained models, we push existing benchmark results\n",
      "to a new limit, which outperforms unsupervised pre-trained\n",
      "models and ImageNet supervised pre-trained models by a\n",
      "\n",
      "(a) Correction for Noise-I\n",
      "\n",
      "(b) Correction for Noise-II\n",
      "\n",
      "Figure 5. Visualizing the label correction functionality of our PNL\n",
      "framework with respect to the two noise types from LUPerson-\n",
      "NL. Person images in the same rectangle indicate that they are\n",
      "recognized as the same identity. The right-hand similarity matrices\n",
      "are calculated based on the image features all learned using our\n",
      "PNL framework with label correction.\n",
      "\n",
      "both prototype-based contrastive learning and the label cor-\n",
      "rection, are very important under various settings, as veri-\n",
      "fied by comparing row 1 with row 4; row 3 with row 6; and\n",
      "row 5 with row 7. iv) Our label-guided contrastive learning\n",
      "component consistently outperforms the vanilla instance-\n",
      "wise contrastive learning under various settings, as verified\n",
      "by comparing row 3 with row 5, and row 6 with row 7. v)\n",
      "Combining all our components (supervised classification,\n",
      "prototypes and label-guided contrastive learning) together\n",
      "leads to the best performance as shown by row 7.\n",
      "5.7. Label Correction\n",
      "\n",
      "Our PNL can indeed correct noisy labels. We demon-\n",
      "strate two typical examples in Figure 5 visualizing the la-\n",
      "bel corrections with respect to the two kinds of noises. As\n",
      "we can see, in Figure 5a the same person are marked as\n",
      "three different persons in LUPerson-NL due to Noise-I\n",
      "in labels. After our PNL pre-training, these three track-\n",
      "lets are merged together since their trained features are very\n",
      "close, as verified by the right-hand similarity matrix. In Fig-\n",
      "ure 5b different persons are labeled as the same identity\n",
      "in LUPerson-NL due to Noise-II in labels. After PNL\n",
      "training, these mis-labeled person identities are all correctly\n",
      "re-grouped into two identities, which can also be reflected\n",
      "by the right-hand similarity matrix.\n",
      "\n",
      "we also ablate the label correction module in Table 7\n",
      "with different settings, and observe it can improve the per-\n",
      "It also validates the importance of combining\n",
      "formance.\n",
      "label rectification with label-guided contrastive learning to-\n",
      "gether, where more accurate positive/negative pairs can be\n",
      "leveraged.\n",
      "\n",
      "BeforeAfter2103876954Similarity MatrixFinal ID=179919BeforeAfter2103876954Similarity MatrixFinal ID=419043Final ID=73264\f",
      "large margin.\n",
      "Acknowledgement. This work is partially supported by\n",
      "the National Natural Science Foundation of China (NSFC,\n",
      "61836011).\n",
      "\n",
      "References\n",
      "\n",
      "[1] Binghui Chen, Weihong Deng, and Jiani Hu. Mixed high-\n",
      "order attention network for person re-identification. In Pro-\n",
      "ceedings of the IEEE International Conference on Computer\n",
      "Vision, pages 371–381, 2019. 8\n",
      "\n",
      "[2] Guangyi Chen, Chunze Lin, Liangliang Ren, Jiwen Lu, and\n",
      "Self-critical attention learning for person re-\n",
      "Jie Zhou.\n",
      "In Proceedings of the IEEE International\n",
      "identification.\n",
      "Conference on Computer Vision, pages 9637–9646, 2019. 8\n",
      "[3] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang\n",
      "Chen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-\n",
      "In Pro-\n",
      "net: Attentive but diverse person re-identification.\n",
      "ceedings of the IEEE International Conference on Computer\n",
      "Vision, pages 8351–8361, 2019. 8\n",
      "\n",
      "[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\n",
      "offrey Hinton. A simple framework for contrastive learning\n",
      "of visual representations. arXiv preprint arXiv:2002.05709,\n",
      "2020. 2, 4, 5\n",
      "\n",
      "[5] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\n",
      "Norouzi, and Geoffrey Hinton. Big self-supervised mod-\n",
      "arXiv preprint\n",
      "els are strong semi-supervised learners.\n",
      "arXiv:2006.10029, 2020. 2, 4, 5\n",
      "\n",
      "[6] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi\n",
      "Huang. Beyond triplet loss: a deep quadruplet network for\n",
      "In Proceedings of the IEEE con-\n",
      "person re-identification.\n",
      "ference on computer vision and pattern recognition, pages\n",
      "403–412, 2017. 2\n",
      "\n",
      "[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\n",
      "Improved baselines with momentum contrastive learning.\n",
      "arXiv preprint arXiv:2003.04297, 2020. 2, 4, 5\n",
      "\n",
      "[8] Zhirui Chen, Jianheng Li, and Wei-Shi Zheng. Weakly su-\n",
      "pervised tracklet person re-identification by deep feature-\n",
      "arXiv preprint arXiv:1910.14333,\n",
      "wise mutual learning.\n",
      "2019. 3\n",
      "\n",
      "[9] Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu,\n",
      "and Ping Tan. Batch dropblock network for person re-\n",
      "identification and beyond. In Proceedings of the IEEE Inter-\n",
      "national Conference on Computer Vision, pages 3691–3701,\n",
      "2019. 8\n",
      "\n",
      "[10] Piotr Doll´ar, Ron Appel, Serge Belongie, and Pietro Per-\n",
      "IEEE\n",
      "ona. Fast feature pyramids for object detection.\n",
      "transactions on pattern analysis and machine intelligence,\n",
      "36(8):1532–1545, 2014. 4\n",
      "\n",
      "[11] Pedro F Felzenszwalb, Ross B Girshick, David McAllester,\n",
      "and Deva Ramanan. Object detection with discriminatively\n",
      "IEEE transactions on pattern\n",
      "trained part-based models.\n",
      "analysis and machine intelligence, 32(9):1627–1645, 2009.\n",
      "4\n",
      "\n",
      "ings of the IEEE conference on computer vision and pattern\n",
      "recognition, 2021. 1, 2, 3, 4, 6, 7, 8\n",
      "\n",
      "[13] Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen,\n",
      "Jianmin Bao, Gang Hua, and Houqiang Li. Improving person\n",
      "re-identification with iterative impression aggregation. IEEE\n",
      "Transactions on Image Processing, 29:9559–9571, 2020. 8\n",
      "[14] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-\n",
      "teaching: Pseudo label refinery for unsupervised domain\n",
      "adaptation on person re-identification. In International Con-\n",
      "ference on Learning Representations, 2019. 2\n",
      "\n",
      "[15] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hong-\n",
      "sheng Li. Self-paced contrastive learning with hybrid mem-\n",
      "ory for domain adaptive object re-id. In Advances in Neural\n",
      "Information Processing Systems, 2020. 2, 7\n",
      "\n",
      "[16] Douglas Gray and Hai Tao. Viewpoint invariant pedes-\n",
      "trian recognition with an ensemble of localized features. In\n",
      "European conference on computer vision, pages 262–275.\n",
      "Springer, 2008. 4\n",
      "\n",
      "[17] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\n",
      "Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\n",
      "ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\n",
      "mad Gheshlaghi Azar, et al. Bootstrap your own latent: A\n",
      "new approach to self-supervised learning. arXiv preprint\n",
      "arXiv:2006.07733, 2020. 2, 4, 5\n",
      "\n",
      "[18] Xinqian Gu, Bingpeng Ma, Hong Chang, Shiguang Shan,\n",
      "and Xilin Chen. Temporal knowledge propagation for image-\n",
      "In Proceedings of the\n",
      "to-video person re-identification.\n",
      "IEEE/CVF International Conference on Computer Vision,\n",
      "pages 9647–9656, 2019. 2\n",
      "\n",
      "[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\n",
      "Girshick. Momentum contrast for unsupervised visual rep-\n",
      "resentation learning. In Proceedings of the IEEE/CVF Con-\n",
      "ference on Computer Vision and Pattern Recognition, pages\n",
      "9729–9738, 2020. 2, 4, 5\n",
      "\n",
      "[20] Lingxiao He and Wu Liu. Guided saliency feature learning\n",
      "for person re-identification in crowded scenes. In European\n",
      "Conference on Computer Vision, pages 357–373. Springer,\n",
      "2020. 8\n",
      "\n",
      "[21] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-\n",
      "fense of the triplet loss for person re-identification. arXiv\n",
      "preprint arXiv:1703.07737, 2017. 2, 6\n",
      "\n",
      "[22] Yan Huang, Qiang Wu, JingSong Xu, and Yi Zhong. Sbsgan:\n",
      "Suppression of inter-domain background shift for person re-\n",
      "identification. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 9527–9536, 2019. 2\n",
      "[23] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li\n",
      "Zhang. Style normalization and restitution for generalizable\n",
      "In Proceedings of the IEEE/CVF\n",
      "person re-identification.\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pages 3143–3152, 2020. 2\n",
      "\n",
      "[24] Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, and\n",
      "Zhibo Chen. Semantics-aligned representation learning for\n",
      "person re-identification. In AAAI, pages 11173–11180, 2020.\n",
      "8\n",
      "\n",
      "[12] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu\n",
      "Yuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsu-\n",
      "pervised pre-training for person re-identification. Proceed-\n",
      "\n",
      "[25] Srikrishna Karanam, Mengran Gou, Ziyan Wu, Angels\n",
      "Rates-Borras, Octavia Camps, and Richard J Radke. A\n",
      "comprehensive evaluation and benchmark for person re-\n",
      "\n",
      "\f",
      "identification: Features, metrics, and datasets. arXiv preprint\n",
      "arXiv:1605.09653, 2(3):5, 2016. 1, 4\n",
      "\n",
      "[26] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\n",
      "Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\n",
      "Dilip Krishnan. Supervised contrastive learning. Advances\n",
      "in Neural Information Processing Systems, 33, 2020. 7\n",
      "[27] Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, and Shiliang\n",
      "Zhang. Global-local temporal representations for video per-\n",
      "son re-identification. In Proceedings of the IEEE/CVF Inter-\n",
      "national Conference on Computer Vision, pages 3958–3967,\n",
      "2019. 2\n",
      "\n",
      "[28] Junnan Li, Caiming Xiong, and Steven Hoi. Mopro: Webly\n",
      "supervised learning with momentum prototypes. In Interna-\n",
      "tional Conference on Learning Representations, 2020. 2, 4\n",
      "[29] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsuper-\n",
      "vised tracklet person re-identification. IEEE transactions on\n",
      "pattern analysis and machine intelligence, 42(7):1770–1782,\n",
      "2019. 3\n",
      "\n",
      "[30] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-\n",
      "reid: Deep filter pairing neural network for person re-\n",
      "In Proceedings of the IEEE conference on\n",
      "identification.\n",
      "computer vision and pattern recognition, pages 152–159,\n",
      "2014. 4\n",
      "\n",
      "[31] Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, and\n",
      "Yu-Chiang Frank Wang. Recover and identify: A generative\n",
      "dual model for cross-resolution person re-identification. In\n",
      "Proceedings of the IEEE/CVF International Conference on\n",
      "Computer Vision, pages 8090–8099, 2019. 2\n",
      "\n",
      "[32] Yu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank\n",
      "Wang. Cross-dataset person re-identification via unsuper-\n",
      "vised pose disentanglement and adaptation. In Proceedings\n",
      "of the IEEE/CVF International Conference on Computer Vi-\n",
      "sion, pages 7919–7929, 2019. 2\n",
      "\n",
      "[33] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi\n",
      "Yang. A bottom-up clustering approach to unsupervised per-\n",
      "In Proceedings of the AAAI Confer-\n",
      "son re-identification.\n",
      "ence on Artificial Intelligence, volume 33, pages 8738–8745,\n",
      "2019. 2\n",
      "\n",
      "[34] Fangyi Liu and Lei Zhang. View confusion feature learning\n",
      "for person re-identification. In Proceedings of the IEEE/CVF\n",
      "International Conference on Computer Vision, pages 6639–\n",
      "6648, 2019. 2\n",
      "\n",
      "[35] Chen Change Loy, Chunxiao Liu, and Shaogang Gong. Per-\n",
      "son re-identification by manifold ranking. In 2013 IEEE In-\n",
      "ternational Conference on Image Processing, pages 3567–\n",
      "3571. IEEE, 2013. 4\n",
      "\n",
      "[36] Chuanchen Luo, Yuntao Chen, Naiyan Wang, and Zhaoxi-\n",
      "ang Zhang. Spectral feature transformation for person re-\n",
      "identification. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 4976–4985, 2019. 2\n",
      "[37] Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao,\n",
      "Shenqi Lai, and Jianyang Gu. A strong baseline and batch\n",
      "normalization neck for deep person re-identification. IEEE\n",
      "Transactions on Multimedia, 2019. 8\n",
      "\n",
      "[38] Jingke Meng, Sheng Wu, and Wei-Shi Zheng. Weakly su-\n",
      "In Proceedings of the\n",
      "pervised person re-identification.\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 760–769, 2019. 3\n",
      "\n",
      "[39] Hyunjong Park and Bumsub Ham. Relation network for per-\n",
      "son re-identification. In Proceedings of the AAAI Conference\n",
      "on Artificial Intelligence, volume 34, pages 11839–11847,\n",
      "2020. 8\n",
      "\n",
      "[40] Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie\n",
      "Qiu, Yang Wu, Yu-Gang Jiang, and Xiangyang Xue. Pose-\n",
      "normalized image generation for person re-identification. In\n",
      "Proceedings of the European conference on computer vision\n",
      "(ECCV), pages 650–667, 2018. 2\n",
      "\n",
      "[41] Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, and Yi\n",
      "Yang. Auto-reid: Searching for a part-aware convnet for\n",
      "person re-identification. In Proceedings of the IEEE Inter-\n",
      "national Conference on Computer Vision, pages 3750–3759,\n",
      "2019. 8\n",
      "\n",
      "[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n",
      "Faster r-cnn: Towards real-time object detection with region\n",
      "proposal networks. In Advances in neural information pro-\n",
      "cessing systems, pages 91–99, 2015. 4\n",
      "\n",
      "[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\n",
      "jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\n",
      "Aditya Khosla, Michael Bernstein, et al.\n",
      "Imagenet large\n",
      "scale visual recognition challenge. International journal of\n",
      "computer vision, 115(3):211–252, 2015. 6\n",
      "\n",
      "[44] Dong Shen, Shuai Zhao, Jinming Hu, Hao Feng, Deng Cai,\n",
      "and Xiaofei He. Es-net: Erasing salient parts to learn more\n",
      "in re-identification. IEEE Transactions on Image Processing,\n",
      "2020. 8\n",
      "\n",
      "[45] Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen,\n",
      "and Xiaogang Wang. Person re-identification with deep\n",
      "similarity-guided graph neural network. In Proceedings of\n",
      "the European conference on computer vision (ECCV), pages\n",
      "486–504, 2018. 2\n",
      "\n",
      "[46] Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Ky-\n",
      "oung Mu Lee. Part-aligned bilinear representations for per-\n",
      "son re-identification. In Proceedings of the European Con-\n",
      "ference on Computer Vision (ECCV), pages 402–419, 2018.\n",
      "2\n",
      "\n",
      "[47] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\n",
      "high-resolution representation learning for human pose esti-\n",
      "mation. In CVPR, 2019. 3\n",
      "\n",
      "[48] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\n",
      "Wang. Beyond part models: Person retrieval with refined\n",
      "part pooling (and a strong convolutional baseline). In ECCV,\n",
      "2018. 2\n",
      "\n",
      "[49] Dongkai Wang and Shiliang Zhang. Unsupervised person re-\n",
      "identification via multi-label classification. In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 10981–10990, 2020. 2\n",
      "\n",
      "[50] Guangrun Wang, Guangcong Wang, Xujie Zhang, Jianhuang\n",
      "Lai, Zhengtao Yu, and Liang Lin. Weakly supervised per-\n",
      "son re-id: Differentiable graphical learning and a new bench-\n",
      "mark. IEEE Transactions on Neural Networks and Learning\n",
      "Systems, 32(5):2142–2156, 2020. 3, 4\n",
      "\n",
      "[51] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\n",
      "Zhou. Learning discriminative features with multiple granu-\n",
      "larities for person re-identification. In 2018 ACM Multime-\n",
      "dia Conference on Multimedia Conference, pages 274–282.\n",
      "ACM, 2018. 1, 2, 6, 8\n",
      "\n",
      "\f",
      "[52] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\n",
      "Person transfer gan to bridge domain gap for person re-\n",
      "In Proceedings of the IEEE Conference on\n",
      "identification.\n",
      "Computer Vision and Pattern Recognition, pages 79–88,\n",
      "2018. 1, 4\n",
      "\n",
      "[53] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\n",
      "Unsupervised feature learning via non-parametric instance\n",
      "In Proceedings of the IEEE Conference\n",
      "discrimination.\n",
      "on Computer Vision and Pattern Recognition, pages 3733–\n",
      "3742, 2018. 2\n",
      "\n",
      "[54] Bryan Ning Xia, Yuan Gong, Yizhe Zhang, and Christian\n",
      "Poellabauer. Second-order non-local attention networks for\n",
      "person re-identification. In Proceedings of the IEEE Inter-\n",
      "national Conference on Computer Vision, pages 3760–3769,\n",
      "2019. 8\n",
      "\n",
      "[55] Ye Yuan, Wuyang Chen, Yang Yang, and Zhangyang Wang.\n",
      "In defense of the triplet loss again: Learning robust person\n",
      "re-identification with fast approximated triplet loss and label\n",
      "distillation. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition Workshops, pages\n",
      "354–355, 2020. 2\n",
      "\n",
      "[56] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\n",
      "and Wenyu Liu. Fairmot: On the fairness of detection and\n",
      "re-identification in multiple object tracking. arXiv e-prints,\n",
      "pages arXiv–2004, 2020. 2, 3, 4\n",
      "\n",
      "[57] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo\n",
      "Chen. Densely semantically aligned person re-identification.\n",
      "In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pages 667–676, 2019. 8\n",
      "\n",
      "[58] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\n",
      "dong Wang, and Qi Tian. Scalable person re-identification:\n",
      "A benchmark. 2015 IEEE International Conference on Com-\n",
      "puter Vision (ICCV), pages 1116–1124, 2015. 1, 4\n",
      "\n",
      "[59] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan\n",
      "Chandraker, Yi Yang, and Qi Tian. Person re-identification\n",
      "in the wild. In Proceedings of the IEEE Conference on Com-\n",
      "puter Vision and Pattern Recognition, pages 1367–1376,\n",
      "2017. 1, 2, 6\n",
      "\n",
      "[60] Zhedong Zheng, Liang Zheng, and Yi Yang. A discrimi-\n",
      "natively learned cnn embedding for person reidentification.\n",
      "ACM Transactions on Multimedia Computing, Communica-\n",
      "tions, and Applications (TOMM), 14(1):1–20, 2017. 2\n",
      "[61] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-\n",
      "ples generated by gan improve the person re-identification\n",
      "baseline in vitro. In Proceedings of the IEEE International\n",
      "Conference on Computer Vision, 2017. 1, 4\n",
      "\n",
      "[62] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-\n",
      "ranking person re-identification with k-reciprocal encoding.\n",
      "In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pages 1318–1327, 2017. 6, 8\n",
      "[63] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao\n",
      "Wang. Identity-guided human semantic parsing for person\n",
      "re-identification. ECCV, 2020. 8\n",
      "\n",
      "\f",
      "In this material, we will 1) show demo cases for scene\n",
      "changing for specific person in LUPerson-NL, 2) share the\n",
      "training details for PNL, 3) provide the detailed algorithm\n",
      "table for PNL, 4) compare models pre-trained on SYSU30K\n",
      "and LUPerson-NL 5) analyze the hyper parameters of our\n",
      "PNL, 6) demonstrate results of our method using a stronger\n",
      "backbones, 7) explore the impact of pre-training image\n",
      "scale, and 8) list more detailed results for our “small-scale”\n",
      "and “few-shot” experiments. All these experiments are un-\n",
      "der MGN settings.\n",
      "\n",
      "A. Scene changing in LUPerson-NL\n",
      "\n",
      "Our LUPerson-NL are driven from street view videos,\n",
      "Figure 6 shows a demo person in our LUPerson-NL. As we\n",
      "can see, our LUPerson-NL is able to cover multiple scenes,\n",
      "since the videos we use have lots of moving cameras and\n",
      "moving persons, and only traklets with ≥ 200 frames are\n",
      "selected. It is approximate close to the real person Re-ID\n",
      "scenario.\n",
      "\n",
      "C. Algorithm for PNL\n",
      "\n",
      "Algorithm 1 shows the procedure of training PNL, we\n",
      "train our framework for 90 epochs, and apply label correc-\n",
      "tion from 10 epochs. Once the rectification begins, we keep\n",
      "rectifying for every iteration.\n",
      "\n",
      "Algorithm 1: PNL algorithm.\n",
      "\n",
      "Input: total epochs N , correction start epochs Ns,\n",
      "prototype feature vectors {⃗c1, ⃗c2, . . . , ⃗cK}, where\n",
      "K is the number of identities, temperature τ ,\n",
      "threshold T , momentum m, encoder network\n",
      "Eq(·), classifier h(·), momentum encoder Ek(·),\n",
      "loss weight λpro and λlgc.\n",
      "for epoch = 1 : N do\n",
      "\n",
      "{(⃗xi, yi)}b\n",
      "for i ∈ {1, ..., b} do\n",
      "\n",
      "i=1 sampled from data loader.\n",
      "\n",
      "˜⃗xi = aug1(⃗xi), ˜⃗x′\n",
      "i = aug2(⃗xi)\n",
      "⃗qi = Eq(˜⃗xi), ⃗ki = Ek(˜⃗x′\n",
      "i)\n",
      "⃗pi = h(⃗qi)\n",
      "k=1, sk\n",
      "i }K\n",
      "⃗si = {sk\n",
      "⃗li = (⃗pi + ⃗si)/2\n",
      "if epoch > Ns and maxk lj\n",
      "\n",
      "(cid:80)K\n",
      "\n",
      "i = exp( ⃗qi·⃗ck/τ )\n",
      "\n",
      "k=1 exp(⃗qi·⃗ck/τ )\n",
      "\n",
      "i > T then\n",
      "\n",
      "ˆyi = arg maxj\n",
      "\n",
      "⃗lj\n",
      "i\n",
      "\n",
      "else\n",
      "\n",
      "ˆyi = ⃗yi\n",
      "\n",
      "Li\n",
      "Li\n",
      "Li\n",
      "\n",
      "ce = − log(⃗pi[ˆyi])\n",
      "pro = −log\n",
      "lgc =\n",
      "\n",
      "(cid:80)K\n",
      "\n",
      "exp(⃗qi·⃗c ˆyi /τ )\n",
      "j=1 exp(⃗qi·⃗cj /τ )\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "⃗qi·⃗k+\n",
      "τ\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:80)\n",
      "X\n",
      "⃗qi·⃗k+\n",
      "τ\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "−1\n",
      "|P(i)| log\n",
      "\n",
      "⃗qi·⃗k−\n",
      "τ\n",
      "\n",
      "exp\n",
      "\n",
      "+(cid:80)\n",
      "exp\n",
      "⃗k−∈N (i)\n",
      "\n",
      "(cid:80)\n",
      "⃗k+ ∈P(i)\n",
      "⃗cˆyi = m⃗cˆyi + (1 − m)⃗qi\n",
      "ce + λproLi\n",
      "Li = Li\n",
      "update networks Eq, h to minimize Li\n",
      "update networks Eq, h to minimize Li\n",
      "update momentum encoder Ek.\n",
      "\n",
      "pro + λlgcLi\n",
      "\n",
      "lgc\n",
      "\n",
      "D. Compare with SYSU30K\n",
      "\n",
      "As show in Table 9, we compare the pre-trained mod-\n",
      "els between LUPerson-NL and SYSU30K. We pre-train\n",
      "PNL on both LUPerson-NL and SYSU30K for 40 epochs\n",
      "with same experiment settings for a fair comparison. The\n",
      "performance of LUPerson-NL pre-training is much better\n",
      "than SYSU30K pre-training, showing the superiority of our\n",
      "LUPerson-NL, and also suggesting that a large number of\n",
      "images with limited diversity does not bring more represen-\n",
      "tative representation, but large-scale images with diversity\n",
      "does.\n",
      "\n",
      "Figure 6. Scene changing for a specific person in LUPerson-NL.\n",
      "\n",
      "B. Training details\n",
      "\n",
      "During training, all\n",
      "\n",
      "images are resized to 256 ×\n",
      "128 and pass through the same augmentations veri-\n",
      "fied in [12], which are Random Resized Crop, Hor-\n",
      "izontal Flip, Normalization, Random Gaussian Blur,\n",
      "Specifi-\n",
      "Random Gray Scale and Random Erasing.\n",
      "cally, the images are normalized with mean and std of\n",
      "[0.3452, 0.3070, 0.3114], [0.2633, 0.2500, 0.2480], which\n",
      "are calculated from all images in LUPerson-NL. We train\n",
      "our model on 8 × V 100 GPUs for 90 epochs with a batch\n",
      "size of 1, 536. The initial learning rate is set to 0.4 with\n",
      "a step-wise decay by 0.1 for every 40 epochs. The opti-\n",
      "mizer is SGD with a momentum of 0.9 and a weight de-\n",
      "cay of 0.0001. We set the hyper-parameters τ = 0.1 and\n",
      "T = 0.8. The momentum m for updating both the momen-\n",
      "tum encoder Ek and the prototypes is set to 0.999. We de-\n",
      "sign a large queue with a size of 65, 536 to increase the oc-\n",
      "currence of positive samples for the label guided contrastive\n",
      "learning. The label correction according to Equation 3 starts\n",
      "from the 10-th epoch. The deployment of the label guided\n",
      "contrastive loss Li\n",
      "\n",
      "lgc starts from the 15-th epoch.\n",
      "\n",
      "\f",
      "E.1. Temperature Factor τ\n",
      "\n",
      "Table 13. Comparison for different pre-training data scale\n",
      "\n",
      "Dataset\n",
      "MSMT17\n",
      "\n",
      "LUPerson-NL\n",
      "66.1/84.6\n",
      "\n",
      "SYSU30K\n",
      "55.2/76.7\n",
      "\n",
      "Table 9. Comparison of applying our PNL on both LUPerson-NL\n",
      "and SYSU30K.\n",
      "\n",
      "τ\n",
      "\n",
      "0.05\n",
      "0.07\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "\n",
      "DukeMTMC\n",
      "\n",
      "MSMT17\n",
      "\n",
      "40%\n",
      "76.1/87.2\n",
      "76.4/87.5\n",
      "77.0/87.3\n",
      "75.8/86.8\n",
      "74.7/86.2\n",
      "\n",
      "100%\n",
      "83.5/91.4\n",
      "83.6/91.4\n",
      "84.3/92.0\n",
      "83.4/91.0\n",
      "82.7/90.6\n",
      "\n",
      "40%\n",
      "49.8/73.3\n",
      "50.7/74.1\n",
      "51.9/74.9\n",
      "50.1/73.7\n",
      "48.6/72.0\n",
      "\n",
      "100%\n",
      "65.4/83.8\n",
      "67.2/85.3\n",
      "68.0/86.0\n",
      "66.4/85.0\n",
      "65.5/83.7\n",
      "\n",
      "Table 10. Performances under different τ values on DukeMTMC\n",
      "and MSMT17 with data percentages 40% and 100% under the\n",
      "small scale setting. The threshold is set as T = 0.8. The best\n",
      "scores are in bold and the second ones are underlined.\n",
      "\n",
      "E. Hyper Parameter Analysis\n",
      "\n",
      "In our PNL, there are two key hyper-parameters: tem-\n",
      "perature factor τ and the threshold for correction T . Here,\n",
      "we provide the analysis of these two parameters.\n",
      "\n",
      "Table 10 shows the performance comparison with differ-\n",
      "ent τ values with a fixed label correction threshold T = 0.8.\n",
      "As we can see, the setting τ = 0.1 achieves the best re-\n",
      "sults on both DukeMTMC and MSMT17, while τ = 0.07\n",
      "achieves the second best. When we use a larger τ , the\n",
      "performance drops rapidly. It may be because Re-ID is a\n",
      "more fine-grained task, larger τ will cause smaller inter-\n",
      "class variations and make positive samples too close to neg-\n",
      "ative samples. In all the experiments, we set τ = 0.1.\n",
      "\n",
      "E.2. Threshold T\n",
      "\n",
      "Table 11 shows the results with different label correc-\n",
      "tion threshold values T . As we can see, the performance\n",
      "is relatively stable to different T values varying in a large\n",
      "range of 0.6 ∼ 0.8. However, if T is too small or too large,\n",
      "the performance drops rapidly. For the former case, labels\n",
      "are easier to be modified, which may cause wrong rectifi-\n",
      "cations, while for the latter case, the label noises become\n",
      "harder to be corrected, which also has consistently negative\n",
      "effects on the performance. In all the experiments, we set\n",
      "T = 0.8.\n",
      "\n",
      "F. Results for stronger backbones\n",
      "\n",
      "We train our PNL using two stronger backbones\n",
      "ResNet101 and ResNet152, and report the results in Ta-\n",
      "ble 12. As we can see, the stronger ResNet bring more\n",
      "superior performances. These results also outperform the\n",
      "\n",
      "T\n",
      "\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "\n",
      "DukeMTMC\n",
      "\n",
      "MSMT17\n",
      "\n",
      "40%\n",
      "76.1/86.5\n",
      "77.1/87.7\n",
      "77.0/87.5\n",
      "77.0/87.3\n",
      "75.7/86.4\n",
      "\n",
      "100%\n",
      "83.3/91.0\n",
      "84.1/91.6\n",
      "84.0/91.8\n",
      "84.3/92.0\n",
      "83.0/90.8\n",
      "\n",
      "40%\n",
      "51.1/74.6\n",
      "52.3/75.5\n",
      "51.9/75.6\n",
      "51.9/75.0\n",
      "50.9/74.3\n",
      "\n",
      "100%\n",
      "67.5/85.2\n",
      "68.1/85.7\n",
      "68.2/85.8\n",
      "68.0/86.0\n",
      "67.2/85.0\n",
      "\n",
      "Table 11. Performances under different T values on DukeMTMC\n",
      "and MSMT17 with data percentages 40% and 100% under the\n",
      "small scale setting. The temperature factor is set as τ = 0.1. The\n",
      "best scores are in bold and the second ones are underlined.\n",
      "\n",
      "Arch CUHK03 Market1501 DukeMTMC MSMT17\n",
      "68.0/86.0\n",
      "R50\n",
      "80.4/80.9\n",
      "70.8/87.1\n",
      "R101 80.5/81.2\n",
      "71.6/87.5\n",
      "R152 80.6/81.2\n",
      "\n",
      "84.3/92.0\n",
      "85.5/92.8\n",
      "85.6/92.4\n",
      "\n",
      "91.9/96.6\n",
      "92.5/96.9\n",
      "92.7/96.8\n",
      "\n",
      "Table 12. Results with different ResNet backbones. R50, R101\n",
      "and R152 stand for ResNet50, ResNet101 and ResNet152 respec-\n",
      "tively.\n",
      "\n",
      "Scale\n",
      "MSMT17\n",
      "\n",
      "10%\n",
      "57.4/79.2\n",
      "\n",
      "30%\n",
      "62.2/82.1\n",
      "\n",
      "100%\n",
      "68.0/86.0\n",
      "\n",
      "scores reported in Table 8 of our main submission. Most\n",
      "importantly, we are the FIRST to obtain a mAP score on\n",
      "MSMT17 that is larger than 70 without any post-processing\n",
      "for convolutional network.\n",
      "\n",
      "G. Pre-training data scales\n",
      "\n",
      "We study the impact of pre-training data scale. Specif-\n",
      "ically, we involve various percentages (10%,30%,100%\n",
      "pseudo based) of LUPerson-NL into pre-training and then\n",
      "evaluate the finetuing performance on the target datasets.\n",
      "As shown in Table 13, the learned representation is much\n",
      "stronger with the increase of the pre-training data scale, in-\n",
      "dicating the necessity of building large-scale dataset, and\n",
      "our LUPerson-NL is very important.\n",
      "\n",
      "H. More results for small-scale and few-shot\n",
      "\n",
      "To complement Table 3 in the main text, we provide\n",
      "more detailed results under “small scale” and “few shot”\n",
      "settings in Table 14. As we can see, our weakly pre-trained\n",
      "models are consistently better than other pre-trained mod-\n",
      "els. Our advantage is much larger with less training data,\n",
      "suggesting the potential practical value of our pre-trained\n",
      "models for real-world person ReID applications.\n",
      "\n",
      "\f",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "10%\n",
      "75\n",
      "1,170\n",
      "53.1/76.9\n",
      "58.4/81.7\n",
      "64.6/85.5\n",
      "72.4/88.8\n",
      "\n",
      "10%\n",
      "751\n",
      "1,293\n",
      "21.1/41.8\n",
      "18.6/36.1\n",
      "26.4/47.5\n",
      "42.0/61.6\n",
      "\n",
      "10%\n",
      "70\n",
      "1,670\n",
      "45.1/65.3\n",
      "48.1/66.9\n",
      "53.5/72.0\n",
      "60.6/75.8\n",
      "\n",
      "10%\n",
      "702\n",
      "1,679\n",
      "31.5/47.1\n",
      "32.4/48.0\n",
      "35.8/50.2\n",
      "52.2/64.1\n",
      "\n",
      "10%\n",
      "104\n",
      "3,659\n",
      "23.2/50.2\n",
      "22.6/48.8\n",
      "25.5/51.1\n",
      "28.2/51.1\n",
      "\n",
      "10%\n",
      "1,041\n",
      "3,262\n",
      "14.7/34.1\n",
      "13.2/29.2\n",
      "17.0/36.0\n",
      "24.5/42.7\n",
      "\n",
      "20%\n",
      "150\n",
      "2,643\n",
      "67.7/86.8\n",
      "70.2/89.1\n",
      "76.9/92.1\n",
      "81.7/93.2\n",
      "\n",
      "20%\n",
      "751\n",
      "2,587\n",
      "53.2/75.1\n",
      "56.5/77.5\n",
      "63.5/83.0\n",
      "75.7/89.1\n",
      "\n",
      "20%\n",
      "140\n",
      "3,192\n",
      "58.3/75.4\n",
      "60.6/76.6\n",
      "65.0/78.9\n",
      "70.5/83.3\n",
      "\n",
      "20%\n",
      "702\n",
      "3,321\n",
      "56.2/72.1\n",
      "56.4/72.2\n",
      "61.0/74.9\n",
      "71.7/82.5\n",
      "\n",
      "20%\n",
      "208\n",
      "6,471\n",
      "34.6/64.0\n",
      "32.7/60.9\n",
      "36.0/62.6\n",
      "39.6/63.7\n",
      "\n",
      "20%\n",
      "1,041\n",
      "6,524\n",
      "35.6/61.4\n",
      "33.5/58.6\n",
      "37.4/61.4\n",
      "45.6/67.2\n",
      "\n",
      "30%\n",
      "225\n",
      "3,962\n",
      "75.2/90.8\n",
      "76.6/91.9\n",
      "81.9/93.7\n",
      "85.2/94.2\n",
      "\n",
      "30%\n",
      "751\n",
      "3,880\n",
      "68.1/87.6\n",
      "69.3/87.8\n",
      "78.3/92.1\n",
      "83.7/94.0\n",
      "\n",
      "30%\n",
      "210\n",
      "5,530\n",
      "64.7/80.2\n",
      "65.8/80.2\n",
      "69.4/81.9\n",
      "74.5/86.3\n",
      "\n",
      "30%\n",
      "702\n",
      "4,938\n",
      "65.4/79.8\n",
      "65.3/80.2\n",
      "72.3/83.8\n",
      "77.7/87.9\n",
      "\n",
      "30%\n",
      "312\n",
      "9,787\n",
      "41.9/70.8\n",
      "40.4/68.7\n",
      "44.6/71.4\n",
      "47.7/71.2\n",
      "\n",
      "30%\n",
      "1,041\n",
      "9,786\n",
      "44.5/71.1\n",
      "41.4/67.1\n",
      "49.0/73.6\n",
      "53.2/74.4\n",
      "\n",
      "40%\n",
      "300\n",
      "5,226\n",
      "79.1/92.5\n",
      "80.0/93.0\n",
      "84.1/94.4\n",
      "87.3/95.1\n",
      "\n",
      "50%\n",
      "375\n",
      "6,408\n",
      "81.5/93.5\n",
      "82.0/94.1\n",
      "85.8/94.9\n",
      "88.3/95.5\n",
      "\n",
      "60%\n",
      "450\n",
      "7,814\n",
      "81.5/93.5\n",
      "83.7/94.3\n",
      "87.8/95.8\n",
      "89.6/96.0\n",
      "\n",
      "(a) Market1501 small-scale\n",
      "40%\n",
      "751\n",
      "5,174\n",
      "75.4/90.4\n",
      "78.8/88.3\n",
      "80.3/92.7\n",
      "86.0/94.3\n",
      "\n",
      "50%\n",
      "751\n",
      "6,468\n",
      "80.2/92.8\n",
      "78.3/90.9\n",
      "84.2/93.9\n",
      "88.1/95.2\n",
      "\n",
      "60%\n",
      "751\n",
      "7,758\n",
      "83.0/93.6\n",
      "81.7/93.3\n",
      "86.7/94.7\n",
      "89.8/95.8\n",
      "\n",
      "(b) Market1501 few-shot\n",
      "\n",
      "40%\n",
      "280\n",
      "6,924\n",
      "68.5/83.0\n",
      "69.5/82.9\n",
      "72.8/84.7\n",
      "77.0/87.3\n",
      "\n",
      "50%\n",
      "351\n",
      "8,723\n",
      "71.8/84.6\n",
      "72.5/84.4\n",
      "75.6/86.7\n",
      "78.8/88.3\n",
      "\n",
      "60%\n",
      "421\n",
      "10,197\n",
      "74.1/85.6\n",
      "75.0/86.2\n",
      "77.6/87.1\n",
      "80.5/89.2\n",
      "\n",
      "(c) DukeMTMC small-scale\n",
      "40%\n",
      "702\n",
      "6,599\n",
      "71.0/83.9\n",
      "70.2/83.4\n",
      "75.2/86.8\n",
      "79.3/88.1\n",
      "\n",
      "50%\n",
      "702\n",
      "8,278\n",
      "73.9/85.7\n",
      "73.7/85.1\n",
      "77.7/87.4\n",
      "81.1/89.6\n",
      "\n",
      "60%\n",
      "702\n",
      "9,923\n",
      "75.8/86.6\n",
      "75.8/86.7\n",
      "79.4/88.4\n",
      "82.3/90.2\n",
      "\n",
      "(d) DukeMTMC few-shot\n",
      "40%\n",
      "416\n",
      "13,006\n",
      "46.7/74.5\n",
      "45.1/72.2\n",
      "49.2/74.9\n",
      "51.9/74.9\n",
      "\n",
      "50%\n",
      "520\n",
      "15,917\n",
      "50.3/76.9\n",
      "49.0/75.0\n",
      "53.0/77.7\n",
      "55.5/77.2\n",
      "\n",
      "60%\n",
      "624\n",
      "19,672\n",
      "53.9/79.4\n",
      "52.7/78.0\n",
      "56.4/79.7\n",
      "59.1/80.1\n",
      "\n",
      "(e) MSMT17 small-scale\n",
      "\n",
      "40%\n",
      "1,041\n",
      "13,048\n",
      "52.0/76.9\n",
      "47.7/72.7\n",
      "53.9/78.5\n",
      "58.6/78.8\n",
      "\n",
      "50%\n",
      "1,041\n",
      "16,310\n",
      "56.2/79.5\n",
      "53.3/77.6\n",
      "57.4/80.5\n",
      "62.2/81.0\n",
      "\n",
      "60%\n",
      "1,041\n",
      "19,572\n",
      "58.8/81.7\n",
      "56.5/79.6\n",
      "60.0/82.1\n",
      "64.1/82.6\n",
      "\n",
      "(f) MSMT17 few-shot\n",
      "\n",
      "70%\n",
      "525\n",
      "9,120\n",
      "84.8/94.5\n",
      "85.4/94.5\n",
      "88.8/95.9\n",
      "90.1/96.2\n",
      "\n",
      "70%\n",
      "751\n",
      "9,055\n",
      "84.2/94.0\n",
      "84.4/94.1\n",
      "88.4/95.5\n",
      "90.5/96.3\n",
      "\n",
      "70%\n",
      "491\n",
      "11,939\n",
      "75.5/86.8\n",
      "76.3/86.9\n",
      "78.9/88.2\n",
      "81.6/89.5\n",
      "\n",
      "70%\n",
      "702\n",
      "11,564\n",
      "77.2/87.8\n",
      "77.7/88.2\n",
      "80.8/89.2\n",
      "83.2/91.1\n",
      "\n",
      "70%\n",
      "728\n",
      "22,680\n",
      "56.9/81.2\n",
      "55.7/79.9\n",
      "59.5/81.8\n",
      "61.6/81.8\n",
      "\n",
      "70%\n",
      "1,041\n",
      "22,834\n",
      "60.9/82.8\n",
      "59.1/81.5\n",
      "62.9/83.5\n",
      "65.8/83.8\n",
      "\n",
      "80%\n",
      "600\n",
      "11,417\n",
      "85.9/95.2\n",
      "86.4/95.0\n",
      "89.8/96.2\n",
      "90.9/96.4\n",
      "\n",
      "80%\n",
      "751\n",
      "10,348\n",
      "86.3/94.7\n",
      "86.4/95.0\n",
      "89.8/96.0\n",
      "91.2/96.4\n",
      "\n",
      "80%\n",
      "561\n",
      "13,500\n",
      "76.8/87.3\n",
      "77.4/87.3\n",
      "80.2/89.2\n",
      "82.9/90.6\n",
      "\n",
      "80%\n",
      "702\n",
      "13,201\n",
      "78.3/88.6\n",
      "78.7/88.7\n",
      "81.7/90.3\n",
      "84.0/91.6\n",
      "\n",
      "80%\n",
      "832\n",
      "26,335\n",
      "59.6/82.4\n",
      "58.6/82.0\n",
      "61.9/83.6\n",
      "64.2/83.3\n",
      "\n",
      "80%\n",
      "1,041\n",
      "26,096\n",
      "62.5/84.2\n",
      "60.9/82.3\n",
      "64.2/84.5\n",
      "67.2/84.7\n",
      "\n",
      "90%\n",
      "675\n",
      "11,727\n",
      "86.9/95.2\n",
      "87.4/95.5\n",
      "90.5/96.4\n",
      "91.3/96.4\n",
      "\n",
      "90%\n",
      "751\n",
      "11,642\n",
      "86.7/94.6\n",
      "87.1/95.2\n",
      "90.4/96.3\n",
      "91.6/96.4\n",
      "\n",
      "90%\n",
      "631\n",
      "15,111\n",
      "78.0/88.3\n",
      "78.5/88.7\n",
      "81.1/90.0\n",
      "83.3/91.2\n",
      "\n",
      "90%\n",
      "702\n",
      "14,860\n",
      "79.1/88.8\n",
      "79.4/89.0\n",
      "82.0/90.6\n",
      "84.1/91.3\n",
      "\n",
      "90%\n",
      "936\n",
      "29,529\n",
      "61.9/84.2\n",
      "60.9/83.0\n",
      "63.7/85.0\n",
      "66.1/84.8\n",
      "\n",
      "90%\n",
      "1,041\n",
      "29,358\n",
      "63.4/84.5\n",
      "62.4/83.8\n",
      "65.0/85.1\n",
      "67.4/85.3\n",
      "\n",
      "100%\n",
      "751\n",
      "12,936\n",
      "87.5/95.1\n",
      "88.2/95.3\n",
      "91.0/96.4\n",
      "91.9/96.6\n",
      "\n",
      "100%\n",
      "751\n",
      "12,936\n",
      "87.5/95.1\n",
      "88.2/95.3\n",
      "91.0/96.4\n",
      "91.9/96.6\n",
      "\n",
      "100%\n",
      "702\n",
      "16,522\n",
      "79.4/89.0\n",
      "79.5/89.1\n",
      "82.1/91.0\n",
      "84.3/92.0\n",
      "\n",
      "100%\n",
      "702\n",
      "16,522\n",
      "79.4/89.0\n",
      "79.5/89.1\n",
      "82.1/91.0\n",
      "84.3/92.0\n",
      "\n",
      "100%\n",
      "1,041\n",
      "32,621\n",
      "63.7/85.1\n",
      "62.7/84.3\n",
      "65.7/85.5\n",
      "68.0/86.0\n",
      "\n",
      "100%\n",
      "1,041\n",
      "32,621\n",
      "63.7/85.1\n",
      "62.7/84.3\n",
      "65.7/85.5\n",
      "68.0/86.0\n",
      "\n",
      "Table 14. Performance for small-scale and few-shot setting with MGN method for Market1501, DukeMTMC and MSMT17. “IN sup.” and\n",
      "“IN unsup.” refer to supervised and unsupervised pre-trained model on ImageNet, “LUP unsup.” refers to unsupervised pre-trained model\n",
      "on LUPerson, “LUPws wsp.“ refers to our model pre-trained on LUPerson-WS using WSP. The first number is mAP and second is cmc1.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "\n",
    "print(array_authors[0])\n",
    "print(array_pdf_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cc82c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dengpan.Fu', 'Dongdong.Chen', 'Hao.Yang', 'Jianmin.Bao', 'Lu.Yuan', 'Lei.Zhang', 'Houqiang.Li', 'Fang.Wen', 'Dong.Chen']\n",
      "Understanding 3D Object Articulation in Internet Videos\n",
      "\n",
      "Shengyi Qian\n",
      "\n",
      "Linyi Jin\n",
      "\n",
      "Chris Rockwell\n",
      "\n",
      "Siyi Chen\n",
      "\n",
      "David F. Fouhey\n",
      "\n",
      "University of Michigan\n",
      "{syqian,jinlinyi,cnris,siyich,fouhey}@umich.edu\n",
      "https://jasonqsy.github.io/Articulation3D\n",
      "\n",
      "Figure 1. Given an ordinary video, our system produces a 3D planar representation of the observed articulation. The 3D renderings\n",
      "illustrate how the microwave (in Pink) can be articulated in 3D space. We also show the predicted rotation axis using a Blue arrow.\n",
      "\n",
      "Abstract\n",
      "\n",
      "We propose to investigate detecting and characterizing\n",
      "the 3D planar articulation of objects from ordinary RGB\n",
      "videos. While seemingly easy for humans, this problem\n",
      "poses many challenges for computers. Our approach is\n",
      "based on a top-down detection system that finds planes that\n",
      "can be articulated. This approach is followed by optimizing\n",
      "for a 3D plane that explains a sequence of detected articu-\n",
      "lations. We show that this system can be trained on a com-\n",
      "bination of videos and 3D scan datasets. When tested on\n",
      "a dataset of challenging Internet videos and the Charades\n",
      "dataset, our approach obtains strong performance.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "How would you make sense of Figure 1? Behind the set\n",
      "of RGB pixels that make up the video is a real 3D trans-\n",
      "formation consisting of a 3D planar door rotating about an\n",
      "axis. The goal of this paper is to give the same ability to\n",
      "computers. We focus on planar articulation taking the form\n",
      "of a rotation or translation along an axis. This special case\n",
      "of articulation is ubiquitous in human scenes and under-\n",
      "standing it lets a system understand objects ranging from\n",
      "refrigerators and drawers to closets and cabinets. While we\n",
      "often learn about these shapes and articulations with phys-\n",
      "ical embodiment [53], we have no difficulty understanding\n",
      "them from video cues alone, for instance while watching a\n",
      "\n",
      "movie or seeing another person perform an action. We for-\n",
      "malize this ability for computers as recognizing and charac-\n",
      "terizing a class-agnostic planar articulation via a 3D planar\n",
      "segment, articulation type (rotation or translation), 3D ar-\n",
      "ticulation axis, and articulation angle.\n",
      "\n",
      "This problem is beyond the current state of the art in\n",
      "scene understanding since it requires reconciling single\n",
      "image 3D understanding with dynamic 3D understanding.\n",
      "While there has been substantial work on 3D reconstruction\n",
      "from a single image [5,10,13,65], including work dedicated\n",
      "to planes [34], these works focus on reconstructing static\n",
      "scenes. On the other hand, while there has been work under-\n",
      "standing articulation, these works often require the place-\n",
      "ment of tags for tracking [37, 43], a complete 3D model or\n",
      "depth sensor [21, 31, 41], or successful 3D human recon-\n",
      "struction [69]. Moreover, making progress is challenging\n",
      "because of data. Unsupervised approaches based on motion\n",
      "analysis [44, 54] require something to track, which breaks\n",
      "in realistic data since many human-made articulated ob-\n",
      "jects are untextured (e.g., refrigerators) or transparent (e.g.,\n",
      "ovens). While supervised approaches [31, 40, 41] can per-\n",
      "haps bypass tracking features, they seemingly require ac-\n",
      "cess to large amounts of RGBD data of interactions. For\n",
      "now, this data does not exist, and training on synthetic data\n",
      "can fall short when tested on real data (as our experiments\n",
      "empirically demonstrate).\n",
      "\n",
      "We overcome these challenges with a learning-based ap-\n",
      "proach that combines both detection and 3D optimization\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "and is trained with supervision from multiple sources (Sec-\n",
      "tion 4). The foundation of our approach is a top-down de-\n",
      "tection approach that recognizes articulation axes and types\n",
      "and 3D planes; this approach’s outputs are processed with\n",
      "an optimization method that seeks to explain the per-frame\n",
      "results in terms of a single coherent 3D articulation.\n",
      "\n",
      "Via this model, we show that one can build an under-\n",
      "standing of 3D object dynamics via a mix of 2D supervision\n",
      "on Internet videos of objects undergoing articulation as well\n",
      "as 3D supervision on existing 3D datasets that do not de-\n",
      "pict articulations. To provide 2D supervision, we introduce\n",
      "(Section 3) a new set of 9447 Creative Commons Internet\n",
      "videos. These videos depict articulation with a variety of\n",
      "objects as well as negative samples and come with sparse\n",
      "frame annotations of articulation boxes, axes, and surface\n",
      "normals that can be used for training and evaluating planar\n",
      "articulation models.\n",
      "\n",
      "Our experiments (Section 5) evaluate how well our ap-\n",
      "proach can recognize and characterize articulation. We\n",
      "evaluate on our new dataset of videos as well as the Cha-\n",
      "rades [51] dataset. We compare with a variety of alter-\n",
      "nate approaches, including bottom-up signals like optical\n",
      "flow [56] and changes in surface normals [4], training on\n",
      "synthetic data [68], as well as systems that analyze human-\n",
      "object interaction [69]. Our approach outperforms these\n",
      "approaches on our data, often even when the baselines are\n",
      "given access to ground-truth location of articulation.\n",
      "\n",
      "Our primary contributions include:\n",
      "\n",
      "(1) The new task\n",
      "of detecting 3D object articulation on unconstrained ordi-\n",
      "nary RGB videos without requiring RGBD video at train-\n",
      "ing time; (2) A dataset of Internet videos, with sparse frame\n",
      "annotations of articulation boxes, axes, and surface normals\n",
      "that can be used for training and evaluating planar articu-\n",
      "lation models; (3) A top-down detection network and op-\n",
      "timization to tackle this problem, which has strong perfor-\n",
      "mance on the Internet video dataset and Charades.\n",
      "\n",
      "2. Related Work\n",
      "\n",
      "Our paper proposes to extract 3D models of articulation\n",
      "from ordinary RGB videos. This problem lies at the inter-\n",
      "section of 3D vision, learning from videos, and touches on\n",
      "robotics applications. We note that there are specialized ap-\n",
      "proaches for understanding general articulation (e.g., non-\n",
      "rigid structure from motion [58]) as well as for understand-\n",
      "ing specialized motion models (e.g., for a full human 3D\n",
      "mesh models [73] or quadrupeds [29]) or for understanding\n",
      "more general transformations [20, 63]. Our work focuses\n",
      "on understanding the articulation of general objects whose\n",
      "articulated pieces can be represented by a 3D plane rotating\n",
      "or translating.\n",
      "\n",
      "Due to the ubiquitous nature of articulated objects, the\n",
      "task of understanding them has long been an interest across\n",
      "all of artificial intelligence. In vision, the understanding of\n",
      "\n",
      "the motion of rigid objects undergoing transformations was\n",
      "one of the early successes of computer vision [25, 57, 62].\n",
      "Unfortunately, these early works rely on reliable motion\n",
      "tracks, which is made difficult by the textureless or re-\n",
      "flective nature of many indoor planes (e.g., refrigerator\n",
      "doors). Our top-down detector gives 3D planes that can\n",
      "help provide correspondence between frames where corre-\n",
      "spondence is challenging.\n",
      "\n",
      "More recent work in robotics has used the value of 3D\n",
      "and integrated it into their modeling approaches [6,9,39,44,\n",
      "54]; however their approaches often use an RGBD sensor,\n",
      "unlike our use of ordinary RGB sensors. This dependence\n",
      "on RGBD has been carried forward to the most recent work\n",
      "that uses deep learning frameworks [1, 21, 31, 36, 66, 68].\n",
      "In fact, some methods require full 3D models [41], which\n",
      "is typically unavailable in real world 3D scans. [40] by Mo\n",
      "et al. can be run on 2D images as long as the point cloud\n",
      "encoder is replaced with a RGB encoder, but its 2D im-\n",
      "ages contain a single object without any background, in-\n",
      "stead of challenging Internet videos. While there has been\n",
      "increasing amounts of work aimed at virtual articulated ob-\n",
      "jects [55, 68], simultaneously achieving scale and quality\n",
      "is challenging. For instance ReplicaCAD [55] has only 92\n",
      "objects. In contrast, our approach works at test time on stan-\n",
      "dard RGB videos by bringing its own 3D via a learned de-\n",
      "tector [34] trained on RGBD data [7].\n",
      "\n",
      "While our outputs are 3D planar regions, our approach\n",
      "is deeply connected to the task of understanding human-\n",
      "object interactions. In these works [3, 14, 50], the goal is\n",
      "to recognize the relationship between humans and the ob-\n",
      "jects they interact with. The interactions that we study are\n",
      "caused by these humans, and so we use an approach that can\n",
      "predict human-object interactions [50] to help identify the\n",
      "data we train our systems on. The most related work in this\n",
      "area is [69], which aims to jointly understand dynamic 3D\n",
      "human-object interactions in 3D. This work, however as-\n",
      "sumes that the object CAD model is known once the articu-\n",
      "lated object is detected, which we do not need. Our method\n",
      "also works with articulation videos that are more varied in\n",
      "viewpoint and perspective. A more thorough understand-\n",
      "ing of the joint relationship between articulated objects and\n",
      "human-object interaction, akin to early work [11, 28], is be-\n",
      "yond the scope of this work, but of future value.\n",
      "\n",
      "We solve the problem of describing 3D articulation by\n",
      "producing 3D planar models. This uses advances in 3D\n",
      "from a single image.\n",
      "In particular, we build on PlaneR-\n",
      "CNN [34], which is part of a growing body of works aimed\n",
      "at extracting planes from single images [35, 70, 71]. These\n",
      "planes have advantages for the articulation reasoning since\n",
      "they offer a compact representation to track and describe.\n",
      "While we use plane recognition, the plane is just one com-\n",
      "ponent of our output (along with rotation axes) and we ana-\n",
      "lyze our output in a video with temporal optimization.\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "3. Dataset\n",
      "\n",
      "One critical component of our approach is accurate 2D\n",
      "annotations of articulation occurring in RGB data. We show\n",
      "that these 2D annotations can be combined with existing\n",
      "RGBD data and the right method to build systems that un-\n",
      "derstand 3D articulation on video data. We next describe\n",
      "how we collect a dataset of articulations. Our goals are to\n",
      "have a large collection of annotations of object box, artic-\n",
      "ulation type, and axis. Rather than directly look for exam-\n",
      "ples of people articulating objects, we follow the data-first\n",
      "approach of [8, 12, 50, 72], namely to gather data containing\n",
      "many related activities and then analyze and annotate it.\n",
      "\n",
      "Data Collection. Our pipeline generates a set of candidate\n",
      "clips to be annotated from a collection of candidate videos\n",
      "via an automatic pipeline that aims to eliminate frames that\n",
      "are easy to see as not depicting articulation. We begin with\n",
      "candidate videos that are found by variants of searches for\n",
      "a set of 10 objects among Creative Commons videos on\n",
      "YouTube. Within these videos, we find stationary contin-\n",
      "uous shots in these videos with homographies [16] fit on\n",
      "ORB [49] features. Many of these clips cannot depict in-\n",
      "teraction since they do not contain any people or do not\n",
      "contain the objects of interest. We filter by responses by\n",
      "a hand detector [50] trained on 100K+ frames of Internet\n",
      "data, as well as object detectors trained on COCO [33] and\n",
      "LVIS [15]. These filtering steps maximize the use of an-\n",
      "notator time by eliminating clear negatives, and generate a\n",
      "large number of candidate clips.\n",
      "\n",
      "With a collection of candidate clips of interest, we then\n",
      "turn to manual annotation. For a given clip, we hire an\n",
      "annotation company to annotate frames sparsely (every 10\n",
      "frames) within the clip. They annotate: (box) a box around\n",
      "the articulated plane and its type, if it exists; and (axis) the\n",
      "projection of the articulation axis, framed as a line segmen-\n",
      "tation annotation problem. This results in a set of 19411\n",
      "frames, containing 19411 boxes around articulating planes\n",
      "with 13508 rotation axes and 2755 translation axes, as well\n",
      "as 39411 negative frames. The number of articulation axes\n",
      "is not equal to the number of boxes, since some articulation\n",
      "axes are outside the image. We provide training, validation,\n",
      "and test splits based on uploader, leading to 7845/601/1001\n",
      "videos in the train/val/test split. A more complete descrip-\n",
      "tion of our annotation pipeline appears in the supplement.\n",
      "\n",
      "We collect two additional annotations. For the test set,\n",
      "we also annotate the surface normal of the plane follow-\n",
      "ing [4], so we can evaluate how well our model can learn 3D\n",
      "properties. To show generalization, we also collect the same\n",
      "annotations except surface normals on the Charades [51]\n",
      "dataset.\n",
      "\n",
      "Data Availability and Ethics. Our data consists of videos\n",
      "that users uploaded publicly and chose to share as Creative\n",
      "Commons data. These do not involve interaction with hu-\n",
      "\n",
      "mans or private data. We filtered obviously offensive con-\n",
      "tent, videos depicting children, and cartoons. Examples\n",
      "appear throughout the paper; screenshots of annotation in-\n",
      "structions and details appear in the supplement.\n",
      "\n",
      "4. Approach\n",
      "\n",
      "The goal of our approach is to detect and characterize\n",
      "planar articulation in an unseen RGB video clip. These\n",
      "articulations are an important special case that are ubiqui-\n",
      "tous in human scenes. As shown in Figure 2, we propose a\n",
      "3D Articulation Detection Network (3DADN) to solve the\n",
      "task. As output, the 3DADN produces the type of motion\n",
      "(rotation or translation), a bounding box around where the\n",
      "motion is located, the 2D location of the rotation or transla-\n",
      "tion axis, and the 3D location of the articulated plane. The\n",
      "3DADN’s output is followed by post-processing to find a\n",
      "consistent explanation over the whole video.\n",
      "\n",
      "4.1. 3D Articulation Detection Network\n",
      "\n",
      "The 3DADN processes each frame independently.\n",
      "Its\n",
      "output consists of: a segment mask Mi; plane parameters\n",
      "πi = [ni, oi] giving the plane equation πT\n",
      "i [x, y, z, −1] = 0\n",
      "(where ni is the the plane normal with ||ni||2 = 1 and oi\n",
      "is the plane offset); a projected rotation or translation axis\n",
      "ai = [θ, p] which is the projection of the 3D articulation\n",
      "axis; and articulation type.\n",
      "\n",
      "We use a top-down approach to detect this representa-\n",
      "tion, which we train on RGB videos that depict articulation\n",
      "without 3D information as well as RGBD images with 3D\n",
      "information that do not depict articulation. Our backbone is\n",
      "a Faster R-CNN [47] style network that first detects bound-\n",
      "ing boxes for the articulating objects and classifies them into\n",
      "two classes (rotation and translation). These boxes provide\n",
      "ROI-pooled features that are passed into detection heads\n",
      "that predict our outputs (Mi, πi, ai). Our heads and losses\n",
      "for Mi follow the common practice of Mask R-CNN [17].\n",
      "We describe ai and πi below.\n",
      "Parameterizing Rotation and Translation Axis. We\n",
      "model the projected articulation axis as a 2D line in the\n",
      "image. This projected axis is the projection of the 3D ar-\n",
      "ticulation axis (e.g., the hinge of a door). We describe the\n",
      "projected axis with the normal form of the line, x cos(θ) +\n",
      "y sin(θ) = p where p ≥ 0 is the distance from the box to\n",
      "the center and θ is the inclination of the normal of the axis\n",
      "in pixel coordinates. Since a translation corresponds to a di-\n",
      "rection/family of lines as opposed to a line, we define p = 0\n",
      "for translation arbitrarily.\n",
      "\n",
      "The articulation head contains two independent branches\n",
      "for predicting the rotation and translation axes. We handle\n",
      "the circularity of the prediction of θ by lifting predictions\n",
      "and ground-truth for the angle to the 2D unit circle; since\n",
      "the line is 180-degree-ambiguous (i.e., θ + π is the same as\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Figure 2. Overview of our approach. (a) Given an ordinary video clip, we first apply our 3D Articulation Detection Network (3DADN) to\n",
      "detect 3D planes can be articulated for each frame. (b) We then apply temporal optimization to fit the articulation model. Final results are\n",
      "demonstrated in both 2D image and 3D rendering.\n",
      "\n",
      "θ), we predict a 2D vector [sin(2θ), cos(2θ)]. The resulting\n",
      "network thus predicts a 3D vector containing θ and p, which\n",
      "we supervise with a L1 loss.\n",
      "\n",
      "Parameterizing Plane Parameters. Following a line of\n",
      "work on predicting planes in images, we use a 3D plane [35]\n",
      "to represent the 3D locations of the articulated objects, since\n",
      "many common articulated objects like doors, refrigerators,\n",
      "and microwaves can be modeled as planes and because past\n",
      "literature [22, 23, 34] suggests that R-CNN-style networks\n",
      "are adept at predicting plane representations.\n",
      "\n",
      "A 3D plane is represented by plane parameters πi =\n",
      "[ni, oi] giving the plane equation πT\n",
      "i [x, y, z, −1] = 0 With\n",
      "camera intrinsics, planes can be recovered in 3D and with a\n",
      "mask, this plane can be converted to a plane segment. Fol-\n",
      "lowing [23, 34], we extend R-CNN by adding a plane head\n",
      "which directly regresses the normal of the plane. A depth\n",
      "head is used to predict depth of the image. The depth is only\n",
      "used to calculate the offset value of the plane. We supervise\n",
      "with L2 loss for the plane normal regression and L1 loss for\n",
      "the depth regression.\n",
      "\n",
      "Training. There is no dataset that is non-synthetic and\n",
      "large enough to train a 3DADN directly: the 3DADN needs\n",
      "both realistic interactions and 3D information. However,\n",
      "we can train the 3DADN in parts. In the first stage, we train\n",
      "the backbone, RPN, and axis heads directly on our Inter-\n",
      "net video training set, which has boxes and axes. We then\n",
      "freeze the backbone, RPN, and axis heads and fine-tune the\n",
      "mask and plane head on a modified version of ScanNet [7].\n",
      "In particular, we found that humans often occlude the\n",
      "objects they articulate and models that had not seen humans\n",
      "in training produced worse qualitative results. We therefore\n",
      "composited humans from SURREAL [60] into the scenes.\n",
      "We randomly sample 98,235 ScanNet images, select a syn-\n",
      "In\n",
      "thetic human and render it on ScanNet backgrounds.\n",
      "\n",
      "training, we do not change the ground truth, pretending\n",
      "the ground truth plane is partially occluded by humans and\n",
      "training our model to identify them.\n",
      "\n",
      "Meanwhile, we found that the order of training the heads\n",
      "was crucial. Planes in ScanNet [7] are defined geometri-\n",
      "cally, and so unopened doors often merge with walls; sim-\n",
      "ilarly, ScanNet [7] does not contain transitional moments\n",
      "during which planes are articulating. Thus, RPNs trained\n",
      "on ScanNet [7] perform poorly on articulation videos. In-\n",
      "stead, it is important to train the RPN on our Internet videos,\n",
      "freeze the backbone, and only rely on ScanNet to train plane\n",
      "parameters and masks, which are unavailable in Internet\n",
      "videos. During inference we keep the ScanNet camera since\n",
      "our data does not have camera intrinsics.\n",
      "Implementation Details. Full architectural details of our\n",
      "is im-\n",
      "approach are in the supplemental. Our model\n",
      "plemented using Detectron2 [67]. The backbone uses\n",
      "ResNet50-FPN [32] pretrained on COCO [33].\n",
      "\n",
      "4.2. Temporal Optimization\n",
      "\n",
      ", π(t)\n",
      "i\n",
      "\n",
      "After the 3DADN provides per-frame estimates of artic-\n",
      "ulations, we perform temporal optimization to find a sin-\n",
      "gle explanation for the detections across frames. We are\n",
      "given a sequence of detections indexed by a time of the form\n",
      "[M(t)\n",
      ", a(t)\n",
      "]. We aim to find a single consistent expla-\n",
      "i\n",
      "i\n",
      "nation for these detections.\n",
      "Tracking. Optimizing requires a sequence of planes to op-\n",
      "timize over. We match box i with the box in the next frame\n",
      "according to pairwise intersection over union (IoU). Box\n",
      "i at t matches box j = arg maxj′ IoU(M(t)\n",
      ") at\n",
      "time t + 1; we then track greedily to get a sequence. We\n",
      "subsequently drop the subscripts for clarity.\n",
      "Articulation Model Fitting. Given a sequence of detec-\n",
      "tions, we find a consistent explanation via a RANSAC-like\n",
      "\n",
      ", M(t+1)\n",
      "j′\n",
      "\n",
      "i\n",
      "\n",
      "4\n",
      "\n",
      "][Per-frameDetectionPlaneMaskPlaneParamsArticulationAxisPredictionsVideo ClipPerframeDetection CNN2D boxand axis3D planesand axisTemporal Optimization……Across frames\f",
      "Input\n",
      "\n",
      "Pred1\n",
      "\n",
      "Pred2\n",
      "\n",
      "Pred3\n",
      "\n",
      "Pred4\n",
      "\n",
      "Pred 3D\n",
      "\n",
      "Figure 3. Predictions on Internet videos. For each example, we show the input (left), detected 2D planes and how they will be articulated\n",
      "using the predicted articulation axes and surface normals (middle). We also show 3D renderings to illustrate how these common objects\n",
      "are articulated in the 3D space (right). The predicted rotation axis is shown as the Blue arrow, and translation axis is the Pink arrow.\n",
      "\n",
      "approach. We begin with a hypothesis of a plane segment\n",
      "π and articulation axis a, which we obtain by selecting out-\n",
      "put on a reference frame. Along with an assumed camera\n",
      "intrinsics K, the plane parameters let us lift the plane seg-\n",
      "ment and axis to 3D, producing 3D plane segment Π and\n",
      "3D axis A. Then, for each frame t, we solve for an artic-\n",
      "ulation degree α(t) maximizing the reprojection agreement\n",
      "with the predicted mask at time t. Let us define the repro-\n",
      "jection score as\n",
      "\n",
      "r(α, t) = IoU\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "M(t), K [Rα, tα] Π\n",
      "\n",
      ",\n",
      "\n",
      "(1)\n",
      "\n",
      "where RA,α and tA,α are α steps over the rotation and\n",
      "translation for axis A. We then solve for α(t) by solving\n",
      "arg maxα r(α), which gives a per-frame angle using grid\n",
      "search. We detect articulation by calculating how well the\n",
      "rotation degree α(t) can be explained as a linear function\n",
      "of t (i.e., that there is constant motion). Since many scenes\n",
      "are not constant motion, we have loose thresholds: we con-\n",
      "sider R2 ≥ 0.4 and slope k > 0.1 to be an articulation. We\n",
      "exclude hypotheses where all r(α(t), t) < 0.5.\n",
      "\n",
      "5. Experiments\n",
      "\n",
      "We have introduced a method that can infer 3D articu-\n",
      "lation in Section 4. In the experiments, we aim to answer\n",
      "the following questions: (1) how well can one detect 3D\n",
      "articulating objects from ordinary videos; (2) how well do\n",
      "alternate approaches to the problem do?\n",
      "\n",
      "5.1. Experimental Setup\n",
      "\n",
      "We first describe the setup of our experiments. Our\n",
      "method aims to look at an ordinary RGB video and infer\n",
      "information about an articulated plane on a object in 3D,\n",
      "including: whether the object is articulating, its extent, and\n",
      "the projection of its rotation or translation axis. We there-\n",
      "fore evaluate our approach on two challenging datasets, us-\n",
      "ing metrics that capture various aspects of a 3D plane artic-\n",
      "ulating in 3D.\n",
      "\n",
      "Datasets: We validate our approach on both Internet videos\n",
      "(described in Section 3) and the Charades dataset [52]. We\n",
      "use Charades for cross-dataset evaluations. We focus on\n",
      "Charades videos that are opening objects (doors, refrigera-\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Flow+Normal\n",
      "\n",
      "SAPIEN\n",
      "\n",
      "SAPIEN w/ gtbox\n",
      "\n",
      "D3D-HOI\n",
      "\n",
      "Ours\n",
      "\n",
      "GT\n",
      "\n",
      "Figure 4. We compare our approach with four baselines. See detailed discussions in the text. We show translation in Pink and rotation in\n",
      "Blue, except D3D-HOI which uses a different detector.\n",
      "\n",
      "tors, etc.), and annotate 2491 frames across 479 videos; we\n",
      "also randomly sample 479 negative videos containing 4401\n",
      "negative frames. Our Charades annotation process is simi-\n",
      "lar to Internet videos, with the exceptions that: we annotate\n",
      "only rotations as Charades contains few translation articu-\n",
      "lations; and we do not annotate surface normals.\n",
      "\n",
      "Evaluation Criteria: Evaluation of our approach is non-\n",
      "trivial, since our assumed input (RGB videos) precludes\n",
      "measuring outputs quantitatively in 3D. We therefore eval-\n",
      "uate our approach on a series of subsets of the problem. We\n",
      "stress from the start though that these metrics are what can\n",
      "be measured (due to the use of RGB inputs), as opposed to\n",
      "the full rich output.\n",
      "Articulation Recognition: We first independently evaluate\n",
      "the ability to detect whether someone is articulating this ob-\n",
      "ject at a point in time. We frame this as a binary prediction\n",
      "problem. This is surprisingly difficult in real scenes because\n",
      "objects are typically partially occluded by humans when hu-\n",
      "mans articulate them, and because humans often touch a ar-\n",
      "ticulated objects (e.g., cleaning the surface) without open-\n",
      "ing it. We use AUROC to measure the performance.\n",
      "Articulation Description: We next evaluate the ability of\n",
      "a system to detect the articulated object, corresponding ar-\n",
      "ticulation type (rotation/translation), axes, and surface nor-\n",
      "mals. We follow other approaches [23, 30, 42, 45, 59] that\n",
      "reconstruct the scene factored into components and treat it\n",
      "as a 3D detection problem, evaluated using average preci-\n",
      "sion (AP). We define error metrics as follows: (Bounding\n",
      "\n",
      "box) IoU, thresholded as 0.5. We find the normal COCO\n",
      "AP, which measures IoU up to 0.95, to be too strict be-\n",
      "cause the precise boundaries of articulating parts are often\n",
      "occluded by people and hard to annotate. (Axes) EA-score\n",
      "from the semantic line detection literature [74]. This metric\n",
      "handles a number of edge cases; we use 0.5 as the thresh-\n",
      "old as recommended by [74]. (Surface normal) mean angle\n",
      "error, thresholded at 30◦, following [10, 64]. A prediction\n",
      "is a true positive only if all errors are lower than our thresh-\n",
      "olds. We calculate the precision-recall curve based on that\n",
      "and report AP for varying combinations of metrics.\n",
      "\n",
      "Baselines: Prior approaches for articulation detection have\n",
      "focused on robots, synthetic datasets, and real-world RGBD\n",
      "scans. These are different from our setting for two reasons.\n",
      "First, videos of people articulating objects show a noisy\n",
      "background with a person interacting with and occluding\n",
      "the object, as opposed to an isolated articulated object in a\n",
      "simulator. Second, RGB videos do not have depth, which is\n",
      "often a requirement of existing articulation models. For ex-\n",
      "ample [31] requires depth, and while they show results on\n",
      "real-world depth scans, their RGBD scans only contain a\n",
      "static object without humans. We propose to compare with\n",
      "the following methods.\n",
      "\n",
      "3DADN + SAPIEN [68] Data: To test whether we can solve\n",
      "the problem just by training on synthetic data, we create a\n",
      "synthetic data-based method where we train our 3DADN\n",
      "system on synthetic data. We render a synthetic dataset us-\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Table 1. We report AUROC for Articulation Recognition, as well as AP for Articulation Description. To separate out difficulties in\n",
      "detecting articulation and characterizing its parameters, we assist Flow+Normal and 3DADN+SAPIEN with ground truth bounding box\n",
      "and denote it as gtbox. 3DADN+SAPIEN cannot detect most objects without the help of gtbox.\n",
      "\n",
      "Methods\n",
      "\n",
      "Flow [56] + Normal [4]\n",
      "Flow [56] + Normal [4]\n",
      "D3D-HOI [69] Upper Bound\n",
      "3DADN + SAPIEN [68]\n",
      "Ours\n",
      "\n",
      "✗\n",
      "✔\n",
      "✗\n",
      "✔\n",
      "✗\n",
      "\n",
      "Recog.\n",
      "gtbox AUROC bbox\n",
      "\n",
      "Rotation\n",
      "\n",
      "Translation\n",
      "\n",
      "bbox+axis\n",
      "\n",
      "bbox+axis+normal\n",
      "\n",
      "bbox\n",
      "\n",
      "bbox+axis\n",
      "\n",
      "bbox+axis+normal\n",
      "\n",
      "68.5\n",
      "-\n",
      "62.7\n",
      "-\n",
      "76.6\n",
      "\n",
      "7.7\n",
      "-\n",
      "28.8\n",
      "-\n",
      "61.3\n",
      "\n",
      "0.3\n",
      "3.0\n",
      "19.7\n",
      "16.8\n",
      "30.4\n",
      "\n",
      "0.0\n",
      "0.3\n",
      "n/a\n",
      "1.40\n",
      "17.2\n",
      "\n",
      "0.3\n",
      "-\n",
      "4.70\n",
      "-\n",
      "34.0\n",
      "\n",
      "0.0\n",
      "1.4\n",
      "4.7\n",
      "15.1\n",
      "27.1\n",
      "\n",
      "0.0\n",
      "0.7\n",
      "n/a\n",
      "0.40\n",
      "17.9\n",
      "\n",
      "ing SAPIEN [68] by randomly sampling and driving 3D\n",
      "objects. We filtered 1053 objects of 18 categories with\n",
      "movable planes from PartNet-Mobility Dataset [68], such\n",
      "as doors and laptops. We render frames with the objects\n",
      "articulated, with location parameters picked to give plau-\n",
      "sible scenes, and extract the information needed to train\n",
      "3DADN. Without a background, the detection problem be-\n",
      "comes trivial, so to mimic real 3D scenes, we blend the\n",
      "renderings with random ScanNet [7] images as the back-\n",
      "ground and render synthetic humans from SURREAL [60].\n",
      "For fair comparison, we use the same ScanNet+SURREAL\n",
      "images used to train our system’s plane parameter head.\n",
      "When evaluated on SAPIEN data, this approach performs\n",
      "well and obtains an AP of (bbox) 60.3, (bbox+rot) 64.1,\n",
      "(bbox+rot+normal) 41.0.\n",
      "\n",
      "Bottom-up Optical Flow [56] and Surface Normal\n",
      "Change [4] (Flow+Normal): To test whether the data can\n",
      "be solved by the use of fairly simple cues, we construct\n",
      "a baseline that uses Optical Flow [56] (since articulating\n",
      "objects tend to cause movement) and Surface Normals [4]\n",
      "(since rotating planes change their orientation). Both flow\n",
      "and normals provide a H × W map that can be analyzed.\n",
      "We also use the output of a human segmentation system [19]\n",
      "that was trained on multiple datasets and mask normal and\n",
      "flow magnitude maps wherever it improves performance.\n",
      "Given these maps, we recognize the presence of articulation\n",
      "via logistic regression on a feature vector consisting of the\n",
      "fraction of pixels above multiple thresholds; we recognize\n",
      "bounding boxes via thresholding and finding the tightest en-\n",
      "closing box; we estimate rotation axis as perpendicular to\n",
      "the mean flow change in the bounding box (flow tends to\n",
      "increase away from hinges); we find translation axis using\n",
      "mean flow direction in the box; we find articulation normal\n",
      "using mean predicted normals in the box. Throughout, we\n",
      "use the optimal option of surface normals and flow; this hy-\n",
      "brid system performs substantially better than either flow or\n",
      "normals alone.\n",
      "\n",
      "Baselines with + GT Box: To separate out difficulties in\n",
      "detecting articulation and characterizing its parameters, we\n",
      "also experiment with giving baselines ground-truth bound-\n",
      "ing box information about the articulating object. This gives\n",
      "\n",
      "an upper-bound on performance.\n",
      "\n",
      "D3D-HOI [69] Upper Bound: We compare with D3D-\n",
      "HOI since it accepts RGB video as input and detects how\n",
      "humans articulate objects. A direct comparison with D3D-\n",
      "HOI is challenging since it only works when EFT [24] re-\n",
      "constructs 3D human poses and Pointrend [26] detects the\n",
      "objects that are assumed to articulate and correct CAD mod-\n",
      "els are chosen for the object. However, EFT does not work\n",
      "well on the dataset due to truncated or multiple humans on\n",
      "Internet videos [27, 48]. We therefore report upper-bounds\n",
      "on the performance. We assume it predicts the ground\n",
      "truth bounding box, when EFT mask and pseudo ground\n",
      "truth 2D human segmentation mask [18] has IoU > 0.5 and\n",
      "PointRend [26] produces a mask on articulated objects with\n",
      "confidence > 0.7.\n",
      "Ours: This is our proposed method. It includes both the per-\n",
      "frame approach described in Section 4.1 and the optimiza-\n",
      "tion approach of Section 4.2. We note that this approach\n",
      "also produces outputs that are not being quantitatively mea-\n",
      "sured, such as a 3D plane articulating in 3D. These are qual-\n",
      "itatively shown in Figures 1 and 3.\n",
      "\n",
      "5.2. Results\n",
      "\n",
      "We first show qualitative results in Figure 3. On chal-\n",
      "lenging Internet videos, our approach usually detects and\n",
      "recovers the 3D articulated plane regardless of categories.\n",
      "\n",
      "In Figure 4, we compare our approach with four base-\n",
      "lines visually. Flow can occasionally locate articulation\n",
      "(third row), but in most cases, flow is not localized to only\n",
      "the object articulating (e.g. camera movement, top row).\n",
      "Training purely on SAPIEN [68] data has difficulty detect-\n",
      "ing articulated objects in Internet videos, even if we show\n",
      "all detected objects with confidence score > 0.1. It learns\n",
      "some information of articulation axes when we assist it with\n",
      "ground truth bounding boxes. D3D-HOI [69] relies on both\n",
      "EFT [24] to detect humans and PointRend [26] to detect ob-\n",
      "jects. However, EFT has diffculty predicting 3D humans on\n",
      "Internet videos.\n",
      "\n",
      "Quantitative Results. We evaluate the approach quan-\n",
      "titatively on the three tasks in Table 1. Our approach\n",
      "substantially outperforms the alternate methods. While\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Pred\n",
      "\n",
      "GT\n",
      "\n",
      "Pred\n",
      "\n",
      "GT\n",
      "\n",
      "Pred\n",
      "\n",
      "GT\n",
      "\n",
      "Pred\n",
      "\n",
      "GT\n",
      "\n",
      "Pred\n",
      "\n",
      "GT\n",
      "\n",
      "Pred\n",
      "\n",
      "GT\n",
      "\n",
      "Figure 5. Qualitative results on Charades dataset. Without fine-\n",
      "tuning on Charades data, our model obtains strong performance\n",
      "on detecting and characterizing 3D articulation.\n",
      "\n",
      "Table 2. Evaluation on Charades dataset [52]. We only report rota-\n",
      "tion AP since Charades does not have sufficient translation motion.\n",
      "\n",
      "Methods\n",
      "\n",
      "Flow [56] + Normal [4]\n",
      "Flow [56] + Normal [4]\n",
      "D3D-HOI Upper Bound\n",
      "3DADN + SAPIEN [68]\n",
      "Ours\n",
      "\n",
      "Recog.\n",
      "gtbox AUROC\n",
      "\n",
      "Rotation\n",
      "\n",
      "bbox\n",
      "\n",
      "bbox+axis\n",
      "\n",
      "✗\n",
      "✔\n",
      "✗\n",
      "✔\n",
      "✗\n",
      "\n",
      "53.7\n",
      "-\n",
      "55.9\n",
      "-\n",
      "58.4\n",
      "\n",
      "3.1\n",
      "-\n",
      "14.9\n",
      "-\n",
      "12.0\n",
      "\n",
      "0.2\n",
      "4.2\n",
      "13.7\n",
      "1.54\n",
      "12.8\n",
      "\n",
      "statistically-combined bottom-up cues [4, 56] do better than\n",
      "chance at predicting the presence of an articulation, they\n",
      "are substantially worse than the proposed approach and\n",
      "fail to obtain sensible bounding boxes. Even when given\n",
      "this method fails to obtain good\n",
      "the ground-truth box,\n",
      "axes. Due to the frequency of truncated humans in Internet\n",
      "videos [27, 48], D3D-HOI [69]’s performance upper-bound\n",
      "is substantially lower than our method’s performance. The\n",
      "detection system when trained on synthetic data from [68]\n",
      "fails on our system; when given a good bounding box, syn-\n",
      "thetic training data obtains reasonable, but inferior numbers\n",
      "and poor accuracy in predicting normals.\n",
      "\n",
      "Ablations – Optimization. Our optimization produces\n",
      "modest gains in recognition accuracy and axis localization\n",
      "in 2D: It improves recognition AUROC from 74.0 to 76.6,\n",
      "rotation AP from 16.6 to 17.2 and translation AP from 14.3\n",
      "to 17.9. This small gain is understandable because the\n",
      "evaluation is per-frame and the optimization mainly seeks\n",
      "to make the predictions more consistent.\n",
      "If we quantify\n",
      "the consistency in the results before and after optimization,\n",
      "we find that the EAScore [74] between tracked predicted\n",
      "frames increases from 0.69 (before optimization) to 0.96\n",
      "(after optimization).\n",
      "\n",
      "5.3. Generalization Results\n",
      "\n",
      "We next test our trained models without fine-tuning on\n",
      "Charades [51]. We show results in Figure 5. Our approach\n",
      "typically generates reasonable estimations. We find that the\n",
      "video quality and resolution of Charades is lower relative to\n",
      "our videos, with many dark or blurry videos.\n",
      "\n",
      "We also show quantitative evaluations in Table 2. Here,\n",
      "our performance is slightly diminished. However, we sub-\n",
      "\n",
      "Figure 6. Typical failure modes.\n",
      "(1) Ambiguity of articulation\n",
      "type; (2) Axis outside of the frame or ambiguity of articulation\n",
      "axis location due to symmetry; (3) Object has complex motions (a\n",
      "person moving an object while articulating it; the rotation axis is\n",
      "outside of the articulating surface).\n",
      "\n",
      "stantially outperform the baselines. We are only marginally\n",
      "outperformed by D3D-HOI upper bound, which assumes\n",
      "perfect performance so long as the data can be obtained.\n",
      "\n",
      "5.4. Limitations and Failure Modes\n",
      "\n",
      "We finally discuss our limitations and typical failure\n",
      "modes in Figure 6. We find some examples are particu-\n",
      "larly challenging: (1) Column 1: some images may contain\n",
      "hard examples where the axis types are hard to figure out.\n",
      "(2) Column 2: the axis is outside of the image frame or its\n",
      "location is ambiguous due to symmetry or occlusion. (3)\n",
      "Column 3: the object has complex dynamics or dual axes;\n",
      "for example, a person moving a laptop while opening it or\n",
      "the cabinet has multiple joints.\n",
      "\n",
      "6. Conclusion\n",
      "\n",
      "We have demonstrated our approach’s ability to detect\n",
      "and characterize 3D planar articulation of objects from or-\n",
      "dinary videos. Future work includes combining 3D shape\n",
      "reconstruction with the articulation detection pipeline.\n",
      "\n",
      "Our approach can have positive impacts by helping build\n",
      "smart robots that are able to understand and manipulate ar-\n",
      "ticulated objects. On the other hand, our approach may be\n",
      "useful for surveillance activities. Moreover, our network is\n",
      "trained on Internet videos and deep networks may amplify\n",
      "biases in the data.\n",
      "\n",
      "Acknowledgments This work was supported by the\n",
      "DARPA Machine Common Sense Program and Toyota Re-\n",
      "search Institute. Toyota Research Institute (“TRI”) provided\n",
      "funds to assist the authors with their research but this article\n",
      "solely reflects the opinions and conclusions of its authors\n",
      "and not TRI or any other Toyota entity. We thank Dandan\n",
      "Shan, Jiaqi Geng, Sarah Jabbour and Ruiyu Li for their help\n",
      "with data collection, Mohamed El Banani for his help of\n",
      "blender, Fanbo Xiang for his help of SAPIEN, as well as\n",
      "Yichen Yang and Ziyang Chen for their help of Figure 2.\n",
      "We also thank Justin Johnson, Jiteng Mu, Tiange Luo and\n",
      "Max Smith for helpful discussions.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "References\n",
      "\n",
      "[1] Dhruv Batra, Angel X Chang, Sonia Chernova, Andrew J\n",
      "Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jiten-\n",
      "dra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Re-\n",
      "arrangement: A challenge for embodied ai. arXiv preprint\n",
      "arXiv:2011.01975, 2020. 2\n",
      "\n",
      "[2] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\n",
      "Halber, Matthias Niessner, Manolis Savva, Shuran Song,\n",
      "Andy Zeng, and Yinda Zhang. Matterport3d: Learning\n",
      "arXiv preprint\n",
      "from rgb-d data in indoor environments.\n",
      "arXiv:1709.06158, 2017. 12\n",
      "\n",
      "[3] Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and\n",
      "Jia Deng. Hico: A benchmark for recognizing human-object\n",
      "interactions in images. In ICCV, 2015. 2\n",
      "\n",
      "[4] Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima,\n",
      "Max Hamilton, and Jia Deng. Oasis: A large-scale dataset\n",
      "for single image 3d in the wild. In CVPR, 2020. 2, 3, 7, 8,\n",
      "13\n",
      "\n",
      "[5] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\n",
      "Chen, and Silvio Savarese. 3D-R2N2: A unified approach\n",
      "for single and multi-view 3d object reconstruction. In ECCV,\n",
      "2016. 1\n",
      "\n",
      "[6] Cristina Garcia Cifuentes, Jan Issac, Manuel W¨uthrich, Ste-\n",
      "fan Schaal, and Jeannette Bohg. Probabilistic articulated\n",
      "IEEE Robotics\n",
      "real-time tracking for robot manipulation.\n",
      "and Automation Letters, 2(2):577–584, 2016. 2\n",
      "\n",
      "[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\n",
      "ber, Thomas Funkhouser, and Matthias Nießner. Scannet:\n",
      "Richly-annotated 3d reconstructions of indoor scenes.\n",
      "In\n",
      "CVPR, 2017. 2, 4, 7, 12, 16\n",
      "\n",
      "[8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\n",
      "Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide\n",
      "Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and\n",
      "Michael Wray. Scaling egocentric vision: The epic-kitchens\n",
      "dataset. In ECCV, 2018. 3\n",
      "\n",
      "[9] Karthik Desingh, Shiyang Lu, Anthony Opipari, and\n",
      "Odest Chadwicke Jenkins. Factored pose estimation of ar-\n",
      "ticulated objects using efficient nonparametric belief propa-\n",
      "gation. In 2019 International Conference on Robotics and\n",
      "Automation (ICRA), pages 7221–7227. IEEE, 2019. 2\n",
      "[10] David Eigen and Rob Fergus. Predicting depth, surface nor-\n",
      "mals and semantic labels with a common multi-scale convo-\n",
      "lutional architecture. In ICCV, 2015. 1, 6\n",
      "\n",
      "[11] David F. Fouhey, Vincent Delaitre, Abhinav Gupta,\n",
      "Alexei A. Efros, Ivan Laptev, and Josef Sivic. People watch-\n",
      "ing: Human actions as a cue for single-view geometry. In\n",
      "ECCV, 2012. 2\n",
      "\n",
      "[15] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\n",
      "dataset for large vocabulary instance segmentation. In CVPR,\n",
      "2019. 3, 12\n",
      "\n",
      "[16] R. I. Hartley and A. Zisserman. Multiple View Geometry\n",
      "in Computer Vision. Cambridge University Press, ISBN:\n",
      "0521540518, second edition, 2004. 3, 12\n",
      "\n",
      "[17] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-\n",
      "\n",
      "shick. Mask r-cnn. In ICCV, 2017. 3\n",
      "\n",
      "[18] Vladimir\n",
      "\n",
      "Iglovikov.\n",
      "\n",
      "Binary segmentation of peo-\n",
      "ple. https://github.com/ternaus/people_\n",
      "segmentation, 2020. 7\n",
      "\n",
      "[19] Vladimir Iglovikov and Alexey Shvets. Ternausnet: U-net\n",
      "with vgg11 encoder pre-trained on imagenet for image seg-\n",
      "mentation. arXiv preprint arXiv:1801.05746, 2018. 7\n",
      "[20] Phillip Isola, Joseph J. Lim, and Edward H. Adelson. Dis-\n",
      "covering states and transformations in image collections. In\n",
      "CVPR, 2015. 2\n",
      "\n",
      "[21] Ajinkya Jain, Rudolf Lioutikov, Caleb Chuck, and Scott\n",
      "Screwnet: Category-independent articulation\n",
      "Niekum.\n",
      "model estimation from depth images using screw theory.\n",
      "arXiv preprint arXiv:2008.10518, 2020. 1, 2\n",
      "\n",
      "[22] Ziyu Jiang, Buyu Liu, Samuel Schulter, Zhangyang Wang,\n",
      "and Manmohan Chandraker. Peek-a-boo: Occlusion reason-\n",
      "ing in indoor scenes with plane representations. In CVPR,\n",
      "2020. 4\n",
      "\n",
      "[23] Linyi Jin, Shengyi Qian, Andrew Owens, and David F.\n",
      "Fouhey. Planar surface reconstruction from sparse views. In\n",
      "ICCV, 2021. 4, 6, 12, 13\n",
      "\n",
      "[24] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-\n",
      "emplar fine-tuning for 3d human model fitting towards\n",
      "arXiv preprint\n",
      "in-the-wild 3d human pose estimation.\n",
      "arXiv:2004.03686, 2020. 7\n",
      "\n",
      "[25] Ken-ichi Kanatani. Motion segmentation by subspace sepa-\n",
      "\n",
      "ration and model selection. In ICCV, 2001. 2\n",
      "\n",
      "[26] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-\n",
      "In\n",
      "\n",
      "Image segmentation as rendering.\n",
      "\n",
      "shick. Pointrend:\n",
      "CVPR, 2020. 7\n",
      "\n",
      "[27] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,\n",
      "and Michael J Black. Pare: Part attention regressor for 3d\n",
      "human body estimation. In ICCV, 2021. 7, 8\n",
      "\n",
      "[28] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh Sax-\n",
      "ena. Learning human activities and object affordances from\n",
      "RGB-D videos. The International Journal of Robotics Re-\n",
      "search, 32(8):951–970, 2013. 2\n",
      "\n",
      "[29] Nilesh Kulkarni, Abhinav Gupta, David Fouhey, and Shub-\n",
      "ham Tulsiani. Articulation-aware canonical surface map-\n",
      "ping. In CVPR, 2020. 2\n",
      "\n",
      "[12] David F. Fouhey, Weicheng Kuo, Alexei A. Efros, and Jiten-\n",
      "dra Malik. From lifestyle VLOGs to everyday interactions.\n",
      "In CVPR, 2018. 3\n",
      "\n",
      "[30] Nilesh Kulkarni, Ishan Misra, Shubham Tulsiani, and Abhi-\n",
      "nav Gupta. 3D-RelNet: Joint object and relational network\n",
      "for 3D prediction. In ICCV, 2019. 6\n",
      "\n",
      "[13] R. Girdhar, D.F. Fouhey, M. Rodriguez, and A. Gupta.\n",
      "Learning a predictable and generative vector representation\n",
      "for objects. In ECCV, 2016. 1\n",
      "\n",
      "[31] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn\n",
      "Abbott, and Shuran Song. Category-level articulated object\n",
      "pose estimation. In CVPR, 2020. 1, 2, 6\n",
      "\n",
      "[14] Georgia Gkioxari, Ross Girshick, Piotr Dollar, and Kaiming\n",
      "He. Detecting and recognizing human-object interactions. In\n",
      "CVPR, 2018. 2\n",
      "\n",
      "[32] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,\n",
      "Bharath Hariharan, and Serge Belongie. Feature pyramid\n",
      "networks for object detection. In CVPR, 2017. 4\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\n",
      "Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C. Lawrence\n",
      "Zitnick. Microsoft coco: Common objects in context.\n",
      "In\n",
      "ECCV, 2014. 3, 4, 12\n",
      "\n",
      "[34] Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, and\n",
      "Jan Kautz. PlaneRCNN: 3D plane detection and reconstruc-\n",
      "tion from a single image. In CVPR, 2019. 1, 2, 4\n",
      "\n",
      "[35] Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, and Ya-\n",
      "sutaka Furukawa. Planenet: Piece-wise planar reconstruc-\n",
      "tion from a single rgb image. In CVPR, 2018. 2, 4\n",
      "\n",
      "[36] Liu Liu, Han Xue, Wenqiang Xu, Haoyuan Fu, and Cewu\n",
      "Lu. Towards real-world category-level articulation pose es-\n",
      "timation. IEEE Transactions on Image Processing, 2022. 2\n",
      "[37] Yizhou Liu, Fusheng Zha, Lining Sun, Jingxuan Li, Man-\n",
      "tian Li, and Xin Wang. Learning articulated constraints from\n",
      "a one-shot demonstration for robot manipulation planning.\n",
      "IEEE Access, 7:172584–172596, 2019. 1\n",
      "\n",
      "[38] David G Lowe. Distinctive image features from scale-\n",
      "International journal of computer vi-\n",
      "\n",
      "invariant keypoints.\n",
      "sion, 60(2):91–110, 2004. 12\n",
      "\n",
      "[39] Frank Michel, Alexander Krull,\n",
      "\n",
      "Eric Brachmann,\n",
      "Michael Ying Yang, Stefan Gumhold, and Carsten Rother.\n",
      "Pose estimation of kinematic chain instances via object\n",
      "coordinate regression. In BMVC, 2015. 2\n",
      "\n",
      "[40] Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav\n",
      "Gupta, and Shubham Tulsiani. Where2act: From pixels to\n",
      "actions for articulated 3d objects. In ICCV, 2021. 1, 2\n",
      "[41] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille,\n",
      "Nuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning\n",
      "disentangled signed distance functions for articulated shape\n",
      "representation. In ICCV, 2021. 1, 2\n",
      "\n",
      "[42] Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian\n",
      "Chang, and Jian Jun Zhang. Total3dunderstanding: Joint lay-\n",
      "out, object pose and mesh reconstruction for indoor scenes\n",
      "from a single image. In CVPR, 2020. 6\n",
      "\n",
      "[43] Claudia P´erez-D’Arpino and Julie A Shah. C-learn: Learn-\n",
      "ing geometric constraints from demonstrations for multi-step\n",
      "manipulation in shared autonomy. In ICRA, 2017. 1\n",
      "\n",
      "[44] Sudeep Pillai, Matthew R Walter, and Seth Teller. Learning\n",
      "articulated motions from visual demonstration. In RSS, 2014.\n",
      "1, 2\n",
      "\n",
      "[45] Shengyi Qian, Linyi Jin, and David F. Fouhey. Associa-\n",
      "tive3d: Volumetric reconstruction from sparse views.\n",
      "In\n",
      "ECCV, 2020. 6\n",
      "\n",
      "[46] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\n",
      "lor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\n",
      "Gkioxari. Accelerating 3d deep learning with pytorch3d.\n",
      "arXiv:2007.08501, 2020. 12\n",
      "\n",
      "[47] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n",
      "Faster r-cnn: Towards real-time object detection with region\n",
      "proposal networks. In Advances in neural information pro-\n",
      "cessing systems, pages 91–99, 2015. 3\n",
      "\n",
      "[48] Chris Rockwell and David F Fouhey. Full-body awareness\n",
      "\n",
      "from partial observations. In ECCV, 2020. 7, 8\n",
      "\n",
      "[50] Dandan Shan, Jiaqi Geng, Michelle Shu, and David Fouhey.\n",
      "Understanding human hands in contact at internet scale. In\n",
      "CVPR, 2020. 2, 3, 12\n",
      "\n",
      "[51] Gunnar A. Sigurdsson, G¨ul Varol, Xiaolong Wang, Ali\n",
      "Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in\n",
      "homes: Crowdsourcing data collection for activity under-\n",
      "standing. In ECCV, 2016. 2, 3, 8\n",
      "\n",
      "[52] Gunnar A Sigurdsson, G¨ul Varol, Xiaolong Wang, Ali\n",
      "Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in\n",
      "homes: Crowdsourcing data collection for activity under-\n",
      "standing. In ECCV, 2016. 5, 8, 12\n",
      "\n",
      "[53] Linda Smith and Michael Gasser. The development of em-\n",
      "bodied cognition: Six lessons from babies. Artificial life,\n",
      "11(1-2):13–29, 2005. 1\n",
      "\n",
      "[54] J¨urgen Sturm, Cyrill Stachniss, and Wolfram Burgard. A\n",
      "probabilistic framework for learning kinematic models of ar-\n",
      "ticulated objects. Journal of Artificial Intelligence Research,\n",
      "41:477–526, 2011. 1, 2\n",
      "\n",
      "[55] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans,\n",
      "Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam,\n",
      "Devendra Chaplot, Oleksandr Maksymets, et al. Habitat 2.0:\n",
      "Training home assistants to rearrange their habitat. arXiv\n",
      "preprint arXiv:2106.14405, 2021. 2\n",
      "\n",
      "[56] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\n",
      "\n",
      "transforms for optical flow. In ECCV, 2020. 2, 7, 8\n",
      "\n",
      "[57] Carlo Tomasi and Takeo Kanade. Shape and motion from im-\n",
      "age streams under orthography: a factorization method. In-\n",
      "ternational journal of computer vision, 9(2):137–154, 1992.\n",
      "2\n",
      "\n",
      "[58] Lorenzo Torresani, Aaron Hertzmann, and Chris Bregler.\n",
      "Nonrigid structure-from-motion: Estimating shape and mo-\n",
      "tion with hierarchical priors. IEEE transactions on pattern\n",
      "analysis and machine intelligence, 30(5):878–892, 2008. 2\n",
      "\n",
      "[59] Shubham Tulsiani, Saurabh Gupta, David F Fouhey,\n",
      "Alexei A Efros, and Jitendra Malik. Factoring shape, pose,\n",
      "and layout from the 2D image of a 3D scene. In CVPR, 2018.\n",
      "6\n",
      "\n",
      "[60] G¨ul Varol, Javier Romero, Xavier Martin, Naureen Mah-\n",
      "mood, Michael J. Black, Ivan Laptev, and Cordelia Schmid.\n",
      "Learning from synthetic humans. In CVPR, 2017. 4, 7\n",
      "[61] Gul Varol, Javier Romero, Xavier Martin, Naureen Mah-\n",
      "mood, Michael J Black, Ivan Laptev, and Cordelia Schmid.\n",
      "Learning from synthetic humans. In Proceedings of the IEEE\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pages 109–117, 2017. 16\n",
      "\n",
      "[62] John YA Wang and Edward H Adelson. Representing mov-\n",
      "ing images with layers. IEEE transactions on image process-\n",
      "ing, 3(5):625–638, 1994. 2\n",
      "\n",
      "[63] Xiaolong Wang, Ali Farhadi, and Abhinav Gupta. Actions ˜\n",
      "\n",
      "transformations. In CVPR, 2016. 2\n",
      "\n",
      "[64] Xiaolong Wang, David Fouhey, and Abhinav Gupta. Design-\n",
      "ing deep networks for surface normal estimation. In CVPR,\n",
      "2015. 6\n",
      "\n",
      "[49] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary\n",
      "Bradski. Orb: An efficient alternative to sift or surf. In ICCV,\n",
      "2011. 3, 12\n",
      "\n",
      "[65] Xiaolong Wang, David F. Fouhey, and Abhinav Gupta. De-\n",
      "signing deep networks for surface normal estimation.\n",
      "In\n",
      "CVPR, 2015. 1\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "[66] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qin-\n",
      "ping Zhao, and Kai Xu. Shape2motion: Joint analysis of\n",
      "motion parts and attributes from 3d shapes. In CVPR, 2019.\n",
      "2\n",
      "\n",
      "[67] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\n",
      "Lo, and Ross Girshick. Detectron2. https://github.\n",
      "com/facebookresearch/detectron2, 2019. 4, 12\n",
      "[68] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao\n",
      "Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,\n",
      "He Wang, et al. Sapien: A simulated part-based interactive\n",
      "environment. In CVPR, 2020. 2, 6, 7, 8, 16\n",
      "\n",
      "[69] Xiang Xu, Hanbyul Joo, Greg Mori, and Manolis Savva.\n",
      "interactions from\n",
      "\n",
      "D3d-hoi: Dynamic 3d human-object\n",
      "videos. arXiv preprint arXiv:2108.08420, 2021. 1, 2, 7, 8\n",
      "\n",
      "[70] Fengting Yang and Zihan Zhou. Recovering 3d planes from\n",
      "a single image via convolutional neural networks. In ECCV,\n",
      "2018. 2\n",
      "\n",
      "[71] Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, and\n",
      "Shenghua Gao. Single-image piece-wise planar 3d recon-\n",
      "struction via associative embedding. In CVPR, 2019. 2\n",
      "[72] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\n",
      "From recognition to cognition: Visual commonsense reason-\n",
      "ing. In CVPR, 2019. 3\n",
      "\n",
      "[73] Jason Y. Zhang, Panna Felsen, Angjoo Kanazawa, and Jiten-\n",
      "dra Malik. Predicting 3d human dynamics from video. In\n",
      "ICCV, 2019. 2\n",
      "\n",
      "[74] Kai Zhao, Qi Han, Chang-Bin Zhang, Jun Xu, and Ming-\n",
      "Ming Cheng. Deep hough transform for semantic line detec-\n",
      "tion. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 2021. 6, 8\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "A. Implementation Details\n",
      "\n",
      "Detector. Our network architecture is shown in Table 3.\n",
      "We use Detectron2 [67] to implement our articulation de-\n",
      "tection and take the codebase from SparsePlanes [23]. Our\n",
      "articulation head predicts rotation and translation axis sep-\n",
      "arately. Each of the branch takes the RoI feature from the\n",
      "backbone and uses four convolutional layers with 256 chan-\n",
      "nels and two linear layers to regress the axes. The rotation\n",
      "branch predicts a three dimensional vector and the transla-\n",
      "tion branch predicts a two dimensional vector. While they\n",
      "train the model on Matterport3D [2], we start from COCO\n",
      "pretraining and train the model on our Internet videos. The\n",
      "training is run on a single RTX 2080Ti.\n",
      "\n",
      "Temporal Optimization. For tracking, we use 0.5 as our\n",
      "IoU threshold. For articulation model fitting, we use the\n",
      "ScanNet camera intrinsics as our assumed camera intrisics,\n",
      "since the plane and depth heads of the model is trained on\n",
      "ScanNet [7]. We use PyTorch to implement the temporal\n",
      "optimization. For 3D transformations we use PyTorch3D\n",
      "[46] so that it is compatible. The optimization is parallel\n",
      "and runs on 8 GTX 1080Ti gpus of an internal cluster to\n",
      "save inference time and it can be run on a single gpu.\n",
      "\n",
      "B. Data Collection Pipeline\n",
      "\n",
      "Our semi-automatic data collection consists of an auto-\n",
      "matic pipeline to download and filter Internet videos to re-\n",
      "move clear negatives, and a manual annotation to label ar-\n",
      "ticulated objects. We also discuss how we filter Charades\n",
      "dataset [52] since it involves slightly different steps.\n",
      "\n",
      "B.1. Filtering Internet videos\n",
      "\n",
      "Youtube queries. We start with 10 initial common articu-\n",
      "lated objects in our daily life: door, laptop, oven, refrigera-\n",
      "tor, washing machine, dishwasher, microwave oven, drawer,\n",
      "cabinet and box. Using the combination of words, we make\n",
      "a list of queries for each initial category, e.g. “best laundry\n",
      "tips”, following 100DOH [50]. To improve the number of\n",
      "videos we can find on the Internet and the diversity of the\n",
      "dataset, we also translate the queries into Chinese, Japanese,\n",
      "Hindi and Spanish. We search these queries on Youtube and\n",
      "download related Creative Commons videos.\n",
      "\n",
      "Converting videos to shots. Within these videos, we find\n",
      "stationary continuous shots by fitting homographies [16] on\n",
      "ORB [49] features. In practice, we find ORB [49] are much\n",
      "faster and slightly more robust than SIFT [38] features, so\n",
      "it saves a lot of computing time.\n",
      "\n",
      "Filtering videos based on interaction. A lot of station-\n",
      "ary shots does not contain any people or objects of interest.\n",
      "We further filter out video shots based on a hand interac-\n",
      "tion detector trained on 100K+ frames of Internet data [50].\n",
      "\n",
      "In practice, we find it works well and we believe it is be-\n",
      "cause [50] is also trained on Internet data. For each video\n",
      "shot, we run the hand interaction detector on frames evenly\n",
      "sampled every 1 second. We remove video shots which do\n",
      "not have hand interactions at all.\n",
      "\n",
      "Filtering categories of interests. We further filter object\n",
      "of interests by an object detector trained on COCO [33] and\n",
      "LVIS [15]. We use the pretrained model of Faster R-CNN\n",
      "(X101-FPN, 3x) from Detectron2 [67]. For categories that\n",
      "COCO does not have annotation (e.g. washing machines),\n",
      "we use LVIS since it has much more categories. Especially,\n",
      "LVIS does not have annotations for doors, and we use door-\n",
      "knob instead.\n",
      "\n",
      "B.2. Annotating Articulated Objects\n",
      "\n",
      "Finally, we use annotate articulated objects using crowd-\n",
      "sourcing. We split video shots into 3s clips, where the fps\n",
      "is 30. Therefore, there are 90 frames per clip.\n",
      "\n",
      "We use Hive 1 as our data annotation platform. We in-\n",
      "clude the screenshot and estimated hourly pay for each step,\n",
      "since these steps are separate from each other. The hourly\n",
      "pay is a rough estimation since we only have limited worker\n",
      "statistics provided by Hive and we do not manage it our-\n",
      "selves.\n",
      "\n",
      "Recognizing articulated clips. The first step is to judge\n",
      "if the video clips have objects which are being articulated.\n",
      "This is a binary classification question. We show 9 key\n",
      "frames of the video shot (sample every 10 frames) and ask\n",
      "workers to classify. The screenshot is shown in Figure 7.\n",
      "We pay $0.015 for each clip, each clip is annotated by about\n",
      "2 workers (which is managed by Hive based on consensus\n",
      "and out of our control), and we estimate they can annotate\n",
      "8 clips per minute. Therefore, the estimated hourly pay is\n",
      "0.015 · 8 · 60/2 = $3.6.\n",
      "\n",
      "Annotating bounding boxes of articulated objects. After\n",
      "labelling positive video clips, we annotate bounding boxes\n",
      "of objects which are being articulated. We also ask workers\n",
      "to specify the object is being rotated or translated. We only\n",
      "annotate on 9 key frames of the video clip, since consecutive\n",
      "frames tend to be similar. The screenshot is shown in Figure\n",
      "8. We pay $0.14 for each clip, each clip is annotated by\n",
      "about 2 workers, and we estimate they can annotate 1.5 clips\n",
      "per minute. Therefore, the estimated hourly pay is 0.14·1.5·\n",
      "60/2 = $6.3.\n",
      "\n",
      "Annotating rotation axis. For objects which are annotated\n",
      "to be rotated, we annotate their rotation axes. We ask work-\n",
      "ers to draw a line to represent the 2D rotation axis. The\n",
      "screenshot is shown in Figure 9. We pay $0.04 for each\n",
      "clip, each clip is annotated by about 2 workers, and we es-\n",
      "\n",
      "1https://thehive.ai/\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Table 3. Overall architecture for our proposed network. The backbone, RPN and plane branches are identical to [23]. The RPN predicts a\n",
      "bounding box for each of A anchors in the input feature map. C is the number of categories (here = 2 for rotation and translation). We use\n",
      "class agnostic mask because the mask head is trained on ScanNet. TConv is a transpose convolution with stride 2. ReLU is used between\n",
      "all Linear, Conv and TConv operations. Depth branch uses Conv and Deconv layers to generate a depthmap with the same resolution as\n",
      "the input image.\n",
      "\n",
      "Index\n",
      "\n",
      "Inputs Operation\n",
      "\n",
      "Inputs\n",
      "(1)\n",
      "(2)\n",
      "\n",
      "Input Image\n",
      "Backbone: ResNet50-FPN\n",
      "RPN\n",
      "(2),(3) RoIAlign\n",
      "\n",
      "(1)\n",
      "(2)\n",
      "(3)\n",
      "(4)\n",
      "(5)\n",
      "(6)\n",
      "(7)\n",
      "(8)\n",
      "(9)\n",
      "(10)\n",
      "\n",
      "(4)\n",
      "(4)\n",
      "(4)\n",
      "(4)\n",
      "(4)\n",
      "(2)\n",
      "\n",
      "Box: 2×downsample, Flatten, Linear(7 × 7 × 256 → 1024), Linear(1024 → 5C)\n",
      "Mask: 4 × Conv(256 → 256, 3 × 3), TConv(256 → 256, 2 × 2, 2), Conv(256 → 1 × 1)\n",
      "Normal: 4 × Conv(256 → 256, 3 × 3), Linear(14 × 14 × 256 → 1024), Linear(1024 → 3)\n",
      "Rotation Axis: 4 × Conv(256 → 256, 3 × 3), Linear(14 × 14 × 256 → 1024), Linear(1024 → 3)\n",
      "Translation Axis: 4 × Conv(256 → 256, 3 × 3), Linear(14 × 14 × 256 → 1024), Linear(1024 → 2)\n",
      "Depth\n",
      "\n",
      "Output shape\n",
      "\n",
      "H × W × 3\n",
      "h × w × 256\n",
      "h × w × A × 4\n",
      "14 × 14 × 256\n",
      "C × 5\n",
      "1 × 28 × 28\n",
      "1 × 3\n",
      "C × 3\n",
      "C × 2\n",
      "H × W × 1\n",
      "\n",
      "Figure 7. The screenshot of recognize articulated clips.\n",
      "\n",
      "timate they can annotate 4 axes per minute. Therefore, the\n",
      "estimated hourly pay is 0.04 · 4 · 60/2 = $4.8.\n",
      "\n",
      "Annotating translation axis. For objects which are anno-\n",
      "tated to be translated, we annotate their translation direc-\n",
      "tion. We also ask workers to draw a line to represent the\n",
      "2D rotation direction. However, since translation is only\n",
      "related to the angle of the line and does not need the line\n",
      "offset, we draw a circle at the center of the bounding box\n",
      "and ask workers to start there. The screenshot is shown in\n",
      "Figure 10. The estimated hourly pay is $4.8, which is the\n",
      "same as annotating rotation axis since it is defined as the\n",
      "same task “line segment” on Hive.\n",
      "\n",
      "Annotating surface normals. All bounding boxes and ar-\n",
      "ticulation axes are annotated in 2D. However, in this paper,\n",
      "we are interested in 3D object articulation. Thus, for the\n",
      "test set, we also annotate the o annotate the surface normal\n",
      "of the plane following [4], so we can evaluate how well our\n",
      "model can learn 3D properties. Since the annotation of sur-\n",
      "face normals are not available on Hive and we only need\n",
      "surface normals on the test set for evaluation purpose, we\n",
      "do all surface normals annotations ourselves. The screen-\n",
      "shot is show in Figure 11.\n",
      "\n",
      "Finally, we postprocess the dataset to make sure the\n",
      "dataset does not have offensive content, cartoons, and any\n",
      "videos depicting children. Since the distribution is unbal-\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Figure 8. The screenshot of annotating bounding boxes.\n",
      "\n",
      "Figure 9. The screenshot of annotating rotation axes.\n",
      "\n",
      "anced and negative examples are much more than positive\n",
      "ones, we only sample a negative clip with hand interaction\n",
      "from the same video shot of positive clips.\n",
      "\n",
      "B.3. Annotating Charades Dataset\n",
      "\n",
      "We test generalization of our method on Charades with-\n",
      "out additional training. Data is prepared and annotated us-\n",
      "ing the following process on the original Charades test set.\n",
      "\n",
      "The Charades test set contains 1863 videos of people\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Figure 10. The screenshot of annotating translation axes.\n",
      "\n",
      "Figure 11. The screenshot of annotating surface normals.\n",
      "\n",
      "acting out designated scripts. Videos are typically short at\n",
      "around 30 seconds. The dataset contains script informa-\n",
      "tion and additional annotation, which can be used to filter\n",
      "videos that are highly likely to contain articulation. After\n",
      "\n",
      "initial filtering, we split filtered videos into clips of a pre-\n",
      "defined length. These clips are then annotated as to whether\n",
      "they contain articulation, for articulation bounding box lo-\n",
      "cation (if applicable), and articulation angle (if applicable).\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Figure 12. Random examples from Sapien renderings.\n",
      "\n",
      "Annotation is performed using a similar setup to YouTube\n",
      "via Hive. Because Charades is small, we use all clips which\n",
      "Hive workers have labeled as containing articulation.\n",
      "Filtering criteria. Charades contains videos with acting\n",
      "information, which we use to perform filtering. Each video\n",
      "has corresponding categorical actions that can be used to\n",
      "find dense articulation instances. These categorical actions\n",
      "fall into 157 categories such as ”Holding some clothes”\n",
      "or ”Opening a bag”, and are annotated on a one-tenth of\n",
      "a second basis. For example, one given video may have\n",
      "two corresponding actions ”Holding some clothes” and\n",
      "”Opening a bag”, which correspond to seconds 1.2 - 11.7\n",
      "and 12.3 - 18.3, correspondingly. We selecct video clips\n",
      "orresponding to eight categorical actions for our dataset:\n",
      "”Opening a door”, ”Closing a door”, ”Opening a laptop”,\n",
      "”Closing a laptop”, ”Opening a closet/cabinet”, ”Closing\n",
      "a closet/cabinet”, ”Opening a refrigerator”, and ”Closing a\n",
      "refrigerator”.\n",
      "Gathering filtered clips. To find corresponding video\n",
      "clips, we first calculate the middle of each action – e.g. for\n",
      "the 1.2 - 11.7 interval this would be 6.45. Next, we select\n",
      "a 7.5 seconds both greater than and less than the middle of\n",
      "the action. This is subject to beginning and ending of video,\n",
      "and we remove overlapping clips. In our example, ”Hold-\n",
      "ing some clothes” has a middle of 6.45 seconds, so the clip\n",
      "would begin at 0 seconds and end at 13.95 seconds. The\n",
      "second clip has a middle of 15.3, and cannot overlap with\n",
      "the first, so would start at 13.95 seconds and end at 22.8.\n",
      "This totals 649 filtered clips of <= 15 seconds.\n",
      "Splitting video clips. Given a set of video clips which cor-\n",
      "respond to the selected categorical actions, we next split\n",
      "\n",
      "16\n",
      "\n",
      "clips into 3 second clips to be used for a standardized articu-\n",
      "lation framework. Any clip less than 3 seconds is truncated.\n",
      "This results in 2232 3-second mini-clips.\n",
      "\n",
      "C. Rendering Sapien data\n",
      "\n",
      "In our experiments, we test whether the model trained\n",
      "on synthetic data transfer to Internet videos using Sapien\n",
      "renderings [68]. Here we provide additional details about\n",
      "rendering and example images. Examples of our renderings\n",
      "are shown in Figure 12.\n",
      "\n",
      "To generate these results, we first randomly sam-\n",
      "ple 3D objects with articulation. We filtered 1053 ob-\n",
      "jects of 18 caterories with movable planes from PartNet-\n",
      "Mobility Dataset [68]. 18 categories are: “Box”, “Dish-\n",
      "washer”, “Display”, “Door”, “FoldingChair”, “Laptop”,\n",
      "“Microwave”, “Oven”, “Phone”, “Refrigerator”, “Safe”,\n",
      "“StorageFurniture”, “Suitcase”, “Table”, “Toilet”, “Trash-\n",
      "Can”, “WashingMachine”, and “Window”. They have sig-\n",
      "nificant overlapping with our queries to generate Internet\n",
      "videos, since these objects are common objects which can\n",
      "be articulated. We control its rotation or translation and ren-\n",
      "der ground truth depth, surface normal, mask, motion type\n",
      "and 3D rotation axis. The outputs are object articulation\n",
      "videos without backgrounds.\n",
      "\n",
      "To mimic real 3D scenes, we blend random backgrounds.\n",
      "Otherwise the detection problem becomes trivial. We use\n",
      "ScanNet [7] images with synthetic humans [61] used to train\n",
      "our approach, to ensure the fair comparison.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "\n",
    "print(array_authors[0])\n",
    "print(array_pdf_text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1220700",
   "metadata": {},
   "source": [
    "### Extract all word after the term: \"References\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e6468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large-Scale Pre-training for Person Re-identification with Noisy Labels\\n\\nDengpan Fu1 Dongdong Chen3 Hao Yang2\\nLei Zhang4 Houqiang Li1\\n\\nLu Yuan3\\n\\nJianmin Bao2*\\n\\nFang Wen 2 Dong Chen2\\n\\n1University of Science and Technology of China\\n\\nfdpan@mail.ustc.edu.cn\\n\\n2Microsoft Research, 3Microsoft Cloud AI, 4IDEA\\nlihq@ustc.edu.cn\\n\\ncddlyf@gmail.com\\n\\n{jianbao,haya,luyuan,fangwen,doch}@microsoft.com, leizhang@idea.edu.cn\\n\\nAbstract\\n\\nThis paper aims to address the problem of pre-training\\nfor person re-identification (Re-ID) with noisy labels. To\\nsetup the pre-training task, we apply a simple online multi-\\nobject tracking system on raw videos of an existing un-\\nlabeled Re-ID dataset “LUPerson” and build the Noisy\\nLabeled variant called “LUPerson-NL”. Since theses ID\\nlabels automatically derived from tracklets inevitably con-\\ntain noises, we develop a large-scale Pre-training frame-\\nwork utilizing Noisy Labels (PNL), which consists of three\\nlearning modules: supervised Re-ID learning, prototype-\\nbased contrastive learning, and label-guided contrastive\\nlearning. In principle, joint learning of these three mod-\\nules not only clusters similar examples to one prototype,\\nbut also rectifies noisy labels based on the prototype as-\\nsignment. We demonstrate that learning directly from raw\\nvideos is a promising alternative for pre-training, which\\nutilizes spatial and temporal correlations as weak super-\\nvision. This simple pre-training task provides a scalable\\nway to learn SOTA Re-ID representations from scratch on\\n“LUPerson-NL” without bells and whistles. For example,\\nby applying on the same supervised Re-ID method MGN,\\nour pre-trained model improves the mAP over the unsu-\\npervised pre-training counterpart by 5.7%, 2.2%, 2.3% on\\nCUHK03, DukeMTMC, and MSMT17 respectively. Under\\nthe small-scale or few-shot setting, the performance gain is\\neven more significant, suggesting a better transferability of\\nthe learned representation. Code is available at https:\\n//github.com/DengpanFu/LUPerson-NL.\\n\\n1. Introduction\\n\\nA large high-quality labeled dataset for person re-\\nidentification (Re-ID) is labor intensive and costly to cre-\\nate. Existing fully labeled datasets [25, 52, 58, 61] for per-\\nson Re-ID are all of limited scale and diversity compared\\nto other vision tasks. Therefore, model pre-training be-\\n\\n*Corresponding author.\\n\\n(a) Market1501 with MGN\\n\\n(b) Market1501 with IDE\\n\\n(c) DukeMTMC with MGN\\n\\n(d) DukeMTMC with IDE\\n\\nFigure 1. Comparing person Re-ID performances of three pre-\\ntrained models on two methods (IDE [59] and MGN [51]). Re-\\nsults are reported on Market1501 and DukeMTC, with different\\nscales under the small-scale setting. IN.sup. refers to the model\\nsupervised pre-trained on ImageNet, LUP.unsup. is the model un-\\nsupervised pre-trained on LUPserson, and LUPnl.pnl. is the model\\npre-trained on our LUPerson-NL dataset using our proposed PNL.\\n\\ncomes a crucial approach to achieve good Re-ID perfor-\\nmance. However, due to the lack of large-scale Re-ID\\ndataset, most previous methods simply use the models pre-\\ntrained on the crowd-labeled ImageNet dataset, resulting in\\na limited improvement because of the big domain gap be-\\ntween generic images in ImageNet and person-focused im-\\nages desired by the Re-ID task. To mitigate this problem,\\nthe recent work [12] has demonstrated that unsupervised\\npre-training on a web-scale unlabeled Re-ID image dataset\\n“LUPerson” (sub-sampled from massive streeview videos)\\nsurpasses that of pre-training on ImageNet.\\n\\nIn this paper, our hypothesis is that scalable ReID pre-\\ntraining methods that learn directly from raw videos can\\ngenerate better representations. To verify it, we propose the\\nnoisy labels guided person Re-ID pre-training, which lever-\\n\\n\\x0cages the spatial and temporal correlations in videos as weak\\nsupervision. This supervision is nearly cost-free, and can be\\nachieved by the tracklets of a person over time derived from\\nany multi-object tracking algorithm, such as [56]. In par-\\nticular, we track each person in consecutive video frames,\\nand automatically assign the tracked persons in the same\\ntracklet to the same Re-ID label and vice versa. Enabled by\\nthe large amounts of raw videos in LUPerson [12], publicly\\navailable data of this form on the internet, we create a new\\nvariant named “LUPerson-NL” with derived pseudo Re-ID\\nlabels from tracklets for pre-training with noisy labels. This\\nvariant totally consists of 10M person images from 21K\\nscenes with noisy labels of about 430K identities.\\n\\nWe demonstrate that contrastive pre-training of Re-ID is\\nan effective method of learning from this weak supervision\\nat large scale. This new Pre-training framework utilizing\\nNoisy Labels (PNL) composes three learning modules: (1)\\na simple supervised learning module directly learns from\\nRe-ID labels through classification; (2) a prototype-based\\ncontrastive learning module helps cluster instances to the\\nprototype which is dynamically updated by moving aver-\\naging the centroids of instance features, and progressively\\nrectify the noisy labels based on the prototype assignment.\\nand (3) a label-guided contrastive learning module utilizes\\nthe rectified labels subsequently as the guidance. In contrast\\nto the vanilla momentum contrastive learning [7,12,19] that\\ntreats only features from the same instance as positive sam-\\nples, our label-guided contrastive learning uses the rectified\\nlabels to distinguish positive and negative samples accord-\\ningly, leading to a better performance. In principle, joint\\nlearning of these three modules make the consistency be-\\ntween the prototype assignment from instances and the high\\nconfident (rectified) labels, as possible as it can.\\n\\nThe experiments show that our PNL model achieves re-\\nmarkable improvements on various person Re-ID bench-\\nmarks. Figure 1 indicates that the performance gain from\\nour pre-trained models is consistent on different scales of\\ntraining data. For example, upon the strong MGN [51]\\nimproves the mAP by\\nbaseline, our pre-trained model\\n4.4%, 4.9% on Market1501 and DukeMTMC over the Im-\\nageNet supervised one, and 0.9%, 2.2% over the unsuper-\\nvised pre-training baseline [12]. Moreover, the gains are\\neven larger under the small-scale and few-shot settings,\\nwhere the labeled Re-ID data are extremely limited. To the\\nbest of our knowledge, we are the first to show that large-\\nscale noisy label guided pre-training can significantly ben-\\nefit person Re-ID task.\\n\\nOur key contributions can be summarized as follows:\\n\\n• We propose noisy label guided pre-training for person Re-\\nID, which incorporates supervised learning, prototype-\\nbased contrastive learning, label-guided contrastive learn-\\ning and noisy label rectification to a unified framework.\\n• We construct a large-scale noisy labeled person Re-ID\\n\\ndataset “LUPerson-NL” as a new variant of “LUPerson”.\\nIt is by far the largest noisy labeled person Re-ID dataset\\nwithout any human labeling effort.\\n\\n• Our models pre-trained on LUPerson-NL push the state-\\nof-the-art results on various public benchmarks to a new\\nlimit without bells and whistles.\\n\\n2. Related Work\\nSupervised Person Re-ID. Most studies of person Re-ID\\nemploy supervised learning. Some [6, 21, 55] introduce a\\nhard triplet loss on the global feature, ensuring a closer fea-\\nture distance for the same identity, while some [45, 59, 60]\\nimpose classification loss to learn a global feature from\\nthe whole image. There are also some other works that\\nlearn part-based local features with separate classification\\nlosses. For example, Suh et al. [46] presented part-aligned\\nbi-linear representations and Sun et al. [48] represented fea-\\ntures as horizontal strips. Recent approaches investigate\\nlearning invariant features concerning views [34], resolu-\\ntions [31], poses [32], domains [22,23], or exploiting group-\\nwise losses [36] or temporal information [18, 27] to im-\\nprove performance. The more advantageous results on pub-\\nlic benchmarks are achieved by MGN [51], which learns\\nboth global and local features with multiple losses. In [40],\\nQian et al further demonstrated the potential of generating\\ncross-view images for person re-indentification conditioned\\non normalized poses. In this paper, we focus on model pre-\\ntraining, and our pre-trained models can be applied to these\\nrepresentative methods and boost their performance.\\nUnsupervised Person Re-ID. To alleviate the lack of pre-\\ncise annotations, some works resort to unsupervised train-\\ning on unlabeled datasets. For example, MMCL [49] for-\\nmulates unsupervised person Re-ID as a multi-label classi-\\nfication to progressively seek true labels. BUC [33] jointly\\noptimizes the network and the sample relationship with\\na bottom-up hierarchical clustering. MMT [14] collabo-\\nratively trains two networks to refine both hard and soft\\npseudo labels. SpCL [15] designs a hybrid memory to\\nunify the representations for clustering and instance-wise\\ncontrastive learning. Both MMT [14] and SpCL [15] rely\\non explicit clustering of features from the whole training\\nset, making them quite inefficient on large datasets like\\nMSMT17. Since the appearance ambiguity is difficult to\\naddress without supervision, these unsupervised methods\\nhave limited performance. One alternative to address this\\nissue is introducing model pre-training on large scale data.\\nInspired by the success of self-supervised representation\\nlearning [4,5,7,17,19,28,53], Fu et al. [12] proposed a large\\nscale unlabeled Re-ID dataset, LUPerson, and illustrated\\nthe effectiveness of its unsupervised pre-trained models. In\\nthis work, we further try to make use of noisy labels from\\nvideo tracklets to improve the pre-training quality through\\nlarge-scale weakly-supervised pre-training.\\nWeakly Supervised Person Re-ID. Several approaches\\n\\n\\x0calso employ weak supervision in person Re-ID training.\\nInstead of requiring bounding boxes within each frame,\\nMeng et al. [38] rely on precise video-level labels, which\\nreduces annotation cost but still need manual efforts to la-\\nbel videos. On the contrary, we resort to noisy labels that\\ncan be automatically generated from tracklets on a much\\nlarger scale. Some [8, 29, 50] also leverage tracklets to su-\\npervise the training of Re-ID tasks. But unlike these ap-\\nproaches, we are proposing a large-scale pre-training strat-\\negy for person Re-ID, by both building a new very large-\\nscale dataset and devising a new pre-training framework:\\nthe new dataset, LUPerson-NL, is even larger than LU-\\nPerson [12] and has large amount of noisy Re-ID labels;\\nThe new framework, PNL, combines supervised learning,\\nlabel-guided contrastive learning and prototype based con-\\ntrastive learning to exploit the knowledge under large-scale\\nnoise labels. Most importantly, our pre-trained models have\\ndemonstrated remarkable performance and generalization\\nability, helping achieve state-of-the-art results superior to\\nall existing methods on public person Re-ID benchmarks.\\n3. LUPerson-NL: LUPerson With Noisy Labels\\nSupervised models based on deep networks are always\\ndata-hungry, but the labeled data they rely on are expen-\\nsive to acquire.\\nIt is a tremendous issue for person Re-\\nID task, since the human labelers need to check across\\nmultiple views to ensure the correctness of Re-ID labels.\\nThe data shortage is partially alleviated by a recently pub-\\nlished dataset, LUPerson [12], a dataset of unlabeled per-\\nson images with a significantly larger scale than previous\\nperson Re-ID datasets. Unsupervised pre-trained models\\n[12] on LUPerson have demonstrated remarkable effective-\\nness without utilizing additional manual annotations, which\\narouses our curiosity: can we further improve the perfor-\\nmance of pre-training directly by utilizing temporal cor-\\nrelation as weak supervision? To verify this, we build a\\nnew variant of LUPerson on top of the raw videos from\\nLUPerson and assign label to each person image with au-\\ntomatically generated tracklet. We name it LUPerson-NL\\nIt consists of 10M\\nwith NL standing for Noisy Labels.\\nimages with about 430K identities collected from 21K\\nscenes. To our best knowledge, this is the largest person\\nRe-ID dataset constructed without human labelling by far.\\nOur LUPerson-NL will be released for scientific research\\nonly, while any usage for other purpose is forbidden.\\n3.1. Constructing LUPerson-NL\\n\\nWe utilize the off-the-shelf tracking algorithm [56] 1 to\\ndetect persons and extract person tracklets from the same\\nraw videos of [12]. We assign each tracklet with a unique\\nclass label. The detection is not perfect: e.g. the bounding\\nboxes may only cover partial bodies without heads or upper\\nparts. Human pose estimation [47] is thus appended that\\n\\n1FairMOT: https://github.com/ifzhang/FairMOT\\n\\nFigure 2.\\n(X, Y ) indicates Y % of identities each has less than X images.\\n\\nIdentity distribution of LUPerson-NL. A curve point\\n\\nFigure 3. Besides the correctly labeled identities as shown by (a),\\nthere are two types of labeling errors in LUPerson-NL. Noise-I:\\nsame person labeled as different identities, e.g. D, E and F shown\\nin (b). Noise-II: different persons labeled as the same identity,\\ne.g. G shown in (c).\\n\\nhelps filter out imperfect boxes by predicting landmarks.\\n\\nWe track every person in the video frame by frame. In\\norder to guarantee both the sufficiency and diversity, we\\nadopt the following strategy:\\ni) We first remove the per-\\nson identities that appear in too few frames, i.e. no more\\nthan 200; ii) Within the tracklet of each identity, we then\\nperform sampling with a rate of one image per 20 frames\\nto reduce the number of duplicated images. Thus we can\\nmake sure that there would be at least 10 images associat-\\ning to each identity. Through this filtering procedure, we\\nhave collected 10, 683, 716 images of 433, 997 identities in\\ntotal. They belong to 21, 697 videos which are less than\\nthe videos that [12] uses, due to our extra filtering strat-\\negy for more reliable identity labels. Thus, LUPerson-NL\\nis very different from LUPerson, as it adopts very different\\nsampling and post-processing strategies, not to mention the\\nnoisy labels driven from the spatial-temporal information.\\n3.2. Properties of LUPerson-NL\\n\\nLUPerson-NL is advantageous in following aspects:\\nLarge amount of images and identities. We detail the\\nstatistics of existing popular person Re-ID datasets in Table\\n1. As we can see, the proposed LUPerson-NL, with over\\n10M images and 433K noisy labeled identities, is the sec-\\nond largest among the listed. Indeed, SYSU30K has more\\nimages, but it extracts images from only 1K TV program\\nvideos frame by frame, making it less competitive in vari-\\nability and less compatible in practice, the pre-training per-\\nformance comparison can be found at supplementary mate-\\nrials. Besides, LUPerson-NL was constructed without hu-\\nman labeling effort, making it more suitable to scale-up.\\n\\n102030405060708090#ofpersonimages0255075100%ofidentities𝐴𝐵𝐶(a) Correctly labeled identities(b) Noise-I(c) Noise-II𝐷𝐸𝐹𝐺\\x0cDatasets\\nVIPeR [16]\\nGRID [35]\\nCUHK03 [30]\\nMarket [58]\\nAirport [25]\\nDukeMTMC [61]\\nMSMT17 [52]\\nSYSU30K [50]\\nLUPerson [12]\\nLUPerson-NL\\n\\n#scene\\n#images\\n2\\n1,264\\n8\\n1,275\\n2\\n14, 096\\n6\\n32, 668\\n6\\n39, 902\\n8\\n36, 411\\n15\\n126, 441\\n1,000\\n29,606,918\\n46, 260\\n4, 180, 243\\n10, 683, 716 21, 697 ≃ 433, 997\\n\\n#persons\\n632\\n1,025\\n1, 467\\n1, 501\\n9, 651\\n1, 852\\n4, 101\\n30,508\\n> 200k\\n\\nlabeled environment\\n\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\n\\n-\\nsubway\\ncampus\\ncampus\\nairport\\ncampus\\ncampus\\n\\nweakly TV program\\n\\nno\\nnoisy\\n\\nvary\\nvary\\n\\ncamera view\\nfixed\\nfixed\\nfixed\\nfixed\\nfixed\\nfixed\\nfixed\\ndynamic\\ndynamic\\ndynamic\\n\\ndetector\\nhand\\nhand\\nDPM [11]+hand\\nDPM [11]+hand\\nACF [10]\\nHand\\nFasterRCNN [42]\\nYOLOv2\\nYOLOv5\\nFairMOT [56]\\n\\ncrop size\\n128 × 48\\nvary\\nvary\\n128 × 64\\n128 × 64\\nvary\\nvary\\nvary\\nvary\\nvary\\n\\nTable 1. Comparing statistics among existing popular Re-ID datasets. LUPerson-NL is by far the largest Re-ID dataset with better diversity\\nwithout human labeling effort. SYSU30K is partly annotated by human annotator.\\n\\n4. PNL: Pre-training with Noisy Labels for\\n\\nPerson Re-ID\\n\\nBased on the new LUPerson-NL dataset with large scale\\nnoisy labels, we devise a novel Pretraining framework with\\nNoisy Labels for person Re-ID, namely PNL.\\n\\nDenote all\\n\\nthe data samples from LUPerson-NL as\\n{(xi, yi)}n\\ni=1, with n being the size of the dataset, xi a\\nperson image and yi ∈ {1, . . . , K} its associated identity\\nlabel. Here K represents the number of all identities that\\nare recorded in LUPerson-NL.\\n\\nInspired by recent methods [4, 5, 7, 17, 19, 28], our PNL\\nframework adopts Siamese networks that have been fully in-\\nvestigated for contrastive representation learning. As shown\\nby Figure 4, given an input person image xi, we first per-\\nform two randomly selected augmentations (T, T ′), pro-\\nducing two augmented images ( ˜xi, ˜x′\\ni). We feed one of\\nthem, ˜xi, into an encoder Eq to get a query feature qi;\\nwhile the other one, ˜x′\\ni, is fed into another encoder Ek\\nto get a key feature ki. Following [19], we design Ek to\\nbe a momentum version of Eq, i.e. the two encoders Ek\\nand Eq share the same network structure, but with different\\nweights. The weights in Ek are exponential moving aver-\\nages of the weights in Eq. During training, weights of Ek\\nare refreshed through a momentum update from Eq. And\\nthe detailed algorithm can be found at supplementary mate-\\nrials.\\n4.1. Supervised Classification\\nSince the raw labels {yi}n\\n\\ni=1 in LUPerson-NL contain\\nlots of noises as illustrated in previous section, they have\\nto be rectified during training. Let ˆyi be the rectified label\\nof image xi. As long as ˆyi is given, it would be intuitive\\nthat we train classification based on the corrected label ˆyi.\\nIn particular, we would append a classifier to transform the\\nfeature from Eq into probabilities pi ∈ RK with K being\\nthe number of classes. Then we impose a classification loss\\n\\nLi\\n\\nce = − log(pi[ˆyi]).\\n\\n(1)\\n\\nHowever, the acquisition of ˆyi is not straight-forward.\\nWe resort to prototypes, the moving averaged centroids of\\nfeatures from training instances, to accomplish this task.\\n\\nFigure 4. The overview of our PNL framework.\\nIt comprises\\na supervised classification module, a prototype based contrastive\\nlearning module, and a label-guided contrastive learning module.\\n\\nBalanced distribution of identities. We illustrate the cu-\\nmulative percentage of identities with respect to the number\\nof their corresponding person images as a curve in Figure 2.\\nA point (X, Y ) on the curve represents that there are in total\\nY % identities in LUPerson-NL, that each of them has less\\nthan X images. It can be observed that: i) about 75% of all\\nthe identities in LUPerson-NL have a person image num-\\nber within [10, 25]; ii) the percentage of identities that have\\nmore than 50 person images each, occupy only a very small\\nportion of about 6.4% (27, 767/433, 997) in LUPerson-NL.\\nThese observations all show that our LUPerson-NL is well\\nbalanced in terms of identity distribution, making it a suit-\\nable dataset for person Re-ID tasks.\\n\\nIn spite of our dedicatedly designed tracking and filter-\\ning strategies as proposed in Sec 3.1, the identity labels we\\nobtained can never be very accurate due to the technical up-\\nper bounds of current tracking methods. Figure 3 visualizes\\nthe two noise types in LUPerson-NL that are caused by dif-\\nferent labeling errors, which are Noise-I, where the same\\nperson is split into different tracklets and is mistaken as dif-\\nferent persons; and Noise-II, that different persons are\\nrecognized as the same person.\\n\\nfc𝒙!𝑇𝑇\"#𝒙!#𝒙!\"𝐸#𝐸$𝑦!&𝑦!𝒌!!,𝒌!\",…,𝒌!#$𝑦!!,$𝑦!\",…,$𝑦!#&𝑦!prototypesqueue𝒒!𝒄\",𝒄#,…,𝒄$Label Guided Contrastive LossClassificationLoss𝒌!Label Update&𝑦!Prototype BasedContrastive Loss𝒑!update prototypesenqueuemomentum updatew. gradientw/o. gradient𝒔!\\x0c(2)\\n\\n(3)\\n\\n4.2. Label Rectification with Prototypes\\n\\nAs depicted by Figure 4, we maintain prototypes as a dic-\\ntionary of feature vectors {c1, c2, . . . , cK}, where K is the\\nnumber of identities, ck ∈ Rd is a prototype representing a\\nclass-wise feature centroid. In each training step, we would\\nfirst evaluate the similarity score sk\\ni between the query fea-\\nture qi and each of the current prototypes ck by\\n\\nsk\\ni =\\n\\nexp(qi · ck/τ )\\nk=1 exp(qi · ck/τ )\\n\\n.\\n\\n(cid:80)K\\n\\nLet pi be the classification probability given by the clas-\\nsifier with weights updated in the previous step. The rec-\\ntified label ˆyi for this step is then generated by combining\\nboth the prototype scores si = {sk\\nk=1 and the classifica-\\ntion probability pi as\\n\\ni }K\\n\\nli =\\n\\n(pi + si),\\n\\n1\\n2\\n(cid:40)\\n\\nˆyi =\\n\\narg maxj lj\\nyi\\n\\ni\\n\\nif maxj lj\\notherwise.\\n\\ni > T ,\\n\\nHere we compute a soft pseudo label li and convert it to a\\nhard one ˆyi based on a threshold T . If the highest score in li\\nis larger than T , the corresponding class would be selected\\nas ˆyi, otherwise the original raw label yi would be kept.\\n\\n4.3. Prototype Based Contrastive Learning\\n\\nThe newly rectified label ˆyi can then be used to super-\\nvise the cross-entropy loss Li\\nce for classification as formu-\\nlated by Equation 1. Besides, it also helps train prototypes\\nck in return. In specific, we propose a prototype based con-\\ntrastive loss Li\\npro to constrain that the feature of each sam-\\nple should be closer to the prototype it belongs to. We for-\\nmulate the loss as\\n\\nLi\\n\\npro = −log\\n\\nexp(qi · cˆyi/τ )\\nj=1 exp(qi · cj/τ )\\n\\n,\\n\\n(cid:80)K\\n\\n(4)\\n\\nwith qi being the query feature from Eq, τ being a hyper-\\n\\nparameter representing temperature.\\n\\nAll the prototypes are maintained as a dictionary, with\\n\\nstep-wise updates following a momentum mechanism as\\ncˆyi = mcˆyi + (1 − m)qi.\\n\\n(5)\\n\\n4.4. Label-Guided Contrastive Learning\\n\\nInstance-wise contrastive learning proved to be very ef-\\nfective in self-supervised learning [4, 5, 7, 17, 19]. It learns\\ninstance-level feature discrimination by encouraging simi-\\nlarity among features from the same instance, while pro-\\nmoting dissimilarity between features from different in-\\nstances. The instance-wise contrastive loss is given by\\n\\nLi\\n\\nic = −log\\n\\nexp(qi · k+\\ni /τ ) + (cid:80)M\\n\\ni /τ )\\nj=1 exp(qi · k−\\n\\nj /τ )\\n\\n, (6)\\n\\nexp(qi · k+\\n\\nwith qi being the query feature of current instance i.\\nk+\\ni (= ki) is the positive key feature generated from the\\nmomentum encoder Ek. It is marked positive since it shares\\n∗ ∈ Rd, on the contrary, are\\nthe same instance with qi. k−\\nthe rest features stored in a queue that represent negative\\nsamples. The queue has a size of M . At the end of each\\ntraining step, the queue would be updated by en-queuing\\nthe new key feature and de-queuing the oldest one.\\n\\nSuch instance-level contrastive learning is far from per-\\nfect, as it neglects the relationships among different in-\\nstances. For example, even though two instances depict the\\nsame person, it would still strengthen the gap between their\\nInstead, we propose a label guided contrastive\\nfeatures.\\nlearning module, making use of the rectified labels ˆyi to\\nensure more reasonable grouping of contrastive pairs.\\n\\nWe redesign the queue to additionally record labels ˆyi.\\nRepresented by Q = [(kjt, ˆyjt)]M\\nt=1, our new queue accepts\\nnot only a key feature ki but also its rectified label ˆyi during\\nupdate. These newly recorded labels help better distinguish\\npositive and negative pairs. Let P(i) be the new set of posi-\\ntive features and N (i) the new set of negative features: fea-\\ntures in P(i) share the same rectified label with the current\\ninstance i while features in N (i) do not. Our label guided\\ncontrastive loss can be given by\\n\\nLi\\n\\nlgc =\\n\\n−1\\n|P(i)|\\n\\nlog\\n\\nexp\\n\\n(cid:17)\\n\\n(cid:16) qi·k+\\nτ\\n\\n(cid:80)\\nk+∈P(i)\\n(cid:16) qi·k+\\nτ\\n\\n(cid:17)\\n\\n(cid:17) ,\\n\\n(cid:16) qi·k−\\nτ\\n\\n+ (cid:80)\\nk−∈N (i)\\n\\nexp\\n\\n(cid:80)\\nk+∈P(i)\\n\\nexp\\n\\n(7)\\n\\nwith\\n\\nP(i) = {kjt|ˆyjt = ˆyi, ∀(kjt, ˆyjt) ∈ Q} ∪ {ki},\\nN (i) = {kjt|ˆyjt ̸= ˆyi, ∀(kjt, ˆyjt) ∈ Q},\\n\\n(8)\\n\\nwhere ki and ˆyi are the key feature and the rectified label of\\nthe current instance i.\\n\\nFinally we combine all the components above to pre-\\n\\ntrain models on LUPerson-NL with the following loss\\nce + λproLi\\n\\npro + λlgcLi\\n\\nLi = Li\\n\\nlgc.\\n\\n(9)\\n\\nWe set λpro = λlgc = 1 during training.\\n\\n5. Experiments\\n\\n5.1. Implementation\\n\\nHyper-parameter settings. We set the hyper-parameters\\nτ = 0.1 and T = 0.8. The momentum m for updating\\nboth the momentum encoder Ek and the prototypes is set\\nto 0.999. More hyper-parameters exploration and training\\ndetails can be found at supplementary materials.\\nDataset and protocol. We conduct extensive experiments\\non four popular person Re-ID datasets: CUHK03, Market,\\n\\n\\x0cpre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\npre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\nTrip [21]\\n45.2/63.8\\n55.5/61.2\\n62.6/67.6\\n69.1/73.1\\n\\nTrip [21]\\n65.2/80.7\\n65.4/81.1\\n69.8/83.1\\n71.0/84.7\\n\\nIDE [59]\\n50.6/55.9\\n52.5/57.7\\n57.6/62.3\\n68.3/73.5\\n\\nIDE [59]\\n62.8/80.8\\n63.4/81.6\\n65.9/82.2\\n70.3/85.0\\n\\nMGN [51]\\n70.5/71.2\\n67.1/67.0\\n74.7/75.4\\n80.4/80.9\\n\\nMGN [51]\\n79.4/89.0\\n79.5/89.1\\n82.1/91.0\\n84.3/92.0\\n\\npre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\npre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\nTrip [21]\\n76.2/89.7\\n75.1/88.5\\n79.8/71.5\\n81.2/91.4\\n\\nTrip [21]\\n34.3/54.8\\n34.4/55.4\\n36.6/57.1\\n41.4/61.6\\n\\nIDE [59]\\n74.1/90.2\\n74.5/89.3\\n77.9/91.0\\n82.4/92.8\\n\\nIDE [59]\\n36.2/66.2\\n37.6/67.3\\n39.8/68.9\\n44.0/72.0\\n\\nMGN [51]\\n87.5/95.1\\n88.2/95.3\\n91.0/96.4\\n91.9/96.6\\n\\nMGN [51]\\n63.7/85.1\\n62.7/84.3\\n65.7/85.5\\n68.0/86.0\\n\\n(a) CUHK03\\n\\n(b) Market1501\\n\\n(c) DukeMTMC\\n\\n(d) MSMT17\\n\\nTable 2. Comparing three supervised Re-ID baselines using different pre-trained models. “IN sup.”/“IN unsup.” indicates model that is\\nsupervisely/unsupervisely pre-trained on ImageNet; “LUP unsup.” is the model unsupervisely pre-trained on LUPerson; “LUPnl pnl.“\\nrefers to the model that pre-trained on LUPerson-NL using our PNL framework. All results are shown in mAP/cmc1.\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\n10%\\n53.1/76.9\\n58.4/81.7\\n64.6/85.5\\n72.4/88.8\\n\\n30%\\n75.2/90.8\\n76.6/91.9\\n81.9/93.7\\n85.2/94.2\\n\\n10%\\n45.1/65.3\\n48.1/66.9\\n53.5/72.0\\n60.6/75.8\\n\\n30%\\n64.7/80.2\\n65.8/80.2\\n69.4/81.9\\n74.5/86.3\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\n10%\\n23.2/50.2\\n22.6/48.8\\n25.5/51.1\\n28.2/51.1\\n\\n30%\\n41.9/70.8\\n40.4/68.7\\n44.6/71.4\\n47.7/71.2\\n\\nsmall-scale\\n50%\\n81.5/93.5\\n82.0/94.1\\n85.8/94.9\\n88.3/95.5\\n\\nsmall-scale\\n50%\\n71.8/84.6\\n72.5/84.4\\n75.6/86.7\\n78.8/88.3\\n\\nsmall-scale\\n50%\\n50.3/76.9\\n49.0/75.0\\n53.0/77.7\\n55.5/77.2\\n\\n70%\\n84.8/94.5\\n85.4/94.5\\n88.8/95.9\\n90.1/96.2\\n\\n90%\\n86.9/95.2\\n87.4/95.5\\n90.5/96.4\\n91.3/96.4\\n\\n10%\\n21.1/41.8\\n18.6/36.1\\n26.4/47.5\\n42.0/61.6\\n\\n30%\\n68.1/87.6\\n69.3/87.8\\n78.3/92.1\\n83.7/94.0\\n\\n(a) Market1501\\n\\n70%\\n75.5/86.8\\n76.3/86.9\\n78.9/88.2\\n81.6/89.5\\n\\n90%\\n78.0/88.3\\n78.5/88.7\\n81.1/90.0\\n83.3/91.2\\n\\n10%\\n31.5/47.1\\n32.4/48.0\\n35.8/50.2\\n52.2/64.1\\n\\n30%\\n65.4/79.8\\n65.3/80.2\\n72.3/83.8\\n77.7/87.9\\n\\n(b) DukeMTMC\\n\\n70%\\n56.9/81.2\\n55.7/79.9\\n59.5/81.8\\n61.6/81.8\\n\\n90%\\n61.9/84.2\\n60.9/83.0\\n63.7/85.0\\n66.1/84.8\\n\\n10%\\n14.7/34.1\\n13.2/29.2\\n17.0/36.0\\n24.5/42.7\\n\\n30%\\n44.5/71.1\\n41.4/67.1\\n49.0/73.6\\n53.2/74.4\\n\\nfew-shot\\n50%\\n80.2/92.8\\n78.3/90.9\\n84.2/93.9\\n88.1/95.2\\n\\nfew-shot\\n50%\\n73.9/85.7\\n73.7/85.1\\n77.7/87.4\\n81.1/89.6\\n\\nfew-shot\\n50%\\n56.2/79.5\\n53.3/77.6\\n57.4/80.5\\n62.2/81.0\\n\\n70%\\n84.2/94.0\\n84.4/94.1\\n88.4/95.5\\n90.5/96.3\\n\\n90%\\n86.7/94.6\\n87.1/95.2\\n90.4/96.3\\n91.6/96.4\\n\\n70%\\n77.2/87.8\\n77.7/87.8\\n80.8/89.2\\n83.2/91.1\\n\\n90%\\n79.1/88.8\\n79.4/89.0\\n82.0/90.6\\n84.1/91.3\\n\\n70%\\n60.9/82.8\\n59.1/81.5\\n62.9/83.5\\n65.8/83.8\\n\\n90%\\n63.4/84.5\\n62.4/83.8\\n65.0/85.1\\n67.4/85.3\\n\\n(c) MSMT17\\nTable 3. Comparing pre-trained models on three labeled Re-ID datasets, under the small-scale setting and the few-shot setting, with\\ndifferent usable data percentages. “LUPnl pnl.” is our model pre-trained on LUPerson-NL using PNL. Results are shown in mAP/cmc1.\\n\\nDukeMTMC and MSMT17. We adopt their official set-\\ntings, except CUHK03 where its labeled counterpart with\\nnew protocols proposed in [62] is used. We follow the stan-\\ndard evaluation metrics: the mean Average Precision (mAP)\\nand the Cumulated Matching Characteristics top-1 (cmc1).\\n5.2. Improving Supervised Re-ID\\n\\nTo evaluate our pre-trained model based on LUPerson-\\nNL with respect to supervised person Re-ID tasks. we\\nconduct experiments using three representative supervised\\nRe-ID baselines with different pre-training models. These\\nbaseline methods include two simpler approaches driven\\nonly by the triplet loss (Trip [21]) or the classification loss\\n(IDE [59]), as well as a stronger and more complex method\\nMGN [51] that use both triplet and classification losses.\\n\\n{“IN”, “LUP”, “LUPnl”} represent ImageNet [43], LU-\\nPerson [12] and our LUPerson-NL respectively; while the\\n{“sup.”, “unsup.”, “pnl.”} stand for the {“supervised”, “un-\\nsupervised”, and “pretrain with noisy label”} pre-training\\nmethods. e.g. the “LUPnl pnl.” in the bottom rows of Ta-\\nble 2 all refer to our model, which is pre-trained on our\\nLUPerson-NL dataset using our PNL framework.\\n\\nFrom Table 2 we can see, for all of the three base-\\nline methods, our pre-trained model improves their perfor-\\nmances greatly on the four popular person Re-ID datasets.\\nSpecifically, the improvements are at least 5.7%, 0.9%,\\n1.2% and 2.3% in terms of mAP on CUHK03, Market1501,\\nDukeMTMC and MSMT17 respectively.\\n\\nWe report results in Table 2, where the abbreviations\\n\\nNote that even though the performance of the baseline\\n\\n\\x0cMGN on Market1501 has been extremely high, our model\\nstill brings considerable improvement over it. The other\\nway around, our pre-trained models obtain more signifi-\\ncant improvements on relatively weak methods (Trip and\\nIDE), unveiling that model initialization plays a critical part\\nin person Re-ID training.\\n\\nOur noisy label guided pre-training models are also sig-\\nnificantly advantageous over the previous “LUPerson un-\\nsup“ models, which emphasizes the superiority of our PNL\\nframework and our LUPerson-NL dataset.\\n\\n5.3. Improving Unsupervised Re-ID Methods\\n\\nOur pre-trained model can also benefit unsupervised per-\\nson Re-ID methods. Based on the state-of-the-art unsuper-\\nvised method SpCL [15], we explore different pre-training\\nmodels utilizing two settings proposed by SpCL: the pure\\nunsupervised learning (USL) and the unsupervised domain\\nadaptation (UDA). Results in Table 4 illustrate that our pre-\\ntrained model outperforms the others in all UDA tasks, as\\nwell as the USL task on DukeMTMC dataset. In the USL\\ntask on Market1501, we achieve the second best scores\\nslightly lower than the LUPerson model [12].\\n\\n5.4. Comparison on Small-scale and Few-shot\\n\\nFollowing the same protocols proposed by [12], we con-\\nduct experiments under two small data settings: the small-\\nscale setting and the few-shot setting. The small-scale set-\\nting restricts the percentage of usable identities, while the\\nfew-shot setting restricts the percentage of usable person\\nimages each identity has. Under both settings, we vary\\nthe usable data percentages of three popular datasets from\\n10% ∼ 100%. We compare different pre-trained models\\nunder these settings with MGN as the baseline method. The\\nresults shown in Table 3 verify the consistent improvements\\nbrought by our model on all the datasets under both settings.\\nBesides, the results in Table 3 show that the gains of\\nour pre-trained models are even larger under a more lim-\\nited amount of labeled data. For example, under the “small-\\nscale” setting, our model outperforms “LUPerson unsup”\\nby 7.8%, 7.1% and 2.7% on Market1501, DukeMTMC and\\nMSMT17 respectively with 10% identities. The improve-\\nments rise to 15.6%, 16.4% and 6.5% under the “few-shot”\\nsetting with 10% person images.\\n\\nMost importantly, our pre-trained “LUPnl pnl” model\\nhelps achieve advantageous results with a mAP of 72.4 and\\na cmc1 of 88.8, using only 10% labeled data from the Mar-\\nket1501 training set. The task is really challenging, con-\\nsidering that the training set composes only 1, 170 images\\nbelonging to 75 identities; while evaluations are performed\\non a much larger testing set with 19, 281 images belong-\\ning to 750 identities. We consider these results extremely\\nappealing as they demonstrate the strong potential of our\\npre-trained models in real-world applications.\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\nUSL\\n\\nM\\n72.4/87.8\\n72.9/88.6\\n76.2/90.2\\n75.6/89.3\\n\\nD\\n64.9/80.3\\n62.6/78.8\\n67.1/81.6\\n68.1/82.0\\n\\nUDA\\n\\nD → M\\n76.4/90.1\\n77.1/90.6\\n79.2/91.7\\n80.7/92.2\\n\\nM → D\\n67.9/82.3\\n66.3/81.6\\n69.1/83.2\\n72.2/84.9\\n\\nTable 4. Performances of different pre-trained models on the un-\\nsupervised Re-ID method SpCL [15] under two unsupervised task\\nsettings: the pure unsupervised learning (USL) and the unsuper-\\nvised domain adaptation (UDA). Here M and D refer to the Mar-\\nket1501 dataset and the DukeMTMC dataset respectively.\\n\\nmethod\\nMSMT17\\n\\nSupCont [26]\\n66.5/84.7\\n\\nLUP [12]\\n65.3/84.0\\n\\nPNL(ours)\\n68.0/86.0\\n\\n20%\\n\\nTable 5. Performance comparison for different pre-training meth-\\nods on LUPerson-NL dataset.\\n# ce ic pro lgc\\n1 ✓\\n✓\\n2\\n3 ✓ ✓\\n4 ✓\\n5 ✓\\n6 ✓ ✓ ✓\\n7 ✓\\n\\n40%\\n32.0/56.1 45.0/69.5 62.7/83.0\\n34.5/59.5 47.9/72.6 65.3/84.0\\n37.6/62.6 49.6/73.5 66.5/84.7\\n35.7/59.1 48.5/72.4 65.8/84.1\\n✓ 38.5/63.0 50.9/74.5 67.1/85.2\\n39.0/63.4 51.7/74.4 67.4/85.4\\n✓ ✓ 39.6/63.7 51.9/75.0 68.0/86.0\\n\\n100%\\n\\n✓\\n\\nTable 6. Ablating components of PNL on MSMT with data per-\\ncentages 20%, 40% and 100% under the small scale setting. ce:\\nsupervised classification; ic:\\ninstance-wise contrastive learning;\\npro: prototypes for both prototype-based contrastive learning and\\nlabel rectification; lgc: label-guided contrastive learning.\\n\\n5.5. Comparison with other pre-training methods\\n\\nWe compare our proposed PNL with some other popu-\\nlar pre-training methods in Table 5. LUP [12] is a varient\\nof MoCoV2 for person Re-ID based on unsupervised con-\\nstrastive learning, while SupCont [26] considers both su-\\npervised and constrastive learning. Our PNL outperforms\\nall these rep-resentative pre-training methods, indicating the\\nsuperiority of our proposed method.\\n5.6. Ablation Study\\n\\nWe also investigate the effectiveness of each designed\\ncomponent in PNL through ablation experiments. Results\\nshown by Table 6 illustrate the efficacy of our proposed\\ncomponents. We have the following observations: i) Train-\\ning with an instance-wise contrastive loss Li\\nic (row 2) with-\\nout using any labels leads to even better performance than\\ntraining with a classification loss Li\\nce (row 1) that utilizes\\nthe labels from LUPerson-NL, implying that the noisy la-\\nbels in LUPerson-NL would misguide representation learn-\\nii) Jointly training\\ning if directly adopted as supervision.\\nwith both losses Li\\nic (row 3) improves over us-\\ning only one loss (row 1, row 2), suggesting that learning\\ninstance-wise discriminative representations complements\\niii) The prototypes which contribute to\\nlabel supervision.\\n\\nce and Li\\n\\n\\x0csetting\\n\\nMSMT17\\n\\nce+pro\\n\\nce+pro+lgc\\n\\nw/o. lc\\n64.8/83.4\\n\\nw. lc\\n65.8/84.1\\n\\nw/o. lc\\n66.7/85.0\\n\\nw. lc\\n68.0/86.0\\n\\nTable 7. Ablating the label correction. lc: “label correction”.\\n\\nMethod\\nMGN† [51] (2018)\\nBOT [37] (2019)\\nDSA [57] (2019)\\nAuto [41] (2019)\\nABDNet [3] (2019)\\nSCAL [2] (2019)\\nMHN [1] (2019)\\nBDB [9] (2019)\\nSONA [54] (2019)\\nGCP [39] (2020)\\nSAN [24] (2020)\\nISP [63] (2020)\\nGASM [20] (2020)\\nESNET [44] (2020)\\nLUP [12](2020)\\nOurs+MGN\\nOurs+BDB\\n\\nCUHK03 Market1501 DukeMTMC MSMT17\\n63.7/85.1\\n70.5/71.2\\n-\\n-\\n-\\n75.2/78.9\\n52.5/78.2\\n73.0/77.9\\n60.8/82.3\\n-\\n-\\n72.3/74.8\\n-\\n72.4/77.2\\n-\\n76.7/79.4\\n-\\n79.2/81.8\\n-\\n75.6/77.9\\n55.7/79.2\\n76.4/80.1\\n-\\n74.1/76.5\\n52.5/79.5\\n-\\n57.3/80.5\\n-\\n65.7/85.5\\n79.6/81.9*\\n68.0/86.0\\n80.4/80.9\\n82.3/84.7\\n53.4/79.0\\n\\n79.4/89.0\\n76.4/86.4\\n74.3/86.2\\n-\\n78.6/89.0\\n79.6/89.0\\n77.2/89.1\\n76.0/89.0\\n78.3/89.4\\n78.6/87.9\\n75.5/87.9\\n80.0/89.6\\n74.4/88.3\\n78.7/88.5\\n82.1/91.0\\n84.3/92.0\\n79.0/89.2\\n\\n87.5/95.1\\n85.9/94.5\\n87.6/95.7\\n85.1/94.5\\n88.3/95.6\\n89.3/95.8\\n85.0/95.1\\n86.7/95.3\\n88.8/95.6\\n88.9/95.2\\n88.0/96.1\\n88.6/95.3\\n84.7/95.3\\n88.6/95.7\\n91.0/96.4\\n91.9/96.6\\n88.4/95.4\\n\\nTable 8. Comparison with the state of the art. Numbers of MGN†\\ncome from a re-implementation based on FastReID, which are\\neven better than the original. Numbers of PNL marked by * are ob-\\ntained on BDB, the rest without the * mark are obtained on MGN.\\nWe show best scores in bold and the second scores underlined.\\n\\n5.8. Comparison with State-of-the-Art Methods\\n\\nWe compare our results with current state-of-the-art\\nmethods on four public benchmarks. We don’t apply any\\npost-processing techniques such as IIA [13] and RR [62].\\nTo ensure fairness, we adopt ResNet50 as our backbone and\\ndoes not compare with methods that rely on stronger back-\\nbones (results with stronger backbones e.g. ResNet101 can\\nbe found at supplementary materials). Results in Table 8\\nverify the remarkable advantage brought by our pre-trained\\nmodels. Without bells and whistles, we achieve state-of-\\nthe-art performance on all four benchmarks, outperforming\\nthe second with clear margins.\\n\\n6. Conclusion\\n\\nIn this paper, we demonstrate that large-scale Re-ID\\nrepresentation can be directly learned from massive raw\\nvideos by leveraging the spatial and temporal information.\\nWe not only build a large-scale noisy labeled person Re-\\nID dataset LUPerson-NL based on tracklets of raw videos\\nfrom LUPerson without using manual annotations, but also\\ndesign a novel weakly supervised pretraining framework\\nPNL comprising different learning modules including su-\\npervised learning, prototypes-based learning, label-guided\\ncontrastive learning and label rectification. Equipped with\\nour pre-trained models, we push existing benchmark results\\nto a new limit, which outperforms unsupervised pre-trained\\nmodels and ImageNet supervised pre-trained models by a\\n\\n(a) Correction for Noise-I\\n\\n(b) Correction for Noise-II\\n\\nFigure 5. Visualizing the label correction functionality of our PNL\\nframework with respect to the two noise types from LUPerson-\\nNL. Person images in the same rectangle indicate that they are\\nrecognized as the same identity. The right-hand similarity matrices\\nare calculated based on the image features all learned using our\\nPNL framework with label correction.\\n\\nboth prototype-based contrastive learning and the label cor-\\nrection, are very important under various settings, as veri-\\nfied by comparing row 1 with row 4; row 3 with row 6; and\\nrow 5 with row 7. iv) Our label-guided contrastive learning\\ncomponent consistently outperforms the vanilla instance-\\nwise contrastive learning under various settings, as verified\\nby comparing row 3 with row 5, and row 6 with row 7. v)\\nCombining all our components (supervised classification,\\nprototypes and label-guided contrastive learning) together\\nleads to the best performance as shown by row 7.\\n5.7. Label Correction\\n\\nOur PNL can indeed correct noisy labels. We demon-\\nstrate two typical examples in Figure 5 visualizing the la-\\nbel corrections with respect to the two kinds of noises. As\\nwe can see, in Figure 5a the same person are marked as\\nthree different persons in LUPerson-NL due to Noise-I\\nin labels. After our PNL pre-training, these three track-\\nlets are merged together since their trained features are very\\nclose, as verified by the right-hand similarity matrix. In Fig-\\nure 5b different persons are labeled as the same identity\\nin LUPerson-NL due to Noise-II in labels. After PNL\\ntraining, these mis-labeled person identities are all correctly\\nre-grouped into two identities, which can also be reflected\\nby the right-hand similarity matrix.\\n\\nwe also ablate the label correction module in Table 7\\nwith different settings, and observe it can improve the per-\\nIt also validates the importance of combining\\nformance.\\nlabel rectification with label-guided contrastive learning to-\\ngether, where more accurate positive/negative pairs can be\\nleveraged.\\n\\nBeforeAfter2103876954Similarity MatrixFinal ID=179919BeforeAfter2103876954Similarity MatrixFinal ID=419043Final ID=73264\\x0clarge margin.\\nAcknowledgement. This work is partially supported by\\nthe National Natural Science Foundation of China (NSFC,\\n61836011).\\n\\nReferences\\n\\n[1] Binghui Chen, Weihong Deng, and Jiani Hu. Mixed high-\\norder attention network for person re-identification. In Pro-\\nceedings of the IEEE International Conference on Computer\\nVision, pages 371–381, 2019. 8\\n\\n[2] Guangyi Chen, Chunze Lin, Liangliang Ren, Jiwen Lu, and\\nSelf-critical attention learning for person re-\\nJie Zhou.\\nIn Proceedings of the IEEE International\\nidentification.\\nConference on Computer Vision, pages 9637–9646, 2019. 8\\n[3] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang\\nChen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-\\nIn Pro-\\nnet: Attentive but diverse person re-identification.\\nceedings of the IEEE International Conference on Computer\\nVision, pages 8351–8361, 2019. 8\\n\\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. arXiv preprint arXiv:2002.05709,\\n2020. 2, 4, 5\\n\\n[5] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\\nNorouzi, and Geoffrey Hinton. Big self-supervised mod-\\narXiv preprint\\nels are strong semi-supervised learners.\\narXiv:2006.10029, 2020. 2, 4, 5\\n\\n[6] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi\\nHuang. Beyond triplet loss: a deep quadruplet network for\\nIn Proceedings of the IEEE con-\\nperson re-identification.\\nference on computer vision and pattern recognition, pages\\n403–412, 2017. 2\\n\\n[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\\nImproved baselines with momentum contrastive learning.\\narXiv preprint arXiv:2003.04297, 2020. 2, 4, 5\\n\\n[8] Zhirui Chen, Jianheng Li, and Wei-Shi Zheng. Weakly su-\\npervised tracklet person re-identification by deep feature-\\narXiv preprint arXiv:1910.14333,\\nwise mutual learning.\\n2019. 3\\n\\n[9] Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu,\\nand Ping Tan. Batch dropblock network for person re-\\nidentification and beyond. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pages 3691–3701,\\n2019. 8\\n\\n[10] Piotr Doll´ar, Ron Appel, Serge Belongie, and Pietro Per-\\nIEEE\\nona. Fast feature pyramids for object detection.\\ntransactions on pattern analysis and machine intelligence,\\n36(8):1532–1545, 2014. 4\\n\\n[11] Pedro F Felzenszwalb, Ross B Girshick, David McAllester,\\nand Deva Ramanan. Object detection with discriminatively\\nIEEE transactions on pattern\\ntrained part-based models.\\nanalysis and machine intelligence, 32(9):1627–1645, 2009.\\n4\\n\\nings of the IEEE conference on computer vision and pattern\\nrecognition, 2021. 1, 2, 3, 4, 6, 7, 8\\n\\n[13] Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen,\\nJianmin Bao, Gang Hua, and Houqiang Li. Improving person\\nre-identification with iterative impression aggregation. IEEE\\nTransactions on Image Processing, 29:9559–9571, 2020. 8\\n[14] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-\\nteaching: Pseudo label refinery for unsupervised domain\\nadaptation on person re-identification. In International Con-\\nference on Learning Representations, 2019. 2\\n\\n[15] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hong-\\nsheng Li. Self-paced contrastive learning with hybrid mem-\\nory for domain adaptive object re-id. In Advances in Neural\\nInformation Processing Systems, 2020. 2, 7\\n\\n[16] Douglas Gray and Hai Tao. Viewpoint invariant pedes-\\ntrian recognition with an ensemble of localized features. In\\nEuropean conference on computer vision, pages 262–275.\\nSpringer, 2008. 4\\n\\n[17] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\\nnew approach to self-supervised learning. arXiv preprint\\narXiv:2006.07733, 2020. 2, 4, 5\\n\\n[18] Xinqian Gu, Bingpeng Ma, Hong Chang, Shiguang Shan,\\nand Xilin Chen. Temporal knowledge propagation for image-\\nIn Proceedings of the\\nto-video person re-identification.\\nIEEE/CVF International Conference on Computer Vision,\\npages 9647–9656, 2019. 2\\n\\n[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\nGirshick. Momentum contrast for unsupervised visual rep-\\nresentation learning. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n9729–9738, 2020. 2, 4, 5\\n\\n[20] Lingxiao He and Wu Liu. Guided saliency feature learning\\nfor person re-identification in crowded scenes. In European\\nConference on Computer Vision, pages 357–373. Springer,\\n2020. 8\\n\\n[21] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-\\nfense of the triplet loss for person re-identification. arXiv\\npreprint arXiv:1703.07737, 2017. 2, 6\\n\\n[22] Yan Huang, Qiang Wu, JingSong Xu, and Yi Zhong. Sbsgan:\\nSuppression of inter-domain background shift for person re-\\nidentification. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision, pages 9527–9536, 2019. 2\\n[23] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li\\nZhang. Style normalization and restitution for generalizable\\nIn Proceedings of the IEEE/CVF\\nperson re-identification.\\nConference on Computer Vision and Pattern Recognition,\\npages 3143–3152, 2020. 2\\n\\n[24] Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, and\\nZhibo Chen. Semantics-aligned representation learning for\\nperson re-identification. In AAAI, pages 11173–11180, 2020.\\n8\\n\\n[12] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu\\nYuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsu-\\npervised pre-training for person re-identification. Proceed-\\n\\n[25] Srikrishna Karanam, Mengran Gou, Ziyan Wu, Angels\\nRates-Borras, Octavia Camps, and Richard J Radke. A\\ncomprehensive evaluation and benchmark for person re-\\n\\n\\x0cidentification: Features, metrics, and datasets. arXiv preprint\\narXiv:1605.09653, 2(3):5, 2016. 1, 4\\n\\n[26] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\\nDilip Krishnan. Supervised contrastive learning. Advances\\nin Neural Information Processing Systems, 33, 2020. 7\\n[27] Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, and Shiliang\\nZhang. Global-local temporal representations for video per-\\nson re-identification. In Proceedings of the IEEE/CVF Inter-\\nnational Conference on Computer Vision, pages 3958–3967,\\n2019. 2\\n\\n[28] Junnan Li, Caiming Xiong, and Steven Hoi. Mopro: Webly\\nsupervised learning with momentum prototypes. In Interna-\\ntional Conference on Learning Representations, 2020. 2, 4\\n[29] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsuper-\\nvised tracklet person re-identification. IEEE transactions on\\npattern analysis and machine intelligence, 42(7):1770–1782,\\n2019. 3\\n\\n[30] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-\\nreid: Deep filter pairing neural network for person re-\\nIn Proceedings of the IEEE conference on\\nidentification.\\ncomputer vision and pattern recognition, pages 152–159,\\n2014. 4\\n\\n[31] Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, and\\nYu-Chiang Frank Wang. Recover and identify: A generative\\ndual model for cross-resolution person re-identification. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 8090–8099, 2019. 2\\n\\n[32] Yu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank\\nWang. Cross-dataset person re-identification via unsuper-\\nvised pose disentanglement and adaptation. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vi-\\nsion, pages 7919–7929, 2019. 2\\n\\n[33] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi\\nYang. A bottom-up clustering approach to unsupervised per-\\nIn Proceedings of the AAAI Confer-\\nson re-identification.\\nence on Artificial Intelligence, volume 33, pages 8738–8745,\\n2019. 2\\n\\n[34] Fangyi Liu and Lei Zhang. View confusion feature learning\\nfor person re-identification. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 6639–\\n6648, 2019. 2\\n\\n[35] Chen Change Loy, Chunxiao Liu, and Shaogang Gong. Per-\\nson re-identification by manifold ranking. In 2013 IEEE In-\\nternational Conference on Image Processing, pages 3567–\\n3571. IEEE, 2013. 4\\n\\n[36] Chuanchen Luo, Yuntao Chen, Naiyan Wang, and Zhaoxi-\\nang Zhang. Spectral feature transformation for person re-\\nidentification. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision, pages 4976–4985, 2019. 2\\n[37] Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao,\\nShenqi Lai, and Jianyang Gu. A strong baseline and batch\\nnormalization neck for deep person re-identification. IEEE\\nTransactions on Multimedia, 2019. 8\\n\\n[38] Jingke Meng, Sheng Wu, and Wei-Shi Zheng. Weakly su-\\nIn Proceedings of the\\npervised person re-identification.\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 760–769, 2019. 3\\n\\n[39] Hyunjong Park and Bumsub Ham. Relation network for per-\\nson re-identification. In Proceedings of the AAAI Conference\\non Artificial Intelligence, volume 34, pages 11839–11847,\\n2020. 8\\n\\n[40] Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie\\nQiu, Yang Wu, Yu-Gang Jiang, and Xiangyang Xue. Pose-\\nnormalized image generation for person re-identification. In\\nProceedings of the European conference on computer vision\\n(ECCV), pages 650–667, 2018. 2\\n\\n[41] Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, and Yi\\nYang. Auto-reid: Searching for a part-aware convnet for\\nperson re-identification. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pages 3750–3759,\\n2019. 8\\n\\n[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. In Advances in neural information pro-\\ncessing systems, pages 91–99, 2015. 4\\n\\n[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, et al.\\nImagenet large\\nscale visual recognition challenge. International journal of\\ncomputer vision, 115(3):211–252, 2015. 6\\n\\n[44] Dong Shen, Shuai Zhao, Jinming Hu, Hao Feng, Deng Cai,\\nand Xiaofei He. Es-net: Erasing salient parts to learn more\\nin re-identification. IEEE Transactions on Image Processing,\\n2020. 8\\n\\n[45] Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen,\\nand Xiaogang Wang. Person re-identification with deep\\nsimilarity-guided graph neural network. In Proceedings of\\nthe European conference on computer vision (ECCV), pages\\n486–504, 2018. 2\\n\\n[46] Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Ky-\\noung Mu Lee. Part-aligned bilinear representations for per-\\nson re-identification. In Proceedings of the European Con-\\nference on Computer Vision (ECCV), pages 402–419, 2018.\\n2\\n\\n[47] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\\nhigh-resolution representation learning for human pose esti-\\nmation. In CVPR, 2019. 3\\n\\n[48] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\\nWang. Beyond part models: Person retrieval with refined\\npart pooling (and a strong convolutional baseline). In ECCV,\\n2018. 2\\n\\n[49] Dongkai Wang and Shiliang Zhang. Unsupervised person re-\\nidentification via multi-label classification. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 10981–10990, 2020. 2\\n\\n[50] Guangrun Wang, Guangcong Wang, Xujie Zhang, Jianhuang\\nLai, Zhengtao Yu, and Liang Lin. Weakly supervised per-\\nson re-id: Differentiable graphical learning and a new bench-\\nmark. IEEE Transactions on Neural Networks and Learning\\nSystems, 32(5):2142–2156, 2020. 3, 4\\n\\n[51] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\\nZhou. Learning discriminative features with multiple granu-\\nlarities for person re-identification. In 2018 ACM Multime-\\ndia Conference on Multimedia Conference, pages 274–282.\\nACM, 2018. 1, 2, 6, 8\\n\\n\\x0c[52] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\\nPerson transfer gan to bridge domain gap for person re-\\nIn Proceedings of the IEEE Conference on\\nidentification.\\nComputer Vision and Pattern Recognition, pages 79–88,\\n2018. 1, 4\\n\\n[53] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\nUnsupervised feature learning via non-parametric instance\\nIn Proceedings of the IEEE Conference\\ndiscrimination.\\non Computer Vision and Pattern Recognition, pages 3733–\\n3742, 2018. 2\\n\\n[54] Bryan Ning Xia, Yuan Gong, Yizhe Zhang, and Christian\\nPoellabauer. Second-order non-local attention networks for\\nperson re-identification. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pages 3760–3769,\\n2019. 8\\n\\n[55] Ye Yuan, Wuyang Chen, Yang Yang, and Zhangyang Wang.\\nIn defense of the triplet loss again: Learning robust person\\nre-identification with fast approximated triplet loss and label\\ndistillation. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition Workshops, pages\\n354–355, 2020. 2\\n\\n[56] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\\nand Wenyu Liu. Fairmot: On the fairness of detection and\\nre-identification in multiple object tracking. arXiv e-prints,\\npages arXiv–2004, 2020. 2, 3, 4\\n\\n[57] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo\\nChen. Densely semantically aligned person re-identification.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 667–676, 2019. 8\\n\\n[58] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\\ndong Wang, and Qi Tian. Scalable person re-identification:\\nA benchmark. 2015 IEEE International Conference on Com-\\nputer Vision (ICCV), pages 1116–1124, 2015. 1, 4\\n\\n[59] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan\\nChandraker, Yi Yang, and Qi Tian. Person re-identification\\nin the wild. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition, pages 1367–1376,\\n2017. 1, 2, 6\\n\\n[60] Zhedong Zheng, Liang Zheng, and Yi Yang. A discrimi-\\nnatively learned cnn embedding for person reidentification.\\nACM Transactions on Multimedia Computing, Communica-\\ntions, and Applications (TOMM), 14(1):1–20, 2017. 2\\n[61] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-\\nples generated by gan improve the person re-identification\\nbaseline in vitro. In Proceedings of the IEEE International\\nConference on Computer Vision, 2017. 1, 4\\n\\n[62] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-\\nranking person re-identification with k-reciprocal encoding.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 1318–1327, 2017. 6, 8\\n[63] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao\\nWang. Identity-guided human semantic parsing for person\\nre-identification. ECCV, 2020. 8\\n\\n\\x0cIn this material, we will 1) show demo cases for scene\\nchanging for specific person in LUPerson-NL, 2) share the\\ntraining details for PNL, 3) provide the detailed algorithm\\ntable for PNL, 4) compare models pre-trained on SYSU30K\\nand LUPerson-NL 5) analyze the hyper parameters of our\\nPNL, 6) demonstrate results of our method using a stronger\\nbackbones, 7) explore the impact of pre-training image\\nscale, and 8) list more detailed results for our “small-scale”\\nand “few-shot” experiments. All these experiments are un-\\nder MGN settings.\\n\\nA. Scene changing in LUPerson-NL\\n\\nOur LUPerson-NL are driven from street view videos,\\nFigure 6 shows a demo person in our LUPerson-NL. As we\\ncan see, our LUPerson-NL is able to cover multiple scenes,\\nsince the videos we use have lots of moving cameras and\\nmoving persons, and only traklets with ≥ 200 frames are\\nselected. It is approximate close to the real person Re-ID\\nscenario.\\n\\nC. Algorithm for PNL\\n\\nAlgorithm 1 shows the procedure of training PNL, we\\ntrain our framework for 90 epochs, and apply label correc-\\ntion from 10 epochs. Once the rectification begins, we keep\\nrectifying for every iteration.\\n\\nAlgorithm 1: PNL algorithm.\\n\\nInput: total epochs N , correction start epochs Ns,\\nprototype feature vectors {⃗c1, ⃗c2, . . . , ⃗cK}, where\\nK is the number of identities, temperature τ ,\\nthreshold T , momentum m, encoder network\\nEq(·), classifier h(·), momentum encoder Ek(·),\\nloss weight λpro and λlgc.\\nfor epoch = 1 : N do\\n\\n{(⃗xi, yi)}b\\nfor i ∈ {1, ..., b} do\\n\\ni=1 sampled from data loader.\\n\\n˜⃗xi = aug1(⃗xi), ˜⃗x′\\ni = aug2(⃗xi)\\n⃗qi = Eq(˜⃗xi), ⃗ki = Ek(˜⃗x′\\ni)\\n⃗pi = h(⃗qi)\\nk=1, sk\\ni }K\\n⃗si = {sk\\n⃗li = (⃗pi + ⃗si)/2\\nif epoch > Ns and maxk lj\\n\\n(cid:80)K\\n\\ni = exp( ⃗qi·⃗ck/τ )\\n\\nk=1 exp(⃗qi·⃗ck/τ )\\n\\ni > T then\\n\\nˆyi = arg maxj\\n\\n⃗lj\\ni\\n\\nelse\\n\\nˆyi = ⃗yi\\n\\nLi\\nLi\\nLi\\n\\nce = − log(⃗pi[ˆyi])\\npro = −log\\nlgc =\\n\\n(cid:80)K\\n\\nexp(⃗qi·⃗c ˆyi /τ )\\nj=1 exp(⃗qi·⃗cj /τ )\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n⃗qi·⃗k+\\nτ\\n\\nexp\\n\\n(cid:80)\\nX\\n⃗qi·⃗k+\\nτ\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n−1\\n|P(i)| log\\n\\n⃗qi·⃗k−\\nτ\\n\\nexp\\n\\n+(cid:80)\\nexp\\n⃗k−∈N (i)\\n\\n(cid:80)\\n⃗k+ ∈P(i)\\n⃗cˆyi = m⃗cˆyi + (1 − m)⃗qi\\nce + λproLi\\nLi = Li\\nupdate networks Eq, h to minimize Li\\nupdate networks Eq, h to minimize Li\\nupdate momentum encoder Ek.\\n\\npro + λlgcLi\\n\\nlgc\\n\\nD. Compare with SYSU30K\\n\\nAs show in Table 9, we compare the pre-trained mod-\\nels between LUPerson-NL and SYSU30K. We pre-train\\nPNL on both LUPerson-NL and SYSU30K for 40 epochs\\nwith same experiment settings for a fair comparison. The\\nperformance of LUPerson-NL pre-training is much better\\nthan SYSU30K pre-training, showing the superiority of our\\nLUPerson-NL, and also suggesting that a large number of\\nimages with limited diversity does not bring more represen-\\ntative representation, but large-scale images with diversity\\ndoes.\\n\\nFigure 6. Scene changing for a specific person in LUPerson-NL.\\n\\nB. Training details\\n\\nDuring training, all\\n\\nimages are resized to 256 ×\\n128 and pass through the same augmentations veri-\\nfied in [12], which are Random Resized Crop, Hor-\\nizontal Flip, Normalization, Random Gaussian Blur,\\nSpecifi-\\nRandom Gray Scale and Random Erasing.\\ncally, the images are normalized with mean and std of\\n[0.3452, 0.3070, 0.3114], [0.2633, 0.2500, 0.2480], which\\nare calculated from all images in LUPerson-NL. We train\\nour model on 8 × V 100 GPUs for 90 epochs with a batch\\nsize of 1, 536. The initial learning rate is set to 0.4 with\\na step-wise decay by 0.1 for every 40 epochs. The opti-\\nmizer is SGD with a momentum of 0.9 and a weight de-\\ncay of 0.0001. We set the hyper-parameters τ = 0.1 and\\nT = 0.8. The momentum m for updating both the momen-\\ntum encoder Ek and the prototypes is set to 0.999. We de-\\nsign a large queue with a size of 65, 536 to increase the oc-\\ncurrence of positive samples for the label guided contrastive\\nlearning. The label correction according to Equation 3 starts\\nfrom the 10-th epoch. The deployment of the label guided\\ncontrastive loss Li\\n\\nlgc starts from the 15-th epoch.\\n\\n\\x0cE.1. Temperature Factor τ\\n\\nTable 13. Comparison for different pre-training data scale\\n\\nDataset\\nMSMT17\\n\\nLUPerson-NL\\n66.1/84.6\\n\\nSYSU30K\\n55.2/76.7\\n\\nTable 9. Comparison of applying our PNL on both LUPerson-NL\\nand SYSU30K.\\n\\nτ\\n\\n0.05\\n0.07\\n0.1\\n0.2\\n0.3\\n\\nDukeMTMC\\n\\nMSMT17\\n\\n40%\\n76.1/87.2\\n76.4/87.5\\n77.0/87.3\\n75.8/86.8\\n74.7/86.2\\n\\n100%\\n83.5/91.4\\n83.6/91.4\\n84.3/92.0\\n83.4/91.0\\n82.7/90.6\\n\\n40%\\n49.8/73.3\\n50.7/74.1\\n51.9/74.9\\n50.1/73.7\\n48.6/72.0\\n\\n100%\\n65.4/83.8\\n67.2/85.3\\n68.0/86.0\\n66.4/85.0\\n65.5/83.7\\n\\nTable 10. Performances under different τ values on DukeMTMC\\nand MSMT17 with data percentages 40% and 100% under the\\nsmall scale setting. The threshold is set as T = 0.8. The best\\nscores are in bold and the second ones are underlined.\\n\\nE. Hyper Parameter Analysis\\n\\nIn our PNL, there are two key hyper-parameters: tem-\\nperature factor τ and the threshold for correction T . Here,\\nwe provide the analysis of these two parameters.\\n\\nTable 10 shows the performance comparison with differ-\\nent τ values with a fixed label correction threshold T = 0.8.\\nAs we can see, the setting τ = 0.1 achieves the best re-\\nsults on both DukeMTMC and MSMT17, while τ = 0.07\\nachieves the second best. When we use a larger τ , the\\nperformance drops rapidly. It may be because Re-ID is a\\nmore fine-grained task, larger τ will cause smaller inter-\\nclass variations and make positive samples too close to neg-\\native samples. In all the experiments, we set τ = 0.1.\\n\\nE.2. Threshold T\\n\\nTable 11 shows the results with different label correc-\\ntion threshold values T . As we can see, the performance\\nis relatively stable to different T values varying in a large\\nrange of 0.6 ∼ 0.8. However, if T is too small or too large,\\nthe performance drops rapidly. For the former case, labels\\nare easier to be modified, which may cause wrong rectifi-\\ncations, while for the latter case, the label noises become\\nharder to be corrected, which also has consistently negative\\neffects on the performance. In all the experiments, we set\\nT = 0.8.\\n\\nF. Results for stronger backbones\\n\\nWe train our PNL using two stronger backbones\\nResNet101 and ResNet152, and report the results in Ta-\\nble 12. As we can see, the stronger ResNet bring more\\nsuperior performances. These results also outperform the\\n\\nT\\n\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n\\nDukeMTMC\\n\\nMSMT17\\n\\n40%\\n76.1/86.5\\n77.1/87.7\\n77.0/87.5\\n77.0/87.3\\n75.7/86.4\\n\\n100%\\n83.3/91.0\\n84.1/91.6\\n84.0/91.8\\n84.3/92.0\\n83.0/90.8\\n\\n40%\\n51.1/74.6\\n52.3/75.5\\n51.9/75.6\\n51.9/75.0\\n50.9/74.3\\n\\n100%\\n67.5/85.2\\n68.1/85.7\\n68.2/85.8\\n68.0/86.0\\n67.2/85.0\\n\\nTable 11. Performances under different T values on DukeMTMC\\nand MSMT17 with data percentages 40% and 100% under the\\nsmall scale setting. The temperature factor is set as τ = 0.1. The\\nbest scores are in bold and the second ones are underlined.\\n\\nArch CUHK03 Market1501 DukeMTMC MSMT17\\n68.0/86.0\\nR50\\n80.4/80.9\\n70.8/87.1\\nR101 80.5/81.2\\n71.6/87.5\\nR152 80.6/81.2\\n\\n84.3/92.0\\n85.5/92.8\\n85.6/92.4\\n\\n91.9/96.6\\n92.5/96.9\\n92.7/96.8\\n\\nTable 12. Results with different ResNet backbones. R50, R101\\nand R152 stand for ResNet50, ResNet101 and ResNet152 respec-\\ntively.\\n\\nScale\\nMSMT17\\n\\n10%\\n57.4/79.2\\n\\n30%\\n62.2/82.1\\n\\n100%\\n68.0/86.0\\n\\nscores reported in Table 8 of our main submission. Most\\nimportantly, we are the FIRST to obtain a mAP score on\\nMSMT17 that is larger than 70 without any post-processing\\nfor convolutional network.\\n\\nG. Pre-training data scales\\n\\nWe study the impact of pre-training data scale. Specif-\\nically, we involve various percentages (10%,30%,100%\\npseudo based) of LUPerson-NL into pre-training and then\\nevaluate the finetuing performance on the target datasets.\\nAs shown in Table 13, the learned representation is much\\nstronger with the increase of the pre-training data scale, in-\\ndicating the necessity of building large-scale dataset, and\\nour LUPerson-NL is very important.\\n\\nH. More results for small-scale and few-shot\\n\\nTo complement Table 3 in the main text, we provide\\nmore detailed results under “small scale” and “few shot”\\nsettings in Table 14. As we can see, our weakly pre-trained\\nmodels are consistently better than other pre-trained mod-\\nels. Our advantage is much larger with less training data,\\nsuggesting the potential practical value of our pre-trained\\nmodels for real-world person ReID applications.\\n\\n\\x0cscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\n10%\\n75\\n1,170\\n53.1/76.9\\n58.4/81.7\\n64.6/85.5\\n72.4/88.8\\n\\n10%\\n751\\n1,293\\n21.1/41.8\\n18.6/36.1\\n26.4/47.5\\n42.0/61.6\\n\\n10%\\n70\\n1,670\\n45.1/65.3\\n48.1/66.9\\n53.5/72.0\\n60.6/75.8\\n\\n10%\\n702\\n1,679\\n31.5/47.1\\n32.4/48.0\\n35.8/50.2\\n52.2/64.1\\n\\n10%\\n104\\n3,659\\n23.2/50.2\\n22.6/48.8\\n25.5/51.1\\n28.2/51.1\\n\\n10%\\n1,041\\n3,262\\n14.7/34.1\\n13.2/29.2\\n17.0/36.0\\n24.5/42.7\\n\\n20%\\n150\\n2,643\\n67.7/86.8\\n70.2/89.1\\n76.9/92.1\\n81.7/93.2\\n\\n20%\\n751\\n2,587\\n53.2/75.1\\n56.5/77.5\\n63.5/83.0\\n75.7/89.1\\n\\n20%\\n140\\n3,192\\n58.3/75.4\\n60.6/76.6\\n65.0/78.9\\n70.5/83.3\\n\\n20%\\n702\\n3,321\\n56.2/72.1\\n56.4/72.2\\n61.0/74.9\\n71.7/82.5\\n\\n20%\\n208\\n6,471\\n34.6/64.0\\n32.7/60.9\\n36.0/62.6\\n39.6/63.7\\n\\n20%\\n1,041\\n6,524\\n35.6/61.4\\n33.5/58.6\\n37.4/61.4\\n45.6/67.2\\n\\n30%\\n225\\n3,962\\n75.2/90.8\\n76.6/91.9\\n81.9/93.7\\n85.2/94.2\\n\\n30%\\n751\\n3,880\\n68.1/87.6\\n69.3/87.8\\n78.3/92.1\\n83.7/94.0\\n\\n30%\\n210\\n5,530\\n64.7/80.2\\n65.8/80.2\\n69.4/81.9\\n74.5/86.3\\n\\n30%\\n702\\n4,938\\n65.4/79.8\\n65.3/80.2\\n72.3/83.8\\n77.7/87.9\\n\\n30%\\n312\\n9,787\\n41.9/70.8\\n40.4/68.7\\n44.6/71.4\\n47.7/71.2\\n\\n30%\\n1,041\\n9,786\\n44.5/71.1\\n41.4/67.1\\n49.0/73.6\\n53.2/74.4\\n\\n40%\\n300\\n5,226\\n79.1/92.5\\n80.0/93.0\\n84.1/94.4\\n87.3/95.1\\n\\n50%\\n375\\n6,408\\n81.5/93.5\\n82.0/94.1\\n85.8/94.9\\n88.3/95.5\\n\\n60%\\n450\\n7,814\\n81.5/93.5\\n83.7/94.3\\n87.8/95.8\\n89.6/96.0\\n\\n(a) Market1501 small-scale\\n40%\\n751\\n5,174\\n75.4/90.4\\n78.8/88.3\\n80.3/92.7\\n86.0/94.3\\n\\n50%\\n751\\n6,468\\n80.2/92.8\\n78.3/90.9\\n84.2/93.9\\n88.1/95.2\\n\\n60%\\n751\\n7,758\\n83.0/93.6\\n81.7/93.3\\n86.7/94.7\\n89.8/95.8\\n\\n(b) Market1501 few-shot\\n\\n40%\\n280\\n6,924\\n68.5/83.0\\n69.5/82.9\\n72.8/84.7\\n77.0/87.3\\n\\n50%\\n351\\n8,723\\n71.8/84.6\\n72.5/84.4\\n75.6/86.7\\n78.8/88.3\\n\\n60%\\n421\\n10,197\\n74.1/85.6\\n75.0/86.2\\n77.6/87.1\\n80.5/89.2\\n\\n(c) DukeMTMC small-scale\\n40%\\n702\\n6,599\\n71.0/83.9\\n70.2/83.4\\n75.2/86.8\\n79.3/88.1\\n\\n50%\\n702\\n8,278\\n73.9/85.7\\n73.7/85.1\\n77.7/87.4\\n81.1/89.6\\n\\n60%\\n702\\n9,923\\n75.8/86.6\\n75.8/86.7\\n79.4/88.4\\n82.3/90.2\\n\\n(d) DukeMTMC few-shot\\n40%\\n416\\n13,006\\n46.7/74.5\\n45.1/72.2\\n49.2/74.9\\n51.9/74.9\\n\\n50%\\n520\\n15,917\\n50.3/76.9\\n49.0/75.0\\n53.0/77.7\\n55.5/77.2\\n\\n60%\\n624\\n19,672\\n53.9/79.4\\n52.7/78.0\\n56.4/79.7\\n59.1/80.1\\n\\n(e) MSMT17 small-scale\\n\\n40%\\n1,041\\n13,048\\n52.0/76.9\\n47.7/72.7\\n53.9/78.5\\n58.6/78.8\\n\\n50%\\n1,041\\n16,310\\n56.2/79.5\\n53.3/77.6\\n57.4/80.5\\n62.2/81.0\\n\\n60%\\n1,041\\n19,572\\n58.8/81.7\\n56.5/79.6\\n60.0/82.1\\n64.1/82.6\\n\\n(f) MSMT17 few-shot\\n\\n70%\\n525\\n9,120\\n84.8/94.5\\n85.4/94.5\\n88.8/95.9\\n90.1/96.2\\n\\n70%\\n751\\n9,055\\n84.2/94.0\\n84.4/94.1\\n88.4/95.5\\n90.5/96.3\\n\\n70%\\n491\\n11,939\\n75.5/86.8\\n76.3/86.9\\n78.9/88.2\\n81.6/89.5\\n\\n70%\\n702\\n11,564\\n77.2/87.8\\n77.7/88.2\\n80.8/89.2\\n83.2/91.1\\n\\n70%\\n728\\n22,680\\n56.9/81.2\\n55.7/79.9\\n59.5/81.8\\n61.6/81.8\\n\\n70%\\n1,041\\n22,834\\n60.9/82.8\\n59.1/81.5\\n62.9/83.5\\n65.8/83.8\\n\\n80%\\n600\\n11,417\\n85.9/95.2\\n86.4/95.0\\n89.8/96.2\\n90.9/96.4\\n\\n80%\\n751\\n10,348\\n86.3/94.7\\n86.4/95.0\\n89.8/96.0\\n91.2/96.4\\n\\n80%\\n561\\n13,500\\n76.8/87.3\\n77.4/87.3\\n80.2/89.2\\n82.9/90.6\\n\\n80%\\n702\\n13,201\\n78.3/88.6\\n78.7/88.7\\n81.7/90.3\\n84.0/91.6\\n\\n80%\\n832\\n26,335\\n59.6/82.4\\n58.6/82.0\\n61.9/83.6\\n64.2/83.3\\n\\n80%\\n1,041\\n26,096\\n62.5/84.2\\n60.9/82.3\\n64.2/84.5\\n67.2/84.7\\n\\n90%\\n675\\n11,727\\n86.9/95.2\\n87.4/95.5\\n90.5/96.4\\n91.3/96.4\\n\\n90%\\n751\\n11,642\\n86.7/94.6\\n87.1/95.2\\n90.4/96.3\\n91.6/96.4\\n\\n90%\\n631\\n15,111\\n78.0/88.3\\n78.5/88.7\\n81.1/90.0\\n83.3/91.2\\n\\n90%\\n702\\n14,860\\n79.1/88.8\\n79.4/89.0\\n82.0/90.6\\n84.1/91.3\\n\\n90%\\n936\\n29,529\\n61.9/84.2\\n60.9/83.0\\n63.7/85.0\\n66.1/84.8\\n\\n90%\\n1,041\\n29,358\\n63.4/84.5\\n62.4/83.8\\n65.0/85.1\\n67.4/85.3\\n\\n100%\\n751\\n12,936\\n87.5/95.1\\n88.2/95.3\\n91.0/96.4\\n91.9/96.6\\n\\n100%\\n751\\n12,936\\n87.5/95.1\\n88.2/95.3\\n91.0/96.4\\n91.9/96.6\\n\\n100%\\n702\\n16,522\\n79.4/89.0\\n79.5/89.1\\n82.1/91.0\\n84.3/92.0\\n\\n100%\\n702\\n16,522\\n79.4/89.0\\n79.5/89.1\\n82.1/91.0\\n84.3/92.0\\n\\n100%\\n1,041\\n32,621\\n63.7/85.1\\n62.7/84.3\\n65.7/85.5\\n68.0/86.0\\n\\n100%\\n1,041\\n32,621\\n63.7/85.1\\n62.7/84.3\\n65.7/85.5\\n68.0/86.0\\n\\nTable 14. Performance for small-scale and few-shot setting with MGN method for Market1501, DukeMTMC and MSMT17. “IN sup.” and\\n“IN unsup.” refer to supervised and unsupervised pre-trained model on ImageNet, “LUP unsup.” refers to unsupervised pre-trained model\\non LUPerson, “LUPws wsp.“ refers to our model pre-trained on LUPerson-WS using WSP. The first number is mAP and second is cmc1.\\n\\n\\x0c'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with one pdf first\n",
    "\n",
    "mypdftext=array_pdf_text[0]\n",
    "mypdftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a99fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[1] Binghui Chen, Weihong Deng, and Jiani Hu. Mixed high-\n",
      "order attention network for person re-identification. In Pro-\n",
      "ceedings of the IEEE International Conference on Computer\n",
      "Vision, pages 371–381, 2019. 8\n",
      "\n",
      "[2] Guangyi Chen, Chunze Lin, Liangliang Ren, Jiwen Lu, and\n",
      "Self-critical attention learning for person re-\n",
      "Jie Zhou.\n",
      "In Proceedings of the IEEE International\n",
      "identification.\n",
      "Conference on Computer Vision, pages 9637–9646, 2019. 8\n",
      "[3] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang\n",
      "Chen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-\n",
      "In Pro-\n",
      "net: Attentive but diverse person re-identification.\n",
      "ceedings of the IEEE International Conference on Computer\n",
      "Vision, pages 8351–8361, 2019. 8\n",
      "\n",
      "[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\n",
      "offrey Hinton. A simple framework for contrastive learning\n",
      "of visual representations. arXiv preprint arXiv:2002.05709,\n",
      "2020. 2, 4, 5\n",
      "\n",
      "[5] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\n",
      "Norouzi, and Geoffrey Hinton. Big self-supervised mod-\n",
      "arXiv preprint\n",
      "els are strong semi-supervised learners.\n",
      "arXiv:2006.10029, 2020. 2, 4, 5\n",
      "\n",
      "[6] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi\n",
      "Huang. Beyond triplet loss: a deep quadruplet network for\n",
      "In Proceedings of the IEEE con-\n",
      "person re-identification.\n",
      "ference on computer vision and pattern recognition, pages\n",
      "403–412, 2017. 2\n",
      "\n",
      "[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\n",
      "Improved baselines with momentum contrastive learning.\n",
      "arXiv preprint arXiv:2003.04297, 2020. 2, 4, 5\n",
      "\n",
      "[8] Zhirui Chen, Jianheng Li, and Wei-Shi Zheng. Weakly su-\n",
      "pervised tracklet person re-identification by deep feature-\n",
      "arXiv preprint arXiv:1910.14333,\n",
      "wise mutual learning.\n",
      "2019. 3\n",
      "\n",
      "[9] Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu,\n",
      "and Ping Tan. Batch dropblock network for person re-\n",
      "identification and beyond. In Proceedings of the IEEE Inter-\n",
      "national Conference on Computer Vision, pages 3691–3701,\n",
      "2019. 8\n",
      "\n",
      "[10] Piotr Doll´ar, Ron Appel, Serge Belongie, and Pietro Per-\n",
      "IEEE\n",
      "ona. Fast feature pyramids for object detection.\n",
      "transactions on pattern analysis and machine intelligence,\n",
      "36(8):1532–1545, 2014. 4\n",
      "\n",
      "[11] Pedro F Felzenszwalb, Ross B Girshick, David McAllester,\n",
      "and Deva Ramanan. Object detection with discriminatively\n",
      "IEEE transactions on pattern\n",
      "trained part-based models.\n",
      "analysis and machine intelligence, 32(9):1627–1645, 2009.\n",
      "4\n",
      "\n",
      "ings of the IEEE conference on computer vision and pattern\n",
      "recognition, 2021. 1, 2, 3, 4, 6, 7, 8\n",
      "\n",
      "[13] Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen,\n",
      "Jianmin Bao, Gang Hua, and Houqiang Li. Improving person\n",
      "re-identification with iterative impression aggregation. IEEE\n",
      "Transactions on Image Processing, 29:9559–9571, 2020. 8\n",
      "[14] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-\n",
      "teaching: Pseudo label refinery for unsupervised domain\n",
      "adaptation on person re-identification. In International Con-\n",
      "ference on Learning Representations, 2019. 2\n",
      "\n",
      "[15] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hong-\n",
      "sheng Li. Self-paced contrastive learning with hybrid mem-\n",
      "ory for domain adaptive object re-id. In Advances in Neural\n",
      "Information Processing Systems, 2020. 2, 7\n",
      "\n",
      "[16] Douglas Gray and Hai Tao. Viewpoint invariant pedes-\n",
      "trian recognition with an ensemble of localized features. In\n",
      "European conference on computer vision, pages 262–275.\n",
      "Springer, 2008. 4\n",
      "\n",
      "[17] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\n",
      "Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\n",
      "ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\n",
      "mad Gheshlaghi Azar, et al. Bootstrap your own latent: A\n",
      "new approach to self-supervised learning. arXiv preprint\n",
      "arXiv:2006.07733, 2020. 2, 4, 5\n",
      "\n",
      "[18] Xinqian Gu, Bingpeng Ma, Hong Chang, Shiguang Shan,\n",
      "and Xilin Chen. Temporal knowledge propagation for image-\n",
      "In Proceedings of the\n",
      "to-video person re-identification.\n",
      "IEEE/CVF International Conference on Computer Vision,\n",
      "pages 9647–9656, 2019. 2\n",
      "\n",
      "[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\n",
      "Girshick. Momentum contrast for unsupervised visual rep-\n",
      "resentation learning. In Proceedings of the IEEE/CVF Con-\n",
      "ference on Computer Vision and Pattern Recognition, pages\n",
      "9729–9738, 2020. 2, 4, 5\n",
      "\n",
      "[20] Lingxiao He and Wu Liu. Guided saliency feature learning\n",
      "for person re-identification in crowded scenes. In European\n",
      "Conference on Computer Vision, pages 357–373. Springer,\n",
      "2020. 8\n",
      "\n",
      "[21] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-\n",
      "fense of the triplet loss for person re-identification. arXiv\n",
      "preprint arXiv:1703.07737, 2017. 2, 6\n",
      "\n",
      "[22] Yan Huang, Qiang Wu, JingSong Xu, and Yi Zhong. Sbsgan:\n",
      "Suppression of inter-domain background shift for person re-\n",
      "identification. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 9527–9536, 2019. 2\n",
      "[23] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li\n",
      "Zhang. Style normalization and restitution for generalizable\n",
      "In Proceedings of the IEEE/CVF\n",
      "person re-identification.\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pages 3143–3152, 2020. 2\n",
      "\n",
      "[24] Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, and\n",
      "Zhibo Chen. Semantics-aligned representation learning for\n",
      "person re-identification. In AAAI, pages 11173–11180, 2020.\n",
      "8\n",
      "\n",
      "[12] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu\n",
      "Yuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsu-\n",
      "pervised pre-training for person re-identification. Proceed-\n",
      "\n",
      "[25] Srikrishna Karanam, Mengran Gou, Ziyan Wu, Angels\n",
      "Rates-Borras, Octavia Camps, and Richard J Radke. A\n",
      "comprehensive evaluation and benchmark for person re-\n",
      "\n",
      "\f",
      "identification: Features, metrics, and datasets. arXiv preprint\n",
      "arXiv:1605.09653, 2(3):5, 2016. 1, 4\n",
      "\n",
      "[26] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\n",
      "Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\n",
      "Dilip Krishnan. Supervised contrastive learning. Advances\n",
      "in Neural Information Processing Systems, 33, 2020. 7\n",
      "[27] Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, and Shiliang\n",
      "Zhang. Global-local temporal representations for video per-\n",
      "son re-identification. In Proceedings of the IEEE/CVF Inter-\n",
      "national Conference on Computer Vision, pages 3958–3967,\n",
      "2019. 2\n",
      "\n",
      "[28] Junnan Li, Caiming Xiong, and Steven Hoi. Mopro: Webly\n",
      "supervised learning with momentum prototypes. In Interna-\n",
      "tional Conference on Learning Representations, 2020. 2, 4\n",
      "[29] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsuper-\n",
      "vised tracklet person re-identification. IEEE transactions on\n",
      "pattern analysis and machine intelligence, 42(7):1770–1782,\n",
      "2019. 3\n",
      "\n",
      "[30] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-\n",
      "reid: Deep filter pairing neural network for person re-\n",
      "In Proceedings of the IEEE conference on\n",
      "identification.\n",
      "computer vision and pattern recognition, pages 152–159,\n",
      "2014. 4\n",
      "\n",
      "[31] Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, and\n",
      "Yu-Chiang Frank Wang. Recover and identify: A generative\n",
      "dual model for cross-resolution person re-identification. In\n",
      "Proceedings of the IEEE/CVF International Conference on\n",
      "Computer Vision, pages 8090–8099, 2019. 2\n",
      "\n",
      "[32] Yu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank\n",
      "Wang. Cross-dataset person re-identification via unsuper-\n",
      "vised pose disentanglement and adaptation. In Proceedings\n",
      "of the IEEE/CVF International Conference on Computer Vi-\n",
      "sion, pages 7919–7929, 2019. 2\n",
      "\n",
      "[33] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi\n",
      "Yang. A bottom-up clustering approach to unsupervised per-\n",
      "In Proceedings of the AAAI Confer-\n",
      "son re-identification.\n",
      "ence on Artificial Intelligence, volume 33, pages 8738–8745,\n",
      "2019. 2\n",
      "\n",
      "[34] Fangyi Liu and Lei Zhang. View confusion feature learning\n",
      "for person re-identification. In Proceedings of the IEEE/CVF\n",
      "International Conference on Computer Vision, pages 6639–\n",
      "6648, 2019. 2\n",
      "\n",
      "[35] Chen Change Loy, Chunxiao Liu, and Shaogang Gong. Per-\n",
      "son re-identification by manifold ranking. In 2013 IEEE In-\n",
      "ternational Conference on Image Processing, pages 3567–\n",
      "3571. IEEE, 2013. 4\n",
      "\n",
      "[36] Chuanchen Luo, Yuntao Chen, Naiyan Wang, and Zhaoxi-\n",
      "ang Zhang. Spectral feature transformation for person re-\n",
      "identification. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 4976–4985, 2019. 2\n",
      "[37] Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao,\n",
      "Shenqi Lai, and Jianyang Gu. A strong baseline and batch\n",
      "normalization neck for deep person re-identification. IEEE\n",
      "Transactions on Multimedia, 2019. 8\n",
      "\n",
      "[38] Jingke Meng, Sheng Wu, and Wei-Shi Zheng. Weakly su-\n",
      "In Proceedings of the\n",
      "pervised person re-identification.\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 760–769, 2019. 3\n",
      "\n",
      "[39] Hyunjong Park and Bumsub Ham. Relation network for per-\n",
      "son re-identification. In Proceedings of the AAAI Conference\n",
      "on Artificial Intelligence, volume 34, pages 11839–11847,\n",
      "2020. 8\n",
      "\n",
      "[40] Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie\n",
      "Qiu, Yang Wu, Yu-Gang Jiang, and Xiangyang Xue. Pose-\n",
      "normalized image generation for person re-identification. In\n",
      "Proceedings of the European conference on computer vision\n",
      "(ECCV), pages 650–667, 2018. 2\n",
      "\n",
      "[41] Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, and Yi\n",
      "Yang. Auto-reid: Searching for a part-aware convnet for\n",
      "person re-identification. In Proceedings of the IEEE Inter-\n",
      "national Conference on Computer Vision, pages 3750–3759,\n",
      "2019. 8\n",
      "\n",
      "[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n",
      "Faster r-cnn: Towards real-time object detection with region\n",
      "proposal networks. In Advances in neural information pro-\n",
      "cessing systems, pages 91–99, 2015. 4\n",
      "\n",
      "[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\n",
      "jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\n",
      "Aditya Khosla, Michael Bernstein, et al.\n",
      "Imagenet large\n",
      "scale visual recognition challenge. International journal of\n",
      "computer vision, 115(3):211–252, 2015. 6\n",
      "\n",
      "[44] Dong Shen, Shuai Zhao, Jinming Hu, Hao Feng, Deng Cai,\n",
      "and Xiaofei He. Es-net: Erasing salient parts to learn more\n",
      "in re-identification. IEEE Transactions on Image Processing,\n",
      "2020. 8\n",
      "\n",
      "[45] Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen,\n",
      "and Xiaogang Wang. Person re-identification with deep\n",
      "similarity-guided graph neural network. In Proceedings of\n",
      "the European conference on computer vision (ECCV), pages\n",
      "486–504, 2018. 2\n",
      "\n",
      "[46] Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Ky-\n",
      "oung Mu Lee. Part-aligned bilinear representations for per-\n",
      "son re-identification. In Proceedings of the European Con-\n",
      "ference on Computer Vision (ECCV), pages 402–419, 2018.\n",
      "2\n",
      "\n",
      "[47] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\n",
      "high-resolution representation learning for human pose esti-\n",
      "mation. In CVPR, 2019. 3\n",
      "\n",
      "[48] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\n",
      "Wang. Beyond part models: Person retrieval with refined\n",
      "part pooling (and a strong convolutional baseline). In ECCV,\n",
      "2018. 2\n",
      "\n",
      "[49] Dongkai Wang and Shiliang Zhang. Unsupervised person re-\n",
      "identification via multi-label classification. In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 10981–10990, 2020. 2\n",
      "\n",
      "[50] Guangrun Wang, Guangcong Wang, Xujie Zhang, Jianhuang\n",
      "Lai, Zhengtao Yu, and Liang Lin. Weakly supervised per-\n",
      "son re-id: Differentiable graphical learning and a new bench-\n",
      "mark. IEEE Transactions on Neural Networks and Learning\n",
      "Systems, 32(5):2142–2156, 2020. 3, 4\n",
      "\n",
      "[51] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\n",
      "Zhou. Learning discriminative features with multiple granu-\n",
      "larities for person re-identification. In 2018 ACM Multime-\n",
      "dia Conference on Multimedia Conference, pages 274–282.\n",
      "ACM, 2018. 1, 2, 6, 8\n",
      "\n",
      "\f",
      "[52] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\n",
      "Person transfer gan to bridge domain gap for person re-\n",
      "In Proceedings of the IEEE Conference on\n",
      "identification.\n",
      "Computer Vision and Pattern Recognition, pages 79–88,\n",
      "2018. 1, 4\n",
      "\n",
      "[53] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\n",
      "Unsupervised feature learning via non-parametric instance\n",
      "In Proceedings of the IEEE Conference\n",
      "discrimination.\n",
      "on Computer Vision and Pattern Recognition, pages 3733–\n",
      "3742, 2018. 2\n",
      "\n",
      "[54] Bryan Ning Xia, Yuan Gong, Yizhe Zhang, and Christian\n",
      "Poellabauer. Second-order non-local attention networks for\n",
      "person re-identification. In Proceedings of the IEEE Inter-\n",
      "national Conference on Computer Vision, pages 3760–3769,\n",
      "2019. 8\n",
      "\n",
      "[55] Ye Yuan, Wuyang Chen, Yang Yang, and Zhangyang Wang.\n",
      "In defense of the triplet loss again: Learning robust person\n",
      "re-identification with fast approximated triplet loss and label\n",
      "distillation. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition Workshops, pages\n",
      "354–355, 2020. 2\n",
      "\n",
      "[56] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\n",
      "and Wenyu Liu. Fairmot: On the fairness of detection and\n",
      "re-identification in multiple object tracking. arXiv e-prints,\n",
      "pages arXiv–2004, 2020. 2, 3, 4\n",
      "\n",
      "[57] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo\n",
      "Chen. Densely semantically aligned person re-identification.\n",
      "In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pages 667–676, 2019. 8\n",
      "\n",
      "[58] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\n",
      "dong Wang, and Qi Tian. Scalable person re-identification:\n",
      "A benchmark. 2015 IEEE International Conference on Com-\n",
      "puter Vision (ICCV), pages 1116–1124, 2015. 1, 4\n",
      "\n",
      "[59] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan\n",
      "Chandraker, Yi Yang, and Qi Tian. Person re-identification\n",
      "in the wild. In Proceedings of the IEEE Conference on Com-\n",
      "puter Vision and Pattern Recognition, pages 1367–1376,\n",
      "2017. 1, 2, 6\n",
      "\n",
      "[60] Zhedong Zheng, Liang Zheng, and Yi Yang. A discrimi-\n",
      "natively learned cnn embedding for person reidentification.\n",
      "ACM Transactions on Multimedia Computing, Communica-\n",
      "tions, and Applications (TOMM), 14(1):1–20, 2017. 2\n",
      "[61] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-\n",
      "ples generated by gan improve the person re-identification\n",
      "baseline in vitro. In Proceedings of the IEEE International\n",
      "Conference on Computer Vision, 2017. 1, 4\n",
      "\n",
      "[62] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-\n",
      "ranking person re-identification with k-reciprocal encoding.\n",
      "In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pages 1318–1327, 2017. 6, 8\n",
      "[63] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao\n",
      "Wang. Identity-guided human semantic parsing for person\n",
      "re-identification. ECCV, 2020. 8\n",
      "\n",
      "\f",
      "In this material, we will 1) show demo cases for scene\n",
      "changing for specific person in LUPerson-NL, 2) share the\n",
      "training details for PNL, 3) provide the detailed algorithm\n",
      "table for PNL, 4) compare models pre-trained on SYSU30K\n",
      "and LUPerson-NL 5) analyze the hyper parameters of our\n",
      "PNL, 6) demonstrate results of our method using a stronger\n",
      "backbones, 7) explore the impact of pre-training image\n",
      "scale, and 8) list more detailed results for our “small-scale”\n",
      "and “few-shot” experiments. All these experiments are un-\n",
      "der MGN settings.\n",
      "\n",
      "A. Scene changing in LUPerson-NL\n",
      "\n",
      "Our LUPerson-NL are driven from street view videos,\n",
      "Figure 6 shows a demo person in our LUPerson-NL. As we\n",
      "can see, our LUPerson-NL is able to cover multiple scenes,\n",
      "since the videos we use have lots of moving cameras and\n",
      "moving persons, and only traklets with ≥ 200 frames are\n",
      "selected. It is approximate close to the real person Re-ID\n",
      "scenario.\n",
      "\n",
      "C. Algorithm for PNL\n",
      "\n",
      "Algorithm 1 shows the procedure of training PNL, we\n",
      "train our framework for 90 epochs, and apply label correc-\n",
      "tion from 10 epochs. Once the rectification begins, we keep\n",
      "rectifying for every iteration.\n",
      "\n",
      "Algorithm 1: PNL algorithm.\n",
      "\n",
      "Input: total epochs N , correction start epochs Ns,\n",
      "prototype feature vectors {⃗c1, ⃗c2, . . . , ⃗cK}, where\n",
      "K is the number of identities, temperature τ ,\n",
      "threshold T , momentum m, encoder network\n",
      "Eq(·), classifier h(·), momentum encoder Ek(·),\n",
      "loss weight λpro and λlgc.\n",
      "for epoch = 1 : N do\n",
      "\n",
      "{(⃗xi, yi)}b\n",
      "for i ∈ {1, ..., b} do\n",
      "\n",
      "i=1 sampled from data loader.\n",
      "\n",
      "˜⃗xi = aug1(⃗xi), ˜⃗x′\n",
      "i = aug2(⃗xi)\n",
      "⃗qi = Eq(˜⃗xi), ⃗ki = Ek(˜⃗x′\n",
      "i)\n",
      "⃗pi = h(⃗qi)\n",
      "k=1, sk\n",
      "i }K\n",
      "⃗si = {sk\n",
      "⃗li = (⃗pi + ⃗si)/2\n",
      "if epoch > Ns and maxk lj\n",
      "\n",
      "(cid:80)K\n",
      "\n",
      "i = exp( ⃗qi·⃗ck/τ )\n",
      "\n",
      "k=1 exp(⃗qi·⃗ck/τ )\n",
      "\n",
      "i > T then\n",
      "\n",
      "ˆyi = arg maxj\n",
      "\n",
      "⃗lj\n",
      "i\n",
      "\n",
      "else\n",
      "\n",
      "ˆyi = ⃗yi\n",
      "\n",
      "Li\n",
      "Li\n",
      "Li\n",
      "\n",
      "ce = − log(⃗pi[ˆyi])\n",
      "pro = −log\n",
      "lgc =\n",
      "\n",
      "(cid:80)K\n",
      "\n",
      "exp(⃗qi·⃗c ˆyi /τ )\n",
      "j=1 exp(⃗qi·⃗cj /τ )\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "⃗qi·⃗k+\n",
      "τ\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:80)\n",
      "X\n",
      "⃗qi·⃗k+\n",
      "τ\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "−1\n",
      "|P(i)| log\n",
      "\n",
      "⃗qi·⃗k−\n",
      "τ\n",
      "\n",
      "exp\n",
      "\n",
      "+(cid:80)\n",
      "exp\n",
      "⃗k−∈N (i)\n",
      "\n",
      "(cid:80)\n",
      "⃗k+ ∈P(i)\n",
      "⃗cˆyi = m⃗cˆyi + (1 − m)⃗qi\n",
      "ce + λproLi\n",
      "Li = Li\n",
      "update networks Eq, h to minimize Li\n",
      "update networks Eq, h to minimize Li\n",
      "update momentum encoder Ek.\n",
      "\n",
      "pro + λlgcLi\n",
      "\n",
      "lgc\n",
      "\n",
      "D. Compare with SYSU30K\n",
      "\n",
      "As show in Table 9, we compare the pre-trained mod-\n",
      "els between LUPerson-NL and SYSU30K. We pre-train\n",
      "PNL on both LUPerson-NL and SYSU30K for 40 epochs\n",
      "with same experiment settings for a fair comparison. The\n",
      "performance of LUPerson-NL pre-training is much better\n",
      "than SYSU30K pre-training, showing the superiority of our\n",
      "LUPerson-NL, and also suggesting that a large number of\n",
      "images with limited diversity does not bring more represen-\n",
      "tative representation, but large-scale images with diversity\n",
      "does.\n",
      "\n",
      "Figure 6. Scene changing for a specific person in LUPerson-NL.\n",
      "\n",
      "B. Training details\n",
      "\n",
      "During training, all\n",
      "\n",
      "images are resized to 256 ×\n",
      "128 and pass through the same augmentations veri-\n",
      "fied in [12], which are Random Resized Crop, Hor-\n",
      "izontal Flip, Normalization, Random Gaussian Blur,\n",
      "Specifi-\n",
      "Random Gray Scale and Random Erasing.\n",
      "cally, the images are normalized with mean and std of\n",
      "[0.3452, 0.3070, 0.3114], [0.2633, 0.2500, 0.2480], which\n",
      "are calculated from all images in LUPerson-NL. We train\n",
      "our model on 8 × V 100 GPUs for 90 epochs with a batch\n",
      "size of 1, 536. The initial learning rate is set to 0.4 with\n",
      "a step-wise decay by 0.1 for every 40 epochs. The opti-\n",
      "mizer is SGD with a momentum of 0.9 and a weight de-\n",
      "cay of 0.0001. We set the hyper-parameters τ = 0.1 and\n",
      "T = 0.8. The momentum m for updating both the momen-\n",
      "tum encoder Ek and the prototypes is set to 0.999. We de-\n",
      "sign a large queue with a size of 65, 536 to increase the oc-\n",
      "currence of positive samples for the label guided contrastive\n",
      "learning. The label correction according to Equation 3 starts\n",
      "from the 10-th epoch. The deployment of the label guided\n",
      "contrastive loss Li\n",
      "\n",
      "lgc starts from the 15-th epoch.\n",
      "\n",
      "\f",
      "E.1. Temperature Factor τ\n",
      "\n",
      "Table 13. Comparison for different pre-training data scale\n",
      "\n",
      "Dataset\n",
      "MSMT17\n",
      "\n",
      "LUPerson-NL\n",
      "66.1/84.6\n",
      "\n",
      "SYSU30K\n",
      "55.2/76.7\n",
      "\n",
      "Table 9. Comparison of applying our PNL on both LUPerson-NL\n",
      "and SYSU30K.\n",
      "\n",
      "τ\n",
      "\n",
      "0.05\n",
      "0.07\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "\n",
      "DukeMTMC\n",
      "\n",
      "MSMT17\n",
      "\n",
      "40%\n",
      "76.1/87.2\n",
      "76.4/87.5\n",
      "77.0/87.3\n",
      "75.8/86.8\n",
      "74.7/86.2\n",
      "\n",
      "100%\n",
      "83.5/91.4\n",
      "83.6/91.4\n",
      "84.3/92.0\n",
      "83.4/91.0\n",
      "82.7/90.6\n",
      "\n",
      "40%\n",
      "49.8/73.3\n",
      "50.7/74.1\n",
      "51.9/74.9\n",
      "50.1/73.7\n",
      "48.6/72.0\n",
      "\n",
      "100%\n",
      "65.4/83.8\n",
      "67.2/85.3\n",
      "68.0/86.0\n",
      "66.4/85.0\n",
      "65.5/83.7\n",
      "\n",
      "Table 10. Performances under different τ values on DukeMTMC\n",
      "and MSMT17 with data percentages 40% and 100% under the\n",
      "small scale setting. The threshold is set as T = 0.8. The best\n",
      "scores are in bold and the second ones are underlined.\n",
      "\n",
      "E. Hyper Parameter Analysis\n",
      "\n",
      "In our PNL, there are two key hyper-parameters: tem-\n",
      "perature factor τ and the threshold for correction T . Here,\n",
      "we provide the analysis of these two parameters.\n",
      "\n",
      "Table 10 shows the performance comparison with differ-\n",
      "ent τ values with a fixed label correction threshold T = 0.8.\n",
      "As we can see, the setting τ = 0.1 achieves the best re-\n",
      "sults on both DukeMTMC and MSMT17, while τ = 0.07\n",
      "achieves the second best. When we use a larger τ , the\n",
      "performance drops rapidly. It may be because Re-ID is a\n",
      "more fine-grained task, larger τ will cause smaller inter-\n",
      "class variations and make positive samples too close to neg-\n",
      "ative samples. In all the experiments, we set τ = 0.1.\n",
      "\n",
      "E.2. Threshold T\n",
      "\n",
      "Table 11 shows the results with different label correc-\n",
      "tion threshold values T . As we can see, the performance\n",
      "is relatively stable to different T values varying in a large\n",
      "range of 0.6 ∼ 0.8. However, if T is too small or too large,\n",
      "the performance drops rapidly. For the former case, labels\n",
      "are easier to be modified, which may cause wrong rectifi-\n",
      "cations, while for the latter case, the label noises become\n",
      "harder to be corrected, which also has consistently negative\n",
      "effects on the performance. In all the experiments, we set\n",
      "T = 0.8.\n",
      "\n",
      "F. Results for stronger backbones\n",
      "\n",
      "We train our PNL using two stronger backbones\n",
      "ResNet101 and ResNet152, and report the results in Ta-\n",
      "ble 12. As we can see, the stronger ResNet bring more\n",
      "superior performances. These results also outperform the\n",
      "\n",
      "T\n",
      "\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "\n",
      "DukeMTMC\n",
      "\n",
      "MSMT17\n",
      "\n",
      "40%\n",
      "76.1/86.5\n",
      "77.1/87.7\n",
      "77.0/87.5\n",
      "77.0/87.3\n",
      "75.7/86.4\n",
      "\n",
      "100%\n",
      "83.3/91.0\n",
      "84.1/91.6\n",
      "84.0/91.8\n",
      "84.3/92.0\n",
      "83.0/90.8\n",
      "\n",
      "40%\n",
      "51.1/74.6\n",
      "52.3/75.5\n",
      "51.9/75.6\n",
      "51.9/75.0\n",
      "50.9/74.3\n",
      "\n",
      "100%\n",
      "67.5/85.2\n",
      "68.1/85.7\n",
      "68.2/85.8\n",
      "68.0/86.0\n",
      "67.2/85.0\n",
      "\n",
      "Table 11. Performances under different T values on DukeMTMC\n",
      "and MSMT17 with data percentages 40% and 100% under the\n",
      "small scale setting. The temperature factor is set as τ = 0.1. The\n",
      "best scores are in bold and the second ones are underlined.\n",
      "\n",
      "Arch CUHK03 Market1501 DukeMTMC MSMT17\n",
      "68.0/86.0\n",
      "R50\n",
      "80.4/80.9\n",
      "70.8/87.1\n",
      "R101 80.5/81.2\n",
      "71.6/87.5\n",
      "R152 80.6/81.2\n",
      "\n",
      "84.3/92.0\n",
      "85.5/92.8\n",
      "85.6/92.4\n",
      "\n",
      "91.9/96.6\n",
      "92.5/96.9\n",
      "92.7/96.8\n",
      "\n",
      "Table 12. Results with different ResNet backbones. R50, R101\n",
      "and R152 stand for ResNet50, ResNet101 and ResNet152 respec-\n",
      "tively.\n",
      "\n",
      "Scale\n",
      "MSMT17\n",
      "\n",
      "10%\n",
      "57.4/79.2\n",
      "\n",
      "30%\n",
      "62.2/82.1\n",
      "\n",
      "100%\n",
      "68.0/86.0\n",
      "\n",
      "scores reported in Table 8 of our main submission. Most\n",
      "importantly, we are the FIRST to obtain a mAP score on\n",
      "MSMT17 that is larger than 70 without any post-processing\n",
      "for convolutional network.\n",
      "\n",
      "G. Pre-training data scales\n",
      "\n",
      "We study the impact of pre-training data scale. Specif-\n",
      "ically, we involve various percentages (10%,30%,100%\n",
      "pseudo based) of LUPerson-NL into pre-training and then\n",
      "evaluate the finetuing performance on the target datasets.\n",
      "As shown in Table 13, the learned representation is much\n",
      "stronger with the increase of the pre-training data scale, in-\n",
      "dicating the necessity of building large-scale dataset, and\n",
      "our LUPerson-NL is very important.\n",
      "\n",
      "H. More results for small-scale and few-shot\n",
      "\n",
      "To complement Table 3 in the main text, we provide\n",
      "more detailed results under “small scale” and “few shot”\n",
      "settings in Table 14. As we can see, our weakly pre-trained\n",
      "models are consistently better than other pre-trained mod-\n",
      "els. Our advantage is much larger with less training data,\n",
      "suggesting the potential practical value of our pre-trained\n",
      "models for real-world person ReID applications.\n",
      "\n",
      "\f",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "scale\n",
      "#id\n",
      "#images\n",
      "IN sup.\n",
      "IN unsup.\n",
      "LUP unsup.\n",
      "LUPws wsp.\n",
      "\n",
      "10%\n",
      "75\n",
      "1,170\n",
      "53.1/76.9\n",
      "58.4/81.7\n",
      "64.6/85.5\n",
      "72.4/88.8\n",
      "\n",
      "10%\n",
      "751\n",
      "1,293\n",
      "21.1/41.8\n",
      "18.6/36.1\n",
      "26.4/47.5\n",
      "42.0/61.6\n",
      "\n",
      "10%\n",
      "70\n",
      "1,670\n",
      "45.1/65.3\n",
      "48.1/66.9\n",
      "53.5/72.0\n",
      "60.6/75.8\n",
      "\n",
      "10%\n",
      "702\n",
      "1,679\n",
      "31.5/47.1\n",
      "32.4/48.0\n",
      "35.8/50.2\n",
      "52.2/64.1\n",
      "\n",
      "10%\n",
      "104\n",
      "3,659\n",
      "23.2/50.2\n",
      "22.6/48.8\n",
      "25.5/51.1\n",
      "28.2/51.1\n",
      "\n",
      "10%\n",
      "1,041\n",
      "3,262\n",
      "14.7/34.1\n",
      "13.2/29.2\n",
      "17.0/36.0\n",
      "24.5/42.7\n",
      "\n",
      "20%\n",
      "150\n",
      "2,643\n",
      "67.7/86.8\n",
      "70.2/89.1\n",
      "76.9/92.1\n",
      "81.7/93.2\n",
      "\n",
      "20%\n",
      "751\n",
      "2,587\n",
      "53.2/75.1\n",
      "56.5/77.5\n",
      "63.5/83.0\n",
      "75.7/89.1\n",
      "\n",
      "20%\n",
      "140\n",
      "3,192\n",
      "58.3/75.4\n",
      "60.6/76.6\n",
      "65.0/78.9\n",
      "70.5/83.3\n",
      "\n",
      "20%\n",
      "702\n",
      "3,321\n",
      "56.2/72.1\n",
      "56.4/72.2\n",
      "61.0/74.9\n",
      "71.7/82.5\n",
      "\n",
      "20%\n",
      "208\n",
      "6,471\n",
      "34.6/64.0\n",
      "32.7/60.9\n",
      "36.0/62.6\n",
      "39.6/63.7\n",
      "\n",
      "20%\n",
      "1,041\n",
      "6,524\n",
      "35.6/61.4\n",
      "33.5/58.6\n",
      "37.4/61.4\n",
      "45.6/67.2\n",
      "\n",
      "30%\n",
      "225\n",
      "3,962\n",
      "75.2/90.8\n",
      "76.6/91.9\n",
      "81.9/93.7\n",
      "85.2/94.2\n",
      "\n",
      "30%\n",
      "751\n",
      "3,880\n",
      "68.1/87.6\n",
      "69.3/87.8\n",
      "78.3/92.1\n",
      "83.7/94.0\n",
      "\n",
      "30%\n",
      "210\n",
      "5,530\n",
      "64.7/80.2\n",
      "65.8/80.2\n",
      "69.4/81.9\n",
      "74.5/86.3\n",
      "\n",
      "30%\n",
      "702\n",
      "4,938\n",
      "65.4/79.8\n",
      "65.3/80.2\n",
      "72.3/83.8\n",
      "77.7/87.9\n",
      "\n",
      "30%\n",
      "312\n",
      "9,787\n",
      "41.9/70.8\n",
      "40.4/68.7\n",
      "44.6/71.4\n",
      "47.7/71.2\n",
      "\n",
      "30%\n",
      "1,041\n",
      "9,786\n",
      "44.5/71.1\n",
      "41.4/67.1\n",
      "49.0/73.6\n",
      "53.2/74.4\n",
      "\n",
      "40%\n",
      "300\n",
      "5,226\n",
      "79.1/92.5\n",
      "80.0/93.0\n",
      "84.1/94.4\n",
      "87.3/95.1\n",
      "\n",
      "50%\n",
      "375\n",
      "6,408\n",
      "81.5/93.5\n",
      "82.0/94.1\n",
      "85.8/94.9\n",
      "88.3/95.5\n",
      "\n",
      "60%\n",
      "450\n",
      "7,814\n",
      "81.5/93.5\n",
      "83.7/94.3\n",
      "87.8/95.8\n",
      "89.6/96.0\n",
      "\n",
      "(a) Market1501 small-scale\n",
      "40%\n",
      "751\n",
      "5,174\n",
      "75.4/90.4\n",
      "78.8/88.3\n",
      "80.3/92.7\n",
      "86.0/94.3\n",
      "\n",
      "50%\n",
      "751\n",
      "6,468\n",
      "80.2/92.8\n",
      "78.3/90.9\n",
      "84.2/93.9\n",
      "88.1/95.2\n",
      "\n",
      "60%\n",
      "751\n",
      "7,758\n",
      "83.0/93.6\n",
      "81.7/93.3\n",
      "86.7/94.7\n",
      "89.8/95.8\n",
      "\n",
      "(b) Market1501 few-shot\n",
      "\n",
      "40%\n",
      "280\n",
      "6,924\n",
      "68.5/83.0\n",
      "69.5/82.9\n",
      "72.8/84.7\n",
      "77.0/87.3\n",
      "\n",
      "50%\n",
      "351\n",
      "8,723\n",
      "71.8/84.6\n",
      "72.5/84.4\n",
      "75.6/86.7\n",
      "78.8/88.3\n",
      "\n",
      "60%\n",
      "421\n",
      "10,197\n",
      "74.1/85.6\n",
      "75.0/86.2\n",
      "77.6/87.1\n",
      "80.5/89.2\n",
      "\n",
      "(c) DukeMTMC small-scale\n",
      "40%\n",
      "702\n",
      "6,599\n",
      "71.0/83.9\n",
      "70.2/83.4\n",
      "75.2/86.8\n",
      "79.3/88.1\n",
      "\n",
      "50%\n",
      "702\n",
      "8,278\n",
      "73.9/85.7\n",
      "73.7/85.1\n",
      "77.7/87.4\n",
      "81.1/89.6\n",
      "\n",
      "60%\n",
      "702\n",
      "9,923\n",
      "75.8/86.6\n",
      "75.8/86.7\n",
      "79.4/88.4\n",
      "82.3/90.2\n",
      "\n",
      "(d) DukeMTMC few-shot\n",
      "40%\n",
      "416\n",
      "13,006\n",
      "46.7/74.5\n",
      "45.1/72.2\n",
      "49.2/74.9\n",
      "51.9/74.9\n",
      "\n",
      "50%\n",
      "520\n",
      "15,917\n",
      "50.3/76.9\n",
      "49.0/75.0\n",
      "53.0/77.7\n",
      "55.5/77.2\n",
      "\n",
      "60%\n",
      "624\n",
      "19,672\n",
      "53.9/79.4\n",
      "52.7/78.0\n",
      "56.4/79.7\n",
      "59.1/80.1\n",
      "\n",
      "(e) MSMT17 small-scale\n",
      "\n",
      "40%\n",
      "1,041\n",
      "13,048\n",
      "52.0/76.9\n",
      "47.7/72.7\n",
      "53.9/78.5\n",
      "58.6/78.8\n",
      "\n",
      "50%\n",
      "1,041\n",
      "16,310\n",
      "56.2/79.5\n",
      "53.3/77.6\n",
      "57.4/80.5\n",
      "62.2/81.0\n",
      "\n",
      "60%\n",
      "1,041\n",
      "19,572\n",
      "58.8/81.7\n",
      "56.5/79.6\n",
      "60.0/82.1\n",
      "64.1/82.6\n",
      "\n",
      "(f) MSMT17 few-shot\n",
      "\n",
      "70%\n",
      "525\n",
      "9,120\n",
      "84.8/94.5\n",
      "85.4/94.5\n",
      "88.8/95.9\n",
      "90.1/96.2\n",
      "\n",
      "70%\n",
      "751\n",
      "9,055\n",
      "84.2/94.0\n",
      "84.4/94.1\n",
      "88.4/95.5\n",
      "90.5/96.3\n",
      "\n",
      "70%\n",
      "491\n",
      "11,939\n",
      "75.5/86.8\n",
      "76.3/86.9\n",
      "78.9/88.2\n",
      "81.6/89.5\n",
      "\n",
      "70%\n",
      "702\n",
      "11,564\n",
      "77.2/87.8\n",
      "77.7/88.2\n",
      "80.8/89.2\n",
      "83.2/91.1\n",
      "\n",
      "70%\n",
      "728\n",
      "22,680\n",
      "56.9/81.2\n",
      "55.7/79.9\n",
      "59.5/81.8\n",
      "61.6/81.8\n",
      "\n",
      "70%\n",
      "1,041\n",
      "22,834\n",
      "60.9/82.8\n",
      "59.1/81.5\n",
      "62.9/83.5\n",
      "65.8/83.8\n",
      "\n",
      "80%\n",
      "600\n",
      "11,417\n",
      "85.9/95.2\n",
      "86.4/95.0\n",
      "89.8/96.2\n",
      "90.9/96.4\n",
      "\n",
      "80%\n",
      "751\n",
      "10,348\n",
      "86.3/94.7\n",
      "86.4/95.0\n",
      "89.8/96.0\n",
      "91.2/96.4\n",
      "\n",
      "80%\n",
      "561\n",
      "13,500\n",
      "76.8/87.3\n",
      "77.4/87.3\n",
      "80.2/89.2\n",
      "82.9/90.6\n",
      "\n",
      "80%\n",
      "702\n",
      "13,201\n",
      "78.3/88.6\n",
      "78.7/88.7\n",
      "81.7/90.3\n",
      "84.0/91.6\n",
      "\n",
      "80%\n",
      "832\n",
      "26,335\n",
      "59.6/82.4\n",
      "58.6/82.0\n",
      "61.9/83.6\n",
      "64.2/83.3\n",
      "\n",
      "80%\n",
      "1,041\n",
      "26,096\n",
      "62.5/84.2\n",
      "60.9/82.3\n",
      "64.2/84.5\n",
      "67.2/84.7\n",
      "\n",
      "90%\n",
      "675\n",
      "11,727\n",
      "86.9/95.2\n",
      "87.4/95.5\n",
      "90.5/96.4\n",
      "91.3/96.4\n",
      "\n",
      "90%\n",
      "751\n",
      "11,642\n",
      "86.7/94.6\n",
      "87.1/95.2\n",
      "90.4/96.3\n",
      "91.6/96.4\n",
      "\n",
      "90%\n",
      "631\n",
      "15,111\n",
      "78.0/88.3\n",
      "78.5/88.7\n",
      "81.1/90.0\n",
      "83.3/91.2\n",
      "\n",
      "90%\n",
      "702\n",
      "14,860\n",
      "79.1/88.8\n",
      "79.4/89.0\n",
      "82.0/90.6\n",
      "84.1/91.3\n",
      "\n",
      "90%\n",
      "936\n",
      "29,529\n",
      "61.9/84.2\n",
      "60.9/83.0\n",
      "63.7/85.0\n",
      "66.1/84.8\n",
      "\n",
      "90%\n",
      "1,041\n",
      "29,358\n",
      "63.4/84.5\n",
      "62.4/83.8\n",
      "65.0/85.1\n",
      "67.4/85.3\n",
      "\n",
      "100%\n",
      "751\n",
      "12,936\n",
      "87.5/95.1\n",
      "88.2/95.3\n",
      "91.0/96.4\n",
      "91.9/96.6\n",
      "\n",
      "100%\n",
      "751\n",
      "12,936\n",
      "87.5/95.1\n",
      "88.2/95.3\n",
      "91.0/96.4\n",
      "91.9/96.6\n",
      "\n",
      "100%\n",
      "702\n",
      "16,522\n",
      "79.4/89.0\n",
      "79.5/89.1\n",
      "82.1/91.0\n",
      "84.3/92.0\n",
      "\n",
      "100%\n",
      "702\n",
      "16,522\n",
      "79.4/89.0\n",
      "79.5/89.1\n",
      "82.1/91.0\n",
      "84.3/92.0\n",
      "\n",
      "100%\n",
      "1,041\n",
      "32,621\n",
      "63.7/85.1\n",
      "62.7/84.3\n",
      "65.7/85.5\n",
      "68.0/86.0\n",
      "\n",
      "100%\n",
      "1,041\n",
      "32,621\n",
      "63.7/85.1\n",
      "62.7/84.3\n",
      "65.7/85.5\n",
      "68.0/86.0\n",
      "\n",
      "Table 14. Performance for small-scale and few-shot setting with MGN method for Market1501, DukeMTMC and MSMT17. “IN sup.” and\n",
      "“IN unsup.” refer to supervised and unsupervised pre-trained model on ImageNet, “LUP unsup.” refers to unsupervised pre-trained model\n",
      "on LUPerson, “LUPws wsp.“ refers to our model pre-trained on LUPerson-WS using WSP. The first number is mAP and second is cmc1.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#First filter to have only the References from a pdf\n",
    "\n",
    "def after_references(mypdftext): \n",
    "    keyword1 = 'References'\n",
    "    keyword2 = 'REFERENCES'\n",
    "    keyword3 = 'R EFERENCES'\n",
    "    keyword4 = 'Reference'\n",
    "    keyword5='[1]' \n",
    "\n",
    "    if keyword1 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword1)\n",
    "    elif keyword2 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword2)\n",
    "    elif keyword3 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword3)\n",
    "    elif keyword4 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword4)\n",
    "    elif keyword5 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword5)\n",
    "    else:\n",
    "        after_keyword = mypdftext[:10000]\n",
    "    return after_keyword\n",
    "\n",
    "#All references in a variable\n",
    "\n",
    "references=after_references(mypdftext)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439d489",
   "metadata": {},
   "source": [
    "## Preprocess to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e325730d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First cleaning\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns): \n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s) \n",
    "        return s\n",
    "\n",
    "replacer=RegexpReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e46e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess a text before using IA\n",
    "\n",
    "def preprocess_text(test):\n",
    "\n",
    "    #Removing Numbers\n",
    "    test=re.sub(r'\\d+','',test)\n",
    "\n",
    "    #Removing Letter alone\n",
    "    test = re.sub(r'\\b\\w\\b', ' ', test)\n",
    "\n",
    "    #Removing white spaces\n",
    "    test=test.strip()\n",
    "    \n",
    "    #Replacer replace\n",
    "    text_replaced = replacer.replace(test)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text_replaced)\n",
    "\n",
    "    #Tokenize words\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = tokenizer.tokenize(sentences[i])\n",
    "\n",
    "    #Remove stop words\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stops=set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [word for word in sentences[i] if word not in stops]\n",
    "\n",
    "    #Lemmatize\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer_output=WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            sentences[i][j] = lemmatizer_output.lemmatize(sentences[i][j])\n",
    "\n",
    "\n",
    "    #Join the words back into a sentence.\n",
    "    a=[' '.join(s) for s in sentences]\n",
    "    b=', '.join(a)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c077ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binghui Chen Weihong Deng Jiani Hu, Mixed high order attention network person identification, In Pro ceedings IEEE International Conference Computer Vision page, Guangyi Chen Chunze Lin Liangliang Ren Jiwen Lu Self critical attention learning person Jie Zhou, In Proceedings IEEE International identification, Conference Computer Vision page, Tianlong Chen Shaojin Ding Jingyi Xie Ye Yuan Wuyang Chen Yang Yang Zhou Ren Zhangyang Wang, Abd In Pro net Attentive diverse person identification, ceedings IEEE International Conference Computer Vision page, Ting Chen Simon Kornblith Mohammad Norouzi Ge offrey Hinton, simple framework contrastive learning visual representation, arXiv preprint arXiv, Ting Chen Simon Kornblith Kevin Swersky Mohammad Norouzi Geoffrey Hinton, Big self supervised mod arXiv preprint el strong semi supervised learner, arXiv, Weihua Chen Xiaotang Chen Jianguo Zhang Kaiqi Huang, Beyond triplet loss deep quadruplet network In Proceedings IEEE con person identification, ference computer vision pattern recognition page, Xinlei Chen Haoqi Fan Ross Girshick Kaiming He, Improved baseline momentum contrastive learning, arXiv preprint arXiv, Zhirui Chen Jianheng Li Wei Shi Zheng, Weakly su pervised tracklet person identification deep feature arXiv preprint arXiv wise mutual learning Zuozhuo Dai Mingqiang Chen Xiaodong Gu Siyu Zhu Ping Tan, Batch dropblock network person identification beyond, In Proceedings IEEE Inter national Conference Computer Vision page, Piotr Doll ar Ron Appel Serge Belongie Pietro Per IEEE ona, Fast feature pyramid object detection, transaction pattern analysis machine intelligence, Pedro Felzenszwalb Ross Girshick David McAllester Deva Ramanan, Object detection discriminatively IEEE transaction pattern trained part based model, analysis machine intelligence, ings IEEE conference computer vision pattern recognition, Dengpan Fu Bo Xin Jingdong Wang Dongdong Chen Jianmin Bao Gang Hua Houqiang Li, Improving person identification iterative impression aggregation, IEEE Transactions Image Processing, Yixiao Ge Dapeng Chen Hongsheng Li, Mutual mean teaching Pseudo label refinery unsupervised domain adaptation person identification, In International Con ference Learning Representations, Yixiao Ge Feng Zhu Dapeng Chen Rui Zhao Hong sheng Li, Self paced contrastive learning hybrid mem ory domain adaptive object id, In Advances Neural Information Processing Systems, Douglas Gray Hai Tao, Viewpoint invariant pes trian recognition ensemble localized feature, In European conference computer vision page, Springer, Jean Bastien Grill Florian Strub Florent Altch Corentin Tallec Pierre Richemond Elena Buchatskaya Carl Do ersch Bernardo Avila Pires Zhaohan Daniel Guo Moham mad Gheshlaghi Azar et al, Bootstrap latent new approach self supervised learning, arXiv preprint arXiv, Xinqian Gu Bingpeng Ma Hong Chang Shiguang Shan Xilin Chen, Temporal knowledge propagation image In Proceedings video person identification, IEEE CVF International Conference Computer Vision page, Kaiming He Haoqi Fan Yuxin Wu Saining Xie Ross Girshick, Momentum contrast unsupervised visual rep resentation learning, In Proceedings IEEE CVF Con ference Computer Vision Pattern Recognition page, Lingxiao He Wu Liu, Guided saliency feature learning person identification crowded scene, In European Conference Computer Vision page, Springer, Alexander Hermans Lucas Beyer Bastian Leibe, In de fense triplet loss person identification, arXiv preprint arXiv, Yan Huang Qiang Wu JingSong Xu Yi Zhong, Sbsgan Suppression inter domain background shift person identification, In Proceedings IEEE CVF International Conference Computer Vision page, Xin Jin Cuiling Lan Wenjun Zeng Zhibo Chen Li Zhang, Style normalization restitution generalizable In Proceedings IEEE CVF person identification, Conference Computer Vision Pattern Recognition page, Xin Jin Cuiling Lan Wenjun Zeng Guoqiang Wei Zhibo Chen, Semantics aligned representation learning person identification, In AAAI page, Dengpan Fu Dongdong Chen Jianmin Bao Hao Yang Lu Yuan Lei Zhang Houqiang Li Dong Chen, Unsu pervised pre training person identification, Proceed Srikrishna Karanam Mengran Gou Ziyan Wu Angels Rates Borras Octavia Camps Richard Radke, comprehensive evaluation benchmark person identification Features metric datasets, arXiv preprint arXiv, Prannay Khosla Piotr Teterwak Chen Wang Aaron Sarna Yonglong Tian Phillip Isola Aaron Maschinot Ce Liu Dilip Krishnan, Supervised contrastive learning, Advances Neural Information Processing Systems, Jianing Li Jingdong Wang Qi Tian Wen Gao Shiliang Zhang, Global local temporal representation video per son identification, In Proceedings IEEE CVF Inter national Conference Computer Vision page, Junnan Li Caiming Xiong Steven Hoi, Mopro Webly supervised learning momentum prototype, In Interna tional Conference Learning Representations, Minxian Li Xiatian Zhu Shaogang Gong, Unsuper vised tracklet person identification, IEEE transaction pattern analysis machine intelligence, Wei Li Rui Zhao Tong Xiao Xiaogang Wang, Deep reid Deep filter pairing neural network person In Proceedings IEEE conference identification, computer vision pattern recognition page, Yu Jhe Li Yun Chun Chen Yen Yu Lin Xiaofei Du Yu Chiang Frank Wang, Recover identify generative dual model cross resolution person identification, In Proceedings IEEE CVF International Conference Computer Vision page, Yu Jhe Li Ci Siang Lin Yan Bo Lin Yu Chiang Frank Wang, Cross dataset person identification via unsuper vised pose disentanglement adaptation, In Proceedings IEEE CVF International Conference Computer Vi sion page, Yutian Lin Xuanyi Dong Liang Zheng Yan Yan Yi Yang, bottom clustering approach unsupervised per In Proceedings AAAI Confer son identification, ence Artificial Intelligence volume page, Fangyi Liu Lei Zhang, View confusion feature learning person identification, In Proceedings IEEE CVF International Conference Computer Vision page, Chen Change Loy Chunxiao Liu Shaogang Gong, Per son identification manifold ranking, In IEEE In ternational Conference Image Processing page, IEEE, Chuanchen Luo Yuntao Chen Naiyan Wang Zhaoxi ang Zhang, Spectral feature transformation person identification, In Proceedings IEEE CVF International Conference Computer Vision page, Hao Luo Wei Jiang Youzhi Gu Fuxu Liu Xingyu Liao Shenqi Lai Jianyang Gu, strong baseline batch normalization neck deep person identification, IEEE Transactions Multimedia, Jingke Meng Sheng Wu Wei Shi Zheng, Weakly su In Proceedings pervised person identification, IEEE CVF Conference Computer Vision Pattern Recognition page, Hyunjong Park Bumsub Ham, Relation network per son identification, In Proceedings AAAI Conference Artificial Intelligence volume page, Xuelin Qian Yanwei Fu Tao Xiang Wenxuan Wang Jie Qiu Yang Wu Yu Gang Jiang Xiangyang Xue, Pose normalized image generation person identification, In Proceedings European conference computer vision ECCV page, Ruijie Quan Xuanyi Dong Yu Wu Linchao Zhu Yi Yang, Auto reid Searching part aware convnet person identification, In Proceedings IEEE Inter national Conference Computer Vision page, Shaoqing Ren Kaiming He Ross Girshick Jian Sun, Faster cnn Towards real time object detection region proposal network, In Advances neural information pro cessing system page, Olga Russakovsky Jia Deng Hao Su Jonathan Krause San jeev Satheesh Sean Ma Zhiheng Huang Andrej Karpathy Aditya Khosla Michael Bernstein et al, Imagenet large scale visual recognition challenge, International journal computer vision, Dong Shen Shuai Zhao Jinming Hu Hao Feng Deng Cai Xiaofei He, Es net Erasing salient part learn identification, IEEE Transactions Image Processing, Yantao Shen Hongsheng Li Shuai Yi Dapeng Chen Xiaogang Wang, Person identification deep similarity guided graph neural network, In Proceedings European conference computer vision ECCV page, Yumin Suh Jingdong Wang Siyu Tang Tao Mei Ky oung Mu Lee, Part aligned bilinear representation per son identification, In Proceedings European Con ference Computer Vision ECCV page, Ke Sun Bin Xiao Dong Liu Jingdong Wang, Deep high resolution representation learning human pose esti mation, In CVPR, Yifan Sun Liang Zheng Yi Yang Qi Tian Shengjin Wang, Beyond part model Person retrieval refined part pooling strong convolutional baseline, In ECCV, Dongkai Wang Shiliang Zhang, Unsupervised person identification via multi label classification, In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition page, Guangrun Wang Guangcong Wang Xujie Zhang Jianhuang Lai Zhengtao Yu Liang Lin, Weakly supervised per son id Differentiable graphical learning new bench mark, IEEE Transactions Neural Networks Learning Systems, Guanshuo Wang Yufeng Yuan Xiong Chen Jiwei Li Xi Zhou, Learning discriminative feature multiple granu larities person identification, In ACM Multime dia Conference Multimedia Conference page, ACM, Longhui Wei Shiliang Zhang Wen Gao Qi Tian, Person transfer gan bridge domain gap person In Proceedings IEEE Conference identification, Computer Vision Pattern Recognition page, Zhirong Wu Yuanjun Xiong Stella Yu Dahua Lin, Unsupervised feature learning via non parametric instance In Proceedings IEEE Conference discrimination, Computer Vision Pattern Recognition page, Bryan Ning Xia Yuan Gong Yizhe Zhang Christian Poellabauer, Second order non local attention network person identification, In Proceedings IEEE Inter national Conference Computer Vision page, Ye Yuan Wuyang Chen Yang Yang Zhangyang Wang, In defense triplet loss Learning robust person identification fast approximated triplet loss label distillation, In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition Workshops page, Yifu Zhang Chunyu Wang Xinggang Wang Wenjun Zeng Wenyu Liu, Fairmot On fairness detection identification multiple object tracking, arXiv print page arXiv, Zhizheng Zhang Cuiling Lan Wenjun Zeng Zhibo Chen, Densely semantically aligned person identification, In Proceedings IEEE Conference Computer Vision Pattern Recognition page, Liang Zheng Liyue Shen Lu Tian Shengjin Wang Jing dong Wang Qi Tian, Scalable person identification benchmark, IEEE International Conference Com puter Vision ICCV page, Liang Zheng Hengheng Zhang Shaoyan Sun Manmohan Chandraker Yi Yang Qi Tian, Person identification wild, In Proceedings IEEE Conference Com puter Vision Pattern Recognition page, Zhedong Zheng Liang Zheng Yi Yang, discrimi natively learned cnn embedding person reidentification, ACM Transactions Multimedia Computing Communica tions Applications TOMM, Zhedong Zheng Liang Zheng Yi Yang, Unlabeled sam ples generated gan improve person identification baseline vitro, In Proceedings IEEE International Conference Computer Vision, Zhun Zhong Liang Zheng Donglin Cao Shaozi Li, Re ranking person identification reciprocal encoding, In Proceedings IEEE Conference Computer Vision Pattern Recognition page, Kuan Zhu Haiyun Guo Zhiwei Liu Ming Tang Jinqiao Wang, Identity guided human semantic parsing person identification, ECCV, In material show demo case scene changing specific person LUPerson NL share training detail PNL provide detailed algorithm table PNL compare model pre trained SYSUK LUPerson NL analyze hyper parameter PNL demonstrate result method using stronger backbone explore impact pre training image scale list detailed result small scale shot experiment, All experiment un der MGN setting, Scene changing LUPerson NL Our LUPerson NL driven street view video Figure show demo person LUPerson NL, As see LUPerson NL able cover multiple scene since video use lot moving camera moving person traklets frame selected, It approximate close real person Re ID scenario Algorithm PNL Algorithm show procedure training PNL train framework epoch apply label correc tion epoch, Once rectification begin keep rectifying every iteration, Algorithm PNL algorithm, Input total epoch correction start epoch Ns prototype feature vector, cK number identity temperature threshold momentum encoder network Eq classifier momentum encoder Ek loss weight λpro λlgc, epoch xi yi sampled data loader, xi aug xi aug xi qi Eq xi ki Ek pi qi sk si sk li pi si epoch Ns maxk lj cid exp qi ck exp qi ck ˆyi arg maxj lj else ˆyi yi Li Li Li ce log pi ˆyi pro log lgc cid exp qi ˆyi exp qi cj cid cid cid cid qi exp cid qi cid cid log qi exp cid exp cid cˆyi cˆyi qi ce λproLi Li Li update network Eq minimize Li update network Eq minimize Li update momentum encoder Ek, pro λlgcLi lgc, Compare SYSUK As show Table compare pre trained mod el LUPerson NL SYSUK, We pre train PNL LUPerson NL SYSUK epoch experiment setting fair comparison, The performance LUPerson NL pre training much better SYSUK pre training showing superiority LUPerson NL also suggesting large number image limited diversity bring represen tative representation large scale image diversity, Figure, Scene changing specific person LUPerson NL Training detail During training image resized pas augmentation veri fied Random Resized Crop Hor izontal Flip Normalization Random Gaussian Blur Specifi Random Gray Scale Random Erasing, cally image normalized mean std, , calculated image LUPerson NL, We train model GPUs epoch batch size, The initial learning rate set, step wise decay, every epoch, The opti mizer SGD momentum, weight de cay We set hyper parameter, , The momentum updating momen tum encoder Ek prototype set We de sign large queue size increase oc currence positive sample label guided contrastive learning, The label correction according Equation start th epoch, The deployment label guided contrastive loss Li lgc start th epoch, , Temperature Factor Table, Comparison different pre training data scale Dataset MSMT LUPerson NL, SYSUK, Table, Comparison applying PNL LUPerson NL SYSUK DukeMTMC MSMT, , , , , , , , , , , , , , , , , , , , Table, Performances different value DukeMTMC MSMT data percentage small scale setting, The threshold set, The best score bold second one underlined Hyper Parameter Analysis In PNL two key hyper parameter tem perature factor threshold correction, Here provide analysis two parameter, Table show performance comparison differ ent value fixed label correction threshold As see setting, achieves best sults DukeMTMC MSMT, achieves second best, When use larger performance drop rapidly, It may Re ID fine grained task larger cause smaller inter class variation make positive sample close neg ative sample, In experiment set, Threshold Table show result different label correc tion threshold value, As see performance relatively stable different value varying large range, , However small large performance drop rapidly, For former case label easier modified may cause wrong rectifi cation latter case label noise become harder corrected also consistently negative effect performance, In experiment set, Results stronger backbone We train PNL using two stronger backbone ResNet ResNet report result Ta ble, As see stronger ResNet bring superior performance, These result also outperform, DukeMTMC MSMT, , , , , , , , , , , , , , , , , , , , Table, Performances different value DukeMTMC MSMT data percentage small scale setting, The temperature factor set, The best score bold second one underlined, Arch CUHK Market DukeMTMC MSMT, , , , , , , , , , , , Table, Results different ResNet backbone, stand ResNet ResNet ResNet respec tively, Scale MSMT, , , score reported Table main submission, Most importantly FIRST obtain mAP score MSMT larger without post processing convolutional network Pre training data scale We study impact pre training data scale, Specif ically involve various percentage pseudo based LUPerson NL pre training evaluate finetuing performance target datasets, As shown Table learned representation much stronger increase pre training data scale dicating necessity building large scale dataset LUPerson NL important More result small scale shot To complement Table main text provide detailed result small scale shot setting Table, As see weakly pre trained model consistently better pre trained mod el, Our advantage much larger le training data suggesting potential practical value pre trained model real world person ReID application, scale id image IN sup, IN unsup, LUP unsup, LUPws wsp, scale id image IN sup, IN unsup, LUP unsup, LUPws wsp, scale id image IN sup, IN unsup, LUP unsup, LUPws wsp, scale id image IN sup, IN unsup, LUP unsup, LUPws wsp, scale id image IN sup, IN unsup, LUP unsup, LUPws wsp, scale id image IN sup, IN unsup, LUP unsup, LUPws wsp, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , Market small scale, , , , , , , , , , , , Market shot, , , , , , , , , , , , DukeMTMC small scale, , , , , , , , , , , , DukeMTMC shot, , , , , , , , , , , , MSMT small scale, , , , , , , , , , , , MSMT shot, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , Table, Performance small scale shot setting MGN method Market DukeMTMC MSMT, IN sup IN unsup refer supervised unsupervised pre trained model ImageNet LUP unsup refers unsupervised pre trained model LUPerson LUPws wsp refers model pre trained LUPerson WS using WSP, The first number mAP second cmc\n"
     ]
    }
   ],
   "source": [
    "references_clean= preprocess_text(references)\n",
    "print(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5af5e9",
   "metadata": {},
   "source": [
    "## Get_human_names Algorithme using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa296d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    \n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    for person in person_list:\n",
    "        person_split = person.split(\" \")\n",
    "        for name in person_split:\n",
    "            if wordnet.synsets(name):\n",
    "                if(name in person):\n",
    "                    person_names.remove(person)\n",
    "                    break\n",
    "    return person_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "864d30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 52 and without 160\n",
      "Ex: Weihua Chen Xiaotang Chen Jianguo Zhang Kaiqi Huang\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "\n",
    "print(\"Len with preprocess\",len(get_human_names(references_clean)), \"and without\",len(get_human_names(references)))\n",
    "print(\"Ex:\",get_human_names(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8dd9ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dengpan Fu Bo Xin Jingdong Wang Dongdong Chen Jianmin Bao Gang Hua Houqiang Li',\n",
       " 'Zhao Hong',\n",
       " 'Bernardo Avila Pires Zhaohan',\n",
       " 'Richard Radke',\n",
       " 'Mopro Webly',\n",
       " 'Zhao Tong Xiao Xiaogang Wang',\n",
       " 'Jiang Xiangyang Xue',\n",
       " 'Dongkai Wang Shiliang Zhang',\n",
       " 'Yifu Zhang Chunyu Wang Xinggang Wang Wenjun Zeng Wenyu Liu',\n",
       " 'Kuan Zhu Haiyun Guo Zhiwei Liu',\n",
       " 'Dataset MSMT LUPerson NL',\n",
       " 'Weihong Deng',\n",
       " 'Jiani Hu',\n",
       " 'Liangliang Ren',\n",
       " 'Jingyi Xie',\n",
       " 'Zhangyang Wang',\n",
       " 'Kevin Swersky',\n",
       " 'Geoffrey Hinton',\n",
       " 'Jianguo Zhang',\n",
       " 'Kaiqi Huang',\n",
       " 'Zuozhuo Dai',\n",
       " 'Siyu Zhu',\n",
       " 'Piotr Doll´ar',\n",
       " 'Deva Ramanan',\n",
       " 'Dengpan Fu',\n",
       " 'Bo Xin',\n",
       " 'Jingdong Wang',\n",
       " 'Jianmin Bao',\n",
       " 'Feng Zhu',\n",
       " 'Rui Zhao',\n",
       " 'Florian Strub',\n",
       " 'Florent Altch´e',\n",
       " 'Corentin Tallec',\n",
       " 'Elena Buchatskaya',\n",
       " 'Bernardo Avila Pires',\n",
       " 'Gheshlaghi Azar',\n",
       " 'Lucas Beyer',\n",
       " 'Bastian Leibe',\n",
       " 'Wenjun Zeng',\n",
       " 'Mengran Gou',\n",
       " 'Prannay Khosla',\n",
       " 'Piotr Teterwak',\n",
       " 'Yonglong Tian',\n",
       " 'Phillip Isola',\n",
       " 'Dilip Krishnan',\n",
       " 'Shiliang Zhang',\n",
       " 'Caiming Xiong',\n",
       " 'Steven Hoi',\n",
       " 'Xiatian Zhu',\n",
       " 'Tong Xiao',\n",
       " 'Xiaogang Wang',\n",
       " 'Xiaofei Du',\n",
       " 'Liang Zheng',\n",
       " 'Fangyi Liu',\n",
       " 'Chunxiao Liu',\n",
       " 'Naiyan Wang',\n",
       " 'Fuxu Liu',\n",
       " 'Shenqi Lai',\n",
       " 'Jingke Meng',\n",
       " 'Xuelin Qian',\n",
       " 'Yanwei Fu',\n",
       " 'Wenxuan Wang',\n",
       " 'Jie Qiu',\n",
       " 'Yu-Gang Jiang',\n",
       " 'Xiangyang Xue',\n",
       " 'Ruijie Quan',\n",
       " 'Linchao Zhu',\n",
       " 'Jia Deng',\n",
       " 'Zhiheng Huang',\n",
       " 'Andrej Karpathy',\n",
       " 'Shuai Zhao',\n",
       " 'Jinming Hu',\n",
       " 'Deng Cai',\n",
       " 'Yumin Suh',\n",
       " 'Dongkai Wang',\n",
       " 'Guangcong Wang',\n",
       " 'Xujie Zhang',\n",
       " 'Jianhuang Lai',\n",
       " 'Zhengtao Yu',\n",
       " 'Guanshuo Wang',\n",
       " 'Yuanjun Xiong',\n",
       " 'Yizhe Zhang',\n",
       " 'Yifu Zhang',\n",
       " 'Chunyu Wang',\n",
       " 'Xinggang Wang',\n",
       " 'Wenyu Liu',\n",
       " 'Zhizheng Zhang',\n",
       " 'Liyue Shen',\n",
       " 'Hengheng Zhang',\n",
       " 'Manmohan Chandraker',\n",
       " 'Zhedong Zheng',\n",
       " 'Zhun Zhong',\n",
       " 'Donglin Cao',\n",
       " 'Kuan Zhu',\n",
       " 'Haiyun Guo',\n",
       " 'Zhiwei Liu',\n",
       " 'Jinqiao Wang',\n",
       " 'Dataset MSMT17',\n",
       " 'Jie Zhou',\n",
       " 'Xinlei Chen Haoqi Fan Ross Girshick',\n",
       " 'Ron Appel Serge Belongie Pietro Per',\n",
       " 'Zhu Dapeng Chen Rui',\n",
       " 'Xinqian Gu Bingpeng Ma Hong Chang Shiguang Shan Xilin Chen',\n",
       " 'Sbsgan Suppression',\n",
       " 'Prannay Khosla Piotr Teterwak Chen Wang Aaron Sarna Yonglong Tian Phillip Isola Aaron Maschinot Ce Liu Dilip Krishnan',\n",
       " 'Wei Li Rui',\n",
       " 'Chen Change Loy Chunxiao Liu Shaogang Gong',\n",
       " 'Pattern Recognition',\n",
       " 'Ross Girshick Jian Sun',\n",
       " 'Yumin Suh Jingdong Wang Siyu Tang Tao Mei Ky',\n",
       " 'Networks Learning Systems',\n",
       " 'Ye Yuan Wuyang Chen Yang Yang Zhangyang Wang',\n",
       " 'Liang Zheng Hengheng Zhang Shaoyan Sun Manmohan Chandraker Yi Yang Qi Tian',\n",
       " 'Tang Jinqiao Wang',\n",
       " 'Temperature Factor Table',\n",
       " 'Market DukeMTMC MSMT',\n",
       " 'Tianlong Chen Shaojin',\n",
       " 'Chen Simon Kornblith Kevin Swersky Mohammad Norouzi Geoffrey Hinton',\n",
       " 'Zhu Ping Tan',\n",
       " 'Pedro Felzenszwalb Ross Girshick',\n",
       " 'Douglas Gray Hai Tao',\n",
       " 'Haoqi Fan Yuxin Wu',\n",
       " 'Yan Huang Qiang Wu JingSong Xu Yi Zhong',\n",
       " 'Proceed Srikrishna Karanam Mengran Gou Ziyan Wu Angels Rates Borras Octavia Camps',\n",
       " 'Junnan Li Caiming Xiong Steven Hoi',\n",
       " 'Yu Jhe Li Yun Chun Chen Yen Yu Lin Xiaofei Du Yu Chiang Frank Wang',\n",
       " 'Fangyi Liu Lei Zhang',\n",
       " 'Jingke Meng Sheng Wu Wei Shi Zheng',\n",
       " 'Xuelin Qian Yanwei Fu Tao Xiang Wenxuan Wang Jie Qiu Yang Wu Yu Gang',\n",
       " 'Olga Russakovsky Jia Deng Hao Su Jonathan Krause San',\n",
       " 'Zhao Jinming Hu Hao Feng Deng Cai',\n",
       " 'Guangrun Wang Guangcong Wang Xujie Zhang Jianhuang Lai Zhengtao Yu Liang Lin',\n",
       " 'Longhui Wei Shiliang Zhang Wen Gao Qi Tian',\n",
       " 'Zhizheng Zhang Cuiling Lan Wenjun Zeng Zhibo Chen',\n",
       " 'Vision ICCV',\n",
       " 'Zhun Zhong Liang Zheng Donglin Cao Shaozi Li',\n",
       " 'Random Resized Crop Hor',\n",
       " 'Hyper Parameter Analysis',\n",
       " 'Scale MSMT']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the result\n",
    "get_human_names(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fcb76",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "656d73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "\n",
    "def nlp_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    PROPN=[token.lemma_ for token in doc if token.pos_ == \"PROPN\"]\n",
    "    \n",
    "#     Remove duplicate\n",
    "    PROPN = list(dict.fromkeys(PROPN))\n",
    "#     Remove word with first letter as lowercase \n",
    "    for word in PROPN:\n",
    "        if word[0].islower():\n",
    "            PROPN.remove(word)\n",
    "    return PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4230cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n"
     ]
    }
   ],
   "source": [
    "final_names_prep_spacy=nlp_entities(references_clean)\n",
    "final_names_spacy=nlp_entities(references)\n",
    "print(len(final_names_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d123b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An other function with more accuracy\n",
    "\n",
    "def extract(text:str) :\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    doc = spacy_nlp(text.strip())\n",
    "    named_entities = []\n",
    "    \n",
    "    for i in doc.ents:\n",
    "        entry = str(i.lemma_).lower()\n",
    "        text = text.replace(str(i).lower(), \"\")\n",
    "        if i.label_ in [\"ART\", \"EVE\", \"NAT\", \"PERSON\"]:\n",
    "            named_entities.append(entry.title().replace(\" \", \"_\").replace(\"\\n\",\"_\"))\n",
    "        named_entities = list(dict.fromkeys(named_entities))\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f8b14fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 187 and without 226\n",
      "Ex: Binghui_Chen\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "\n",
    "print(\"Len with preprocess\",len(extract(references_clean)), \"and without\",len(extract(references)))\n",
    "print(\"Ex:\",extract(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "167b9906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Binghui_Chen',\n",
       " 'Weihong_Deng_Jiani_Hu',\n",
       " 'Mixed',\n",
       " 'Guangyi_Chen_Chunze',\n",
       " 'Lin_Liangliang',\n",
       " 'Jiwen_Lu',\n",
       " 'Jie_Zhou',\n",
       " 'Tianlong_Chen',\n",
       " 'Ding_Jingyi_Xie_Ye',\n",
       " 'Yuan_Wuyang',\n",
       " 'Chen_Yang_Yang',\n",
       " 'Zhou_Ren_Zhangyang_Wang',\n",
       " 'Abd_In_Pro',\n",
       " 'Ting_Chen',\n",
       " 'Simon_Kornblith_Mohammad_Norouzi_Ge',\n",
       " 'Ting_Chen_Simon_Kornblith_Kevin_Swersky_Mohammad_Norouzi_Geoffrey_Hinton',\n",
       " 'Weihua_Chen_Xiaotang',\n",
       " 'Chen_Jianguo_Zhang',\n",
       " 'Kaiqi_Huang',\n",
       " 'Xinlei_Chen',\n",
       " 'Ross_Girshick_Kaiming',\n",
       " 'Zhirui_Chen',\n",
       " 'Jianheng_Li',\n",
       " 'Wei_Shi_Zheng',\n",
       " 'Zuozhuo_Dai_Mingqiang',\n",
       " 'Chen_Xiaodong_Gu',\n",
       " 'Zhu_Ping_Tan',\n",
       " 'Batch',\n",
       " 'Ron_Appel',\n",
       " 'Serge_Belongie_Pietro_Per_Ieee',\n",
       " 'Pedro_Felzenszwalb',\n",
       " 'Ross_Girshick',\n",
       " 'David_Mcallester',\n",
       " 'Deva_Ramanan',\n",
       " 'Dengpan_Fu',\n",
       " 'Jingdong_Wang_Dongdong',\n",
       " 'Chen_Jianmin_Bao_Gang_Hua',\n",
       " 'Houqiang_Li',\n",
       " 'Yixiao_Ge',\n",
       " 'Dapeng_Chen',\n",
       " 'Hongsheng_Li',\n",
       " 'Learning_Representations',\n",
       " 'Feng_Zhu',\n",
       " 'Dapeng_Chen_Rui',\n",
       " 'Zhao_Hong_Sheng_Li',\n",
       " 'Douglas_Gray_Hai_Tao',\n",
       " 'Jean_Bastien_Grill',\n",
       " 'Corentin_Tallec',\n",
       " 'Pierre_Richemond_Elena_Buchatskaya_Carl_Do',\n",
       " 'Bernardo_Avila',\n",
       " 'Daniel_Guo_Moham',\n",
       " 'Xinqian_Gu',\n",
       " 'Bingpeng_Ma',\n",
       " 'Hong_Chang',\n",
       " 'Shiguang_Shan_Xilin_Chen',\n",
       " 'Wu_Saining_Xie_Ross_Girshick',\n",
       " 'Lingxiao_He',\n",
       " 'Wu_Liu',\n",
       " 'Alexander_Hermans_Lucas_Beyer',\n",
       " 'Bastian_Leibe',\n",
       " 'Yan_Huang_Qiang_Wu',\n",
       " 'Jingsong_Xu',\n",
       " 'Yi_Zhong',\n",
       " 'Xin_Jin_Cuiling',\n",
       " 'Lan_Wenjun',\n",
       " 'Zeng_Zhibo',\n",
       " 'Chen_Li_Zhang',\n",
       " 'Lan_Wenjun_Zeng_Guoqiang',\n",
       " 'Wei_Zhibo_Chen',\n",
       " 'Dengpan_Fu_Dongdong_Chen_Jianmin_Bao_Hao_Yang',\n",
       " 'Yuan_Lei_Zhang',\n",
       " 'Houqiang_Li_Dong_Chen',\n",
       " 'Gou_Ziyan',\n",
       " 'Richard_Radke',\n",
       " 'Prannay_Khosla_Piotr',\n",
       " 'Chen_Wang',\n",
       " 'Liu_Dilip_Krishnan',\n",
       " 'Jiane_Li',\n",
       " 'Wen_Gao_Shiliang_Zhang',\n",
       " 'Junnan_Li_Caime_Xiong_Steven_Hoi',\n",
       " 'Li_Xiatian',\n",
       " 'Zhu_Shaogang_Gong',\n",
       " 'Wei_Li',\n",
       " 'Rui_Zhao_Tong_Xiao_Xiaogang_Wang',\n",
       " 'Deep_Reid',\n",
       " 'Yu_Jhe_Li_Yun_Chun_Chen_Yen_Yu_Lin',\n",
       " 'Du_Yu_Chiang_Frank_Wang',\n",
       " 'Yu_Jhe_Li_Ci',\n",
       " 'Lin_Yan',\n",
       " 'Lin_Yu_Chiang_Frank_Wang',\n",
       " 'Yutian_Lin_Xuanyi',\n",
       " 'Dong_Liang_Zheng_Yan_Yan',\n",
       " 'Yi_Yang',\n",
       " 'Confer',\n",
       " 'Fangyi_Liu',\n",
       " 'Lei_Zhang',\n",
       " 'Chen_Change_Loy',\n",
       " 'Liu_Shaogang_Gong',\n",
       " 'Chuanchen_Luo_Yuntao_Chen_Naiyan',\n",
       " 'Wang_Zhaoxi',\n",
       " 'Ang_Zhang',\n",
       " 'Hao_Luo',\n",
       " 'Wei_Jiang',\n",
       " 'Youzhi_Gu',\n",
       " 'Liu_Xingyu_Liao',\n",
       " 'Lai_Jianyang_Gu',\n",
       " 'Jingke_Meng_Sheng',\n",
       " 'Wu_Wei_Shi_Zheng',\n",
       " 'Hyunjong_Park_Bumsub',\n",
       " 'Xuelin_Qian',\n",
       " 'Xiang_Wenxuan',\n",
       " 'Wang_Jie',\n",
       " 'Qiu_Yang',\n",
       " 'Wu_Yu',\n",
       " 'Gang_Jiang_Xiangyang_Xue',\n",
       " 'Ruijie_Quan_Xuanyi',\n",
       " 'Dong_Yu_Wu',\n",
       " 'Linchao_Zhu_Yi_Yang',\n",
       " 'Shaoqing_Ren_Kaiming',\n",
       " 'Jian_Sun',\n",
       " 'Faster',\n",
       " 'Olga_Russakovsky_Jia',\n",
       " 'Deng_Hao_Su_Jonathan_Krause_San',\n",
       " 'Sean_Ma',\n",
       " 'Zhiheng_Huang',\n",
       " 'Michael_Bernstein',\n",
       " 'Dong_Shen_Shuai',\n",
       " 'Zhao_Jinming',\n",
       " 'Hao_Feng',\n",
       " 'Deng_Cai_Xiaofei',\n",
       " 'Yantao_Shen',\n",
       " 'Shuai_Yi',\n",
       " 'Dapeng_Chen_Xiaogang_Wang',\n",
       " 'Yumin_Suh_Jingdong',\n",
       " 'Tang',\n",
       " 'Tao_Mei_Ky',\n",
       " 'Mu_Lee',\n",
       " 'Ke_Sun_Bin_Xiao_Dong_Liu',\n",
       " 'Jingdong_Wang',\n",
       " 'Yifan_Sun',\n",
       " 'Liang_Zheng',\n",
       " 'Shengjin_Wang',\n",
       " 'Person',\n",
       " 'Dongkai_Wang_Shiliang_Zhang',\n",
       " 'Wang_Guangcong',\n",
       " 'Xujie_Zhang_Jianhuang_Lai',\n",
       " 'Yu_Liang_Lin',\n",
       " 'Yuan_Xiong',\n",
       " 'Chen_Jiwei_Li_Xi_Zhou',\n",
       " 'Longhui_Wei_Shiliang_Zhang_Wen_Gao',\n",
       " 'Zhirong_Wu',\n",
       " 'Stella_Yu_Dahua_Lin',\n",
       " 'Bryan_Ning_Xia_Yuan',\n",
       " 'Yizhe_Zhang',\n",
       " 'Wuyang_Chen_Yang_Yang_Zhangyang_Wang',\n",
       " 'Learning',\n",
       " 'Yifu_Zhang_Chunyu',\n",
       " 'Wang_Xinggang',\n",
       " 'Wang_Wenjun',\n",
       " 'Zeng_Wenyu_Liu',\n",
       " 'Fairmot',\n",
       " 'Zhizheng_Zhang',\n",
       " 'Zeng_Zhibo_Chen',\n",
       " 'Liang_Zheng_Liyue',\n",
       " 'Tian',\n",
       " 'Wang_Qi',\n",
       " 'Liang_Zheng_Hengheng_Zhang',\n",
       " 'Zhedong_Zheng',\n",
       " 'Liang_Zheng_Yi_Yang',\n",
       " 'Zhun_Zhong',\n",
       " 'Liang_Zheng_Donglin_Cao_Shaozi_Li',\n",
       " 'Kuan_Zhu',\n",
       " 'Zhiwei_Liu',\n",
       " 'Tang_Jinqiao_Wang',\n",
       " 'Eq',\n",
       " 'Ek',\n",
       " 'Li_Li_Li',\n",
       " 'CˆYi_CˆYi',\n",
       " 'Li_Li',\n",
       " 'Eq_Minimize_Li',\n",
       " 'Li_Lgc_Start_Th_Epoch',\n",
       " 'Dukemtmc_Msmt',\n",
       " 'Threshold_Table',\n",
       " 'Arch_Cuhk_Market_Dukemtmc_Msmt',\n",
       " 'Scale_Msmt',\n",
       " 'Reid',\n",
       " 'Market_Dukemtmc_Msmt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Result\n",
    "\n",
    "extract(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dbd17",
   "metadata": {},
   "source": [
    "## TextBlob : TextBlob est une bibliothèque python et propose une API simple pour accéder à ses méthodes et effectuer des tâches NLP de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86155dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_names_TextBlob= TextBlob(references_clean)\n",
    "PROPN=[words for words, tag in final_names_TextBlob.tags if tag == \"NNP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cc696d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 1139\n",
      "Ex: Binghui\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess\n",
    "final_names_TextBlob=PROPN\n",
    "#With preprocess\n",
    "print(\"Len with preprocess\",len(final_names_TextBlob))\n",
    "print(\"Ex:\",final_names_TextBlob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60bbf0",
   "metadata": {},
   "source": [
    "Also efficence but not useful here because spacy hasa better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f8eab",
   "metadata": {},
   "source": [
    "## API using NLTK package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccb418",
   "metadata": {},
   "source": [
    "### Service Docker (de cette API):\n",
    "\n",
    "Go in the main folder and type to build the image:\n",
    "\n",
    "    $ docker build -t pfr .\n",
    "\n",
    "Then run the container:\n",
    "\n",
    "    $ docker run -d --name mycontainer -p 80:80 pfr\n",
    "\n",
    "Interactive API docs: \n",
    "\n",
    "Now you can go to http://127.0.0.1/docs and try the API to analyse a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73cd41c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=80): Max retries exceeded with url: /NLTK/NER?text=Aux%20groupes%20pr%C3%A9sents%20depuis%20le%20Pal%C3%A9olithique%20et%20le%20N%C3%A9olithique,%20sont%20venues%20s'ajouter,%20%C3%A0%20l'%C3%82ge%20du%20bronze%20et%20%C3%A0%20l'%C3%82ge%20du%20fer,%20des%20vagues%20successives%20de%20Celtes,%20puis%20au%20iiie%20si%C3%A8cle%20de%20peuples%20germains%20(Francs,%20Wisigoths,%20Alamans,%20Burgondes)%20et%20au%20ixe%20si%C3%A8cle%20de%20scandinaves%20appel%C3%A9s%20Normands. (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000014CA1063D30>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\util\\connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 398\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m    238\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\http\\client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\http\\client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1301\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\http\\client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1251\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\http\\client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1011\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1014\u001b[0m \n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\http\\client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x0000014CA1063D30>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 440\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\connectionpool.py:785\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    783\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 785\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=80): Max retries exceeded with url: /NLTK/NER?text=Aux%20groupes%20pr%C3%A9sents%20depuis%20le%20Pal%C3%A9olithique%20et%20le%20N%C3%A9olithique,%20sont%20venues%20s'ajouter,%20%C3%A0%20l'%C3%82ge%20du%20bronze%20et%20%C3%A0%20l'%C3%82ge%20du%20fer,%20des%20vagues%20successives%20de%20Celtes,%20puis%20au%20iiie%20si%C3%A8cle%20de%20peuples%20germains%20(Francs,%20Wisigoths,%20Alamans,%20Burgondes)%20et%20au%20ixe%20si%C3%A8cle%20de%20scandinaves%20appel%C3%A9s%20Normands. (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000014CA1063D30>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      2\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAux groupes présents depuis le Paléolithique et le Néolithique, sont venues s\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124majouter, à l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mÂge du bronze et à l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mÂge du fer, des vagues successives de Celtes, puis au iiie siècle de peuples germains (Francs, Wisigoths, Alamans, Burgondes) et au ixe siècle de scandinaves appelés Normands.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://127.0.0.1/NLTK/NER?text=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\requests\\api.py:75\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    524\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[0;32m    527\u001b[0m }\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    648\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFR1\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=80): Max retries exceeded with url: /NLTK/NER?text=Aux%20groupes%20pr%C3%A9sents%20depuis%20le%20Pal%C3%A9olithique%20et%20le%20N%C3%A9olithique,%20sont%20venues%20s'ajouter,%20%C3%A0%20l'%C3%82ge%20du%20bronze%20et%20%C3%A0%20l'%C3%82ge%20du%20fer,%20des%20vagues%20successives%20de%20Celtes,%20puis%20au%20iiie%20si%C3%A8cle%20de%20peuples%20germains%20(Francs,%20Wisigoths,%20Alamans,%20Burgondes)%20et%20au%20ixe%20si%C3%A8cle%20de%20scandinaves%20appel%C3%A9s%20Normands. (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000014CA1063D30>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "text=\"Aux groupes présents depuis le Paléolithique et le Néolithique, sont venues s'ajouter, à l'Âge du bronze et à l'Âge du fer, des vagues successives de Celtes, puis au iiie siècle de peuples germains (Francs, Wisigoths, Alamans, Burgondes) et au ixe siècle de scandinaves appelés Normands.\"\n",
    "\n",
    "response = requests.get(f'http://127.0.0.1/NLTK/NER?text={text}')\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f'http://127.0.0.1/NLTK/NER?text={references_clean}')\n",
    "result_API=response.json()\n",
    "result_API=print(result_API['Names entities in text from NLTK '])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb67165",
   "metadata": {},
   "source": [
    "## Apply Spacy functions on all pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b585bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Large-Scale Pre-training for Person Re-identification with Noisy Labels\\n\\nDengpan Fu1 Dongdong Chen3 Hao Yang2\\nLei Zhang4 Houqiang Li1\\n\\nLu Yuan3\\n\\nJianmin Bao2*\\n\\nFang Wen 2 Dong Chen2\\n\\n1University of Science and Technology of China\\n\\nfdpan@mail.ustc.edu.cn\\n\\n2Microsoft Research, 3Microsoft Cloud AI, 4IDEA\\nlihq@ustc.edu.cn\\n\\ncddlyf@gmail.com\\n\\n{jianbao,haya,luyuan,fangwen,doch}@microsoft.com, leizhang@idea.edu.cn\\n\\nAbstract\\n\\nThis paper aims to address the problem of pre-training\\nfor person re-identification (Re-ID) with noisy labels. To\\nsetup the pre-training task, we apply a simple online multi-\\nobject tracking system on raw videos of an existing un-\\nlabeled Re-ID dataset \\xe2\\x80\\x9cLUPerson\\xe2\\x80\\x9d and build the Noisy\\nLabeled variant called \\xe2\\x80\\x9cLUPerson-NL\\xe2\\x80\\x9d. Since theses ID\\nlabels automatically derived from tracklets inevitably con-\\ntain noises, we develop a large-scale Pre-training frame-\\nwork utilizing Noisy Labels (PNL), which consists of three\\nlearning modules: supervised Re-ID learning, prototype-\\nbased contrastive learning, and label-guided contrastive\\nlearning. In principle, joint learning of these three mod-\\nules not only clusters similar examples to one prototype,\\nbut also rectifies noisy labels based on the prototype as-\\nsignment. We demonstrate that learning directly from raw\\nvideos is a promising alternative for pre-training, which\\nutilizes spatial and temporal correlations as weak super-\\nvision. This simple pre-training task provides a scalable\\nway to learn SOTA Re-ID representations from scratch on\\n\\xe2\\x80\\x9cLUPerson-NL\\xe2\\x80\\x9d without bells and whistles. For example,\\nby applying on the same supervised Re-ID method MGN,\\nour pre-trained model improves the mAP over the unsu-\\npervised pre-training counterpart by 5.7%, 2.2%, 2.3% on\\nCUHK03, DukeMTMC, and MSMT17 respectively. Under\\nthe small-scale or few-shot setting, the performance gain is\\neven more significant, suggesting a better transferability of\\nthe learned representation. Code is available at https:\\n//github.com/DengpanFu/LUPerson-NL.\\n\\n1. Introduction\\n\\nA large high-quality labeled dataset for person re-\\nidentification (Re-ID) is labor intensive and costly to cre-\\nate. Existing fully labeled datasets [25, 52, 58, 61] for per-\\nson Re-ID are all of limited scale and diversity compared\\nto other vision tasks. Therefore, model pre-training be-\\n\\n*Corresponding author.\\n\\n(a) Market1501 with MGN\\n\\n(b) Market1501 with IDE\\n\\n(c) DukeMTMC with MGN\\n\\n(d) DukeMTMC with IDE\\n\\nFigure 1. Comparing person Re-ID performances of three pre-\\ntrained models on two methods (IDE [59] and MGN [51]). Re-\\nsults are reported on Market1501 and DukeMTC, with different\\nscales under the small-scale setting. IN.sup. refers to the model\\nsupervised pre-trained on ImageNet, LUP.unsup. is the model un-\\nsupervised pre-trained on LUPserson, and LUPnl.pnl. is the model\\npre-trained on our LUPerson-NL dataset using our proposed PNL.\\n\\ncomes a crucial approach to achieve good Re-ID perfor-\\nmance. However, due to the lack of large-scale Re-ID\\ndataset, most previous methods simply use the models pre-\\ntrained on the crowd-labeled ImageNet dataset, resulting in\\na limited improvement because of the big domain gap be-\\ntween generic images in ImageNet and person-focused im-\\nages desired by the Re-ID task. To mitigate this problem,\\nthe recent work [12] has demonstrated that unsupervised\\npre-training on a web-scale unlabeled Re-ID image dataset\\n\\xe2\\x80\\x9cLUPerson\\xe2\\x80\\x9d (sub-sampled from massive streeview videos)\\nsurpasses that of pre-training on ImageNet.\\n\\nIn this paper, our hypothesis is that scalable ReID pre-\\ntraining methods that learn directly from raw videos can\\ngenerate better representations. To verify it, we propose the\\nnoisy labels guided person Re-ID pre-training, which lever-\\n\\n\\x0cages the spatial and temporal correlations in videos as weak\\nsupervision. This supervision is nearly cost-free, and can be\\nachieved by the tracklets of a person over time derived from\\nany multi-object tracking algorithm, such as [56]. In par-\\nticular, we track each person in consecutive video frames,\\nand automatically assign the tracked persons in the same\\ntracklet to the same Re-ID label and vice versa. Enabled by\\nthe large amounts of raw videos in LUPerson [12], publicly\\navailable data of this form on the internet, we create a new\\nvariant named \\xe2\\x80\\x9cLUPerson-NL\\xe2\\x80\\x9d with derived pseudo Re-ID\\nlabels from tracklets for pre-training with noisy labels. This\\nvariant totally consists of 10M person images from 21K\\nscenes with noisy labels of about 430K identities.\\n\\nWe demonstrate that contrastive pre-training of Re-ID is\\nan effective method of learning from this weak supervision\\nat large scale. This new Pre-training framework utilizing\\nNoisy Labels (PNL) composes three learning modules: (1)\\na simple supervised learning module directly learns from\\nRe-ID labels through classification; (2) a prototype-based\\ncontrastive learning module helps cluster instances to the\\nprototype which is dynamically updated by moving aver-\\naging the centroids of instance features, and progressively\\nrectify the noisy labels based on the prototype assignment.\\nand (3) a label-guided contrastive learning module utilizes\\nthe rectified labels subsequently as the guidance. In contrast\\nto the vanilla momentum contrastive learning [7,12,19] that\\ntreats only features from the same instance as positive sam-\\nples, our label-guided contrastive learning uses the rectified\\nlabels to distinguish positive and negative samples accord-\\ningly, leading to a better performance. In principle, joint\\nlearning of these three modules make the consistency be-\\ntween the prototype assignment from instances and the high\\nconfident (rectified) labels, as possible as it can.\\n\\nThe experiments show that our PNL model achieves re-\\nmarkable improvements on various person Re-ID bench-\\nmarks. Figure 1 indicates that the performance gain from\\nour pre-trained models is consistent on different scales of\\ntraining data. For example, upon the strong MGN [51]\\nimproves the mAP by\\nbaseline, our pre-trained model\\n4.4%, 4.9% on Market1501 and DukeMTMC over the Im-\\nageNet supervised one, and 0.9%, 2.2% over the unsuper-\\nvised pre-training baseline [12]. Moreover, the gains are\\neven larger under the small-scale and few-shot settings,\\nwhere the labeled Re-ID data are extremely limited. To the\\nbest of our knowledge, we are the first to show that large-\\nscale noisy label guided pre-training can significantly ben-\\nefit person Re-ID task.\\n\\nOur key contributions can be summarized as follows:\\n\\n\\xe2\\x80\\xa2 We propose noisy label guided pre-training for person Re-\\nID, which incorporates supervised learning, prototype-\\nbased contrastive learning, label-guided contrastive learn-\\ning and noisy label rectification to a unified framework.\\n\\xe2\\x80\\xa2 We construct a large-scale noisy labeled person Re-ID\\n\\ndataset \\xe2\\x80\\x9cLUPerson-NL\\xe2\\x80\\x9d as a new variant of \\xe2\\x80\\x9cLUPerson\\xe2\\x80\\x9d.\\nIt is by far the largest noisy labeled person Re-ID dataset\\nwithout any human labeling effort.\\n\\n\\xe2\\x80\\xa2 Our models pre-trained on LUPerson-NL push the state-\\nof-the-art results on various public benchmarks to a new\\nlimit without bells and whistles.\\n\\n2. Related Work\\nSupervised Person Re-ID. Most studies of person Re-ID\\nemploy supervised learning. Some [6, 21, 55] introduce a\\nhard triplet loss on the global feature, ensuring a closer fea-\\nture distance for the same identity, while some [45, 59, 60]\\nimpose classification loss to learn a global feature from\\nthe whole image. There are also some other works that\\nlearn part-based local features with separate classification\\nlosses. For example, Suh et al. [46] presented part-aligned\\nbi-linear representations and Sun et al. [48] represented fea-\\ntures as horizontal strips. Recent approaches investigate\\nlearning invariant features concerning views [34], resolu-\\ntions [31], poses [32], domains [22,23], or exploiting group-\\nwise losses [36] or temporal information [18, 27] to im-\\nprove performance. The more advantageous results on pub-\\nlic benchmarks are achieved by MGN [51], which learns\\nboth global and local features with multiple losses. In [40],\\nQian et al further demonstrated the potential of generating\\ncross-view images for person re-indentification conditioned\\non normalized poses. In this paper, we focus on model pre-\\ntraining, and our pre-trained models can be applied to these\\nrepresentative methods and boost their performance.\\nUnsupervised Person Re-ID. To alleviate the lack of pre-\\ncise annotations, some works resort to unsupervised train-\\ning on unlabeled datasets. For example, MMCL [49] for-\\nmulates unsupervised person Re-ID as a multi-label classi-\\nfication to progressively seek true labels. BUC [33] jointly\\noptimizes the network and the sample relationship with\\na bottom-up hierarchical clustering. MMT [14] collabo-\\nratively trains two networks to refine both hard and soft\\npseudo labels. SpCL [15] designs a hybrid memory to\\nunify the representations for clustering and instance-wise\\ncontrastive learning. Both MMT [14] and SpCL [15] rely\\non explicit clustering of features from the whole training\\nset, making them quite inefficient on large datasets like\\nMSMT17. Since the appearance ambiguity is difficult to\\naddress without supervision, these unsupervised methods\\nhave limited performance. One alternative to address this\\nissue is introducing model pre-training on large scale data.\\nInspired by the success of self-supervised representation\\nlearning [4,5,7,17,19,28,53], Fu et al. [12] proposed a large\\nscale unlabeled Re-ID dataset, LUPerson, and illustrated\\nthe effectiveness of its unsupervised pre-trained models. In\\nthis work, we further try to make use of noisy labels from\\nvideo tracklets to improve the pre-training quality through\\nlarge-scale weakly-supervised pre-training.\\nWeakly Supervised Person Re-ID. Several approaches\\n\\n\\x0calso employ weak supervision in person Re-ID training.\\nInstead of requiring bounding boxes within each frame,\\nMeng et al. [38] rely on precise video-level labels, which\\nreduces annotation cost but still need manual efforts to la-\\nbel videos. On the contrary, we resort to noisy labels that\\ncan be automatically generated from tracklets on a much\\nlarger scale. Some [8, 29, 50] also leverage tracklets to su-\\npervise the training of Re-ID tasks. But unlike these ap-\\nproaches, we are proposing a large-scale pre-training strat-\\negy for person Re-ID, by both building a new very large-\\nscale dataset and devising a new pre-training framework:\\nthe new dataset, LUPerson-NL, is even larger than LU-\\nPerson [12] and has large amount of noisy Re-ID labels;\\nThe new framework, PNL, combines supervised learning,\\nlabel-guided contrastive learning and prototype based con-\\ntrastive learning to exploit the knowledge under large-scale\\nnoise labels. Most importantly, our pre-trained models have\\ndemonstrated remarkable performance and generalization\\nability, helping achieve state-of-the-art results superior to\\nall existing methods on public person Re-ID benchmarks.\\n3. LUPerson-NL: LUPerson With Noisy Labels\\nSupervised models based on deep networks are always\\ndata-hungry, but the labeled data they rely on are expen-\\nsive to acquire.\\nIt is a tremendous issue for person Re-\\nID task, since the human labelers need to check across\\nmultiple views to ensure the correctness of Re-ID labels.\\nThe data shortage is partially alleviated by a recently pub-\\nlished dataset, LUPerson [12], a dataset of unlabeled per-\\nson images with a significantly larger scale than previous\\nperson Re-ID datasets. Unsupervised pre-trained models\\n[12] on LUPerson have demonstrated remarkable effective-\\nness without utilizing additional manual annotations, which\\narouses our curiosity: can we further improve the perfor-\\nmance of pre-training directly by utilizing temporal cor-\\nrelation as weak supervision? To verify this, we build a\\nnew variant of LUPerson on top of the raw videos from\\nLUPerson and assign label to each person image with au-\\ntomatically generated tracklet. We name it LUPerson-NL\\nIt consists of 10M\\nwith NL standing for Noisy Labels.\\nimages with about 430K identities collected from 21K\\nscenes. To our best knowledge, this is the largest person\\nRe-ID dataset constructed without human labelling by far.\\nOur LUPerson-NL will be released for scientific research\\nonly, while any usage for other purpose is forbidden.\\n3.1. Constructing LUPerson-NL\\n\\nWe utilize the off-the-shelf tracking algorithm [56] 1 to\\ndetect persons and extract person tracklets from the same\\nraw videos of [12]. We assign each tracklet with a unique\\nclass label. The detection is not perfect: e.g. the bounding\\nboxes may only cover partial bodies without heads or upper\\nparts. Human pose estimation [47] is thus appended that\\n\\n1FairMOT: https://github.com/ifzhang/FairMOT\\n\\nFigure 2.\\n(X, Y ) indicates Y % of identities each has less than X images.\\n\\nIdentity distribution of LUPerson-NL. A curve point\\n\\nFigure 3. Besides the correctly labeled identities as shown by (a),\\nthere are two types of labeling errors in LUPerson-NL. Noise-I:\\nsame person labeled as different identities, e.g. D, E and F shown\\nin (b). Noise-II: different persons labeled as the same identity,\\ne.g. G shown in (c).\\n\\nhelps filter out imperfect boxes by predicting landmarks.\\n\\nWe track every person in the video frame by frame. In\\norder to guarantee both the sufficiency and diversity, we\\nadopt the following strategy:\\ni) We first remove the per-\\nson identities that appear in too few frames, i.e. no more\\nthan 200; ii) Within the tracklet of each identity, we then\\nperform sampling with a rate of one image per 20 frames\\nto reduce the number of duplicated images. Thus we can\\nmake sure that there would be at least 10 images associat-\\ning to each identity. Through this filtering procedure, we\\nhave collected 10, 683, 716 images of 433, 997 identities in\\ntotal. They belong to 21, 697 videos which are less than\\nthe videos that [12] uses, due to our extra filtering strat-\\negy for more reliable identity labels. Thus, LUPerson-NL\\nis very different from LUPerson, as it adopts very different\\nsampling and post-processing strategies, not to mention the\\nnoisy labels driven from the spatial-temporal information.\\n3.2. Properties of LUPerson-NL\\n\\nLUPerson-NL is advantageous in following aspects:\\nLarge amount of images and identities. We detail the\\nstatistics of existing popular person Re-ID datasets in Table\\n1. As we can see, the proposed LUPerson-NL, with over\\n10M images and 433K noisy labeled identities, is the sec-\\nond largest among the listed. Indeed, SYSU30K has more\\nimages, but it extracts images from only 1K TV program\\nvideos frame by frame, making it less competitive in vari-\\nability and less compatible in practice, the pre-training per-\\nformance comparison can be found at supplementary mate-\\nrials. Besides, LUPerson-NL was constructed without hu-\\nman labeling effort, making it more suitable to scale-up.\\n\\n102030405060708090#ofpersonimages0255075100%ofidentities\\xf0\\x9d\\x90\\xb4\\xf0\\x9d\\x90\\xb5\\xf0\\x9d\\x90\\xb6(a) Correctly labeled identities(b) Noise-I(c) Noise-II\\xf0\\x9d\\x90\\xb7\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x90\\xb9\\xf0\\x9d\\x90\\xba\\x0cDatasets\\nVIPeR [16]\\nGRID [35]\\nCUHK03 [30]\\nMarket [58]\\nAirport [25]\\nDukeMTMC [61]\\nMSMT17 [52]\\nSYSU30K [50]\\nLUPerson [12]\\nLUPerson-NL\\n\\n#scene\\n#images\\n2\\n1,264\\n8\\n1,275\\n2\\n14, 096\\n6\\n32, 668\\n6\\n39, 902\\n8\\n36, 411\\n15\\n126, 441\\n1,000\\n29,606,918\\n46, 260\\n4, 180, 243\\n10, 683, 716 21, 697 \\xe2\\x89\\x83 433, 997\\n\\n#persons\\n632\\n1,025\\n1, 467\\n1, 501\\n9, 651\\n1, 852\\n4, 101\\n30,508\\n> 200k\\n\\nlabeled environment\\n\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\nyes\\n\\n-\\nsubway\\ncampus\\ncampus\\nairport\\ncampus\\ncampus\\n\\nweakly TV program\\n\\nno\\nnoisy\\n\\nvary\\nvary\\n\\ncamera view\\nfixed\\nfixed\\nfixed\\nfixed\\nfixed\\nfixed\\nfixed\\ndynamic\\ndynamic\\ndynamic\\n\\ndetector\\nhand\\nhand\\nDPM [11]+hand\\nDPM [11]+hand\\nACF [10]\\nHand\\nFasterRCNN [42]\\nYOLOv2\\nYOLOv5\\nFairMOT [56]\\n\\ncrop size\\n128 \\xc3\\x97 48\\nvary\\nvary\\n128 \\xc3\\x97 64\\n128 \\xc3\\x97 64\\nvary\\nvary\\nvary\\nvary\\nvary\\n\\nTable 1. Comparing statistics among existing popular Re-ID datasets. LUPerson-NL is by far the largest Re-ID dataset with better diversity\\nwithout human labeling effort. SYSU30K is partly annotated by human annotator.\\n\\n4. PNL: Pre-training with Noisy Labels for\\n\\nPerson Re-ID\\n\\nBased on the new LUPerson-NL dataset with large scale\\nnoisy labels, we devise a novel Pretraining framework with\\nNoisy Labels for person Re-ID, namely PNL.\\n\\nDenote all\\n\\nthe data samples from LUPerson-NL as\\n{(xi, yi)}n\\ni=1, with n being the size of the dataset, xi a\\nperson image and yi \\xe2\\x88\\x88 {1, . . . , K} its associated identity\\nlabel. Here K represents the number of all identities that\\nare recorded in LUPerson-NL.\\n\\nInspired by recent methods [4, 5, 7, 17, 19, 28], our PNL\\nframework adopts Siamese networks that have been fully in-\\nvestigated for contrastive representation learning. As shown\\nby Figure 4, given an input person image xi, we first per-\\nform two randomly selected augmentations (T, T \\xe2\\x80\\xb2), pro-\\nducing two augmented images ( \\xcb\\x9cxi, \\xcb\\x9cx\\xe2\\x80\\xb2\\ni). We feed one of\\nthem, \\xcb\\x9cxi, into an encoder Eq to get a query feature qi;\\nwhile the other one, \\xcb\\x9cx\\xe2\\x80\\xb2\\ni, is fed into another encoder Ek\\nto get a key feature ki. Following [19], we design Ek to\\nbe a momentum version of Eq, i.e. the two encoders Ek\\nand Eq share the same network structure, but with different\\nweights. The weights in Ek are exponential moving aver-\\nages of the weights in Eq. During training, weights of Ek\\nare refreshed through a momentum update from Eq. And\\nthe detailed algorithm can be found at supplementary mate-\\nrials.\\n4.1. Supervised Classification\\nSince the raw labels {yi}n\\n\\ni=1 in LUPerson-NL contain\\nlots of noises as illustrated in previous section, they have\\nto be rectified during training. Let \\xcb\\x86yi be the rectified label\\nof image xi. As long as \\xcb\\x86yi is given, it would be intuitive\\nthat we train classification based on the corrected label \\xcb\\x86yi.\\nIn particular, we would append a classifier to transform the\\nfeature from Eq into probabilities pi \\xe2\\x88\\x88 RK with K being\\nthe number of classes. Then we impose a classification loss\\n\\nLi\\n\\nce = \\xe2\\x88\\x92 log(pi[\\xcb\\x86yi]).\\n\\n(1)\\n\\nHowever, the acquisition of \\xcb\\x86yi is not straight-forward.\\nWe resort to prototypes, the moving averaged centroids of\\nfeatures from training instances, to accomplish this task.\\n\\nFigure 4. The overview of our PNL framework.\\nIt comprises\\na supervised classification module, a prototype based contrastive\\nlearning module, and a label-guided contrastive learning module.\\n\\nBalanced distribution of identities. We illustrate the cu-\\nmulative percentage of identities with respect to the number\\nof their corresponding person images as a curve in Figure 2.\\nA point (X, Y ) on the curve represents that there are in total\\nY % identities in LUPerson-NL, that each of them has less\\nthan X images. It can be observed that: i) about 75% of all\\nthe identities in LUPerson-NL have a person image num-\\nber within [10, 25]; ii) the percentage of identities that have\\nmore than 50 person images each, occupy only a very small\\nportion of about 6.4% (27, 767/433, 997) in LUPerson-NL.\\nThese observations all show that our LUPerson-NL is well\\nbalanced in terms of identity distribution, making it a suit-\\nable dataset for person Re-ID tasks.\\n\\nIn spite of our dedicatedly designed tracking and filter-\\ning strategies as proposed in Sec 3.1, the identity labels we\\nobtained can never be very accurate due to the technical up-\\nper bounds of current tracking methods. Figure 3 visualizes\\nthe two noise types in LUPerson-NL that are caused by dif-\\nferent labeling errors, which are Noise-I, where the same\\nperson is split into different tracklets and is mistaken as dif-\\nferent persons; and Noise-II, that different persons are\\nrecognized as the same person.\\n\\nfc\\xf0\\x9d\\x92\\x99!\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x87\"#\\xf0\\x9d\\x92\\x99!#\\xf0\\x9d\\x92\\x99!\"\\xf0\\x9d\\x90\\xb8#\\xf0\\x9d\\x90\\xb8$\\xf0\\x9d\\x91\\xa6!&\\xf0\\x9d\\x91\\xa6!\\xf0\\x9d\\x92\\x8c!!,\\xf0\\x9d\\x92\\x8c!\",\\xe2\\x80\\xa6,\\xf0\\x9d\\x92\\x8c!#$\\xf0\\x9d\\x91\\xa6!!,$\\xf0\\x9d\\x91\\xa6!\",\\xe2\\x80\\xa6,$\\xf0\\x9d\\x91\\xa6!#&\\xf0\\x9d\\x91\\xa6!prototypesqueue\\xf0\\x9d\\x92\\x92!\\xf0\\x9d\\x92\\x84\",\\xf0\\x9d\\x92\\x84#,\\xe2\\x80\\xa6,\\xf0\\x9d\\x92\\x84$Label Guided Contrastive LossClassificationLoss\\xf0\\x9d\\x92\\x8c!Label Update&\\xf0\\x9d\\x91\\xa6!Prototype BasedContrastive Loss\\xf0\\x9d\\x92\\x91!update prototypesenqueuemomentum updatew. gradientw/o. gradient\\xf0\\x9d\\x92\\x94!\\x0c(2)\\n\\n(3)\\n\\n4.2. Label Rectification with Prototypes\\n\\nAs depicted by Figure 4, we maintain prototypes as a dic-\\ntionary of feature vectors {c1, c2, . . . , cK}, where K is the\\nnumber of identities, ck \\xe2\\x88\\x88 Rd is a prototype representing a\\nclass-wise feature centroid. In each training step, we would\\nfirst evaluate the similarity score sk\\ni between the query fea-\\nture qi and each of the current prototypes ck by\\n\\nsk\\ni =\\n\\nexp(qi \\xc2\\xb7 ck/\\xcf\\x84 )\\nk=1 exp(qi \\xc2\\xb7 ck/\\xcf\\x84 )\\n\\n.\\n\\n(cid:80)K\\n\\nLet pi be the classification probability given by the clas-\\nsifier with weights updated in the previous step. The rec-\\ntified label \\xcb\\x86yi for this step is then generated by combining\\nboth the prototype scores si = {sk\\nk=1 and the classifica-\\ntion probability pi as\\n\\ni }K\\n\\nli =\\n\\n(pi + si),\\n\\n1\\n2\\n(cid:40)\\n\\n\\xcb\\x86yi =\\n\\narg maxj lj\\nyi\\n\\ni\\n\\nif maxj lj\\notherwise.\\n\\ni > T ,\\n\\nHere we compute a soft pseudo label li and convert it to a\\nhard one \\xcb\\x86yi based on a threshold T . If the highest score in li\\nis larger than T , the corresponding class would be selected\\nas \\xcb\\x86yi, otherwise the original raw label yi would be kept.\\n\\n4.3. Prototype Based Contrastive Learning\\n\\nThe newly rectified label \\xcb\\x86yi can then be used to super-\\nvise the cross-entropy loss Li\\nce for classification as formu-\\nlated by Equation 1. Besides, it also helps train prototypes\\nck in return. In specific, we propose a prototype based con-\\ntrastive loss Li\\npro to constrain that the feature of each sam-\\nple should be closer to the prototype it belongs to. We for-\\nmulate the loss as\\n\\nLi\\n\\npro = \\xe2\\x88\\x92log\\n\\nexp(qi \\xc2\\xb7 c\\xcb\\x86yi/\\xcf\\x84 )\\nj=1 exp(qi \\xc2\\xb7 cj/\\xcf\\x84 )\\n\\n,\\n\\n(cid:80)K\\n\\n(4)\\n\\nwith qi being the query feature from Eq, \\xcf\\x84 being a hyper-\\n\\nparameter representing temperature.\\n\\nAll the prototypes are maintained as a dictionary, with\\n\\nstep-wise updates following a momentum mechanism as\\nc\\xcb\\x86yi = mc\\xcb\\x86yi + (1 \\xe2\\x88\\x92 m)qi.\\n\\n(5)\\n\\n4.4. Label-Guided Contrastive Learning\\n\\nInstance-wise contrastive learning proved to be very ef-\\nfective in self-supervised learning [4, 5, 7, 17, 19]. It learns\\ninstance-level feature discrimination by encouraging simi-\\nlarity among features from the same instance, while pro-\\nmoting dissimilarity between features from different in-\\nstances. The instance-wise contrastive loss is given by\\n\\nLi\\n\\nic = \\xe2\\x88\\x92log\\n\\nexp(qi \\xc2\\xb7 k+\\ni /\\xcf\\x84 ) + (cid:80)M\\n\\ni /\\xcf\\x84 )\\nj=1 exp(qi \\xc2\\xb7 k\\xe2\\x88\\x92\\n\\nj /\\xcf\\x84 )\\n\\n, (6)\\n\\nexp(qi \\xc2\\xb7 k+\\n\\nwith qi being the query feature of current instance i.\\nk+\\ni (= ki) is the positive key feature generated from the\\nmomentum encoder Ek. It is marked positive since it shares\\n\\xe2\\x88\\x97 \\xe2\\x88\\x88 Rd, on the contrary, are\\nthe same instance with qi. k\\xe2\\x88\\x92\\nthe rest features stored in a queue that represent negative\\nsamples. The queue has a size of M . At the end of each\\ntraining step, the queue would be updated by en-queuing\\nthe new key feature and de-queuing the oldest one.\\n\\nSuch instance-level contrastive learning is far from per-\\nfect, as it neglects the relationships among different in-\\nstances. For example, even though two instances depict the\\nsame person, it would still strengthen the gap between their\\nInstead, we propose a label guided contrastive\\nfeatures.\\nlearning module, making use of the rectified labels \\xcb\\x86yi to\\nensure more reasonable grouping of contrastive pairs.\\n\\nWe redesign the queue to additionally record labels \\xcb\\x86yi.\\nRepresented by Q = [(kjt, \\xcb\\x86yjt)]M\\nt=1, our new queue accepts\\nnot only a key feature ki but also its rectified label \\xcb\\x86yi during\\nupdate. These newly recorded labels help better distinguish\\npositive and negative pairs. Let P(i) be the new set of posi-\\ntive features and N (i) the new set of negative features: fea-\\ntures in P(i) share the same rectified label with the current\\ninstance i while features in N (i) do not. Our label guided\\ncontrastive loss can be given by\\n\\nLi\\n\\nlgc =\\n\\n\\xe2\\x88\\x921\\n|P(i)|\\n\\nlog\\n\\nexp\\n\\n(cid:17)\\n\\n(cid:16) qi\\xc2\\xb7k+\\n\\xcf\\x84\\n\\n(cid:80)\\nk+\\xe2\\x88\\x88P(i)\\n(cid:16) qi\\xc2\\xb7k+\\n\\xcf\\x84\\n\\n(cid:17)\\n\\n(cid:17) ,\\n\\n(cid:16) qi\\xc2\\xb7k\\xe2\\x88\\x92\\n\\xcf\\x84\\n\\n+ (cid:80)\\nk\\xe2\\x88\\x92\\xe2\\x88\\x88N (i)\\n\\nexp\\n\\n(cid:80)\\nk+\\xe2\\x88\\x88P(i)\\n\\nexp\\n\\n(7)\\n\\nwith\\n\\nP(i) = {kjt|\\xcb\\x86yjt = \\xcb\\x86yi, \\xe2\\x88\\x80(kjt, \\xcb\\x86yjt) \\xe2\\x88\\x88 Q} \\xe2\\x88\\xaa {ki},\\nN (i) = {kjt|\\xcb\\x86yjt \\xcc\\xb8= \\xcb\\x86yi, \\xe2\\x88\\x80(kjt, \\xcb\\x86yjt) \\xe2\\x88\\x88 Q},\\n\\n(8)\\n\\nwhere ki and \\xcb\\x86yi are the key feature and the rectified label of\\nthe current instance i.\\n\\nFinally we combine all the components above to pre-\\n\\ntrain models on LUPerson-NL with the following loss\\nce + \\xce\\xbbproLi\\n\\npro + \\xce\\xbblgcLi\\n\\nLi = Li\\n\\nlgc.\\n\\n(9)\\n\\nWe set \\xce\\xbbpro = \\xce\\xbblgc = 1 during training.\\n\\n5. Experiments\\n\\n5.1. Implementation\\n\\nHyper-parameter settings. We set the hyper-parameters\\n\\xcf\\x84 = 0.1 and T = 0.8. The momentum m for updating\\nboth the momentum encoder Ek and the prototypes is set\\nto 0.999. More hyper-parameters exploration and training\\ndetails can be found at supplementary materials.\\nDataset and protocol. We conduct extensive experiments\\non four popular person Re-ID datasets: CUHK03, Market,\\n\\n\\x0cpre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\npre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\nTrip [21]\\n45.2/63.8\\n55.5/61.2\\n62.6/67.6\\n69.1/73.1\\n\\nTrip [21]\\n65.2/80.7\\n65.4/81.1\\n69.8/83.1\\n71.0/84.7\\n\\nIDE [59]\\n50.6/55.9\\n52.5/57.7\\n57.6/62.3\\n68.3/73.5\\n\\nIDE [59]\\n62.8/80.8\\n63.4/81.6\\n65.9/82.2\\n70.3/85.0\\n\\nMGN [51]\\n70.5/71.2\\n67.1/67.0\\n74.7/75.4\\n80.4/80.9\\n\\nMGN [51]\\n79.4/89.0\\n79.5/89.1\\n82.1/91.0\\n84.3/92.0\\n\\npre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\npre-train\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\nTrip [21]\\n76.2/89.7\\n75.1/88.5\\n79.8/71.5\\n81.2/91.4\\n\\nTrip [21]\\n34.3/54.8\\n34.4/55.4\\n36.6/57.1\\n41.4/61.6\\n\\nIDE [59]\\n74.1/90.2\\n74.5/89.3\\n77.9/91.0\\n82.4/92.8\\n\\nIDE [59]\\n36.2/66.2\\n37.6/67.3\\n39.8/68.9\\n44.0/72.0\\n\\nMGN [51]\\n87.5/95.1\\n88.2/95.3\\n91.0/96.4\\n91.9/96.6\\n\\nMGN [51]\\n63.7/85.1\\n62.7/84.3\\n65.7/85.5\\n68.0/86.0\\n\\n(a) CUHK03\\n\\n(b) Market1501\\n\\n(c) DukeMTMC\\n\\n(d) MSMT17\\n\\nTable 2. Comparing three supervised Re-ID baselines using different pre-trained models. \\xe2\\x80\\x9cIN sup.\\xe2\\x80\\x9d/\\xe2\\x80\\x9cIN unsup.\\xe2\\x80\\x9d indicates model that is\\nsupervisely/unsupervisely pre-trained on ImageNet; \\xe2\\x80\\x9cLUP unsup.\\xe2\\x80\\x9d is the model unsupervisely pre-trained on LUPerson; \\xe2\\x80\\x9cLUPnl pnl.\\xe2\\x80\\x9c\\nrefers to the model that pre-trained on LUPerson-NL using our PNL framework. All results are shown in mAP/cmc1.\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\n10%\\n53.1/76.9\\n58.4/81.7\\n64.6/85.5\\n72.4/88.8\\n\\n30%\\n75.2/90.8\\n76.6/91.9\\n81.9/93.7\\n85.2/94.2\\n\\n10%\\n45.1/65.3\\n48.1/66.9\\n53.5/72.0\\n60.6/75.8\\n\\n30%\\n64.7/80.2\\n65.8/80.2\\n69.4/81.9\\n74.5/86.3\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\n10%\\n23.2/50.2\\n22.6/48.8\\n25.5/51.1\\n28.2/51.1\\n\\n30%\\n41.9/70.8\\n40.4/68.7\\n44.6/71.4\\n47.7/71.2\\n\\nsmall-scale\\n50%\\n81.5/93.5\\n82.0/94.1\\n85.8/94.9\\n88.3/95.5\\n\\nsmall-scale\\n50%\\n71.8/84.6\\n72.5/84.4\\n75.6/86.7\\n78.8/88.3\\n\\nsmall-scale\\n50%\\n50.3/76.9\\n49.0/75.0\\n53.0/77.7\\n55.5/77.2\\n\\n70%\\n84.8/94.5\\n85.4/94.5\\n88.8/95.9\\n90.1/96.2\\n\\n90%\\n86.9/95.2\\n87.4/95.5\\n90.5/96.4\\n91.3/96.4\\n\\n10%\\n21.1/41.8\\n18.6/36.1\\n26.4/47.5\\n42.0/61.6\\n\\n30%\\n68.1/87.6\\n69.3/87.8\\n78.3/92.1\\n83.7/94.0\\n\\n(a) Market1501\\n\\n70%\\n75.5/86.8\\n76.3/86.9\\n78.9/88.2\\n81.6/89.5\\n\\n90%\\n78.0/88.3\\n78.5/88.7\\n81.1/90.0\\n83.3/91.2\\n\\n10%\\n31.5/47.1\\n32.4/48.0\\n35.8/50.2\\n52.2/64.1\\n\\n30%\\n65.4/79.8\\n65.3/80.2\\n72.3/83.8\\n77.7/87.9\\n\\n(b) DukeMTMC\\n\\n70%\\n56.9/81.2\\n55.7/79.9\\n59.5/81.8\\n61.6/81.8\\n\\n90%\\n61.9/84.2\\n60.9/83.0\\n63.7/85.0\\n66.1/84.8\\n\\n10%\\n14.7/34.1\\n13.2/29.2\\n17.0/36.0\\n24.5/42.7\\n\\n30%\\n44.5/71.1\\n41.4/67.1\\n49.0/73.6\\n53.2/74.4\\n\\nfew-shot\\n50%\\n80.2/92.8\\n78.3/90.9\\n84.2/93.9\\n88.1/95.2\\n\\nfew-shot\\n50%\\n73.9/85.7\\n73.7/85.1\\n77.7/87.4\\n81.1/89.6\\n\\nfew-shot\\n50%\\n56.2/79.5\\n53.3/77.6\\n57.4/80.5\\n62.2/81.0\\n\\n70%\\n84.2/94.0\\n84.4/94.1\\n88.4/95.5\\n90.5/96.3\\n\\n90%\\n86.7/94.6\\n87.1/95.2\\n90.4/96.3\\n91.6/96.4\\n\\n70%\\n77.2/87.8\\n77.7/87.8\\n80.8/89.2\\n83.2/91.1\\n\\n90%\\n79.1/88.8\\n79.4/89.0\\n82.0/90.6\\n84.1/91.3\\n\\n70%\\n60.9/82.8\\n59.1/81.5\\n62.9/83.5\\n65.8/83.8\\n\\n90%\\n63.4/84.5\\n62.4/83.8\\n65.0/85.1\\n67.4/85.3\\n\\n(c) MSMT17\\nTable 3. Comparing pre-trained models on three labeled Re-ID datasets, under the small-scale setting and the few-shot setting, with\\ndifferent usable data percentages. \\xe2\\x80\\x9cLUPnl pnl.\\xe2\\x80\\x9d is our model pre-trained on LUPerson-NL using PNL. Results are shown in mAP/cmc1.\\n\\nDukeMTMC and MSMT17. We adopt their official set-\\ntings, except CUHK03 where its labeled counterpart with\\nnew protocols proposed in [62] is used. We follow the stan-\\ndard evaluation metrics: the mean Average Precision (mAP)\\nand the Cumulated Matching Characteristics top-1 (cmc1).\\n5.2. Improving Supervised Re-ID\\n\\nTo evaluate our pre-trained model based on LUPerson-\\nNL with respect to supervised person Re-ID tasks. we\\nconduct experiments using three representative supervised\\nRe-ID baselines with different pre-training models. These\\nbaseline methods include two simpler approaches driven\\nonly by the triplet loss (Trip [21]) or the classification loss\\n(IDE [59]), as well as a stronger and more complex method\\nMGN [51] that use both triplet and classification losses.\\n\\n{\\xe2\\x80\\x9cIN\\xe2\\x80\\x9d, \\xe2\\x80\\x9cLUP\\xe2\\x80\\x9d, \\xe2\\x80\\x9cLUPnl\\xe2\\x80\\x9d} represent ImageNet [43], LU-\\nPerson [12] and our LUPerson-NL respectively; while the\\n{\\xe2\\x80\\x9csup.\\xe2\\x80\\x9d, \\xe2\\x80\\x9cunsup.\\xe2\\x80\\x9d, \\xe2\\x80\\x9cpnl.\\xe2\\x80\\x9d} stand for the {\\xe2\\x80\\x9csupervised\\xe2\\x80\\x9d, \\xe2\\x80\\x9cun-\\nsupervised\\xe2\\x80\\x9d, and \\xe2\\x80\\x9cpretrain with noisy label\\xe2\\x80\\x9d} pre-training\\nmethods. e.g. the \\xe2\\x80\\x9cLUPnl pnl.\\xe2\\x80\\x9d in the bottom rows of Ta-\\nble 2 all refer to our model, which is pre-trained on our\\nLUPerson-NL dataset using our PNL framework.\\n\\nFrom Table 2 we can see, for all of the three base-\\nline methods, our pre-trained model improves their perfor-\\nmances greatly on the four popular person Re-ID datasets.\\nSpecifically, the improvements are at least 5.7%, 0.9%,\\n1.2% and 2.3% in terms of mAP on CUHK03, Market1501,\\nDukeMTMC and MSMT17 respectively.\\n\\nWe report results in Table 2, where the abbreviations\\n\\nNote that even though the performance of the baseline\\n\\n\\x0cMGN on Market1501 has been extremely high, our model\\nstill brings considerable improvement over it. The other\\nway around, our pre-trained models obtain more signifi-\\ncant improvements on relatively weak methods (Trip and\\nIDE), unveiling that model initialization plays a critical part\\nin person Re-ID training.\\n\\nOur noisy label guided pre-training models are also sig-\\nnificantly advantageous over the previous \\xe2\\x80\\x9cLUPerson un-\\nsup\\xe2\\x80\\x9c models, which emphasizes the superiority of our PNL\\nframework and our LUPerson-NL dataset.\\n\\n5.3. Improving Unsupervised Re-ID Methods\\n\\nOur pre-trained model can also benefit unsupervised per-\\nson Re-ID methods. Based on the state-of-the-art unsuper-\\nvised method SpCL [15], we explore different pre-training\\nmodels utilizing two settings proposed by SpCL: the pure\\nunsupervised learning (USL) and the unsupervised domain\\nadaptation (UDA). Results in Table 4 illustrate that our pre-\\ntrained model outperforms the others in all UDA tasks, as\\nwell as the USL task on DukeMTMC dataset. In the USL\\ntask on Market1501, we achieve the second best scores\\nslightly lower than the LUPerson model [12].\\n\\n5.4. Comparison on Small-scale and Few-shot\\n\\nFollowing the same protocols proposed by [12], we con-\\nduct experiments under two small data settings: the small-\\nscale setting and the few-shot setting. The small-scale set-\\nting restricts the percentage of usable identities, while the\\nfew-shot setting restricts the percentage of usable person\\nimages each identity has. Under both settings, we vary\\nthe usable data percentages of three popular datasets from\\n10% \\xe2\\x88\\xbc 100%. We compare different pre-trained models\\nunder these settings with MGN as the baseline method. The\\nresults shown in Table 3 verify the consistent improvements\\nbrought by our model on all the datasets under both settings.\\nBesides, the results in Table 3 show that the gains of\\nour pre-trained models are even larger under a more lim-\\nited amount of labeled data. For example, under the \\xe2\\x80\\x9csmall-\\nscale\\xe2\\x80\\x9d setting, our model outperforms \\xe2\\x80\\x9cLUPerson unsup\\xe2\\x80\\x9d\\nby 7.8%, 7.1% and 2.7% on Market1501, DukeMTMC and\\nMSMT17 respectively with 10% identities. The improve-\\nments rise to 15.6%, 16.4% and 6.5% under the \\xe2\\x80\\x9cfew-shot\\xe2\\x80\\x9d\\nsetting with 10% person images.\\n\\nMost importantly, our pre-trained \\xe2\\x80\\x9cLUPnl pnl\\xe2\\x80\\x9d model\\nhelps achieve advantageous results with a mAP of 72.4 and\\na cmc1 of 88.8, using only 10% labeled data from the Mar-\\nket1501 training set. The task is really challenging, con-\\nsidering that the training set composes only 1, 170 images\\nbelonging to 75 identities; while evaluations are performed\\non a much larger testing set with 19, 281 images belong-\\ning to 750 identities. We consider these results extremely\\nappealing as they demonstrate the strong potential of our\\npre-trained models in real-world applications.\\n\\npre-train\\n\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPnl pnl.\\n\\nUSL\\n\\nM\\n72.4/87.8\\n72.9/88.6\\n76.2/90.2\\n75.6/89.3\\n\\nD\\n64.9/80.3\\n62.6/78.8\\n67.1/81.6\\n68.1/82.0\\n\\nUDA\\n\\nD \\xe2\\x86\\x92 M\\n76.4/90.1\\n77.1/90.6\\n79.2/91.7\\n80.7/92.2\\n\\nM \\xe2\\x86\\x92 D\\n67.9/82.3\\n66.3/81.6\\n69.1/83.2\\n72.2/84.9\\n\\nTable 4. Performances of different pre-trained models on the un-\\nsupervised Re-ID method SpCL [15] under two unsupervised task\\nsettings: the pure unsupervised learning (USL) and the unsuper-\\nvised domain adaptation (UDA). Here M and D refer to the Mar-\\nket1501 dataset and the DukeMTMC dataset respectively.\\n\\nmethod\\nMSMT17\\n\\nSupCont [26]\\n66.5/84.7\\n\\nLUP [12]\\n65.3/84.0\\n\\nPNL(ours)\\n68.0/86.0\\n\\n20%\\n\\nTable 5. Performance comparison for different pre-training meth-\\nods on LUPerson-NL dataset.\\n# ce ic pro lgc\\n1 \\xe2\\x9c\\x93\\n\\xe2\\x9c\\x93\\n2\\n3 \\xe2\\x9c\\x93 \\xe2\\x9c\\x93\\n4 \\xe2\\x9c\\x93\\n5 \\xe2\\x9c\\x93\\n6 \\xe2\\x9c\\x93 \\xe2\\x9c\\x93 \\xe2\\x9c\\x93\\n7 \\xe2\\x9c\\x93\\n\\n40%\\n32.0/56.1 45.0/69.5 62.7/83.0\\n34.5/59.5 47.9/72.6 65.3/84.0\\n37.6/62.6 49.6/73.5 66.5/84.7\\n35.7/59.1 48.5/72.4 65.8/84.1\\n\\xe2\\x9c\\x93 38.5/63.0 50.9/74.5 67.1/85.2\\n39.0/63.4 51.7/74.4 67.4/85.4\\n\\xe2\\x9c\\x93 \\xe2\\x9c\\x93 39.6/63.7 51.9/75.0 68.0/86.0\\n\\n100%\\n\\n\\xe2\\x9c\\x93\\n\\nTable 6. Ablating components of PNL on MSMT with data per-\\ncentages 20%, 40% and 100% under the small scale setting. ce:\\nsupervised classification; ic:\\ninstance-wise contrastive learning;\\npro: prototypes for both prototype-based contrastive learning and\\nlabel rectification; lgc: label-guided contrastive learning.\\n\\n5.5. Comparison with other pre-training methods\\n\\nWe compare our proposed PNL with some other popu-\\nlar pre-training methods in Table 5. LUP [12] is a varient\\nof MoCoV2 for person Re-ID based on unsupervised con-\\nstrastive learning, while SupCont [26] considers both su-\\npervised and constrastive learning. Our PNL outperforms\\nall these rep-resentative pre-training methods, indicating the\\nsuperiority of our proposed method.\\n5.6. Ablation Study\\n\\nWe also investigate the effectiveness of each designed\\ncomponent in PNL through ablation experiments. Results\\nshown by Table 6 illustrate the efficacy of our proposed\\ncomponents. We have the following observations: i) Train-\\ning with an instance-wise contrastive loss Li\\nic (row 2) with-\\nout using any labels leads to even better performance than\\ntraining with a classification loss Li\\nce (row 1) that utilizes\\nthe labels from LUPerson-NL, implying that the noisy la-\\nbels in LUPerson-NL would misguide representation learn-\\nii) Jointly training\\ning if directly adopted as supervision.\\nwith both losses Li\\nic (row 3) improves over us-\\ning only one loss (row 1, row 2), suggesting that learning\\ninstance-wise discriminative representations complements\\niii) The prototypes which contribute to\\nlabel supervision.\\n\\nce and Li\\n\\n\\x0csetting\\n\\nMSMT17\\n\\nce+pro\\n\\nce+pro+lgc\\n\\nw/o. lc\\n64.8/83.4\\n\\nw. lc\\n65.8/84.1\\n\\nw/o. lc\\n66.7/85.0\\n\\nw. lc\\n68.0/86.0\\n\\nTable 7. Ablating the label correction. lc: \\xe2\\x80\\x9clabel correction\\xe2\\x80\\x9d.\\n\\nMethod\\nMGN\\xe2\\x80\\xa0 [51] (2018)\\nBOT [37] (2019)\\nDSA [57] (2019)\\nAuto [41] (2019)\\nABDNet [3] (2019)\\nSCAL [2] (2019)\\nMHN [1] (2019)\\nBDB [9] (2019)\\nSONA [54] (2019)\\nGCP [39] (2020)\\nSAN [24] (2020)\\nISP [63] (2020)\\nGASM [20] (2020)\\nESNET [44] (2020)\\nLUP [12](2020)\\nOurs+MGN\\nOurs+BDB\\n\\nCUHK03 Market1501 DukeMTMC MSMT17\\n63.7/85.1\\n70.5/71.2\\n-\\n-\\n-\\n75.2/78.9\\n52.5/78.2\\n73.0/77.9\\n60.8/82.3\\n-\\n-\\n72.3/74.8\\n-\\n72.4/77.2\\n-\\n76.7/79.4\\n-\\n79.2/81.8\\n-\\n75.6/77.9\\n55.7/79.2\\n76.4/80.1\\n-\\n74.1/76.5\\n52.5/79.5\\n-\\n57.3/80.5\\n-\\n65.7/85.5\\n79.6/81.9*\\n68.0/86.0\\n80.4/80.9\\n82.3/84.7\\n53.4/79.0\\n\\n79.4/89.0\\n76.4/86.4\\n74.3/86.2\\n-\\n78.6/89.0\\n79.6/89.0\\n77.2/89.1\\n76.0/89.0\\n78.3/89.4\\n78.6/87.9\\n75.5/87.9\\n80.0/89.6\\n74.4/88.3\\n78.7/88.5\\n82.1/91.0\\n84.3/92.0\\n79.0/89.2\\n\\n87.5/95.1\\n85.9/94.5\\n87.6/95.7\\n85.1/94.5\\n88.3/95.6\\n89.3/95.8\\n85.0/95.1\\n86.7/95.3\\n88.8/95.6\\n88.9/95.2\\n88.0/96.1\\n88.6/95.3\\n84.7/95.3\\n88.6/95.7\\n91.0/96.4\\n91.9/96.6\\n88.4/95.4\\n\\nTable 8. Comparison with the state of the art. Numbers of MGN\\xe2\\x80\\xa0\\ncome from a re-implementation based on FastReID, which are\\neven better than the original. Numbers of PNL marked by * are ob-\\ntained on BDB, the rest without the * mark are obtained on MGN.\\nWe show best scores in bold and the second scores underlined.\\n\\n5.8. Comparison with State-of-the-Art Methods\\n\\nWe compare our results with current state-of-the-art\\nmethods on four public benchmarks. We don\\xe2\\x80\\x99t apply any\\npost-processing techniques such as IIA [13] and RR [62].\\nTo ensure fairness, we adopt ResNet50 as our backbone and\\ndoes not compare with methods that rely on stronger back-\\nbones (results with stronger backbones e.g. ResNet101 can\\nbe found at supplementary materials). Results in Table 8\\nverify the remarkable advantage brought by our pre-trained\\nmodels. Without bells and whistles, we achieve state-of-\\nthe-art performance on all four benchmarks, outperforming\\nthe second with clear margins.\\n\\n6. Conclusion\\n\\nIn this paper, we demonstrate that large-scale Re-ID\\nrepresentation can be directly learned from massive raw\\nvideos by leveraging the spatial and temporal information.\\nWe not only build a large-scale noisy labeled person Re-\\nID dataset LUPerson-NL based on tracklets of raw videos\\nfrom LUPerson without using manual annotations, but also\\ndesign a novel weakly supervised pretraining framework\\nPNL comprising different learning modules including su-\\npervised learning, prototypes-based learning, label-guided\\ncontrastive learning and label rectification. Equipped with\\nour pre-trained models, we push existing benchmark results\\nto a new limit, which outperforms unsupervised pre-trained\\nmodels and ImageNet supervised pre-trained models by a\\n\\n(a) Correction for Noise-I\\n\\n(b) Correction for Noise-II\\n\\nFigure 5. Visualizing the label correction functionality of our PNL\\nframework with respect to the two noise types from LUPerson-\\nNL. Person images in the same rectangle indicate that they are\\nrecognized as the same identity. The right-hand similarity matrices\\nare calculated based on the image features all learned using our\\nPNL framework with label correction.\\n\\nboth prototype-based contrastive learning and the label cor-\\nrection, are very important under various settings, as veri-\\nfied by comparing row 1 with row 4; row 3 with row 6; and\\nrow 5 with row 7. iv) Our label-guided contrastive learning\\ncomponent consistently outperforms the vanilla instance-\\nwise contrastive learning under various settings, as verified\\nby comparing row 3 with row 5, and row 6 with row 7. v)\\nCombining all our components (supervised classification,\\nprototypes and label-guided contrastive learning) together\\nleads to the best performance as shown by row 7.\\n5.7. Label Correction\\n\\nOur PNL can indeed correct noisy labels. We demon-\\nstrate two typical examples in Figure 5 visualizing the la-\\nbel corrections with respect to the two kinds of noises. As\\nwe can see, in Figure 5a the same person are marked as\\nthree different persons in LUPerson-NL due to Noise-I\\nin labels. After our PNL pre-training, these three track-\\nlets are merged together since their trained features are very\\nclose, as verified by the right-hand similarity matrix. In Fig-\\nure 5b different persons are labeled as the same identity\\nin LUPerson-NL due to Noise-II in labels. After PNL\\ntraining, these mis-labeled person identities are all correctly\\nre-grouped into two identities, which can also be reflected\\nby the right-hand similarity matrix.\\n\\nwe also ablate the label correction module in Table 7\\nwith different settings, and observe it can improve the per-\\nIt also validates the importance of combining\\nformance.\\nlabel rectification with label-guided contrastive learning to-\\ngether, where more accurate positive/negative pairs can be\\nleveraged.\\n\\nBeforeAfter2103876954Similarity MatrixFinal ID=179919BeforeAfter2103876954Similarity MatrixFinal ID=419043Final ID=73264\\x0clarge margin.\\nAcknowledgement. This work is partially supported by\\nthe National Natural Science Foundation of China (NSFC,\\n61836011).\\n\\nReferences\\n\\n[1] Binghui Chen, Weihong Deng, and Jiani Hu. Mixed high-\\norder attention network for person re-identification. In Pro-\\nceedings of the IEEE International Conference on Computer\\nVision, pages 371\\xe2\\x80\\x93381, 2019. 8\\n\\n[2] Guangyi Chen, Chunze Lin, Liangliang Ren, Jiwen Lu, and\\nSelf-critical attention learning for person re-\\nJie Zhou.\\nIn Proceedings of the IEEE International\\nidentification.\\nConference on Computer Vision, pages 9637\\xe2\\x80\\x939646, 2019. 8\\n[3] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang\\nChen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-\\nIn Pro-\\nnet: Attentive but diverse person re-identification.\\nceedings of the IEEE International Conference on Computer\\nVision, pages 8351\\xe2\\x80\\x938361, 2019. 8\\n\\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. arXiv preprint arXiv:2002.05709,\\n2020. 2, 4, 5\\n\\n[5] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\\nNorouzi, and Geoffrey Hinton. Big self-supervised mod-\\narXiv preprint\\nels are strong semi-supervised learners.\\narXiv:2006.10029, 2020. 2, 4, 5\\n\\n[6] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi\\nHuang. Beyond triplet loss: a deep quadruplet network for\\nIn Proceedings of the IEEE con-\\nperson re-identification.\\nference on computer vision and pattern recognition, pages\\n403\\xe2\\x80\\x93412, 2017. 2\\n\\n[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\\nImproved baselines with momentum contrastive learning.\\narXiv preprint arXiv:2003.04297, 2020. 2, 4, 5\\n\\n[8] Zhirui Chen, Jianheng Li, and Wei-Shi Zheng. Weakly su-\\npervised tracklet person re-identification by deep feature-\\narXiv preprint arXiv:1910.14333,\\nwise mutual learning.\\n2019. 3\\n\\n[9] Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu,\\nand Ping Tan. Batch dropblock network for person re-\\nidentification and beyond. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pages 3691\\xe2\\x80\\x933701,\\n2019. 8\\n\\n[10] Piotr Doll\\xc2\\xb4ar, Ron Appel, Serge Belongie, and Pietro Per-\\nIEEE\\nona. Fast feature pyramids for object detection.\\ntransactions on pattern analysis and machine intelligence,\\n36(8):1532\\xe2\\x80\\x931545, 2014. 4\\n\\n[11] Pedro F Felzenszwalb, Ross B Girshick, David McAllester,\\nand Deva Ramanan. Object detection with discriminatively\\nIEEE transactions on pattern\\ntrained part-based models.\\nanalysis and machine intelligence, 32(9):1627\\xe2\\x80\\x931645, 2009.\\n4\\n\\nings of the IEEE conference on computer vision and pattern\\nrecognition, 2021. 1, 2, 3, 4, 6, 7, 8\\n\\n[13] Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen,\\nJianmin Bao, Gang Hua, and Houqiang Li. Improving person\\nre-identification with iterative impression aggregation. IEEE\\nTransactions on Image Processing, 29:9559\\xe2\\x80\\x939571, 2020. 8\\n[14] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-\\nteaching: Pseudo label refinery for unsupervised domain\\nadaptation on person re-identification. In International Con-\\nference on Learning Representations, 2019. 2\\n\\n[15] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hong-\\nsheng Li. Self-paced contrastive learning with hybrid mem-\\nory for domain adaptive object re-id. In Advances in Neural\\nInformation Processing Systems, 2020. 2, 7\\n\\n[16] Douglas Gray and Hai Tao. Viewpoint invariant pedes-\\ntrian recognition with an ensemble of localized features. In\\nEuropean conference on computer vision, pages 262\\xe2\\x80\\x93275.\\nSpringer, 2008. 4\\n\\n[17] Jean-Bastien Grill, Florian Strub, Florent Altch\\xc2\\xb4e, Corentin\\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\\nnew approach to self-supervised learning. arXiv preprint\\narXiv:2006.07733, 2020. 2, 4, 5\\n\\n[18] Xinqian Gu, Bingpeng Ma, Hong Chang, Shiguang Shan,\\nand Xilin Chen. Temporal knowledge propagation for image-\\nIn Proceedings of the\\nto-video person re-identification.\\nIEEE/CVF International Conference on Computer Vision,\\npages 9647\\xe2\\x80\\x939656, 2019. 2\\n\\n[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\nGirshick. Momentum contrast for unsupervised visual rep-\\nresentation learning. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n9729\\xe2\\x80\\x939738, 2020. 2, 4, 5\\n\\n[20] Lingxiao He and Wu Liu. Guided saliency feature learning\\nfor person re-identification in crowded scenes. In European\\nConference on Computer Vision, pages 357\\xe2\\x80\\x93373. Springer,\\n2020. 8\\n\\n[21] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-\\nfense of the triplet loss for person re-identification. arXiv\\npreprint arXiv:1703.07737, 2017. 2, 6\\n\\n[22] Yan Huang, Qiang Wu, JingSong Xu, and Yi Zhong. Sbsgan:\\nSuppression of inter-domain background shift for person re-\\nidentification. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision, pages 9527\\xe2\\x80\\x939536, 2019. 2\\n[23] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li\\nZhang. Style normalization and restitution for generalizable\\nIn Proceedings of the IEEE/CVF\\nperson re-identification.\\nConference on Computer Vision and Pattern Recognition,\\npages 3143\\xe2\\x80\\x933152, 2020. 2\\n\\n[24] Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, and\\nZhibo Chen. Semantics-aligned representation learning for\\nperson re-identification. In AAAI, pages 11173\\xe2\\x80\\x9311180, 2020.\\n8\\n\\n[12] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu\\nYuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsu-\\npervised pre-training for person re-identification. Proceed-\\n\\n[25] Srikrishna Karanam, Mengran Gou, Ziyan Wu, Angels\\nRates-Borras, Octavia Camps, and Richard J Radke. A\\ncomprehensive evaluation and benchmark for person re-\\n\\n\\x0cidentification: Features, metrics, and datasets. arXiv preprint\\narXiv:1605.09653, 2(3):5, 2016. 1, 4\\n\\n[26] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\\nDilip Krishnan. Supervised contrastive learning. Advances\\nin Neural Information Processing Systems, 33, 2020. 7\\n[27] Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, and Shiliang\\nZhang. Global-local temporal representations for video per-\\nson re-identification. In Proceedings of the IEEE/CVF Inter-\\nnational Conference on Computer Vision, pages 3958\\xe2\\x80\\x933967,\\n2019. 2\\n\\n[28] Junnan Li, Caiming Xiong, and Steven Hoi. Mopro: Webly\\nsupervised learning with momentum prototypes. In Interna-\\ntional Conference on Learning Representations, 2020. 2, 4\\n[29] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsuper-\\nvised tracklet person re-identification. IEEE transactions on\\npattern analysis and machine intelligence, 42(7):1770\\xe2\\x80\\x931782,\\n2019. 3\\n\\n[30] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep-\\nreid: Deep filter pairing neural network for person re-\\nIn Proceedings of the IEEE conference on\\nidentification.\\ncomputer vision and pattern recognition, pages 152\\xe2\\x80\\x93159,\\n2014. 4\\n\\n[31] Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, and\\nYu-Chiang Frank Wang. Recover and identify: A generative\\ndual model for cross-resolution person re-identification. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 8090\\xe2\\x80\\x938099, 2019. 2\\n\\n[32] Yu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank\\nWang. Cross-dataset person re-identification via unsuper-\\nvised pose disentanglement and adaptation. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vi-\\nsion, pages 7919\\xe2\\x80\\x937929, 2019. 2\\n\\n[33] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi\\nYang. A bottom-up clustering approach to unsupervised per-\\nIn Proceedings of the AAAI Confer-\\nson re-identification.\\nence on Artificial Intelligence, volume 33, pages 8738\\xe2\\x80\\x938745,\\n2019. 2\\n\\n[34] Fangyi Liu and Lei Zhang. View confusion feature learning\\nfor person re-identification. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 6639\\xe2\\x80\\x93\\n6648, 2019. 2\\n\\n[35] Chen Change Loy, Chunxiao Liu, and Shaogang Gong. Per-\\nson re-identification by manifold ranking. In 2013 IEEE In-\\nternational Conference on Image Processing, pages 3567\\xe2\\x80\\x93\\n3571. IEEE, 2013. 4\\n\\n[36] Chuanchen Luo, Yuntao Chen, Naiyan Wang, and Zhaoxi-\\nang Zhang. Spectral feature transformation for person re-\\nidentification. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision, pages 4976\\xe2\\x80\\x934985, 2019. 2\\n[37] Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao,\\nShenqi Lai, and Jianyang Gu. A strong baseline and batch\\nnormalization neck for deep person re-identification. IEEE\\nTransactions on Multimedia, 2019. 8\\n\\n[38] Jingke Meng, Sheng Wu, and Wei-Shi Zheng. Weakly su-\\nIn Proceedings of the\\npervised person re-identification.\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 760\\xe2\\x80\\x93769, 2019. 3\\n\\n[39] Hyunjong Park and Bumsub Ham. Relation network for per-\\nson re-identification. In Proceedings of the AAAI Conference\\non Artificial Intelligence, volume 34, pages 11839\\xe2\\x80\\x9311847,\\n2020. 8\\n\\n[40] Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie\\nQiu, Yang Wu, Yu-Gang Jiang, and Xiangyang Xue. Pose-\\nnormalized image generation for person re-identification. In\\nProceedings of the European conference on computer vision\\n(ECCV), pages 650\\xe2\\x80\\x93667, 2018. 2\\n\\n[41] Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, and Yi\\nYang. Auto-reid: Searching for a part-aware convnet for\\nperson re-identification. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pages 3750\\xe2\\x80\\x933759,\\n2019. 8\\n\\n[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. In Advances in neural information pro-\\ncessing systems, pages 91\\xe2\\x80\\x9399, 2015. 4\\n\\n[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, et al.\\nImagenet large\\nscale visual recognition challenge. International journal of\\ncomputer vision, 115(3):211\\xe2\\x80\\x93252, 2015. 6\\n\\n[44] Dong Shen, Shuai Zhao, Jinming Hu, Hao Feng, Deng Cai,\\nand Xiaofei He. Es-net: Erasing salient parts to learn more\\nin re-identification. IEEE Transactions on Image Processing,\\n2020. 8\\n\\n[45] Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen,\\nand Xiaogang Wang. Person re-identification with deep\\nsimilarity-guided graph neural network. In Proceedings of\\nthe European conference on computer vision (ECCV), pages\\n486\\xe2\\x80\\x93504, 2018. 2\\n\\n[46] Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Ky-\\noung Mu Lee. Part-aligned bilinear representations for per-\\nson re-identification. In Proceedings of the European Con-\\nference on Computer Vision (ECCV), pages 402\\xe2\\x80\\x93419, 2018.\\n2\\n\\n[47] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\\nhigh-resolution representation learning for human pose esti-\\nmation. In CVPR, 2019. 3\\n\\n[48] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\\nWang. Beyond part models: Person retrieval with refined\\npart pooling (and a strong convolutional baseline). In ECCV,\\n2018. 2\\n\\n[49] Dongkai Wang and Shiliang Zhang. Unsupervised person re-\\nidentification via multi-label classification. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 10981\\xe2\\x80\\x9310990, 2020. 2\\n\\n[50] Guangrun Wang, Guangcong Wang, Xujie Zhang, Jianhuang\\nLai, Zhengtao Yu, and Liang Lin. Weakly supervised per-\\nson re-id: Differentiable graphical learning and a new bench-\\nmark. IEEE Transactions on Neural Networks and Learning\\nSystems, 32(5):2142\\xe2\\x80\\x932156, 2020. 3, 4\\n\\n[51] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\\nZhou. Learning discriminative features with multiple granu-\\nlarities for person re-identification. In 2018 ACM Multime-\\ndia Conference on Multimedia Conference, pages 274\\xe2\\x80\\x93282.\\nACM, 2018. 1, 2, 6, 8\\n\\n\\x0c[52] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\\nPerson transfer gan to bridge domain gap for person re-\\nIn Proceedings of the IEEE Conference on\\nidentification.\\nComputer Vision and Pattern Recognition, pages 79\\xe2\\x80\\x9388,\\n2018. 1, 4\\n\\n[53] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\nUnsupervised feature learning via non-parametric instance\\nIn Proceedings of the IEEE Conference\\ndiscrimination.\\non Computer Vision and Pattern Recognition, pages 3733\\xe2\\x80\\x93\\n3742, 2018. 2\\n\\n[54] Bryan Ning Xia, Yuan Gong, Yizhe Zhang, and Christian\\nPoellabauer. Second-order non-local attention networks for\\nperson re-identification. In Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pages 3760\\xe2\\x80\\x933769,\\n2019. 8\\n\\n[55] Ye Yuan, Wuyang Chen, Yang Yang, and Zhangyang Wang.\\nIn defense of the triplet loss again: Learning robust person\\nre-identification with fast approximated triplet loss and label\\ndistillation. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition Workshops, pages\\n354\\xe2\\x80\\x93355, 2020. 2\\n\\n[56] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\\nand Wenyu Liu. Fairmot: On the fairness of detection and\\nre-identification in multiple object tracking. arXiv e-prints,\\npages arXiv\\xe2\\x80\\x932004, 2020. 2, 3, 4\\n\\n[57] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo\\nChen. Densely semantically aligned person re-identification.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 667\\xe2\\x80\\x93676, 2019. 8\\n\\n[58] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\\ndong Wang, and Qi Tian. Scalable person re-identification:\\nA benchmark. 2015 IEEE International Conference on Com-\\nputer Vision (ICCV), pages 1116\\xe2\\x80\\x931124, 2015. 1, 4\\n\\n[59] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan\\nChandraker, Yi Yang, and Qi Tian. Person re-identification\\nin the wild. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition, pages 1367\\xe2\\x80\\x931376,\\n2017. 1, 2, 6\\n\\n[60] Zhedong Zheng, Liang Zheng, and Yi Yang. A discrimi-\\nnatively learned cnn embedding for person reidentification.\\nACM Transactions on Multimedia Computing, Communica-\\ntions, and Applications (TOMM), 14(1):1\\xe2\\x80\\x9320, 2017. 2\\n[61] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-\\nples generated by gan improve the person re-identification\\nbaseline in vitro. In Proceedings of the IEEE International\\nConference on Computer Vision, 2017. 1, 4\\n\\n[62] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-\\nranking person re-identification with k-reciprocal encoding.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 1318\\xe2\\x80\\x931327, 2017. 6, 8\\n[63] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao\\nWang. Identity-guided human semantic parsing for person\\nre-identification. ECCV, 2020. 8\\n\\n\\x0cIn this material, we will 1) show demo cases for scene\\nchanging for specific person in LUPerson-NL, 2) share the\\ntraining details for PNL, 3) provide the detailed algorithm\\ntable for PNL, 4) compare models pre-trained on SYSU30K\\nand LUPerson-NL 5) analyze the hyper parameters of our\\nPNL, 6) demonstrate results of our method using a stronger\\nbackbones, 7) explore the impact of pre-training image\\nscale, and 8) list more detailed results for our \\xe2\\x80\\x9csmall-scale\\xe2\\x80\\x9d\\nand \\xe2\\x80\\x9cfew-shot\\xe2\\x80\\x9d experiments. All these experiments are un-\\nder MGN settings.\\n\\nA. Scene changing in LUPerson-NL\\n\\nOur LUPerson-NL are driven from street view videos,\\nFigure 6 shows a demo person in our LUPerson-NL. As we\\ncan see, our LUPerson-NL is able to cover multiple scenes,\\nsince the videos we use have lots of moving cameras and\\nmoving persons, and only traklets with \\xe2\\x89\\xa5 200 frames are\\nselected. It is approximate close to the real person Re-ID\\nscenario.\\n\\nC. Algorithm for PNL\\n\\nAlgorithm 1 shows the procedure of training PNL, we\\ntrain our framework for 90 epochs, and apply label correc-\\ntion from 10 epochs. Once the rectification begins, we keep\\nrectifying for every iteration.\\n\\nAlgorithm 1: PNL algorithm.\\n\\nInput: total epochs N , correction start epochs Ns,\\nprototype feature vectors {\\xe2\\x83\\x97c1, \\xe2\\x83\\x97c2, . . . , \\xe2\\x83\\x97cK}, where\\nK is the number of identities, temperature \\xcf\\x84 ,\\nthreshold T , momentum m, encoder network\\nEq(\\xc2\\xb7), classifier h(\\xc2\\xb7), momentum encoder Ek(\\xc2\\xb7),\\nloss weight \\xce\\xbbpro and \\xce\\xbblgc.\\nfor epoch = 1 : N do\\n\\n{(\\xe2\\x83\\x97xi, yi)}b\\nfor i \\xe2\\x88\\x88 {1, ..., b} do\\n\\ni=1 sampled from data loader.\\n\\n\\xcb\\x9c\\xe2\\x83\\x97xi = aug1(\\xe2\\x83\\x97xi), \\xcb\\x9c\\xe2\\x83\\x97x\\xe2\\x80\\xb2\\ni = aug2(\\xe2\\x83\\x97xi)\\n\\xe2\\x83\\x97qi = Eq(\\xcb\\x9c\\xe2\\x83\\x97xi), \\xe2\\x83\\x97ki = Ek(\\xcb\\x9c\\xe2\\x83\\x97x\\xe2\\x80\\xb2\\ni)\\n\\xe2\\x83\\x97pi = h(\\xe2\\x83\\x97qi)\\nk=1, sk\\ni }K\\n\\xe2\\x83\\x97si = {sk\\n\\xe2\\x83\\x97li = (\\xe2\\x83\\x97pi + \\xe2\\x83\\x97si)/2\\nif epoch > Ns and maxk lj\\n\\n(cid:80)K\\n\\ni = exp( \\xe2\\x83\\x97qi\\xc2\\xb7\\xe2\\x83\\x97ck/\\xcf\\x84 )\\n\\nk=1 exp(\\xe2\\x83\\x97qi\\xc2\\xb7\\xe2\\x83\\x97ck/\\xcf\\x84 )\\n\\ni > T then\\n\\n\\xcb\\x86yi = arg maxj\\n\\n\\xe2\\x83\\x97lj\\ni\\n\\nelse\\n\\n\\xcb\\x86yi = \\xe2\\x83\\x97yi\\n\\nLi\\nLi\\nLi\\n\\nce = \\xe2\\x88\\x92 log(\\xe2\\x83\\x97pi[\\xcb\\x86yi])\\npro = \\xe2\\x88\\x92log\\nlgc =\\n\\n(cid:80)K\\n\\nexp(\\xe2\\x83\\x97qi\\xc2\\xb7\\xe2\\x83\\x97c \\xcb\\x86yi /\\xcf\\x84 )\\nj=1 exp(\\xe2\\x83\\x97qi\\xc2\\xb7\\xe2\\x83\\x97cj /\\xcf\\x84 )\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n\\xe2\\x83\\x97qi\\xc2\\xb7\\xe2\\x83\\x97k+\\n\\xcf\\x84\\n\\nexp\\n\\n(cid:80)\\nX\\n\\xe2\\x83\\x97qi\\xc2\\xb7\\xe2\\x83\\x97k+\\n\\xcf\\x84\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n\\xe2\\x88\\x921\\n|P(i)| log\\n\\n\\xe2\\x83\\x97qi\\xc2\\xb7\\xe2\\x83\\x97k\\xe2\\x88\\x92\\n\\xcf\\x84\\n\\nexp\\n\\n+(cid:80)\\nexp\\n\\xe2\\x83\\x97k\\xe2\\x88\\x92\\xe2\\x88\\x88N (i)\\n\\n(cid:80)\\n\\xe2\\x83\\x97k+ \\xe2\\x88\\x88P(i)\\n\\xe2\\x83\\x97c\\xcb\\x86yi = m\\xe2\\x83\\x97c\\xcb\\x86yi + (1 \\xe2\\x88\\x92 m)\\xe2\\x83\\x97qi\\nce + \\xce\\xbbproLi\\nLi = Li\\nupdate networks Eq, h to minimize Li\\nupdate networks Eq, h to minimize Li\\nupdate momentum encoder Ek.\\n\\npro + \\xce\\xbblgcLi\\n\\nlgc\\n\\nD. Compare with SYSU30K\\n\\nAs show in Table 9, we compare the pre-trained mod-\\nels between LUPerson-NL and SYSU30K. We pre-train\\nPNL on both LUPerson-NL and SYSU30K for 40 epochs\\nwith same experiment settings for a fair comparison. The\\nperformance of LUPerson-NL pre-training is much better\\nthan SYSU30K pre-training, showing the superiority of our\\nLUPerson-NL, and also suggesting that a large number of\\nimages with limited diversity does not bring more represen-\\ntative representation, but large-scale images with diversity\\ndoes.\\n\\nFigure 6. Scene changing for a specific person in LUPerson-NL.\\n\\nB. Training details\\n\\nDuring training, all\\n\\nimages are resized to 256 \\xc3\\x97\\n128 and pass through the same augmentations veri-\\nfied in [12], which are Random Resized Crop, Hor-\\nizontal Flip, Normalization, Random Gaussian Blur,\\nSpecifi-\\nRandom Gray Scale and Random Erasing.\\ncally, the images are normalized with mean and std of\\n[0.3452, 0.3070, 0.3114], [0.2633, 0.2500, 0.2480], which\\nare calculated from all images in LUPerson-NL. We train\\nour model on 8 \\xc3\\x97 V 100 GPUs for 90 epochs with a batch\\nsize of 1, 536. The initial learning rate is set to 0.4 with\\na step-wise decay by 0.1 for every 40 epochs. The opti-\\nmizer is SGD with a momentum of 0.9 and a weight de-\\ncay of 0.0001. We set the hyper-parameters \\xcf\\x84 = 0.1 and\\nT = 0.8. The momentum m for updating both the momen-\\ntum encoder Ek and the prototypes is set to 0.999. We de-\\nsign a large queue with a size of 65, 536 to increase the oc-\\ncurrence of positive samples for the label guided contrastive\\nlearning. The label correction according to Equation 3 starts\\nfrom the 10-th epoch. The deployment of the label guided\\ncontrastive loss Li\\n\\nlgc starts from the 15-th epoch.\\n\\n\\x0cE.1. Temperature Factor \\xcf\\x84\\n\\nTable 13. Comparison for different pre-training data scale\\n\\nDataset\\nMSMT17\\n\\nLUPerson-NL\\n66.1/84.6\\n\\nSYSU30K\\n55.2/76.7\\n\\nTable 9. Comparison of applying our PNL on both LUPerson-NL\\nand SYSU30K.\\n\\n\\xcf\\x84\\n\\n0.05\\n0.07\\n0.1\\n0.2\\n0.3\\n\\nDukeMTMC\\n\\nMSMT17\\n\\n40%\\n76.1/87.2\\n76.4/87.5\\n77.0/87.3\\n75.8/86.8\\n74.7/86.2\\n\\n100%\\n83.5/91.4\\n83.6/91.4\\n84.3/92.0\\n83.4/91.0\\n82.7/90.6\\n\\n40%\\n49.8/73.3\\n50.7/74.1\\n51.9/74.9\\n50.1/73.7\\n48.6/72.0\\n\\n100%\\n65.4/83.8\\n67.2/85.3\\n68.0/86.0\\n66.4/85.0\\n65.5/83.7\\n\\nTable 10. Performances under different \\xcf\\x84 values on DukeMTMC\\nand MSMT17 with data percentages 40% and 100% under the\\nsmall scale setting. The threshold is set as T = 0.8. The best\\nscores are in bold and the second ones are underlined.\\n\\nE. Hyper Parameter Analysis\\n\\nIn our PNL, there are two key hyper-parameters: tem-\\nperature factor \\xcf\\x84 and the threshold for correction T . Here,\\nwe provide the analysis of these two parameters.\\n\\nTable 10 shows the performance comparison with differ-\\nent \\xcf\\x84 values with a fixed label correction threshold T = 0.8.\\nAs we can see, the setting \\xcf\\x84 = 0.1 achieves the best re-\\nsults on both DukeMTMC and MSMT17, while \\xcf\\x84 = 0.07\\nachieves the second best. When we use a larger \\xcf\\x84 , the\\nperformance drops rapidly. It may be because Re-ID is a\\nmore fine-grained task, larger \\xcf\\x84 will cause smaller inter-\\nclass variations and make positive samples too close to neg-\\native samples. In all the experiments, we set \\xcf\\x84 = 0.1.\\n\\nE.2. Threshold T\\n\\nTable 11 shows the results with different label correc-\\ntion threshold values T . As we can see, the performance\\nis relatively stable to different T values varying in a large\\nrange of 0.6 \\xe2\\x88\\xbc 0.8. However, if T is too small or too large,\\nthe performance drops rapidly. For the former case, labels\\nare easier to be modified, which may cause wrong rectifi-\\ncations, while for the latter case, the label noises become\\nharder to be corrected, which also has consistently negative\\neffects on the performance. In all the experiments, we set\\nT = 0.8.\\n\\nF. Results for stronger backbones\\n\\nWe train our PNL using two stronger backbones\\nResNet101 and ResNet152, and report the results in Ta-\\nble 12. As we can see, the stronger ResNet bring more\\nsuperior performances. These results also outperform the\\n\\nT\\n\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n\\nDukeMTMC\\n\\nMSMT17\\n\\n40%\\n76.1/86.5\\n77.1/87.7\\n77.0/87.5\\n77.0/87.3\\n75.7/86.4\\n\\n100%\\n83.3/91.0\\n84.1/91.6\\n84.0/91.8\\n84.3/92.0\\n83.0/90.8\\n\\n40%\\n51.1/74.6\\n52.3/75.5\\n51.9/75.6\\n51.9/75.0\\n50.9/74.3\\n\\n100%\\n67.5/85.2\\n68.1/85.7\\n68.2/85.8\\n68.0/86.0\\n67.2/85.0\\n\\nTable 11. Performances under different T values on DukeMTMC\\nand MSMT17 with data percentages 40% and 100% under the\\nsmall scale setting. The temperature factor is set as \\xcf\\x84 = 0.1. The\\nbest scores are in bold and the second ones are underlined.\\n\\nArch CUHK03 Market1501 DukeMTMC MSMT17\\n68.0/86.0\\nR50\\n80.4/80.9\\n70.8/87.1\\nR101 80.5/81.2\\n71.6/87.5\\nR152 80.6/81.2\\n\\n84.3/92.0\\n85.5/92.8\\n85.6/92.4\\n\\n91.9/96.6\\n92.5/96.9\\n92.7/96.8\\n\\nTable 12. Results with different ResNet backbones. R50, R101\\nand R152 stand for ResNet50, ResNet101 and ResNet152 respec-\\ntively.\\n\\nScale\\nMSMT17\\n\\n10%\\n57.4/79.2\\n\\n30%\\n62.2/82.1\\n\\n100%\\n68.0/86.0\\n\\nscores reported in Table 8 of our main submission. Most\\nimportantly, we are the FIRST to obtain a mAP score on\\nMSMT17 that is larger than 70 without any post-processing\\nfor convolutional network.\\n\\nG. Pre-training data scales\\n\\nWe study the impact of pre-training data scale. Specif-\\nically, we involve various percentages (10%,30%,100%\\npseudo based) of LUPerson-NL into pre-training and then\\nevaluate the finetuing performance on the target datasets.\\nAs shown in Table 13, the learned representation is much\\nstronger with the increase of the pre-training data scale, in-\\ndicating the necessity of building large-scale dataset, and\\nour LUPerson-NL is very important.\\n\\nH. More results for small-scale and few-shot\\n\\nTo complement Table 3 in the main text, we provide\\nmore detailed results under \\xe2\\x80\\x9csmall scale\\xe2\\x80\\x9d and \\xe2\\x80\\x9cfew shot\\xe2\\x80\\x9d\\nsettings in Table 14. As we can see, our weakly pre-trained\\nmodels are consistently better than other pre-trained mod-\\nels. Our advantage is much larger with less training data,\\nsuggesting the potential practical value of our pre-trained\\nmodels for real-world person ReID applications.\\n\\n\\x0cscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\nscale\\n#id\\n#images\\nIN sup.\\nIN unsup.\\nLUP unsup.\\nLUPws wsp.\\n\\n10%\\n75\\n1,170\\n53.1/76.9\\n58.4/81.7\\n64.6/85.5\\n72.4/88.8\\n\\n10%\\n751\\n1,293\\n21.1/41.8\\n18.6/36.1\\n26.4/47.5\\n42.0/61.6\\n\\n10%\\n70\\n1,670\\n45.1/65.3\\n48.1/66.9\\n53.5/72.0\\n60.6/75.8\\n\\n10%\\n702\\n1,679\\n31.5/47.1\\n32.4/48.0\\n35.8/50.2\\n52.2/64.1\\n\\n10%\\n104\\n3,659\\n23.2/50.2\\n22.6/48.8\\n25.5/51.1\\n28.2/51.1\\n\\n10%\\n1,041\\n3,262\\n14.7/34.1\\n13.2/29.2\\n17.0/36.0\\n24.5/42.7\\n\\n20%\\n150\\n2,643\\n67.7/86.8\\n70.2/89.1\\n76.9/92.1\\n81.7/93.2\\n\\n20%\\n751\\n2,587\\n53.2/75.1\\n56.5/77.5\\n63.5/83.0\\n75.7/89.1\\n\\n20%\\n140\\n3,192\\n58.3/75.4\\n60.6/76.6\\n65.0/78.9\\n70.5/83.3\\n\\n20%\\n702\\n3,321\\n56.2/72.1\\n56.4/72.2\\n61.0/74.9\\n71.7/82.5\\n\\n20%\\n208\\n6,471\\n34.6/64.0\\n32.7/60.9\\n36.0/62.6\\n39.6/63.7\\n\\n20%\\n1,041\\n6,524\\n35.6/61.4\\n33.5/58.6\\n37.4/61.4\\n45.6/67.2\\n\\n30%\\n225\\n3,962\\n75.2/90.8\\n76.6/91.9\\n81.9/93.7\\n85.2/94.2\\n\\n30%\\n751\\n3,880\\n68.1/87.6\\n69.3/87.8\\n78.3/92.1\\n83.7/94.0\\n\\n30%\\n210\\n5,530\\n64.7/80.2\\n65.8/80.2\\n69.4/81.9\\n74.5/86.3\\n\\n30%\\n702\\n4,938\\n65.4/79.8\\n65.3/80.2\\n72.3/83.8\\n77.7/87.9\\n\\n30%\\n312\\n9,787\\n41.9/70.8\\n40.4/68.7\\n44.6/71.4\\n47.7/71.2\\n\\n30%\\n1,041\\n9,786\\n44.5/71.1\\n41.4/67.1\\n49.0/73.6\\n53.2/74.4\\n\\n40%\\n300\\n5,226\\n79.1/92.5\\n80.0/93.0\\n84.1/94.4\\n87.3/95.1\\n\\n50%\\n375\\n6,408\\n81.5/93.5\\n82.0/94.1\\n85.8/94.9\\n88.3/95.5\\n\\n60%\\n450\\n7,814\\n81.5/93.5\\n83.7/94.3\\n87.8/95.8\\n89.6/96.0\\n\\n(a) Market1501 small-scale\\n40%\\n751\\n5,174\\n75.4/90.4\\n78.8/88.3\\n80.3/92.7\\n86.0/94.3\\n\\n50%\\n751\\n6,468\\n80.2/92.8\\n78.3/90.9\\n84.2/93.9\\n88.1/95.2\\n\\n60%\\n751\\n7,758\\n83.0/93.6\\n81.7/93.3\\n86.7/94.7\\n89.8/95.8\\n\\n(b) Market1501 few-shot\\n\\n40%\\n280\\n6,924\\n68.5/83.0\\n69.5/82.9\\n72.8/84.7\\n77.0/87.3\\n\\n50%\\n351\\n8,723\\n71.8/84.6\\n72.5/84.4\\n75.6/86.7\\n78.8/88.3\\n\\n60%\\n421\\n10,197\\n74.1/85.6\\n75.0/86.2\\n77.6/87.1\\n80.5/89.2\\n\\n(c) DukeMTMC small-scale\\n40%\\n702\\n6,599\\n71.0/83.9\\n70.2/83.4\\n75.2/86.8\\n79.3/88.1\\n\\n50%\\n702\\n8,278\\n73.9/85.7\\n73.7/85.1\\n77.7/87.4\\n81.1/89.6\\n\\n60%\\n702\\n9,923\\n75.8/86.6\\n75.8/86.7\\n79.4/88.4\\n82.3/90.2\\n\\n(d) DukeMTMC few-shot\\n40%\\n416\\n13,006\\n46.7/74.5\\n45.1/72.2\\n49.2/74.9\\n51.9/74.9\\n\\n50%\\n520\\n15,917\\n50.3/76.9\\n49.0/75.0\\n53.0/77.7\\n55.5/77.2\\n\\n60%\\n624\\n19,672\\n53.9/79.4\\n52.7/78.0\\n56.4/79.7\\n59.1/80.1\\n\\n(e) MSMT17 small-scale\\n\\n40%\\n1,041\\n13,048\\n52.0/76.9\\n47.7/72.7\\n53.9/78.5\\n58.6/78.8\\n\\n50%\\n1,041\\n16,310\\n56.2/79.5\\n53.3/77.6\\n57.4/80.5\\n62.2/81.0\\n\\n60%\\n1,041\\n19,572\\n58.8/81.7\\n56.5/79.6\\n60.0/82.1\\n64.1/82.6\\n\\n(f) MSMT17 few-shot\\n\\n70%\\n525\\n9,120\\n84.8/94.5\\n85.4/94.5\\n88.8/95.9\\n90.1/96.2\\n\\n70%\\n751\\n9,055\\n84.2/94.0\\n84.4/94.1\\n88.4/95.5\\n90.5/96.3\\n\\n70%\\n491\\n11,939\\n75.5/86.8\\n76.3/86.9\\n78.9/88.2\\n81.6/89.5\\n\\n70%\\n702\\n11,564\\n77.2/87.8\\n77.7/88.2\\n80.8/89.2\\n83.2/91.1\\n\\n70%\\n728\\n22,680\\n56.9/81.2\\n55.7/79.9\\n59.5/81.8\\n61.6/81.8\\n\\n70%\\n1,041\\n22,834\\n60.9/82.8\\n59.1/81.5\\n62.9/83.5\\n65.8/83.8\\n\\n80%\\n600\\n11,417\\n85.9/95.2\\n86.4/95.0\\n89.8/96.2\\n90.9/96.4\\n\\n80%\\n751\\n10,348\\n86.3/94.7\\n86.4/95.0\\n89.8/96.0\\n91.2/96.4\\n\\n80%\\n561\\n13,500\\n76.8/87.3\\n77.4/87.3\\n80.2/89.2\\n82.9/90.6\\n\\n80%\\n702\\n13,201\\n78.3/88.6\\n78.7/88.7\\n81.7/90.3\\n84.0/91.6\\n\\n80%\\n832\\n26,335\\n59.6/82.4\\n58.6/82.0\\n61.9/83.6\\n64.2/83.3\\n\\n80%\\n1,041\\n26,096\\n62.5/84.2\\n60.9/82.3\\n64.2/84.5\\n67.2/84.7\\n\\n90%\\n675\\n11,727\\n86.9/95.2\\n87.4/95.5\\n90.5/96.4\\n91.3/96.4\\n\\n90%\\n751\\n11,642\\n86.7/94.6\\n87.1/95.2\\n90.4/96.3\\n91.6/96.4\\n\\n90%\\n631\\n15,111\\n78.0/88.3\\n78.5/88.7\\n81.1/90.0\\n83.3/91.2\\n\\n90%\\n702\\n14,860\\n79.1/88.8\\n79.4/89.0\\n82.0/90.6\\n84.1/91.3\\n\\n90%\\n936\\n29,529\\n61.9/84.2\\n60.9/83.0\\n63.7/85.0\\n66.1/84.8\\n\\n90%\\n1,041\\n29,358\\n63.4/84.5\\n62.4/83.8\\n65.0/85.1\\n67.4/85.3\\n\\n100%\\n751\\n12,936\\n87.5/95.1\\n88.2/95.3\\n91.0/96.4\\n91.9/96.6\\n\\n100%\\n751\\n12,936\\n87.5/95.1\\n88.2/95.3\\n91.0/96.4\\n91.9/96.6\\n\\n100%\\n702\\n16,522\\n79.4/89.0\\n79.5/89.1\\n82.1/91.0\\n84.3/92.0\\n\\n100%\\n702\\n16,522\\n79.4/89.0\\n79.5/89.1\\n82.1/91.0\\n84.3/92.0\\n\\n100%\\n1,041\\n32,621\\n63.7/85.1\\n62.7/84.3\\n65.7/85.5\\n68.0/86.0\\n\\n100%\\n1,041\\n32,621\\n63.7/85.1\\n62.7/84.3\\n65.7/85.5\\n68.0/86.0\\n\\nTable 14. Performance for small-scale and few-shot setting with MGN method for Market1501, DukeMTMC and MSMT17. \\xe2\\x80\\x9cIN sup.\\xe2\\x80\\x9d and\\n\\xe2\\x80\\x9cIN unsup.\\xe2\\x80\\x9d refer to supervised and unsupervised pre-trained model on ImageNet, \\xe2\\x80\\x9cLUP unsup.\\xe2\\x80\\x9d refers to unsupervised pre-trained model\\non LUPerson, \\xe2\\x80\\x9cLUPws wsp.\\xe2\\x80\\x9c refers to our model pre-trained on LUPerson-WS using WSP. The first number is mAP and second is cmc1.\\n\\n\\x0c', b'Understanding 3D Object Articulation in Internet Videos\\n\\nShengyi Qian\\n\\nLinyi Jin\\n\\nChris Rockwell\\n\\nSiyi Chen\\n\\nDavid F. Fouhey\\n\\nUniversity of Michigan\\n{syqian,jinlinyi,cnris,siyich,fouhey}@umich.edu\\nhttps://jasonqsy.github.io/Articulation3D\\n\\nFigure 1. Given an ordinary video, our system produces a 3D planar representation of the observed articulation. The 3D renderings\\nillustrate how the microwave (in Pink) can be articulated in 3D space. We also show the predicted rotation axis using a Blue arrow.\\n\\nAbstract\\n\\nWe propose to investigate detecting and characterizing\\nthe 3D planar articulation of objects from ordinary RGB\\nvideos. While seemingly easy for humans, this problem\\nposes many challenges for computers. Our approach is\\nbased on a top-down detection system that finds planes that\\ncan be articulated. This approach is followed by optimizing\\nfor a 3D plane that explains a sequence of detected articu-\\nlations. We show that this system can be trained on a com-\\nbination of videos and 3D scan datasets. When tested on\\na dataset of challenging Internet videos and the Charades\\ndataset, our approach obtains strong performance.\\n\\n1. Introduction\\n\\nHow would you make sense of Figure 1? Behind the set\\nof RGB pixels that make up the video is a real 3D trans-\\nformation consisting of a 3D planar door rotating about an\\naxis. The goal of this paper is to give the same ability to\\ncomputers. We focus on planar articulation taking the form\\nof a rotation or translation along an axis. This special case\\nof articulation is ubiquitous in human scenes and under-\\nstanding it lets a system understand objects ranging from\\nrefrigerators and drawers to closets and cabinets. While we\\noften learn about these shapes and articulations with phys-\\nical embodiment [53], we have no difficulty understanding\\nthem from video cues alone, for instance while watching a\\n\\nmovie or seeing another person perform an action. We for-\\nmalize this ability for computers as recognizing and charac-\\nterizing a class-agnostic planar articulation via a 3D planar\\nsegment, articulation type (rotation or translation), 3D ar-\\nticulation axis, and articulation angle.\\n\\nThis problem is beyond the current state of the art in\\nscene understanding since it requires reconciling single\\nimage 3D understanding with dynamic 3D understanding.\\nWhile there has been substantial work on 3D reconstruction\\nfrom a single image [5,10,13,65], including work dedicated\\nto planes [34], these works focus on reconstructing static\\nscenes. On the other hand, while there has been work under-\\nstanding articulation, these works often require the place-\\nment of tags for tracking [37, 43], a complete 3D model or\\ndepth sensor [21, 31, 41], or successful 3D human recon-\\nstruction [69]. Moreover, making progress is challenging\\nbecause of data. Unsupervised approaches based on motion\\nanalysis [44, 54] require something to track, which breaks\\nin realistic data since many human-made articulated ob-\\njects are untextured (e.g., refrigerators) or transparent (e.g.,\\novens). While supervised approaches [31, 40, 41] can per-\\nhaps bypass tracking features, they seemingly require ac-\\ncess to large amounts of RGBD data of interactions. For\\nnow, this data does not exist, and training on synthetic data\\ncan fall short when tested on real data (as our experiments\\nempirically demonstrate).\\n\\nWe overcome these challenges with a learning-based ap-\\nproach that combines both detection and 3D optimization\\n\\n1\\n\\n\\x0cand is trained with supervision from multiple sources (Sec-\\ntion 4). The foundation of our approach is a top-down de-\\ntection approach that recognizes articulation axes and types\\nand 3D planes; this approach\\xe2\\x80\\x99s outputs are processed with\\nan optimization method that seeks to explain the per-frame\\nresults in terms of a single coherent 3D articulation.\\n\\nVia this model, we show that one can build an under-\\nstanding of 3D object dynamics via a mix of 2D supervision\\non Internet videos of objects undergoing articulation as well\\nas 3D supervision on existing 3D datasets that do not de-\\npict articulations. To provide 2D supervision, we introduce\\n(Section 3) a new set of 9447 Creative Commons Internet\\nvideos. These videos depict articulation with a variety of\\nobjects as well as negative samples and come with sparse\\nframe annotations of articulation boxes, axes, and surface\\nnormals that can be used for training and evaluating planar\\narticulation models.\\n\\nOur experiments (Section 5) evaluate how well our ap-\\nproach can recognize and characterize articulation. We\\nevaluate on our new dataset of videos as well as the Cha-\\nrades [51] dataset. We compare with a variety of alter-\\nnate approaches, including bottom-up signals like optical\\nflow [56] and changes in surface normals [4], training on\\nsynthetic data [68], as well as systems that analyze human-\\nobject interaction [69]. Our approach outperforms these\\napproaches on our data, often even when the baselines are\\ngiven access to ground-truth location of articulation.\\n\\nOur primary contributions include:\\n\\n(1) The new task\\nof detecting 3D object articulation on unconstrained ordi-\\nnary RGB videos without requiring RGBD video at train-\\ning time; (2) A dataset of Internet videos, with sparse frame\\nannotations of articulation boxes, axes, and surface normals\\nthat can be used for training and evaluating planar articu-\\nlation models; (3) A top-down detection network and op-\\ntimization to tackle this problem, which has strong perfor-\\nmance on the Internet video dataset and Charades.\\n\\n2. Related Work\\n\\nOur paper proposes to extract 3D models of articulation\\nfrom ordinary RGB videos. This problem lies at the inter-\\nsection of 3D vision, learning from videos, and touches on\\nrobotics applications. We note that there are specialized ap-\\nproaches for understanding general articulation (e.g., non-\\nrigid structure from motion [58]) as well as for understand-\\ning specialized motion models (e.g., for a full human 3D\\nmesh models [73] or quadrupeds [29]) or for understanding\\nmore general transformations [20, 63]. Our work focuses\\non understanding the articulation of general objects whose\\narticulated pieces can be represented by a 3D plane rotating\\nor translating.\\n\\nDue to the ubiquitous nature of articulated objects, the\\ntask of understanding them has long been an interest across\\nall of artificial intelligence. In vision, the understanding of\\n\\nthe motion of rigid objects undergoing transformations was\\none of the early successes of computer vision [25, 57, 62].\\nUnfortunately, these early works rely on reliable motion\\ntracks, which is made difficult by the textureless or re-\\nflective nature of many indoor planes (e.g., refrigerator\\ndoors). Our top-down detector gives 3D planes that can\\nhelp provide correspondence between frames where corre-\\nspondence is challenging.\\n\\nMore recent work in robotics has used the value of 3D\\nand integrated it into their modeling approaches [6,9,39,44,\\n54]; however their approaches often use an RGBD sensor,\\nunlike our use of ordinary RGB sensors. This dependence\\non RGBD has been carried forward to the most recent work\\nthat uses deep learning frameworks [1, 21, 31, 36, 66, 68].\\nIn fact, some methods require full 3D models [41], which\\nis typically unavailable in real world 3D scans. [40] by Mo\\net al. can be run on 2D images as long as the point cloud\\nencoder is replaced with a RGB encoder, but its 2D im-\\nages contain a single object without any background, in-\\nstead of challenging Internet videos. While there has been\\nincreasing amounts of work aimed at virtual articulated ob-\\njects [55, 68], simultaneously achieving scale and quality\\nis challenging. For instance ReplicaCAD [55] has only 92\\nobjects. In contrast, our approach works at test time on stan-\\ndard RGB videos by bringing its own 3D via a learned de-\\ntector [34] trained on RGBD data [7].\\n\\nWhile our outputs are 3D planar regions, our approach\\nis deeply connected to the task of understanding human-\\nobject interactions. In these works [3, 14, 50], the goal is\\nto recognize the relationship between humans and the ob-\\njects they interact with. The interactions that we study are\\ncaused by these humans, and so we use an approach that can\\npredict human-object interactions [50] to help identify the\\ndata we train our systems on. The most related work in this\\narea is [69], which aims to jointly understand dynamic 3D\\nhuman-object interactions in 3D. This work, however as-\\nsumes that the object CAD model is known once the articu-\\nlated object is detected, which we do not need. Our method\\nalso works with articulation videos that are more varied in\\nviewpoint and perspective. A more thorough understand-\\ning of the joint relationship between articulated objects and\\nhuman-object interaction, akin to early work [11, 28], is be-\\nyond the scope of this work, but of future value.\\n\\nWe solve the problem of describing 3D articulation by\\nproducing 3D planar models. This uses advances in 3D\\nfrom a single image.\\nIn particular, we build on PlaneR-\\nCNN [34], which is part of a growing body of works aimed\\nat extracting planes from single images [35, 70, 71]. These\\nplanes have advantages for the articulation reasoning since\\nthey offer a compact representation to track and describe.\\nWhile we use plane recognition, the plane is just one com-\\nponent of our output (along with rotation axes) and we ana-\\nlyze our output in a video with temporal optimization.\\n\\n2\\n\\n\\x0c3. Dataset\\n\\nOne critical component of our approach is accurate 2D\\nannotations of articulation occurring in RGB data. We show\\nthat these 2D annotations can be combined with existing\\nRGBD data and the right method to build systems that un-\\nderstand 3D articulation on video data. We next describe\\nhow we collect a dataset of articulations. Our goals are to\\nhave a large collection of annotations of object box, artic-\\nulation type, and axis. Rather than directly look for exam-\\nples of people articulating objects, we follow the data-first\\napproach of [8, 12, 50, 72], namely to gather data containing\\nmany related activities and then analyze and annotate it.\\n\\nData Collection. Our pipeline generates a set of candidate\\nclips to be annotated from a collection of candidate videos\\nvia an automatic pipeline that aims to eliminate frames that\\nare easy to see as not depicting articulation. We begin with\\ncandidate videos that are found by variants of searches for\\na set of 10 objects among Creative Commons videos on\\nYouTube. Within these videos, we find stationary contin-\\nuous shots in these videos with homographies [16] fit on\\nORB [49] features. Many of these clips cannot depict in-\\nteraction since they do not contain any people or do not\\ncontain the objects of interest. We filter by responses by\\na hand detector [50] trained on 100K+ frames of Internet\\ndata, as well as object detectors trained on COCO [33] and\\nLVIS [15]. These filtering steps maximize the use of an-\\nnotator time by eliminating clear negatives, and generate a\\nlarge number of candidate clips.\\n\\nWith a collection of candidate clips of interest, we then\\nturn to manual annotation. For a given clip, we hire an\\nannotation company to annotate frames sparsely (every 10\\nframes) within the clip. They annotate: (box) a box around\\nthe articulated plane and its type, if it exists; and (axis) the\\nprojection of the articulation axis, framed as a line segmen-\\ntation annotation problem. This results in a set of 19411\\nframes, containing 19411 boxes around articulating planes\\nwith 13508 rotation axes and 2755 translation axes, as well\\nas 39411 negative frames. The number of articulation axes\\nis not equal to the number of boxes, since some articulation\\naxes are outside the image. We provide training, validation,\\nand test splits based on uploader, leading to 7845/601/1001\\nvideos in the train/val/test split. A more complete descrip-\\ntion of our annotation pipeline appears in the supplement.\\n\\nWe collect two additional annotations. For the test set,\\nwe also annotate the surface normal of the plane follow-\\ning [4], so we can evaluate how well our model can learn 3D\\nproperties. To show generalization, we also collect the same\\nannotations except surface normals on the Charades [51]\\ndataset.\\n\\nData Availability and Ethics. Our data consists of videos\\nthat users uploaded publicly and chose to share as Creative\\nCommons data. These do not involve interaction with hu-\\n\\nmans or private data. We filtered obviously offensive con-\\ntent, videos depicting children, and cartoons. Examples\\nappear throughout the paper; screenshots of annotation in-\\nstructions and details appear in the supplement.\\n\\n4. Approach\\n\\nThe goal of our approach is to detect and characterize\\nplanar articulation in an unseen RGB video clip. These\\narticulations are an important special case that are ubiqui-\\ntous in human scenes. As shown in Figure 2, we propose a\\n3D Articulation Detection Network (3DADN) to solve the\\ntask. As output, the 3DADN produces the type of motion\\n(rotation or translation), a bounding box around where the\\nmotion is located, the 2D location of the rotation or transla-\\ntion axis, and the 3D location of the articulated plane. The\\n3DADN\\xe2\\x80\\x99s output is followed by post-processing to find a\\nconsistent explanation over the whole video.\\n\\n4.1. 3D Articulation Detection Network\\n\\nThe 3DADN processes each frame independently.\\nIts\\noutput consists of: a segment mask Mi; plane parameters\\n\\xcf\\x80i = [ni, oi] giving the plane equation \\xcf\\x80T\\ni [x, y, z, \\xe2\\x88\\x921] = 0\\n(where ni is the the plane normal with ||ni||2 = 1 and oi\\nis the plane offset); a projected rotation or translation axis\\nai = [\\xce\\xb8, p] which is the projection of the 3D articulation\\naxis; and articulation type.\\n\\nWe use a top-down approach to detect this representa-\\ntion, which we train on RGB videos that depict articulation\\nwithout 3D information as well as RGBD images with 3D\\ninformation that do not depict articulation. Our backbone is\\na Faster R-CNN [47] style network that first detects bound-\\ning boxes for the articulating objects and classifies them into\\ntwo classes (rotation and translation). These boxes provide\\nROI-pooled features that are passed into detection heads\\nthat predict our outputs (Mi, \\xcf\\x80i, ai). Our heads and losses\\nfor Mi follow the common practice of Mask R-CNN [17].\\nWe describe ai and \\xcf\\x80i below.\\nParameterizing Rotation and Translation Axis. We\\nmodel the projected articulation axis as a 2D line in the\\nimage. This projected axis is the projection of the 3D ar-\\nticulation axis (e.g., the hinge of a door). We describe the\\nprojected axis with the normal form of the line, x cos(\\xce\\xb8) +\\ny sin(\\xce\\xb8) = p where p \\xe2\\x89\\xa5 0 is the distance from the box to\\nthe center and \\xce\\xb8 is the inclination of the normal of the axis\\nin pixel coordinates. Since a translation corresponds to a di-\\nrection/family of lines as opposed to a line, we define p = 0\\nfor translation arbitrarily.\\n\\nThe articulation head contains two independent branches\\nfor predicting the rotation and translation axes. We handle\\nthe circularity of the prediction of \\xce\\xb8 by lifting predictions\\nand ground-truth for the angle to the 2D unit circle; since\\nthe line is 180-degree-ambiguous (i.e., \\xce\\xb8 + \\xcf\\x80 is the same as\\n\\n3\\n\\n\\x0cFigure 2. Overview of our approach. (a) Given an ordinary video clip, we first apply our 3D Articulation Detection Network (3DADN) to\\ndetect 3D planes can be articulated for each frame. (b) We then apply temporal optimization to fit the articulation model. Final results are\\ndemonstrated in both 2D image and 3D rendering.\\n\\n\\xce\\xb8), we predict a 2D vector [sin(2\\xce\\xb8), cos(2\\xce\\xb8)]. The resulting\\nnetwork thus predicts a 3D vector containing \\xce\\xb8 and p, which\\nwe supervise with a L1 loss.\\n\\nParameterizing Plane Parameters. Following a line of\\nwork on predicting planes in images, we use a 3D plane [35]\\nto represent the 3D locations of the articulated objects, since\\nmany common articulated objects like doors, refrigerators,\\nand microwaves can be modeled as planes and because past\\nliterature [22, 23, 34] suggests that R-CNN-style networks\\nare adept at predicting plane representations.\\n\\nA 3D plane is represented by plane parameters \\xcf\\x80i =\\n[ni, oi] giving the plane equation \\xcf\\x80T\\ni [x, y, z, \\xe2\\x88\\x921] = 0 With\\ncamera intrinsics, planes can be recovered in 3D and with a\\nmask, this plane can be converted to a plane segment. Fol-\\nlowing [23, 34], we extend R-CNN by adding a plane head\\nwhich directly regresses the normal of the plane. A depth\\nhead is used to predict depth of the image. The depth is only\\nused to calculate the offset value of the plane. We supervise\\nwith L2 loss for the plane normal regression and L1 loss for\\nthe depth regression.\\n\\nTraining. There is no dataset that is non-synthetic and\\nlarge enough to train a 3DADN directly: the 3DADN needs\\nboth realistic interactions and 3D information. However,\\nwe can train the 3DADN in parts. In the first stage, we train\\nthe backbone, RPN, and axis heads directly on our Inter-\\nnet video training set, which has boxes and axes. We then\\nfreeze the backbone, RPN, and axis heads and fine-tune the\\nmask and plane head on a modified version of ScanNet [7].\\nIn particular, we found that humans often occlude the\\nobjects they articulate and models that had not seen humans\\nin training produced worse qualitative results. We therefore\\ncomposited humans from SURREAL [60] into the scenes.\\nWe randomly sample 98,235 ScanNet images, select a syn-\\nIn\\nthetic human and render it on ScanNet backgrounds.\\n\\ntraining, we do not change the ground truth, pretending\\nthe ground truth plane is partially occluded by humans and\\ntraining our model to identify them.\\n\\nMeanwhile, we found that the order of training the heads\\nwas crucial. Planes in ScanNet [7] are defined geometri-\\ncally, and so unopened doors often merge with walls; sim-\\nilarly, ScanNet [7] does not contain transitional moments\\nduring which planes are articulating. Thus, RPNs trained\\non ScanNet [7] perform poorly on articulation videos. In-\\nstead, it is important to train the RPN on our Internet videos,\\nfreeze the backbone, and only rely on ScanNet to train plane\\nparameters and masks, which are unavailable in Internet\\nvideos. During inference we keep the ScanNet camera since\\nour data does not have camera intrinsics.\\nImplementation Details. Full architectural details of our\\nis im-\\napproach are in the supplemental. Our model\\nplemented using Detectron2 [67]. The backbone uses\\nResNet50-FPN [32] pretrained on COCO [33].\\n\\n4.2. Temporal Optimization\\n\\n, \\xcf\\x80(t)\\ni\\n\\nAfter the 3DADN provides per-frame estimates of artic-\\nulations, we perform temporal optimization to find a sin-\\ngle explanation for the detections across frames. We are\\ngiven a sequence of detections indexed by a time of the form\\n[M(t)\\n, a(t)\\n]. We aim to find a single consistent expla-\\ni\\ni\\nnation for these detections.\\nTracking. Optimizing requires a sequence of planes to op-\\ntimize over. We match box i with the box in the next frame\\naccording to pairwise intersection over union (IoU). Box\\ni at t matches box j = arg maxj\\xe2\\x80\\xb2 IoU(M(t)\\n) at\\ntime t + 1; we then track greedily to get a sequence. We\\nsubsequently drop the subscripts for clarity.\\nArticulation Model Fitting. Given a sequence of detec-\\ntions, we find a consistent explanation via a RANSAC-like\\n\\n, M(t+1)\\nj\\xe2\\x80\\xb2\\n\\ni\\n\\n4\\n\\n][Per-frameDetectionPlaneMaskPlaneParamsArticulationAxisPredictionsVideo ClipPerframeDetection CNN2D boxand axis3D planesand axisTemporal Optimization\\xe2\\x80\\xa6\\xe2\\x80\\xa6Across frames\\x0cInput\\n\\nPred1\\n\\nPred2\\n\\nPred3\\n\\nPred4\\n\\nPred 3D\\n\\nFigure 3. Predictions on Internet videos. For each example, we show the input (left), detected 2D planes and how they will be articulated\\nusing the predicted articulation axes and surface normals (middle). We also show 3D renderings to illustrate how these common objects\\nare articulated in the 3D space (right). The predicted rotation axis is shown as the Blue arrow, and translation axis is the Pink arrow.\\n\\napproach. We begin with a hypothesis of a plane segment\\n\\xcf\\x80 and articulation axis a, which we obtain by selecting out-\\nput on a reference frame. Along with an assumed camera\\nintrinsics K, the plane parameters let us lift the plane seg-\\nment and axis to 3D, producing 3D plane segment \\xce\\xa0 and\\n3D axis A. Then, for each frame t, we solve for an artic-\\nulation degree \\xce\\xb1(t) maximizing the reprojection agreement\\nwith the predicted mask at time t. Let us define the repro-\\njection score as\\n\\nr(\\xce\\xb1, t) = IoU\\n\\n(cid:16)\\n\\n(cid:17)\\nM(t), K [R\\xce\\xb1, t\\xce\\xb1] \\xce\\xa0\\n\\n,\\n\\n(1)\\n\\nwhere RA,\\xce\\xb1 and tA,\\xce\\xb1 are \\xce\\xb1 steps over the rotation and\\ntranslation for axis A. We then solve for \\xce\\xb1(t) by solving\\narg max\\xce\\xb1 r(\\xce\\xb1), which gives a per-frame angle using grid\\nsearch. We detect articulation by calculating how well the\\nrotation degree \\xce\\xb1(t) can be explained as a linear function\\nof t (i.e., that there is constant motion). Since many scenes\\nare not constant motion, we have loose thresholds: we con-\\nsider R2 \\xe2\\x89\\xa5 0.4 and slope k > 0.1 to be an articulation. We\\nexclude hypotheses where all r(\\xce\\xb1(t), t) < 0.5.\\n\\n5. Experiments\\n\\nWe have introduced a method that can infer 3D articu-\\nlation in Section 4. In the experiments, we aim to answer\\nthe following questions: (1) how well can one detect 3D\\narticulating objects from ordinary videos; (2) how well do\\nalternate approaches to the problem do?\\n\\n5.1. Experimental Setup\\n\\nWe first describe the setup of our experiments. Our\\nmethod aims to look at an ordinary RGB video and infer\\ninformation about an articulated plane on a object in 3D,\\nincluding: whether the object is articulating, its extent, and\\nthe projection of its rotation or translation axis. We there-\\nfore evaluate our approach on two challenging datasets, us-\\ning metrics that capture various aspects of a 3D plane artic-\\nulating in 3D.\\n\\nDatasets: We validate our approach on both Internet videos\\n(described in Section 3) and the Charades dataset [52]. We\\nuse Charades for cross-dataset evaluations. We focus on\\nCharades videos that are opening objects (doors, refrigera-\\n\\n5\\n\\n\\x0cFlow+Normal\\n\\nSAPIEN\\n\\nSAPIEN w/ gtbox\\n\\nD3D-HOI\\n\\nOurs\\n\\nGT\\n\\nFigure 4. We compare our approach with four baselines. See detailed discussions in the text. We show translation in Pink and rotation in\\nBlue, except D3D-HOI which uses a different detector.\\n\\ntors, etc.), and annotate 2491 frames across 479 videos; we\\nalso randomly sample 479 negative videos containing 4401\\nnegative frames. Our Charades annotation process is simi-\\nlar to Internet videos, with the exceptions that: we annotate\\nonly rotations as Charades contains few translation articu-\\nlations; and we do not annotate surface normals.\\n\\nEvaluation Criteria: Evaluation of our approach is non-\\ntrivial, since our assumed input (RGB videos) precludes\\nmeasuring outputs quantitatively in 3D. We therefore eval-\\nuate our approach on a series of subsets of the problem. We\\nstress from the start though that these metrics are what can\\nbe measured (due to the use of RGB inputs), as opposed to\\nthe full rich output.\\nArticulation Recognition: We first independently evaluate\\nthe ability to detect whether someone is articulating this ob-\\nject at a point in time. We frame this as a binary prediction\\nproblem. This is surprisingly difficult in real scenes because\\nobjects are typically partially occluded by humans when hu-\\nmans articulate them, and because humans often touch a ar-\\nticulated objects (e.g., cleaning the surface) without open-\\ning it. We use AUROC to measure the performance.\\nArticulation Description: We next evaluate the ability of\\na system to detect the articulated object, corresponding ar-\\nticulation type (rotation/translation), axes, and surface nor-\\nmals. We follow other approaches [23, 30, 42, 45, 59] that\\nreconstruct the scene factored into components and treat it\\nas a 3D detection problem, evaluated using average preci-\\nsion (AP). We define error metrics as follows: (Bounding\\n\\nbox) IoU, thresholded as 0.5. We find the normal COCO\\nAP, which measures IoU up to 0.95, to be too strict be-\\ncause the precise boundaries of articulating parts are often\\noccluded by people and hard to annotate. (Axes) EA-score\\nfrom the semantic line detection literature [74]. This metric\\nhandles a number of edge cases; we use 0.5 as the thresh-\\nold as recommended by [74]. (Surface normal) mean angle\\nerror, thresholded at 30\\xe2\\x97\\xa6, following [10, 64]. A prediction\\nis a true positive only if all errors are lower than our thresh-\\nolds. We calculate the precision-recall curve based on that\\nand report AP for varying combinations of metrics.\\n\\nBaselines: Prior approaches for articulation detection have\\nfocused on robots, synthetic datasets, and real-world RGBD\\nscans. These are different from our setting for two reasons.\\nFirst, videos of people articulating objects show a noisy\\nbackground with a person interacting with and occluding\\nthe object, as opposed to an isolated articulated object in a\\nsimulator. Second, RGB videos do not have depth, which is\\noften a requirement of existing articulation models. For ex-\\nample [31] requires depth, and while they show results on\\nreal-world depth scans, their RGBD scans only contain a\\nstatic object without humans. We propose to compare with\\nthe following methods.\\n\\n3DADN + SAPIEN [68] Data: To test whether we can solve\\nthe problem just by training on synthetic data, we create a\\nsynthetic data-based method where we train our 3DADN\\nsystem on synthetic data. We render a synthetic dataset us-\\n\\n6\\n\\n\\x0cTable 1. We report AUROC for Articulation Recognition, as well as AP for Articulation Description. To separate out difficulties in\\ndetecting articulation and characterizing its parameters, we assist Flow+Normal and 3DADN+SAPIEN with ground truth bounding box\\nand denote it as gtbox. 3DADN+SAPIEN cannot detect most objects without the help of gtbox.\\n\\nMethods\\n\\nFlow [56] + Normal [4]\\nFlow [56] + Normal [4]\\nD3D-HOI [69] Upper Bound\\n3DADN + SAPIEN [68]\\nOurs\\n\\n\\xe2\\x9c\\x97\\n\\xe2\\x9c\\x94\\n\\xe2\\x9c\\x97\\n\\xe2\\x9c\\x94\\n\\xe2\\x9c\\x97\\n\\nRecog.\\ngtbox AUROC bbox\\n\\nRotation\\n\\nTranslation\\n\\nbbox+axis\\n\\nbbox+axis+normal\\n\\nbbox\\n\\nbbox+axis\\n\\nbbox+axis+normal\\n\\n68.5\\n-\\n62.7\\n-\\n76.6\\n\\n7.7\\n-\\n28.8\\n-\\n61.3\\n\\n0.3\\n3.0\\n19.7\\n16.8\\n30.4\\n\\n0.0\\n0.3\\nn/a\\n1.40\\n17.2\\n\\n0.3\\n-\\n4.70\\n-\\n34.0\\n\\n0.0\\n1.4\\n4.7\\n15.1\\n27.1\\n\\n0.0\\n0.7\\nn/a\\n0.40\\n17.9\\n\\ning SAPIEN [68] by randomly sampling and driving 3D\\nobjects. We filtered 1053 objects of 18 categories with\\nmovable planes from PartNet-Mobility Dataset [68], such\\nas doors and laptops. We render frames with the objects\\narticulated, with location parameters picked to give plau-\\nsible scenes, and extract the information needed to train\\n3DADN. Without a background, the detection problem be-\\ncomes trivial, so to mimic real 3D scenes, we blend the\\nrenderings with random ScanNet [7] images as the back-\\nground and render synthetic humans from SURREAL [60].\\nFor fair comparison, we use the same ScanNet+SURREAL\\nimages used to train our system\\xe2\\x80\\x99s plane parameter head.\\nWhen evaluated on SAPIEN data, this approach performs\\nwell and obtains an AP of (bbox) 60.3, (bbox+rot) 64.1,\\n(bbox+rot+normal) 41.0.\\n\\nBottom-up Optical Flow [56] and Surface Normal\\nChange [4] (Flow+Normal): To test whether the data can\\nbe solved by the use of fairly simple cues, we construct\\na baseline that uses Optical Flow [56] (since articulating\\nobjects tend to cause movement) and Surface Normals [4]\\n(since rotating planes change their orientation). Both flow\\nand normals provide a H \\xc3\\x97 W map that can be analyzed.\\nWe also use the output of a human segmentation system [19]\\nthat was trained on multiple datasets and mask normal and\\nflow magnitude maps wherever it improves performance.\\nGiven these maps, we recognize the presence of articulation\\nvia logistic regression on a feature vector consisting of the\\nfraction of pixels above multiple thresholds; we recognize\\nbounding boxes via thresholding and finding the tightest en-\\nclosing box; we estimate rotation axis as perpendicular to\\nthe mean flow change in the bounding box (flow tends to\\nincrease away from hinges); we find translation axis using\\nmean flow direction in the box; we find articulation normal\\nusing mean predicted normals in the box. Throughout, we\\nuse the optimal option of surface normals and flow; this hy-\\nbrid system performs substantially better than either flow or\\nnormals alone.\\n\\nBaselines with + GT Box: To separate out difficulties in\\ndetecting articulation and characterizing its parameters, we\\nalso experiment with giving baselines ground-truth bound-\\ning box information about the articulating object. This gives\\n\\nan upper-bound on performance.\\n\\nD3D-HOI [69] Upper Bound: We compare with D3D-\\nHOI since it accepts RGB video as input and detects how\\nhumans articulate objects. A direct comparison with D3D-\\nHOI is challenging since it only works when EFT [24] re-\\nconstructs 3D human poses and Pointrend [26] detects the\\nobjects that are assumed to articulate and correct CAD mod-\\nels are chosen for the object. However, EFT does not work\\nwell on the dataset due to truncated or multiple humans on\\nInternet videos [27, 48]. We therefore report upper-bounds\\non the performance. We assume it predicts the ground\\ntruth bounding box, when EFT mask and pseudo ground\\ntruth 2D human segmentation mask [18] has IoU > 0.5 and\\nPointRend [26] produces a mask on articulated objects with\\nconfidence > 0.7.\\nOurs: This is our proposed method. It includes both the per-\\nframe approach described in Section 4.1 and the optimiza-\\ntion approach of Section 4.2. We note that this approach\\nalso produces outputs that are not being quantitatively mea-\\nsured, such as a 3D plane articulating in 3D. These are qual-\\nitatively shown in Figures 1 and 3.\\n\\n5.2. Results\\n\\nWe first show qualitative results in Figure 3. On chal-\\nlenging Internet videos, our approach usually detects and\\nrecovers the 3D articulated plane regardless of categories.\\n\\nIn Figure 4, we compare our approach with four base-\\nlines visually. Flow can occasionally locate articulation\\n(third row), but in most cases, flow is not localized to only\\nthe object articulating (e.g. camera movement, top row).\\nTraining purely on SAPIEN [68] data has difficulty detect-\\ning articulated objects in Internet videos, even if we show\\nall detected objects with confidence score > 0.1. It learns\\nsome information of articulation axes when we assist it with\\nground truth bounding boxes. D3D-HOI [69] relies on both\\nEFT [24] to detect humans and PointRend [26] to detect ob-\\njects. However, EFT has diffculty predicting 3D humans on\\nInternet videos.\\n\\nQuantitative Results. We evaluate the approach quan-\\ntitatively on the three tasks in Table 1. Our approach\\nsubstantially outperforms the alternate methods. While\\n\\n7\\n\\n\\x0cPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nPred\\n\\nGT\\n\\nFigure 5. Qualitative results on Charades dataset. Without fine-\\ntuning on Charades data, our model obtains strong performance\\non detecting and characterizing 3D articulation.\\n\\nTable 2. Evaluation on Charades dataset [52]. We only report rota-\\ntion AP since Charades does not have sufficient translation motion.\\n\\nMethods\\n\\nFlow [56] + Normal [4]\\nFlow [56] + Normal [4]\\nD3D-HOI Upper Bound\\n3DADN + SAPIEN [68]\\nOurs\\n\\nRecog.\\ngtbox AUROC\\n\\nRotation\\n\\nbbox\\n\\nbbox+axis\\n\\n\\xe2\\x9c\\x97\\n\\xe2\\x9c\\x94\\n\\xe2\\x9c\\x97\\n\\xe2\\x9c\\x94\\n\\xe2\\x9c\\x97\\n\\n53.7\\n-\\n55.9\\n-\\n58.4\\n\\n3.1\\n-\\n14.9\\n-\\n12.0\\n\\n0.2\\n4.2\\n13.7\\n1.54\\n12.8\\n\\nstatistically-combined bottom-up cues [4, 56] do better than\\nchance at predicting the presence of an articulation, they\\nare substantially worse than the proposed approach and\\nfail to obtain sensible bounding boxes. Even when given\\nthis method fails to obtain good\\nthe ground-truth box,\\naxes. Due to the frequency of truncated humans in Internet\\nvideos [27, 48], D3D-HOI [69]\\xe2\\x80\\x99s performance upper-bound\\nis substantially lower than our method\\xe2\\x80\\x99s performance. The\\ndetection system when trained on synthetic data from [68]\\nfails on our system; when given a good bounding box, syn-\\nthetic training data obtains reasonable, but inferior numbers\\nand poor accuracy in predicting normals.\\n\\nAblations \\xe2\\x80\\x93 Optimization. Our optimization produces\\nmodest gains in recognition accuracy and axis localization\\nin 2D: It improves recognition AUROC from 74.0 to 76.6,\\nrotation AP from 16.6 to 17.2 and translation AP from 14.3\\nto 17.9. This small gain is understandable because the\\nevaluation is per-frame and the optimization mainly seeks\\nto make the predictions more consistent.\\nIf we quantify\\nthe consistency in the results before and after optimization,\\nwe find that the EAScore [74] between tracked predicted\\nframes increases from 0.69 (before optimization) to 0.96\\n(after optimization).\\n\\n5.3. Generalization Results\\n\\nWe next test our trained models without fine-tuning on\\nCharades [51]. We show results in Figure 5. Our approach\\ntypically generates reasonable estimations. We find that the\\nvideo quality and resolution of Charades is lower relative to\\nour videos, with many dark or blurry videos.\\n\\nWe also show quantitative evaluations in Table 2. Here,\\nour performance is slightly diminished. However, we sub-\\n\\nFigure 6. Typical failure modes.\\n(1) Ambiguity of articulation\\ntype; (2) Axis outside of the frame or ambiguity of articulation\\naxis location due to symmetry; (3) Object has complex motions (a\\nperson moving an object while articulating it; the rotation axis is\\noutside of the articulating surface).\\n\\nstantially outperform the baselines. We are only marginally\\noutperformed by D3D-HOI upper bound, which assumes\\nperfect performance so long as the data can be obtained.\\n\\n5.4. Limitations and Failure Modes\\n\\nWe finally discuss our limitations and typical failure\\nmodes in Figure 6. We find some examples are particu-\\nlarly challenging: (1) Column 1: some images may contain\\nhard examples where the axis types are hard to figure out.\\n(2) Column 2: the axis is outside of the image frame or its\\nlocation is ambiguous due to symmetry or occlusion. (3)\\nColumn 3: the object has complex dynamics or dual axes;\\nfor example, a person moving a laptop while opening it or\\nthe cabinet has multiple joints.\\n\\n6. Conclusion\\n\\nWe have demonstrated our approach\\xe2\\x80\\x99s ability to detect\\nand characterize 3D planar articulation of objects from or-\\ndinary videos. Future work includes combining 3D shape\\nreconstruction with the articulation detection pipeline.\\n\\nOur approach can have positive impacts by helping build\\nsmart robots that are able to understand and manipulate ar-\\nticulated objects. On the other hand, our approach may be\\nuseful for surveillance activities. Moreover, our network is\\ntrained on Internet videos and deep networks may amplify\\nbiases in the data.\\n\\nAcknowledgments This work was supported by the\\nDARPA Machine Common Sense Program and Toyota Re-\\nsearch Institute. Toyota Research Institute (\\xe2\\x80\\x9cTRI\\xe2\\x80\\x9d) provided\\nfunds to assist the authors with their research but this article\\nsolely reflects the opinions and conclusions of its authors\\nand not TRI or any other Toyota entity. We thank Dandan\\nShan, Jiaqi Geng, Sarah Jabbour and Ruiyu Li for their help\\nwith data collection, Mohamed El Banani for his help of\\nblender, Fanbo Xiang for his help of SAPIEN, as well as\\nYichen Yang and Ziyang Chen for their help of Figure 2.\\nWe also thank Justin Johnson, Jiteng Mu, Tiange Luo and\\nMax Smith for helpful discussions.\\n\\n8\\n\\n\\x0cReferences\\n\\n[1] Dhruv Batra, Angel X Chang, Sonia Chernova, Andrew J\\nDavison, Jia Deng, Vladlen Koltun, Sergey Levine, Jiten-\\ndra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Re-\\narrangement: A challenge for embodied ai. arXiv preprint\\narXiv:2011.01975, 2020. 2\\n\\n[2] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\\nHalber, Matthias Niessner, Manolis Savva, Shuran Song,\\nAndy Zeng, and Yinda Zhang. Matterport3d: Learning\\narXiv preprint\\nfrom rgb-d data in indoor environments.\\narXiv:1709.06158, 2017. 12\\n\\n[3] Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and\\nJia Deng. Hico: A benchmark for recognizing human-object\\ninteractions in images. In ICCV, 2015. 2\\n\\n[4] Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima,\\nMax Hamilton, and Jia Deng. Oasis: A large-scale dataset\\nfor single image 3d in the wild. In CVPR, 2020. 2, 3, 7, 8,\\n13\\n\\n[5] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\\nChen, and Silvio Savarese. 3D-R2N2: A unified approach\\nfor single and multi-view 3d object reconstruction. In ECCV,\\n2016. 1\\n\\n[6] Cristina Garcia Cifuentes, Jan Issac, Manuel W\\xc2\\xa8uthrich, Ste-\\nfan Schaal, and Jeannette Bohg. Probabilistic articulated\\nIEEE Robotics\\nreal-time tracking for robot manipulation.\\nand Automation Letters, 2(2):577\\xe2\\x80\\x93584, 2016. 2\\n\\n[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\\nber, Thomas Funkhouser, and Matthias Nie\\xc3\\x9fner. Scannet:\\nRichly-annotated 3d reconstructions of indoor scenes.\\nIn\\nCVPR, 2017. 2, 4, 7, 12, 16\\n\\n[8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\\nSanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide\\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and\\nMichael Wray. Scaling egocentric vision: The epic-kitchens\\ndataset. In ECCV, 2018. 3\\n\\n[9] Karthik Desingh, Shiyang Lu, Anthony Opipari, and\\nOdest Chadwicke Jenkins. Factored pose estimation of ar-\\nticulated objects using efficient nonparametric belief propa-\\ngation. In 2019 International Conference on Robotics and\\nAutomation (ICRA), pages 7221\\xe2\\x80\\x937227. IEEE, 2019. 2\\n[10] David Eigen and Rob Fergus. Predicting depth, surface nor-\\nmals and semantic labels with a common multi-scale convo-\\nlutional architecture. In ICCV, 2015. 1, 6\\n\\n[11] David F. Fouhey, Vincent Delaitre, Abhinav Gupta,\\nAlexei A. Efros, Ivan Laptev, and Josef Sivic. People watch-\\ning: Human actions as a cue for single-view geometry. In\\nECCV, 2012. 2\\n\\n[15] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\\ndataset for large vocabulary instance segmentation. In CVPR,\\n2019. 3, 12\\n\\n[16] R. I. Hartley and A. Zisserman. Multiple View Geometry\\nin Computer Vision. Cambridge University Press, ISBN:\\n0521540518, second edition, 2004. 3, 12\\n\\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll\\xc2\\xb4ar, and Ross Gir-\\n\\nshick. Mask r-cnn. In ICCV, 2017. 3\\n\\n[18] Vladimir\\n\\nIglovikov.\\n\\nBinary segmentation of peo-\\nple. https://github.com/ternaus/people_\\nsegmentation, 2020. 7\\n\\n[19] Vladimir Iglovikov and Alexey Shvets. Ternausnet: U-net\\nwith vgg11 encoder pre-trained on imagenet for image seg-\\nmentation. arXiv preprint arXiv:1801.05746, 2018. 7\\n[20] Phillip Isola, Joseph J. Lim, and Edward H. Adelson. Dis-\\ncovering states and transformations in image collections. In\\nCVPR, 2015. 2\\n\\n[21] Ajinkya Jain, Rudolf Lioutikov, Caleb Chuck, and Scott\\nScrewnet: Category-independent articulation\\nNiekum.\\nmodel estimation from depth images using screw theory.\\narXiv preprint arXiv:2008.10518, 2020. 1, 2\\n\\n[22] Ziyu Jiang, Buyu Liu, Samuel Schulter, Zhangyang Wang,\\nand Manmohan Chandraker. Peek-a-boo: Occlusion reason-\\ning in indoor scenes with plane representations. In CVPR,\\n2020. 4\\n\\n[23] Linyi Jin, Shengyi Qian, Andrew Owens, and David F.\\nFouhey. Planar surface reconstruction from sparse views. In\\nICCV, 2021. 4, 6, 12, 13\\n\\n[24] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-\\nemplar fine-tuning for 3d human model fitting towards\\narXiv preprint\\nin-the-wild 3d human pose estimation.\\narXiv:2004.03686, 2020. 7\\n\\n[25] Ken-ichi Kanatani. Motion segmentation by subspace sepa-\\n\\nration and model selection. In ICCV, 2001. 2\\n\\n[26] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-\\nIn\\n\\nImage segmentation as rendering.\\n\\nshick. Pointrend:\\nCVPR, 2020. 7\\n\\n[27] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,\\nand Michael J Black. Pare: Part attention regressor for 3d\\nhuman body estimation. In ICCV, 2021. 7, 8\\n\\n[28] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh Sax-\\nena. Learning human activities and object affordances from\\nRGB-D videos. The International Journal of Robotics Re-\\nsearch, 32(8):951\\xe2\\x80\\x93970, 2013. 2\\n\\n[29] Nilesh Kulkarni, Abhinav Gupta, David Fouhey, and Shub-\\nham Tulsiani. Articulation-aware canonical surface map-\\nping. In CVPR, 2020. 2\\n\\n[12] David F. Fouhey, Weicheng Kuo, Alexei A. Efros, and Jiten-\\ndra Malik. From lifestyle VLOGs to everyday interactions.\\nIn CVPR, 2018. 3\\n\\n[30] Nilesh Kulkarni, Ishan Misra, Shubham Tulsiani, and Abhi-\\nnav Gupta. 3D-RelNet: Joint object and relational network\\nfor 3D prediction. In ICCV, 2019. 6\\n\\n[13] R. Girdhar, D.F. Fouhey, M. Rodriguez, and A. Gupta.\\nLearning a predictable and generative vector representation\\nfor objects. In ECCV, 2016. 1\\n\\n[31] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn\\nAbbott, and Shuran Song. Category-level articulated object\\npose estimation. In CVPR, 2020. 1, 2, 6\\n\\n[14] Georgia Gkioxari, Ross Girshick, Piotr Dollar, and Kaiming\\nHe. Detecting and recognizing human-object interactions. In\\nCVPR, 2018. 2\\n\\n[32] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyramid\\nnetworks for object detection. In CVPR, 2017. 4\\n\\n9\\n\\n\\x0c[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll\\xc2\\xb4ar, and C. Lawrence\\nZitnick. Microsoft coco: Common objects in context.\\nIn\\nECCV, 2014. 3, 4, 12\\n\\n[34] Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, and\\nJan Kautz. PlaneRCNN: 3D plane detection and reconstruc-\\ntion from a single image. In CVPR, 2019. 1, 2, 4\\n\\n[35] Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, and Ya-\\nsutaka Furukawa. Planenet: Piece-wise planar reconstruc-\\ntion from a single rgb image. In CVPR, 2018. 2, 4\\n\\n[36] Liu Liu, Han Xue, Wenqiang Xu, Haoyuan Fu, and Cewu\\nLu. Towards real-world category-level articulation pose es-\\ntimation. IEEE Transactions on Image Processing, 2022. 2\\n[37] Yizhou Liu, Fusheng Zha, Lining Sun, Jingxuan Li, Man-\\ntian Li, and Xin Wang. Learning articulated constraints from\\na one-shot demonstration for robot manipulation planning.\\nIEEE Access, 7:172584\\xe2\\x80\\x93172596, 2019. 1\\n\\n[38] David G Lowe. Distinctive image features from scale-\\nInternational journal of computer vi-\\n\\ninvariant keypoints.\\nsion, 60(2):91\\xe2\\x80\\x93110, 2004. 12\\n\\n[39] Frank Michel, Alexander Krull,\\n\\nEric Brachmann,\\nMichael Ying Yang, Stefan Gumhold, and Carsten Rother.\\nPose estimation of kinematic chain instances via object\\ncoordinate regression. In BMVC, 2015. 2\\n\\n[40] Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav\\nGupta, and Shubham Tulsiani. Where2act: From pixels to\\nactions for articulated 3d objects. In ICCV, 2021. 1, 2\\n[41] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille,\\nNuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning\\ndisentangled signed distance functions for articulated shape\\nrepresentation. In ICCV, 2021. 1, 2\\n\\n[42] Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian\\nChang, and Jian Jun Zhang. Total3dunderstanding: Joint lay-\\nout, object pose and mesh reconstruction for indoor scenes\\nfrom a single image. In CVPR, 2020. 6\\n\\n[43] Claudia P\\xc2\\xb4erez-D\\xe2\\x80\\x99Arpino and Julie A Shah. C-learn: Learn-\\ning geometric constraints from demonstrations for multi-step\\nmanipulation in shared autonomy. In ICRA, 2017. 1\\n\\n[44] Sudeep Pillai, Matthew R Walter, and Seth Teller. Learning\\narticulated motions from visual demonstration. In RSS, 2014.\\n1, 2\\n\\n[45] Shengyi Qian, Linyi Jin, and David F. Fouhey. Associa-\\ntive3d: Volumetric reconstruction from sparse views.\\nIn\\nECCV, 2020. 6\\n\\n[46] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\\nlor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\\nGkioxari. Accelerating 3d deep learning with pytorch3d.\\narXiv:2007.08501, 2020. 12\\n\\n[47] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. In Advances in neural information pro-\\ncessing systems, pages 91\\xe2\\x80\\x9399, 2015. 3\\n\\n[48] Chris Rockwell and David F Fouhey. Full-body awareness\\n\\nfrom partial observations. In ECCV, 2020. 7, 8\\n\\n[50] Dandan Shan, Jiaqi Geng, Michelle Shu, and David Fouhey.\\nUnderstanding human hands in contact at internet scale. In\\nCVPR, 2020. 2, 3, 12\\n\\n[51] Gunnar A. Sigurdsson, G\\xc2\\xa8ul Varol, Xiaolong Wang, Ali\\nFarhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in\\nhomes: Crowdsourcing data collection for activity under-\\nstanding. In ECCV, 2016. 2, 3, 8\\n\\n[52] Gunnar A Sigurdsson, G\\xc2\\xa8ul Varol, Xiaolong Wang, Ali\\nFarhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in\\nhomes: Crowdsourcing data collection for activity under-\\nstanding. In ECCV, 2016. 5, 8, 12\\n\\n[53] Linda Smith and Michael Gasser. The development of em-\\nbodied cognition: Six lessons from babies. Artificial life,\\n11(1-2):13\\xe2\\x80\\x9329, 2005. 1\\n\\n[54] J\\xc2\\xa8urgen Sturm, Cyrill Stachniss, and Wolfram Burgard. A\\nprobabilistic framework for learning kinematic models of ar-\\nticulated objects. Journal of Artificial Intelligence Research,\\n41:477\\xe2\\x80\\x93526, 2011. 1, 2\\n\\n[55] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans,\\nYili Zhao, John Turner, Noah Maestre, Mustafa Mukadam,\\nDevendra Chaplot, Oleksandr Maksymets, et al. Habitat 2.0:\\nTraining home assistants to rearrange their habitat. arXiv\\npreprint arXiv:2106.14405, 2021. 2\\n\\n[56] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\\n\\ntransforms for optical flow. In ECCV, 2020. 2, 7, 8\\n\\n[57] Carlo Tomasi and Takeo Kanade. Shape and motion from im-\\nage streams under orthography: a factorization method. In-\\nternational journal of computer vision, 9(2):137\\xe2\\x80\\x93154, 1992.\\n2\\n\\n[58] Lorenzo Torresani, Aaron Hertzmann, and Chris Bregler.\\nNonrigid structure-from-motion: Estimating shape and mo-\\ntion with hierarchical priors. IEEE transactions on pattern\\nanalysis and machine intelligence, 30(5):878\\xe2\\x80\\x93892, 2008. 2\\n\\n[59] Shubham Tulsiani, Saurabh Gupta, David F Fouhey,\\nAlexei A Efros, and Jitendra Malik. Factoring shape, pose,\\nand layout from the 2D image of a 3D scene. In CVPR, 2018.\\n6\\n\\n[60] G\\xc2\\xa8ul Varol, Javier Romero, Xavier Martin, Naureen Mah-\\nmood, Michael J. Black, Ivan Laptev, and Cordelia Schmid.\\nLearning from synthetic humans. In CVPR, 2017. 4, 7\\n[61] Gul Varol, Javier Romero, Xavier Martin, Naureen Mah-\\nmood, Michael J Black, Ivan Laptev, and Cordelia Schmid.\\nLearning from synthetic humans. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 109\\xe2\\x80\\x93117, 2017. 16\\n\\n[62] John YA Wang and Edward H Adelson. Representing mov-\\ning images with layers. IEEE transactions on image process-\\ning, 3(5):625\\xe2\\x80\\x93638, 1994. 2\\n\\n[63] Xiaolong Wang, Ali Farhadi, and Abhinav Gupta. Actions \\xcb\\x9c\\n\\ntransformations. In CVPR, 2016. 2\\n\\n[64] Xiaolong Wang, David Fouhey, and Abhinav Gupta. Design-\\ning deep networks for surface normal estimation. In CVPR,\\n2015. 6\\n\\n[49] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary\\nBradski. Orb: An efficient alternative to sift or surf. In ICCV,\\n2011. 3, 12\\n\\n[65] Xiaolong Wang, David F. Fouhey, and Abhinav Gupta. De-\\nsigning deep networks for surface normal estimation.\\nIn\\nCVPR, 2015. 1\\n\\n10\\n\\n\\x0c[66] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qin-\\nping Zhao, and Kai Xu. Shape2motion: Joint analysis of\\nmotion parts and attributes from 3d shapes. In CVPR, 2019.\\n2\\n\\n[67] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\\nLo, and Ross Girshick. Detectron2. https://github.\\ncom/facebookresearch/detectron2, 2019. 4, 12\\n[68] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao\\nZhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,\\nHe Wang, et al. Sapien: A simulated part-based interactive\\nenvironment. In CVPR, 2020. 2, 6, 7, 8, 16\\n\\n[69] Xiang Xu, Hanbyul Joo, Greg Mori, and Manolis Savva.\\ninteractions from\\n\\nD3d-hoi: Dynamic 3d human-object\\nvideos. arXiv preprint arXiv:2108.08420, 2021. 1, 2, 7, 8\\n\\n[70] Fengting Yang and Zihan Zhou. Recovering 3d planes from\\na single image via convolutional neural networks. In ECCV,\\n2018. 2\\n\\n[71] Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, and\\nShenghua Gao. Single-image piece-wise planar 3d recon-\\nstruction via associative embedding. In CVPR, 2019. 2\\n[72] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\\nFrom recognition to cognition: Visual commonsense reason-\\ning. In CVPR, 2019. 3\\n\\n[73] Jason Y. Zhang, Panna Felsen, Angjoo Kanazawa, and Jiten-\\ndra Malik. Predicting 3d human dynamics from video. In\\nICCV, 2019. 2\\n\\n[74] Kai Zhao, Qi Han, Chang-Bin Zhang, Jun Xu, and Ming-\\nMing Cheng. Deep hough transform for semantic line detec-\\ntion. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 2021. 6, 8\\n\\n11\\n\\n\\x0cA. Implementation Details\\n\\nDetector. Our network architecture is shown in Table 3.\\nWe use Detectron2 [67] to implement our articulation de-\\ntection and take the codebase from SparsePlanes [23]. Our\\narticulation head predicts rotation and translation axis sep-\\narately. Each of the branch takes the RoI feature from the\\nbackbone and uses four convolutional layers with 256 chan-\\nnels and two linear layers to regress the axes. The rotation\\nbranch predicts a three dimensional vector and the transla-\\ntion branch predicts a two dimensional vector. While they\\ntrain the model on Matterport3D [2], we start from COCO\\npretraining and train the model on our Internet videos. The\\ntraining is run on a single RTX 2080Ti.\\n\\nTemporal Optimization. For tracking, we use 0.5 as our\\nIoU threshold. For articulation model fitting, we use the\\nScanNet camera intrinsics as our assumed camera intrisics,\\nsince the plane and depth heads of the model is trained on\\nScanNet [7]. We use PyTorch to implement the temporal\\noptimization. For 3D transformations we use PyTorch3D\\n[46] so that it is compatible. The optimization is parallel\\nand runs on 8 GTX 1080Ti gpus of an internal cluster to\\nsave inference time and it can be run on a single gpu.\\n\\nB. Data Collection Pipeline\\n\\nOur semi-automatic data collection consists of an auto-\\nmatic pipeline to download and filter Internet videos to re-\\nmove clear negatives, and a manual annotation to label ar-\\nticulated objects. We also discuss how we filter Charades\\ndataset [52] since it involves slightly different steps.\\n\\nB.1. Filtering Internet videos\\n\\nYoutube queries. We start with 10 initial common articu-\\nlated objects in our daily life: door, laptop, oven, refrigera-\\ntor, washing machine, dishwasher, microwave oven, drawer,\\ncabinet and box. Using the combination of words, we make\\na list of queries for each initial category, e.g. \\xe2\\x80\\x9cbest laundry\\ntips\\xe2\\x80\\x9d, following 100DOH [50]. To improve the number of\\nvideos we can find on the Internet and the diversity of the\\ndataset, we also translate the queries into Chinese, Japanese,\\nHindi and Spanish. We search these queries on Youtube and\\ndownload related Creative Commons videos.\\n\\nConverting videos to shots. Within these videos, we find\\nstationary continuous shots by fitting homographies [16] on\\nORB [49] features. In practice, we find ORB [49] are much\\nfaster and slightly more robust than SIFT [38] features, so\\nit saves a lot of computing time.\\n\\nFiltering videos based on interaction. A lot of station-\\nary shots does not contain any people or objects of interest.\\nWe further filter out video shots based on a hand interac-\\ntion detector trained on 100K+ frames of Internet data [50].\\n\\nIn practice, we find it works well and we believe it is be-\\ncause [50] is also trained on Internet data. For each video\\nshot, we run the hand interaction detector on frames evenly\\nsampled every 1 second. We remove video shots which do\\nnot have hand interactions at all.\\n\\nFiltering categories of interests. We further filter object\\nof interests by an object detector trained on COCO [33] and\\nLVIS [15]. We use the pretrained model of Faster R-CNN\\n(X101-FPN, 3x) from Detectron2 [67]. For categories that\\nCOCO does not have annotation (e.g. washing machines),\\nwe use LVIS since it has much more categories. Especially,\\nLVIS does not have annotations for doors, and we use door-\\nknob instead.\\n\\nB.2. Annotating Articulated Objects\\n\\nFinally, we use annotate articulated objects using crowd-\\nsourcing. We split video shots into 3s clips, where the fps\\nis 30. Therefore, there are 90 frames per clip.\\n\\nWe use Hive 1 as our data annotation platform. We in-\\nclude the screenshot and estimated hourly pay for each step,\\nsince these steps are separate from each other. The hourly\\npay is a rough estimation since we only have limited worker\\nstatistics provided by Hive and we do not manage it our-\\nselves.\\n\\nRecognizing articulated clips. The first step is to judge\\nif the video clips have objects which are being articulated.\\nThis is a binary classification question. We show 9 key\\nframes of the video shot (sample every 10 frames) and ask\\nworkers to classify. The screenshot is shown in Figure 7.\\nWe pay $0.015 for each clip, each clip is annotated by about\\n2 workers (which is managed by Hive based on consensus\\nand out of our control), and we estimate they can annotate\\n8 clips per minute. Therefore, the estimated hourly pay is\\n0.015 \\xc2\\xb7 8 \\xc2\\xb7 60/2 = $3.6.\\n\\nAnnotating bounding boxes of articulated objects. After\\nlabelling positive video clips, we annotate bounding boxes\\nof objects which are being articulated. We also ask workers\\nto specify the object is being rotated or translated. We only\\nannotate on 9 key frames of the video clip, since consecutive\\nframes tend to be similar. The screenshot is shown in Figure\\n8. We pay $0.14 for each clip, each clip is annotated by\\nabout 2 workers, and we estimate they can annotate 1.5 clips\\nper minute. Therefore, the estimated hourly pay is 0.14\\xc2\\xb71.5\\xc2\\xb7\\n60/2 = $6.3.\\n\\nAnnotating rotation axis. For objects which are annotated\\nto be rotated, we annotate their rotation axes. We ask work-\\ners to draw a line to represent the 2D rotation axis. The\\nscreenshot is shown in Figure 9. We pay $0.04 for each\\nclip, each clip is annotated by about 2 workers, and we es-\\n\\n1https://thehive.ai/\\n\\n12\\n\\n\\x0cTable 3. Overall architecture for our proposed network. The backbone, RPN and plane branches are identical to [23]. The RPN predicts a\\nbounding box for each of A anchors in the input feature map. C is the number of categories (here = 2 for rotation and translation). We use\\nclass agnostic mask because the mask head is trained on ScanNet. TConv is a transpose convolution with stride 2. ReLU is used between\\nall Linear, Conv and TConv operations. Depth branch uses Conv and Deconv layers to generate a depthmap with the same resolution as\\nthe input image.\\n\\nIndex\\n\\nInputs Operation\\n\\nInputs\\n(1)\\n(2)\\n\\nInput Image\\nBackbone: ResNet50-FPN\\nRPN\\n(2),(3) RoIAlign\\n\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n(7)\\n(8)\\n(9)\\n(10)\\n\\n(4)\\n(4)\\n(4)\\n(4)\\n(4)\\n(2)\\n\\nBox: 2\\xc3\\x97downsample, Flatten, Linear(7 \\xc3\\x97 7 \\xc3\\x97 256 \\xe2\\x86\\x92 1024), Linear(1024 \\xe2\\x86\\x92 5C)\\nMask: 4 \\xc3\\x97 Conv(256 \\xe2\\x86\\x92 256, 3 \\xc3\\x97 3), TConv(256 \\xe2\\x86\\x92 256, 2 \\xc3\\x97 2, 2), Conv(256 \\xe2\\x86\\x92 1 \\xc3\\x97 1)\\nNormal: 4 \\xc3\\x97 Conv(256 \\xe2\\x86\\x92 256, 3 \\xc3\\x97 3), Linear(14 \\xc3\\x97 14 \\xc3\\x97 256 \\xe2\\x86\\x92 1024), Linear(1024 \\xe2\\x86\\x92 3)\\nRotation Axis: 4 \\xc3\\x97 Conv(256 \\xe2\\x86\\x92 256, 3 \\xc3\\x97 3), Linear(14 \\xc3\\x97 14 \\xc3\\x97 256 \\xe2\\x86\\x92 1024), Linear(1024 \\xe2\\x86\\x92 3)\\nTranslation Axis: 4 \\xc3\\x97 Conv(256 \\xe2\\x86\\x92 256, 3 \\xc3\\x97 3), Linear(14 \\xc3\\x97 14 \\xc3\\x97 256 \\xe2\\x86\\x92 1024), Linear(1024 \\xe2\\x86\\x92 2)\\nDepth\\n\\nOutput shape\\n\\nH \\xc3\\x97 W \\xc3\\x97 3\\nh \\xc3\\x97 w \\xc3\\x97 256\\nh \\xc3\\x97 w \\xc3\\x97 A \\xc3\\x97 4\\n14 \\xc3\\x97 14 \\xc3\\x97 256\\nC \\xc3\\x97 5\\n1 \\xc3\\x97 28 \\xc3\\x97 28\\n1 \\xc3\\x97 3\\nC \\xc3\\x97 3\\nC \\xc3\\x97 2\\nH \\xc3\\x97 W \\xc3\\x97 1\\n\\nFigure 7. The screenshot of recognize articulated clips.\\n\\ntimate they can annotate 4 axes per minute. Therefore, the\\nestimated hourly pay is 0.04 \\xc2\\xb7 4 \\xc2\\xb7 60/2 = $4.8.\\n\\nAnnotating translation axis. For objects which are anno-\\ntated to be translated, we annotate their translation direc-\\ntion. We also ask workers to draw a line to represent the\\n2D rotation direction. However, since translation is only\\nrelated to the angle of the line and does not need the line\\noffset, we draw a circle at the center of the bounding box\\nand ask workers to start there. The screenshot is shown in\\nFigure 10. The estimated hourly pay is $4.8, which is the\\nsame as annotating rotation axis since it is defined as the\\nsame task \\xe2\\x80\\x9cline segment\\xe2\\x80\\x9d on Hive.\\n\\nAnnotating surface normals. All bounding boxes and ar-\\nticulation axes are annotated in 2D. However, in this paper,\\nwe are interested in 3D object articulation. Thus, for the\\ntest set, we also annotate the o annotate the surface normal\\nof the plane following [4], so we can evaluate how well our\\nmodel can learn 3D properties. Since the annotation of sur-\\nface normals are not available on Hive and we only need\\nsurface normals on the test set for evaluation purpose, we\\ndo all surface normals annotations ourselves. The screen-\\nshot is show in Figure 11.\\n\\nFinally, we postprocess the dataset to make sure the\\ndataset does not have offensive content, cartoons, and any\\nvideos depicting children. Since the distribution is unbal-\\n\\n13\\n\\n\\x0cFigure 8. The screenshot of annotating bounding boxes.\\n\\nFigure 9. The screenshot of annotating rotation axes.\\n\\nanced and negative examples are much more than positive\\nones, we only sample a negative clip with hand interaction\\nfrom the same video shot of positive clips.\\n\\nB.3. Annotating Charades Dataset\\n\\nWe test generalization of our method on Charades with-\\nout additional training. Data is prepared and annotated us-\\ning the following process on the original Charades test set.\\n\\nThe Charades test set contains 1863 videos of people\\n\\n14\\n\\n\\x0cFigure 10. The screenshot of annotating translation axes.\\n\\nFigure 11. The screenshot of annotating surface normals.\\n\\nacting out designated scripts. Videos are typically short at\\naround 30 seconds. The dataset contains script informa-\\ntion and additional annotation, which can be used to filter\\nvideos that are highly likely to contain articulation. After\\n\\ninitial filtering, we split filtered videos into clips of a pre-\\ndefined length. These clips are then annotated as to whether\\nthey contain articulation, for articulation bounding box lo-\\ncation (if applicable), and articulation angle (if applicable).\\n\\n15\\n\\n\\x0cFigure 12. Random examples from Sapien renderings.\\n\\nAnnotation is performed using a similar setup to YouTube\\nvia Hive. Because Charades is small, we use all clips which\\nHive workers have labeled as containing articulation.\\nFiltering criteria. Charades contains videos with acting\\ninformation, which we use to perform filtering. Each video\\nhas corresponding categorical actions that can be used to\\nfind dense articulation instances. These categorical actions\\nfall into 157 categories such as \\xe2\\x80\\x9dHolding some clothes\\xe2\\x80\\x9d\\nor \\xe2\\x80\\x9dOpening a bag\\xe2\\x80\\x9d, and are annotated on a one-tenth of\\na second basis. For example, one given video may have\\ntwo corresponding actions \\xe2\\x80\\x9dHolding some clothes\\xe2\\x80\\x9d and\\n\\xe2\\x80\\x9dOpening a bag\\xe2\\x80\\x9d, which correspond to seconds 1.2 - 11.7\\nand 12.3 - 18.3, correspondingly. We selecct video clips\\norresponding to eight categorical actions for our dataset:\\n\\xe2\\x80\\x9dOpening a door\\xe2\\x80\\x9d, \\xe2\\x80\\x9dClosing a door\\xe2\\x80\\x9d, \\xe2\\x80\\x9dOpening a laptop\\xe2\\x80\\x9d,\\n\\xe2\\x80\\x9dClosing a laptop\\xe2\\x80\\x9d, \\xe2\\x80\\x9dOpening a closet/cabinet\\xe2\\x80\\x9d, \\xe2\\x80\\x9dClosing\\na closet/cabinet\\xe2\\x80\\x9d, \\xe2\\x80\\x9dOpening a refrigerator\\xe2\\x80\\x9d, and \\xe2\\x80\\x9dClosing a\\nrefrigerator\\xe2\\x80\\x9d.\\nGathering filtered clips. To find corresponding video\\nclips, we first calculate the middle of each action \\xe2\\x80\\x93 e.g. for\\nthe 1.2 - 11.7 interval this would be 6.45. Next, we select\\na 7.5 seconds both greater than and less than the middle of\\nthe action. This is subject to beginning and ending of video,\\nand we remove overlapping clips. In our example, \\xe2\\x80\\x9dHold-\\ning some clothes\\xe2\\x80\\x9d has a middle of 6.45 seconds, so the clip\\nwould begin at 0 seconds and end at 13.95 seconds. The\\nsecond clip has a middle of 15.3, and cannot overlap with\\nthe first, so would start at 13.95 seconds and end at 22.8.\\nThis totals 649 filtered clips of <= 15 seconds.\\nSplitting video clips. Given a set of video clips which cor-\\nrespond to the selected categorical actions, we next split\\n\\n16\\n\\nclips into 3 second clips to be used for a standardized articu-\\nlation framework. Any clip less than 3 seconds is truncated.\\nThis results in 2232 3-second mini-clips.\\n\\nC. Rendering Sapien data\\n\\nIn our experiments, we test whether the model trained\\non synthetic data transfer to Internet videos using Sapien\\nrenderings [68]. Here we provide additional details about\\nrendering and example images. Examples of our renderings\\nare shown in Figure 12.\\n\\nTo generate these results, we first randomly sam-\\nple 3D objects with articulation. We filtered 1053 ob-\\njects of 18 caterories with movable planes from PartNet-\\nMobility Dataset [68]. 18 categories are: \\xe2\\x80\\x9cBox\\xe2\\x80\\x9d, \\xe2\\x80\\x9cDish-\\nwasher\\xe2\\x80\\x9d, \\xe2\\x80\\x9cDisplay\\xe2\\x80\\x9d, \\xe2\\x80\\x9cDoor\\xe2\\x80\\x9d, \\xe2\\x80\\x9cFoldingChair\\xe2\\x80\\x9d, \\xe2\\x80\\x9cLaptop\\xe2\\x80\\x9d,\\n\\xe2\\x80\\x9cMicrowave\\xe2\\x80\\x9d, \\xe2\\x80\\x9cOven\\xe2\\x80\\x9d, \\xe2\\x80\\x9cPhone\\xe2\\x80\\x9d, \\xe2\\x80\\x9cRefrigerator\\xe2\\x80\\x9d, \\xe2\\x80\\x9cSafe\\xe2\\x80\\x9d,\\n\\xe2\\x80\\x9cStorageFurniture\\xe2\\x80\\x9d, \\xe2\\x80\\x9cSuitcase\\xe2\\x80\\x9d, \\xe2\\x80\\x9cTable\\xe2\\x80\\x9d, \\xe2\\x80\\x9cToilet\\xe2\\x80\\x9d, \\xe2\\x80\\x9cTrash-\\nCan\\xe2\\x80\\x9d, \\xe2\\x80\\x9cWashingMachine\\xe2\\x80\\x9d, and \\xe2\\x80\\x9cWindow\\xe2\\x80\\x9d. They have sig-\\nnificant overlapping with our queries to generate Internet\\nvideos, since these objects are common objects which can\\nbe articulated. We control its rotation or translation and ren-\\nder ground truth depth, surface normal, mask, motion type\\nand 3D rotation axis. The outputs are object articulation\\nvideos without backgrounds.\\n\\nTo mimic real 3D scenes, we blend random backgrounds.\\nOtherwise the detection problem becomes trivial. We use\\nScanNet [7] images with synthetic humans [61] used to train\\nour approach, to ensure the fair comparison.\\n\\n\\x0c', b'Learning Instance-Specific Adaptation\\nfor Cross-Domain Segmentation\\n\\nYuliang Zou1 Zizhao Zhang2 Chun-Liang Li2 Han Zhang3\\nTomas Pfister2 Jia-Bin Huang4\\n\\n1Virginia Tech 2Google Cloud AI\\n3Google Brain 4University of Maryland, College Park\\n\\nn\\n\\ni\\na\\nm\\no\\nd\\n-\\ns\\ns\\no\\nr\\nC\\n\\nn\\no\\ni\\nt\\na\\nt\\nn\\ne\\nm\\ng\\ne\\ns\\n\\nc\\ni\\nt\\nn\\na\\nm\\ne\\nS\\n\\nc\\ni\\nt\\np\\no\\nn\\na\\nP\\n\\n)\\nl\\na\\ne\\nr\\n\\n\\xe2\\x86\\x92\\nn\\ny\\ns\\n(\\n\\n)\\ng\\no\\nf\\n\\n\\xe2\\x86\\x92\\nn\\nu\\ns\\n(\\n\\nTarget domain input\\n\\nPre-trained\\n\\nOurs\\n\\nFig. 1: Cross-domain segmentation. Models trained with the standard recipe on\\nsource domain data perform poorly on unseen target domains. On the contrary, the\\nproposed method significantly improves upon off-the-shelf pre-trained models, without\\naccessing the target domain at training time or parameter optimization at test-time.\\n\\nAbstract. We propose a test-time adaptation method for cross-domain\\nimage segmentation. Our method is simple: Given a new unseen instance\\nat test time, we adapt a pre-trained model by conducting instance-specific\\nBatchNorm (statistics) calibration. Our approach has two core compo-\\nnents. First, we replace the manually designed BatchNorm calibration rule\\nwith a learnable module. Second, we leverage strong data augmentation\\nto simulate random domain shifts for learning the calibration rule. In con-\\ntrast to existing domain adaptation methods, our method does not require\\naccessing the target domain data at training time or conducting compu-\\ntationally expensive test-time model training/optimization. Equipping\\nour method with models trained by standard recipes achieves significant\\nimprovement, comparing favorably with several state-of-the-art domain\\ngeneralization and one-shot unsupervised domain adaptation approaches.\\n\\nPredictionGround truthLossSunny imagesTraining timeTest time\\xf0\\x9d\\x9c\\x83A segmentation modelPredictionImages from other domainsA segmentation model\\x0cCombining our method with the domain generalization methods further\\nimproves performance, reaching a new state of the art.\\n\\n2\\n\\nY. Zou et al.\\n\\n1 Introduction\\n\\nDeep neural networks have shown impressive results in many computer vision\\napplications. However, these models suffer from inevitable performance drops\\nwhen deployed in out-of-distribution environments due to domain shift [3]. For\\nexample, segmentation models trained on sunny images may perform poorly on\\nfoggy or rainy scenes [10]. Improving the cross-domain performance of deep vision\\nmodels has thus received considerable attention in recent years.\\n\\nDomain adaptation. One straightforward approach for reducing the domain\\nshift is to collect diverse labeled data in the target domain of interest for supervised\\nfine-tuning. However, collecting sufficient annotated data in the target domain\\ncould be expensive or infeasible (e.g., in continuously changing environments).\\nThis is particularly challenging for many dense prediction tasks such as image\\nsegmentation as it requires dense (pixel-wise) labels. Unsupervised domain\\nadaptation (UDA) [18,33,53,56] is an alternative route for reducing the domain\\ngap by using unlabeled target data. However, UDA methods require accessing\\ntarget domain data for model training before deployment. Such assumptions may\\nnot hold as we are not able to anticipate what scenarios the model would encounter\\n(e.g., different weather conditions) and therefore cannot collect the unlabeled\\ndata accordingly. One-shot UDA [4,32] relaxes the constraint by requiring only\\none target example for model training. The model can thus use the first example\\nencountered in the unseen target domain as the training example. However, the\\nadaptation procedure often requires thousands of training steps [32], hindering its\\napplicability as a plug-and-play module when deployed on new target domains.\\nThese UDA methods also require access to source data during the adaptation\\nprocess, which may be unrealistic at test time.\\n\\n[1,28,35,63] overcomes the above limitations\\nDomain generalization (DG)\\nby learning invariant representations using multiple source domains to improve\\nmodel robustness on unseen or continuously changing environments. Recent\\napproaches [41,62] relax the constraint by requiring one single source domain\\nonly. However, as Dubey et al. [15] points out, the optimal model learned from\\ntraining domains may be far from being optimal for an unseen target domain.\\n\\nTest-time adaptation approaches have been proposed to tackle exactly the\\nsame problem. These methods can be roughly categorized into two groups: 1)\\noptimizing model parameters at test time with a proxy task [2,51], prediction\\npseudo-label [30], or entropy regularizations [57], and 2) BatchNorm calibra-\\ntion [23,37,48]. These approaches can be applied to update models along with\\nobserving each target test data, thus observing the entire test distribution. Al-\\nternatively, they can be used to create an instance-specific model for each test\\nexample individually. Despite the flexibility, the optimization-based methods all\\nrequire time-consuming backprop computation to update model parameters.\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n3\\n\\nOur work. In this paper, we present a simple test-time adaptation method for\\ncross-domain segmentation (Figure 1). Building upon BatchNorm calibration\\nmethods [37,48], we propose to learn instance-specific calibration rules using\\nstrong data augmentations to simulate various pseudo source domains. Our\\napproach offers several advantages. First, compared with existing work [37,48]\\nwith manually determined calibration rules that require time-consuming grid\\nsearches and may not transfer to different models, our approach is data-driven and\\ninstance-specific. Second, unlike other test-time adaptation methods [2,30,51,57],\\nour work does not involve expensive gradient-based optimization for updating\\nmodel parameters at test time. Third, our method learns to calibrate BatchNorm\\nstatistics with one single instance (i.e., without accessing to a batch of samples).1\\nWe validate our proposed methods on cross-domain semantic and panoptic\\nsegmentation tasks on several benchmarks. Our experiments show a sizable boost\\nover existing adaptation methods.\\nContributions. In summary, we make the following contributions:\\n\\n\\xe2\\x80\\x93 We propose a simple instance-specific test-time adaptation method and show\\nits applicability to off-the-shelf segmentation models containing BatchNorm.\\n\\xe2\\x80\\x93 We conduct a detailed ablation study and analysis to validate our design\\nchoices in semantic segmentation. Applying the optimal configuration to the\\nmore complex panoptic segmentation task leads to promising performance.\\n\\xe2\\x80\\x93 When combined with the models pre-trained by standard recipes, our method\\ncompares favorably with state-of-the-art one-shot UDA methods and domain\\ngeneralizing semantic segmentation methods. Our approach can also be\\ncombined with existing DG methods to improve the performance further.\\n\\n2 Related Work\\n\\nDomain adaptation. Models trained on one (source) domain often suffers from\\na severe performance drop when processing samples from unseen (target) domains.\\nDomain adaptation methods aim to mitigate this issue by adapting a pre-trained\\nmodel using samples from target domains. Unsupervised domain adaptation\\n(UDA) methods show promising results by leveraging unlabeled target data. These\\nUDA techniques include 1) domain invariant learning, 2) generative models, and\\n3) self-training. Domain invariant learning methods learn invariant features for\\nthe source and target domains by imposing an adversarial loss [18,33,53,56],\\nminimizing the domain distribution distance (e.g., MMD) [31,54] or correlation\\ndistance [50]. Applying data augmentation with generative models can also reduce\\ndomain gap using image-to-image translation [5,49,64], style transfer [16], or\\nhybrid methods that integrate with domain invariance learning methods [8,22].\\nSelf-training methods [65,66] select confident/reliable target data predictions and\\nconvert them into pseudo labels. These methods then iterate the fine-tuning and\\npseudo-labeling procedures until convergence.\\n\\n1 As we validated in Table 3, even examples come for the same test distribution,\\n\\nbatch-wise calibration may be sub-optimal when the batch size is small.\\n\\n\\x0c4\\n\\nY. Zou et al.\\n\\nWhile we have observed remarkable progress in UDA, pre-collected target\\ndomain data requirement makes it less practical. Recently, one-shot UDA meth-\\nods [4,32] have been proposed to tackle this problem. Instead of training on\\nmany unlabeled target data, these approaches require only one unlabeled target\\ndata. However, these methods require time-consuming offline training before\\ndeploying on the target domain. In contrast, our proposed method efficiently\\nadapts the model by calibrating BatchNorm on each target example on the fly,\\nwithout offline training on each target domain separately. Models trained with\\nour method can be easily applied to many different unseen domains.\\n\\nDomain generalization. Instead of adapting models to using target domain\\ndata, domain generalization [1,28,35,63] aims to train a model on source domains\\nthat are generalizable to unseen target domains by encouraging the networks to\\nlearn domain-invariant representations. However, these approaches require multi-\\nple source domains for training, which poses additional challenges in (labeled)\\ndata collection and restricts their feasibility in practical usage. To mitigate the\\ndata collection issues, single domain generalization [41] trains models on one\\nsingle source domain only by either exploiting strong data augmentation strate-\\ngies [41,55,62] to diversify the source domain training data, or performing feature\\nwhitening operations or normalization [10,17,24,40] to remove domain-specific\\ninformation during training. Similar to these methods, our method also exploits\\ndata augmentation to diversify one single source domain data. However, instead\\nof enforcing models to learn domain-invariant features, we encourage models to\\ncalibrate BatchNorm on each unseen target data at test time, by training them\\non diverse pseudo domains generated with strong data augmentation strategies\\nin training time. Our proposed method can also complement (single) domain\\ngeneralization approaches to improve the performance further.\\n\\nTest-time adaptation. Depending on the use of online training/optimization,\\ntest-time adaptation methods can be divided into two groups. First, optimization-\\nfree test-time adaptation methods mostly focus on calibrating the running statis-\\ntics inside BatchNorm layers [23,26,36,37,48] because these feature statistics carry\\ndomain-specific information [29]. However, these methods either directly replace\\nthe running statistics with current input batch statistics or mix the running\\nstatistics and current input batch statistics with a pre-defined calibration rule.\\nIn contrast, we propose to learn the instance-specific BatchNorm calibration rule\\nfrom source domain data. Second, test-time optimization methods adapt the\\nmodel parameters using a training objective such as entropy minimization [57],\\npseudo-labeling [30], or self-supervised proxy tasks [11,51]. Our experiments show\\nthat integrating our method with test-time optimization boosts performance.\\n\\n3 Learning Instance-Specific BatchNorm Calibration\\n\\nOur method applies to off-the-shelf pre-trained segmentation models containing\\nBatchNorm layers [25], a reasonable assumption in most modern CNN models.\\nIn section 3.1, we first review BatchNorm and recent test-time calibration tech-\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n5\\n\\nFig. 2: Overview.\\n(a) At training time, we learn the BatchNorm calibration rule\\n(equation 8) by training only the newly initialized parameters on the strongly-augmented\\nsource domain data; (b) At test time, we conduct instance-specific BatchNorm calibration\\nusing the learned calibration rule. Note that our method does not perform test-time\\ntraining or optimization, and thus the model parameters are fixed after training.\\n\\nniques. We then introduce our method (unconditional and conditional BatchNorm\\ncalibration) in section 3.2.\\n\\n3.1 Background\\n\\nA brief review of BatchNorm. BatchNorm has been empirically shown\\nto stabilize model training and improve model convergence speed [58], making\\nit an essential component in most modern CNN models. The inputs to each\\nBatchNorm layer are CNN features x \\xe2\\x88\\x88 RB\\xc3\\x97C\\xc3\\x97H\\xc3\\x97W , where B denotes the batch\\nsize, C denotes the number of feature channels, H \\xc3\\x97 W denotes the spatial size.\\nBatchNorm conducts normalization followed by an affine transformation on the\\ninputs x to get outputs y \\xe2\\x88\\x88 RB\\xc3\\x97C\\xc3\\x97H\\xc3\\x97W\\n\\ny =\\n\\n\\xe2\\x88\\x9a\\n\\n\\xc3\\x97 \\xce\\xb3 + \\xce\\xb2,\\n\\nx \\xe2\\x88\\x92 \\xc2\\xb5\\n\\xcf\\x832 + \\xcf\\xb5\\n\\nwhere \\xcf\\xb5 is a small constant for numerical stability, \\xce\\xb3 \\xe2\\x88\\x88 RC and \\xce\\xb2 \\xe2\\x88\\x88 RC are learn-\\nable parameters. We generalize the tensor operations by assuming broadcasting\\nwhen the dimensions are not exactly the same. Note that the definition of \\xc2\\xb5 and\\n\\xcf\\x832 differs in training and test time. In training time, \\xc2\\xb5 and \\xcf\\x832 are set as the\\ninput batch statistics.2\\n\\nAt test time, \\xc2\\xb5 and \\xcf\\x832 are set to population statistics \\xc2\\xb5pop, \\xcf\\x832\\n\\npop, accumulated\\n\\nin training time using exponential moving averaging\\n\\n\\xc2\\xb5B = mean(x, axis = (B, H, W ))\\n\\xcf\\x832\\nB = var(x, axis = (B, H, W ))\\n\\n\\xc2\\xb5pop,t = (1 \\xe2\\x88\\x92 \\xce\\xb1) \\xc3\\x97 \\xc2\\xb5pop,t\\xe2\\x88\\x921 + \\xce\\xb1 \\xc3\\x97 \\xc2\\xb5B\\npop,t\\xe2\\x88\\x921 + \\xce\\xb1 \\xc3\\x97 \\xcf\\x832\\npop,t = (1 \\xe2\\x88\\x92 \\xce\\xb1) \\xc3\\x97 \\xcf\\x832\\n\\xcf\\x832\\nB\\n\\n2 Following the \\xe2\\x80\\x9cNamedTensor\\xe2\\x80\\x9d practice [44], this computes the statistics over the B,\\n\\nH, W dimensions and return vectors with dimension C.\\n\\n(1)\\n\\n(2)\\n\\n(3)\\n\\n(4)\\n\\n(5)\\n\\nPredictionGround truthSource domain dataLossLearning BatchNorm calibration ruleStrongly-augmentedPredictionTarget domain dataCalibrate BatchNorm w/ the learned rule(a) Training time(b) Test timePre-trained model(parameters fixed)Pre-trained model(parameters fixed)\\x0c6\\n\\nY. Zou et al.\\n\\nwhere \\xce\\xb1 is a scalar called momentum, and the default value is 0.1 (in PyTorch\\nconvention). Note that the population statistics update happens during training\\nin every feed-forward step.\\n\\nManual BatchNorm calibration. Despite the empirical success in in-domain\\ntesting, models with BatchNorm layers suffer from a significant performance\\ndrop when testing on out-of-distribution data. One potential reason is that\\nthe population statistics within the BatchNorm layers carry domain-specific\\ninformation [29], and thus these statistics are not suitable for normalizing inputs\\nfrom a different domain. Recent studies [29,48,57] show that, by calibrating the\\npopulation statistics with input statistics, the cross-domain performance can be\\nsignificantly improved:\\n\\ny =\\n\\nx \\xe2\\x88\\x92 ((1 \\xe2\\x88\\x92 m) \\xc3\\x97 \\xc2\\xb5pop + m \\xc3\\x97 \\xc2\\xb5ins)\\n(cid:113)(cid:0)(1 \\xe2\\x88\\x92 m) \\xc3\\x97 \\xcf\\x832\\n(cid:1) + \\xcf\\xb5\\n\\npop + m \\xc3\\x97 \\xcf\\x832\\n\\nins\\n\\n\\xc3\\x97 \\xce\\xb3 + \\xce\\xb2,\\n\\n(6)\\n\\nwhere m indicates calibration strength, each method has a different empirically\\nspecified value, \\xc2\\xb5ins \\xe2\\x88\\x88 RB\\xc3\\x97C and \\xcf\\x832\\nins \\xe2\\x88\\x88 RB\\xc3\\x97C indicate instance mean and\\nvariance. Note that the above calibration step happens in every BatchNorm layer,\\nand thus the input features in later BatchNorm layers will be increasingly more\\ncalibrated.\\n\\nIn our study (Table 1(a)), we show that simply setting calibration strength m\\nas the default momentum value 0.1 can improve overall cross-domain performance.\\nHowever, we find several potential issues. First, the calibration strength m\\nis specified empirically, requiring a grid search to obtain the optimal value.\\nNevertheless, the optimal value for one setting might not well transfer to other\\nsettings with different pre-trained models or target domains of interest. Second,\\nthe calibration strength m is a scalar. However, different feature channels may\\nencode different semantic information [59]. Therefore, we may use different\\ncalibration strengths for different feature channels. Third, calibrating the mean\\nand variance with the same strength leads to sub-optimal results.\\n\\n3.2 The proposed method\\n\\nLearning to calibrate BatchNorm (InstCal-U). To address the aforemen-\\ntioned issues, we propose to learn the calibration strengths during training instead\\nof manually specifying them at test time. For simplicity, we define the following\\nfunction\\n\\nfc(a, b, m) = (\\xe2\\x83\\x971 \\xe2\\x88\\x92 m) \\xc3\\x97 a + m \\xc3\\x97 b\\n\\nThe proposed calibration and normalization process can thus be written as\\n\\n(7)\\n\\n(8)\\n\\ny =\\n\\nx \\xe2\\x88\\x92 fc (\\xc2\\xb5pop, \\xc2\\xb5ins, m\\xc2\\xb5)\\n(cid:113)\\n(cid:1) + \\xcf\\xb5\\n\\n(cid:0)\\xcf\\x832\\n\\npop, \\xcf\\x832\\n\\nins, m\\xcf\\x83\\n\\nfc\\n\\n\\xc3\\x97 \\xce\\xb3 + \\xce\\xb2,\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n7\\n\\nwhere m\\xc2\\xb5 \\xe2\\x88\\x88 RC and m\\xcf\\x83 \\xe2\\x88\\x88 RC are two learnable parameters. More specifically,\\nwe initialize two learnable parameters m\\xc2\\xb5 and m\\xcf\\x83 with the default momentum\\nvalue 0.1. We provide a detailed PyTorch implementation of this module in the\\nsupplementary material.\\n\\nGiven an off-the-shelf model, we convert all BatchNorm layers into the instance-\\nspecific calibrated format in equation 8. Note that we only train the newly\\ninitialized calibration parameters m\\xc2\\xb5 and m\\xcf\\x83, and we keep the other learnable\\nparameters (including \\xce\\xb3 and \\xce\\xb2) fixed.\\n\\nUsing training data in the source domain, we train parameters m\\xc2\\xb5 and m\\xcf\\x83\\non a diverse set of domains. Our intuition is that, by exposing the model to\\ndiverse (simulated) domains, we implicitly constrain the learnable calibration\\nparameters m\\xc2\\xb5 and m\\xcf\\x83 to be robust and invariant to unseen target domains.\\nHowever, since we only have one single source domain, we need to generate\\nmultiple pseudo domains based on the source domain. Instead of adopting complex\\ngenerative models to generate pseudo domains, we find that applying appropriate\\nstrong data augmentation during training leads to promising results. We explore\\nthree different augmentation strategies: RandAugment [13], AugMix [21], and\\nDeepAugment [20], and empirically find that DeepAugment performs the best\\n(Table 1(b)). The details of augmentations are in supplementary materials.\\nLearning to conditionally calibrate BatchNorm (InstCal-C). While the\\ngoal is to learn the BatchNorm calibration parameters so that the models can\\nadapt to unseen domains at test time, the learnable parameters m\\xc2\\xb5 and m\\xcf\\x83\\nare fixed after training. We propose an optional module to enable conditional\\ncalibration to increase the flexibility.\\n\\nInstead of directly learning the parameters m\\xc2\\xb5 and m\\xcf\\x83, we propose to learn\\na set of parameters m\\xc2\\xb5,i and m\\xcf\\x83,i for mean and variance, respectively. These\\nparameters can be viewed as the basis of calibration rules. We will use two\\nlightweight MLPs (one for each statistic) to predict the coefficients to combine\\nthe basis to get the actual calibration strength for each test example, given the\\nconcatenation of instance and population statistics. Take m\\xc2\\xb5 as an example, the\\ncomputation step can be written as follows\\n\\n{c\\xc2\\xb5,i}K\\n\\n1 = Softmax (g\\xc2\\xb5 (Concat(\\xc2\\xb5pop, \\xc2\\xb5ins)))\\n\\nm\\xc2\\xb5 =\\n\\nc\\xc2\\xb5,im\\xc2\\xb5,i\\n\\nK\\n(cid:88)\\n\\ni\\n\\n(9)\\n\\n(10)\\n\\nwhere c\\xc2\\xb5,i is a scalar, Concat(\\xc2\\xb7) is the channel-wise concatenation operation,\\nand g\\xc2\\xb5(\\xc2\\xb7) is a small 2-layer MLP. The computation of m\\xcf\\x83 is similar. We provide a\\ndetailed PyTorch implementation of this module in the supplementary material.\\nThanks to the redesign, we further increase the learnable instance-specific\\nBatchNorm calibration flexibility by setting the calibration rule to be conditional\\non the input features. As a result, the calibration rule is now dynamically changing\\naccording to different test target samples, while the inference process is still done\\nwithin one forward pass. As shown in section 4, in general, the performance\\nof instance-specific calibration (InstCal-C) improves upon the unconditional\\n\\n\\x0c8\\n\\nY. Zou et al.\\n\\ncalibration (InstCal-U) on synthetic-to-real settings where significant domain\\nshifts exist.\\n\\n4 Experimental Results\\n\\nWe mainly validate and analyze our method using the semantic segmentation\\ntasks. In section 4.8, we also apply the proposed method to panoptic segmentation\\nand observe promising results.\\n\\n4.1 Experimental setup\\n\\nWe conduct experiments on the public semantic segmentation benchmarks:\\nGTA5 [42], SYNTHIA [43], Cityscapes [12], BDD100k [60], Mapillary [38], and\\nWildDash2 [61] datasets. The GTA5 and SYNTHIA datasets are synthetic, while\\nthe others are real-world datasets. For both synthetic datasets, we split the data\\nfollowing Chen et al. [7]. For the WildDash2 dataset, we only evaluate the 19\\nclasses overlapping with Cityscapes and ignore the remaining classes. We evaluate\\nmodel performance using the standard mean intersection-over-union (mIoU)\\nmetric. We provide the implementation details in the supplementary material.\\nWe will release our source code and pre-trained models for reproducibility.\\n\\n4.2 Ablation study\\n\\nWe use GTA5 as the source domain for ablation experiments and Cityscapes\\nas the unseen target domain. We use the DeepLabv2 model with a ResNet-101\\nbackbone.\\n\\nTable 1: Ablation study. We show results from a DeepLabv2 model with a ResNet-\\n101 backbone. We train models on the GTA5 dataset and treat the Cityscapes dataset\\nas the unseen target domain for evaluation.\\n\\n(a) Calibration parameters to learn\\n\\n(b) Different augmentations\\n\\nStrategy\\n\\nmIoU (%)\\n\\nPre-trained\\nm = 0.1, fixed\\nm \\xe2\\x88\\x88 R\\nm \\xe2\\x88\\x88 RC\\nm\\xc2\\xb5 \\xe2\\x88\\x88 RC, m\\xcf\\x83 \\xe2\\x88\\x88 RC\\n\\n35.7\\n40.1\\n\\n39.8\\n41.1\\n41.5\\n\\nAugmentation\\n\\nmIoU (%)\\n\\nDefault\\nAugMix [21]\\nRandAugment [13]\\nDeepAugment [20]\\n\\n35.7\\n35.9\\n37.9\\n31.7\\n\\nAugmentation\\n\\nInstCal-U InstCal-C\\n\\nDefault\\nAugMix [21]\\nRandAugment [13]\\nDeepAugment [20]\\n\\n39.7\\n40.6\\n41.1\\n41.5\\n\\n40.9\\n41.3\\n40.0\\n42.2\\n\\n#basis mIoU (%)\\n\\n2\\n4\\n8\\n16\\n\\n40.9\\n41.6\\n42.2\\n40.7\\n\\n(c) Not enough to pre-train with strong aug.\\n\\n(d) Number of basis for InstCal-C\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n9\\n\\nWhat calibration parameters should we learn? We first conduct experi-\\nments to study what calibration parameters should be learned. As shown in Ta-\\nble 1(a), suppose we directly learn a scalar parameter shared by the mean and\\nvariance. The performance is worse than using a default value of calibration\\nstrength (0.1) to calibrate BatchNorm. Learning a vector parameter works much\\nbetter than a single scalar and outperforms the baseline calibration. Separating\\nthe learned vector for mean and variance leads to further improved performance.\\nWhich data augmentation strategy should we use? As we mentioned\\nin section 3.2, since we only require one source domain for model training, we\\nneed to use strong data augmentation to simulate a diverse set of training domains.\\nIn this experiment, we study the impact of data augmentation methods.\\n\\nTable 1(b) shows that using the default weak augmentation, e.g., random\\nscaling, cropping, the performance is even worse than the default baseline. While\\nRandAugment [13] and AugMix [21] work well for InstCal-U or InstCal-C sepa-\\nrately, these two augmentation strategies do not work well in both variants. Our\\nresults show that DeepAugment [20] achieves the best overall performance. We\\nthus adopt DeepAugment as our default strong data augmentation strategy.\\nIn the\\nPre-training with strong data augmentation is not sufficient.\\nprevious study, we show that the selection of strong data augmentation is critical.\\nOne may wonder if pre-training with strong data augmentation without the\\nproposed adaptation (InstCal-U and InstCal-C) is sufficient for performance\\nimprovement. Table 1(c) shows that pre-training models using strong data\\naugmentations do not achieve the models trained with our proposed adaptation\\nmethods. AugMix [21] and RandAugment [13] can improve the performance over\\nthe baseline with standard weak augmentation, but not as significant as using\\nthem in InstCal-U or InstCal-C. If we directly use DeepAugment [20] for model\\npre-training, the performance even drops significantly. The results suggest that\\nit is necessary to apply strong data augmentation, but we need to use them\\nin the InstCal-U/InstCal-C training stage instead of simply using them during\\npre-training.\\nNumber of basis for conditional calibration. In Table 1(d), we study the\\nimpact of number of basis (K in equation 9) for InstCal-C. Using eight basis\\nleads to the best results among several options.\\n\\n4.3 Comparison with other test-time adaptation methods\\n\\nWe conduct experiments using the DeepLabv2 model with a ResNet-101 back-\\nbone. We first construct a baseline using a default value (m = 0.1) to cali-\\nbrate BatchNorm statistics. We compare with one optimization-based approach\\n(TENT [57]) and two BatchNorm calibration based methods (AdaptiveBN [48]\\nand PT-BN [37]). We use the same protocols to separately conduct test-time\\nadaptation on multiple unseen target domains. (i.e., setting test batch size to 1\\nand adapting to each test example individually).\\n\\nNote that AdaptiveBN [48], PT-BN [37], and our baseline share the same\\nformulation (equation 8) but using different m values. As shown in Table 2,\\n\\n\\x0c10\\n\\nY. Zou et al.\\n\\n(a) Cityscape\\n\\n(b) BDD100k\\n\\n(c) Mapillary\\n\\n(d) WildDash2\\n\\nFig. 3: Manually set calibration strength m. We show results from a DeepLabv2\\nmodel with a ResNet-101 backbone. The source domain is the GTA5 dataset.\\n\\nwhile both the simple baseline and AdaptiveBN [48] show improved results,\\nPT-BN [37] even hurts the pre-trained performance in many cases. TENT [57]\\nalso shows strong results in some of the test settings, but with the price of\\nsignificantly increased computation time. In contrast, the proposed InstCal-U\\nand InstCal-C outperform these test-time adaptation methods in most settings.\\nWe also note that InstCal-U performs better in real-world cross-domain settings,\\nwhile InstCal-C achieves more promising results in synthetic-to-real settings.\\n\\nIn addition to setting the calibration strength to the default value (0.1), we\\nalso experiment with different values. We try from 0.0 to 1.0 with a step size of\\n0.1, and visualize the results in Figure 3. As we can see, using scalar as strength\\nto calibrate BatchNorm is highly sensitive to selecting the values. On the contrary,\\nthe proposed InstCal-U and InstCal-C consistently perform well across unseen\\ntarget domains.\\n\\nTable 2: Generalizing across multiple domains. We show results from DeepLabv2\\nmodels with a ResNet-101 backbone. The baseline uses calibration strength m = 0.1.\\n\\xe2\\x80\\x9cC\\xe2\\x80\\x9d indicates Cityscapes, \\xe2\\x80\\x9cB\\xe2\\x80\\x9d indicates BDD100k, \\xe2\\x80\\x9cM\\xe2\\x80\\x9d indicates Mapillary, and \\xe2\\x80\\x9cW\\xe2\\x80\\x9d\\nindicates WildDash2. The best performance is in bold and the second best is underlined.\\n\\nSource: GTA5\\n\\nSource: Cityscapes\\n\\nMethod\\n\\nC\\n\\nB M W Avg. B M W Avg.\\n\\nPre-trained\\n\\n35.7 32.9 41.1 27.4 34.3 41.2 49.5 33.9 41.5\\n\\nBaseline (m = 0.1) 40.1 37.4 45.3 32.0 38.7 42.8 51.9 37.6 44.1\\n39.0 36.3 44.3 30.8 37.6 43.0 52.4 37.2 44.2\\nAdaptiveBN [48]\\n33.9 34.3 40.5 27.8 34.1 34.4 39.1 28.8 34.1\\nPT-BN [37]\\n38.1 37.8 44.7 32.5 38.3 44.2 52.3 36.8 44.4\\nTENT [57]\\nInstCal-U (Ours)\\n41.5 39.4 46.0 34.4 40.3 45.1 52.2 40.3 45.9\\nInstCal-C (Ours) 42.2 40.2 46.8 35.3 41.1 44.3 51.5 39.3 45.0\\n\\n4.4 Analysis\\n\\nFor the following studies, we use DeepLabv2 models with a ResNet-101 backbone.\\nImprovement on in-domain performance. We test if our models can im-\\nprove the performance for source domain. We do so by evaluating the trained\\n\\n0.00.10.20.30.40.50.60.70.80.91.0Calibration strength3436384042mIoU (%)InstCal-UInstCal-CBaseline (m=0.1)0.00.10.20.30.40.50.60.70.80.91.0Calibration strength3334353637383940mIoU (%)InstCal-UInstCal-CBaseline (m=0.1)0.00.10.20.30.40.50.60.70.80.91.0Calibration strength41424344454647mIoU (%)InstCal-UInstCal-CBaseline (m=0.1)0.00.10.20.30.40.50.60.70.80.91.0Calibration strength272829303132333435mIoU (%)InstCal-UInstCal-CBaseline (m=0.1)\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n11\\n\\nmodel on the test split of the source data. We report in Table 3(a) that our\\nlearned BatchNorm calibration (both unconditional and conditional) still acheive\\nsizable performance gain.\\n\\nTable 3: Analysis. Results are from DeepLabv2 models with a ResNet-101 backbone.\\n\\n(a) In-domain performance\\n\\nMethod\\n\\nGTA5 Cityscapes\\n\\nPre-trained 69.1\\nInstCal-U 70.3\\nInstCal-C 70.5\\n(c) Model calibration\\n\\n66.1\\n66.6\\n66.8\\n\\n(b) Input batch statistics\\n16\\n1\\n\\n8\\n\\n4\\n\\n2\\n\\nBatch size\\n\\nBaseline (m = 1) 40.1 39.8 39.7 39.7 39.6\\n41.5 41.2 40.9 40.8 40.7\\nInstCal-U\\n42.2 41.8 41.5 41.5 41.4\\nInstCal-C\\n(d) Test-time optimization\\n\\nMethod\\n\\nPre-trained\\nTENT [57]\\n\\nInstCal-U\\nInstCal-U + entropy min. [57]\\n\\nInstCal-C\\nInstCal-C + entropy min. [57]\\n\\nmIoU (%)\\n\\n35.7\\n38.1\\n\\n41.5\\n44.1\\n\\n42.2\\n44.2\\n\\nInput batch statistics v.s. instance statistics As mentioned in section 3.2,\\nwe compute input instance statistics for each test example, instead of computing\\nthe batch statistics for mixing statistics across different examples within a mini-\\nbatch. We validate this design choice by replacing instance statistics with batch\\nstatics using different batch sizes during test time. Table 3(b) shows that the\\nperformance drops as we increase the batch size, even though the test examples\\ncome from the same target distribution. We conjecture the performance will\\nworsen if the mini-batch contains test examples from multiple target domains.\\nThus, we stick to using the input instance statistics.\\nImprovement on model calibration. We compute the expected calibration\\nerror (ECE) [19] for the pre-trained model, baseline update (m = 0.1), PT-BN [37],\\nand the proposed InstCal-U/InstCal-C. As shown in Table 3(c), calibrating\\nBatchNorm statistics indeed reduces model calibration error.\\nCompatible with test-time optimization. We incorporate the optimization-\\nbased method, TENT [57], into our methods, by optimizing the prediction entropy\\nat test-time. Following TENT [57], we only optimize the weight \\xce\\xb3 and bias \\xce\\xb2 in\\nBatchNorm layers. Moreover, we conduct instance-specific adaptation. Table 3(d)\\nshows the complementary nature of these two strategies.\\nRunning time. We test the inference speed on Cityscapes on a single V100 GPU.\\nThe pre-trained model takes 39 ms to process each testing sample (1024\\xc3\\x97512\\nresolution). The BatchNorm calibration method [37] induce a 60 ms overhead.\\nOur method increases the inference time by 58 ms (for InstCal-U) and 149 ms\\n(InstCal-C).\\n\\nGTA5 -> CS101214161820ECE (%)Pre-trainedBaseline (m=0.1)PT-BNInstCal-U (Ours)InstCal-C (Ours)\\x0c12\\n\\nY. Zou et al.\\n\\n4.5 Comparison with one-shot unsupervised domain adaptation\\n\\nThis section compares the proposed method with recent state-of-the-art one-\\nshot UDA methods. One-shot UDA methods adapt source domain pre-trained\\nmodels on one single unlabeled target example offline. In contrast, InstCal-\\nU and InstCal-C adapt pre-trained models on the fly on each test example\\nindividually. Conceptually, one-shot UDA methods and InstCal-U/InstCal-C\\nuse the same amount of data for adaptation. However, one-shot UDA methods\\nusually require time-consuming offline training, and thus it is impossible to\\nadapt models on each target example separately. So these methods only adapt\\nthe models using one single unlabeled (training) example and then deploy the\\nadapted model at test-time without adaptation. As shown in Table 4, simply\\naugmenting pre-trained models with InstCal-U/InstCal-C, compares favorably\\nwith recent one-shot UDA methods and even outperforms the state of the arts\\nby a large margin in Synthia\\xe2\\x86\\x92Cityscapes setting.\\n\\nTable 4: Comparison with one-shot unsupervised domain adaptation. All\\nresults are from modified DeepLabv2 models (specific for domain adaptation). The best\\nperformance is in bold and the second best is underlined.\\n\\nGTA5\\xe2\\x86\\x92Cityscapes\\n\\nSynthia\\xe2\\x86\\x92Cityscapes\\n\\nmIoU\\n\\nmIoU (13-class) mIoU (16-class)\\n\\nMethod\\n\\nCLAN [33]\\nAdvEnt [56]\\nCBST [65]\\nOST [4]\\nASM [32]\\n\\nSource-only pre-trained\\n+ InstCal-U (Ours)\\n+ InstCal-C (Ours)\\n\\n37.7\\n36.1\\n37.1\\n42.3\\n43.2\\n\\n36.2\\n42.4\\n42.2\\n\\n40.4\\n39.9\\n38.5\\n42.8\\n40.7\\n\\n36.2\\n43.5\\n44.1\\n\\n-\\n-\\n-\\n-\\n34.6\\n\\n31.6\\n37.7\\n38.1\\n\\n4.6 Comparison with domain generalizing segmentation\\n\\nThis section compares our InstCal-U/InstCal-C with recent domain generalizing\\n(DG) semantic segmentation approaches. We use the DeepLabv3+ model with a\\nResNet-50 backbone. As shown in Table 5, upgrading non-DG pre-trained weak\\nmodels with InstCal-U/InstCal-C compares favorably with these strong domain\\ngeneralizing segmentation methods across different testing settings. Our method\\neven outperforms all the methods except ISW [10] by a large margin.\\n\\nNote that our method and these domain generalizing methods complement\\neach other. Thus, we can also incorporate our methods on top of these domain\\ngeneralizing segmentation methods. As shown in Figure 4, the proposed method\\nconsistently improves the performance of these methods. Our method can even\\nimprove the strong ISW [10] approach and achieve a new state of the art.\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n13\\n\\nTable 5: Comparison with state-of-the-art domain generalizing semantic\\nsegmentation methods. We show results from a DeepLabv3+ model with a ResNet-\\n50 backbone. \\xe2\\x80\\x9cC\\xe2\\x80\\x9d indicates Cityscapes, \\xe2\\x80\\x9cB\\xe2\\x80\\x9d indicates BDD100k, and \\xe2\\x80\\x9cM\\xe2\\x80\\x9d indicates\\nMapillary. The best performance is in bold and the second best is underlined.\\n\\nSource: GTA5\\n\\nSource: Cityscapes\\n\\nMethod\\n\\nC\\n\\nB M Avg. B M Avg.\\n\\nSW [40]\\nIBN-Net [39]\\nIterNorm [24]\\nISW [10]\\n\\n52.2\\n29.9 27.5 29.7 29.0 48.5 55.8\\n52.8\\n33.9 32.3 37.8 34.7 48.6 57.0\\n31.8 32.7 33.9 32.8 49.2 56.3\\n52.8\\n36.6 35.2 40.3 37.4 50.7 58.6 54.7\\n\\n49.3\\nNon-DG pre-trained 29.6 25.7 28.5 27.9 46.1 52.5\\n+ InstCal-U (Ours) 39.8 32.9 38.6 37.1 51.1 58.5 54.8\\n54.1\\n+ InstCal-C (Ours) 40.3 32.9 38.7 37.3 50.5 57.7\\n\\n(a) GTA5\\xe2\\x86\\x92Cityscapes\\n\\n(b) GTA5\\xe2\\x86\\x92BDD100k\\n\\n(c) GTA5\\xe2\\x86\\x92Mapillary\\n\\nFig. 4: Combining domain generalization with our method. In addition to the\\nmodel pre-trained with the standard recipe (non-DG, labeled as \\xe2\\x80\\x9cDefault\\xe2\\x80\\x9d), we choose\\nthree DG methods: SW [40], IBN-Net [39], and ISW [10]. All methods use DeepLabv3+\\nmodels with a ResNet-50 backbone, trained on the GTA5 dataset.\\n\\n4.7 Backbone network agnostic\\n\\nIn previous sections, we have shown the proposed method can improve pre-trained\\nmodel performance on multiple unseen target domains. However, we only conduct\\nexperiments using the ResNet backbones. In this section, we use the DeepLabv3+\\nmodel with ShuffleNetV2 [34] and MobileNetV2 [47] as backbones, to demonstrate\\nour methods is network-agnostic. As shown in Table 6, the proposed methods\\ncan also improve pre-trained models with these backbones by a large margin,\\noutperforming the recent state of the arts.\\n\\n4.8 Panoptic segmentation results\\n\\nIn this section, we directly apply InstCal-U/InstCal-C to an even more challenging\\ntask, panoptic segmentation [27]. We start with the off-the-shelf models from\\nPanoptic-DeepLab [9] and train these models on the Cityscapes dataset. We test\\nthe models on Foggy Cityscapes [46], which inserts synthetic fog into the original\\nCityscapes clear images with three strength levels (0.005, 0.01, and 0.02). We\\nadopt panoptic quality (PQ), mean intersection-over-union (mIoU), and mean\\n\\nDefaultSWIBN-NetISW283032343638404244mIoU (%)Pre-trained+ InstCal-U+ InstCal-CDefaultSWIBN-NetISW2426283032343638mIoU (%)Pre-trained+ InstCal-U+ InstCal-CDefaultSWIBN-NetISW283032343638404244mIoU (%)Pre-trained+ InstCal-U+ InstCal-C\\x0c14\\n\\nY. Zou et al.\\n\\nTable 6: The proposed module is backbone network agnostic. We show results\\nfrom a DeepLabv3+ model with ShuffleNetV2 and MobileNetV2 as backbones. These\\nmodels are trained on the GTA5 dataset. \\xe2\\x80\\x9cC\\xe2\\x80\\x9d indicates Cityscapes, \\xe2\\x80\\x9cB\\xe2\\x80\\x9d indicates\\nBDD100k, and \\xe2\\x80\\x9cM\\xe2\\x80\\x9d indicates Mapillary. The best performance is in bold and the\\nsecond best is underlined.\\n\\nShuffleNetV2\\n\\nMobileNetV2\\n\\nMethod\\n\\nIBN-Net [39]\\nISW [10]\\n\\nC\\n\\nB M Avg. C\\n\\nB M Avg.\\n\\n27.1 31.8 34.9 31.3 30.1 27.7 27.1 28.3\\n31.0 32.1 35.3 32.8 30.9 30.1 30.7 30.6\\n\\nNon-DG pre-trained 25.7 22.1 28.3 25.4 27.1 27.5 27.3 27.3\\n+ InstCal-U (Ours) 35.8 31.1 36.4 34.4 37.2 31.2 34.5 34.3\\n+ InstCal-C (Ours) 35.9 30.8 35.4 34.0 37.8 30.0 33.9 33.9\\n\\naverage precision (mAP) as the evaluation metrics. As shown in Table 7, InstCal-\\nU/InstCal-C greatly improves off-the-shelf Panoptic-DeepLab performance on\\nout-of-distribution foggy scenes by a large margin, validating the proposed method\\nis universally applicable to different image segmentation tasks without further\\ntuning. We also provide visual results in Figure 1 and supplementary material.\\n\\nTable 7: Panoptic segmentation results. We show results on two Panoptic-DeepLab\\nmodel variants (w/ and w/o depthwise separable convolution). The best performance is\\nin bold and the second best is underlined.\\n\\n0.005\\n\\nSynthetic fog strength\\n0.01\\n\\n0.02\\n\\nMethod\\n\\nw/ DSConv PQ mIoU mAP PQ mIoU mAP PQ mIoU mAP\\n\\nPre-trained\\n+ InstCal-U\\n+ InstCal-C\\n\\nPre-trained\\n+ InstCal-U\\n+ InstCal-C\\n\\n\\xc3\\x97\\n\\xc3\\x97\\n\\xc3\\x97\\n\\n\\xe2\\x9c\\x93\\n\\xe2\\x9c\\x93\\n\\xe2\\x9c\\x93\\n\\n53.3 72.2 25.3 45.0 64.9 18.8 32.6 52.8 11.6\\n56.6 75.7 28.9 51.1 71.9 24.3 42.8 64.5 18.5\\n56.6 75.7 28.5 51.2 71.9 24.4 42.4 64.8 18.0\\n\\n53.0 73.2 24.5 45.3 66.5 18.3 33.1 54.8 11.5\\n55.5 76.0 27.2 49.1 71.3 21.8 40.4 63.2 16.2\\n55.6 76.3 27.6 48.9 71.6 22.4 40.9 64.1 16.8\\n\\n5 Discussions\\n\\nThis paper proposes a simple learning-based test-time adaptation method for\\ncross-domain segmentation. The proposed method is learned to perform instance-\\nspecific BatchNorm calibration during training, without time-consuming test-\\ntime parameter optimization. As a result, our method is efficient and effective,\\ndemonstrating competitive performance across multiple cross-domain image\\nsegmentation settings.\\nLimitations. Currently, we conduct calibration for every BatchNorm layers.\\nIt will be interesting to study which layer is more important and thus only\\ncalibrate specific layers to increase inference speed. And it will be interesting\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n15\\n\\nto extend our method to other normalization layers (e.g., LayerNorm for Vision\\nTransformers [14]) and other challenging tasks. We leave these as future work.\\n\\nAcknowledgement\\n\\nWe thank Sayna Ebrahimi, Shih-Yang Su, and Yun-Chun Chen for their valuable\\ncomments. Y. Zou and J.-B. Huang were supported in part by NSF under Grant\\nNo. (#1755785).\\n\\nReferences\\n\\n1. Balaji, Y., Sankaranarayanan, S., Chellappa, R.: Metareg: Towards domain gener-\\n\\nalization using meta-regularization. In: NeurIPS (2018) 2, 4\\n\\n2. Bartler, A., B\\xc2\\xa8uhler, A., Wiewel, F., D\\xc2\\xa8obler, M., Yang, B.: Mt3: Meta test-time\\ntraining for self-supervised test-time adaption. arXiv preprint arXiv:2103.16201\\n(2021) 2, 3\\n\\n3. Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., Vaughan, J.W.: A\\ntheory of learning from different domains. Machine learning 79(1), 151\\xe2\\x80\\x93175 (2010)\\n2\\n\\n4. Benaim, S., Wolf, L.: One-shot unsupervised cross domain translation. In: NeurIPS\\n\\n(2018) 2, 4, 12\\n\\n5. Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D.: Unsupervised\\npixel-level domain adaptation with generative adversarial networks. In: CVPR\\n(2017) 3\\n\\n6. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:\\nSemantic image segmentation with deep convolutional nets, atrous convolution,\\nand fully connected crfs. TPAMI (2017) 18\\n\\n7. Chen, M., Xue, H., Cai, D.: Domain adaptation for semantic segmentation with\\n\\nmaximum squares loss. In: ICCV (2019) 8, 18\\n\\n8. Chen, Y.C., Lin, Y.Y., Yang, M.H., Huang, J.B.: Crdoco: Pixel-level domain transfer\\n\\nwith cross-domain consistency. In: CVPR (2019) 3\\n\\n9. Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.:\\nPanoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic\\nsegmentation. In: CVPR (2020) 13, 19\\n\\n10. Choi, S., Jung, S., Yun, H., Kim, J.T., Kim, S., Choo, J.: Robustnet: Improving\\ndomain generalization in urban-scene segmentation via instance selective whitening.\\nIn: CVPR (2021) 2, 4, 12, 13, 14, 18\\n\\n11. Cohen, T., Shulman, N., Morgenstern, H., Mechrez, R., Farhan, E.: Self-supervised\\ndynamic networks for covariate shift robustness. arXiv preprint arXiv:2006.03952\\n(2020) 4\\n\\n12. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\\nFranke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\\nunderstanding. In: CVPR (2016) 8\\n\\n13. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated\\ndata augmentation with a reduced search space. In: CVPR Workshop (2020) 7, 8,\\n9, 19\\n\\n14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth\\n16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 15\\n\\n\\x0c16\\n\\nY. Zou et al.\\n\\n15. Dubey, A., Ramanathan, V., Pentland, A., Mahajan, D.: Adaptive methods for\\n\\nreal-world domain generalization. In: CVPR (2021) 2\\n\\n16. Dundar, A., Liu, M.Y., Wang, T.C., Zedlewski, J., Kautz, J.: Domain stylization:\\nA strong, simple baseline for synthetic to real image domain adaptation. arXiv\\npreprint arXiv:1807.09384 (2018) 3\\n\\n17. Fan, X., Wang, Q., Ke, J., Yang, F., Gong, B., Zhou, M.: Adversarially adaptive\\n\\nnormalization for single domain generalization. In: CVPR (2021) 4\\n\\n18. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F.,\\nMarchand, M., Lempitsky, V.: Domain-adversarial training of neural networks.\\nJMLR 17(1), 2096\\xe2\\x80\\x932030 (2016) 2, 3\\n\\n19. Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural\\n\\nnetworks. In: ICML (2017) 11\\n\\n20. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai,\\nR., Zhu, T., Parajuli, S., Guo, M., et al.: The many faces of robustness: A critical\\nanalysis of out-of-distribution generalization. In: ICCV (2021) 7, 8, 9, 19\\n\\n21. Hendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.:\\nAugmix: A simple data processing method to improve robustness and uncertainty.\\nIn: ICLR (2020) 7, 8, 9, 19\\n\\n22. Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A., Darrell,\\nT.: Cycada: Cycle-consistent adversarial domain adaptation. In: ICML (2018) 3\\n23. Hu, X., Uzunbas, G., Chen, S., Wang, R., Shah, A., Nevatia, R., Lim, S.N.:\\nMixnorm: Test-time adaptation through online normalization estimation. arXiv\\npreprint arXiv:2110.11478 (2021) 2, 4\\n\\n24. Huang, L., Zhou, Y., Zhu, F., Liu, L., Shao, L.: Iterative normalization: Beyond\\n\\nstandardization towards efficient whitening. In: CVPR (2019) 4, 13\\n\\n25. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\\n\\nreducing internal covariate shift. In: ICML (2015) 4\\n\\n26. Khurana, A., Paul, S., Rai, P., Biswas, S., Aggarwal, G.: Sita: Single image test-time\\n\\nadaptation. arXiv preprint arXiv:2112.02355 (2021) 4\\n\\n27. Kirillov, A., He, K., Girshick, R., Rother, C., Doll\\xc2\\xb4ar, P.: Panoptic segmentation. In:\\n\\nCVPR (2019) 13\\n\\n28. Li, D., Zhang, J., Yang, Y., Liu, C., Song, Y.Z., Hospedales, T.M.: Episodic training\\n\\nfor domain generalization. In: ICCV (2019) 2, 4\\n\\n29. Li, Y., Wang, N., Shi, J., Liu, J., Hou, X.: Revisiting batch normalization for\\n\\npractical domain adaptation. In: ICLR Workshop (2017) 4, 6\\n\\n30. Liang, J., Hu, D., Feng, J.: Do we really need to access the source data? source\\nhypothesis transfer for unsupervised domain adaptation. In: ICML (2020) 2, 3, 4\\n31. Long, M., Cao, Y., Wang, J., Jordan, M.: Learning transferable features with deep\\n\\nadaptation networks. In: ICML (2015) 3\\n\\n32. Luo, Y., Liu, P., Guan, T., Yu, J., Yang, Y.: Adversarial style mining for one-shot\\n\\nunsupervised domain adaptation. In: NeurIPS (2020) 2, 4, 12\\n\\n33. Luo, Y., Zheng, L., Guan, T., Yu, J., Yang, Y.: Taking a closer look at domain\\nshift: Category-level adversaries for semantics consistent domain adaptation. In:\\nCVPR (2019) 2, 3, 12\\n\\n34. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines for\\n\\nefficient cnn architecture design. In: ECCV (2018) 13\\n\\n35. Matsuura, T., Harada, T.: Domain generalization using a mixture of multiple latent\\n\\ndomains. In: AAAI (2020) 2, 4\\n\\n36. Mirza, M.J., Micorek, J., Possegger, H., Bischof, H.: The norm must go on: Dynamic\\nunsupervised domain adaptation by normalization. arXiv preprint arXiv:2112.00463\\n(2021) 4\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n17\\n\\n37. Nado, Z., Padhy, S., Sculley, D., D\\xe2\\x80\\x99Amour, A., Lakshminarayanan, B., Snoek, J.:\\nEvaluating prediction-time batch normalization for robustness under covariate shift.\\narXiv preprint arXiv:2006.10963 (2020) 2, 3, 4, 9, 10, 11\\n\\n38. Neuhold, G., Ollmann, T., Rota Bulo, S., Kontschieder, P.: The mapillary vistas\\n\\ndataset for semantic understanding of street scenes. In: ICCV (2017) 8\\n\\n39. Pan, X., Luo, P., Shi, J., Tang, X.: Two at once: Enhancing learning and general-\\n\\nization capacities via ibn-net. In: ECCV (2018) 13, 14\\n\\n40. Pan, X., Zhan, X., Shi, J., Tang, X., Luo, P.: Switchable whitening for deep\\n\\nrepresentation learning. In: ICCV (2019) 4, 13\\n\\n41. Qiao, F., Zhao, L., Peng, X.: Learning to learn single domain generalization. In:\\n\\nCVPR (2020) 2, 4\\n\\n42. Richter, S.R., Vineet, V., Roth, S., Koltun, V.: Playing for data: Ground truth\\n\\nfrom computer games. In: ECCV (2016) 8\\n\\n43. Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia dataset:\\nA large collection of synthetic images for semantic segmentation of urban scenes.\\nIn: CVPR (2016) 8\\n44. Rush, A.: Tensor\\nNamedTensor 5\\n\\nhttp://nlp.seas.harvard.edu/\\n\\nconsidered\\n\\nharmful,\\n\\n45. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpa-\\nthy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recognition\\nchallenge. IJCV 115(3), 211\\xe2\\x80\\x93252 (2015) 18\\n\\n46. Sakaridis, C., Dai, D., Van Gool, L.: Semantic foggy scene understanding with\\n\\nsynthetic data. IJCV 126(9), 973\\xe2\\x80\\x93992 (Sep 2018) 13\\n\\n47. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted\\n\\nresiduals and linear bottlenecks. In: CVPR (2018) 13\\n\\n48. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., Bethge, M.: Im-\\nproving robustness against common corruptions by covariate shift adaptation. In:\\nNeurIPS (2020) 2, 3, 4, 6, 9, 10\\n\\n49. Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., Webb, R.: Learning\\nfrom simulated and unsupervised images through adversarial training. In: CVPR\\n(2017) 3\\n\\n50. Sun, B., Saenko, K.: Deep coral: Correlation alignment for deep domain adaptation.\\n\\nIn: ECCV (2016) 3\\n\\n51. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., Hardt, M.: Test-time training with\\nself-supervision for generalization under distribution shifts. In: ICML (2020) 2, 3, 4\\n52. Theis, L., Shi, W., Cunningham, A., Husz\\xc2\\xb4ar, F.: Lossy image compression with\\n\\ncompressive autoencoders. In: ICLR (2017) 19\\n\\n53. Tzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial discriminative domain\\n\\nadaptation. In: CVPR (2017) 2, 3\\n\\n54. Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T.: Deep domain confusion:\\nMaximizing for domain invariance. arXiv preprint arXiv:1412.3474 (2014) 3\\n55. Volpi, R., Namkoong, H., Sener, O., Duchi, J.C., Murino, V., Savarese, S.: Gener-\\nalizing to unseen domains via adversarial data augmentation. In: NeurIPS (2018)\\n4\\n\\n56. Vu, T.H., Jain, H., Bucher, M., Cord, M., P\\xc2\\xb4erez, P.: Advent: Adversarial entropy\\nminimization for domain adaptation in semantic segmentation. In: CVPR (2019) 2,\\n3, 12\\n\\n57. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., Darrell, T.: Tent: Fully test-time\\n\\nadaptation by entropy minimization. In: ICLR (2021) 2, 3, 4, 6, 9, 10, 11\\n\\n58. Wu, Y., Johnson, J.: Rethinking\\xe2\\x80\\x9d batch\\xe2\\x80\\x9d in batchnorm. arXiv preprint\\n\\narXiv:2105.07576 (2021) 5\\n\\n\\x0c18\\n\\nY. Zou et al.\\n\\n59. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., Lipson, H.: Understanding neural\\n\\nnetworks through deep visualization. In: ICML Workshop (2014) 6\\n\\n60. Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell,\\nT.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In:\\nCVPR (2020) 8\\n\\n61. Zendel, O., Honauer, K., Murschitz, M., Steininger, D., Dominguez, G.F.: Wilddash\\n\\n- creating hazard-aware benchmarks. In: ECCV (2018) 8\\n\\n62. Zhao, L., Liu, T., Peng, X., Metaxas, D.: Maximum-entropy adversarial data\\naugmentation for improved generalization and robustness. In: NeurIPS (2020) 2, 4\\n63. Zhao, S., Gong, M., Liu, T., Fu, H., Tao, D.: Domain generalization via entropy\\n\\nregularization. In: NeurIPS (2020) 2, 4\\n\\n64. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\\n\\nusing cycle-consistent adversarial networks. In: ICCV (2017) 3\\n\\n65. Zou, Y., Yu, Z., Kumar, B., Wang, J.: Unsupervised domain adaptation for semantic\\n\\nsegmentation via class-balanced self-training. In: ECCV (2018) 3, 12\\n\\n66. Zou, Y., Yu, Z., Liu, X., Kumar, B., Wang, J.: Confidence regularized self-training.\\n\\nIn: ICCV (2019) 3\\n\\nSupplementary Matarial\\n\\nIn this supplementary document, we provide additional experimental results and\\ndetails to complement the main manuscript. First, we provide the implementation\\ndetails of our experiments. Second, we describe the three data augmentation\\nstrategies we explore. Lastly, we show qualitative results of different cross-domain\\nsegmentation settings.\\n\\nA Implementation details\\n\\nWe conduct all the experiments using the PyTorch framework with one single\\nV100 GPU.\\nSemantic segmentation. We start with a DeepLabv2 model [6] with a ResNet-\\n101 backbone pre-trained on ImageNet [45]. We follow Chen et al. [7] to pre-train\\nthe source-only model. In our proposed learning stage, we freeze the model\\nparameters except for the proposed module and train the model with the SGD\\noptimizer with momentum 0.9 and weight decay 5\\xc3\\x9710\\xe2\\x88\\x924. We use a learning rate\\nof 2.5\\xc3\\x9710\\xe2\\x88\\x923 for InstCal-U and a learning rate of 2.5\\xc3\\x9710\\xe2\\x88\\x922 for InstCal-C. We\\nadopt the polynomial learning rate decay schedule as in Chen et al. [7] and set\\nthe total number of training iterations to 80,000. We set the batch size to one.\\nTo compare with recent domain generalizing semantic segmentation methods,\\nwe adopt the implementation and off-the-shelf models from RobustNet [10]. We\\ndo not conduct pre-training and directly train these off-the-shelf models using\\nthe proposed method. For these DeepLabv3+ models, we set the learning rate as\\n2.5\\xc3\\x9710\\xe2\\x88\\x923 and reduce the batch size for single GPU training (i.e., 8 for ResNet-50,\\n4 for ShuffleNetV2 and MobileNetV2). The other training hyper-parameters are\\nkept the same. Please refer to Choi et al. [10] for more details.\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n19\\n\\nPanoptic segmentation. We adopt off-the-shelf models from Panoptic-DeepLab [9],\\nimplemented in the PyTorch Detectron2 codebase. We use a learning rate of\\n2.5\\xc3\\x9710\\xe2\\x88\\x924. We set the batch size to 4. The other training hyper-parameters are\\nkept the same. Please refer to the PyTorch implementation for more details.3\\n\\nB Strong data augmentation strategies\\n\\nWe provide details for the three strong data augmentation strategies we adopted.\\nRandAugment [13]. RandAugment samples m operations from a pre-defined\\nlist of image augmentation operations and composes them to form the final\\naugmentation for each input data. In this paper, we set m = 2, and we only\\nadopt the color-related augmentation operations in the pre-defined list: Identity,\\nAutoContrast, Invert, Equalize, Solarize, Posterize, Color, Brightness, Sharpness.\\nAugMix [21]. AugMix is proposed to improve model robustness and uncertainty\\nestimation. AugMix constructs three augmentation paths with cascaded one, two,\\nand three augmentation operations for each input data. The three augmented\\ndata are then linearly combined, where the combination weights are sampled from\\na Dirichlet distribution. Finally, the original and augmented images are linearly\\ncombined to construct the final augmented image, where the combination weight\\nis sampled from a Beta distribution. We adopt all the augmentation operations,\\nincluding geometric transforms, and we use the original annotation as the ground\\ntruth for the final augmented image. We only use the augmentation but not the\\nJensen-Shannon divergence consistency loss described in Hendrycks et al. [21].\\nDeepAugment [20]. Instead of composing basic image augmentation operators\\nto construct a strong data augmentation, DeepAugment transforms the input\\nimage by feeding it into an image-to-image translation network and randomly\\nperturbing the intermediate feature representations. There are three options\\nprovided in the official implementation. We adopt the CAE [52] variant, which is\\ninitially used for the image compression task.\\n\\nC Qualitative results\\n\\nWe visualize model prediction results in Figure 5 and Figure 6. For cross-domain\\nsemantic segmentation, the proposed method consistently improves over different\\nsettings. For cross-domain panoptic segmentation, we can see that the proposed\\nmethod dramatically improves the background segmentation, such as trees.\\n\\n3 https://github.com/facebookresearch/detectron2/tree/main/projects/\\n\\nPanoptic-DeepLab\\n\\n\\x0c20\\n\\nY. Zou et al.\\n\\nInput\\n\\nGround truth\\n\\nPre-trained\\n\\nInstCal-U\\n\\nInstCal-C\\n\\nFig. 5: Qualitative results of semantic segmentation. Models are trained on the\\nGTA5 dataset. We treat the Cityscapes, BDD100k, Mapillary, and WildDash2 datasets\\nas the unseen target domains.\\n\\n\\x0cLearning Instance-Specific Adaptation for Cross-Domain Segmentation\\n\\n21\\n\\nInput\\n\\nPre-trained\\n\\nInstCal-U\\n\\nInstCal-C\\n\\nFig. 6: Qualitative results of panoptic segmentation. Models are trained on the\\nclean Cityscapes dataset and tested on the Foggy Cityscapes dataset.\\n\\n\\x0c', b'CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic\\nSurface Representation via Neural Homeomorphism\\n\\nJiahui Lei Kostas Daniilidis\\nUniversity of Pennsylvania\\n\\nhttps://www.cis.upenn.edu/\\xcb\\x9cleijh/projects/cadex\\n\\nFigure 1. We model the deformable surface through a learned canonical shape (the middle shape) and factorize the deformation (orange\\narrow) by learnable continuous bijective canonical maps (green bidirectional arrows) that provide the cycle consistency and topology\\npreservation. Visual results are from the test set on dynamic animals, human bodies and articulated objects.\\n\\nAbstract\\n\\nWhile neural representations for static 3D shapes are\\nwidely studied, representations for deformable surfaces\\nare limited to be template-dependent or to lack efficiency.\\nWe introduce Canonical Deformation Coordinate Space\\n(CaDeX), a unified representation of both shape and non-\\nrigid motion. Our key insight is the factorization of the de-\\nformation between frames by continuous bijective canon-\\nical maps (homeomorphisms) and their inverses that go\\nthrough a learned canonical shape. Our novel deforma-\\ntion representation and its implementation are simple, ef-\\nficient, and guarantee cycle consistency, topology preser-\\nvation, and, if needed, volume conservation. Our mod-\\nelling of the learned canonical shapes provides a flexible\\nand stable space for shape prior learning. We demonstrate\\nstate-of-the-art performance in modelling a wide range of\\ndeformable geometries: human bodies, animal bodies, and\\narticulated objects.\\n\\n1. Introduction\\n\\nHumans perceive, interact, and learn in a continuously\\nchanging real world. One of our key perceptual capabil-\\nities is the modeling of a dynamic 3D world. Such geo-\\nmetric intelligence requires sufficiently general neural rep-\\nresentations that can model different dynamic geometries\\nin 4D sequences to facilitate solving robotics [55], com-\\nputer vision [29], and graphics [44] tasks. Unlike the widely\\nstudied 3D neural representations, a dynamic representa-\\ntion has to be able to associate (for example, finding corre-\\n\\nspondence) and aggregate (for example, reconstruction and\\ntexturing) information across the deformation states of the\\nworld. Directly extending a successful static 3D represen-\\ntation (for example, [33]) to each deformed frame leads to\\nlow efficiency [36], and the inability to model the infor-\\nmation flow across frames, which is critical when solving\\nill-posed problems as in [44]. Our desired dynamic rep-\\nresentation needs to simultaneously represent a global sur-\\nface (canonical/reference shape) across all frames and the\\nconsistent deformation (correspondence/flow/motion) be-\\ntween any frame pair (Fig. 1), so that we can recover the dy-\\nnamic geometry by reconstructing only one reference sur-\\nface and generating the rest of the deformed surfaces by us-\\ning the consistent deformation representation as well as as-\\nsociate and aggregate information across frames (Fig. 2A).\\n\\nThe majority of dynamic representations that satisfy the\\nabove desired properties are model-based and rely on para-\\nmetric models for specific categories like human bodies [1,\\n31] (Fig. 2B), faces [4, 30], or hands [47]. On the contrary,\\nrecent model-free methods like the implicit flow [36, 50]\\n(Fig. 2C) apply one universal 4D representation but model\\nthe canonical shape in an ad hoc chosen frame [36, 50] that\\ncomplicates the shape prior. Alternatively, the choice of an\\napproximate mean/neutral shape [58] as the canonical shape\\ncan limit the shape expressibility. Modeling of the defor-\\nmation is done by either MLPs [50, 58] that ignore the real\\nworld deformation properties, or by ODEs [36] that are in-\\nefficient for space deformation, or by an optimized embed-\\nded graph [6] or Atlas [2] that are sequence specific.\\n\\nIn this work, we introduce a novel and general archi-\\n\\n1\\n\\n\\x0ctecture and representation that enable a competitive re-\\nconstruction of every frame and the recovery of consis-\\ntent correspondence across frames. Our approach is rooted\\nin the factorization of deformation (Sec. 3.1).\\nIf we as-\\nsume that the topology does not change during deformation,\\nall deformed surfaces of one instance can be regarded as\\nequivalent through continuous bijective mappings (home-\\nomorphisms). This allows us to factorize the deforma-\\ntion between two frames by the composition of two con-\\ntinuous invertible functions such that one maps the source\\nframe into a common 3D Canonical Deformation Coordi-\\nnate Space (CaDeX) while another maps it back to the des-\\ntination frame. Such a factorization and its implementa-\\ntion (Sec. 3.2) is novel, simple, and efficient (compared to\\nODEs [36]) while it guarantees cycle consistency, topol-\\nogy preservation, and, if necessary, volume conservation\\n(Sec.3.3). The canonical shape embedded in the CaDeX\\ncan be regarded as the representative element, while the\\nassociated invertible mappings that transform between de-\\nformed frames and the CaDeX are the canonical maps.\\nTherefore, we model the reference surface directly in the\\nCaDeX via an implicit field [33] (Sec. 3.4), which can be\\noptimized together with the canonical maps during training.\\nIn summary, our main contributions are: (1) A novel\\ngeneral representation and architecture for dynamic sur-\\nfaces that jointly solve the canonical shape and consistent\\ndeformation problems. (2) Learnable continuous bijective\\ncanonical maps and canonical shapes that jointly factorize\\nthe shape deformation, and are novel, simple, efficient, and\\nguarantee cycle consistency and topology preservation. (3)\\nA novel solution to the dynamic surface reconstruction and\\ncorrespondence tasks given sparse point clouds or depth\\nviews based on the proposed representation. (4) We demon-\\nstrate state-of-the-art performance on modelling different\\ndeformable categories: Human bodies [5], Animals [57]\\nand Articulated Objects [53].\\n\\n2. Related Work\\n\\nProposed neural representations for static 3D geome-\\ntry [9, 11, 17, 18, 20, 26, 33, 34, 37, 40, 43, 56] are promising,\\nbut most of them do not involve modeling of deformations.\\nA few recent approaches represent or process 3D shapes via\\ndeformation [13, 22, 23, 25, 59], but they focus on static 3D\\nshape collections that do not meet the requirements (e.g, ef-\\nficiency) for processing 4D data. We will focus our related\\nwork on dynamic representations of deformable geometry.\\nModel-Based Dynamic Representation: Many success-\\nful 3D parametric models for specific shape categories have\\nbeen introduced, for example, the morphable model [4] and\\nFLAME [30] for faces, SCAPE [1] and SMPL [31] for hu-\\nman bodies, and MANO [47] for hands . These model-\\nbased representations (Fig. 2B) suffer from limited expres-\\nsivity, which can be mitigated by neural networks. Net-\\n\\nFigure 2. (A) Problem definition: A list, or a set of deformed sur-\\nfaces S = {Si} of one instance should be represented by 1.) one\\ncanonical 3D surface U (in the green box) and 2.) the consistent\\ndeformation between surfaces (yellow arrows); (B) Model-based\\nmethods: Si is obtained through the skinning function (green ar-\\nrows) from the template mesh [31]; (C) Implicit-flow methods:\\nthe first frame serves as the reference shape and the deformation is\\nmodeled by Neural-ODEs [36] or MLPs [50].\\n\\nworks can express detailed template shapes based on tem-\\nplate meshes [32, 38, 48, 52] or skeletons [12, 27, 51], and\\ncan learn more detailed skinning functions [8,48,52] or for-\\nward deformations [38]. However, they rely on the strong\\nassumption of canonicalization through the pose, skeleton,\\nand template mesh, which makes them limited to specific\\ncategories, and insufficient for modeling the rich dynamic\\n3D world. Our method does not rely on any hard-wired\\ntemplate mesh or skeleton, and the same architecture is uni-\\nversal for all shape categories.\\nModel-Free Dynamic Representation: Recent works [6,\\n24, 36, 50] extend the success of static 3D representa-\\ntions [9, 33, 40] to 4D by modeling the deformation be-\\ntween frames. Fig. 2C illustrates how the two closest works\\nto ours, O-Flow [36] and LPDC [50], are related to our\\nproblem formulation. First, our method differs from O-\\nFlow [36] and LPDC [50] in the representation of the space\\ndeformation. We represent the deformation through a novel\\ncanonical map factorization that is efficient and guarantees\\nreal world properties based on conditional neural home-\\nomorphisms [14, 15], while O-Flow [36] uses a Neural-\\nODE [7] that also guarantees the production of a well-\\nbehaved deformation (see [22] for details) but with higher\\ncomputational complexity than ours. LPDC [50] replaces\\nthe Neural-ODE [7] by a Multilayer Perceptron (MLP) to\\nlearn correspondences in parallel. However, the MLP de-\\nformation [20, 41, 44, 50] has difficulty to model a home-\\nomorphism or express real world deformation properties.\\nNote that both O-Flow [36] and LPDC [50] compute the\\nreference surface in the first frame, which turns out to be a\\nrandom choice, since the shape can be in an arbitrary defor-\\nmation state in the first frame. Our reference shape is mod-\\neled in the learned canonical space induced by the canonical\\nmap, which is more stable and can be optimized (Fig. 1).\\n\\nI3DMM [58] learns a near neutral/mean canonical tem-\\nplate from human head scans, which limits its expressive\\nability. CASPR [46] and Garment Nets [10] learn a canon-\\nicalization of deformable objects, but rely on the ground\\n\\n2\\n\\n\\x0ctruth canonical coordinate supervision, which is often inac-\\ncessible. Other neural dynamic representations include the\\nlearned embedded graph [6] first proposed in [49] and para-\\nmetric atlases [2]. Beyond 4D data, A-SDF [35] models the\\ngeneral articulated objects with a specially designed disen-\\ntanglement network, but it cannot model correspondence.\\nInstead, our method achieves stronger disentanglement by\\nexplicitly modeling the deformation.\\nInvertible Networks for 3D representation: Many\\nworks [3, 7, 14, 15, 19, 28, 39] have been proposed to con-\\nstruct invertible networks for generative models. In 3D deep\\nlearning, Neural-ODE [7] is widely used as a good model\\nof deformation [22, 23, 25, 36] or transformation of point\\ncloud [56]. ShapeFlow [25] learns a \\xe2\\x80\\x9cHub-and-spoke\\xe2\\x80\\x9d sur-\\nface deformation for 3D shape collection via ODEs, but is\\ninefficient when applied to the 4D data since every frame\\nneeds to be lifted to the \\xe2\\x80\\x9chub\\xe2\\x80\\x9d through integration. Besides\\nODEs, I-ResNet [3] is used in [21] to build invertible defor-\\nmation for shape editing. Our method is inspired by Neural-\\nParts [42] where Real-NVP [15] is used to model the defor-\\nmation from a sphere primitive to a local part. While we\\nalso use Real-NVP [15] for its simplicity and efficiency, we\\nhave two distinct differences compared to [42]: Our canon-\\nical shape is a learned implicit surface instead of a fixed\\nsphere that can only capture local parts; we use the inverse\\nof the Real-NVP to close the factorization cycle, while [42]\\nuses the inverse in a complementary training path.\\n\\n3. Method\\n\\nAs in Fig. 2A, given a sequence1 of point cloud observa-\\ntions of one deforming instance, our goal is to reconstruct\\na sequence of surfaces. Instead of directly solving a corre-\\nspondence map between two frames during reconstruction,\\nwe propose an architecture (Fig. 3) where the interframe\\ncorrespondence is computed via a pivot canonical shape.\\nWe will call the map between a surface in any deformed\\nframe and the canonical shape a canonical map.\\n\\n3.1. CaDeX and Canonical Map\\n\\nLet us denote [xi, yi, zi] \\xe2\\x88\\x88 R3 as the 3D coordinates\\nof the input 3D space2, in which a deformed surface Si at\\ntime ti is embedded. Consider a continuous bijective map-\\nping (homeomorphism) Hi : R3 (cid:55)\\xe2\\x86\\x92 R3 at time ti that maps\\neach deformed coordinate to its global (shared over differ-\\nent time frames) 3D coordinate [u, v, w] = Hi([xi, yi, zi]).\\nNote that [u, v, w] has no index of time and can be seen\\nas a globally consistent indicator of each correspondence\\ntrajectory across time in the input 3D space. Hence, we\\nname [u, v, w] the canonical deformation coordinates of\\nthe position [xi, yi, zi] at time ti and call the uvw 3D space\\n\\n1Or a set, but for conciseness, we will only refer to the sequence.\\n2The superscript i refers to the time index.\\n\\nthe Canonical Deformation Coordinate Space (CaDeX)\\nof the sequence S = {Si}. The homeomorphisms Ht that\\ntransform [xt, yt, zt] to [u, v, w] are called canonical maps.\\nSince CaDeX is globally shared across time, we model the\\ncanonical shape (surface) U directly in CaDeX instead of\\nselecting an input frame as is the case in [36, 50]. Taking\\nadvantage of neural fields [54], we model U as a level set of\\nan occupancy field [33]:\\n\\nU = { [u, v, w] | OccField([u, v, w]) = l } ,\\n\\n(1)\\n\\nwhere l is the surface level. Using the inverse of each canon-\\nical map Hi at time ti, we can directly obtain each deformed\\nsurface at time ti in the input 3D space as:\\n\\nSi = (cid:8) H\\xe2\\x88\\x921\\n\\ni\\n\\n([u, v, w]) | \\xe2\\x88\\x80[u, v, w] \\xe2\\x88\\x88 U (cid:9) .\\n\\n(2)\\n\\nThe correspondence/deformation Fij that associates any\\ncoordinate (for both surface and non-surface points) from\\nthe 3D space at time ti to the 3D space at time tj can be\\nfactorized by the canonical maps as:\\n\\n[xj, yj, zj] = Fij([xi, yi, zi]) = H\\xe2\\x88\\x921\\n\\nj \\xe2\\x97\\xa6 Hi([xi, yi, zi]).\\n(3)\\nNote that Ht must be invertible; otherwise, the above defor-\\nmation function cannot be defined. By now, any surface that\\nis topologically isomorphic to the deformable instance sat-\\nisfies the above definitions, leading to infinitely many valid\\ncanonical shapes and maps. In the following, we will opti-\\nmize the canonical shapes and maps predicted by the archi-\\ntecture in Fig. 3 subject to the priors from the dataset.\\n\\n3.2. Canonical Map Implementation\\n\\nNeural Homeomorphism One key technique of our im-\\nplementation is an efficient way to parameterize and learn\\nthe homeomorphism between coordinate spaces. Unfor-\\ntunately, the widely used Neural-ODEs [7] do not meet\\nour efficiency requirements since a full integration would\\nhave to be applied to every frame.\\nInspired by [42], we\\nutilize the Conditional Real-NVP [15] (Real-valued Non-\\nVolume Preserving) or the NICE [14] (Nonlinear Indepen-\\ndent Component Estimation) normalizing flow implemen-\\ntations to learn the homeomorphism. Taking the more gen-\\neral NVP [15] as an example (Fig. 3-C), with the network\\nbeing a stack of Coupling Blocks [15], we apply NVP to 3D\\ncoordinates. During initialization, each block is randomly\\nassigned an input split pattern; for example, a block always\\nsplits [x, y, z] to [x, y] and [z]. Given a condition latent code\\nc, each block takes in 3D coordinates [x, y, z] and outputs\\nthe transformed coordinates [x\\xe2\\x80\\xb2, y\\xe2\\x80\\xb2, z\\xe2\\x80\\xb2] by changing one part\\nbased on the other part in the input coordinate split:\\n\\n[x\\xe2\\x80\\xb2, y\\xe2\\x80\\xb2, z\\xe2\\x80\\xb2] = [x, y, z exp(s\\xce\\xb8(x, y|c)) + t\\xce\\xb8(x, y|c)]\\n\\n(4)\\n\\nwhere s\\xce\\xb8(\\xc2\\xb7|c) : R2 (cid:55)\\xe2\\x86\\x92 R and t\\xce\\xb8(\\xc2\\xb7|c) : R2 (cid:55)\\xe2\\x86\\x92 R are scale\\nand translation predicted by any network conditioned on c.\\n\\n3\\n\\n\\x0cFigure 3. (A) Canonical Map (Sec. 3.2): A sequence (or a set) of input point clouds is first sent to the Deformation Encoder generating\\ndeformation embeddings ci for each frame. Then the canonical map H can transform any coordinate (e.g. a yellow point in the point cloud,\\na blue query position for implicit field or a purple source point for correspondence) from any deformed frame to the canonical coordinate\\nvia H conditioning on the corresponding deformation embedding. The correspondence prediction (right bottom) can be obtained by\\ndirectly mapping back the canonical coordinate through H \\xe2\\x88\\x921. (B) Canonical Shape Encoder-Decoder (Sec. 3.4): All input multi-frame\\npoint clouds are first transformed to the canonical space via H and are directly unioned to aggregate a canonical observation. The global\\ngeometry embedding g (unique across frames) is encoded via a PointNet [45] \\xcf\\x95, and the occupancy value for the canonical coordinate of a\\nquery position at ti (blue point) is predicted through a standard OccNet [33] \\xcf\\x88. During training, the occupancy is supervised by LR, and\\nthe correspondence can be optionally supervised by LC (Sec. 3.5). (C) The Real-NVP [15] invertible architecture of H (Sec. 3.2).\\n\\nSuch a block models a bijection since the inverse can be\\nimmediately derived as:\\n\\n[x, y, z] =\\n\\n(cid:20)\\nx\\xe2\\x80\\xb2, y\\xe2\\x80\\xb2,\\n\\nz\\xe2\\x80\\xb2 \\xe2\\x88\\x92 t\\xce\\xb8(x\\xe2\\x80\\xb2, y\\xe2\\x80\\xb2|c)\\nexp(s\\xce\\xb8(x\\xe2\\x80\\xb2, y\\xe2\\x80\\xb2|c))\\n\\n(cid:21)\\n\\n.\\n\\n(5)\\n\\nTherefore, the whole stack of blocks is invertible.\\nIf the\\nactivation functions in each block are continuous, then the\\nwhole network models a homeomorphism. NICE [14] is\\nsimply removing the scale freedom from the NVP block, i.e:\\ns\\xce\\xb8(\\xc2\\xb7|c) \\xe2\\x89\\xa1 0. Note that the inverse of NVP and NICE is as\\nsimple as the forward, which induces our desired efficiency\\nand simplicity, and enables using the definition in Eq.3.\\nH architecture Note that in Eq. 2, each deformed sur-\\nface Si has a different canonical map Hi that associates Si\\nwith U . We implement them with the conditional real-NVP\\nor NICE, denoted by H (noncalligraphic). Given the vec-\\ntor ci that encodes the deformation information at time ti\\nsuch that Hi(\\xc2\\xb7) \\xe2\\x89\\xa1 H(\\xc2\\xb7 ; ci), where the network H is shared\\nacross different time frames. The canonical deformation co-\\nordinates are predicted as (Fig. 3-A, boxes marked with H):\\n\\n[u, v, w] = H([xi, yi, zi] ; ci).\\n\\n(6)\\n\\nNote that on the right side of Eq. 6, the input coordinates\\nand the deformation embedding have the index ti since\\nthey come from each deformed frame. However, after ap-\\nplication of the canonical map, the coordinates on the left\\nside are independent of the index because there is only one\\nglobal CaDeX for this sequence. Finally, the correspon-\\ndence/deformation between two deformed frames (Eq. 3)\\ncan be implemented as:\\n\\n[\\xcb\\x86xj, \\xcb\\x86yj, \\xcb\\x86zj] = H \\xe2\\x88\\x921 (cid:0)H([xi, yi, zi] ; ci) ; cj\\n\\n(cid:1)\\n\\n(7)\\n\\nwhere [\\xcb\\x86xj, \\xcb\\x86yj, \\xcb\\x86zj] is the mapped position at time tj of the\\noriginal position [xi, yi, zi] at time ti. Regarding the choice\\nof H, Real-NVP [15] can provide more flexible defor-\\nmation since it has one more degree of freedom (scale);\\nNICE [14] guarantees volume conservation (Sec. 3.3) that\\nresults in a more regularized deformation.\\nDeformation Encoder To obtain the per-frame deforma-\\ntion embedding ci that is used as the condition of H, we\\ndemonstrate two kinds of inputs and three encoder types\\n(Fig. 3-A, orange box). One direct approach is to employ a\\nPointNet that summarizes the deformation code separately\\nper frame (PF). If the input is a sequence of point clouds,\\n\\n4\\n\\n\\x0cwe can alternatively use the ST-PointNet variant proposed\\nin [50] to get the deformation code (ST). The ST encoder\\nprocesses the 4D coordinates and applies the pooling spa-\\ntially and temporally. If the input is a set without order, we\\ndevelop a 2-phase PointNet to obtain a global set deforma-\\ntion code (SET), and then use a 1-D code query network\\nto output the deformation embedding ci taking the query\\narticulation angle and the global deformation code as in-\\nput. Since these are not our main contributions, we refer the\\nreader to the supplementary for details on these encoders.\\n\\n3.3. Properties of the Canonical Map\\n\\nThe novel factorization and its implementation induce\\nthe following desired properties of real world deformation:\\nCycle consistency: The deformation/correspondence be-\\ntween deformed frames predicted by our factorization\\n(Eq. 3, 7) is cycle consistent (path-invariant). The reason\\nis that every canonical map maps any deformed frame in\\nthe sequence (or set) to the global CaDeX of this sequence\\n(or set), and the canonical maps are invertible:\\n\\nFjk \\xe2\\x97\\xa6 Fij = H\\xe2\\x88\\x921\\n\\nk \\xe2\\x97\\xa6 Hj \\xe2\\x97\\xa6 H\\xe2\\x88\\x921\\n\\nj \\xe2\\x97\\xa6 Hi = H\\xe2\\x88\\x921\\n\\nk \\xe2\\x97\\xa6 Hi = Fik.\\n\\n(8)\\nTopology preserving deformation: Since our factorization\\n(Eq. 3, 7) is a composition of two homeomorphisms, the\\ninduced deformation function is thus a homeomorphism as\\nwell, and therefore never changes the surface topology.\\nVolume conservation (NICE): If H is implemented by\\nNICE [14], then the predicted deformation preserves the\\nvolume of the geometry, which can be proved by the fact\\nthat the determinant of the Jacobian of every coupling block\\nin NICE [14] is 1 (see Supp. for more details).\\nContinuous deformation if c is continuous: Some appli-\\ncations require the sequence S = {S} to be dense on time\\naxis, for example, modeling continuous deformation across\\ntime [36]. In this case, the deformation codes c become a\\nfunction of time c(t). Since all activation functions we are\\nusing in the canonical map are continuous, it is obvious that\\nif c(t) is continuous, then the predicted deformation in Eq. 7\\nmust be continuous across t.\\n\\n3.4. Representing Canonical Shape\\n\\nGeometry Encoder We represent the canonical shape in\\nthe CaDeX by a standard Occupancy Network [33]. The\\ncanonical map brings additional benefits for encoding the\\nglobal geometry embedding (Fig. 3-B). Denote the ob-\\nj, yi\\nserved point cloud at time ti as Xi = { [xi\\nj] | j =\\n0, 1, . . . , Ni }3, where [xi\\nj, zi\\nj] is the 3D coordinate of\\neach point in the point cloud. The observations from dif-\\nferent ti\\xe2\\x80\\x99s are partial, noisy, and not aligned. We overcome\\nsuch irregularity by using the same canonical map (Sec.3.2)\\n\\nj, yi\\n\\nj, zi\\n\\n3Note here the superscripts are still the index of the time, the subscripts\\n\\nare the index of the points in the cloud.\\n\\nto obtain a canonical aggregated observation. Given the de-\\nformation embedding ci per-frame, the canonical observa-\\ntions are merged via set union as:\\n\\n\\xc2\\xafX =\\n\\n(cid:91)\\n\\nti\\n\\n{ H([xi\\n\\nj, yi\\n\\nj, zi\\n\\nj] ; ci) | \\xe2\\x88\\x80[xi\\n\\nj, yi\\n\\nj, zi\\n\\nj] \\xe2\\x88\\x88 Xi }.\\n\\n(9)\\n\\nThe global geometry embedding g of the sequence S is en-\\ncoded by a PointNet \\xcf\\x95: g = \\xcf\\x95( \\xc2\\xafX).\\nGeometry Decoder Given the global geometry embed-\\nding g, we obtain the canonical shape encoded in g via an\\noccupancy network [33] that takes g as well as the query po-\\nsition [u, v, w] in the CaDeX as input, and predicts the oc-\\ncupancy in the CaDeX: \\xcb\\x86o = \\xcf\\x88([u, v, w]; g), where the de-\\ncoder \\xcf\\x88 is an MLP. However, the ground truth ([u, v, w], o\\xe2\\x88\\x97)\\nsupervision pair is unavailable in the CaDeX since the\\ncanonical shape is not known in advance and is learned dur-\\ning training. Available types of supervision are the query-\\noccupancy pairs ([xi, yi, zi], oi\\xe2\\x88\\x97) in each deformed coordi-\\nnate space where the deformed surface Si is embedded at\\neach time ti. Therefore, we predict the occupancy field of\\nany deformed frame through the canonical map via Eq. 6:\\n\\n\\xcb\\x86o = \\xcf\\x88 (cid:0)H([xi, yi, zi] ; ci) ; g(cid:1)\\n\\n(10)\\n\\n3.5. Losses, Training, Inference\\n\\nOur model is fully differentiable and is trained end-to-\\nend. Following Eq. 10, the main loss function is the recon-\\nstruction loss in each deformed frame:\\n\\nLR =\\n\\n1\\nT\\n\\nT\\n(cid:88)\\n\\nMi(cid:88)\\n\\n1\\nMi\\n\\ni=1\\n\\nj=1\\n\\nBCE (cid:2)\\xcf\\x88 (cid:0)H(pi\\n\\nj; ci); g(cid:1) , oi\\xe2\\x88\\x97\\n\\nj\\n\\n(cid:3) (11)\\n\\nwhere T is the total number of frames that have occupancy\\nfield supervision and Mi is the number of queried positions\\nat each frame. We denote by pi\\nj the jth query position in\\nframe ti, and by oi\\xe2\\x88\\x97\\nthe corresponding ground truth occu-\\nj\\npancy state. Optionally, if the ground truth correspondence\\npairs are given, we can utilize them as a supervision signal\\nvia Eq. 7. The additional correspondence loss reads:\\n\\nLC =\\n\\n1\\n|Q|\\n\\n(cid:88)\\n\\n(cid:13)\\n(cid:13)H \\xe2\\x88\\x921 (cid:0)H(pi\\n(cid:13)\\n\\nk; ci); cj\\n\\n(cid:1) \\xe2\\x88\\x92 pj\\n\\nk\\n\\n(cid:13)\\n(cid:13)\\n(cid:13)l\\n\\n(12)\\n\\n(pi\\n\\nk,pj\\n\\nk)\\xe2\\x88\\x88Q\\n\\nwhere Q is the set of ground truth correspondence pairs: pi\\nk\\nis the source position (Fig. 3 purple coordinate) in frame ti\\nand pj\\nk is the ground truth corresponding position in frame\\ntj; and k is the index of all supervision pairs. We denote by l\\nthe order of the error norm. Note that the cycle consistency\\nguaranteed by our method (Sec. 3.3) does not depend on\\nLC. The overall loss function is L = wRLR + wCLC,\\nwhere wC can be zero if no correspondence supervision is\\nprovided. Note that there is no loss directly applied to the\\n\\n5\\n\\n\\x0cpredicted canonical deformation coordinates [u, v, w]. This\\ngives the maximum freedom to the canonical shape to form\\na pattern that helps the prediction accuracy. All patterns of\\nthe canonical shape emerge automatically during training\\n(see the Supplement for an additional discussion).\\n\\nDuring training, our model\\n\\nis trained directly from\\nscratch with the mandatory reconstruction loss (Eq. 11). For\\nefficiency, at each training iteration, we randomly select a\\nsubset of frames in the input sequence and supervise the\\noccupancy prediction. If the ground-truth correspondence\\nsupervision is also provided, we predict the corresponding\\nposition of surface points in the first frame for every other\\nframe and minimize the correspondence loss in Eq. 12.\\n\\nDuring inference, our model generates all surfaces of a\\nsequence in parallel after a single marching cubes mesh ex-\\ntraction. Directly marching the CaDeX is intractable since\\nit is learned. However, by using Eq. 10 as a query function,\\nwe can extract the mesh (V0, E0) in the first frame, which\\nis equivalent to marching the CaDeX given the canoni-\\ncal map. The equivalent canonical mesh in the CaDeX is\\n(Vc, Ec) = (H(V0; c0), E0). Then any mesh in other frames\\ncan be extracted as:\\n\\n(Vi, Ei) = (H \\xe2\\x88\\x921(Vc; ci), Ec).\\n\\n(13)\\n\\nNote that all meshes above share the same connectivity E0,\\nso the mesh correspondence is produced. Eq. 13 can be\\nimplemented in batch to achieve better efficiency.\\n\\n4. Results\\n\\nTo demonstrate CaDeX as a general and expressive rep-\\nresentation, we investigate the performance in modeling\\nthree distinct categories: human bodies (Sec. 4.1), animals\\n(Sec. 4.2) and articulated objects (Sec. 4.3). Finally, we ex-\\namine the effectiveness of our design choice in Sec. 4.4.\\nMetrics: To measure our performance for shape and cor-\\nrespondence modeling, we follow the paradigm of [36, 50]\\nand use the same metrics: evaluating the reconstruction ac-\\ncuracy using the IoU and Chamfer Distance, and the motion\\naccuracy by correspondence l2-distance error.\\nBaselines: We compare with the closest model-free dy-\\nnamic representations. The main baselines described in\\nSec. 2 are: O-Flow [36] and LPDC [50] for sequence in-\\nputs and A-SDF [35] for articulated object set inputs.\\n\\n4.1. Modeling Dynamic Human Bodies\\n\\nWe first demonstrate the power of modeling the dynamic\\nhuman body across time. We use the same experiment\\nsetup, dataset, and split as [24, 36, 50]. The data are gen-\\nerated from D-FAUST [5], a real 4D human scan dataset.\\nFollowing the setting of [36], the input is a randomly sam-\\npled sparse point cloud trajectory (300 points) of 17 frames\\nevenly sampled across time. The ground-truth occupancy\\n\\nMethod\\n\\nSeen Individual\\nCD\\xe2\\x86\\x93\\n\\nIoU\\xe2\\x86\\x91\\n\\nCorr\\xe2\\x86\\x93\\n\\nUnseen Individual\\nCD\\xe2\\x86\\x93\\nIoU\\xe2\\x86\\x91\\n\\nCorr\\xe2\\x86\\x93\\n\\nPSGN-4D [16]\\nONet-4D [33]\\nO-Flow [36]\\nLCR [24]\\nLCR-F [24]\\nOurs\\n\\n-\\n\\n0.108\\n77.9% 0.084\\n79.9% 0.073\\n81.8% 0.068\\n81.5% 0.068\\n85.5% 0.056\\n\\n3.234\\n-\\n0.122\\n-\\n-\\n0.100\\n\\n-\\n\\n0.127\\n66.6% 0.140\\n69.6% 0.095\\n68.2% 0.100\\n69.9% 0.094\\n75.4% 0.074\\n\\n3.041\\n-\\n0.149\\n-\\n-\\n0.126\\n\\nTable 1. Results on D-FAUST [5] human bodies, trained without\\ncorrespondence supervision.\\n\\nMethod\\n\\nSeen Individual\\nCD\\xe2\\x86\\x93\\n\\nIoU\\xe2\\x86\\x91\\n\\nCorr\\xe2\\x86\\x93\\n\\nUnseen Individual\\nCD\\xe2\\x86\\x93\\nIoU\\xe2\\x86\\x91\\n\\nCorr\\xe2\\x86\\x93\\n\\nPSGN-4D [16]\\nO-Flow [36]\\nLPDC [50]\\nOurs(NICE)\\nOurs(ST)\\nOurs(PF)\\n\\n-\\n\\n0.101\\n81.5% 0.065\\n84.9% 0.055\\n85.4% 0.051\\n86.7% 0.046\\n89.1% 0.039\\n\\n0.102\\n0.094\\n0.080\\n0.082\\n0.077\\n0.070\\n\\n-\\n\\n0.119\\n72.3% 0.084\\n76.2% 0.071\\n75.6% 0.070\\n78.1% 0.063\\n80.7% 0.055\\n\\n0.131\\n0.117\\n0.098\\n0.104\\n0.095\\n0.087\\n\\nTable 2. Results on D-FAUST [5] human bodies, trained with cor-\\nrespondence supervision.\\n\\nfield as well as the optional surface point correspondence\\nare provided. Our default model is configured by using\\nthe ST-encoder (Sec. 3.2) and an NVP homeomorphism.\\nThe following tables and sections assume such a configu-\\nration if not otherwise specified. We test the performance\\nof the PF-encoder and the NICE homeomorphism variants\\nas well. The experiments are divided into training without\\ncorrespondence (Tab. 1) and training with correspondence\\n(Tab. 2) tracks for fair comparison between methods. The\\ntesting set has two difficulty levels: unseen motion and un-\\nseen individuals [36].\\n\\nQuantitative comparisons in Tab. 1, 2 indicate that our\\nmethod outperforms state-of-the-art methods by a signifi-\\ncant margin. The qualitative comparison in Fig. 4 shows the\\nadvantage of our method in capturing fast moving parts and\\nshape details (marked with red). We attribute such improve-\\nments to two main reasons: First, our factorization of the\\ndeformation and its implementation provide a strong regu-\\nlarization that other approaches like O-flow [36] can only\\nachieve with an ODE integration. Additionally, supervising\\na per-frame implicit reconstruction in our model is equiva-\\nlent to the dense cross-frame reconstruction supervision in\\n[50]. Second, our shape prior is stored in the learned canon-\\nical space (marked green in Fig. 4), which is relatively sta-\\nble across different sequences as shown in the figure. When\\ntraining without correspondence (Tab. 1), our method can\\nlearn the correspondence implicitly and reach a similar re-\\nconstruction performance as [50] in Tab. 2, which is trained\\nwith dense parallel correspondence supervision. Compar-\\ning the different configurations of our method in Tab. 2, the\\nNICE [14] version has a performance drop since the defor-\\nmation is strongly regularized to conserve the volume, but\\n\\n6\\n\\n\\x0cFigure 4. Left: Human body modeling (Sec. 4.1); Right: Animal body modeling (Sec. 4.2). The left top figure marked in the green box is\\nour canonical shape, the first input is not displayed. The colors of the meshes encode the correspondence. More results are in the the Supp.\\n\\nInput\\n\\nMehtod\\n\\nSeen individual\\nCD\\xe2\\x86\\x93\\n\\nIoU\\xe2\\x86\\x91\\n\\nCorr\\xe2\\x86\\x93\\n\\nUnseen individual\\nCD\\xe2\\x86\\x93\\nIoU\\xe2\\x86\\x91\\n\\nCorr\\xe2\\x86\\x93\\n\\nPCL\\n\\nDep\\n\\nO-Flow [36]\\nLPDC [50]\\nOurs\\n\\nO-Flow [36]\\nLPDC [50]\\nOurs\\n\\n70.6% 0.104\\n72.4% 0.085\\n80.3% 0.061\\n\\n63.0% 0.131\\n58.4% 0.160\\n71.1% 0.094\\n\\n0.204\\n0.162\\n0.133\\n\\n0.250\\n0.249\\n0.186\\n\\n57.3% 0.175\\n59.4% 0.149\\n64.7% 0.127\\n\\n49.0% 0.228\\n45.8% 0.261\\n55.7% 0.175\\n\\n0.285\\n0.262\\n0.239\\n\\n0.374\\n0.388\\n0.301\\n\\nTable 3. Results on DeformingThings4D [57] animal bodies, PCL\\nand Dep correspond to the input types.\\n\\nthis is achieved by freezing half of the capacity (scale free-\\ndom). We note that the naive per-frame encoder (PF) works\\nbetter compared to the spatial-temporal encoder [50] (ST).\\nA potential reason is that the per-frame encoder provides a\\nhigher canonicalization level since when deciding the de-\\nformation code ci, no information from other frames can be\\nconsidered, so the PF encoder might avoid overfitting.\\n\\n4.2. Modeling Dynamic Animals\\n\\nWe experiment with a more challenging setting: mod-\\neling different categories of animals with one model. We\\ngenerate the same supervision types as Sec. 4.1 based on\\nthe DeformingThings4D-Animals [57] dataset (DT4D-A).\\nWe use 17 animal categories and generate 2 types of input\\nobservations: Sparse point cloud input as Sec. 4.1 as well\\nas the monocular depth video input from a randomly posed\\nstatic camera. We assume that the camera view point esti-\\nmation problem is solved so all partial observations live in\\none global world frame. All models are trained across all\\nanimal categories. We refer the reader to the supplementary\\nmaterial for more details. Such a setting is more challenging\\nbecause animals have both large shape and motion variance\\nacross categories. Additionally, the models are required to\\naggregate information across time and hallucinate the miss-\\ning parts in depth observation inputs. Quantitative results\\nof both the sparse point cloud input and the depth input in\\nTab. 3 as well as the qualitative results in Fig. 4 indicate that\\nour method outperforms state-of-the-art methods in these\\nchallenging settings. In addition to the reasons mentioned\\nin Sec. 4.1, the improvement when predicting from depth\\n\\nInput Method\\n\\nIoU\\xe2\\x86\\x91\\n\\nCD\\xe2\\x86\\x93\\n\\nCorr\\xe2\\x86\\x93\\n\\nt(s)\\n\\n\\xce\\xb8 (deg)\\n\\nPCL\\n\\nDep\\n\\nA-SDF [35]\\nLPDC [50]\\nOurs\\n\\nA-SDF [35]\\nLPDC [50]\\nOurs\\n\\n55.2% 0.127\\n49.2% 0.171\\n58.9% 0.118\\n\\n53.9% 0.127\\n46.4% 0.195\\n56.4% 0.116\\n\\n-\\n0.230\\n0.160\\n\\n-\\n0.269\\n0.161\\n\\n3.44\\n0.53\\n1.12\\n\\n3.65\\n0.54\\n1.26\\n\\n3.38\\n3.00\\n2.75\\n\\n5.06\\n4.85\\n4.34\\n\\nTable 4. Results on Shape2Motion [35, 53] articulated objects,\\nPCL and Dep correspond to the input types. The average perfor-\\nmance across 7 categories is reported, we refer the readers to our\\nsupplementary for the full table. t is the surface generation average\\ntime and \\xce\\xb8 is the average angle prediction error.\\n\\nobservation can be attributed to our design of the canoni-\\ncal observation encoder (Sec. 3.4) that explicitly aggregates\\nobservations in the CaDeX.\\n\\n4.3. Modeling Articulated Objects\\n\\nWe extend CaDeX from modeling the 4D nonrigid sur-\\nface sequence to representing semi-nonrigid articulated ob-\\nject sets. We generate the dataset and inputs as in Sec. 4.2\\nfrom [35] based on Shape2Motion [53], which contains 7\\ncategories of articulated objects with 1 or 2 deformable\\nangles. We configure the model with the SET encoder\\n(Sec. 3.2) that produces the global dynamic code and then\\nuse the articulation angle to query the deformation code for\\neach frame (for details, see the Supplement). During train-\\ning, we input the sparse point cloud of 4 randomly sampled\\ndeformed frames of one object and then use the ground-\\ntruth angle to query per-frame deformation codes; finally,\\nthe model predicts the occupancy field for 4 seen (input)\\nframes and 4 unseen frames. We supervise both LR and\\nLC. For completeness, we also predict the articulation an-\\ngles of the input frames by a small head in the encoder\\nand supervise them. Each category is trained separately for\\nall methods. Note that A-SDF [35] demonstrates the auto-\\ndecoder setup, but it only solves half of our problem without\\ncorrespondence. Simultaneously solving the shape and the\\ncorrespondence leads to difficulties when applying an auto-\\ndecoder with optimization during testing, so we leave this\\nas a future direction. For fair comparison, we adapt A-SDF\\n\\n7\\n\\n\\x0cIoU\\xe2\\x86\\x91\\n\\nCD\\xe2\\x86\\x93\\n\\nCorr\\xe2\\x86\\x93\\n\\nFull\\nMLP\\nNo G-Enc\\n\\n66.5% 0.128\\n61.9% 0.161\\n63.4% 0.141\\n\\n0.223\\n0.303\\n0.216\\n\\nt(s)\\n\\n1.8\\n20.5\\n1.7\\n\\nTable 5. Ablation study, t is the average surface generation time.\\n\\nto extract meshes in this version. Second, we remove the\\ngeometry encoder in the canonical space and obtain the\\nglobal geometry embedding via a latent fusion using the\\nST-encoder [50]. We demonstrate the performance of the\\ndeer subcategory from DT4D-A [57] with point cloud in-\\nputs (Sec. 4.2). Tab. 5 shows the performance difference,\\nwhere we observe a significant performance decrease as\\nwell as longer inference times when using MLP instead of\\nhomeomorphisms. Additionally, we observe the drop in\\nreconstruction accuracy when removing the geometry en-\\ncoder in the canonical space (Sec. 3.4). We present more\\ndetails in the supplementary material.\\n\\n5. Limitations\\n\\nOur method guarantees several desirable properties and\\nachieves state-of-the-art performance on a wide range of\\nshapes, but still has limitations that need future exploration.\\nAlthough we can produce continuous deformation across\\ntime if c(t) is continuous, the continuity of c is not guaran-\\nteed in the ST-encoder [50] that we use. Therefore, when\\nthe input undergoes a large discontinuity, we do observe a\\ntrembling in the output of both LPDC [50] and our method.\\nAnother issue is that although our method preserves the\\ntopology, sometimes the real world deformation also results\\nin topology changes. Future work can explore how to selec-\\ntively preserve or alter the topology. Finally, it is currently\\nnontrivial to adapt our method in an auto-decoder frame-\\nwork [35, 40] since it requires simultaneously optimizing\\nthe canonical map (deformation) and the canonical shape\\nduring testing, which future work can explore.\\n\\n6. Conclusion\\n\\nWe introduced a novel and general representation for dy-\\nnamic surface reconstruction and correspondence. Our key\\ninsight is the factorization of the deformation by continuous\\nbijective canonical maps through a learned canonical shape.\\nWe prove that our representation guarantees cycle consis-\\ntency and topology preservation, as well as (if desired) vol-\\nume conservation. Extensive experiments on reconstructing\\nhumans, animals, and articulated objects demonstrate the\\neffectiveness and versatility of our approach. We believe\\nthat CaDeX enables more possibilities for future research\\non modeling and learning from our dynamic real world.\\nAcknowledgement: The authors appreciate the support of the fol-\\nlowing grants: ARL MURI W911NF-20-1-0080, NSF TRIPODS\\n1934960, NSF CPS 2038873, ARL DCIST CRA W911NF-17-2-\\n0181, and ONR N00014-17-1-2093.\\n\\nFigure 5. Articulated objects modeling (Sec. 4.3) with 7 distinct\\ncategories. The left-top figure marked in the green box is our\\ncanonical shape, the four small figures next to it are the inputs.\\nThe first row is the reconstruction of an observed deformation an-\\ngle and the second is for an unobserved angle. Note that A-SDF\\nhas no correspondence so is not colored.\\n\\nwith a similar encoder as our model and adapt the decoder\\nto predict the occupancy. We also compare with LPDC [50]\\nwhich is also adapted to use a similar encoder as ours.\\n\\nTab. 4 summarizes the average performance across 7 ob-\\nject categories while Fig. 5 presents the qualitative com-\\nparison. Both of them show our state-of-the-art perfor-\\nmance on modeling general articulated objects. We produce\\nan accurate reconstruction, while providing the correspon-\\ndence prediction that A-SDF [35] can not predict. Thus, the\\nmarching cube is needed for each frame in [35] and results\\nin a longer inference time as shown in Tab. 4. Note that our\\nmethod preserves the topology when deforming the objects\\n(Fig. 5 oven) while [35] does not have such guarantees. This\\nis the main reason that our method has a performance drop\\non eyeglasses category since the dataset contains many un-\\nrealistic deformations where the legs of the eyeglasses get\\ncrossed. Additionally, our method models more details in\\nthe moving parts (e.g, the inner side of the refrigerator door\\nin Fig. 5) due to the learned canonical space, which provides\\na stable container for the shape prior.\\n\\n4.4. Ablation Study\\n\\nWe show the effectiveness of our design as the following:\\nFirst, we replace the invertible canonical map with a one-\\nway MLP that maps the deformed coordinates to the canon-\\nical space (such setting is similar to [13, 58, 59]). Since\\nthe mapping is one-way, we supervise the correspondence\\nby enforcing the consistency in the canonical space. Ev-\\nery frame needs a separate application of marching cubes\\n\\n8\\n\\n\\x0cReferences\\n\\n[1] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-\\nbastian Thrun, Jim Rodgers, and James Davis. Scape: shape\\ncompletion and animation of people. In ACM SIGGRAPH\\n2005 Papers, pages 408\\xe2\\x80\\x93416. 2005. 1, 2\\n\\n[2] Jan Bednarik, Vladimir G Kim, Siddhartha Chaudhuri,\\nShaifali Parashar, Mathieu Salzmann, Pascal Fua, and Noam\\nAigerman. Temporally-coherent surface reconstruction via\\nmetric-consistent atlases. arXiv preprint arXiv:2104.06950,\\n2021. 1, 3\\n\\n[3] Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Du-\\nvenaud, and J\\xc2\\xa8orn-Henrik Jacobsen. Invertible residual net-\\nworks. In International Conference on Machine Learning,\\npages 573\\xe2\\x80\\x93582. PMLR, 2019. 3\\n\\n[4] Volker Blanz and Thomas Vetter. A morphable model for\\nIn Proceedings of the 26th an-\\nthe synthesis of 3d faces.\\nnual conference on Computer graphics and interactive tech-\\nniques, pages 187\\xe2\\x80\\x93194, 1999. 1, 2\\n\\n[5] Federica Bogo, Javier Romero, Gerard Pons-Moll, and\\nMichael J. Black. Dynamic FAUST: Registering human bod-\\nies in motion. In IEEE Conf. on Computer Vision and Pattern\\nRecognition (CVPR), July 2017. 2, 6\\n\\n[6] Aljaz Bozic, Pablo Palafox, Michael Zollhofer, Justus Thies,\\nAngela Dai, and Matthias Nie\\xc3\\x9fner. Neural deformation\\ngraphs for globally-consistent non-rigid reconstruction.\\nIn\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition, pages 1450\\xe2\\x80\\x931459, 2021. 1, 2,\\n3\\n\\n[7] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and\\nDavid Duvenaud. Neural ordinary differential equations.\\narXiv preprint arXiv:1806.07366, 2018. 2, 3\\n\\n[8] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,\\nand Andreas Geiger. Snarf: Differentiable forward skin-\\nning for animating non-rigid neural implicit shapes. arXiv\\npreprint arXiv:2104.03953, 2021. 2\\n\\n[9] Zhiqin Chen and Hao Zhang. Learning implicit fields for\\ngenerative shape modeling. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 5939\\xe2\\x80\\x935948, 2019. 2\\n\\n[10] Cheng Chi and Shuran Song. Garmentnets: Category-level\\npose estimation for garments via canonical space shape com-\\npletion. arXiv preprint arXiv:2104.05177, 2021. 2\\n\\n[11] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll.\\nImplicit functions in feature space for 3d shape reconstruc-\\ntion and completion. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n6970\\xe2\\x80\\x936981, 2020. 2\\n\\n[12] Boyang Deng, John P Lewis, Timothy Jeruzalski, Gerard\\nPons-Moll, Geoffrey Hinton, Mohammad Norouzi, and An-\\ndrea Tagliasacchi. Nasa neural articulated shape approxi-\\nIn Computer Vision\\xe2\\x80\\x93ECCV 2020: 16th European\\nmation.\\nConference, Glasgow, UK, August 23\\xe2\\x80\\x9328, 2020, Proceed-\\nings, Part VII 16, pages 612\\xe2\\x80\\x93628. Springer, 2020. 2\\n\\n[13] Yu Deng, Jiaolong Yang, and Xin Tong. Deformed implicit\\nfield: Modeling 3d shapes with learned dense correspon-\\ndence. In Proceedings of the IEEE/CVF Conference on Com-\\n\\nputer Vision and Pattern Recognition, pages 10286\\xe2\\x80\\x9310296,\\n2021. 2, 8\\n\\n[14] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:\\narXiv\\n\\nNon-linear independent components estimation.\\npreprint arXiv:1410.8516, 2014. 2, 3, 4, 5, 6\\n\\n[15] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-\\narXiv preprint\\n\\ngio. Density estimation using real nvp.\\narXiv:1605.08803, 2016. 2, 3, 4\\n\\n[16] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set\\ngeneration network for 3d object reconstruction from a single\\nimage. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 605\\xe2\\x80\\x93613, 2017. 6\\n[17] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,\\nand Thomas Funkhouser. Local deep implicit functions for\\nIn Proceedings of the IEEE/CVF Conference\\n3d shape.\\non Computer Vision and Pattern Recognition, pages 4857\\xe2\\x80\\x93\\n4866, 2020. 2\\n\\n[18] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,\\nand Thomas A Funkhouser. Deep structured implicit func-\\ntions. 2019. 2\\n\\n[19] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo\\nLarochelle. Made: Masked autoencoder for distribution es-\\ntimation. In International Conference on Machine Learning,\\npages 881\\xe2\\x80\\x93889. PMLR, 2015. 3\\n\\n[20] Thibault Groueix, Matthew Fisher, Vladimir G Kim,\\nBryan C Russell, and Mathieu Aubry. A papier-m\\xcb\\x86ach\\xc2\\xb4e ap-\\nproach to learning 3d surface generation. In Proceedings of\\nthe IEEE conference on computer vision and pattern recog-\\nnition, pages 216\\xe2\\x80\\x93224, 2018. 2\\n\\n[21] Bharath Hariharan Guandao Yang, Serge Belongie and\\nVladlen Koltun. Geometry processing with neural fields. Ad-\\nvances in Neural Information Processing Systems, 33, 2021.\\n3\\n\\n[22] Kunal Gupta. Neural Mesh Flow: 3D Manifold Mesh Gen-\\neration via Diffeomorphic Flows. University of California,\\nSan Diego, 2020. 2, 3\\n\\n[23] Jingwei Huang, Chiyu Max Jiang, Baiqiang Leng, Bin\\nWang, and Leonidas Guibas. Meshode: A robust and\\nscalable framework for mesh deformation. arXiv preprint\\narXiv:2005.11617, 2020. 2, 3\\n\\n[24] Boyan Jiang, Yinda Zhang, Xingkui Wei, Xiangyang Xue,\\nLearning compositional representation\\nand Yanwei Fu.\\nIn Proceedings of the\\nfor 4d captures with neural ode.\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 5340\\xe2\\x80\\x935350, 2021. 2, 6\\n\\n[25] Chiyu Jiang, Jingwei Huang, Andrea Tagliasacchi, Leonidas\\nGuibas, et al. Shapeflow: Learnable deformations among 3d\\nshapes. arXiv preprint arXiv:2006.07982, 2020. 2, 3\\n[26] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei\\nHuang, Matthias Nie\\xc3\\x9fner, Thomas Funkhouser, et al. Local\\nimplicit grid representations for 3d scenes. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 6001\\xe2\\x80\\x936010, 2020. 2\\n\\n[27] Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar\\nHilliges, and Siyu Tang. A skeleton-driven neural occu-\\npancy representation for articulated hands. arXiv preprint\\narXiv:2109.11399, 2021. 2\\n\\n9\\n\\n\\x0c[28] Diederik P Kingma and Prafulla Dhariwal. Glow: Gener-\\native flow with invertible 1x1 convolutions. arXiv preprint\\narXiv:1807.03039, 2018. 3\\n\\n[29] Zihang Lai, Sifei Liu, Alexei A Efros, and Xiaolong\\nWang. Video autoencoder: self-supervised disentanglement\\nIn Proceedings of the\\nof static 3d structure and motion.\\nIEEE/CVF International Conference on Computer Vision,\\npages 9730\\xe2\\x80\\x939740, 2021. 1\\n\\n[30] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier\\nRomero. Learning a model of facial shape and expression\\nfrom 4d scans. ACM Trans. Graph., 36(6):194\\xe2\\x80\\x931, 2017. 1, 2\\n[31] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\\nmulti-person linear model. ACM Trans. Graphics (Proc.\\nSIGGRAPH Asia), 34(6):248:1\\xe2\\x80\\x93248:16, Oct. 2015. 1, 2\\n[32] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,\\nGerard Pons-Moll, Siyu Tang, and Michael J Black. Learn-\\ning to dress 3d people in generative clothing. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, pages 6469\\xe2\\x80\\x936478, 2020. 2\\n\\n[33] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\\nLearning 3d reconstruction in function space. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, pages 4460\\xe2\\x80\\x934470, 2019. 1, 2, 3, 4, 5, 6\\n[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\nRepresenting scenes as neural radiance fields for view syn-\\nIn European conference on computer vision, pages\\nthesis.\\n405\\xe2\\x80\\x93421. Springer, 2020. 2\\n\\n[35] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille,\\nNuno Vasconcelos, and Xiaolong Wang. A-sdf: Learning\\ndisentangled signed distance functions for articulated shape\\nrepresentation. arXiv preprint arXiv:2104.07645, 2021. 3,\\n6, 7, 8\\n\\n[36] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\\nAndreas Geiger. Occupancy flow: 4d reconstruction by\\nlearning particle dynamics. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 5379\\xe2\\x80\\x93\\n5389, 2019. 1, 2, 3, 5, 6, 7\\n\\n[37] Michael Oechsle, Songyou Peng, and Andreas Geiger.\\nimplicit surfaces and radi-\\nUnisurf: Unifying neural\\nance fields for multi-view reconstruction. arXiv preprint\\narXiv:2104.10078, 2021. 2\\n\\n[38] Pablo Palafox, Alja\\xcb\\x87z Bo\\xcb\\x87zi\\xcb\\x87c, Justus Thies, Matthias Nie\\xc3\\x9fner,\\nand Angela Dai. Npms: Neural parametric models for 3d\\ndeformable shapes. arXiv preprint arXiv:2104.00702, 2021.\\n2\\n\\n[39] George Papamakarios, Theo Pavlakou, and Iain Murray.\\nMasked autoregressive flow for density estimation. arXiv\\npreprint arXiv:1705.07057, 2017. 3\\n\\n[40] Jeong Joon Park, Peter Florence, Julian Straub, Richard\\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\\ntinuous signed distance functions for shape representation.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 165\\xe2\\x80\\x93174, 2019. 2, 8\\n\\n[41] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\\n\\nMartin-Brualla. Deformable neural radiance fields. arXiv\\npreprint arXiv:2011.12948, 2020. 2\\n\\n[42] Despoina Paschalidou, Angelos Katharopoulos, Andreas\\nGeiger, and Sanja Fidler. Neural parts: Learning expres-\\nsive 3d shape abstractions with invertible neural networks.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 3204\\xe2\\x80\\x933215, 2021. 3\\n\\n[43] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\\nPollefeys, and Andreas Geiger. Convolutional occupancy\\nnetworks. In Computer Vision\\xe2\\x80\\x93ECCV 2020: 16th European\\nConference, Glasgow, UK, August 23\\xe2\\x80\\x9328, 2020, Proceed-\\nings, Part III 16, pages 523\\xe2\\x80\\x93540. Springer, 2020. 2\\n\\n[44] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields\\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n10318\\xe2\\x80\\x9310327, 2021. 1, 2\\n\\n[45] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\\nPointnet: Deep learning on point sets for 3d classification\\nIn Proceedings of the IEEE conference\\nand segmentation.\\non computer vision and pattern recognition, pages 652\\xe2\\x80\\x93660,\\n2017. 4\\n\\n[46] Davis Rempe, Tolga Birdal, Yongheng Zhao, Zan Gojcic,\\nSrinath Sridhar, and Leonidas J Guibas. Caspr: Learning\\ncanonical spatiotemporal point cloud representations. arXiv\\npreprint arXiv:2008.02792, 2020. 2\\n\\n[47] Javier Romero, Dimitrios Tzionas, and Michael J. Black.\\nEmbodied hands: Modeling and capturing hands and bod-\\nies together. ACM Transactions on Graphics, (Proc. SIG-\\nGRAPH Asia), 36(6), Nov. 2017. 1, 2\\n\\n[48] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J\\nBlack. Scanimate: Weakly supervised learning of skinned\\nIn Proceedings of the IEEE/CVF\\nclothed avatar networks.\\nConference on Computer Vision and Pattern Recognition,\\npages 2886\\xe2\\x80\\x932897, 2021. 2\\n\\n[49] Robert W Sumner, Johannes Schmid, and Mark Pauly. Em-\\nbedded deformation for shape manipulation. In ACM SIG-\\nGRAPH 2007 papers, pages 80\\xe2\\x80\\x93es. 2007. 3\\n\\n[50] Jiapeng Tang, Dan Xu, Kui Jia, and Lei Zhang. Learning par-\\nallel dense correspondence from spatio-temporal descriptors\\nfor efficient and robust 4d reconstruction. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 6022\\xe2\\x80\\x936031, 2021. 1, 2, 3, 5, 6, 7, 8\\n[51] Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, and Ger-\\nard Pons-Moll. Neural-gif: Neural generalized implicit\\nfunctions for animating people in clothing. arXiv preprint\\narXiv:2108.08807, 2021. 2\\n\\n[52] Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas\\nGeiger, and Siyu Tang. Metaavatar: Learning animat-\\nable clothed human models from few depth images. arXiv\\npreprint arXiv:2106.11944, 2021. 2\\n\\n[53] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qin-\\nping Zhao, and Kai Xu. Shape2motion: Joint analysis of\\nmotion parts and attributes from 3d shapes. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, pages 8876\\xe2\\x80\\x938884, 2019. 2, 7\\n\\n10\\n\\n\\x0c[54] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,\\nShiqin Yan, Numair Khan, Federico Tombari, James Tomp-\\nkin, Vincent Sitzmann, and Srinath Sridhar.\\nNeural\\narXiv preprint\\nfields in visual computing and beyond.\\narXiv:2111.11426, 2021. 3\\n\\n[55] Zhenjia Xu, Zhanpeng He, Jiajun Wu, and Shuran Song.\\nLearning 3d dynamic scene representations for robot manip-\\nulation. arXiv preprint arXiv:2011.01968, 2020. 1\\n\\n[56] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge\\nBelongie, and Bharath Hariharan. Pointflow: 3d point cloud\\ngeneration with continuous normalizing flows. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision, pages 4541\\xe2\\x80\\x934550, 2019. 2, 3\\n\\n[57] Takafumi Taketomi Yang Li, Hikari Takehara, Bo Zheng,\\nand Matthias Nie\\xc3\\x9fner. 4dcomplete: Non-rigid motion es-\\narXiv preprint\\ntimation beyond the observable surface.\\narXiv:2105.01905, 2021. 2, 7, 8\\n\\n[58] Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-\\nPeter Seidel, Mohamed Elgharib, Daniel Cremers, and\\ni3dmm: Deep implicit 3d morphable\\nChristian Theobalt.\\nIn Proceedings of the IEEE/CVF\\nmodel of human heads.\\nConference on Computer Vision and Pattern Recognition,\\npages 12803\\xe2\\x80\\x9312813, 2021. 1, 2, 8\\n\\n[59] Zerong Zheng, Tao Yu, Qionghai Dai, and Yebin Liu. Deep\\nimplicit templates for 3d shape representation. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 1429\\xe2\\x80\\x931439, 2021. 2, 8\\n\\n11\\n\\n\\x0c', b'L3U-net: Low-Latency Lightweight U-net Based Image\\nSegmentation Model for Parallel CNN Processors\\n\\nOsman Erman Okman\\xe2\\x88\\x97\\nMehmet Gorkem Ulkar\\xe2\\x88\\x97\\nGulnur Selda Uyanik\\nerman.okman@analog.com\\ngorkem.ulkar@analog.com\\nselda.uyanik@analog.com\\nAnalog Devices Inc.\\nIstanbul, Turkey\\n\\n2\\n2\\n0\\n2\\n \\nr\\na\\n\\nM\\n \\n0\\n3\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n8\\n2\\n5\\n6\\n1\\n.\\n3\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nABSTRACT\\nIn this research, we propose a tiny image segmentation model,\\nL3U-net, that works on low-resource edge devices in real-time. We\\nintroduce a data folding technique that reduces inference latency by\\nleveraging the parallel convolutional layer processing capability of\\nthe CNN accelerators. We also deploy the proposed model to such\\na device, MAX78000, and the results show that L3U-net achieves\\nmore than 90% accuracy over two different segmentation datasets\\nwith 10 fps.\\n\\nKEYWORDS\\nneural networks, image segmentation, low latency, edge cnn pro-\\ncessor\\n\\nACM Reference Format:\\nOsman Erman Okman, Mehmet Gorkem Ulkar, and Gulnur Selda Uyanik.\\n2022. L3U-net: Low-Latency Lightweight U-net Based Image Segmenta-\\ntion Model for Parallel CNN Processors. In Proceedings of tinyML Research\\nSymposium (tinyML Research Symposium\\xe2\\x80\\x9922). ACM, New York, NY, USA,\\n6 pages.\\n\\n1 INTRODUCTION\\nSemantic segmentation is one of the fundamental computer vi-\\nsion tasks that assign each pixel of an image with a predefined\\nlabel. This task is effectively utilized for many applications like\\nvideo surveillance, autonomous vehicle guidance, robotics, and\\nbiomedical image analysis. For most of these applications, real-time\\nprocessing with high-resolution inputs and the low energy con-\\nsumption is critical as many of these applications are designed to be\\na part of battery-powered devices like drones or personal gadgets.\\nThis makes TinyML research essential to develop applications that\\ncan run on hardware using mWs of power, KBs of RAM, and less\\nthan KBs of flash [27].\\n\\nWith the advance of deep learning (DL), the performance of\\nsemantic segmentation models has been improved significantly\\n\\n\\xe2\\x88\\x97Both authors contributed equally to this research.\\n\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\ntinyML Research Symposium\\xe2\\x80\\x9922, March 2022, San Jose, CA\\n\\xc2\\xa9 2022 Copyright held by the owner/author(s).\\n\\nand now very accurate results are possible. There are various ap-\\nproaches in the literature for this task that differ in particular as-\\npects such as network architecture, cost function, training strate-\\ngies, training data, etc. [25]. The initial approaches adopt fully\\nconvolutional networks (FCN) [19, 21] where the global context\\ninformation cannot be efficiently used. To add more contextual\\ninformation to FCN approaches, Conditional Random Fields (CRFs)\\nand Markov Random Fields (MRFs) are integrated back to the DL\\napproaches. Liu et al. proposed a CNN model that enables MRFs in\\na single forward pass [20].\\n\\nOne of the most popular image segmentation approaches uses\\nconvolutional encoder-decoder model architectures. Noh et al. [26]\\nproposed to use transposed convolution (deconvolution) layers to\\nreconstruct the segmentation map from the VGG 16-layer network\\nand present promising results on PASCAL VOC 2012 dataset. U-net\\n[28] and V-Net [24] are popular auto encode-based approaches.\\nThese architectures are different from [26] as there are residual\\nconnections between specified encoding and decoding layers. Vari-\\nous modified U-net models have also been used for many different\\napplications like 3D image segmentation [9] or land use determina-\\ntion [8, 32, 35]. U-net is still a prevalent approach, and many recent\\nstudies propose modifications like adding dense skip connections\\n[14] or proposing attention-based U-net architectures [12, 16] for\\ndifferent applications. Weng et al. [34] proposed a neural architec-\\nture search (NAS) approach to search highly accurate U-net-based\\nmodels for medical image segmentation effectively.\\n\\nDeepLab [4] family models are another group of commonly pre-\\nferred semantic segmentation DL models that effectively utilize\\ndilated (a.k.a. atrous) convolutions. This operator controls the res-\\nolution at which the features are computed in the DL models. By\\nincreasing the dilation parameter, the field of the view of the utilized\\nkernels is increased to deal with larger context effectively without\\nincreasing the number of model parameters. Further improvements\\nare proposed to this model [5\\xe2\\x80\\x937]. Finally, DeepLab v3+, which com-\\nbines cascaded and parallel dilated convolution modules and uses\\nan auto-encoder approach, achieved an 89% mean intersection over\\nunion (mIoU) score.\\n\\nA study [25] compares many other approaches like attention-\\nbased, generative or recurrent neural networks, as well as other\\npopular approaches like RefineNet [17]. Nevertheless, these meth-\\nods are not deployable to low-power, low-memory edge devices\\ndue to their high architectural and computational requirements.\\n\\n\\x0ctinyML Research Symposium\\xe2\\x80\\x9922, March 2022, San Jose, CA\\n\\nOkman and Ulkar, et al.\\n\\nMost of them cannot achieve real-time inference speeds. Besides,\\nthe existing studies on TinyML semantic segmentation are min-\\nimal. [23] proposes to use a new convolutional module, efficient\\nspatial pyramid (ESP), efficient computation, memory, and power. A\\nhuman-machine collaborative design strategy is used in [18] to cre-\\nate networks with customized module-level macro architecture and\\nmicro-architecture designs specifically for semantic segmentation\\ntasks. A modified dilated convolution approach is also proposed in\\n[10], where many layers are optimized for this task, and the back-\\nbone is changed with MobileNetv2 [29]. Despite these approaches\\nreducing the computational load and testing on edge, they are not\\napplied on TinyML hardware. Bahl et al. proposed a lightweight\\nencoder-decoder model with depthwise separable convolutions and\\ndecreasing memory usage by removing skip connections between\\nencoder and decoder [2]. The model is also realized on an FPGA,\\nand its results are given on very low-resolution satellite images.\\nAnother recent work demonstrates an attention condenser neural\\nnetwork, AttendSeg, and its results on CamVid dataset [33], which\\nis stated as deployable to a tiny edge device.\\n\\nThis study introduces a very lightweight U-net-based encoder-\\ndecoder model to solve the semantic segmentation problems on tiny\\nbattery-powered hardware. A data reforming (i.e., folding) approach\\nis also proposed to be used for hardware composed of parallel\\nCNN processor units. This strategy also enables the hardware to\\nhandle higher spatial resolution images. The complete design is\\nimplemented on a low power edge device, MAX78000 [1], which is\\nshown as the most energy-efficient CNN inference hardware in the\\nmarket [11, 22, 31]. To summarize, our contributions are:\\n\\n\\xe2\\x80\\xa2 A fully CNN very lightweight modified U-net model for\\n\\nsemantic segmentation problems,\\n\\n\\xe2\\x80\\xa2 A data folding approach that enables higher resolution through-\\n\\nput for parallel CNN processing hardware architectures,\\n\\xe2\\x80\\xa2 An implementation of the proposed approach on such a\\ndevice, MAX78000, and presentation of the results in terms\\nof accuracy, speed, and energy consumption.\\n\\nThe remainder of this paper is organized as follows: In Section\\n2, we present the proposed data folding technique. The proposed\\nnetwork architecture is detailed in Section 3. The model deployment\\nplatform is described in Section 4. We evaluate the proposed models\\nin Section 5.3 trained with the datasets presented in Section 5.1.\\nThe training approach and parameters are presented in Section 5.2.\\nFinally, Section 6 summarizes the results.\\n\\n2 DATA FOLDING TECHNIQUE\\nInference latency plays a critical role for neural network accelera-\\ntors at the edge since it directly affects those resource-constrained\\ndevices\\xe2\\x80\\x99 frame-per-second (FPS) performance metrics. The convo-\\nlution operation is parallelizable in the channel domain, and this\\nfeature has already been utilized in neural network accelerators\\n[1]. The initial layers of CNN networks generally have the highest\\nspatial resolution and spatial size, whereas they have the lowest\\nchannel counts compared to deeper layers of the network. These\\ninitial layers induce a significant proportion of the entire network\\xe2\\x80\\x99s\\nlatency since only a small number of cores can be used to process\\nthe small number of input channels that are big in spatial size.\\n\\nHowever, the spatial resolutions decrease for most network archi-\\ntectures, and the channel counts increase as the network proceeds\\nover the following layers. Therefore, these middle layers are more\\nsuitable for parallel processing. U-net is an example of such net-\\nworks, and the highest spatial resolution is in the initial layers. In\\nthe original U-net architecture, the input image consists of only\\none channel, and its size is 572x572 [28]. Suppose U-net is deployed\\nin a neural network accelerator. In that case, only one processor\\ncore out of many will be active and busy processing the big input,\\nwhich generates a significant amount of all network latency.\\n\\nTo optimize the latency of the initial layers and to distribute the\\nuneven processor loads to many processor cores more evenly, we\\npropose a novel technique, data folding. Applying this technique\\nand performing a regular convolution with stride=1 is mathemat-\\nically equivalent to strided convolutions; however, the latency is\\nreduced by distributing the processing load to the parallel cores.\\nThe proposed data folding technique works by creating downsam-\\npled versions of the input data by first shifting in height and width,\\nthen combining those downsampled versions of the original data\\nin the channel domain. Although the spatial resolution appears\\nto shrink, all of the original information is fed to the network by\\nincreasing the channel count. Consequently, the data becomes more\\nsuitable for parallel processing since more channels mean more\\nprocessing cores can be utilized for the same amount of data pro-\\ncessing. Likewise, the processing latency drops as each processor\\nneeds to handle smaller data due to lowered resolution after folding.\\nIn Fig. 1, the folding of input data of 3x8x8 size into 48x2x2 is illus-\\ntrated, where the shapes are given as channel size x height x width\\nformat (CHW). As shown, the channels of the folded data are the\\ndownsampled versions of the original channels. In this example, the\\nfolding factor, \\xf0\\x9d\\x9b\\xbc, is assumed to be 4. The downsampled data from\\neach channel with sampling offsets are placed through the channels\\nof the new kernel, where the sampling offsets vary between 0 to\\n\\xf0\\x9d\\x9b\\xbc \\xe2\\x88\\x92 1 in both height and width axes. In Fig 1, blue channels show\\nthe downsampled data with sampling offset is equal to 0 for both\\nheight and width. On the other hand, yellow channels illustrate\\nthe downsampled data with sampling offset is equal to 0 for the\\nheight but 1 for the width. Yellow channels follow the blue channels\\nstacked in the channel axis. Likewise, other downsampled versions\\nwith increasing offset values are stacked further. Figure 2 shows an\\nexample of a folded image when \\xf0\\x9d\\x9b\\xbc is 2.\\n\\nSince the data in the neighbor pixels are folded into channels, the\\nsame data portion can be processed with a smaller 2d convolutional\\nkernel after the folding. In Fig. 3 A, the data with a shape \\xf0\\x9d\\x91\\x81\\xf0\\x9d\\x91\\x90\\xe2\\x84\\x8e \\xe2\\x88\\x97 \\xf0\\x9d\\x91\\x81\\xe2\\x84\\x8e \\xe2\\x88\\x97\\n\\xf0\\x9d\\x91\\x81\\xf0\\x9d\\x91\\xa4 is shown. The 2d convolutional kernel with a side length equal\\nto \\xf0\\x9d\\x9b\\xbc \\xe2\\x88\\x97 \\xf0\\x9d\\x91\\x98 is illustrated with a yellow square prism in the same figure.\\n\\xf0\\x9d\\x91\\x81\\xf0\\x9d\\x91\\xa4\\nAfter folding, the 3d data matrix takes the shape of \\xf0\\x9d\\x9b\\xbc 2\\xf0\\x9d\\x91\\x81\\xf0\\x9d\\x91\\x90\\xe2\\x84\\x8e \\xe2\\x88\\x97\\n\\xf0\\x9d\\x9b\\xbc .\\nThe data in the slice of the kernel highlighted in yellow in Fig. 3 A\\nis now placed in another square prism. The side length of this prism\\nis now \\xf0\\x9d\\x91\\x98 instead of \\xf0\\x9d\\x9b\\xbc \\xe2\\x88\\x97\\xf0\\x9d\\x91\\x98 as seen in Fig. 3 B. Therefore, the same data\\ncan be accessed with a smaller convolutional kernel after folding.\\nData folding not only changes the kernel lengths, but also the stride\\nparameter. Since the spatial dimensions shrink after folding, any\\nstep in these shrunk dimensions corresponds to larger steps in\\nthe original data tensor. Numerically, a convolution stride of \\xf0\\x9d\\x91\\xa0 in\\nthe folded tensor corresponds to a stride of \\xf0\\x9d\\x9b\\xbc \\xe2\\x88\\x97 \\xf0\\x9d\\x91\\xa0 in the original\\n\\n\\xf0\\x9d\\x91\\x81\\xe2\\x84\\x8e\\n\\xf0\\x9d\\x9b\\xbc \\xe2\\x88\\x97\\n\\n\\x0cL3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors\\n\\ntinyML Research Symposium\\xe2\\x80\\x9922, March 2022, San Jose, CA\\n\\nFigure 1: Visualization of the proposed data folding tech-\\nnique\\n\\nFigure 3: 2D convolution operation before and after folding\\n\\nFigure 2: Example for input image folding for \\xf0\\x9d\\x9b\\xbc=2. The orig-\\ninal image is 3x352x352 and the folded image is 12x176x176.\\n\\nunfolded tensor as seen in Fig. 3 D and C. Thus, data folding is a data\\nreshaping operation that when a 2d convolution operation follows\\nit, it becomes equivalent to a convolution operation on the original\\ntensor with a larger kernel and a larger stride. As mentioned above,\\ndata folding allows distributing the convolution processing load\\nto a larger number of parallel processors. In Fig. 4, the number of\\nmultiplications done by each parallel processor for a 2d convolution\\noperation on 352x352 data is shown. The red bar shows the case\\nwhere there is no data folding. The yellow bar shows the case with\\nfolding and the folding factor, \\xf0\\x9d\\x9b\\xbc, is equal to 2. Lastly, the turquoise\\nbar shows the case for \\xf0\\x9d\\x9b\\xbc = 4. The input data is assumed to have\\nthree channels, e.g., RGB channels of an image, and the processor\\nis assumed to have at least 48 parallel cores. It is noteworthy that as\\n\\xf0\\x9d\\x9b\\xbc doubles, the processor load on each active processor is quartered\\nif enough idle processors exist. The folding factor must be chosen\\nbased on the number of parallel cores and the desired stride on the\\noriginal tensor. Higher folding factors results in greater steps in the\\nshift of the convolutional kernel in the original tensor.\\n\\n3 PROPOSED L3U-NET ARCHITECTURE\\nThe U-net model [28] is a CNN model that has a condensing path\\nto obtain the context and a symmetrical expanding path to provide\\naccurate localization. There are connections between these paths\\nat different resolutions in the U-net model to allow information\\nshared through the network. To achieve low latency and energy\\n\\nFigure 4: Distribution of convolution operations per proces-\\nsor with regard to the number of input data channels.\\n\\nwhile providing high performance in edge devices, we propose a\\nmodel that combines the data folding technique with a small version\\nof the U-net architecture.\\n\\nBecause of the data reshaping limitations of the chosen deploy-\\nment platform, we utilize data folding only at the beginning layer\\nof the proposed architecture. To reverse the folding action and\\ncreate the segmentation map with the same resolution of the in-\\nput image, data unfolding is applied at the end of the network.\\nThis operation is simply a tensor reshaping. In Fig. 5, the proposed\\nUL-net architecture is illustrated. In the figure, the input image\\nhas three channels, and its size is 352x352. With data folding, a\\n48x88x88 shaped input tensor is obtained. In this spatial resolution,\\nthe architecture sequentially contains three Conv2d layers with\\n1x1 kernel and one Conv2d layer with 3x3 kernel. In the spatial\\n\\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263641020304050607080901001101201301401501601701801902002102202302402502602702802903003103203303403503603703803904004104204304404504604704804905005105205305405505605705805906006106206306401533371050330370100200300400500600700800900100011001200130014001500160017001800190020002100220023002400250026002700280029003000310032003300340035003600370038003900400041004200430044004500460047004800490050005100520053005400550056005700580059006000610062006300640053710050033003700263438206034038020060034003800263438206034038020060034003800153337105033037010050033003700153337105033037010050033003700Channel1Channel2Channel3Downsampled Image1Original ImageFolded Image Shape: 48 x 2 x 2(Shape: 3 x 8 x 8) \\xce\\xb1=2ABCDNumber of processors31248No folding, kernel = 12x12, stride=4Number of multiplications per processor69,696278,7841,115,136, kernel=3x3, stride=1, kernel=6x6, stride=2\\x0ctinyML Research Symposium\\xe2\\x80\\x9922, March 2022, San Jose, CA\\n\\nOkman and Ulkar, et al.\\n\\nresolution contraction path, there are three Conv2d 3x3 layers with\\na 2x2 MaxPool preceding each of them. After the contraction path,\\nthree ConvTranspose2d operations take place in the the expand-\\ning path. Between the contraction and expanding paths at each\\nresolution, skip layers and channel-wise concatenations exist to\\nfacilitate information exchange and gradient propagation. After the\\nresolution is increased back to 88x88, two Conv2d layers with 3x3\\nkernel and four Conv2d layers with 1x1 kernel are utilized to create\\nthe segmentation map.\\n\\nAll of the Conv2d layers contain batch normalization (Batch-\\nNorm), and before quantization-aware training (QAT) starts, the\\nBatchNorm layers are fused into the weights and biases of the\\npreceding Conv2d layers. The total number of parameters of the\\nproposed architecture is 278,176 for four-class segmentation and\\n277,004 for two-class segmentation.\\n\\n4 DEPLOYMENT PLATFORM\\nThe proposed model is intended for deployment on CNN inference\\nengines that work at the edge. As an example deployment platform,\\nAnalog Devices MAX78000 is chosen in this study. MAX78000 is an\\nUltra-Low-Power Arm Cortex-M4 processor with FPU-based MCU\\nwith CNN accelerator that targets battery-powered neural network-\\nbased applications at the edge [1]. MAX78000 has a weight memory\\nof 442KB, and 1, 2, 4, or 8-bit weights are supported. As layers,\\nit supports both Conv1D and Conv2D as convolution operations,\\n2D transposed convolution layer, linear layer, pooling layers, and\\nelement-wise operations. MAX78000 has 64 parallel processors.\\nEach can only read from specific segments of the data memory\\nbut can write to the entire data memory. The CNN parallelization\\nis performed at the input channels. Hence each input channel or\\ngroup of input channels are processed by a different processor. The\\nproposed data folding method leverages this feature by assigning\\nthe same load to a higher number of channels which results in an\\nincrease of parallelism.\\n\\n5 EXPERIMENTS\\n5.1 Dataset\\nThis study conducts experiments on two different datasets, and\\nwe present two possible edge applications. The first dataset, the\\nCambridge-driving Labeled Video Database (CamVid), provides\\nground truth labels that associate each pixel with one of 32 se-\\nmantic classes [3]. Many researchers use this common dataset to\\nunderstand complex street views, enabling different applications,\\nespecially for driving safety and navigation. We used three semantic\\nclasses for our application: \\xe2\\x80\\x98Building\\xe2\\x80\\x99, \\xe2\\x80\\x98Sky\\xe2\\x80\\x99, and \\xe2\\x80\\x98Tree\\xe2\\x80\\x99, where the\\nremaining classes fall into the class \\xe2\\x80\\x98Other\\xe2\\x80\\x99. The CamVid dataset\\nincludes 601 960x720 color images, where 370 of them are used\\nfor training, and the rest are used for testing. Due to the memory\\nlimitations of the deployment platform, MAX78000, all images are\\ncropped into intersecting 352x352 images. For the training set, the\\nintersection ratio is kept around 40 percent to populate the training\\nset sufficiently. For the test set, it is set to one percent to avoid\\nbiasing the accuracy results due to the duplicated pixels in the test\\nset. Further, only the images with at least one of the selected three\\n\\nTable 1: Sample implementation results of proposed ap-\\nproach\\n\\nDataset\\n\\nPixel-to-Pixel\\nAccuracy (%)\\n\\nLatency\\n(ms)\\n\\nEnergy/Inf.\\n(mJ)\\n\\nCamVid\\n\\nAISegment\\n\\n91.05\\n\\n99.19\\n\\n95.1\\n\\n90.3\\n\\n7.3\\n\\n6.9\\n\\nmIoU\\n(%)\\n\\n84.24\\n\\n98.09\\n\\nsemantic classes are selected; the resulting set contains 4428 train-\\ning samples and 1392 test samples. Finally, all images are folded\\nwith \\xf0\\x9d\\x9b\\xbc=4 as explained in Section 2 to form images of size 48x88x88.\\nThe second dataset, AISegment [13] is a human portrait segmen-\\ntation dataset. The dataset includes 34,427 human portrait color\\nimages with a resolution of 600x800. The respective masks of the\\nportrait images are provided in RGBA format. The alpha channel\\nis either zero or one and is used for labeling each pixel with the\\n\\xe2\\x80\\x98Background\\xe2\\x80\\x99 or \\xe2\\x80\\x98Portrait\\xe2\\x80\\x99 label. The dataset training-test split ra-\\ntio is selected as 90% \\xe2\\x88\\x92 10%. The label distribution in both sets\\nis around 43% \\xe2\\x88\\x92 57% for the \\xe2\\x80\\x98Portrait\\xe2\\x80\\x99 and \\xe2\\x80\\x98Background\\xe2\\x80\\x99 classes,\\nrespectively. In the data augmentation phase, three overlapping\\nimages and corresponding matting images of size (600x600) are\\ncropped from the original image, sliding in the y-direction. Each of\\nthe cropped images is then re-scaled to size (352x352). As a result\\nof this cropping operation, the total number of images increases to\\n103,281. Finally, all images are folded with \\xf0\\x9d\\x9b\\xbc=4 to form images of\\nsize 48x88x88.\\n\\n5.2 Training\\nTo be deployed on MAX78000, the model needs to be quantized to\\nat most to 8-bit integers. Since simple post-quantization methods\\ncause performance degradation [31], QAT is employed in this study.\\nThe QAT approach implements the fake (a.k.a. simulated) quantiza-\\ntion approach of [15]. When training L3U-net, an Adam optimizer is\\nused with an initial learning rate of 0.001. For the CamVid dataset,\\nthe training takes 100 epochs, and QAT starts at epoch 40. The\\nlearning rate is halved two times at the 20th and 80th epochs. For\\nthe AISegment dataset, the training takes 200 epochs, QAT starts\\nat epoch 150, and the learning rate is halved three times at the 50th,\\n100th and 140th epochs. In this study, all weights are quantized\\nto 8-bits, and batchnorm parameters are fused into convolutional\\nlayers before the QAT starts.\\n\\n5.3 Results\\nThe results of the proposed approach for the two selected datasets\\nare summarized in Table 1.\\n\\nFig. 6 shows the results of L3U-net for three sample images.\\nAs seen from these samples, the model accurately segments the\\nregions, except for very tiny structures. For the four-class CamVid\\ndataset, our approach provides better accuracy than many of the\\nedge approaches in the literature AttendSeg [33] or EdgeSegNet\\n[18]. As these works give their results for 32-class segmentation, it is\\nnot possible to compare the results in terms of accuracy, only model\\nsize and complexity of the operations. Our approach has 4x and 30x\\nfewer parameters; 10x and 100x fewer MAC operations [18, 33]. In\\naddition, with the proposed data folding approach, we were able to\\n\\n\\x0cL3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors\\n\\ntinyML Research Symposium\\xe2\\x80\\x9922, March 2022, San Jose, CA\\n\\nFigure 5: Proposed tiny U-net model architecture\\n\\nrun the model on a battery-powered edge device, MAX78000, with\\nan inference speed up to 10 fps.\\n\\nThe model output for the AISegment dataset is the binary class\\nlabel of each pixel, and Fig. 7 includes a model input image and\\nremoved background by L3U-net model. A reference model, [30],\\nprovides 98% mIoU for 224x224 image with a floating-point model\\nthat has 8x more parameters than ours. In that sense, L3U-net is a\\nmore effective approach requiring less memory and less computa-\\ntion for larger input data.\\n\\nFigure 6: Sample model outputs for CamVid dataset.\\n\\nFigure 7: Sample model output for portrait segmentation.\\n\\nLastly, we provide energy measurements of the L3U-net model\\non MAX78000. According to the results, with an AA alkaline bat-\\ntery, it is possible to calculate over 1.5 million inferences using the\\nproposed model and setup. To the best of our knowledge, no other\\nstudy gives these values for semantic segmentation at the edge; so\\nwe provide these results for future studies.\\n\\n56ConvTranspose2dConv2d: 3x3ReLUMaxPool: 2x2BatchNormInOut11x1122x2244x4488x88concat828285611256112562856288166448Conv2d: 1x1ReLUBatchNormfolding646432486464646443352x352Conv2d: 1x1ReLUBatchNormConv2d: 1x1ReLUBatchNormConv2d: 3x3ReLUBatchNormConv2d: 3x3ReLUMaxPool: 2x2BatchNormConv2d: 3x3ReLUMaxPool: 2x2BatchNormConv2d: 3x3ReLUBatchNormConvTranspose2dconcatConv2d: 3x3ReLUBatchNormConvTranspose2dconcatConv2d: 3x3ReLUBatchNormConv2d: 3x3ReLUBatchNormConv2d: 1x1ReLUBatchNormConv2d: 1x1ReLUBatchNormConv2d: 1x1BatchNormConv2d: 1x1BatchNormunfolding8\\x0ctinyML Research Symposium\\xe2\\x80\\x9922, March 2022, San Jose, CA\\n\\nOkman and Ulkar, et al.\\n\\n6 CONCLUSION\\nWe presented L3U-net , a lightweight U-net-based fully CNN model\\nwhich is also deployed to a tiny edge device, MAX78000. We also\\nproposed a data folding approach that reduces the latency and en-\\nables processing of higher resolution input images for a multi-core\\nCNN inference engine, MAX78000. The experiments performed\\nshow that L3U-net is capable of providing 91% and 99% accuracy\\nfor four- and two-class segmentation examples at a speed of 10 fps.\\n\\nACKNOWLEDGMENTS\\nThe authors would like to thank Robert Muchsel, Brian Rush, and\\nother members of the AI Development Group at Analog Devices\\nthat contributed to this work.\\n\\nREFERENCES\\n[1] Analog Devices 2021. Artificial Intelligence Microcontroller with Ultra-Low-Power\\nConvolutional Neural Network Accelerator. Analog Devices. https://datasheets.\\nmaximintegrated.com/en/ds/MAX78000.pdf Rev. 1.\\n\\n[2] Gaetan Bahl, Lionel Daniel, Matthieu Moretti, and Florent Lafarge. 2019. Low-\\nPower Neural Networks for Semantic Segmentation of Satellite Images. In Pro-\\nceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)\\nWorkshops.\\n\\n[3] Gabriel J. Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. 2008.\\nSegmentation and Recognition Using Structure from Motion Point Clouds. In\\nECCV (1). 44\\xe2\\x80\\x9357.\\n\\n[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and\\nAlan L Yuille. 2014. Semantic image segmentation with deep convolutional nets\\nand fully connected crfs. arXiv preprint arXiv:1412.7062 (2014).\\n\\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and\\nAlan L. Yuille. 2018. DeepLab: Semantic Image Segmentation with Deep\\nConvolutional Nets, Atrous Convolution, and Fully Connected CRFs.\\nIEEE\\nTransactions on Pattern Analysis and Machine Intelligence 40, 4 (2018), 834\\xe2\\x80\\x93848.\\nhttps://doi.org/10.1109/TPAMI.2017.2699184\\n\\n[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. 2017.\\nRethinking atrous convolution for semantic image segmentation. arXiv preprint\\narXiv:1706.05587 (2017).\\n\\n[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig\\nAdam. 2018. Encoder-Decoder with Atrous Separable Convolution for Semantic\\nImage Segmentation. In Proceedings of the European Conference on Computer\\nVision (ECCV).\\n\\n[8] Zhengquan Chu, Tian Tian, Ruyi Feng, and Lizhe Wang. 2019.\\n\\nSea-Land\\nSegmentation With Res-UNet And Fully Connected CRF. In IGARSS 2019 -\\n2019 IEEE International Geoscience and Remote Sensing Symposium. 3840\\xe2\\x80\\x933843.\\nhttps://doi.org/10.1109/IGARSS.2019.8900625\\n\\n[9] \\xc3\\x96zg\\xc3\\xbcn \\xc3\\x87i\\xc3\\xa7ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf\\nRonneberger. 2016. 3D U-Net: learning dense volumetric segmentation from\\nsparse annotation. In International conference on medical image computing and\\ncomputer-assisted intervention. Springer, 424\\xe2\\x80\\x93432.\\n\\n[10] Taha Emara, Hossam E. Abd El Munim, and Hazem M. Abbas. 2019. LiteSeg: A\\nNovel Lightweight ConvNet for Semantic Segmentation. In 2019 Digital Image\\nComputing: Techniques and Applications (DICTA). 1\\xe2\\x80\\x937. https://doi.org/10.1109/\\nDICTA47822.2019.8945975\\n\\n[11] Marco Giordano and Michele Magno. 2021. A Battery-Free Long-Range Wireless\\nSmart Camera for Face Recognition. In Proceedings of the 19th ACM Conference on\\nEmbedded Networked Sensor Systems (Coimbra, Portugal) (SenSys \\xe2\\x80\\x9921). Association\\nfor Computing Machinery, New York, NY, USA, 594\\xe2\\x80\\x93595. https://doi.org/10.\\n1145/3485730.3493367\\n\\n[12] Changlu Guo, M\\xc3\\xa1rton Szemenyei, Yugen Yi, Wenle Wang, Buer Chen, and\\nChangqi Fan. 2021. SA-UNet: Spatial Attention U-Net for Retinal Vessel Seg-\\nmentation. In 2020 25th International Conference on Pattern Recognition (ICPR).\\n1236\\xe2\\x80\\x931242. https://doi.org/10.1109/ICPR48806.2021.9413346\\n\\n[13] Laurent H. 2018. AISegment.com - Matting Human Datasets. https://www.kaggle.\\ncom/laurentmih/aisegmentcom-matting-human-datasets. Accessed: 2021-12-10.\\n[14] Huimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu, Qiaowei Zhang, Yutaro\\nIwamoto, Xianhua Han, Yen-Wei Chen, and Jian Wu. 2020. UNet 3+: A Full-\\nScale Connected UNet for Medical Image Segmentation. In ICASSP 2020 - 2020\\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\\n1055\\xe2\\x80\\x931059. https://doi.org/10.1109/ICASSP40776.2020.9053405\\n\\n[15] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew\\nHoward, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and\\nTraining of Neural Networks for Efficient Integer-Arithmetic-Only Inference. In\\n\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR).\\n\\n[16] Sheng Lian, Zhiming Luo, Zhun Zhong, Xiang Lin, Songzhi Su, and Shaozi\\nLi. 2018. Attention guided U-Net for accurate iris segmentation. Journal of\\nVisual Communication and Image Representation 56 (2018), 296\\xe2\\x80\\x93304.\\nhttps:\\n//doi.org/10.1016/j.jvcir.2018.10.001\\n\\n[17] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. 2017. RefineNet:\\nMulti-Path Refinement Networks for High-Resolution Semantic Segmentation.\\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR).\\n\\n[18] Zhong Qiu Lin, Brendan Chwyl, and Alexander Wong. 2019. Edgesegnet: A\\ncompact network for semantic segmentation. arXiv preprint arXiv:1905.04222\\n(2019).\\n\\n[19] Wei Liu, Andrew Rabinovich, and Alexander C. Berg. 2015. ParseNet: Looking\\nWider to See Better. CoRR abs/1506.04579 (2015). arXiv:1506.04579 http://arxiv.\\norg/abs/1506.04579\\n\\n[20] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen-Change Loy, and Xiaoou Tang. 2015.\\nSemantic Image Segmentation via Deep Parsing Network. In Proceedings of the\\nIEEE International Conference on Computer Vision (ICCV).\\n\\n[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully Convolutional\\nNetworks for Semantic Segmentation. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR).\\n\\n[22] Michele Magno. 2021. A Battery-Free Long-Range Wireless Smart Camera for\\nFace Detection: An accurate benchmark of novel Edge AI platforms and milliwatt\\nmicrocontrollers. https://www.tinyml.org/event/emea-2021 TinyML Emea 2021\\nTalk.\\n\\n[23] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, and Hannaneh\\nHajishirzi. 2018. ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for\\nSemantic Segmentation. In Proceedings of the European Conference on Computer\\nVision (ECCV).\\n\\n[24] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. 2016. V-Net: Fully\\nConvolutional Neural Networks for Volumetric Medical Image Segmentation.\\nIn 2016 Fourth International Conference on 3D Vision (3DV). 565\\xe2\\x80\\x93571. https:\\n//doi.org/10.1109/3DV.2016.79\\n\\n[25] Shervin Minaee, Yuri Y. Boykov, Fatih Porikli, Antonio J Plaza, Nasser Kehtar-\\nnavaz, and Demetri Terzopoulos. 2021. Image Segmentation Using Deep Learning:\\nA Survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021),\\n1\\xe2\\x80\\x931. https://doi.org/10.1109/TPAMI.2021.3059968\\n\\n[26] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. 2015. Learning Deconvolu-\\ntion Network for Semantic Segmentation. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV).\\n\\n[27] Partha Pratim Ray. 2021. A Review on TinyML: State-of-the-art and Prospects.\\nJournal of King Saud University - Computer and Information Sciences (2021).\\nhttps://doi.org/10.1016/j.jksuci.2021.11.019\\n\\n[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional\\nNetworks for Biomedical Image Segmentation. In Medical Image Computing and\\nComputer-Assisted Intervention \\xe2\\x80\\x93 MICCAI 2015, Nassir Navab, Joachim Horneg-\\nger, William M. Wells, and Alejandro F. Frangi (Eds.). Springer International\\nPublishing, Cham, 234\\xe2\\x80\\x93241.\\n\\n[29] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-\\nChieh Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR).\\n\\n[30] A. Sathyan. 2021. https://github.com/anilsathyan7/Portrait-Segmentation.\\n[31] Mehmet Gorkem Ulkar and Osman Erman Okman. 2021. Ultra-Low Power\\n\\nKeyword Spotting at the Edge. arXiv:2111.04988 [cs.SD]\\n\\n[32] Priit Ulmas and Innar Liiv. 2020. Segmentation of satellite imagery using u-net\\nmodels for land cover classification. arXiv preprint arXiv:2003.02899 (2020).\\n[33] Xiaoyu Wen, Mahmoud Famouri, Andrew Hryniowski, and Alexander Wong.\\n2021. AttendSeg: A Tiny Attention Condenser Neural Network for Semantic\\nSegmentation on the Edge. arXiv preprint arXiv:2104.14623 (2021).\\n\\n[34] Yu Weng, Tianbao Zhou, Yujie Li, and Xiaoyu Qiu. 2019. NAS-Unet: Neural\\nIEEE Access 7 (2019),\\n\\nArchitecture Search for Medical Image Segmentation.\\n44247\\xe2\\x80\\x9344257. https://doi.org/10.1109/ACCESS.2019.2908991\\n\\n[35] Pengbin Zhang, Yinghai Ke, Zhenxin Zhang, Mingli Wang, Peng Li, and\\nShuangyue Zhang. 2018. Urban Land Use and Land Cover Classification Us-\\ning Novel Deep Learning Models Based on High Spatial Resolution Satellite\\nImagery. Sensors 18, 11 (2018). https://doi.org/10.3390/s18113717\\n\\n\\x0c']\n"
     ]
    }
   ],
   "source": [
    "#Encoding all the pdf text in UTF-8\n",
    "\n",
    "elements=[]\n",
    "for i in array_pdf_text:\n",
    "    elements.append(i.encode(\"utf-8\"))\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a4129a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Binghui_Chen',\n",
       "  'Weihong_Deng_Jiani_Hu',\n",
       "  'Mixed',\n",
       "  'Guangyi_Chen_Chunze',\n",
       "  'Lin_Liangliang',\n",
       "  'Jiwen_Lu',\n",
       "  'Nself',\n",
       "  'Tianlong_Chen',\n",
       "  'Ding_Jingyi',\n",
       "  'Yang_Yang',\n",
       "  'Zhou_Ren_Zhangyang_Wang',\n",
       "  'Abd_Nin_Pro',\n",
       "  'Ting_Chen_Simon_Kornblith_Mohammad',\n",
       "  'Ting_Chen_Simon_Kornblith_Kevin_Swersky_Mohammad_Nnorouzi',\n",
       "  'Geoffrey_Hinton',\n",
       "  'Weihua_Chen_Xiaotang',\n",
       "  'Chen_Jianguo_Zhang_Kaiqi_Nhuang',\n",
       "  'Xinlei_Chen',\n",
       "  'Ross_Girshick_Kaiming',\n",
       "  'Zhirui_Chen',\n",
       "  'Jianheng_Li',\n",
       "  'Wei_Shi_Zheng',\n",
       "  'Zuozhuo_Dai_Mingqiang',\n",
       "  'Chen_Xiaodong_Gu',\n",
       "  'Batch',\n",
       "  'Ron_Appel',\n",
       "  'Serge_Belongie_Pietro',\n",
       "  'Intelligence_Xe',\n",
       "  'Pedro_Felzenszwalb',\n",
       "  'Ross_Girshick',\n",
       "  'David_Mcallester',\n",
       "  'Deva_Ramanan',\n",
       "  'Dengpan_Fu',\n",
       "  'Jingdong_Wang_Dongdong',\n",
       "  'Chen_Njianmin_Bao_Gang_Hua_Houqiang_Li',\n",
       "  'Yixiao_Ge',\n",
       "  'Dapeng_Chen',\n",
       "  'Hongsheng_Li',\n",
       "  'Learning_Representations',\n",
       "  'Feng_Zhu',\n",
       "  'Dapeng_Chen_Rui',\n",
       "  'Zhao_Hong_Nsheng_Li',\n",
       "  'Douglas_Gray_Hai_Tao',\n",
       "  'Xe_Nspringer',\n",
       "  'Jean_Bastien_Grill',\n",
       "  'Corentin_Ntallec',\n",
       "  'Pierre_Richemond_Elena_Buchatskaya',\n",
       "  'Bernardo_Avila',\n",
       "  'Daniel_Guo_Moham',\n",
       "  'Xinqian_Gu',\n",
       "  'Bingpeng_Ma',\n",
       "  'Hong_Chang_Shiguang_Shan',\n",
       "  'Xilin_Chen',\n",
       "  'Kaiming_He',\n",
       "  'Wu_Saining',\n",
       "  'Ross_Ngirshick',\n",
       "  'Wu_Liu',\n",
       "  'Alexander_Hermans_Lucas_Beyer',\n",
       "  'Bastian_Leibe',\n",
       "  'Yan_Huang_Qiang_Wu',\n",
       "  'Jingsong_Xu',\n",
       "  'Yi_Zhong',\n",
       "  'Sbsgan_Nsuppression',\n",
       "  'Xin_Jin_Cuiling',\n",
       "  'Lan_Wenjun',\n",
       "  'Zeng_Zhibo',\n",
       "  'Chen_Li_Nzhang',\n",
       "  'Lan_Wenjun_Zeng_Guoqiang_Wei_Nzhibo',\n",
       "  'Chen',\n",
       "  'Chen_Jianmin_Bao_Hao_Yang_Lu',\n",
       "  'Lei_Zhang',\n",
       "  'Houqiang_Li_Dong_Chen',\n",
       "  'Gou_Ziyan',\n",
       "  'Richard_Radke',\n",
       "  'Prannay_Khosla_Piotr',\n",
       "  'Chen_Wang',\n",
       "  'Ce_Liu_Ndilip_Krishnan',\n",
       "  'Jiane_Li',\n",
       "  'Wen_Gao_Shiliang_Nzhang',\n",
       "  'Junnan_Li_Caime_Xiong_Steven_Hoi',\n",
       "  'Li_Xiatian',\n",
       "  'Zhu_Shaogang_Gong',\n",
       "  'Wei_Li',\n",
       "  'Rui_Zhao_Tong_Xiao_Xiaogang_Wang',\n",
       "  'Yu_Jhe_Li_Yun_Chun_Chen_Yen_Yu_Lin',\n",
       "  'Frank_Wang',\n",
       "  'Yu_Jhe_Li_Ci',\n",
       "  'Lin_Yan',\n",
       "  'Lin_Yu_Chiang_Frank_Nwang',\n",
       "  'Yutian_Lin_Xuanyi',\n",
       "  'Liang_Zheng_Yan_Yan_Yi_Nyang',\n",
       "  'Confer',\n",
       "  'Fangyi_Liu',\n",
       "  'Chen_Change_Loy',\n",
       "  'Liu_Shaogang_Gong',\n",
       "  'Chuanchen_Luo_Yuntao_Chen_Naiyan',\n",
       "  'Wang_Zhaoxi',\n",
       "  'Nang_Zhang',\n",
       "  'Hao_Luo',\n",
       "  'Wei_Jiang',\n",
       "  'Youzhi_Gu',\n",
       "  'Liu_Xingyu_Liao',\n",
       "  'Lai_Jianyang_Gu',\n",
       "  'Jingke_Meng_Sheng',\n",
       "  'Wu_Wei_Shi_Zheng',\n",
       "  'Hyunjong_Park_Bumsub',\n",
       "  'Xuelin_Qian',\n",
       "  'Xiang_Wenxuan',\n",
       "  'Wang_Jie',\n",
       "  'Nqiu_Yang',\n",
       "  'Wu_Yu',\n",
       "  'Gang_Jiang_Xiangyang_Xue',\n",
       "  'Nproceedings',\n",
       "  'Ruijie_Quan_Xuanyi',\n",
       "  'Dong_Yu_Wu',\n",
       "  'Linchao_Zhu_Yi_Nyang',\n",
       "  'Shaoqing_Ren_Kaiming',\n",
       "  'Jian_Sun_Nfaster',\n",
       "  'Olga_Russakovsky_Jia',\n",
       "  'Deng_Hao_Su_Jonathan_Krause_San',\n",
       "  'Sean_Ma',\n",
       "  'Zhiheng_Huang',\n",
       "  'Michael_Bernstein',\n",
       "  'Dong_Shen_Shuai',\n",
       "  'Zhao_Jinming',\n",
       "  'Hao_Feng',\n",
       "  'Deng_Cai',\n",
       "  'Yantao_Shen',\n",
       "  'Shuai_Yi',\n",
       "  'Xiaogang_Wang',\n",
       "  'Yumin_Suh_Jingdong',\n",
       "  'Tang',\n",
       "  'Tao_Mei_Ky',\n",
       "  'Mu_Lee',\n",
       "  'Xe_Ke',\n",
       "  'Bin_Xiao_Dong_Liu',\n",
       "  'Jingdong_Wang',\n",
       "  'Yifan_Sun',\n",
       "  'Liang_Zheng',\n",
       "  'Yi_Yang',\n",
       "  'Tian_Shengjin_Nwang',\n",
       "  'Person',\n",
       "  'Dongkai_Wang_Shiliang_Zhang',\n",
       "  'Wang_Guangcong',\n",
       "  'Xujie_Zhang_Jianhuang_Nlai',\n",
       "  'Zhengtao_Yu',\n",
       "  'Liang_Lin',\n",
       "  'Guanshuo_Wang',\n",
       "  'Yuan_Xiong',\n",
       "  'Chen_Jiwei_Li_Xi_Nzhou',\n",
       "  'Xe_Nacm',\n",
       "  'Longhui_Wei_Shiliang_Zhang_Wen_Gao',\n",
       "  'Zhirong_Wu',\n",
       "  'Stella_Yu_Dahua_Lin_Nunsupervised',\n",
       "  'Bryan_Ning_Xia_Yuan',\n",
       "  'Yizhe_Zhang',\n",
       "  'Wuyang_Chen_Yang_Yang_Zhangyang',\n",
       "  'Learning',\n",
       "  'Yifu_Zhang_Chunyu',\n",
       "  'Wang_Xinggang',\n",
       "  'Wang_Wenjun_Zeng',\n",
       "  'Wenyu_Liu',\n",
       "  'Fairmot',\n",
       "  'Zhizheng_Zhang',\n",
       "  'Liang_Zheng_Liyue',\n",
       "  'Tian',\n",
       "  'Wang_Qi',\n",
       "  'Liang_Zheng_Hengheng_Zhang',\n",
       "  'Zhedong_Zheng',\n",
       "  'Liang_Zheng_Yi_Yang',\n",
       "  'Zhun_Zhong',\n",
       "  'Liang_Zheng_Donglin_Cao_Shaozi_Li',\n",
       "  'Kuan_Zhu',\n",
       "  'Zhiwei_Liu',\n",
       "  'Tang_Jinqiao_Nwang',\n",
       "  'Xe_Xa',\n",
       "  'Xe_Xc',\n",
       "  'Ek_Xc',\n",
       "  'Xce_Xbbpro',\n",
       "  'Xce_Xbblgc',\n",
       "  'Xe_Xxi_Yi',\n",
       "  'Xe_Ni',\n",
       "  'Eq_Xcb',\n",
       "  'Xe',\n",
       "  'Ek_Xcb',\n",
       "  'Xcf',\n",
       "  'Xcf_Ni_Xcb',\n",
       "  'Xe_Xlj_Ni_Nelse',\n",
       "  'Nlgc',\n",
       "  'Nexp_Xe_Xqi',\n",
       "  'Xyi_Xcf',\n",
       "  'Xe_Xqi',\n",
       "  'Xcf_Nexp',\n",
       "  'Xe_Xcf',\n",
       "  'Nexp_Xe',\n",
       "  'Xe_Xe_Xn',\n",
       "  'Xe_Xp',\n",
       "  'Nli_Li',\n",
       "  'Eq_Minimize_Li',\n",
       "  'Ek',\n",
       "  'Xce_Xbblgcli_Nlgc_Nd',\n",
       "  'Epoch_Batch',\n",
       "  'Not',\n",
       "  'Li_Nlgc',\n",
       "  'Xcf_Threshold',\n",
       "  'Dukemtmc_Msmt_Xcf',\n",
       "  'Xe_Xbc',\n",
       "  'Reid',\n",
       "  'Market_Dukemtmc_Msmt'],\n",
       " ['Dhruv_Batra',\n",
       "  'Angel_Chang',\n",
       "  'Andrew_Ndavison',\n",
       "  'Jia_Deng_Vladlen_Koltun_Sergey_Levine_Jiten',\n",
       "  'Malik_Igor_Mordatch_Roozbeh_Mottaghi',\n",
       "  'Angela_Dai',\n",
       "  'Manolis_Savva',\n",
       "  'Zeng_Yinda_Zhang',\n",
       "  'Yu_Wei_Chao_Zhan_Wang_Yugeng',\n",
       "  'Deng',\n",
       "  'Hico',\n",
       "  'Weifeng_Chen_Shengyi_Qian',\n",
       "  'Noriyuki_Kojima',\n",
       "  'Hamilton_Jia_Deng',\n",
       "  'Christopher_Choy',\n",
       "  'Cristina_Garcia_Cifuentes',\n",
       "  'Jeannette_Bohg',\n",
       "  'Robotics',\n",
       "  'Angela_Dai_Angel_Chang_Manolis_Savva',\n",
       "  'Thomas_Funkhouser',\n",
       "  'Matthias_Nie',\n",
       "  'Scannet_Nrichly',\n",
       "  'Maria_Farinella_Nsanja_Fidler',\n",
       "  'Jonathan_Munro_Toby',\n",
       "  'Nmichael_Wray',\n",
       "  'Shiyang_Lu',\n",
       "  'Anthony_Opipari',\n",
       "  'David_Eigen',\n",
       "  'Rob_Fergus',\n",
       "  'Abhinav_Gupta_Nalexei',\n",
       "  'Efros_Ivan',\n",
       "  'Josef_Sivic',\n",
       "  'Gupta_Piotr',\n",
       "  'Ross_Girshick',\n",
       "  'Gkioxari_Piotr',\n",
       "  'Ross_Gir',\n",
       "  'Mask_Cnn',\n",
       "  'Vladimir_Niglovikov',\n",
       "  'Vladimir_Iglovikov_Alexey_Shvets',\n",
       "  'Phillip_Isola_Joseph',\n",
       "  'Lim_Edward',\n",
       "  'Ajinkya_Jain_Rudolf_Lioutikov_Caleb_Chuck',\n",
       "  'Ziyu_Jiang_Buyu_Liu',\n",
       "  'Samuel_Schulter_Zhangyang_Wang',\n",
       "  'Manmohan_Chandraker',\n",
       "  'Linyi_Jin_Shengyi',\n",
       "  'Qian_Andrew_Owens',\n",
       "  'David_Nfouhey',\n",
       "  'Hanbyul_Joo',\n",
       "  'Andrea_Vedaldi',\n",
       "  'Ken_Ichi_Kanatani',\n",
       "  'Alexander_Kirillov_Yuxin',\n",
       "  'Wu_Kaiming',\n",
       "  'Chun_Hao_Huang_Otmar_Hilliges',\n",
       "  'Michael_Black',\n",
       "  'Regressor_Nhuman',\n",
       "  'Hema_Swetha',\n",
       "  'Koppula_Rudhir',\n",
       "  'Gupta_Ashutosh_Sax',\n",
       "  'Nilesh_Kulkarni_Abhinav_Gupta',\n",
       "  'David',\n",
       "  'Fouhey_Weicheng_Kuo_Alexei',\n",
       "  'Efros_Jiten',\n",
       "  'Nilesh_Kulkarni_Ishan_Misra_Shubham_Tulsiani_Abhi',\n",
       "  'Girdhar_Fouhey',\n",
       "  'Gupta_Nlearning',\n",
       "  'Xiaolong_Li',\n",
       "  'Wang_Li',\n",
       "  'Lynn_Nabbott',\n",
       "  'Shuran_Song',\n",
       "  'Gkioxari_Ross_Girshick',\n",
       "  'Tsung_Yi_Lin',\n",
       "  'Ross_Girshick_Kaiming',\n",
       "  'Serge_Belongie',\n",
       "  'Michael_Maire_Serge_Belongie',\n",
       "  'James_Hays_Npietro',\n",
       "  'Deva_Ramanan_Piotr',\n",
       "  'Lawrence_Nzitnick',\n",
       "  'Chen_Liu_Kihwan',\n",
       "  'Kim_Jinwei',\n",
       "  'Yasutaka_Furukawa_Njan_Kautz',\n",
       "  'Chen_Liu',\n",
       "  'Yumer_Ya',\n",
       "  'Planenet_Piece',\n",
       "  'Liu_Liu',\n",
       "  'Xue_Wenqiang',\n",
       "  'Haoyuan_Fu_Cewu',\n",
       "  'Yizhou_Liu',\n",
       "  'Fusheng_Zha_Lining',\n",
       "  'Jingxuan_Li',\n",
       "  'Li_Xin_Wang',\n",
       "  'Access_Xe',\n",
       "  'David_Lowe',\n",
       "  'Frank_Michel_Alexander_Krull',\n",
       "  'Ying_Yang',\n",
       "  'Stefan_Gumhold',\n",
       "  'Carsten_Rother_Npose',\n",
       "  'Kaichun_Mo_Leonidas',\n",
       "  'Mustafa_Mukadam_Abhinav',\n",
       "  'Shubham_Tulsiani',\n",
       "  'Jiteng_Mu_Weichao',\n",
       "  'Qiu_Adam',\n",
       "  'Kortylewski',\n",
       "  'Alan_Yuille',\n",
       "  'Vasconcelos_Xiaolong_Wang',\n",
       "  'Sdf_Learning',\n",
       "  'Yinyu_Nie_Xiaoguang',\n",
       "  'Jian_Nchang',\n",
       "  'Jian_Jun_Zhang',\n",
       "  'Xarpino_Julie_Shah',\n",
       "  'Walter_Seth_Teller',\n",
       "  'Qian_Linyi',\n",
       "  'Jin_David',\n",
       "  'Associa',\n",
       "  'Volumetric',\n",
       "  'Gordon_Wan_Yen_Lo',\n",
       "  'Justin_Johnson',\n",
       "  'Shaoqing_Ren_Kaiming',\n",
       "  'Jian_Sun_Nfaster',\n",
       "  'Chris_Rockwell',\n",
       "  'David_Fouhey',\n",
       "  'Dandan_Shan',\n",
       "  'Jiaqi_Geng_Michelle_Shu_David_Fouhey_Nunderstanding',\n",
       "  'Varol_Xiaolong',\n",
       "  'Wang_Ali_Nfarhadi',\n",
       "  'Ivan',\n",
       "  'Laptev_Abhinav_Gupta',\n",
       "  'Gunnar_Sigurdsson',\n",
       "  'Linda_Smith',\n",
       "  'Michael_Gasser',\n",
       "  'Sturm_Cyrill',\n",
       "  'Wolfram_Burgard',\n",
       "  'Andrew_Szot_Alex',\n",
       "  'Eric_Undersander',\n",
       "  'Erik_Wijmans',\n",
       "  'John_Turner',\n",
       "  'Maestre_Mustafa_Mukadam',\n",
       "  'Zachary_Teed',\n",
       "  'Jia_Deng',\n",
       "  'Carlo_Tomasi_Takeo_Kanade',\n",
       "  'Xe_Lorenzo',\n",
       "  'Torresani',\n",
       "  'Aaron_Hertzmann',\n",
       "  'Chris_Bregler_Nnonrigid',\n",
       "  'Intelligence_Xe',\n",
       "  'Gupta_David_Fouhey',\n",
       "  'Efros_Jitendra_Malik',\n",
       "  'Martin_Naureen_Mah',\n",
       "  'Michael',\n",
       "  'Cordelia_Schmid_Nlearning',\n",
       "  'Gul_Varol_Javier',\n",
       "  'John_Ya',\n",
       "  'Wang_Edward_Adelson',\n",
       "  'Xiaolong_Wang_Ali_Farhadi_Abhinav_Gupta',\n",
       "  'Xiaolong_Wang',\n",
       "  'Abhinav_Gupta',\n",
       "  'Ethan_Rublee',\n",
       "  'Vincent_Rabaud',\n",
       "  'Konolige_Gary_Nbradski',\n",
       "  'Xiaolong_Wang_David',\n",
       "  'Fouhey_Abhinav_Gupta',\n",
       "  'De',\n",
       "  'Xiaogang_Wang_Bin_Zhou',\n",
       "  'Chen_Qin',\n",
       "  'Zhao_Kai_Xu',\n",
       "  'Yuxin_Wu',\n",
       "  'Wan_Yen',\n",
       "  'Fanbo_Xiang_Yuzhe',\n",
       "  'Qin_Kaichun',\n",
       "  'Yikuan_Xia_Hao_Nzhu',\n",
       "  'Fangchen_Liu',\n",
       "  'Minghua_Liu',\n",
       "  'Hanxiao_Jiang',\n",
       "  'Wang',\n",
       "  'Sapien',\n",
       "  'Xiang_Xu',\n",
       "  'Greg_Mori_Manolis_Savva',\n",
       "  'Fengting_Yang_Zihan_Zhou',\n",
       "  'Zehao_Yu',\n",
       "  'Jia_Zheng',\n",
       "  'Zihan_Zhou_Nshenghua_Gao',\n",
       "  'Rowan_Zellers',\n",
       "  'Bisk_Ali_Farhadi',\n",
       "  'Zhang_Panna',\n",
       "  'Felsen_Angjoo',\n",
       "  'Kanazawa_Jiten',\n",
       "  'Kai_Zhao',\n",
       "  'Chang_Bin_Zhang',\n",
       "  'Jun_Xu',\n",
       "  'Deep',\n",
       "  'Gpu_Nb',\n",
       "  'Nyoutube',\n",
       "  'Xcb_Laundry',\n",
       "  'Youtube',\n",
       "  'Faster',\n",
       "  'Fpn_Detectron',\n",
       "  'Nif_Video',\n",
       "  'Depth',\n",
       "  'Flatten_Linear',\n",
       "  'Conv',\n",
       "  'Nnormal',\n",
       "  'Ndepth_Noutput',\n",
       "  'Hive_Nannotating',\n",
       "  'Sam_Nple',\n",
       "  'Xe',\n",
       "  'Nwasher_Xe'],\n",
       " ['Balaji_Sankaranarayanan_Chellappa',\n",
       "  'Wiewel',\n",
       "  'Xaobler_Yang',\n",
       "  'Ben_David_Blitzer_Crammer_Kulesza',\n",
       "  'Benaim_Wolf',\n",
       "  'Bousmalis_Silberman_Dohan_Erhan_Krishnan',\n",
       "  'Chen',\n",
       "  'Papandreou_Kokkinos',\n",
       "  'Chen_Xue_Cai',\n",
       "  'Lin',\n",
       "  'Huang_Crdoco_Pixel',\n",
       "  'Cheng_Collins',\n",
       "  'Zhu_Liu_Huang',\n",
       "  'Adam_Chen_Npanoptic',\n",
       "  'Choi_Jung_Yun_Kim',\n",
       "  'Kim_Choo',\n",
       "  'Cohen_Shulman_Morgenstern_Mechrez_Farhan',\n",
       "  'Self',\n",
       "  'Ramos_Rehfeld',\n",
       "  'Benenson_Nfranke_Roth_Schiele',\n",
       "  'Dosovitskiy_Beyer',\n",
       "  'Zhai_Unterthiner_Nt',\n",
       "  'Dehghani_Minderer_Heigold_Gelly',\n",
       "  'Dubey_Ramanathan_Pentland_Mahajan',\n",
       "  'Dundar_Liu',\n",
       "  'Zedlewski_Kautz',\n",
       "  'Fan_Wang',\n",
       "  'Ke_Yang',\n",
       "  'Zhou',\n",
       "  'Ganin_Ustinova',\n",
       "  'Larochelle_Laviolette_Nmarchand_Lempitsky',\n",
       "  'Guo_Pleiss',\n",
       "  'Sun_Weinberger',\n",
       "  'Hendrycks_Basart',\n",
       "  'Zhu_Parajuli',\n",
       "  'Hendrycks_Mu_Cubuk',\n",
       "  'Zoph_Gilmer_Lakshminarayanan',\n",
       "  'Hoffman_Tzeng_Park_Zhu',\n",
       "  'Efros_Darrell_Not',\n",
       "  'Cycada_Cycle',\n",
       "  'Hu_Uzunbas',\n",
       "  'Chen_Wang_Shah_Nevatia_Lim_Nmixnorm',\n",
       "  'Huang_Zhou',\n",
       "  'Zhu_Liu_Shao',\n",
       "  'Ioffe_Szegedy',\n",
       "  'Kirillov_He_Girshick_Rother_Doll',\n",
       "  'Li_Zhang_Yang_Liu_Song',\n",
       "  'Hospedales_Episodic',\n",
       "  'Li_Wang_Shi_Liu_Hou',\n",
       "  'Liang_Hu_Feng',\n",
       "  'Long_Cao_Wang_Jordan',\n",
       "  'Luo_Liu_Guan_Yu_Yang',\n",
       "  'Luo_Zheng_Guan_Yu_Yang',\n",
       "  'Ma_Zhang_Zheng',\n",
       "  'Matsuura_Harada',\n",
       "  'Micorek_Possegger_Bischof',\n",
       "  'Lakshminarayanan_Snoek',\n",
       "  'Neuhold_Ollmann_Rota',\n",
       "  'Pan_Luo_Shi_Tang',\n",
       "  'Pan_Zhan_Shi_Tang_Luo',\n",
       "  'Qiao_Zhao_Peng',\n",
       "  'Vineet_Roth_Koltun',\n",
       "  'Russakovsky_Deng_Su_Krause_Satheesh',\n",
       "  'Huang_Karpa',\n",
       "  'Khosla_Bernstein',\n",
       "  'Sakaridis_Dai_Van_Gool',\n",
       "  'Xe_Sep',\n",
       "  'Sandler_Howard_Zhu',\n",
       "  'Zhmoginov_Chen_Mobilenetv',\n",
       "  'Schneider_Rusak',\n",
       "  'Eck_Bringmann_Brendel_Bethge',\n",
       "  'Wang_Webb',\n",
       "  'Sun_Saenko',\n",
       "  'Sun_Wang_Liu_Miller',\n",
       "  'Efros_Hardt',\n",
       "  'Theis_Shi_Cunningham_Husz',\n",
       "  'Tzeng_Hoffman',\n",
       "  'Saenko_Darrell',\n",
       "  'Jain_Bucher_Cord',\n",
       "  'Xberez',\n",
       "  'Advent_Adversarial',\n",
       "  'Wang_Shelhamer',\n",
       "  'Liu_Olshausen_Darrell',\n",
       "  'Wu_Johnson',\n",
       "  'Rethink_Xe',\n",
       "  'Nguyen_Fuchs_Lipson',\n",
       "  'Yu_Chen',\n",
       "  'Wang_Xian',\n",
       "  'Chen_Liu_Madhavan_Darrell_Not',\n",
       "  'Steininger_Dominguez_Wilddash',\n",
       "  'Zhao_Liu',\n",
       "  'Peng_Metaxas',\n",
       "  'Zhao_Gong_Liu_Fu_Tao',\n",
       "  'Zhu',\n",
       "  'Park_Isola',\n",
       "  'Zou_Yu_Kumar_Wang',\n",
       "  'Zou_Yu_Liu',\n",
       "  'Kumar_Wang',\n",
       "  'Xc_Xe',\n",
       "  'Xe_Instcal',\n",
       "  'Chen_Et_Al',\n",
       "  'Gpu',\n",
       "  'Pytorch',\n",
       "  'Njensen_Shannon',\n",
       "  'Nc_Qualitative',\n",
       "  'Cityscape',\n",
       "  'Mapillary_Wilddash',\n",
       "  'Foggy_Cityscapes'],\n",
       " ['Dragomir_Anguelov',\n",
       "  'Srinivasan_Daphne_Koller_Se',\n",
       "  'Jim_Rodgers_James_Davis',\n",
       "  'Bednarik_Vladimir',\n",
       "  'Kim_Siddhartha',\n",
       "  'Jens_Behrmann',\n",
       "  'Chen_David_Du',\n",
       "  'Henrik_Jacobsen',\n",
       "  'Thomas_Vetter',\n",
       "  'Federica_Bogo',\n",
       "  'Michael_Zollhofer_Justus_Thies',\n",
       "  'Ricky_Tq',\n",
       "  'Chen_Yulia',\n",
       "  'Rubanova_Jesse',\n",
       "  'Ndavid_Duvenaud',\n",
       "  'Xu_Chen_Yufeng_Zheng',\n",
       "  'Michael_Black_Otmar_Hilliges',\n",
       "  'Andreas_Geiger',\n",
       "  'Zhiqin_Chen_Hao_Zhang',\n",
       "  'Learn',\n",
       "  'Cheng_Chi_Shuran_Song',\n",
       "  'Boyang_Deng',\n",
       "  'John_Lewis',\n",
       "  'Timothy_Jeruzalski_Gerard',\n",
       "  'Nconference_Glasgow',\n",
       "  'Yu_Deng',\n",
       "  'Jiaolong_Yang_Xin_Tong',\n",
       "  'David_Krueger_Yoshua_Bengio',\n",
       "  'Jascha_Sohl_Dickstein_Samy_Ben',\n",
       "  'Kyle_Genova',\n",
       "  'Cole_Avneesh_Sud_Aaron_Sarna',\n",
       "  'Thomas_Funkhouser',\n",
       "  'Deep',\n",
       "  'Murray_Hugo_Nlarochelle',\n",
       "  'Vladimir_Kim',\n",
       "  'Russell_Mathieu',\n",
       "  'Xach',\n",
       "  'Bharath_Hariharan',\n",
       "  'Guandao_Yang',\n",
       "  'Serge_Belongie_Nvladlen_Koltun',\n",
       "  'Neural_Mesh_Flow_Manifold_Mesh_Gen',\n",
       "  'Diffeomorphic_Flows',\n",
       "  'Jingwei_Huang',\n",
       "  'Chiyu_Max_Jiang_Baiqiang_Leng_Bin_Nwang_Leonidas_Guibas',\n",
       "  'Meshode',\n",
       "  'Boyan_Jiang',\n",
       "  'Yinda_Zhang',\n",
       "  'Wei_Xiangyang_Xue_Nlearning',\n",
       "  'Yanwei_Fu',\n",
       "  'Chiyu_Jiang',\n",
       "  'Huang_Andrea',\n",
       "  'Tagliasacchi_Leonidas_Nguibas',\n",
       "  'Chiyu_Jiang_Avneesh_Sud_Ameesh_Makadia',\n",
       "  'Korrawe_Karunratanakul',\n",
       "  'Diederik_Kingma_Prafulla_Dhariwal',\n",
       "  'Glow_Gener',\n",
       "  'Zihang_Lai_Sifei',\n",
       "  'Liu_Alexei',\n",
       "  'Efros_Xiaolong_Nwang',\n",
       "  'Tianye_Li',\n",
       "  'Timo_Bolkart',\n",
       "  'Michael_Black',\n",
       "  'Hao_Li_Javier_Nromero',\n",
       "  'Graph_Xe',\n",
       "  'Matthew_Loper_Naureen_Mahmood',\n",
       "  'Pons_Moll_Michael',\n",
       "  'Qianli_Ma',\n",
       "  'Yang_Anurag_Ranjan',\n",
       "  'Lars',\n",
       "  'Michael_Oechsle',\n",
       "  'Michael_Niemeyer_Se',\n",
       "  'Nowozin_Andreas_Geiger',\n",
       "  'Ben_Mildenhall_Pratul',\n",
       "  'Srinivasan_Matthew_Tancik_Njonathan',\n",
       "  'Jiteng_Mu_Weichao',\n",
       "  'Qiu_Adam',\n",
       "  'Kortylewski',\n",
       "  'Alan_Yuille',\n",
       "  'Vasconcelos_Xiaolong_Wang',\n",
       "  'Sdf_Learning',\n",
       "  'Michael_Niemeyer',\n",
       "  'Peng_Andreas_Geiger',\n",
       "  'Palafox_Alja',\n",
       "  'Angela_Dai',\n",
       "  'George_Papamakarios_Theo_Pavlakou',\n",
       "  'Jeong_Joon_Park',\n",
       "  'Richard_Nnewcombe',\n",
       "  'Steven_Lovegrove',\n",
       "  'Barron_Sofien_Nbouaziz',\n",
       "  'Dan_Goldman',\n",
       "  'Steven_Seitz',\n",
       "  'Ricardo_Nmartin_Brualla',\n",
       "  'Despoina_Paschalidou_Angelos_Katharopoulos_Andreas_Ngeiger_Sanja_Fidler',\n",
       "  'Songyou_Peng',\n",
       "  'Marc_Npollefeys_Andreas_Geiger',\n",
       "  'Albert_Pumarola',\n",
       "  'Gerard_Pons_Moll_Nfrancesc_Moreno_Noguer',\n",
       "  'Charles_Qi_Hao_Su_Kaichun_Mo_Leonidas_Guibas',\n",
       "  'Davis_Rempe_Tolga',\n",
       "  'Yongheng_Zhao',\n",
       "  'Caspr_Learning',\n",
       "  'Tzionas_Michael',\n",
       "  'Black_Nembodied',\n",
       "  'Saito_Jinlong',\n",
       "  'Yang',\n",
       "  'Michael_Nblack',\n",
       "  'Robert_Sumner_Johannes',\n",
       "  'Xe_X',\n",
       "  'Jiapeng_Tang_Dan_Xu_Kui_Jia_Lei_Zhang',\n",
       "  'Garvita_Tiwari',\n",
       "  'Tony_Tung_Ger',\n",
       "  'Pons_Moll',\n",
       "  'Shaofei_Wang_Marko',\n",
       "  'Mihajlovic',\n",
       "  'Ngeiger_Siyu_Tang',\n",
       "  'Metaavatar_Learning',\n",
       "  'Xiaogang_Wang_Bin_Zhou',\n",
       "  'Chen_Qin',\n",
       "  'Zhao_Kai_Xu',\n",
       "  'Yiheng_Xie_Towaki_Takikawa',\n",
       "  'Yan_Numair',\n",
       "  'James_Tomp',\n",
       "  'Vincent_Sitzmann',\n",
       "  'Zhenjia_Xu',\n",
       "  'Jiajun_Wu',\n",
       "  'Song_Nlearning',\n",
       "  'Guandao_Yang_Xun_Huang_Zekun_Hao_Ming_Yu_Liu',\n",
       "  'Takafumi_Taketomi',\n",
       "  'Yang_Li_Hikari',\n",
       "  'Zheng',\n",
       "  'Matthias_Nie',\n",
       "  'Tarun_Yenamandra_Ayush_Tewari',\n",
       "  'Bernard_Hans_Npeter_Seidel_Mohamed_Elgharib',\n",
       "  'Nchristian_Theobalt',\n",
       "  'Zerong_Zheng',\n",
       "  'Tao_Yu',\n",
       "  'Qionghai_Dai_Yebin_Liu'],\n",
       " ['Daniel_Matthieu',\n",
       "  'Brostow_Jamie_Shotton',\n",
       "  'Fauqueur',\n",
       "  'Roberto_Cipolla',\n",
       "  'Xe_Liang_Chieh',\n",
       "  'Chen_George_Papandreou',\n",
       "  'Kevin_Murphy',\n",
       "  'Liang_Chieh',\n",
       "  'Tpami_Liang',\n",
       "  'Chen_Yukun',\n",
       "  'Zhu_George_Papandreou',\n",
       "  'Ruyi_Feng_Lizhe_Wang',\n",
       "  'Xzg',\n",
       "  'Ahmed_Abdulkadir_Soeren',\n",
       "  'Taha_Emara_Hossam',\n",
       "  'Abd_El_Munim_Hazem',\n",
       "  'Abbas_Liteseg_Nnovel_Lightweight_Convnet_Semantic_Segmentation',\n",
       "  'Marco_Giordano',\n",
       "  'Michele_Magno_Battery_Free_Long_Range_Wireless',\n",
       "  'Changlu_Guo',\n",
       "  'Buer_Chen_Nchangqi_Fan',\n",
       "  'Matting_Human_Datasets',\n",
       "  'Accessed_Huimin',\n",
       "  'Lin_Ruofeng',\n",
       "  'Tong_Hongjie_Hu',\n",
       "  'Qiaowei_Zhang',\n",
       "  'Han_Yen',\n",
       "  'Wei_Chen_Jian_Wu_Unet_Full_Nscale_Connected_Unet_Medical_Image_Segmentation',\n",
       "  'Kligys_Bo',\n",
       "  'Chen_Menglong_Zhu_Matthew_Tang_Andrew',\n",
       "  'Sheng_Lian',\n",
       "  'Luo_Zhun_Zhong',\n",
       "  'Xiang_Lin',\n",
       "  'Songzhi_Su_Shaozi_Nli',\n",
       "  'Guosheng_Lin_Anton_Milan',\n",
       "  'Chunhua',\n",
       "  'Ian_Reid_Refinenet',\n",
       "  'Zhong_Qiu_Lin_Brendan',\n",
       "  'Alexander_Wong_Edgesegnet',\n",
       "  'Wei_Liu_Andrew_Rabinovich_Alexander',\n",
       "  'Ziwei_Liu_Xiaoxiao_Li_Ping_Luo_Chen_Change_Loy_Xiaoou_Tang',\n",
       "  'Jonathan_Long_Evan_Shelhamer_Trevor_Darrell',\n",
       "  'Mohammad_Rastegari',\n",
       "  'Anat_Caspi',\n",
       "  'Linda_Shapiro_Hannaneh',\n",
       "  'Milletari_Nassir',\n",
       "  'Ahmad_Ahmadi_Net_Fully_Nconvolutional',\n",
       "  'Shervin_Minaee_Yuri',\n",
       "  'Porikli_Antonio_Plaza_Nasser_Kehtar',\n",
       "  'Demetri_Terzopoulos',\n",
       "  'Pratim_Ray_Review',\n",
       "  'Xe_Miccai_Nassir',\n",
       "  'Joachim_Horneg',\n",
       "  'William',\n",
       "  'Mark_Sandler',\n",
       "  'Andrew_Howard_Menglong',\n",
       "  'Zhu_Andrey',\n",
       "  'Zhmoginov_Liang_Nchieh',\n",
       "  'Okman_Ultra',\n",
       "  'Xiaoyu_Wen_Mahmoud',\n",
       "  'Andrew_Hryniowski',\n",
       "  'Alexander_Wong',\n",
       "  'Yu_Weng',\n",
       "  'Zhou',\n",
       "  'Yujie_Li',\n",
       "  'Ke_Zhenxin',\n",
       "  'Zhang_Mingli',\n",
       "  'Peng_Li',\n",
       "  'Nshuangyue_Zhang_Urban_Land_Use_Land_Cover_Classification']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All function in one\n",
    "\n",
    "entities=[]\n",
    "def main():\n",
    "    for text in elements:\n",
    "        temp=after_references(str(text))\n",
    "        temp=preprocess_text(temp)\n",
    "        temp=extract(temp)\n",
    "        entities.append(temp)\n",
    "    return entities\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897bff1",
   "metadata": {},
   "source": [
    "## Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "164621c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready import *\n",
    "\n",
    "onto_path.append(\"owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8470daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Creating new ontology onto <http://test.org/onto.owl>.\n"
     ]
    }
   ],
   "source": [
    "onto = Ontology(\"http://test.org/onto.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aad8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class References(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class quoted_by(Property):\n",
    "    ontolgy = onto\n",
    "    domain = [References]\n",
    "    range = [Author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd4f26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.sax.saxutils import escape\n",
    "\n",
    "def invalid_xml_remove(char):\n",
    "    \"\"\"Tracks illegal unicode characters\"\"\"\n",
    "    #http://stackoverflow.com/questions/1707890\n",
    "    # /fast-way-to-filter-illegal-xml-unicode-chars-in-python\n",
    "    illegal_unichrs = [ (0x00, 0x08), (0x0B, 0x1F), (0x7F, 0x84), (0x86, 0x9F),\n",
    "                    (0xD800, 0xDFFF), (0xFDD0, 0xFDDF), (0xFFFE, 0xFFFF),\n",
    "                    (0x1FFFE, 0x1FFFF), (0x2FFFE, 0x2FFFF), (0x3FFFE, 0x3FFFF),\n",
    "                    (0x4FFFE, 0x4FFFF), (0x5FFFE, 0x5FFFF), (0x6FFFE, 0x6FFFF),\n",
    "                    (0x7FFFE, 0x7FFFF), (0x8FFFE, 0x8FFFF), (0x9FFFE, 0x9FFFF),\n",
    "                    (0xAFFFE, 0xAFFFF), (0xBFFFE, 0xBFFFF), (0xCFFFE, 0xCFFFF),\n",
    "                    (0xDFFFE, 0xDFFFF), (0xEFFFE, 0xEFFFF), (0xFFFFE, 0xFFFFF),\n",
    "                    (0x10FFFE, 0x10FFFF) ]\n",
    "\n",
    "    illegal_ranges = [f\"{chr(low)}-{chr(high)}\"\n",
    "                  for (low, high) in illegal_unichrs\n",
    "                  if low < sys.maxunicode]\n",
    "\n",
    "    illegal_xml_re = re.compile(f'[{\"\".join(illegal_ranges)}]')\n",
    "    if illegal_xml_re.search(char) is not None:\n",
    "        #Replace with space\n",
    "        return ''\n",
    "    else:\n",
    "        return char\n",
    "\n",
    "def clean_char(char):\n",
    "    \"\"\"\n",
    "    Function for remove invalid XML characters from\n",
    "    incoming data.\n",
    "    \"\"\"\n",
    "    #Get rid of the ctrl characters first.\n",
    "    #http://stackoverflow.com/questions/1833873/python-regex-escape-characters\n",
    "    char = re.sub('\\x1b[^m]*m', '', char)\n",
    "    #Clean up invalid xml\n",
    "    char = invalid_xml_remove(char)\n",
    "    replacements = [\n",
    "        ('\\u201c', '\\\"'),\n",
    "        ('\\u0141', '\\\"'),\n",
    "        ('\\u201d', '\\\"'),\n",
    "        (\"\\u001B\", ''), #http://www.fileformat.info/info/unicode/char/1b/index.htm\n",
    "        (\"\\u0019\", ''), #http://www.fileformat.info/info/unicode/char/19/index.htm\n",
    "        (\"\\u0016\", ''), #http://www.fileformat.info/info/unicode/char/16/index.htm\n",
    "        (\"\\u001C\", ''), #http://www.fileformat.info/info/unicode/char/1c/index.htm\n",
    "        (\"\\u0003\", ''), #http://www.utf8-chartable.de/unicode-utf8-table.pl?utf8=0x\n",
    "        (\"\\u000C\", ''),\n",
    "        (\"\\u03b1\", ''),\n",
    "        (\"u\\u039C\", ''),\n",
    "        (\"\\u03C3\", ''),\n",
    "        (\"\\u0141\", ''),\n",
    "        (\"\\u0308\", ''),\n",
    "        (\"\\u2032\", ''),\n",
    "        (\"\\u03b8\", '')\n",
    "\n",
    "    \n",
    "    ]\n",
    "    for rep, new_char in replacements:\n",
    "        if char == rep:\n",
    "            return new_char\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4794f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_value(text: str):\n",
    "        \"\"\"Escape the illegal characters for an ontology property\"\"\"\n",
    "        if text is None:\n",
    "            return None\n",
    "        # function to escape XML character data\n",
    "        text = escape(text)\n",
    "        text = text.replace('\\n', '')\n",
    "        text = text.replace('\\r', '')\n",
    "        text = text.replace('\\f', '')\n",
    "        text = text.replace('\\b', '')\n",
    "        text = text.replace('\"', '')\n",
    "        text = text.replace('[', '')\n",
    "        text = text.replace(']', '')\n",
    "        text = text.replace('{', '')\n",
    "        text = text.replace('}', '')\n",
    "        text = text.replace('#', '')\n",
    "        text = text.replace('|', '')\n",
    "        text = text.replace(' ', '_')\n",
    "        text = clean_char(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2126e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dengpan.Fu]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Dongdong.Chen]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Hao.Yang]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Jianmin.Bao]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lu.Yuan]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Lei.Zhang]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Houqiang.Li]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Fang.Wen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Dong.Chen]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Shengyi.Qian]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Linyi.Jin]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Chris.Rockwell]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.Siyi.Chen]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.David.F..Fouhey]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Yuliang.Zou]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Zizhao.Zhang]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Chun.Liang.Li]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Han.Zhang]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Tomas.Pfister]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jia.Bin.Huang]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Jiahui.Lei]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Kostas.Daniilidis]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Osman.Erman.Okman]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Mehmet.Gorkem.Ulkar]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n",
      "[onto.Gulnur.Selda.Uyanik]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(array_authors)):\n",
    "    for j in range(len(array_authors[i])):\n",
    "        aut_num = Author(escape_value(array_authors[i][j]))\n",
    "        for z in range(len(entities[i])):\n",
    "            ref_num = References(escape_value(entities[i][z]))\n",
    "            ref_num.quoted_by.append(aut_num)\n",
    "            print(ref_num.quoted_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e27e8710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Saving ontology onto to owl\\onto.owl...\n"
     ]
    }
   ],
   "source": [
    "onto.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
