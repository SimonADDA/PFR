{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126a7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdftotext in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (2.2.2)\n",
      "Collecting arxiv\n",
      "  Using cached arxiv-1.4.2-py3-none-any.whl (11 kB)\n",
      "Collecting feedparser\n",
      "  Using cached feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
      "Collecting sgmllib3k\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-1.4.2 feedparser-6.0.8 sgmllib3k-1.0.0\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.2.3-cp38-cp38-win_amd64.whl (11.6 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Collecting pathy>=0.3.5\n",
      "  Using cached pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Using cached pydantic-1.8.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.63.1-py2.py3-none-any.whl (76 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Using cached wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Using cached typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Using cached spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.6-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Using cached srsly-2.4.2-cp38-cp38-win_amd64.whl (452 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.6-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Collecting numpy>=1.15.0\n",
      "  Using cached numpy-1.22.3-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.6-cp38-cp38-win_amd64.whl (113 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Using cached thinc-8.0.15-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: numpy, murmurhash, cymem, click, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, idna, charset-normalizer, blis, tqdm, thinc, spacy-loggers, spacy-legacy, requests, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 charset-normalizer-2.0.12 click-8.0.4 cymem-2.0.6 idna-3.3 langcodes-3.3.0 murmurhash-1.0.6 numpy-1.22.3 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 requests-2.27.1 smart-open-5.2.1 spacy-3.2.3 spacy-legacy-3.0.9 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.15 tqdm-4.63.1 typer-0.4.0 wasabi-0.9.0\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nameparser\n",
      "  Using cached nameparser-1.1.1-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: nameparser\n",
      "Successfully installed nameparser-1.1.1\n",
      "Collecting pdfminer\n",
      "  Using cached pdfminer-20191125-py3-none-any.whl\n",
      "Collecting pycryptodome\n",
      "  Using cached pycryptodome-3.14.1-cp35-abi3-win_amd64.whl (1.8 MB)\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.14.1\n",
      "Collecting refextract\n",
      "  Using cached refextract-1.1.4-py3-none-any.whl (358 kB)\n",
      "Requirement already satisfied: six>=1.10.0,~=1.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from refextract) (1.16.0)\n",
      "Collecting autosemver>=0.5.3,~=0.0\n",
      "  Using cached autosemver-0.5.5-py3-none-any.whl\n",
      "Collecting python-magic>=0.4.15,~=0.0\n",
      "  Using cached python_magic-0.4.25-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: requests>=2.18.4,~=2.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from refextract) (2.27.1)\n",
      "Collecting unidecode>=1.0.22,~=1.0\n",
      "  Using cached Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
      "Collecting inspire-utils>=3.0.0,~=3.0\n",
      "  Using cached inspire_utils-3.0.24-py3-none-any.whl\n",
      "Collecting PyPDF2>=1.26.0,~=1.0\n",
      "  Using cached PyPDF2-1.26.0-py3-none-any.whl\n",
      "Collecting dulwich<0.20,>=0.19.6\n",
      "  Using cached dulwich-0.19.16-py3-none-any.whl (428 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from dulwich<0.20,>=0.19.6->autosemver>=0.5.3,~=0.0->refextract) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from dulwich<0.20,>=0.19.6->autosemver>=0.5.3,~=0.0->refextract) (1.26.9)\n",
      "Collecting lxml>=4.4.0,~=4.0\n",
      "  Using cached lxml-4.8.0-cp38-cp38-win_amd64.whl (3.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1,~=2.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from inspire-utils>=3.0.0,~=3.0->refextract) (2.8.2)\n",
      "Collecting babel>=2.5.1,~=2.0\n",
      "  Using cached Babel-2.9.1-py2.py3-none-any.whl (8.8 MB)\n",
      "Collecting nameparser>=0.5.3,~=0.0\n",
      "  Using cached nameparser-0.5.8-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pytz>=2015.7\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from requests>=2.18.4,~=2.0->refextract) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from requests>=2.18.4,~=2.0->refextract) (2.0.12)\n",
      "Installing collected packages: pytz, unidecode, nameparser, lxml, dulwich, babel, python-magic, PyPDF2, inspire-utils, autosemver, refextract\n",
      "  Attempting uninstall: nameparser\n",
      "    Found existing installation: nameparser 1.1.1\n",
      "    Uninstalling nameparser-1.1.1:\n",
      "      Successfully uninstalled nameparser-1.1.1\n",
      "Successfully installed PyPDF2-1.26.0 autosemver-0.5.5 babel-2.9.1 dulwich-0.19.16 inspire-utils-3.0.24 lxml-4.8.0 nameparser-0.5.8 python-magic-0.4.25 pytz-2022.1 refextract-1.1.4 unidecode-1.3.4\n",
      "Requirement already satisfied: pdfx in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: chardet==4.0.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from pdfx) (4.0.0)\n",
      "Requirement already satisfied: pdfminer.six==20201018 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from pdfx) (20201018)\n",
      "Requirement already satisfied: cryptography in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from pdfminer.six==20201018->pdfx) (36.0.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from pdfminer.six==20201018->pdfx) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from cryptography->pdfminer.six==20201018->pdfx) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from cffi>=1.12->cryptography->pdfminer.six==20201018->pdfx) (2.21)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Collecting nltk>=3.1\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2022.3.15-cp38-cp38-win_amd64.whl (274 kB)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from nltk>=3.1->textblob) (4.63.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Installing collected packages: regex, joblib, nltk, textblob\n",
      "Successfully installed joblib-1.1.0 nltk-3.7 regex-2022.3.15 textblob-0.17.1\n",
      "Collecting owlready\n",
      "  Using cached Owlready-0.3.1-py3-none-any.whl\n",
      "Installing collected packages: owlready\n",
      "Successfully installed owlready-0.3.1\n",
      "Requirement already satisfied: boto3 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (1.21.27)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from boto3) (1.0.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from boto3) (0.5.2)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.27 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from boto3) (1.24.27)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from botocore<1.25.0,>=1.24.27->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from botocore<1.25.0,>=1.24.27->boto3) (1.26.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\pfr\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.27->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!!! Install a conda package in the current Jupyter kernel !!!\n",
    "!pip install pdftotext\n",
    "!pip install arxiv\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install nameparser\n",
    "!pip install pdfminer\n",
    "!pip install refextract\n",
    "!pip install pdfx\n",
    "!pip install -U textblob\n",
    "!pip install owlready\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2ac646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "* Owlready * Creating new ontology owl <http://www.w3.org/2002/07/owl>.\n",
      "* Owlready * Creating new ontology 22-rdf-syntax-ns <http://www.w3.org/1999/02/22-rdf-syntax-ns>.\n",
      "* Owlready * Creating new ontology rdf-schema <http://www.w3.org/2000/01/rdf-schema>.\n",
      "* Owlready * Creating new ontology XMLSchema <http://www.w3.org/2001/XMLSchema>.\n",
      "* Owlready * Creating new ontology anonymous <http://anonymous>.\n",
      "* Owlready * Creating new ontology owlready_ontology <http://www.lesfleursdunormal.fr/static/_downloads/owlready_ontology.owl>.\n",
      "* Owlready *     ...loading ontology owlready_ontology from C:\\Users\\Admin\\anaconda3\\envs\\pfr\\lib\\site-packages\\owlready\\owlready_ontology.owl...\n"
     ]
    }
   ],
   "source": [
    "#Import of librairies\n",
    "\n",
    "#ARVIX\n",
    "import arxiv\n",
    "\n",
    "#PDFTEXT\n",
    "import urllib.request\n",
    "import pdftotext\n",
    "\n",
    "# Prevent future/deprecation warnings from showing in output\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#Local\n",
    "import os\n",
    "import requests as r\n",
    "import sys\n",
    "\n",
    "#AI\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk.tag.stanford as st\n",
    "from nltk.tag.stanford import StanfordNERTagger \n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "import boto3\n",
    "\n",
    "from owlready import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf6f98",
   "metadata": {},
   "source": [
    "## Download pdf file from Arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63be17ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203.13817v1\n",
      "2203.13815v1\n",
      "2203.13814v1\n",
      "2203.13812v1\n",
      "2203.13809v1\n"
     ]
    }
   ],
   "source": [
    "#You can choose here how many pdf you want. Here 5 because it is enough to see the result\n",
    "max_results=5\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = \"Computer Science & AI\",\n",
    "    max_results = max_results,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "for result in search.results():\n",
    "    print(result.pdf_url[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b3ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link:  ['http://arxiv.org/pdf/2203.13817v1', 'http://arxiv.org/pdf/2203.13815v1', 'http://arxiv.org/pdf/2203.13814v1', 'http://arxiv.org/pdf/2203.13812v1', 'http://arxiv.org/pdf/2203.13809v1']\n",
      "Authors:  [['Ziqian.Bai', 'Timur.Bagautdinov', 'Javier.Romero', 'Michael.Zollh.fer', 'Ping.Tan', 'Shunsuke.Saito'], ['Fangzhou.Hong', 'Liang.Pan', 'Zhongang.Cai', 'Ziwei.Liu'], ['Yuki.Kamiya', 'Tetsuo.Hyodo', 'Akira.Ohnishi'], ['Ritika.Chakraborty', 'Nikola.Popovic', 'Danda.Pani.Paudel', 'Thomas.Probst', 'Luc.Van.Gool'], ['Simon.Jones', 'Emma.Milner', 'Mahesh.Sooriyabandara', 'Sabine.Hauert']]\n",
      "Title:  ['AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling', 'Versatile Multi-Modal Pre-Training for Human-Centric Perception', 'Femtoscopic study on $DD^*$ and $D\\\\bar{D}^*$ interactions for $T_{cc}$ and $X(3872)$', 'Spatially Multi-conditional Image Generation', 'DOTS: An Open Testbed for Industrial Swarm Robotic Solutions']\n"
     ]
    }
   ],
   "source": [
    "#Store in array all informations on pdf (title, authors and links)\n",
    "\n",
    "array_link=[]\n",
    "array_authors=[]\n",
    "array_title=[]\n",
    "\n",
    "for result in search.results():\n",
    "    array_link.append(result.pdf_url)\n",
    "    array_title.append(result.title)\n",
    "    temp=result.authors\n",
    "    array_authors.append([re.sub(\"[^A-Za-z0-9]\",\".\",str(i)) for i in temp])\n",
    "    #Download pdf as link.pdf\n",
    "    result.download_pdf(dirpath=\".\\Download\", filename=f'{result.pdf_url[21:]}.pdf')\n",
    "    \n",
    "print(\"Link: \",array_link)\n",
    "print(\"Authors: \",array_authors)\n",
    "print(\"Title: \",array_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0bf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pdf link to a pdf text to work on it\n",
    "\n",
    "def load_pdf(value):\n",
    "    with open(f'Download/{value}.pdf', \"rb\") as f:\n",
    "        text = pdftotext.PDF(f)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c06a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                AutoAvatar: Autoregressive Neural Fields\\r\\n                     for Dynamic Avatar Modeling\\r\\n\\r\\n        Ziqian Bai1,2∗        Timur Bagautdinov2 Javier Romero2             Michael Zollhöfer2\\r\\n                                    Ping Tan1 Shunsuke Saito2\\r\\n                         1                              2\\r\\n                             Simon Fraser University        Reality Labs Research\\r\\n\\r\\n\\r\\n\\r\\n           Abstract. Neural fields such as implicit surfaces have recently enabled avatar\\r\\n           modeling from raw scans without explicit temporal correspondences. In this work,\\r\\n           we exploit autoregressive modeling to further extend this notion to capture dynamic\\r\\n           effects, such as soft-tissue deformations. Although autoregressive models are\\r\\n           naturally capable of handling dynamics, it is non-trivial to apply them to implicit\\r\\n           representations, as explicit state decoding is infeasible due to prohibitive memory\\r\\n           requirements. In this work, for the first time, we enable autoregressive modeling of\\r\\n           implicit avatars. To reduce the memory bottleneck and efficiently model dynamic\\r\\n           implicit surfaces, we introduce the notion of articulated observer points, which\\r\\n           relate implicit states to the explicit surface of a parametric human body model.\\r\\n           We demonstrate that encoding implicit surfaces as a set of height fields defined on\\r\\n           articulated observer points leads to significantly better generalization compared\\r\\n           to a latent representation. The experiments show that our approach outperforms\\r\\n           the state of the art, achieving plausible dynamic deformations even for unseen\\r\\n           motions. https://zqbai-jeremy.github.io/autoavatar.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n1       Introduction\\r\\nAnimatable 3D human body models are key enablers for various applications ranging\\r\\nfrom virtual try-on to social telepresence [4]. While modeling of human avatars from\\r\\n3D scans without surface registration is gaining more and more attention in recent\\r\\nyears [43,24,48,8,26], complex temporal dynamics are often completely ignored and the\\r\\nresulting deformations are often treated exclusively as a function of the pose parameters.\\r\\nHowever, the body shape is not uniquely determined by the current pose of the human,\\r\\nbut also depends on the history of shape deformations due to secondary motion effects.\\r\\nThe goal of our work is to realistically model these history-dependent dynamic effects\\r\\nfor human bodies without requiring precise surface registration.\\r\\n    To this end, we propose AutoAvatar, a novel autoregressive model for dynamically\\r\\ndeforming human bodies. AutoAvatar models body geometry implicitly - using a signed\\r\\ndistance field (SDF) - and is able to directly learn from raw scans without requiring\\r\\ntemporal correspondences for supervision. In addition, akin to physics-based simulation,\\r\\nAutoAvatar infers the complete shape of an avatar given history of shape and motion. The\\r\\n    ∗\\r\\n        Work done while Ziqian Bai was an intern at Reality Labs Research, Pittsburgh, PA, USA.\\r\\n\\x0c\\n\\n2        Z. Bai et al.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 1: AutoAvatar. Given raw 4D scans with self-intersections, holes, and noise (grey meshes)\\r\\nand fitted SMPL models (blue meshes), AutoAvatar automatically learns highly detailed animatable\\r\\nbody models with plausible secondary motion dynamics without requiring a personalized template\\r\\nor surface registration (right).\\r\\n\\r\\naforementioned properties lead to a generalizable method that models complex dynamic\\r\\neffects including inertia and elastic deformations without requiring a personalized\\r\\ntemplate or precise temporal correspondences across training frames.\\r\\n    To model temporal dependencies in the data, prior work has typically resorted to\\r\\nautoregressive models [39,22,28,45]. While the autoregressive framework naturally\\r\\nallows for incorporation of temporal information, combining it with neural implicit\\r\\nsurface representations [29,36,9] for modeling human bodies is non-trivial. Unlike\\r\\nexplicit shape representations, such neural representations implicitly encode the shape in\\r\\nthe parameters of the neural network and latent codes. Thus, in practice, producing the\\r\\nactual shape requires expensive neural network evaluation at each voxel of a dense spatial\\r\\ngrid [36]. This aspect is particularly problematic for autoregressive modeling, since\\r\\nmost of the successful autoregressive models rely on rollout training [28,19] to ensure\\r\\nstability of both training and inference. Unfortunately, rollout training requires multiple\\r\\nevaluations of the model for each time step, and thus becomes prohibitively expensive\\r\\nboth in terms of memory and compute as the resolution of the spatial grid grows. Another\\r\\napproach would be learning an autoregressive model using latent embeddings that encode\\r\\ndynamic shape information [15]. However, it is infeasible to observe the entire span of\\r\\npossible surface deformations from limited real-world scans, which makes the model\\r\\nprone to overfitting and leads to worse generalization at test time.\\r\\n    By addressing these limitations, we, for the first time, enable autoregressive training\\r\\nof a full-body geometry model represented by a neural implicit surface. To tackle\\r\\nthe scalability issues of rollout training for implicit representations, we introduce the\\r\\nnovel notion of articulated observer points. Intuitively, articulated observer points are\\r\\ntemporally coherent locations on the human body surface which store the dynamically\\r\\nchanging state of the implicit function. In practice, we parameterize the observer points\\r\\nusing the underlying body model [22], and then represent the state of the implicit surface\\r\\nas signed heights with respect to the vertices of the pose-dependent geometry produced by\\r\\nthe articulated model (see Fig. 3a). The number of query points is significantly lower than\\r\\nthe number of voxels in a high-resolution grid, which allows for a significant reduction\\r\\n\\x0c\\n\\n              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling     3\\r\\n\\r\\nin terms of memory and compute requirements, making rollout training tractable for\\r\\nimplicit surfaces. In addition, we demonstrate that explicitly encoding shapes as signed\\r\\nheight fields is less prone to overfitting compared to latent embeddings, a common way\\r\\nto represent autoregressive states [19,52].\\r\\n    Our main contributions are the following:\\r\\n    – The first autoregressive approach for modeling history-dependent implicit surfaces\\r\\n      of human bodies,\\r\\n    – Articulated observer points to enable autogressive training of neural fields, and\\r\\n    – Extensive experiments showing that our approach outperforms existing methods\\r\\n      both on shape interpolation and extrapolation tasks.\\r\\n\\r\\n\\r\\n2     Related Work\\r\\nParametric Human Models Since the anatomical structure of humans is shared across\\r\\nidentities, various methods have been proposed to parameterize shape and pose of\\r\\nhuman bodies from large-scale 3D scan data [3,13,22,33,54,1]. SCAPE [3,13] learns\\r\\nstatistical human model models using triangle deformations. The pioneering work\\r\\nby Allen et al. [2] used a vertex-based representation enhanced with pose-dependent\\r\\ndeformations, but the model was complex and trained with insufficient data, resulting\\r\\nin overfitting. SMPL [22] improved the generalizability of [2] by training on more data\\r\\nand removing the shape dependency in the pose-dependent deformations. More recent\\r\\nworks show that sparsity in the pose correctives reduces spurious correlations [33],\\r\\nand that non-linear deformation bases parameterized by neural networks achieve better\\r\\nmodeling accuracy [54]. While most works focus on modeling static human bodies\\r\\nunder different poses, Dyna [39] and DMPL (Dynamic SMPL) [22] enable parametric\\r\\nmodeling of dynamic deformations by learning a linear autoregressive model. Kim\\r\\net al. [16] combine a volumetric parametric model, VSMPL, with an external layer\\r\\ndriven by the finite element method to enable soft tissue dynamics. SoftSMPL [45]\\r\\nlearns a more powerful recurrent neural network to achieve better generalization to\\r\\nunseen subjects. Xiang et al. [52] model dynamically moving clothing from a history\\r\\nof poses. Importantly, the foundation of the aforementioned works is accurate surface\\r\\nregistration of a template body mesh [6,7], which remains non-trivial. Habermann et\\r\\nal. [12] also model dynamic deformations from a history of poses. While they relax the\\r\\nneed of registration by leveraging image-based supervision, a personalized template is\\r\\nstill required as a preprocessing step.\\r\\n     Recently, neural networks promise to enable the modeling of animatable bodies\\r\\nwithout requiring surface registration or a personalized template [10,24,43,48,8]. These\\r\\nmethods leverage structured point clouds [24,26,56] or 3D neural fields [53] to learn\\r\\nanimatable avatars. Approaches based on neural fields parameterize human bodies as\\r\\ncompositional articulated occupancy networks [10] or implicit surface in canonical space\\r\\nwith linear blend skinning [30,43,8,51] and deformation fields [48,35]. Since implicit\\r\\nsurfaces do not require surface correspondences for training, avatars can be learned from\\r\\nraw scans. Similarly, neural radiance fields [31] have been applied to body modeling to\\r\\nbuild animatable avatars from multi-view images [37,20]. However, these approaches\\r\\nrepresent avatars as a function of only pose parameters, and thus are unable to model\\r\\n\\x0c\\n\\n4              Z. Bai et al.\\r\\n\\r\\n                     Shape Encoding                                   Dynamic Feature Encoding                     Articulation-Aware Shape Decoding\\r\\n    SMPL Poses                                              Temporal Derivatives          Localized Pose\\r\\n                                                             · dH\\r\\n                                                                          p· ≈\\r\\n                                                                               dp                                      Query Point\\r\\n                      Articulated                            H≈                       L(p) = (W ⋅ ωi) ∘ p\\r\\n                       Observer                                 dt             dt                                         q\\r\\n                                                                                                                              k(q)\\r\\n                        Points        Signed Heights\\r\\n\\r\\npt−2, pt−1, pt, pt+1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                             Resample\\r\\n                                                                         UNet                                              SDF decoding\\r\\n    Implicit Shapes                                                                                                            f(q)\\r\\n\\r\\n                                                              ·                                                                              St+1\\r\\n                                      Ht−2, Ht−1, Ht     Ht, {Ht+i}                 Zuv                 Zt+1        Features in\\r\\n                                                       L(pt+1), {L( p· t+i)}\\r\\n                      SDF values                                                                                   Posed Space\\r\\n                                                                                                  Dynamic Latent\\r\\n                                                                                                   Embeddings\\r\\n    St−2, St−1, St\\r\\n\\r\\n\\r\\nFig. 2: Overview. AutoAvatar learns a pose-driven animatable human body model with plausible\\r\\ndynamics including secondary motions. Notice that our approach takes the history of implicit\\r\\nshapes in an autoregressive manner for learning dynamics.\\r\\n𝒩\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ndynamics. While our approach is also based on 3D neural fields to eliminate the need\\r\\nfor surface registration, our approach learns not only pose-dependent deformations but\\r\\nalso history-dependent dynamics by enabling autoregressive training of neural implicit\\r\\nsurfaces.\\r\\n\\r\\nLearning Dynamics Traditionally, physics-based simulation [46] is used to model\\r\\ndynamics of objects. While material parameters of physics simulation can be estimated\\r\\nfrom real data [5,50,47,55], accurately simulating dynamic behavior of objects remains\\r\\nan open question. In addition, authenticity of physics-based simulation is bounded by the\\r\\nunderlying model, and complex anisotropic materials such as the human body are still\\r\\nchallenging to model accurately. For this reason, several works attempt to substitute a\\r\\ndeterministic physics-based simulation with a learnable module parameterized by neural\\r\\nnetworks [14,57,44,38]. Such approaches have been applied to cloth simulation [14,38],\\r\\nfluid [44], and elastic bodies [57]. Subspace Neural Physics [14] learns a recurrent\\r\\nneural network from offline simulation to predict the simulation state in a subspace.\\r\\nDeep Emulator [57] first learns an autoregressive model to predict deformations using a\\r\\nsimple primitive (sphere), and applies the learned function to more complex characters.\\r\\nWhile we share the same spirit with the aforementioned works by learning dynamic\\r\\ndeformations in an autoregressive manner, our approach fundamentally differs from\\r\\nthem. The aforementioned approaches all assume that physical quantities such as vertex\\r\\npositions are observable with perfect correspondence in time, and thus results are only\\r\\ndemonstrated on synthetic data. In contrast, we learn dynamic deformations from real-\\r\\nworld observation while requiring only coarse temporal guidance by the fitted SMPL\\r\\nmodels. This property is essential to model faithful dynamics of real humans.\\r\\n\\r\\n\\r\\n3        Method\\r\\nOur approach is an autoregressive model, which takes as inputs human poses and a shape\\r\\nhistory and produces the implicit surface for a future frame. Fig. 2 shows the overview\\r\\nof our approach. Given a sequence of T implicitly encoded shapes {St−T +1 , ..., St }\\r\\nand T + 1 poses {pt−T +1 , ..., pt+1 } with t being the current time frame, our model\\r\\npredicts the implicit surface St+1 of the future frame t + 1. The output shape St+1 is\\r\\n\\x0c\\n\\n                   AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling                                                             5\\r\\n\\r\\n\\r\\n                                                                           ci = cos(q − vi, ni)\\r\\n                        Articulated\\r\\n                         Observer\\r\\n                                                                           di = ∥q − vi∥2\\r\\n                          Points\\r\\n\\r\\n                                                                                           [zit+1, di, ci]\\r\\n                                                                                 vi ni           …\\r\\n                           Signed\\r\\n                                    Height\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                             PointNet\\r\\n                                                                                           [zjt+1, dj, cj]\\r\\n                                                                                     q\\r\\n                                                                                           [zkt+1, dk, ck]\\r\\n                                                                    v{i,j,k} ∈      k(q)\\r\\n                                                         Zt+1                                                                    St+1\\r\\n      Posed SMPL                                 Features in Posed Space                                                Signed Distance Functions\\r\\n\\r\\n         (a) Shape Encoding.                               (b) Articulation-Aware SDF Decoding.\\r\\n                                             𝒩\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3: Shape Encoding/Decoding. Our novel shape encoding via articulated observer points and\\r\\narticulated-aware SDF decoding lead to faithful modeling of dynamics.\\r\\n\\r\\nthen passed as an input to the next frame prediction in an autoregressive manner. Our\\r\\nmodel is supervised directly with raw body scans, and requires a training dataset of 4D\\r\\nscans (sequences of 3D scans) along with fitted SMPL body models [22]. Unfortunately,\\r\\nexplicitly representing shapes St as levelsets of implicit surface is prohibitively expensive\\r\\nfor end-to-end training. To this end, we introduce the concept of articulated observer\\r\\npoints - vertex locations on the underlying articulated model - which are used as a local\\r\\nreference for defining the full body geometry. The underlying implicit surface is encoded\\r\\nas a height field with respect to the articulated observer points (Sec. 3.1). Given a history\\r\\nof height fields and pose parameters, we convert those to dynamic latent feature maps\\r\\nin UV space (Sec. 3.2). Finally, we map the resulting features to SDFs by associating\\r\\ncontinuous 3D space with the learned features on the SMPL vertices, which are directly\\r\\nsupervised by point clouds with surface normals (Sec. 3.3).\\r\\n\\r\\n3.1     Shape Encoding via Articulated Observer Points\\r\\nThe core of our approach is an autoregressive model that operates on implicit neural\\r\\nsurfaces, allowing us to incorporate temporal shape information necessary for modeling\\r\\nchallenging dynamics. The key challenge that arises when training such autoregressive\\r\\nmodels is finding a way to encode the shape - parameterized implicitly as a neural field -\\r\\ninto a representation that can be efficiently computed and fed back into the model. The\\r\\nmost straightforward way is to extract an explicit geometry representation by evaluating\\r\\nthe neural field on a dense spatial grid and running marching cubes [23]. However, in\\r\\npractice this approach is infeasible due to prohibitive memory and computational costs,\\r\\nin particular due to the cubic scaling with respect to the grid dimensions. Instead, we\\r\\npropose to encode the state of the implicit surface into a set of observer points.\\r\\n    Encoding geometry into discrete point sets has been shown to be efficient and\\r\\neffective for learning shape representations from point clouds [40]. Prokudin et al. [40]\\r\\nrelies on a fixed set of randomly sampled observer points in global world coordinates,\\r\\nwhich is not suitable for modeling dynamic humans due to the articulated nature of human\\r\\nmotion. Namely, a model relying on observer points with a fixed 3D location needs to\\r\\naccount for extremely large shape variations including rigid transformations, making\\r\\nthe learning task difficult. Moreover, associating randomly sampled 3D points with a\\r\\nparametric human body is non-trivial. To address these limitations, we further extend the\\r\\n\\x0c\\n\\n6       Z. Bai et al.\\r\\n\\r\\nnotion of observer points to an articulated template represented by the SMPL model [22],\\r\\nwhich provides several advantages for modeling dynamic articulated geometries. In\\r\\nparticular, soft-tissue dynamic deformations appear only around the minimally clothed\\r\\nbody, and we can rely on this notion as an explicit prior to effectively allocate observer\\r\\npoints only to the relevant regions. In addition, the SMPL model provides a mapping of\\r\\n3D vertices to a common UV parameterization, allowing us to effectively process shape\\r\\ninformation using 2D CNNs in a temporally consistent manner.\\r\\n     More specifically, to encode the neural implicit surface into the articulated observer\\r\\npoints, we compute “signed heights” H = {hi }M      i=1 ∈ R\\r\\n                                                            M\\r\\n                                                               from M vertices on a fitted\\r\\nSMPL model. For each vertex, the signed height hi is the signed distance from the\\r\\nvertex to the zero-crossing of the implicit surface along the vertex normal (see Fig. 3a).\\r\\nWe use the iterative secant method as in [32] to compute the zero-crossings. Note that\\r\\nthere can be multiple valid signed heights per vertex since the line along the normal can\\r\\nhit the zero-crossing multiple times. Based on the observation that the SMPL vertices\\r\\nare usually close to the actual surface with their normals roughly facing into the same\\r\\ndirection, we use the minimum signed height within a predefined range [hmin , hmax ]\\r\\n(in our experiments, we use hmin = −2cm, hmax = 8cm). If no zero-crossing is found\\r\\ninside this range, we set the signed height to hmin . Note that the computed heights\\r\\nare signed because the fitted SMPL can go beyond the actual surface due to its limited\\r\\nexpressiveness and inaccuracy in the fitting stage.\\r\\n\\r\\n3.2   Dynamic Feature Encoding\\r\\nThe essence of AutoAvatar is an animatable autoregressive model. In other words,\\r\\na reconstructed avatar is driven by pose parameters, while secondary dynamics is\\r\\nautomatically synthesized from the history of shapes and poses. To enable this, we learn\\r\\na mapping that encodes the history of shape and pose information to latent embeddings\\r\\ncontaining the shape information of the future frame. More specifically, denoting the\\r\\ncurrent time frame as t, we take as input T + 1 poses {pt−T +1 , ..., pt+1 } and T signed\\r\\nheights vectors {Ht−T +1 , ..., Ht }, and produce dynamic features Zt+1 ∈ RM ×C .\\r\\nGiven these inputs, we also compute the temporal derivatives of poses {ṗt+i }1i=−T +2\\r\\nand signed heights {Ḣt+i }0i=−T +2 as follows:\\r\\n\\r\\n                                               \\\\begin {aligned} \\\\dot {\\\\bm {p}}_{k} &= \\\\bm {p}_{k} \\\\bm {p}_{k-1}^{-1} \\\\\\\\ \\\\dot {\\\\bm {H}}_{k} &= \\\\bm {H}_{k} - \\\\bm {H}_{k-1}. \\\\end {aligned} \\r\\n                                                                                                                                                                                             (1)\\r\\n\\r\\nNote that in practice p∗ are represented as quaternions, and pk p−1\\r\\n                                                                 k−1 is computed by first\\r\\nconverting multipliers to rotation matrices, multiplying those, and then converting the\\r\\nproduct back to quaternions. To emphasize small values in Ḣ, we apply the following\\r\\ntransformation g(x) = sign(x) · ln(α|x| + 1) · β, where α = 1000 and β = 0.25.\\r\\nFollowing prior works [43,4], we also localize pose parameters to reduce long range\\r\\nspurious correlations as follows:\\r\\n                                  \\\\begin {aligned} L(\\\\bm {p}) &= (W \\\\cdot \\\\omega _i) \\\\circ \\\\bm {p}, \\\\end {aligned}                                                                           (2)\\r\\nwhere ◦ denotes the element-wise product, i is the vertex index, W ∈ RJ×J is an\\r\\nassociation matrix of J joints, and ωi ∈ RJ×1 is the skinning weights of the i-th vertex.\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling                                                                     7\\r\\n\\r\\nWe set Wn,m = 1 if the n-th joint is within the 1-ring neighborhood of the m-th joint\\r\\n(otherwise Wn,m = 0). Note that the derivative of the root transformation is included in\\r\\n{L(ṗt+i )} without localization. Finally, we map Ht , {Ḣt+i }, L(pt+1 ), and {L(ṗt+i )}\\r\\nto UV space using barycentric interpolation. The concatenated features are fed into a\\r\\nUNet [42] to generate a feature map Zuv . We then resample Zuv on the UV coordinates\\r\\ncorresponding to SMPL vertices to obtain the per-vertex dynamic latent embeddings\\r\\nZ. We empirically found that incorporating temporal derivatives further improves the\\r\\nrealism of dynamics (see Supp. Mat. video for comparison).\\r\\n\\r\\n3.3   Articulation-Aware Shape Decoding\\r\\nGiven the dynamic feature Zt+1 = {z1t+1 , ..., zM   t+1\\r\\n                                                        } and a query point q, we decode\\r\\nsigned distance fields f (q) to obtain the surface geometry of the dynamic avatar. Several\\r\\nmethods model the implicit surface in canonical space by jointly learning a warping\\r\\nfunction from the posed space to the canonical space [43,8]. However, we observe that\\r\\nthe canonicalization step is very sensitive to small fitting error in the SMPL model, and\\r\\nfurther amplifies the error in the canonical space, making it difficult to learn dynamics\\r\\n(see discussion in Sec. 4). Therefore, we directly model the implicit surface in a posed\\r\\nspace while being robust to pose changes. Inspired by Neural Actor [20], we associate\\r\\na queried 3D point with a human body model and pose-agnostic spatial information.\\r\\nSpecifically, Neural Actor uses height from the closest surface point on the SMPL\\r\\nmodel to the query location together with a feature vector sampled on the same surface\\r\\npoint. However, we find that their approach based on the single closest point leads\\r\\nto artifacts around body joints (e.g., armpits) for unseen poses. To better distinguish\\r\\nregions with multiple body parts, we instead use k-nearest neighbor vertices. Fig. 3b\\r\\nshows the illustration of our SDF decoding approach. Given a query point q, we first\\r\\ncompute the k-nearest SMPL vertices {vi }i∈Nk (q) , where Nk (q) is a set of indices\\r\\nof k-nearest neighbor vertices. To encode pose-agnostic spatial information, we use\\r\\nrotation-invariant features. Specifically, we compute the distance di = ∥q − vi ∥2 and\\r\\ncosine value ci = cos(xi , ni ), where xi is the vertex-to-query vector \\x02 xi = q −\\x03vi , and\\r\\nni is the surface normal on vi . We feed the concatenated vector zit+1 , di , ci into a\\r\\nPointNet-like [41] architecture to compute the final SDFs with the max pooling replaced\\r\\nby a weighted average pooling based on jointly predicted weights for better continuity.\\r\\n    As in [43], we employ implicit geometric regularization (IGR) [11] to train our\\r\\nmodel directly from raw scans without requiring watertight meshes. Note that in contrast,\\r\\nother methods [30,8,48] require watertight meshes to compute ground-truth occupancy\\r\\nor signed distance values for training. Our final objective function L is the following:\\r\\n                             \\\\label {eq:igr} \\\\begin {aligned} L &= L_{s} + L_{n} + \\\\lambda _{igr} L_{igr} + \\\\lambda _{o} L_{o}, \\\\\\\\ \\\\end {aligned}    (3)\\r\\nwhere λigr = 1.0, λo = 0.1. Ls promotes SDFs which vanish on the ground    Ptruth surface,\\r\\nwhile LPn encourages that its normal align with the ones from data: Ls =     q∈Qs |f (q)|,\\r\\nLn =      q∈Qs  ∥∇ q f (q) −  n(q)∥  2 , where Q s is the surface of the input raw scans.\\r\\nLigr is the Eikonal regularization term [11] that encourages the function f to satisfy\\r\\nthe Eikonal equation: Ligr = Eq (∥∇q f (q)∥2 − 1)2 , and Lo prevents off-surface SDF\\r\\nvalues from being too close to the zero-crossings as follows: Lo = Eq (exp(−γ ·|f (q)|)),\\r\\nwhere γ = 50.\\r\\n\\x0c\\n\\n8       Z. Bai et al.\\r\\n\\r\\n3.4   Implementation Details\\r\\nNetwork Architectures. In our experiments, we use a UV map of resolution 256 × 256,\\r\\nT = 3, and k = 20. To reduce the imbalance of SMPL vertex density for k-NN\\r\\ncomputation, we use 3928 points subsampled by poisson-disk sampling on the SMPL\\r\\nmesh. Before being fed into the UNet, L(pk+1 ) and {L(ṗt+i )} are compressed to 32\\r\\nchannels using 1×1 convolutions. The UNet uses convolution and transposed convolution\\r\\nlayers with untied biases, a kernel size of 3, no normalization, and LeakyReLU with a\\r\\nslope of 0.2 as the non-linear activation, except for the last layer which uses TanH. The\\r\\nSDF decoder is implemented as an MLP, which takes as input 64-dim features from the\\r\\nUNet, positional encoded di and ci up to 4-th order Fourier features. The number of\\r\\nintermediate neurons in the first part of the MLP is (128, 128, 129), where the output\\r\\nis split into a 128-dim feature vector and a 1-dim scalar, which is converted into non-\\r\\nnegative weights by softmax across the k-NN samples. After weighted average pooling,\\r\\nthe aggregated feature is fed into another MLP with a neuron size of (128, 128, 1) to\\r\\npredict the SDF values. The MLPs use Softplus with β = 100 and a threshold of 20 as\\r\\nnon-linear activation except for the last layer which does not apply any activation.\\r\\nTraining. Our training consists of two stages. First, we train our model using ground-\\r\\ntruth signed heights without rollout for 90000 iterations. Then, we finetune the model\\r\\nusing a rollout of 2 frames for another 7500 iterations to reduce error accumulation for\\r\\nboth training and inference. We use the Adam optimizer with a learning rate of 1.0×10−4\\r\\n(1.0 × 10−5 ) at the first (second) stage. To compute Ln , we sample 10000(1000) points\\r\\non the scan surface. Similarly, for Ligr , we sample 10000(1000) points around the scan\\r\\nsurface by adding Gaussian noise with standard deviation of 10cm to uniformly sampled\\r\\nsurface points, and sample 2000(500) points within the bounding box around the raw\\r\\nscans. The points uniformly sampled inside the bounding box are also used to compute\\r\\nLo . Both stages are trained with a batch size of 1.\\r\\nInference. At the beginning of the animations, we assume ground-truth raw scans are\\r\\navailable for the previous T frames for initialization. If no ground truth initial shape is\\r\\navailable, we initialize the first T frames with our baseline model conditioned only on\\r\\npose parameters. Note that the scan data is extremely noisy around the hand and foot\\r\\nareas, and the SMPL fitting of the head region is especially inaccurate. Therefore, we fix\\r\\nthe dynamic features on the face, hands, and feet to the ones of the first frame.\\r\\n\\r\\n\\r\\n4     Experimental Results\\r\\n4.1   Datasets and Metrics\\r\\nDatasets. We use the DFaust dataset [7] for both training and quantitative evaluation,\\r\\nand AIST++ [49,18] for qualitative evaluation on unseen motions. For the DFaust\\r\\ndataset, we choose 2 subjects (50002 and 50004), who exhibit the most soft-tissue\\r\\ndeformations. The interpolation test evaluates the fidelity of dynamics under the same\\r\\ntype of motions as in training but at different time instance, and the extrapolation\\r\\ntest evaluates performance on unseen motion. For 50002, we use the 2nd half of\\r\\nchicken wings and running on spot for the interpolation test, one leg jump\\r\\nfor the extrapolation test, and the rest for training. For 50004, we use the 2nd half of\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling        9\\r\\n\\r\\nchicken wings and running on spot for interpolation, one leg loose for\\r\\nextrapolation, and the rest for training. The fitted SMPL parameters in DFaust are\\r\\nprovided by the AMASS [27] dataset that uses sparse points on the registered data as\\r\\napproximated mocap marker locations and computes the parameters using MoSh [21].\\r\\nNote that more accurate pose can be obtained by using all the registration vertices (see\\r\\nAppendix A), but this is not required by our method to recover soft-tissue deformation.\\r\\nMetrics. For evaluation, we extract the 0-level set surface at each time step using\\r\\nMarching Cubes [23] with a resolution of 2563 . We also use simplified scans with\\r\\naround 10000 vertices and outlier points (distance to the nearest SMPL vertex larger\\r\\nthan 10cm) have been removed. We evaluate the accuracy of the predicted surface in\\r\\nterms of its position and dynamics accuracy. The surface position accuracy is measured\\r\\nby averaging the distance from each simplified scan vertex to the closest prediction\\r\\nsurface point. Evaluating the dynamics accuracy of the implicit surface efficiently is more\\r\\nchallenging. We approximate the local occupied volume as a scalar per registration vertex\\r\\nrepresenting the ratio of surrounding points contained in the interior of the (ground-truth\\r\\nor inferred) surface. We use 10 points uniformly sampled inside a 5cm cube centered at\\r\\nthe vertex. The head, hands and feet vertices are ignored due to their high noise levels.\\r\\nThe temporal difference of this scalar across adjacent frames can be interpreted as a\\r\\ndynamic measure of the local volume evolution. We report the mean square difference\\r\\nbetween this dynamic descriptor as computed with the ground truth simplified scan\\r\\nand the inferred implicit surface. Since a small phase shift in dynamics may lead to\\r\\nlarge cumulative error, reporting only the averaged errors from the entire frames can\\r\\nbe misleading. Therefore, we report errors along the progression of rollout predictions.\\r\\nFor each evaluation sequence, we start prediction every 20 frames (i.e., 20th frame, 40th\\r\\nframe, ...), and use the ground truth pose and shape history only for the first frame,\\r\\nfollowed by the autoregressive predictions for the error computation. In Tab. 1, we\\r\\nreport the averaged errors for both metrics after 1, 2, 4, 8, 16, and 30 rollouts. The\\r\\nerrors for small number of rollouts evaluate the accuracy of future shape prediction\\r\\ngiven the ground-truth shape history, whereas the errors with longer rollouts evaluate\\r\\nthe accumulated errors by autoregressively taking as input the predictions of previous\\r\\nframes. We discuss the limitation of error metrics with longer rollouts in Sec. 4.2.\\r\\n\\r\\n\\r\\n4.2   Evaluation\\r\\n\\r\\nIn this section, we provide comprehensive analysis to validate our design choices and\\r\\nhighlight the limitations of alternative approaches and SoTA methods based on both\\r\\nimplicit and explicit shape representations. Note that all approaches use the same training\\r\\nset, and are trained with the same number of iterations as our method for fair comparison.\\r\\nEffectiveness of Autoregressive Modeling. While autoregressive modeling is a widely\\r\\nused technique for learning dynamics [39,57,38], several recent methods still employ\\r\\nonly the history of poses for modeling dynamic avatars [52,12]. Thus, to evaluate the\\r\\neffectiveness of autoregressive modeling we compare AutoAvatar with pose-dependent\\r\\nalternatives that use neural implicit surfaces. More specifically, we design the following\\r\\n3 non-autoregressive baselines:\\r\\n\\x0c\\n\\n10        Z. Bai et al.\\r\\n\\r\\n\\r\\nTable 1: Quantitative Comparison with Baseline Methods. Our method produces the most\\r\\naccurate predictions of the future frames given the ground-truth shape history among all baseline\\r\\nmethods (see rollout 1-4). For longer rollouts, more dynamic predictions lead to higher error than\\r\\nless dynamic results due to high sensitivity to initial conditions in dynamic systems [34] (see\\r\\ndiscussion in Sec. 4.2).\\r\\n\\r\\n                      (a) Mean Scan-to-Prediction Distance (mm) ↓ on DFaust.\\r\\n\\r\\n                                                          Rollout (# of frames)\\r\\n                                               1       2       4         8      16      30\\r\\n                                        Interpolation Set\\r\\n                              SNARF [8]      7.428 7.372 7.337 7.476 7.530             7.656\\r\\n                              Pose           4.218 4.202 4.075 4.240 4.409             4.426\\r\\n      Non-Autoregressive\\r\\n                              PoseTCN        4.068 4.118 4.086 4.228 4.405             4.411\\r\\n                              Pose + dPose 3.852 3.841 3.764 3.972 4.164               4.156\\r\\n                              G-embed       2.932 3.006      3.131   3.462     3.756   3.793\\r\\n      Autoregressive          L-embed       1.784 2.138      2.863   4.250     5.448   5.916\\r\\n                              Ours          1.569 1.914      2.587   3.627     4.736   5.255\\r\\n                                        Extrapolation Set\\r\\n                              SNARF [8]     7.264 7.287      7.321   7.387     7.308   7.251\\r\\n                              Pose          4.303 4.306      4.308   4.299     4.385   4.398\\r\\n      Non-Autoregressive\\r\\n                              PoseTCN       4.090 4.091      4.105   4.119     4.233   4.257\\r\\n                              Pose + dPose 3.984 3.991       4.017   4.063     4.162   4.190\\r\\n                              G-embed        2.884   2.926   3.043   3.258     3.577   3.787\\r\\n      Autoregressive          L-embed        1.329   1.539   2.079   3.326     4.578   5.192\\r\\n                              Ours           1.150   1.361   1.834   2.689     3.789   4.526\\r\\n\\r\\n                      (b) Mean Squared Error of Volume Change ↓ on DFaust.\\r\\n\\r\\n                                                          Rollout (# of frames)\\r\\n                                              2         4           8         16         30\\r\\n                                        Interpolation Set\\r\\n                            SNARF [8]     0.01582 0.01552 0.01610 0.01658              0.01682\\r\\n                            Pose          0.01355 0.01305 0.01341 0.01367              0.01387\\r\\n     Non-Autoregressive\\r\\n                            PoseTCN       0.01364 0.01323 0.01350 0.01399              0.01416\\r\\n                            Pose + dPose 0.01288 0.01247 0.01273 0.01311               0.01321\\r\\n                            G-embed       0.01179 0.01168       0.01199    0.01248     0.01265\\r\\n     Autoregressive         L-embed       0.01003 0.01180       0.01466    0.01716     0.01844\\r\\n                            Ours          0.00902 0.01053       0.01258    0.01456     0.01565\\r\\n                                        Extrapolation Set\\r\\n                            SNARF [8]     0.01178 0.01194       0.01251    0.01228     0.01206\\r\\n                            Pose          0.01027 0.01039       0.01074    0.01052     0.01039\\r\\n     Non-Autoregressive\\r\\n                            PoseTCN       0.01020 0.01038       0.01064    0.01040     0.01029\\r\\n                            Pose + dPose 0.00992 0.01014        0.01048    0.01029     0.01013\\r\\n                            G-embed        0.00936   0.00959    0.00995    0.00996     0.00998\\r\\n     Autoregressive         L-embed        0.00648   0.00821    0.01100    0.01308     0.01402\\r\\n                            Ours           0.00567   0.00715    0.00915    0.01039     0.01107\\r\\n\\x0c\\n\\n                   AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling      11\\r\\n\\r\\n\\r\\n\\r\\n    SMPL\\r\\n    Pose\\r\\n    PoseTCN\\r\\n    Pose + dPose\\r\\n    Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 Time\\r\\n\\r\\n\\r\\nFig. 4: Qualitative Comparison with Non-Autoregressive Baselines. In contrast to the rigid\\r\\nresults in non-autoregressive baselines, our approach produces high quality non-rigid dynamics.\\r\\n\\r\\n 1. Pose: We only feed pose parameters of the next frame L(pt+1 ) in our architec-\\r\\n     ture. Prior avatar modeling methods based on neural fields employ this pose-only\\r\\n     parameterization [43,48,8].\\r\\n 2. PoseTCN: Temporal convolutional networks (TCN) [17] support the incorporation\\r\\n     of a long-range history for learning tasks, and have been used in several avatar\\r\\n     modeling methods [52,12]. Thus, we use a TCN that takes as input the sequence\\r\\n     of poses with the length of 16. We first compute localized pose parameters, as in\\r\\n     our method, for each frame and apply the TCN to obtain 64-dim features for each\\r\\n     SMPL vertex. The features are then fed into the UNet and SDF decoders identical\\r\\n     to our method.\\r\\n 3. Pose+dPose: Our approach without autoregressive components (Ht , {Ḣt+i }).\\r\\n    Tab. 1 shows that our approach outperforms the baseline methods for the first 8\\r\\nframes for interpolation, and first 16 frames for extrapolation. In particular, there is a\\r\\nsignificantly large margin for the first 4-8 frames, indicating that our method achieves\\r\\nthe most accurate prediction of the future frames given the ground-truth shape history.\\r\\nWe also observe that the non-autoregressive methods tend to collapse to predicting the\\r\\n“mean” shape under each pose without faithful dynamics for unseen motions (see Fig. 4\\r\\nand Supp. Mat. video). Since the accumulation of small errors in each frame may lead\\r\\nto large deviations from the ground-truth due to high sensitivity to initial conditions\\r\\nin dynamic systems [34], for longer rollouts mean predictions without any dynamics\\r\\ncan produce lower errors than more dynamic predictions. In fact, although our method\\r\\nleads to slightly higher errors on longer rollouts, Fig. 4 clearly shows that our approach\\r\\nproduces the most visually plausible dynamics on the AIST++ sequences. Importantly,\\r\\nwe do not observe any instability or explosions in our autoregressive model for longer\\r\\nrollouts, as can be seen from the error behavior shown in Fig. 7. We also highly encourage\\r\\n\\x0c\\n\\n12             Z. Bai et al.\\r\\n\\r\\n\\r\\n\\r\\n     SMPL\\r\\n     G-embed\\r\\n     L-embed\\r\\n     Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              Time\\r\\n\\r\\n\\r\\nFig. 5: Qualitative Comparison with Latent-space Autoregression. While the latent space-\\r\\nbased autoregression approaches suffer either from overfitting to pose parameters (G-embed)\\r\\nor instability (L-embed, see Supp. Mat. video), our approach based on a physically meaningful\\r\\nquantity (signed height) achieves the most stable and expressive synthesis of dynamics.\\r\\n\\r\\nreaders to see Supp. Mat. video for qualitative comparison in animation. In summary,\\r\\nour results confirm that autoregressive modeling plays a critical role for generalization\\r\\nto unseen motions and improving the realism of dynamics.\\r\\nExplicit Shape Encoding vs. Latent Encoding. Efficiently encoding the geometry of\\r\\nimplicit surfaces is non-trivial. While our proposed approach encodes the geometry via\\r\\nsigned heights on articulated observer points, prior approaches have demonstrated shape\\r\\nencoding based on a learned latent space [36,4]. Therefore, we also investigate different\\r\\nencoding methods for autoregressive modeling with the following 2 baselines:\\r\\n 1. G-embed: Inspired by DeepSDF [36], we first learn per-frame global embeddings\\r\\n    lg ∈ R512 with the UNet and SDF decoder by replacing Ht , {Ḣt+i }, {L(ṗt+i )}\\r\\n    with repeated global embeddings. Then, we train a small MLP with three 512-dim\\r\\n    hidden layers using Softplus except for the last layer, taking as input pt+1 , {ṗt+i },\\r\\n    and 3 embeddings of previous frames to predict the global embedding at time t + 1.\\r\\n 2. L-embed: For modeling mesh-based body avatars, localized embeddings are shown\\r\\n    to be effective [4]. Inspired by this, we also train a model with localized embeddings\\r\\n    ll ∈ R16×64×64 . We first learn per-frame local embeddings ll together with the UNet\\r\\n    and SDF decoder by replacing Ht , {Ḣt+i }, {L(ṗt+i )} with bilinearly upsampled\\r\\n    ll . Then we train another UNet that takes as input L(pt+1 ), {L(ṗt+i )}, and 3\\r\\n    embeddings of previous frames to predict the localized embeddings at time t + 1.\\r\\n    Note that for evaluation, we optimize per-frame embeddings for test sequences using\\r\\nEq. (3) such that the baseline methods can use the best possible history of embeddings\\r\\nfor autoregression. Tab. 1 shows that our method outperforms L-embed in all cases\\r\\nbecause L-embed becomes unstable for the test sequences. For G-embed, we observe the\\r\\nsame trend as for the non-autoregressive baselines: our approach achieves significantly\\r\\n\\x0c\\n\\n              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling              13\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 Jump to top\\r\\n                                                 Fall to bottom\\r\\n              Scan       SNARF       Ours                         Scan    SoftSMPL   Ours\\r\\n\\r\\n\\r\\nFig. 6: Qualitative Comparison with SoTA Methods. Our approach produces significantly more\\r\\nfaithful shapes and dynamics than the state-of-the-art implicit avatar modeling method [8], and\\r\\nshows comparable dynamics with prior art dependent on registrations with fixed topology [45].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n       (a) Scan-to-Prediction Distance ↓                           (b) MSE of Volume Change ↓\\r\\n\\r\\nFig. 7: Comparison with SoftSMPL [45]. We plot the errors on the sequence of\\r\\none leg loose for subject 50004. Surprisingly, our registration-free approach mostly out-\\r\\nperforms this baseline that has to rely on registered data with fixed topology.\\r\\n\\r\\nmore accurate predictions of the future frames given the ground-truth trajectories (see the\\r\\nerrors for 1-4 rollouts), and G-embed tends to predict “mean” shapes without plausible\\r\\ndynamics. The qualitative comparison in Fig. 5 confirms that our approach produces\\r\\nmore plausible dynamics. Please refer to Supp. Mat. video for detailed visual comparison\\r\\nin animation. We summarize that physically meaningful shape encodings (e.g., signed\\r\\nheights) enable more stable learning of dynamics via autoregression than methods relying\\r\\non latent space.\\r\\nComparison to SoTA Methods.. We compare our approach with state-of-the-art meth-\\r\\nods for both implicit surface representations and mesh-based representations. As a\\r\\nmethod using neural implicit surfaces, we choose SNARF [8], which jointly learns a\\r\\npose-conditioned implicit surface in a canonical T-pose and a forward skinning network\\r\\nfor reposing. Similar to ours, SNARF does not require temporal correspondences other\\r\\nthan the fitted SMPL models. We use the training code released by the authors using\\r\\nthe same training data and the fitted SMPL parameters as in our method. Note that in\\r\\nthe DFaust experiment in [8], SNARF is trained using only the fitted SMPL models\\r\\nto DFaust as ground-truth geometry, which do not contain any dynamic deformations.\\r\\n\\x0c\\n\\n14      Z. Bai et al.\\r\\n\\r\\nTab. 1 shows that our approach significantly outperforms SNARF for any number of\\r\\nrollouts. Interestingly, SNARF can produce dynamic effects for training data by severely\\r\\noverfitting to the pose parameters, but this does not generalize to unseen poses as the\\r\\nlearned dynamics is the results of spurious correlations. As mentioned in Sec. 3.3, we\\r\\nalso observe that the performance of SNARF heavily relies on the accuracy of the\\r\\nSMPL fitting for canonicalization, and any small alignment errors in the underlying\\r\\nSMPL registration deteriorates their test-time performance (see Fig. 6). Therefore, this\\r\\nexperiment demonstrates not only the importance of autoregressive dynamic avatar\\r\\nmodeling, but also the efficacy of our articulation-aware shape decoding approach given\\r\\nthe quality of available SMPL fitting for real-world scans.\\r\\n    We also compare against SoftSMPL [45], a state-of-the-art mesh-based method that\\r\\nlearns dynamically deforming human bodies from registered meshes. The authors of\\r\\nSoftSMPL kindly provide their predictions on the sequence of one leg loose for\\r\\nsubject 50004, which is excluded from training for both our method and SoftSMPL for\\r\\nfair comparison. To our surprise, Fig. 7 show that our results are slightly better on both\\r\\nmetrics for the majority of frames, although we tackle a significantly harder problem\\r\\nbecause our approach learns dynamic bodies directly from raw scans, whereas SoftSMPL\\r\\nlearns from the carefully registered data. We speculate that the lower error may be mainly\\r\\nattributed to the higher resolution of our geometry using implicit surfaces in contrast\\r\\nto their predictions on the coarse SMPL topology (see Fig. 6). Nevertheless, this result\\r\\nis highly encouraging as our approach achieves comparable performance on dynamics\\r\\nmodeling without having to rely on surface registration.\\r\\n\\r\\n\\r\\n5    Conclusion\\r\\nWe have introduced AutoAvatar, an autoregressive approach for modeling high-fidelity\\r\\ndynamic deformations of human bodies directly from raw 4D scans using neural\\r\\nimplicit surfaces. The reconstructed avatars can be driven by pose parameters, and\\r\\nautomatically incorporate secondary dynamic effects that depend on the history of\\r\\nshapes. Our experiments indicate that modeling dynamic avatars without relying on\\r\\naccurate registrations is made possible by choosing an efficient representation for our\\r\\nautoregressive model.\\r\\nLimitations and Future Work. While our method has shown to be effective in modeling\\r\\nthe elastic deformations of real humans, we observe that it remains challenging, yet\\r\\npromising, to model clothing deformations that involve high-frequency wrinkles (see\\r\\nAppendix C for details). Our evaluation also suggests that ground-truth comparison\\r\\nwith longer rollouts may not reliably reflect the plausibility of dynamics. Quantitative\\r\\nmetrics that handle the high sensitivity to initial conditions in dynamics could be further\\r\\ninvestigated. Currently, AutoAvatar models subject-specific dynamic human bodies,\\r\\nbut generalizing it to multiple identities, as demonstrated in registration-based shape\\r\\nmodeling [39,22,45], is an interesting direction for future work. The most exciting venue\\r\\nfor future work is to extend the notion of dynamics to image-based avatars [37,20]. In\\r\\ncontrast to implicit surfaces, neural radiance fields [31] do not have an explicit “surface”\\r\\nas they model geometry using density fields. While this remains an open question, we\\r\\nbelieve that our contributions in this work such as efficiently modeling the state of shapes\\r\\nvia articulated observer points might be useful to unlock this application.\\r\\n\\x0c\\n\\n              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling             15\\r\\n\\r\\nReferences\\r\\n 1. Alldieck, T., Xu, H., Sminchisescu, C.: imghum: Implicit generative models of 3d human\\r\\n    shape and articulated pose. In: Proc. of International Conference on Computer Vision (ICCV).\\r\\n    pp. 5461–5470 (2021) 3\\r\\n 2. Allen, B., Curless, B., Popović, Z., Hertzmann, A.: Learning a correlated model of identity\\r\\n    and pose-dependent body shape variation for real-time synthesis. In: Proceedings of the 2006\\r\\n    ACM SIGGRAPH/Eurographics symposium on Computer animation. pp. 147–156 (2006) 3\\r\\n 3. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: SCAPE: shape\\r\\n    completion and animation of people. ACM Trans. on Graphics (TOG) 24(3), 408–416 (2005)\\r\\n    3\\r\\n 4. Bagautdinov, T., Wu, C., Simon, T., Prada, F., Shiratori, T., Wei, S.E., Xu, W., Sheikh, Y.,\\r\\n    Saragih, J.: Driving-signal aware full-body avatars. ACM Trans. on Graphics (TOG) 40(4),\\r\\n    1–17 (2021) 1, 6, 12\\r\\n 5. Bhat, K.S., Twigg, C.D., Hodgins, J.K., Khosla, P., Popovic, Z., Seitz, S.M.: Estimating cloth\\r\\n    simulation parameters from video (2003) 4\\r\\n 6. Bogo, F., Romero, J., Loper, M., Black, M.J.: Faust: Dataset and evaluation for 3d mesh\\r\\n    registration. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 3794–3801\\r\\n    (2014) 3\\r\\n 7. Bogo, F., Romero, J., Pons-Moll, G., Black, M.J.: Dynamic faust: Registering human bodies\\r\\n    in motion. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 6233–6242\\r\\n    (2017) 3, 8, 19\\r\\n 8. Chen, X., Zheng, Y., Black, M.J., Hilliges, O., Geiger, A.: Snarf: Differentiable forward\\r\\n    skinning for animating non-rigid neural implicit shapes. In: Proc. of International Conference\\r\\n    on Computer Vision (ICCV) (2021) 1, 3, 7, 10, 11, 13, 19, 20\\r\\n 9. Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling. In: Proc.\\r\\n    of Computer Vision and Pattern Recognition (CVPR). pp. 5939–5948. Computer Vision\\r\\n    Foundation / IEEE (2019) 2\\r\\n10. Deng, B., Lewis, J.P., Jeruzalski, T., Pons-Moll, G., Hinton, G.E., Norouzi, M., Tagliasacchi,\\r\\n    A.: NASA neural articulated shape approximation. In: Proc. of European Conference on\\r\\n    Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12352, pp. 612–628.\\r\\n    Springer (2020) 3\\r\\n11. Gropp, A., Yariv, L., Haim, N., Atzmon, M., Lipman, Y.: Implicit geometric regularization for\\r\\n    learning shapes. In: Proceedings of the 37th International Conference on Machine Learning\\r\\n    (ICML). Proceedings of Machine Learning Research, vol. 119, pp. 3789–3799. PMLR (2020)\\r\\n    7\\r\\n12. Habermann, M., Liu, L., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.: Real-time deep\\r\\n    dynamic characters. ACM Trans. on Graphics (TOG) 40(4), 1–16 (2021) 3, 9, 11\\r\\n13. Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., Seidel, H.: A statistical model of human\\r\\n    pose and body shape. Computer Graphics Forum 28(2), 337–346 (2009) 3\\r\\n14. Holden, D., Duong, B.C., Datta, S., Nowrouzezahrai, D.: Subspace neural physics:\\r\\n    Fast data-driven interactive simulation. In: Proceedings of the 18th annual ACM SIG-\\r\\n    GRAPH/Eurographics Symposium on Computer Animation. pp. 1–12 (2019) 4\\r\\n15. Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3d human dynamics from video. In:\\r\\n    Proc. of Computer Vision and Pattern Recognition (CVPR) (2019) 2\\r\\n16. Kim, M., Pons-Moll, G., Pujades, S., Bang, S., Kim, J., Black, M.J., Lee, S.: Data-driven\\r\\n    physics for human soft tissue animation. ACM Trans. on Graphics (TOG) 36(4), 54:1–54:12\\r\\n    (2017) 3\\r\\n17. Lea, C., Vidal, R., Reiter, A., Hager, G.D.: Temporal convolutional networks: A unified\\r\\n    approach to action segmentation. In: Proc. of European Conference on Computer Vision\\r\\n    (ECCV). pp. 47–54. Springer (2016) 11\\r\\n\\x0c\\n\\n16       Z. Bai et al.\\r\\n\\r\\n18. Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance\\r\\n    generation with aist++. In: Proc. of International Conference on Computer Vision (ICCV). pp.\\r\\n    13401–13412 (2021) 8\\r\\n19. Ling, H.Y., Zinno, F., Cheng, G., Van De Panne, M.: Character controllers using motion vaes.\\r\\n    ACM Trans. on Graphics (TOG) 39(4), 40–1 (2020) 2, 3\\r\\n20. Liu, L., Habermann, M., Rudnev, V., Sarkar, K., Gu, J., Theobalt, C.: Neural actor: Neural\\r\\n    free-view synthesis of human actors with pose control. ACM Trans. on Graphics (TOG) 40(6),\\r\\n    1–16 (2021) 3, 7, 14, 19\\r\\n21. Loper, M., Mahmood, N., Black, M.J.: Mosh: Motion and shape capture from sparse markers.\\r\\n    ACM Trans. on Graphics (TOG) 33(6), 1–13 (2014) 9, 19\\r\\n22. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-\\r\\n    person linear model. ACM Trans. on Graphics (TOG) 34(6), 248:1–248:16 (2015) 2, 3, 5, 6,\\r\\n    14\\r\\n23. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface construction\\r\\n    algorithm. ACM siggraph computer graphics 21(4), 163–169 (1987) 5, 9\\r\\n24. Ma, Q., Saito, S., Yang, J., Tang, S., Black, M.J.: SCALE: Modeling clothed humans with\\r\\n    a surface codec of articulated local elements. In: Proc. of Computer Vision and Pattern\\r\\n    Recognition (CVPR) (Jun 2021) 1, 3\\r\\n25. Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., Black, M.J.: Learning to\\r\\n    dress 3d people in generative clothing. In: Proc. of Computer Vision and Pattern Recognition\\r\\n    (CVPR). pp. 6469–6478 (2020) 19\\r\\n26. Ma, Q., Yang, J., Tang, S., Black, M.J.: The power of points for modeling humans in clothing.\\r\\n    In: Proc. of International Conference on Computer Vision (ICCV) (Oct 2021) 1, 3\\r\\n27. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of\\r\\n    motion capture as surface shapes. In: Proc. of International Conference on Computer Vision\\r\\n    (ICCV). pp. 5442–5451 (2019) 9, 19, 20\\r\\n28. Martinez, J., Black, M.J., Romero, J.: On human motion prediction using recurrent neural\\r\\n    networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 2891–2900\\r\\n    (2017) 2\\r\\n29. Mescheder, L.M., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\\r\\n    Learning 3D reconstruction in function space. In: Proc. of Computer Vision and Pattern\\r\\n    Recognition (CVPR). pp. 4460–4470. Computer Vision Foundation / IEEE (2019) 2\\r\\n30. Mihajlovic, M., Zhang, Y., Black, M.J., Tang, S.: LEAP: Learning articulated occupancy of\\r\\n    people. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun 2021) 3, 7\\r\\n31. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF:\\r\\n    Representing scenes as neural radiance fields for view synthesis. In: Proc. of European\\r\\n    Conference on Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12346,\\r\\n    pp. 405–421. Springer (2020) 3, 14\\r\\n32. Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: Differentiable volumetric rendering:\\r\\n    Learning implicit 3d representations without 3d supervision. In: Proc. of Computer Vision\\r\\n    and Pattern Recognition (CVPR) (2020) 6\\r\\n33. Osman, A.A.A., Bolkart, T., Black, M.J.: STAR: sparse trained articulated human body\\r\\n    regressor. In: Proc. of European Conference on Computer Vision (ECCV). Lecture Notes in\\r\\n    Computer Science, vol. 12351, pp. 598–613. Springer (2020) 3\\r\\n34. Ott, E., Grebogi, C., Yorke, J.A.: Controlling chaos. Physical review letters 64(11), 1196\\r\\n    (1990) 10, 11\\r\\n35. Palafox, P., Božič, A., Thies, J., Nießner, M., Dai, A.: Npms: Neural parametric models for\\r\\n    3d deformable shapes. In: Proc. of International Conference on Computer Vision (ICCV). pp.\\r\\n    12695–12705 (2021) 3\\r\\n\\x0c\\n\\n               AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling               17\\r\\n\\r\\n36. Park, J.J., Florence, P., Straub, J., Newcombe, R.A., Lovegrove, S.: DeepSDF: Learning\\r\\n    continuous signed distance functions for shape representation. In: Proc. of Computer Vision\\r\\n    and Pattern Recognition (CVPR). pp. 165–174. Computer Vision Foundation / IEEE (2019)\\r\\n    2, 12\\r\\n37. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit\\r\\n    neural representations with structured latent codes for novel view synthesis of dynamic\\r\\n    humans. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 9054–9063\\r\\n    (2021) 3, 14\\r\\n38. Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., Battaglia, P.: Learning mesh-based simulation\\r\\n    with graph networks. In: International Conference on Learning Representations (2021) 4, 9\\r\\n39. Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J.: Dyna: A model of dynamic human\\r\\n    shape in motion. ACM Trans. on Graphics (TOG) 34(4), 1–14 (2015) 2, 3, 9, 14\\r\\n40. Prokudin, S., Lassner, C., Romero, J.: Efficient learning on point clouds with basis point sets.\\r\\n    In: Proc. of International Conference on Computer Vision (ICCV). pp. 4332–4341 (2019) 5\\r\\n41. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d\\r\\n    classification and segmentation. In: Proc. of Computer Vision and Pattern Recognition (CVPR).\\r\\n    pp. 652–660 (2017) 7\\r\\n42. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image\\r\\n    segmentation. In: International Conference on Medical image computing and computer-\\r\\n    assisted intervention. pp. 234–241. Springer (2015) 7\\r\\n43. Saito, S., Yang, J., Ma, Q., Black, M.J.: SCANimate: Weakly supervised learning of skinned\\r\\n    clothed avatar networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun\\r\\n    2021) 1, 3, 6, 7, 11\\r\\n44. Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., Battaglia, P.: Learning\\r\\n    to simulate complex physics with graph networks. In: International Conference on Machine\\r\\n    Learning. pp. 8459–8468. PMLR (2020) 4\\r\\n45. Santesteban, I., Garces, E., Otaduy, M.A., Casas, D.: Softsmpl: Data-driven modeling of\\r\\n    nonlinear soft-tissue dynamics for parametric humans. In: Computer Graphics Forum. vol. 39,\\r\\n    pp. 65–75. Wiley Online Library (2020) 2, 3, 13, 14\\r\\n46. Sifakis, E., Barbic, J.: Fem simulation of 3d deformable solids: a practitioner’s guide to theory,\\r\\n    discretization and model reduction. In: Acm siggraph 2012 courses, pp. 1–50 (2012) 4\\r\\n47. Srinivasan, S.G., Wang, Q., Rojas, J., Klár, G., Kavan, L., Sifakis, E.: Learning active\\r\\n    quasistatic physics-based models from data. ACM Trans. on Graphics (TOG) 40(4), 1–14\\r\\n    (2021) 4\\r\\n48. Tiwari, G., Sarafianos, N., Tung, T., Pons-Moll, G.: Neural-gif: Neural generalized implicit\\r\\n    functions for animating people in clothing. In: Proc. of International Conference on Computer\\r\\n    Vision (ICCV). pp. 11708–11718 (2021) 1, 3, 7, 11\\r\\n49. Tsuchida, S., Fukayama, S., Hamasaki, M., Goto, M.: Aist dance video database: Multi-genre,\\r\\n    multi-dancer, and multi-camera database for dance information processing. In: ISMIR. vol. 1,\\r\\n    p. 6 (2019) 8\\r\\n50. Wang, H., O’Brien, J.F., Ramamoorthi, R.: Data-driven elastic models for cloth: modeling\\r\\n    and measurement. ACM Trans. on Graphics (TOG) 30(4), 1–12 (2011) 4\\r\\n51. Wang, S., Mihajlovic, M., Ma, Q., Geiger, A., Tang, S.: Metaavatar: Learning animatable\\r\\n    clothed human models from few depth images. Proc. of Advances in Neural Information\\r\\n    Processing Systems (NeurIPS) 34 (2021) 3\\r\\n52. Xiang, D., Prada, F., Bagautdinov, T., Xu, W., Dong, Y., Wen, H., Hodgins, J., Wu, C.:\\r\\n    Modeling clothing as a separate layer for an animatable human avatar. ACM Trans. on\\r\\n    Graphics (TOG) 40(6), 1–15 (2021) 3, 9, 11\\r\\n53. Xie, Y., Takikawa, T., Saito, S., Litany, O., Yan, S., Khan, N., Tombari, F., Tompkin, J.,\\r\\n    Sitzmann, V., Sridhar, S.: Neural fields in visual computing and beyond. arXiv preprint\\r\\n    arXiv:2111.11426 (2021) 3\\r\\n\\x0c\\n\\n18       Z. Bai et al.\\r\\n\\r\\n54. Xu, H., Bazavan, E.G., Zanfir, A., Freeman, W.T., Sukthankar, R., Sminchisescu, C.: GHUM\\r\\n    & GHUML: generative 3D human shape and articulated pose models. In: Proc. of Computer\\r\\n    Vision and Pattern Recognition (CVPR). pp. 6183–6192. IEEE (2020) 3\\r\\n55. Yang, S., Liang, J., Lin, M.C.: Learning-based cloth material recovery from video. In: Proc.\\r\\n    of International Conference on Computer Vision (ICCV). pp. 4393–4403. IEEE Computer\\r\\n    Society (2017) 4\\r\\n56. Zakharkin, I., Mazur, K., Grigorev, A., Lempitsky, V.: Point-based modeling of human\\r\\n    clothing. In: Proc. of International Conference on Computer Vision (ICCV). pp. 14718–14727\\r\\n    (2021) 3\\r\\n57. Zheng, M., Zhou, Y., Ceylan, D., Barbic, J.: A deep emulator for secondary motion of 3d\\r\\n    characters. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 5932–5940\\r\\n    (2021) 4, 9\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling      19\\r\\n\\r\\nAppendix\\r\\n\\r\\nA    Analysis on Input Pose Accuracy\\r\\nWe investigate how the accuracy of the input SMPL fitting influences the results on\\r\\nsubject 50002 of DFaust [7]. As discussed in Sec. 4.1, the fitted SMPL parameters in\\r\\nDFaust are provided by the AMASS [27] dataset that uses sparse points on the registered\\r\\ndata as approximated motion capture marker locations and computes the parameters\\r\\nusing MoSh [21]. We observe that the provided pose parameters sometimes exhibit small\\r\\nmisalignment with respect to the input scans. While the fitting quality in the AMASS\\r\\ndataset is sufficient for our approach, we also evaluate the performance on more accurate\\r\\npose parameters by using all the vertices on the registered meshes. More specifically, we\\r\\nfirst compute a better template by unposing the registered meshes in the first frame of\\r\\neach sequence using the LBS skinning weights of the SMPL template, and averaging\\r\\nover all the sequences. Using this new template, we optimize pose parameters for each\\r\\nframe with an L2-loss on all the registered vertices. Note that in this experiment, we use\\r\\nthe original template with the refined pose parameters instead of the refined template in\\r\\norder not to unfairly favor our method over SNARF [8].\\r\\n     In Tab. 1, we report the mean absolute error of scan-to-prediction distance (mm) and\\r\\nthe mean squared error of volume change for our method and SNARF. Tab. 1 shows that\\r\\nSNARF has a large error reduction with refined poses, indicating that SNARF is highly\\r\\nsensitive to the accuracy of the SMPL fit. We also observe that after pose refinement,\\r\\nSNARF overfits more to training poses (e.g., interpolation) as SNARF cannot model\\r\\nhistory-dependent dynamic deformations. In contrast, our method is more robust to the\\r\\nfitting errors, and significantly outperforms SNARF in most settings except for 16-30\\r\\nrollouts in the interpolation set. Note that the results with longer rollouts favor “mean”\\r\\npredictions over more dynamic predictions, and do not inform us of the plausibility of\\r\\nthe synthesized dynamics (see the discussion in Sec. 4.2).\\r\\n\\r\\n\\r\\nB    k-NN vs. Closest Surface Projection\\r\\nAs discussed in Sec. 3.3, our SDF decoding approach uses k-nearest neighbors (k-NN)\\r\\nof the SMPL vertices instead of closest surface projection [20]. Fig. H illustrates the\\r\\nlimitation of this alternative approach proposed in Neural Actor [20]. As shown in Fig. H,\\r\\nwe observe that associating a query location with a single closest point on the surface\\r\\nleads to poor generalization to unseen poses around regions with multiple body parts in\\r\\nclose proximity (e.g. around armpits). In contrast, our approach, which associates query\\r\\npoints with multiple k-NN vertices, produces more plausible surface geometry even for\\r\\nunseen poses.\\r\\n\\r\\n\\r\\nC    Limitation: Clothing Deformations\\r\\nWe also apply our method on the CAPE [25] dataset that contains 4D scans of clothed\\r\\nhumans. We select the subject 03375 longlong, which exhibits the most visible\\r\\n\\x0c\\n\\n20       Z. Bai et al.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTable B: Quantitative Evaluation on Input Pose Accuracy on Subject 50002. We show the\\r\\nresults of our approach and SNARF [8] using the poses provided by the AMASS [27] dataset\\r\\nand the ones after refinement using all vertices in the registered meshes. While SNARF is greatly\\r\\ninfluenced by the accuracy of pose parameters, the slight improvement in our method illustrates its\\r\\nrobustness to SMPL fitting errors. In addition, our approach significantly outperforms SNARF\\r\\neven after pose refinement in most settings except for the 16-30 rollouts in the interpolation set.\\r\\n\\r\\n                         (a) Mean Scan-to-Prediction Distance (mm) ↓\\r\\n\\r\\n                                                 Rollout (# of frames)\\r\\n                                       1       2      4         8      16            30\\r\\n                                    Interpolation Set\\r\\n                          SNARF [8] 7.898 7.715 7.588 7.840 7.898                  8.238\\r\\n         AMASS [27]\\r\\n                          Ours       1.731 2.127 2.953 4.325 5.606                 6.455\\r\\n                          SNARF [8]  3.982 4.001 3.964            4.068    4.029   4.158\\r\\n         Refined Poses\\r\\n                          Ours       1.417 1.703 2.259            3.241    4.044   4.601\\r\\n                                    Extrapolation Set\\r\\n                          SNARF [8] 8.083 8.126 8.160             8.246    8.050   8.025\\r\\n         AMASS [27]\\r\\n                          Ours       1.259 1.479 1.984            2.883    4.023   4.867\\r\\n                          SNARF [8]      4.624   4.632    4.672   4.749    4.548   4.447\\r\\n         Refined Poses\\r\\n                          Ours           1.149   1.329    1.745   2.486    3.313   3.855\\r\\n\\r\\n                          (b) Mean Squared Error of Volume Change ↓\\r\\n\\r\\n                                                      Rollout (# of frames)\\r\\n                                           2         4          8         16          30\\r\\n                                         Interpolation Set\\r\\n                         SNARF [8]     0.01623 0.01590 0.01688 0.01703             0.01829\\r\\n       AMASS [27]\\r\\n                         Ours          0.00990 0.01135 0.01417 0.01597             0.01815\\r\\n                         SNARF [8]     0.01401 0.01349       0.01430    0.01426    0.01524\\r\\n       Refined Poses\\r\\n                         Ours          0.00849 0.01002       0.01248    0.01389    0.01558\\r\\n                                        Extrapolation Set\\r\\n                         SNARF [8]     0.01228 0.01244       0.01333    0.01292    0.01264\\r\\n       AMASS [27]\\r\\n                         Ours          0.00602 0.00756       0.00977    0.01082    0.01140\\r\\n                         SNARF [8]     0.01094    0.01092    0.01148    0.01099    0.01080\\r\\n       Refined Poses\\r\\n                         Ours          0.00559    0.00691    0.00871    0.00953    0.01000\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling            21\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    Scan         Projection        Ours                 Scan         Projection       Ours\\r\\n\\r\\nFig. H: k-NN vs. Closest Surface Projection. While the closest surface projection suffers from\\r\\nartifacts around armpits, our SDF decoding based on k-NN produces more plausible surface\\r\\ngeometry for unseen poses.\\r\\n\\r\\n\\r\\n\\r\\ndynamic deformations for clothing. We exclude 6 sequences (athletics, frisbee,\\r\\nvolleyball, box trial1, swim trial1, twist tilt trial1) from train-\\r\\ning, and use them for testing. We employ as input the template and SMPL poses provided\\r\\nby the CAPE dataset for training our model. Note that we approximate raw scans by\\r\\nsampling point clouds with surface normals computed on the registered meshes as the\\r\\nCAPE dataset only provides registered meshes for 03375 longlong.\\r\\n    Please refer to the supplementary video for qualitative results. While our approach\\r\\nproduces plausible short-term clothing deformations, it remains challenging to model\\r\\ndynamically deforming clothing with longer rollouts. Compared to soft-tissue deforma-\\r\\ntions, dynamics on clothed humans involve high-frequency deformations and topology\\r\\nchange, making the learning of clothing dynamics more difficult. We leave this for future\\r\\nwork.\\r\\n\\x0c',\n",
       " '              Versatile Multi-Modal Pre-Training for Human-Centric Perception\\r\\n\\r\\n                  Fangzhou Hong1 , Liang Pan1 , Zhongang Cai1,2,3 , Ziwei Liu1\\r\\n     1\\r\\n       S-Lab, Nanyang Technological University 2 SenseTime Research 3 Shanghai AI Laboratory\\r\\n                 {fangzhou001, liang.pan, ziwei.liu}@ntu.edu.sg                                       caizhongang@sensetime.com\\r\\n                                                             IN Pre-train          Ours        IN Pre-train      Ours        IN Pre-train      Ours        IN-Pretrain       Ours\\r\\n                                                        56                     54.5       80                            55                   52.6     90\\r\\n                                                                                                               74.2                                   80\\r\\n                                                                                                                                                               76.3\\r\\n                                                        54                                75\\r\\n                                                                                                                        50              46.8\\r\\n                                                                 51.5                                                                                 70                 62.4\\r\\n                                                        52                                70\\r\\n                                     1)         3)                          49.1          65\\r\\n                                                                                                                        45                            60   54.3\\r\\n                                                        50                                         60.9   60.8                                        50\\r\\n                                                        48                                60                            40\\r\\n                                                                                                                                 35.0                 40\\r\\n  Dense Representations                                 46   44.3                         55\\r\\n                                                                                                                        35                            30\\r\\n                                                                                               48.9\\r\\n                                                        44                                50\\r\\n                                                                                                                             28.6                     20              14.2\\r\\n                                                                                                                        30\\r\\n                                                        42                                45                                                          10\\r\\n                                                        40                                40                            25                             0\\r\\n                                                              GPS AP        GPSM AP              mIoU         mAcc             mIoU         mAcc           0.5% Acc 0.1% Acc\\r\\n                                     2)         4)      1) DensePose Estimation           2) RGB Human Parsing          3) Depth Human Parsing        4) Depth 3D Pose Est.\\r\\n  Sparse Representations\\r\\n                           a) System Overview                                                  b) Performance on Downstream Tasks\\r\\nFigure 1. An Overview of HCMoCo. a) We present HCMoCo, a versatile multi-modal pre-training framework that takes multi-modal\\r\\nobservations of human body as input for human-centric perception. The pre-train models can be transferred to various human-centric\\r\\ndownstream tasks with different modalities. b) Our HCMoCo shows superior performance on all four downstream tasks, especially for\\r\\ndata-efficient settings (10% DensePose, 20% RGB/depth human parsing, 0.5/0.1% 3D pose estimation). ‘IN’ stands for ImageNet.\\r\\n                                                                              are available at https://github.com/hongfz16/\\r\\n                                  Abstract                                    HCMoCo.\\r\\n\\r\\n   Human-centric perception plays a vital role in vision and                  1. Introduction\\r\\ngraphics. But their data annotations are prohibitively ex-\\r\\npensive. Therefore, it is desirable to have a versatile pre-                      As a long-standing problem, human-centric perception\\r\\ntrain model that serves as a foundation for data-efficient                    has been studied for decades, ranging from sparse predic-\\r\\ndownstream tasks transfer. To this end, we propose the                        tion tasks, such as human action recognition [8, 27, 42, 50],\\r\\nHuman-Centric Multi-Modal Contrastive Learning frame-                         2D keypoints detection [2, 26, 43, 48] and 3D pose estima-\\r\\nwork HCMoCo that leverages the multi-modal nature of hu-                      tion [22, 31, 40], to dense prediction tasks, such as human\\r\\nman data (e.g. RGB, depth, 2D keypoints) for effective rep-                   parsing [7, 11, 12, 25] and DensePose prediction [14]. Un-\\r\\nresentation learning. The objective comes with two main                       fortunately, to train a model with reasonable generalizabil-\\r\\nchallenges: dense pre-train for multi-modality data, effi-                    ity and robustness, an enormous amount of labeled real data\\r\\ncient usage of sparse human priors. To tackle the chal-                       is necessary, which is extremely expensive to collect and an-\\r\\nlenges, we design the novel Dense Intra-sample Contrastive                    notate. Therefore, it is desirable to have a versatile pre-train\\r\\nLearning and Sparse Structure-aware Contrastive Learning                      model that can serve as a foundation for all the aforemen-\\r\\ntargets by hierarchically learning a modal-invariant latent                   tioned human-centric perception tasks.\\r\\nspace featured with continuous and ordinal feature distri-                        With the development of sensors, the human body can\\r\\nbution and structure-aware semantic consistency. HCMoCo                       be more conveniently perceived and represented in multi-\\r\\nprovides pre-train for different modalities by combining het-                 ple modalities, such as RGB, depth, and infrared. In this\\r\\nerogeneous datasets, which allows efficient usage of exist-                   work, we argue that the multi-modality nature of human-\\r\\ning task-specific human data. Extensive experiments on four                   centric data can induce effective representations that trans-\\r\\ndownstream tasks of different modalities demonstrate the ef-                  fer well to various downstream tasks, due to three major\\r\\nfectiveness of HCMoCo, especially under data-efficient set-                   advantages: 1) Learning a modal-invariant latent space\\r\\ntings (7.16% and 12% improvement on DensePose Estima-                         through pre-training helps efficient task-relevant mutual in-\\r\\ntion and Human Parsing). Moreover, we demonstrate the                         formation extraction. 2) A single versatile pre-train model\\r\\nversatility of HCMoCo by exploring cross-modality super-                      on multi-modal data facilitates multiple downstream tasks\\r\\nvision and missing-modality inference, validating its strong                  using various modalities. 3) Our multi-modal pre-train set-\\r\\nability in cross-modal association and reasoning. Codes                       ting bridges heterogeneous human-centric datasets through\\r\\n                                                                              their common modality, which benefits the generalizability\\r\\n       Corresponding author                                                   of pre-train models.\\r\\n\\r\\n\\r\\n                                                                        1\\r\\n\\x0c\\n\\n    We mainly explore two groups of modalities as shown in             timation (RGB) [14], human parsing using RGB [22] or\\r\\nFig. 1 a): dense representations (e.g. RGB, depth, infrared)           depth frames, and 3D pose estimation (depth) [16]. Un-\\r\\nand sparse representations (e.g. 2D keypoints, 3D pose).               der full set and data-efficient training settings, HCMoCo\\r\\nDense representations can provide rich texture and/or 3D               constantly achieves better performance than training from\\r\\ngeometry information. But they are mostly low-level and                scratch or pre-train on ImageNet. To name a few, as shown\\r\\nnoisy. On the contrary, sparse representations obtained by             in Fig. 1 b), we achieve 7.16% improvement in terms\\r\\noff-the-shelf tools [4, 9] are semantic and structured. But            of GPS AP on 10% training data of DensePose estima-\\r\\nthe sparsity results in insufficient details. We highlight that        tion; 12% improvement in terms of mIoU on 20% training\\r\\nit is non-trivial to integrate these heterogeneous modalities          data of Human3.6M human parsing. Moreover, we eval-\\r\\ninto a unified pre-training framework for the following two            uate the modal-invariance of the latent space learned by\\r\\nmain challenges: 1) learning representations suitable for              HCMoCo for dense prediction on NTURGBD-Parsing-4K\\r\\ndense prediction tasks in the multi-modality setting; 2) us-           with two settings: cross-modality supervision and missing-\\r\\ning weak priors from sparse representations effectively for            modality inference. Compared against conventional con-\\r\\npre-training.                                                          trastive learning targets, our method improves the segmen-\\r\\n    Challenge 1: Dense Targets. Existing methods [21, 30]              tation mIoU by 29% and 24% for the two settings, respec-\\r\\nperform contrastive learning densely on pixel-level features           tively. To the best of our knowledge, we are the first to study\\r\\nto achieve view-invariance for dense prediction tasks. How-            multi-modal pre-training for human-centric perception.\\r\\never, those methods require multiple views of a static 3D                  The main contributions are summarized below: 1) As\\r\\nscene [10], which is inapplicable for human-centric appli-             the first endeavor, we provide an in-depth analysis for\\r\\ncations with only single view. Furthermore, it is preferable           human-centric pre-training, which is formulated as a chal-\\r\\nto learn representations that are continuously and orderly             lenging multi-modal contrastive learning problem. 2) To-\\r\\ndistributed over the human body. In light of this, we gener-           gether with the novel hierarchical contrastive learning ob-\\r\\nalize the widely used InfoNCE [33] and propose a dense                 jectives, a comprehensive framework HCMoCo is pro-\\r\\nintra-sample contrastive learning objective that applies a             posed for effective pre-training for human-centric tasks. 3)\\r\\nsoft pixel-level contrastive target, which can facilitate learn-       Through extensive experiments, HCMoCo achieves supe-\\r\\ning ordinal and continuous dense feature distributions.                rior performance than existing methods, and meanwhile\\r\\n    Challenge 2: Sparse Priors. To employ priors in con-               shows promising modal-invariance properties. 4) To bene-\\r\\ntrastive learning, previous works [3, 23, 46] mainly use the           fit multi-modal human-centric perception, we contribute an\\r\\nsupervision to generate semantically positive pairs. How-              RGB-D human parsing dataset, NTURGBD-Parsing-4K.\\r\\never, these methods only focus on the sample-level con-\\r\\ntrastive learning, which means each sample is encoded to a             2. Related Work\\r\\nglobal embedding. It is not optimal for human dense predic-            Human-Centric Perception. Many efforts have been put\\r\\ntion tasks. To this end, we propose a sparse structure-aware           into human-centric perception in decades. Lots of work in\\r\\ncontrastive learning target, which uses semantic correspon-            2D keypoint detection [2,26,43,48] has achieved robust and\\r\\ndences across samples as positive pairs to complement pos-             accurate performance. 3D pose estimation has long been a\\r\\nitive intra-sample pairs. Particularly, leveraging sparse hu-          challenging problem and is approached from two aspects,\\r\\nman priors leads to an embedding space where semantically              lifting from 2D keypoints [22, 31, 40] and predicting from\\r\\ncorresponding parts are aligned more closely.                          depth map [16, 49]. Human parsing can be defined in two\\r\\n    To sum up, we propose HCMoCo, a Human-Centric                      ways. The first one parses garments together with visible\\r\\nmulti-Modal Contrastive learning framework for versatile               body parts [11, 12, 25]. The second one only focuses on\\r\\nmulti-modal pre-training. To fully leverage multi-modal                parsing human parts [7, 20, 22]. In this work, we focus\\r\\nobservations, HCMoCo effectively utilizes both dense mea-              on the second setting because the depth and 2D keypoints\\r\\nsurements and sparse priors using the following three-levels           do not contain the texture information needed for garment\\r\\nhierarchical contrastive learning objectives: 1) sample-               parsing. There are a few works [19, 32] about human pars-\\r\\nlevel modality-invariant representation learning; 2) dense             ing on depth maps. However, the data and annotations are\\r\\nintra-sample contrastive learning; 3) sparse structure-aware           too coarse or unavailable. To further push the accuracy of\\r\\ncontrastive learning. As an effort towards establishing                human-centric perception, DensePose [14, 44] is proposed\\r\\na comprehensive multi-modal human parsing benchmark                    to densely model each human body surface point. The cost\\r\\ndataset, we label human segments for RGB-D images from                 of DensePose annotation is enormous. Therefore, we also\\r\\nNTU RGB+D dataset [42], and contribute the NTURGBD-                    explore data-efficient learning of DensePose.\\r\\nParsing-4K dataset. To evaluate HCMoCo, we trans-                      Multi-Modal Contrastive Learning. Multi-modality nat-\\r\\nfer our pre-train model to four human-centric downstream               urally provides different views of the same sample which\\r\\ntasks using different modalities, including DensePose es-              fits well into the contrastive learning framework. CMC [45]\\r\\n\\r\\n                                                                   2\\r\\n\\x0c\\n\\n                                 #            #       #                        representations for downstream tasks transfer.\\r\\n                               𝑀!∗ ∘ ℳ      𝑓!$     𝑓!%\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                              (a) Global\\r\\n  𝐼$\"   𝐸$\"       𝐸!$ (𝐼!$ )                                                       To support dense downstream tasks, other than the usual\\r\\n                                 #            #       #                        sample-level global embeddings used in [5,6,13,18,28,45],\\r\\n                               𝑀&∗ ∘ ℳ      𝑓&$     𝑓&%\\r\\n                                                                               we propose to consider different levels of embeddings i.e.\\r\\n  𝐼$#   𝐸$#       𝐸!% (𝐼!% )                                                   global embeddings f g , sparse embeddings f s and dense\\r\\n              …\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                              (b) Dense\\r\\n                                                                               embeddings f d 1 , which are defined as follows: 1) For\\r\\n                                              !         !\\r\\n                                  !\\r\\n                                 𝑀!∗         𝑓!$       𝑓!%                     dense representations Id , the global embedding is obtained\\r\\n  𝐼!\"   𝐸!\"       𝐸&$ (𝐼&$ )                                                   by applying a mapper network Mdg to the mean pooling M\\r\\n                                              &        &\\r\\n                                                                               of the corresponding feature map, which is formulated as\\r\\n                                &\\r\\n                               𝑀!∗ ∘𝒢       𝑓!$      𝑓!%\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                              (c) Sparse\\r\\n                                                                               fdg = Mdg ◦ M ◦ Ed (Id ). Similarly, for sparse representa-\\r\\n  𝐼!#   𝐸!#       𝐸&% (𝐼&% )\\r\\n                                              &        &\\r\\n                                                                               tions Is , the global embedding is defined as fsg = Msg ◦M◦\\r\\n                                  &         𝑓&$      𝑓&$\\r\\n                                 𝑀&∗\\r\\n              …                                                                Es (Is ). 2) Sparse embeddings have the same size as that\\r\\n                                                                               of sparse representations. Formally, for sparse representa-\\r\\nFigure 2. Illustration of the general paradigm of HCMoCo.                      tions Is = G(V, E), where V ∈ RJ×K , the corresponding\\r\\nWe group modalities of human data into dense Id∗ and sparse rep-\\r\\n                                                                               sparse embedding is defined as fss = Mss ◦ Es (Is ), where\\r\\nresentations Is∗ . Three levels of embeddings are extracted (3.1).                             ′\\r\\n\\r\\nCombining the nature of human data and tasks (3.2), we present\\r\\n                                                                               fss ∈ RJ×K , Mss is a mapper network. For dense rep-\\r\\ncontrastive learning targets for each level of embedding (3.3).                resentations, the corresponding sparse features are pooled\\r\\n                                                                               from the dense feature map using the correspondences G.\\r\\nproposes the first multi-view contrastive learning paradigm                    Then the sparse features are mapped to sparse embeddings\\r\\nwhich takes any number of views. CLIP [39] learns a                            as fds = Mds ◦ G ◦ Ed (Id ). 3) Dense embeddings are only\\r\\njoint latent space from large-scale paired image-language                      defined on dense representations, which is formulated as\\r\\ndataset. Extensive studies [1, 15, 17, 34, 35, 41] focus on                    fdd = Mdd ◦ Ed (Id ). With three levels of embeddings de-\\r\\nvideo-audio contrastive learning. Recently, 2D-3D con-                         fined, we formulate the overall learning objective as\\r\\ntrastive learning [21, 29, 30] has also been studied with the\\r\\n                                                                                           \\\\label {eq:whole} \\\\mathcal {L} = \\\\lambda _g\\\\mathcal {L}_g(f^g) + \\\\lambda _d\\\\mathcal {L}_d(f^d) + \\\\lambda _s\\\\mathcal {L}_s(f^s),    (1)\\r\\ndevelopment in 3D computer vision. In this work, aside\\r\\nfrom commonly used modalities, we also explore the poten-                      which is analyzed and explained as follows.\\r\\ntial of 2D keypoints in human-centric contrastive learning.\\r\\n                                                                               3.2. Principles of Learning Targets Design\\r\\n                                                                                   In this subsection, we analyze the intuitions when de-\\r\\n3. Our Approach                                                                signing learning targets, which makes the following three\\r\\n   In this section, we first introduce the general paradigm                    principles. 1) Mutual Information Maximization: In-\\r\\nof HCMoCo (3.1). Following the design principles (3.2),                        spired by [36,47], we propose to maximize the lower bound\\r\\nhierarchical contrastive learning targets are formally intro-                  on mutual information, which has been proved by many pre-\\r\\nduced (3.3). Next, an instantiation of HCMoCo is intro-                        vious works [5, 6, 18, 45] to be able to produce strong pre-\\r\\nduced (3.4). Finally, we propose two applications of HC-                       train models. 2) Continuous and Ordinal Feature Distri-\\r\\nMoCo to show the versatility (3.5).                                            bution: Inspired by the property of human-centric percep-\\r\\n                                                                               tion, it is desirable for the feature maps of the human body\\r\\n3.1. HCMoCo                                                                    to be continuous and ordinal. The human body is a struc-\\r\\n   As shown in Fig. 2, HCMoCo takes multiple modalities                        tural and continuous surface. The dense predictions, e.g.\\r\\nof perceived human body as input. The target is to learn                       human parsing [11, 12, 25], DensePose [14], are also con-\\r\\nhuman-centric representations, which can be transferred to                     tinuous. Therefore, such property should also be reflected\\r\\ndownstream tasks. The input modalities can be categorized                      in the learned representations. Besides, for an anchor point\\r\\ninto dense and sparse representations. Dense representa-                       on human surfaces, closer points have higher probabilities\\r\\ntions Id∗ are the direct output of imaging sensors, e.g. RGB,                  of sharing similar semantics with the anchor point than that\\r\\ndepth, infrared. They typically contain rich information but                   of far away points. Therefore, the learned dense repre-\\r\\nare low-level and noisy. Sparse representations are struc-                     sentations should also align with such ordinal relationship.\\r\\ntured abstractions of the human body, e.g. 2D keypoints, 3D                    3) Structure-Aware Semantic Consistency: Sparse repre-\\r\\npose, which can be formulated as graph Is∗ = G(V, E). Dif-                     sentations are abstractions of the human body, which con-\\r\\nferent representations of the same view of a human should                      tains valuable structural semantics about the human body.\\r\\nbe spatially aligned, which means intra-sample correspon-                      Instead of identity information, the human pose and struc-\\r\\ndences can be obtained for dense contrastive learning. HC-                        1 For easier understanding of the notations, the superscripts of f and\\r\\nMoCo aims to pre-train multiple encoders Ed∗ and Es∗ that                      M stand for the kind of embeddings. The subscripts stand for the kind of\\r\\nproduce embeddings of dense representations and sparse                         representations (‘g’ for ‘global’; ‘d’ for ‘dense’; ‘s’ for ‘sparse’).\\r\\n\\r\\n\\r\\n                                                                           3\\r\\n\\x0c\\n\\n                                                                                                                                                                                                                                         positive pair                                                  soft positive pair                                                                                          negative pair\\r\\n\\r\\n\\r\\n                                                                      𝐸!\"\\r\\n                                                                                                                                                                                                                                                                  %\\r\\n                                                                                                                                                                                                                                                          𝑓!\"                                                                                                                                                                                                                                                                                                  $\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             𝑓!\"\\r\\n                                                                                                                                                                                                                                                                         !\\r\\n                                                                                                                                                                                                                                                                        𝑓!\"\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                w=0.5\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                       w=0.1\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                               w=0.01\\r\\n                                                                      𝐸!#                                                                                                                                                                                         %                                                                                                                                                                                                                                                                                             $\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              𝑓!#\\r\\n                                                                                                                                                                                                                                                           𝑓!#\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                         !\\r\\n                                                                                                                                                                                                                                                                  %     𝑓!#\\r\\n                                                                       𝐸$                                                                                                                                                                                 𝑓$                                                                                                                                                                                                                                                                                                  𝑓$$\\r\\n\\r\\n                                                                                                                       a) Sample-level Modality-invariant                                                                                                                            b) Dense Intra-sample                                                                                                            c) Sparse Structure-aware\\r\\n                               Encoding                                                                                     Representation Learning                                                                                                                                  Contrastive Learning                                                                                                                Contrastive Learning\\r\\n\\r\\nFigure 3. Our Proposed Instantiation of HCMoCo. For dense representations, we choose to use RGB and depth. For sparse represen-\\r\\ntations, 2D keypoints are used for its convenience to obtain. a) At sample-level, the global embeddings are used for modality-invariant\\r\\nrepresentation learning. b) Between paired dense embeddings, soft contrastive learning target is proposed for continuous and ordinal feature\\r\\nlearning. c) Using human prior provided by sparse representations, intra- and inter-sample contrastive learning targets are proposed.\\r\\n\\r\\nture understanding are the keys to our target downstream                                                                                                                                                                                                                                 sentation, 1 ≤ x, x′ , m ≤ H, 1 ≤ y, y ′ , n ≤ W . The\\r\\ntasks. Therefore, it is reasonable to eliminate the iden-                                                                                                                                                                                                                                above equation is a generalized version of InfoNCE [33].\\r\\n                                                                                                                                                                                                                                                                                                                           mn\\r\\ntity information and enhance the structure information by                                                                                                                                                                                                                                InfoNCE is a special case when Wxy    is set to 1 if x = m\\r\\nenforcing structure-aware semantic consistency where se-                                                                                                                                                                                                                                 and y = n else 0. We use the normalized distances as the\\r\\nmantically close embeddings (e.g. embeddings of left hands                                                                                                                                                                                                                               weights, which is formulated as\\r\\nfrom different samples) are pulled close and vice versa.\\r\\n                                                                                                                                                                                                                                                                                                               \\\\mathcal {W}_{xy}^{mn} = \\\\frac {\\\\text {exp}(\\\\sqrt {(x - m)^2 + (y - n)^2})}{\\\\sum _{x\\', y\\'}\\\\text {exp}(\\\\sqrt {(x - x\\')^2 + (y - y\\')^2})}.                                                                                                                (4)\\r\\n3.3. Hierarchical Contrastive Learning Targets\\r\\n                                                                                                                                                                                                                                                                                         For each pair of dense representations, the above learning\\r\\n   Based on the above three principles, we formally define                                                                                                                                                                                                                               target is calculated between each pair of dense embeddings.\\r\\nhierarchical contrastive learning targets in this subsection.                                                                                                                                                                                                                            Therefore, the whole learning target is defined as\\r\\nSample-level modality-invariant representation learn-                                                                                                                                                                                                                                                                                                    \\\\mathcal {L}_d = \\\\underset {\\\\underset {f_{d1}^d, f_{d2}^d \\\\in F_1^d, F_2^d}{F_1^d, F_2^d \\\\in S_d}}{\\\\mathbb {E}} \\\\mathcal {L}_d^{12}(f_{d1}^d, f_{d2}^d),                                                      (5)\\r\\ning aims at learning a joint latent space at the sample level\\r\\nusing global embeddings, which fulfills the first principle.\\r\\nInspired by [45], the learning target can be formulated as                                                                                                                                                                                                                               where F∗d is a set of dense embeddings of one modality, Sd\\r\\n                                                                                                                                                                                                                                                                                                                  d\\r\\n                                                                                                                                                                                                                                                                                         is the set of all F∗d , fd1      d\\r\\n                                                                                                                                                                                                                                                                                                                     and fd2 are two paired embeddings.\\r\\n               \\\\mathcal {L}_g = -\\\\underset {\\\\underset {f_1^g \\\\in F_1^g}{F_1^g, F_2^g \\\\in S_g}}{\\\\mathbb {E}} \\\\left [ \\\\text {log}\\\\frac {\\\\text {exp}(f_1^g \\\\cdot \\\\bar {f_2^g} / \\\\tau )}{\\\\sum _{f_2^g \\\\in F_2^g} \\\\text {exp}(f_1^g \\\\cdot f_2^g / \\\\tau )} \\\\right ],        (2)                It should be noticed that the ‘soft’ learning target cannot\\r\\n                                                                                                                                                                                                                                                                                         guarantee an ordinal feature distribution. Instead, it serves\\r\\n                                                                                                                                                                                                                                                                                         as a computationally efficient relaxation of the requirement\\r\\nwhere F∗g is a set of global embeddings of one modality, Sg                                                                                                                                                                                                                              of ordinal distribution.\\r\\nis the set of F∗g of all modalities, f¯2g is the embedding of the\\r\\npaired view of that of f1g , τ is the temperature. It should be                                                                                                                                                                                                                          Sparse structure-aware contrastive learning takes two\\r\\nnoticed that f1g can be sampled from the global embeddings                                                                                                                                                                                                                               sparse representations f1s and f2s as inputs. The paired fea-\\r\\nof either dense or sparse representations.                                                                                                                                                                                                                                                       s      s\\r\\n                                                                                                                                                                                                                                                                                         tures f1j and f2j  (i.e. features of the j-th joint) should be\\r\\nDense intra-sample contrastive learning is operated on                                                                                                                                                                                                                                   pulled close while unpaired features are pushed away. The\\r\\nthe paired dense representations. For any two paired dense                                                                                                                                                                                                                               two sparse representations can be sampled from the same\\r\\n                                         ′\\r\\nembeddings fd1   d    d\\r\\n                   , fd2 ∈ RH×W ×K , to simultaneously sat-                                                                                                                                                                                                                              or different modalities, intra- or inter-sample. The intra-\\r\\nisfy the first and the second principle, the dense intra-                                                                                                                                                                                                                                sample alignment satisfies the first principle. The inter-\\r\\nsample contrastive learning target between them is defined                                                                                                                                                                                                                               sample alignment follows the third principle. The sparse\\r\\nin a ‘soft’ way as                                                                                                                                                                                                                                                                       structure-aware contrastive learning target is formulated as\\r\\n\\r\\n\\r\\n  \\\\mathcal {L}_d^{12} = -\\\\underset {\\\\underset {m, n}{x, y}}{\\\\mathbb {E}} \\\\left [ \\\\mathcal {W}_{xy}^{mn} \\\\text {log} \\\\frac {\\\\text {exp}(f_{d1}^d(x, y) \\\\cdot f_{d2}^d(m, n)) / \\\\tau )}{\\\\sum \\\\limits _{x\\', y\\'} \\\\text {exp}(f_{d1}^d(x,y) \\\\cdot f_{d2}^d(x\\', y\\') / \\\\tau )} \\\\right ],          \\\\mathcal {L}_s = -\\\\underset {\\\\underset {j; f_1^s, f_2^s \\\\in \\\\{F_1^s, F_2^s\\\\}}{F_1^s, F_2^s \\\\in S_s}}{\\\\mathbb {E}} \\\\left [\\\\text {log}\\\\frac {\\\\text {exp}(f_{1j}^{s}\\\\cdot f_{2j}^{s} / \\\\tau )}{\\\\sum \\\\limits _{j\\'; f_i^s \\\\in \\\\{F_1^s, F_2^s\\\\}} \\\\text {exp}(f_{1j}^s \\\\cdot f_{ij\\'}^s / \\\\tau )} \\\\right ], \\r\\n\\r\\n                                                            (3)                                                                                                                                                                                                                                                                                       (6)\\r\\n           mn\\r\\nwhere Wxy        is the weight, τ is the temperature,                                                                                                                                                                                                                                    where F∗s is a set of sparse embeddings of one modality,\\r\\n(x, y), (m, n), (x′ , y ′ ) are coordinates on the dense repre-                                                                                                                                                                                                                          Ss is the set of F∗s , τ is the temperature, f1s , f2s are sam-\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                     4\\r\\n\\x0c\\n\\n                                                     ℒ′ GT                                                           Label Distribution\\r\\n                    𝐸!\"                      𝐷\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                       RGB\\r\\n                                                     Training\\r\\n                                ℒ\\r\\n                    𝐸!#                      𝐷\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                       Depth\\r\\n                                                     Inference\\r\\n                   (a) Cross-Modality Supervision\\r\\n\\r\\n                                                     ℒ′ GT\\r\\n                    𝐸!\"                      𝐷\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                       Annotation\\r\\n                                                                                                               right hip        right knee    right foot\\r\\n                                                                                                               left hip         left knee     left foot\\r\\n                                                                                                               left shoulder    left elbow    left hand\\r\\n                                                      Training                                                 right shoulder   right elbow   right hand\\r\\n\\r\\n                            ℒ                                                                                  crotch\\r\\n                                                                                                               left thigh\\r\\n                                                                                                                                right thigh\\r\\n                                                                                                                                left calf\\r\\n                                                                                                                                              right calf\\r\\n                                                                                                                                              lower spine\\r\\n                                                                                                               upper spine      head          left arm\\r\\n\\r\\n                    𝐸!#                       𝐷\\r\\n                                                                                                               left forearm     right arm     right forearm\\r\\n                                     OR\\r\\n                                                     Inference         Figure 5. Illustration of the RGB-D human parsing dataset\\r\\n                   (b) Missing-Modality Inference                      NTURGBD-Parsing-4K.\\r\\n    Figure 4. Pipelines of Two Applications of HCMoCo.\\r\\n                                                                       intra-sample contrastive learning target. With the feature\\r\\npled from the union of F1s and F2s . To conclude, the overall          maps of different modalities aligned, it is straightforward to\\r\\nlearning target is formulated as Eq. 1, where λ∗ are the               implement the two extensions, which are shown in Fig. 4.\\r\\nweights to balance the targets.                                        Cross-Modality Supervision is a novel task where we train\\r\\n                                                                       the network on the source modality, while test on the target\\r\\n3.4. Instantiation of HCMoCo                                           modality. This is a practical scenario where people trans-\\r\\n     In this section, we introduce an instantiation of HC-             fer the knowledge of some single modality dataset to other\\r\\nMoCo. As shown in Fig. 3, for dense representations,                   modalities. At training time, an additional downstream task\\r\\nwe use RGB and depth. Large-scale paired human RGB                     head (e.g. segmentation head) D is attached to the backbone\\r\\nand depth data is easy to obtain with affordable sensors e.g.          of the source modality. The hierarchical contrastive learn-\\r\\nKinect. These two modalities are the most commonly en-                 ing targets L together with downstream task loss L′ are used\\r\\ncountered in human-centric tasks [7, 11, 12, 22, 25]. More-            for end-to-end training. At inference time, D is attached to\\r\\nover, a proper pre-train model for depth is highly desired.            the backbone of the target modality. The extracted feature\\r\\nTherefore, RGB and depth are reasonable choices of hu-                 maps of the target modality are passed to D for prediction.\\r\\nman dense representations, both of which are easy to ac-               Missing-Modality Inference is another novel task where\\r\\nquire and important to downstream tasks. For sparse repre-             we train the network using multi-modal data and inference\\r\\nsentations, 2D keypoints are used, which provide positions             on single modality. Multi-modal data collection in practice\\r\\nof human body joints in the image coordinate. Off-the-shelf            would inevitably result in data with incomplete modalities,\\r\\ntools [4, 9] are available to quickly and robustly extract hu-         which brings the requirement of missing-modality infer-\\r\\nman 2D keypoints given RGB images. Using 2D keypoints                  ence. At training time, the feature maps of multiple modal-\\r\\nas the sparse representation is a good balance between the             ities are fused using max-pooling and fed to a downstream\\r\\namount of human prior and acquisition difficulty.                      task head D. Similarly, hierarchical contrastive learning tar-\\r\\n     For RGB inputs Id1 , an image encoder Ed1 [43] is applied         gets L and downstream task loss L′ are used for co-training.\\r\\nto obtain feature maps Ed1 (Id1 ). Similarly, for depth inputs         At inference time, the feature map of a single modality is\\r\\nId2 , an image encoder [43] or 3D encoder [37, 38] Ed2 can             passed to D for missing-modality inference.\\r\\nbe applied to extract feature maps Ed2 (Id2 ). 2D keypoints Is\\r\\nare encoded by a GCN-based encoder [51] Es to produce\\r\\nsparse features Es (Is ). Mapper networks comprise a single\\r\\nlinear layer and a normalization operation.                            4. NTURGBD-Parsing-4K Dataset\\r\\n     As for the implementation of contrastive learning targets,           Although RGB human parsing has been well studied [7,\\r\\nwe choose to use a memory pool to store all the global em-             11, 12, 25], human parsing on depth [19, 32] or RGB-D data\\r\\nbeddings which are updated in a momentum way. Sparse                   has not been fully addressed due to the lack of labeled data.\\r\\nand dense embeddings cannot all fit in memory. Therefore,              Therefore, we contribute the first RGB-D human parsing\\r\\nfor the last two types of contrastive learning targets, the neg-       dataset: NTURGBD-Parsing-4K. The RGB and depth are\\r\\native samples are sampled within a mini-batch.                         uniformly sampled from NTU RGB+D (60/120) [27, 42].\\r\\n                                                                       As shown in Fig. 5, we annotate 24 human parts for paired\\r\\n3.5. Versatility of HCMoCo\\r\\n                                                                       RGB-D data. The partition protocols follow that of [22].\\r\\n   On top of the pre-train framework HCMoCo, we pro-                   The train and test set both have 1963 samples. The whole\\r\\npose to further extend it on two direct applications: cross-           dataset contains 3926 samples. Hopefully, by contributing\\r\\nmodality supervision and missing-modality inference. The               this dataset, we could promote the development of both hu-\\r\\nextensions are based on the key design of HCMoCo: dense                man perception and multi-modality learning.\\r\\n\\r\\n\\r\\n                                                                   5\\r\\n\\x0c\\n\\nTable 1. DensePose Estimation Results on COCO. * randomly initializes the model before pre-training. † initializes the model by ImageNet\\r\\npre-train before pre-training. All results in [%].\\r\\n                                                        Full Data                    10% Data\\r\\n     Method           Pre-train Datasets\\r\\n                                            BBox AP GPS AP GPSM AP IOU AP BBox AP GPS AP GPSM AP IoU AP\\r\\n     From Scratch          -                 57.27       62.03        63.61        65.88      39.38       35.75       41.62    49.92\\r\\n     CMC* [45]        NTURGBD+MPII           60.33       64.97        65.66        66.96      44.92       43.84       47.94    54.00\\r\\n     MMV* [1]         NTURGBD+MPII           59.89       64.23        65.47        67.03      43.24       41.40       45.99    52.52\\r\\n     Ours*            NTURGBD+MPII           61.33       65.89        66.92        67.66      47.76       48.47       51.65    56.15\\r\\n     IN Pre-train          -                 62.66       66.48        67.42        68.63      48.28       44.34       49.11    56.11\\r\\n     CMC† [45]        NTURGBD+MPII           62.76       66.16        67.30        68.06      49.21       48.82       52.57    57.94\\r\\n     MMV† [1]         NTURGBD+MPII           62.97       66.67        67.51        68.29      50.16       50.28       53.54    58.32\\r\\n     Ours†            NTURGBD+MPII           63.11       67.33        68.12        68.72      50.29       51.50       54.47    58.66\\r\\n     CMC† [45]        NTURGBD+COCO           63.58       67.22        67.77        68.46      51.77       53.53       56.18    59.37\\r\\n     Ours†            NTURGBD+COCO           62.95       67.77        68.29        68.63      52.18       54.01       56.64    59.93\\r\\n\\r\\nTable 2. Human Parsing Results on Human3.6M. * randomly initializes the model before pre-training. † initializes the model by ImageNet\\r\\npre-train before pre-training. All results in [%].\\r\\n                               Full Data                     20% Data                      10% Data                   1% Data\\r\\n       Method\\r\\n                       mIoU     mAcc aAcc            mIoU      mAcc aAcc          mIoU       mAcc aAcc        mIoU     mAcc aAcc\\r\\n       From Scratch    44.13     58.88     98.82     42.41    56.25       98.81   32.61     43.76     98.52   7.27     10.97   97.45\\r\\n       CMC* [45]       54.33     68.01     99.09     52.10    65.65       99.03   48.37     61.18     98.95   14.61    20.07   98.07\\r\\n       MMV* [1]        52.69     65.82     99.06     50.66    63.55       99.01   46.23     58.52     98.90   12.86    17.10   97.94\\r\\n       Ours*           61.36     75.09     99.25     59.17    73.44       99.19   57.08     71.75     99.13   16.55    22.27   98.18\\r\\n       IN Pre-train    56.90     69.94     99.14     48.86    60.75       98.97   44.55     56.86     98.87   14.65    20.22   98.09\\r\\n       CMC† [45]       58.93     71.70     99.20     57.41    70.13       99.17   54.35     67.47     99.09   17.77    23.77   98.20\\r\\n       MMV† [1]        59.08     71.57     99.20     57.28    69.69       99.17   53.86     66.46     99.08   17.66    23.54   98.20\\r\\n       Ours†           62.50     75.84     99.27     60.85    74.23       99.23   58.28     71.99     99.17   20.78    27.52   98.34\\r\\n\\r\\n5. Experiments                                                             to use general multi-modal contrastive learning methods\\r\\n                                                                           CMC [45] and MMV [1] as the baselines. Although there\\r\\n5.1. Experimental Setup                                                    are other multi-modal contrastive learning works, they ei-\\r\\nImplementation Details. The default RGB and depth en-                      ther require the multi-view calibration [21] or focus on\\r\\ncoders are HRNet-W18 [43]. The default datasets for pre-                   multi-modal downstream tasks [17, 29] and therefore are\\r\\ntrain are NTU RGB+D [27] and MPII [2]. The former pro-                     not suitable for comparison. In addition, for RGB tasks,\\r\\nvides paired indoor human RGB, depth, and 2D keypoints,                    we also experiment under two settings, one initializes en-\\r\\nThe latter provides in-the-wild human RGB and 2D key-                      coders with supervised ImageNet [24] (IN) pre-train while\\r\\npoints. Mixing human data from different domains helps                     the other does not.\\r\\nour pre-train models adapt to a wilder domain.\\r\\n                                                                           5.2. Performance on Downstream Tasks\\r\\nDownstream Tasks. We test our pre-train models on four\\r\\ndifferent human-centric downstream tasks, two on RGB                       DensePose Estimation. As shown in Tab. 1, we test Dense-\\r\\nimages and two on depth. 1) DensePose estimation on                        Pose estimation [14] under two settings: full and 10% of the\\r\\nCOCO [14]: DensePose aims at mapping pixels of the ob-                     training data. The trained models are tested on the full val-\\r\\nserved human body to the surface of a 3D human body,                       idation set of DensePose. Firstly, if not using IN pre-train,\\r\\nwhich is a highly challenging task. 2) RGB human parsing                   our pre-train model significantly outperforms both ‘From\\r\\non Human3.6M [22]. Human3.6M provides pure human                           Scratch’ and two baseline methods. Especially under 10%\\r\\npart segmentation, which aligns with our objectives. We                    of training data, 12.7% improvement in terms of GPS AP\\r\\nuniformly sample 2fps of the video for training and evalu-                 is observed. And our pre-train model even outperforms that\\r\\nation. 3) Depth human parsing on NTURGBD-Parsing-4K.                       using IN pre-train by 4.13% in terms of GPS AP. When\\r\\n4) 3D pose estimation from depth maps on ITOP [16] (only                   we use IN pre-train as initialization, which is a common\\r\\nside view). For all the above downstream tasks, we use the                 practice for 2D tasks, our method still outperforms all the\\r\\npre-train backbones for end-to-end fine-tune.                              baselines. Our method surpasses IN pre-train by 7.2% and\\r\\nComparison Methods. Since there are few previous                           5.4% in terms of GPS/GPSM AP under 10% setting. To\\r\\nhuman-centric multi-modal pre-train methods, we propose                    further test the performance of in-domain transfer, we also\\r\\n\\r\\n\\r\\n                                                                      6\\r\\n\\x0c\\n\\n                 Table 3. Ablation Study on Densepose/ Human3.6M/ ITOP/ NTURGBD-Parsing-4K. All results in [%].\\r\\n                                             DensePose 10%                ITOP 0.1%/ 0.2%      Human3.6M 10%       NTURGBD 20%\\r\\n    Method\\r\\n                                   BBox      GPS    GPSM          IoU      Acc     Acc         mIoU    mAcc        mIoU  mAcc\\r\\n    Sample-level Mod-invariant     49.21     48.82    52.57       57.94   57.73      50.08     54.35     67.47     30.40     51.54\\r\\n    + Hard Dense Intra-sample      49.40     49.14    52.49       57.30   56.43      54.05     55.36     68.43     31.26     51.54\\r\\n    + Soft Dense Intra-sample      50.21     50.25    53.42       57.70   62.33      51.50     56.35     69.26     32.20     51.06\\r\\n    + Sparse Structure-aware       50.29     51.50    54.47       58.66   65.83      62.36     58.28     71.99     35.01     52.55\\r\\n\\r\\npre-train models using training sets of NTU RGB+D and                     5.3. Ablation Study\\r\\nCOCO. The performance gain under 10% setting further\\r\\n                                                                             In this subsection, we perform a thorough ablation study\\r\\nimproves to 9.7% and 7.5% in terms of GPS/GPSM AP.\\r\\n                                                                          on HCMoCo to justify the design choices. As shown\\r\\nRGB Human Parsing. As shown in Tab. 2, we test four                       in Tab. 3, we firstly report the results of only apply-\\r\\nsettings on Human3.6M [22]: full, 20%, 10% and 1% train-                  ing sample-level modality-invariant representation learning.\\r\\ning data. In all settings, our method outperforms all base-               Then we add dense intra-sample contrastive learning and\\r\\nlines in all metrics. On full training data, we outperform                sparse structure-aware contrastive learning in order. To fur-\\r\\nIN pre-train by 5.6% in terms of mIoU. The performance                    ther demonstrate the effect of the ‘soft’ design in dense\\r\\ngain increases with the amount of training data decreases.                intra-sample contrastive learning, we also report results of\\r\\nIt is worth noticing that with only 10% of training data, our             the ‘hard’ learning target, which takes the form of a classic\\r\\nmethod outperforms IN pre-train with full training data.                  InfoNCE [33]. We report the results of the ablation study\\r\\n                                                                          on all four downstream tasks under data-efficient settings.\\r\\nTable 4. Human Parsing Results on NTURGBD-Parsing-4K [%].                    For DensePose estimation, it is important to learn fea-\\r\\n                     Full Data               20% Data                     ture maps that are continuously and ordinally distributed,\\r\\n  Method                                                                  which is the expected result of soft dense intra-sample con-\\r\\n                 mIoU mAcc aAcc          mIoU mAcc aAcc\\r\\n  IN Pre-train   37.49   57.52   98.36   28.56    46.81   98.10           trastive learning. The performance gain of the soft learn-\\r\\n  CMC [45]       38.20   58.73   98.39   30.40    51.54   98.02           ing target over the hard counterpart justifies the observation\\r\\n  MMV [1]        38.09   58.49   98.37   30.41    50.62   98.07           and the learning target design. The dense intra-sample con-\\r\\n  Ours           39.32   58.79   98.47   35.01    52.55   98.53           trastive learning also shows superiority on three other down-\\r\\n                                                                          stream tasks, which shows the importance of fine-grained\\r\\n                                                                          contrastive learning targets for dense prediction tasks.\\r\\nDepth Human Parsing. As shown in Tab. 4, we test\\r\\n                                                                             Explicitly injecting human prior into the network\\r\\nthe pre-train depth backbone on our proposed Dataset\\r\\n                                                                          through sparse structure-aware contrastive learning also\\r\\nNTURGBD-Parsing-4K with all training data and 20%\\r\\n                                                                          proves its effectiveness by further improving the perfor-\\r\\ntraining data. We outperform all baselines on two settings.\\r\\n                                                                          mance on DensePose. Thanks to the strong hints provided\\r\\nEspecially, only using 20% of training data, we surpass IN\\r\\n                                                                          by 2D keypoints, the performance of 3D pose estimation is\\r\\npre-train by 6.4% and MMV [1] by 4.6% in terms of mIoU.\\r\\n                                                                          improved. Moreover, the sparse structure-aware contrastive\\r\\n                                                                          learning boosts the performance of human parsing both on\\r\\nTable 5. 3D Pose Estimation Results on ITOP. All results in [%].\\r\\n                                                                          RGB and depth maps by 1.9% and 2.8% respectively in\\r\\n  Method         100%    10%      1%       0.5%   0.2%    0.1%            terms of mIoU. Although 2D keypoints are sparse priors,\\r\\n  IN Pre-train   85.19   83.44 77.20 54.31 13.27 14.21                    they still provide the rough location of each part of the hu-\\r\\n  CMC [45]       87.08   85.36 79.49 75.07 57.73 50.08                    man body, which facilitate the feature alignment of same\\r\\n  MMV [1]        86.13   83.49 79.70 71.70 60.83 54.44                    body parts. To summarize, the sparse and dense learning\\r\\n  Ours           87.19   85.49 78.71 76.34 65.83 62.36                    targets both contribute to the performance of our methods,\\r\\n                                                                          which is in line with our analysis.\\r\\n3D Pose Estimation. As shown in Tab. 5, we test the pre-\\r\\n                                                                          5.4. Performance on HCMoCo Versatility\\r\\ntrain depth backbone on ITOP [16] with six different ra-\\r\\ntios of training data. Our pre-train model outperforms all                Cross-Modality Supervision. We test the cross-modality\\r\\nbaselines on most settings. With only 10% training data,                  supervision pipeline on the task of human parsing on\\r\\nthe accuracy of our method outperforms that of IN pre-train               NTURGBD-Parsing-4K because it has two modalities and\\r\\nwith all training data. It is also worth noticing that 0.1% of            respective dense annotations. Two baseline methods are\\r\\ntraining data are 17 samples, which makes this a few-shot                 adopted: 1) using CMC [45] contrastive learning target; 2)\\r\\nlearning setting. With such limited training data, IN pre-                no contrastive learning target. For a fair comparison, the\\r\\ntrain barely produce meaningful results, while our method                 backbones of all methods are initialized by CMC [45] pre-\\r\\nimproves the accuracy by 48.2%.                                           train. At training time, the target modality of training data\\r\\n\\r\\n\\r\\n                                                                     7\\r\\n\\x0c\\n\\nTable 6. Cross-Modality Supervised Human Parsing Results on           Table 8. Experiments on changing the backbone. * stands for\\r\\nNTURGBD-Parsing-4K. All results in [%].                               ‘IN Pre-train’ for DensePose and ‘From Scratch’ for NTURGBD-\\r\\n                                                                      Parsing-4K. All results in [%].\\r\\n                        RGB → Depth             Depth → RGB\\r\\n Method\\r\\n                      mIoU mAcc aAcc          mIoU mAcc aAcc                             DensePose 10%          NTURGBD 20%\\r\\n                                                                       Method\\r\\n No Contrastive 3.94 4.36 92.24 3.71 4.03 91.63                                   BBox    GPS GPSM IoU          mIoU  mAcc\\r\\n CMC [45]        3.86 5.59 86.81 3.85 4.27 91.75                       *        55.10 54.60      57.60   61.73 45.36     59.51\\r\\n Ours           33.19 54.38 94.70 26.80 48.80 92.84                    CMC [45] 53.88 54.62      57.46   61.14 48.74     62.94\\r\\n                                                                       Ours     54.55 55.80      58.36   61.75 49.43     63.52\\r\\nis not available. We experiment on two settings where we\\r\\nsupervise on RGB, test on depth (RGB → Depth), and vice               both the full training data and data-efficient settings.\\r\\nversa (Depth → RGB). As shown in Tab. 6, our method                   Changing Backbone. So far our experiments are all\\r\\noutperforms both baselines under two settings. Specifically,          performed on HRNet-W18. To further demonstrate HC-\\r\\nour method improves the mIoU of both settings by 29.2%                MoCo’s performance on other backbones, for the 2D back-\\r\\nand 23.0%, respectively. Even compared to methods with                bone, we also experiment with HRNet-W32 [43]. For the\\r\\ndirect supervision, we can achieve comparable results.                depth backbone, we choose to test with PointNet++ [38].\\r\\nTable 7. Missing-Modality Human Parsing Results on                    For the RGB pre-train model, we experiment on the 10%\\r\\nNTURGBD-Depth. All results in [%].                                    DensePose estimation. For the depth pre-train model, we\\r\\n                                                                      experiment on the 20% NTURGBD-Parsing-4K. As shown\\r\\n                          Only RGB                Only Depth\\r\\n Method                                                               in Tab. 8, our method outperforms its pre-train counterparts\\r\\n                      mIoU mAcc aAcc          mIoU mAcc aAcc\\r\\n                                                                      by a reasonable margin, which is in line with our previous\\r\\n No Contrastive 13.45 14.77 93.35 24.41 30.49 95.27\\r\\n                                                                      experimental results.\\r\\n CMC [45]       19.62 28.19 92.94 16.58 19.83 93.94\\r\\n Ours           43.88 64.27 96.15 43.98 63.66 96.34\\r\\n                                                                      6. Discussion and Conclusion\\r\\nMissing-Modality Inference. For missing-modality in-                      In this work, we propose the first versatile multi-modal\\r\\nference, we report the experiments on the same dataset                pre-training framework HCMoCo specifically designed for\\r\\nand same baselines as above. As shown in Tab. 7, with                 human-centric perception tasks. Hierarchical contrastive\\r\\nno pixel-level alignment, the two baseline methods strug-             learning targets are designed based on the nature of hu-\\r\\ngle in two missing-modality settings i.e. ‘Only RGB’ and              man datasets and the requirements of human-centric down-\\r\\n‘Only Depth’. While our method improves the segmenta-                 stream tasks. Extensive experiments on four different hu-\\r\\ntion mIoU by 24.3% and 19.6% on two settings.                         man downstream tasks of different modalities demonstrated\\r\\n                                                                      the effectiveness of our pre-training framework. We con-\\r\\n                      IN Pre-train    CMC           Ours              tribute a new RGB-D human parsing dataset NTURGBD-\\r\\n        65                           60                               Parsing-4K to support research of human perception on\\r\\n        55                           50                               RGB-D data. Besides downstream task transfer, we also\\r\\n                                     40                               propose two novel applications of HCMoCo to show its ver-\\r\\n mIoU\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n        45\\r\\n                                     30                               satility and ability in cross-modal reasoning.\\r\\n        35\\r\\n                                     20                               Potential Negative Impacts & Limitations. Usage of\\r\\n        25                           10                               large amounts of data and long training time might nega-\\r\\n        15                            0                               tively impact the environment. Moreover, even though we\\r\\n             1   10 19 28 37 46           0   30    60 90 120\\r\\n                    epochs                         epochs             did not collect any new human data in this work, human data\\r\\n                                                                      collection could happen if our framework is used in other\\r\\nFigure 6. Validation mIoU Changes with Training Epochs In-\\r\\ncrease. Left: Human3.6M human parsing full training set. Right:       applications, which potentially raises privacy concerns. As\\r\\nHuman3.6M human parsing 20% training set.                             for the limitations, due to limited resources, we could only\\r\\n                                                                      experiment with one possible instantiation of HCMoCo.\\r\\n5.5. Further Analysis                                                 And for the same reason, even though the theoretical possi-\\r\\n                                                                      bility exists, we do not have the chance to further scale up\\r\\nFaster Convergence. One of the advantages of pre-training             the amount of human dataset and network size.\\r\\nis the fast convergence speed when transferred to down-               Acknowledgments           This work is supported by NTU\\r\\nstream tasks. Our HCMoCo also shows superiority in this               NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the\\r\\nfeature. We log the validation mIoU of Human3.6M hu-                  RIE2020 Industry Alignment Fund – Industry Collabora-\\r\\nman parsing at different training epochs. As shown in Fig.            tion Projects (IAF-ICP) Funding Initiative, as well as cash\\r\\n6, compared with IN pre-train and CMC [45], our pre-train             and in-kind contribution from the industry partner(s).\\r\\nmodel is able to converge within a few training epochs in\\r\\n\\r\\n\\r\\n                                                                  8\\r\\n\\x0c\\n\\nReferences                                                                    ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\\r\\n                                                                              mad Gheshlaghi Azar, et al. Bootstrap your own latent: A\\r\\n [1] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider,                new approach to self-supervised learning. arXiv preprint\\r\\n     Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lu-                arXiv:2006.07733, 2020. 3\\r\\n     cas Smaira, Sander Dieleman, and Andrew Zisserman. Self-            [14] Rıza Alp Güler, Natalia Neverova, and Iasonas Kokkinos.\\r\\n     supervised multimodal versatile networks. NeurIPS, 2(6):7,               Densepose: Dense human pose estimation in the wild. In\\r\\n     2020. 3, 6, 7, 15, 16, 17                                                Proceedings of the IEEE Conference on Computer Vision\\r\\n [2] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and                 and Pattern Recognition, pages 7297–7306, 2018. 1, 2, 3,\\r\\n     Bernt Schiele. 2d human pose estimation: New benchmark                   6, 12, 13\\r\\n     and state of the art analysis. In IEEE Conference on Com-           [15] Tengda Han, Weidi Xie, and Andrew Zisserman. Self-\\r\\n     puter Vision and Pattern Recognition (CVPR), June 2014. 1,               supervised co-training for video representation learning.\\r\\n     2, 6                                                                     arXiv preprint arXiv:2010.09709, 2020. 3\\r\\n [3] Mahmoud Assran, Nicolas Ballas, Lluis Castrejon, and                [16] Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Ser-\\r\\n     Michael Rabbat. Supervision accelerates pre-training in con-             ena Yeung, and Li Fei-Fei. Towards viewpoint invariant 3d\\r\\n     trastive semi-supervised learning of visual representations.             human pose estimation. In European Conference on Com-\\r\\n     arXiv preprint arXiv:2006.10803, 2020. 2                                 puter Vision, pages 160–177. Springer, 2016. 2, 6, 7, 12\\r\\n [4] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.            [17] Devamanyu Hazarika, Roger Zimmermann, and Soujanya\\r\\n     Sheikh. Openpose: Realtime multi-person 2d pose estima-                  Poria. Misa: Modality-invariant and-specific representations\\r\\n     tion using part affinity fields. IEEE Transactions on Pattern            for multimodal sentiment analysis. In Proceedings of the\\r\\n     Analysis and Machine Intelligence, 2019. 2, 5                            28th ACM International Conference on Multimedia, pages\\r\\n [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-                    1122–1131, 2020. 3, 6\\r\\n     offrey Hinton. A simple framework for contrastive learning          [18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\r\\n     of visual representations. In International conference on ma-            Girshick. Momentum contrast for unsupervised visual rep-\\r\\n     chine learning, pages 1597–1607. PMLR, 2020. 3                           resentation learning. In Proceedings of the IEEE/CVF Con-\\r\\n [6] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.                   ference on Computer Vision and Pattern Recognition, pages\\r\\n     Improved baselines with momentum contrastive learning.                   9729–9738, 2020. 3\\r\\n     arXiv preprint arXiv:2003.04297, 2020. 3                            [19] Antonio Hernández-Vela, Nadezhda Zlateva, Alexander\\r\\n [7] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fi-                   Marinov, Miguel Reyes, Petia Radeva, Dimo Dimov, and\\r\\n     dler, Raquel Urtasun, and Alan Yuille. Detect what you                   Sergio Escalera. Graph cuts optimization for multi-limb hu-\\r\\n     can: Detecting and representing objects using holistic mod-              man segmentation in depth maps. In 2012 IEEE Conference\\r\\n     els and body parts. In Proceedings of the IEEE conference on             on Computer Vision and Pattern Recognition, pages 726–\\r\\n     computer vision and pattern recognition, pages 1971–1978,                732. IEEE, 2012. 2, 5\\r\\n     2014. 1, 2, 5                                                       [20] Fangzhou Hong, Liang Pan, Zhongang Cai, and Ziwei Liu.\\r\\n [8] Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying                     Garment4d: Garment reconstruction from point cloud se-\\r\\n     Deng, and Weiming Hu. Channel-wise topology refinement                   quences. In Thirty-Fifth Conference on Neural Information\\r\\n     graph convolution for skeleton-based action recognition. In              Processing Systems, 2021. 2\\r\\n     Proceedings of the IEEE/CVF International Conference on             [21] Ji Hou, Saining Xie, Benjamin Graham, Angela Dai, and\\r\\n     Computer Vision, pages 13359–13368, 2021. 1                              Matthias Nießner. Pri3d: Can 3d priors help 2d represen-\\r\\n [9] MMPose Contributors. Openmmlab pose estimation tool-                     tation learning? arXiv preprint arXiv:2104.11225, 2021. 2,\\r\\n     box and benchmark. https://github.com/open-                              3, 6\\r\\n     mmlab/mmpose, 2020. 2, 5                                            [22] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\\r\\n[10] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-                    Sminchisescu. Human3.6m: Large scale datasets and predic-\\r\\n     ber, Thomas Funkhouser, and Matthias Nießner. Scannet:                   tive methods for 3d human sensing in natural environments.\\r\\n     Richly-annotated 3d reconstructions of indoor scenes. In                 IEEE Transactions on Pattern Analysis and Machine Intel-\\r\\n     Proceedings of the IEEE conference on computer vision and                ligence, 36(7):1325–1339, jul 2014. 1, 2, 5, 6, 7, 12, 13,\\r\\n     pattern recognition, pages 5828–5839, 2017. 2                            14\\r\\n[11] Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming                [23] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\\r\\n     Yang, and Liang Lin. Instance-level human parsing via part               Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\\r\\n     grouping network. In Proceedings of the European Confer-                 Dilip Krishnan. Supervised contrastive learning. arXiv\\r\\n     ence on Computer Vision (ECCV), pages 770–785, 2018. 1,                  preprint arXiv:2004.11362, 2020. 2\\r\\n     2, 3, 5                                                             [24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\r\\n[12] Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen,                      Imagenet classification with deep convolutional neural net-\\r\\n     and Liang Lin. Look into person: Self-supervised structure-              works. Advances in neural information processing systems,\\r\\n     sensitive learning and a new benchmark for human parsing.                25:1097–1105, 2012. 6\\r\\n     In Proceedings of the IEEE Conference on Computer Vision            [25] Jianshu Li, Jian Zhao, Yunchao Wei, Congyan Lang, Yidong\\r\\n     and Pattern Recognition, pages 932–940, 2017. 1, 2, 3, 5                 Li, Terence Sim, Shuicheng Yan, and Jiashi Feng. Multiple-\\r\\n[13] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin             human parsing in the wild. arXiv preprint arXiv:1705.07206,\\r\\n     Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-                  2017. 1, 2, 3, 5\\r\\n\\r\\n\\r\\n                                                                     9\\r\\n\\x0c\\n\\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,                    Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\r\\n     Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence                  ing transferable visual models from natural language super-\\r\\n     Zitnick. Microsoft coco: Common objects in context. In                      vision. arXiv preprint arXiv:2103.00020, 2021. 3\\r\\n     European conference on computer vision, pages 740–755.                 [40] N Dinesh Reddy, Laurent Guigues, Leonid Pishchulin, Jayan\\r\\n     Springer, 2014. 1, 2                                                        Eledath, and Srinivasa G Narasimhan. Tessetrack: End-to-\\r\\n[27] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,                         end learnable multi-person articulated 3d pose tracking. In\\r\\n     Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-                      Proceedings of the IEEE/CVF Conference on Computer Vi-\\r\\n     scale benchmark for 3d human activity understanding. IEEE                   sion and Pattern Recognition, pages 15190–15200, 2021. 1,\\r\\n     transactions on pattern analysis and machine intelligence,                  2\\r\\n     42(10):2684–2701, 2019. 1, 5, 6                                        [41] Andrew Rouditchenko, Angie Boggust, David Harwath,\\r\\n[28] Songtao Liu, Zeming Li, and Jian Sun. Self-emd: Self-                       Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Au-\\r\\n     supervised object detection without imagenet. arXiv preprint                dhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris,\\r\\n     arXiv:2011.13677, 2020. 3                                                   et al. Avlnet: Learning audio-visual language representations\\r\\n[29] Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong,                          from instructional videos. arXiv preprint arXiv:2006.09199,\\r\\n     Thomas Funkhouser, and Li Yi. Contrastive multimodal fu-                    2020. 3\\r\\n     sion with tupleinfonce. In Proceedings of the IEEE/CVF In-             [42] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\\r\\n     ternational Conference on Computer Vision, pages 754–763,                   Ntu rgb+ d: A large scale dataset for 3d human activity anal-\\r\\n     2021. 3, 6                                                                  ysis. In Proceedings of the IEEE conference on computer\\r\\n[30] Yunze Liu, Li Yi, Shanghang Zhang, Qingnan Fan, Thomas                      vision and pattern recognition, pages 1010–1019, 2016. 1,\\r\\n     Funkhouser, and Hao Dong. P4contrast: Contrastive learn-                    2, 5, 12\\r\\n     ing with pairs of point-pixel pairs for rgb-d scene understand-        [43] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\\r\\n     ing. arXiv preprint arXiv:2012.13089, 2020. 2, 3                            high-resolution representation learning for human pose es-\\r\\n[31] Julieta Martinez, Rayat Hossain, Javier Romero, and James J                 timation. In Proceedings of the IEEE/CVF Conference\\r\\n     Little. A simple yet effective baseline for 3d human pose es-               on Computer Vision and Pattern Recognition, pages 5693–\\r\\n     timation. In Proceedings of the IEEE International Confer-                  5703, 2019. 1, 2, 5, 6, 8, 12\\r\\n     ence on Computer Vision, pages 2640–2649, 2017. 1, 2                   [44] Feitong Tan, Danhang Tang, Mingsong Dou, Kaiwen Guo,\\r\\n[32] Kaichiro Nishi and Jun Miura. Generation of human depth                     Rohit Pandey, Cem Keskin, Ruofei Du, Deqing Sun, Sofien\\r\\n     images with body part labels for complex human pose recog-                  Bouaziz, Sean Fanello, et al. Humangps: Geodesic preserv-\\r\\n     nition. Pattern Recognition, 71:402–413, 2017. 2, 5                         ing feature for dense human correspondences. In Proceed-\\r\\n[33] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-                     ings of the IEEE/CVF Conference on Computer Vision and\\r\\n     sentation learning with contrastive predictive coding. arXiv                Pattern Recognition, pages 1820–1830, 2021. 2\\r\\n     preprint arXiv:1807.03748, 2018. 2, 4, 7                               [45] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-\\r\\n[34] Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth                      trastive multiview coding. In Computer Vision–ECCV 2020:\\r\\n     Fong, Joao F Henriques, Geoffrey Zweig, and Andrea                          16th European Conference, Glasgow, UK, August 23–28,\\r\\n     Vedaldi. Multi-modal self-supervision from generalized data                 2020, Proceedings, Part XI 16, pages 776–794. Springer,\\r\\n     transformations. arXiv preprint arXiv:2003.04298, 2020. 3                   2020. 2, 3, 4, 6, 7, 8, 13, 15, 16, 17\\r\\n[35] Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth                 [46] Longhui Wei, Lingxi Xie, Jianzhong He, Jianlong Chang,\\r\\n     Fong, João F Henriques, Geoffrey Zweig, and Andrea                         Xiaopeng Zhang, Wengang Zhou, Houqiang Li, and Qi Tian.\\r\\n     Vedaldi. On compositions of transformations in contrastive                  Can semantic labels assist self-supervised visual representa-\\r\\n     self-supervised learning. In Proceedings of the IEEE/CVF                    tion learning? arXiv preprint arXiv:2011.08621, 2020. 2\\r\\n     International Conference on Computer Vision, pages 9577–               [47] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\r\\n     9587, 2021. 3                                                               Unsupervised feature learning via non-parametric instance\\r\\n[36] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi,                   discrimination. In Proceedings of the IEEE conference on\\r\\n     and George Tucker. On variational bounds of mutual infor-                   computer vision and pattern recognition, pages 3733–3742,\\r\\n     mation. In International Conference on Machine Learning,                    2018. 3\\r\\n     pages 5171–5180. PMLR, 2019. 3                                         [48] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines\\r\\n[37] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.                    for human pose estimation and tracking. In Proceedings of\\r\\n     Pointnet: Deep learning on point sets for 3d classification                 the European conference on computer vision (ECCV), pages\\r\\n     and segmentation. In Proceedings of the IEEE conference                     466–481, 2018. 1, 2\\r\\n     on computer vision and pattern recognition, pages 652–660,             [49] Fu Xiong, Boshen Zhang, Yang Xiao, Zhiguo Cao, Taidong\\r\\n     2017. 5                                                                     Yu, Joey Tianyi Zhou, and Junsong Yuan. A2j: Anchor-to-\\r\\n[38] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J                       joint regression network for 3d articulated pose estimation\\r\\n     Guibas. Pointnet++: Deep hierarchical feature learning on                   from a single depth image. In Proceedings of the IEEE/CVF\\r\\n     point sets in a metric space. In Advances in neural informa-                International Conference on Computer Vision, pages 793–\\r\\n     tion processing systems, pages 5099–5108, 2017. 5, 8, 13                    802, 2019. 2, 12, 13\\r\\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya                     [50] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-\\r\\n     Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,                       ral graph convolutional networks for skeleton-based action\\r\\n\\r\\n\\r\\n                                                                       10\\r\\n\\x0c\\n\\n     recognition. In Thirty-second AAAI conference on artificial\\r\\n     intelligence, 2018. 1\\r\\n[51] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dim-\\r\\n     itris N Metaxas. Semantic graph convolutional networks for\\r\\n     3d human pose regression. In Proceedings of the IEEE/CVF\\r\\n     Conference on Computer Vision and Pattern Recognition,\\r\\n     pages 3425–3435, 2019. 5\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                   11\\r\\n\\x0c\\n\\n              Supplementary Material                                  takes around 80 hours to train. For the 10% training set, we\\r\\n                                                                      train the network for 13000 iterations with a learning rate\\r\\n    In this supplementary material, we provide more imple-            of 0.005 and other settings the same, which takes 9 hours to\\r\\nmentation details of HCMoCo, downstream tasks and two                 train. The 10% training set is uniformly sampled from the\\r\\napplications (Sec. 1). More detailed quantitative and qual-           default ordered training set.\\r\\nitative results of downstream tasks are also illustrated (Sec.\\r\\n2 and Sec. 3).                                                        1.3. RGB Human Parsing\\r\\n                                                                          For the RGB human parsing on Human3.6M [22], we\\r\\n1. Implementation Details                                             use the official HRNet [43] semantic segmentation imple-\\r\\n1.1. HCMoCo                                                           mentation 4 . Different ratios of training settings are uni-\\r\\n                                                                      formly sampled from the default ordered full training set.\\r\\nNetwork Details. For the proposed instantiation of HC-                For the full training set, we train the network for 50 epochs\\r\\nMoCo, we implement the sample-level modality-invariant                with a learning rate of 0.007, a batch size of 40 on 2\\r\\nrepresentation learning target by maintaining a memory                NVIDIA V100 GPUs. For other data-efficient settings, we\\r\\npool, which is adapted from an open-sourced implemen-                 train the network for 150 epochs with other settings the\\r\\ntation 2 . The memory pool is updated in a momentum                   same. We use the the standard dataset split protocol, where\\r\\nstyle with the momentum of 0.5. For global embeddings,                the subjects 1, 5, 6, 7, 8 are for training and the subjects 9\\r\\nwe sample 16384 negative samples from the memory pool.                and 11 are for evaluation.\\r\\nFor other hyper-parameters, we use a batch size of 224, a\\r\\nlearning rate of 0.03, a temperature of 0.07 for all three            1.4. Depth Human Parsing\\r\\ncontrastive learning targets. For the pre-train, 4 NVIDIA                 For the depth human parsing on NTURGBD-Parsing-\\r\\nV100 GPUs are used. The training process is divided in two            4K, we use the same implementation as that of RGB human\\r\\nsteps. The first step only pre-train the model using sample-          parsing. To use the HRNet to encode depth maps, we repeat\\r\\nlevel modality-invariant representation learning target for           the depth dimension for three times to fit the RGB input,\\r\\n100 epochs. The second stage adds the other two learn-                which is also how HCMoCo deals with depth inputs. For all\\r\\ning targets and trains for another 100 epochs. The whole              training settings, we train the network for 150 epochs with\\r\\ntraining process takes approximately 48 hours.                        a learning rate of 0.007, a batch size of 80 on 2 NVIDIA\\r\\nMixing Heterogeneous Datasets. Since we mix several                   V100 GPUs. Even though the encoder is used to deal with\\r\\nheterogeneous human datasets for pre-train, we need to                depth inputs, we still initialize it using ImageNet pre-train\\r\\nmask out the missing modalities. For example, when we                 for that it might help with the performance proved by some\\r\\nuse NTU RGB+D and MPII for pre-train. The former                      previous works [49].\\r\\ndataset has all the required modalities, while the latter one\\r\\nmisses depth maps. Therefore, for the hierarchical con-               1.5. 3D Pose Estimation on Depth\\r\\ntrastive learning targets, we mask out the missing depth em-              For the 3D pose estimation from depth maps on\\r\\nbeddings of MPII for all the positive pairs sampling. By              ITOP [16], we choose to adapt the official implementation\\r\\nusing the masking technique, it is possible to combine mul-           5\\r\\n                                                                        of A2J [49]. The original implementation uses ResNet as\\r\\ntiple heterogeneous datasets into this pre-train paradigm as          the backbone. And we switch to HRNet. Since the origi-\\r\\nlong as there are at least two common modalities.                     nal implementation only provides validation scripts, we re-\\r\\nDatasets for Pre-train. For NTU RGB+D, we only use                    implement the whole training pipeline. We change the orig-\\r\\nthe version with 60 actions [42]. With the provided RGB-              inal normalization method where a global mean and vari-\\r\\nD videos, we uniformly sample one frame from every 30                 ance is counted for a global normalization. Instead, we per-\\r\\nframes, which makes 143648 samples. The RGB and depth                 form an online instance normalization where we only cen-\\r\\nframes are calibrated by the correspondences provided by              tralize each depth pixel to zero mean but do not normalize\\r\\nthe 2D keypoints positions on RGB and depths. For MPII                its variance, since its a better way to prevent the over-fitting\\r\\nand COCO, we use the full training sets for pre-train.                to the relatively small dataset. We train the network for 50\\r\\n1.2. DensePose Estimation                                             epochs with a learning rate of 0.00035 and a batch size of\\r\\n                                                                      12 on one NVIDIA V100 GPU. As for the dataset, we use\\r\\n    For the DensePose [14] estimation, we use the official            the side-view of ITOP since the depth maps in pre-train are\\r\\nopen-sourced implementation 3 . For the full training set, we         side views. Following the official dataset split, there are\\r\\ntrain the network for 130000 iterations with a batch size of          17991 samples for training and 4863 for testing. Following\\r\\n16, a learning rate of 0.01 on 4 NVIDIA V100 GPUs, which\\r\\n                                                                        4 https : / / github . com / HRNet / HRNet - Semantic -\\r\\n  2 https://github.com/HobbitLong/PyContrast                          Segmentation\\r\\n  3 https://github.com/facebookresearch/detectron2                      5 https://github.com/zhangboshen/A2J\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 12\\r\\n\\x0c\\n\\nthe practice of A2J [49], we initialize the encoders using            dense embeddings of RGB and depth are aligned and the\\r\\nImageNet pre-train.                                                   segmentation header is trained with the fusion of both em-\\r\\n                                                                      beddings, missing one of them will still produce reasonable\\r\\n1.6. Cross-Modality Supervision                                       predictions.\\r\\n   To experiment with the cross-modality supervision,\\r\\nwe choose the downstream task of human parsing on                     2. More Quantitative Results\\r\\nNTURGBD-Parsing-4K. The modalities to experiment with                 DensePose Estimation. Due to the page limitation, we\\r\\nare RGB and depth. To make the experiment fair and the                could not report all metrics for the DensePose [14] esti-\\r\\nnetworks to converge faster, the backbones are initialized            mation. Therefore, we report them in this supplementary\\r\\nby CMC [45] pre-train. The following descriptions are for             material. As shown in Tab. 9, detailed results of all set-\\r\\nthe setting of ‘RGB→Depth’, where the source modality                 tings mentioned in the main paper are listed. Specifically,\\r\\nis RGB and the target modality is depth. To implement                 for the initialization of the network, we test with the net-\\r\\n‘Depth→RGB’, one can simply switch the source and tar-                work randomly initialized (‘From Scratch’) and the network\\r\\nget modalities. At training time, a randomly initialized              initialized by ImageNet pre-train (‘IN Pre-train’). As for\\r\\nsegmentation header, which is the same one used for hu-               the ratio of training data, we test with the full training set\\r\\nman parsing experiments, is attached to the dense mapper              and 10% of the training set. As for the pre-train datasets,\\r\\nnetwork of RGB. Then the network is trained with both                 we test with two combinations: NTU RGB+D + MPII and\\r\\nthe hierarchical contrastive learning targets L and a cross-          NTU RGB+D + COCO. As for the backbone, we test with\\r\\nentropy loss L′ for the supervision of the segmentation. For          HRNet-W18 and HRNet-W32. Compared with the base-\\r\\nthe ‘No Contrastive’ baseline, we only train with L′ . As             line and two other state-of-the-art pre-train counterparts,\\r\\nfor the ‘CMC’ baseline, the network is supervised by both             our method outperforms them in most of the metrics. Espe-\\r\\nthe learning target proposed by CMC [45] L and the seg-               cially, our method has advantages in GPS and GPSM, which\\r\\nmentation loss L′ . Note that, during the whole training              are two critical metrics for DensePose quality. Additionally,\\r\\ntime, including the CMC pre-train, the target modality of             we also report full results of the ablation study. The detailed\\r\\nNTURGBD-Parsing-4K is not exposed to better simulate                  results further validates the analysis in the main paper.\\r\\nthe application scenario. In order to build the connection            RGB Human Parsing. We further report detailed RGB hu-\\r\\nbetween RGB and depth during training time, we mix the                man parsing results on Human3.6M [22] that could not fit\\r\\nNTURGBD-Parsing-4K with NTU RGB+D which is the                        into the main paper. As shown in Tab. 10, we report the\\r\\nsame one used for our pre-train. At inference time, we at-            per-class IoU for all the settings reported in the main paper.\\r\\ntach the trained segmentation head to the mapper network              Similarly, for the initialization of the network, we test with\\r\\nof depth. Since the dense embeddings of RGB and depth                 the network randomly initialized (‘From Scratch’) and the\\r\\nare aligned thanks to our hierarchical contrastive learning           network initialized by ImageNet pre-train (‘IN Pre-train’).\\r\\ntargets, it is reasonable for the segmentation head to be able        As for the ratio of training data, we test with the full train-\\r\\nto handle the dense embeddings of depth.                              ing set, 20%, 10% and 1% of the training set. The pre-\\r\\n                                                                      train datasets are NTU RGB+D + MPII. In most classes,\\r\\n1.7. Missing-Modality Inference\\r\\n                                                                      our method outperforms comparison methods. Moreover,\\r\\n    We also use human parsing on NTURGBD-Parsing-4K                   we also report per-class IoU for the four settings in ablation\\r\\nto experiment with our extension of missing-modality in-              study, which are in line with our analysis in the main paper.\\r\\nference. The basic setup is the same as that of the cross-            Depth Human Parsing. We report detailed depth human\\r\\nmodality supervision experiments. At training time, we take           parsing results on NTURGBD-Parsing-4K. As shown in\\r\\nthe dense embeddings of both RGB and depth together for               Tab. 11, we report the per-class IoU for all the settings re-\\r\\na max pooling operation for a simple feature-level fusion.            ported in the main paper. We initialize the networks using\\r\\nThen the fused dense embedding is passed to a segmen-                 ImageNet pre-train. Two ratios of the training set, i.e. full\\r\\ntation header, which is the same one used by the human                and 20%, are tested. We also change the backbone to Point-\\r\\nparsing experiment, to produce the segmentation predic-               Net++ [38] (‘PN++’). Since it is a point-based backbone,\\r\\ntion. The network is supervised with both the hierarchi-              the ‘background’ class is ignored and not included in the\\r\\ncal contrastive learning targets L and a cross-entropy loss           calculation of mIoU. The per-class IoU results also agree\\r\\nL′ for segmentation supervision. Similarly, the ‘No con-              with the conclusion in the main paper that our method is\\r\\ntrastive’ baseline does not use any contrastive learning tar-         superior than other comparison methods.\\r\\ngets. The ‘CMC’ baseline uses the contrastive learning tar-           Cross-Modality Supervision. As shown in Tab. 12,\\r\\nget proposed in CMC [45] as L. At inference time, if RGB              we report detailed per-class IoU for the experiments of\\r\\nis missing, then the dense embedding of depth is passed to            cross-modality supervision. In both ‘RGB→Depth’ and\\r\\nthe trained segmentation header for prediction. Since the             ‘Depth→RGB’ settings, our method outperforms other\\r\\n\\r\\n\\r\\n                                                                 13\\r\\n\\x0c\\n\\nbaseline methods in all classes. Especially, other baseline\\r\\nmethods barely make correct predictions while ours makes\\r\\na huge improvement.\\r\\nMissing-Modality Inference. As shown in Tab. 12, we\\r\\nlist detailed per-class IoU for the experiments of missing-\\r\\nmodality inference. In both ‘Only RGB’ and ‘Only Depth’\\r\\nsettings, our method outperforms baseline methods in most\\r\\nclasses. Therefore, the detailed results further validates the\\r\\nconclusions made in the main paper.\\r\\n\\r\\n3. More Qualitative Results\\r\\n   More qualitative results of RGB human parsing on Hu-\\r\\nman3.6M [22] and depth human parsing on NTURGBD-\\r\\nParsing-4K are shown in Fig. 7, Fig 8 and Fig. 9. We\\r\\nchoose to visualize both the full training set and 10% train-\\r\\ning set for RGB human parsing. The segmentation results\\r\\nproduced by our pre-train model are superior than those of\\r\\nother comparison methods, especially in data-efficient set-\\r\\ntings. For challenging classes like hands and elbows, our\\r\\nmethod is capable of producing correct predictions con-\\r\\nstantly while other methods struggle. The depth map is a\\r\\nchallenging modality for the dense prediction task like se-\\r\\nmantic segmentation. Our method manages to produce rea-\\r\\nsonable predictions that are better than those of other com-\\r\\nparison methods.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 14\\r\\n\\x0c\\n\\n     Table 9. Detailed DensePose Estimation Results on COCO. ‘Ratio’ stands for the ratio of training data for downstream tasks transfer. * randomly initializes the model before\\r\\n     pre-training. † initializes the model by ImageNet pre-train before pre-training. ‘Ablation1’ is ‘Sample-level Mod-invariant’; ‘Abation2’ is ‘+ Hard Dense Intra-sample’; ‘Ablation3’\\r\\n     is ‘+ Soft Dense Intra-sample’; ‘Ablation4’ is ‘+ Sparse Structure-aware’. ‡ uses HRNet-W32 while others all use HRNet-W18. All results in [%].\\r\\n\\r\\n                                                                 BBox                                                                GPS                                                                            GPSM                                                                                IoU\\r\\n     Methods         Pre-train Datasets   Ratio\\r\\n                                                   AP     AP50 AP75 APs           APm     APl      AP     AP50 AP75 APm           APl   AR        AR50 AR75 ARm           ARl      AP     AP50 AP75 APm           APl  AR         AR50 AR75 ARm           ARl      AP     AP50 AP75 APm           APl      AR     AR50 AR75 ARm           ARl\\r\\n     From Scratch         -               100%    57.27   85.44   61.37   28.31   53.64   72.46   62.03   89.93   69.57   58.29   62.89   69.00   93.05   76.06   60.78   69.55   63.61   91.89   74.56   57.18   64.71   69.24   95.05   79.36   59.72   69.88   65.88   93.86   77.95   58.43   67.04   71.11   96.12   82.97   60.35   71.83\\r\\n     CMC* [45]       NTURGBD+MPII         100%    57.27   85.44   61.37   28.31   53.64   72.46   62.03   89.93   69.57   58.29   62.89   69.00   93.05   76.06   60.78   69.55   63.61   91.89   74.56   57.18   64.71   69.24   95.05   79.36   59.72   69.88   65.88   93.86   77.95   58.43   67.04   71.11   96.12   82.97   60.35   71.83\\r\\n     MMV* [1]        NTURGBD+MPII         100%    60.33   86.82   66.23   30.37   57.29   75.59   64.97   91.10   72.99   59.59   65.90   71.57   93.80   79.00   61.42   72.25   65.66   92.22   77.31   57.81   66.81   70.80   94.96   81.77   59.72   71.54   66.96   93.80   79.14   59.73   67.97   71.83   95.72   83.68   61.42   72.52\\r\\n     Ours*           NTURGBD+MPII         100%    61.33   87.81   66.48   31.80   58.27   76.30   65.89   92.20   75.36   61.12   66.87   72.52   94.69   81.01   62.91   73.17   66.92   93.27   78.72   59.75   67.97   71.87   95.50   82.92   61.63   72.56   67.66   94.41   80.14   61.08   68.71   72.62   96.21   84.80   62.62   73.29\\r\\n     IN Pre-train         -               100%    62.66   89.28   68.47   35.78   59.59   76.47   66.49   92.11   75.47   64.45   67.43   73.41   94.87   81.28   66.24   73.89   67.42   93.20   79.72   62.11   68.52   72.63   95.90   83.82   63.97   73.20   68.63   94.64   81.81   62.56   69.72   73.45   96.30   85.64   64.11   74.08\\r\\n     CMC† [45]       NTURGBD+MPII         100%    62.76   88.32   68.32   33.67   60.22   76.95   66.17   92.45   74.79   62.79   66.96   72.75   94.69   80.34   64.68   73.29   67.31   93.51   79.71   61.55   68.27   72.23   95.68   83.73   63.55   72.80   68.07   94.68   81.41   61.89   69.01   72.90   96.30   85.51   63.62   73.53\\r\\n     MMV† [1]        NTURGBD+MPII         100%    62.97   88.75   68.91   34.62   60.83   76.87   66.67   92.42   76.24   63.09   67.64   73.03   94.78   81.28   64.04   73.63   67.51   93.44   79.72   60.91   68.50   72.29   95.81   83.10   61.99   72.97   68.29   94.52   81.84   61.04   69.20   72.95   96.34   85.42   62.20   73.67\\r\\n     Ours†           NTURGBD+MPII         100%    63.11   88.66   69.64   34.53   60.80   77.01   67.33   93.20   76.27   62.63   68.20   73.77   95.23   81.59   63.83   74.44   68.12   94.09   79.90   61.35   68.99   72.94   96.08   83.95   62.48   73.64   68.72   94.74   81.67   61.50   69.77   73.47   96.26   85.73   62.70   74.19\\r\\n\\r\\n     CMC† [45]       NTURGBD+COCO 100% 63.58 88.94 69.69 35.24 61.37 77.46 67.22 92.68 76.27 64.61 68.20 73.75 95.10 81.59 65.60 74.29 67.77 93.65 79.56 62.62 68.81 72.78 95.99 83.55 63.83 73.38 68.46 93.93 81.80 62.60 69.50 73.37 95.94 85.78 63.76 74.02\\r\\n     Ours†           NTURGBD+COCO 100% 62.95 88.78 68.83 34.77 60.59 77.02 67.77 93.18 77.13 64.02 68.63 74.15 95.23 82.39 65.18 74.75 68.29 93.60 80.53 62.77 69.23 73.21 95.99 84.44 64.11 73.81 68.63 94.53 81.53 62.36 69.51 73.30 96.17 85.69 63.33 73.97\\r\\n     From Scratch         -               10%     39.38   72.29   37.98   12.03   35.59   55.16   35.75   73.78   30.07   27.19   37.28   45.32   81.41   43.69   31.49   46.25   41.62   80.25   38.81   30.71   43.33   49.67   87.34   50.74   35.60   50.61   49.92   85.96   54.21   38.56   51.61   57.90   91.62   65.14   43.19   58.88\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n15\\r\\n     CMC* [45]       NTURGBD+MPII         10%     44.92   76.41   46.34   16.09   41.58   60.88   43.84   79.94   42.97   40.31   45.00   52.25   85.69   54.21   42.55   52.90   47.94   84.22   50.23   41.42   49.33   55.00   89.43   59.61   43.90   55.74   54.00   88.65   61.20   46.16   55.45   60.96   92.73   69.82   48.94   61.77\\r\\n     MMV* [1]        NTURGBD+MPII         10%     43.24   74.53   44.27   13.49   40.43   59.22   41.40   77.54   39.26   34.54   42.93   50.45   84.22   51.67   37.02   51.35   45.99   81.83   48.11   37.22   47.64   53.58   88.19   58.18   39.72   54.51   52.52   87.73   59.10   43.45   54.01   59.80   92.15   68.26   46.17   60.71\\r\\n     Ours*           NTURGBD+MPII         10%     47.76   78.66   50.56   18.59   45.19   63.12   48.47   82.74   51.27   45.21   49.51   56.41   87.56   61.21   47.73   56.99   51.65   85.50   56.06   45.86   52.76   58.27   90.33   64.20   48.44   58.91   56.15   89.63   64.34   49.27   57.43   62.92   93.36   72.54   51.13   63.71\\r\\n     IN Pre-train         -               10%     48.29   80.07   50.90   19.55   44.54   63.64   44.34   78.77   44.83   36.85   46.49   54.54   86.49   57.38   39.29   55.56   49.11   84.02   51.91   40.57   51.38   57.78   90.82   63.17   43.40   58.73   56.11   88.10   63.14   46.91   58.16   63.73   92.56   72.94   49.50   64.69\\r\\n     CMC† [45]       NTURGBD+MPII         10%     49.21   80.43   52.01   21.66   47.22   63.71   48.82   83.85   51.39   43.97   50.05   57.21   88.72   61.35   46.67   57.92   52.57   86.67   57.21   45.69   53.89   59.52   91.26   66.21   48.65   60.25   57.94   90.82   66.11   50.11   59.30   64.80   94.07   74.50   52.34   65.64\\r\\n     MMV† [1]        NTURGBD+MPII         10%     50.16   81.33   53.68   22.17   47.73   64.76   50.28   84.02   53.95   44.06   51.68   58.43   89.03   63.40   45.74   59.28   53.54   86.66   59.25   44.91   55.06   60.35   91.22   67.59   46.74   61.25   58.32   90.15   67.10   49.21   59.89   64.94   93.62   74.90   50.92   65.88\\r\\n     Ours†           NTURGBD+MPII         10%     50.29   80.94   53.62   22.74   47.91   65.25   51.50   84.89   54.64   45.63   52.68   59.03   89.17   63.58   47.80   59.78   54.47   87.13   60.26   46.82   55.71   60.66   91.44   67.99   49.29   61.42   58.66   90.32   68.82   50.63   59.93   64.94   93.89   75.57   52.48   65.77\\r\\n\\r\\n     CMC† [45]       NTURGBD+COCO         10%     51.77 81.86 55.93 23.64 49.54 66.50 53.53 86.23 58.30 46.79 54.78 60.81 90.15 66.47 50.21 61.52 56.18 88.58 64.41 48.38 57.39 62.35 92.33 71.73 52.13 63.04 59.37 90.74 68.92 52.20 60.62 65.83 93.85 76.42 54.68 66.58\\r\\n     Ours†           NTURGBD+COCO         10%     52.18 82.71 56.57 24.92 49.96 66.86 54.01 86.41 58.31 48.15 55.14 61.53 90.55 67.10 50.99 62.24 56.64 88.67 64.79 48.78 57.88 63.05 92.69 72.18 51.70 63.81 59.93 91.31 69.99 52.01 61.08 66.50 94.34 77.40 54.54 67.29\\r\\n     Ablation1       NTURGBD+MPII         10%     49.21   80.43   52.01   21.66   47.22   63.71   48.82   83.85   51.39   43.97   50.05   57.21   88.72   61.35   46.67   57.92   52.57   86.67   57.21   45.69   53.89   59.52   91.26   66.21   48.65   60.25   57.94   90.82   66.11   50.11   59.30   64.80   94.07   74.50   52.34   65.64\\r\\n     Ablation2       NTURGBD+MPII         10%     49.40   80.93   51.61   21.94   47.56   63.72   49.14   82.17   51.83   44.53   50.43   57.65   87.83   62.42   46.88   58.37   52.49   85.69   58.42   46.56   53.84   59.61   90.68   66.87   49.15   60.31   57.30   89.93   65.38   50.58   58.74   64.29   93.80   73.96   53.12   65.03\\r\\n     Ablation3       NTURGBD+MPII         10%     50.21   81.02   53.39   22.86   47.78   64.75   50.25   84.20   53.58   44.74   51.41   57.95   88.50   62.91   47.66   58.64   53.42   86.41   58.95   46.27   54.68   59.85   90.68   66.87   49.01   60.56   57.70   89.75   67.40   50.95   58.89   64.40   93.31   74.72   52.91   65.17\\r\\n     Ablation4       NTURGBD+MPII         10%     50.29   80.94   53.62   22.74   47.91   65.25   51.50   84.89   54.64   45.63   52.68   59.03   89.17   63.58   47.80   59.78   54.47   87.13   60.26   46.82   55.71   60.66   91.44   67.99   49.29   61.42   58.66   90.32   68.82   50.63   59.93   64.94   93.89   75.57   52.48   65.77\\r\\n\\r\\n     IN Pre-train‡        -               10%     55.10 84.95 59.81 27.33 52.01 69.65 54.60 86.42 59.80 48.63 55.93 62.48 90.68 68.12 50.43 63.29 57.60 88.62 66.39 49.61 59.15 64.09 92.55 72.49 51.63 64.92 61.73 91.74 72.52 53.24 63.10 67.99 94.69 78.73 55.11 68.86\\r\\n     CMC‡ [45]       NTURGBD+MPII         10%     53.88 83.61 58.47 26.16 51.24 68.64 54.62 86.83 58.93 47.45 55.86 62.05 90.73 67.19 49.01 62.93 57.46 88.57 65.55 49.59 58.71 63.57 92.33 72.05 51.21 64.39 61.14 91.65 72.58 53.46 62.39 67.21 94.25 78.73 54.96 68.03\\r\\n     Ours‡           NTURGBD+MPII         10%     54.55 83.77 58.83 26.94 52.56 68.78 55.80 87.37 61.22 51.83 56.75 62.70 90.91 68.48 53.12 63.34 58.36 89.29 67.31 52.03 59.39 64.11 92.87 73.52 53.55 64.82 61.75 91.59 72.79 54.77 62.87 67.72 94.38 79.00 56.38 68.48\\r\\n\\x0c\\n\\n     Table 10. Detailed Human Parsing Results on Human3.6M. ‘Ratio’ stands for the ratio of training data for downstream tasks transfer. * randomly initializes the model before\\r\\n     pre-training. † initializes the model by ImageNet pre-train before pre-training. ‘Ablation1’ is ‘Sample-level Mod-invariant’; ‘Abation2’ is ‘+ Hard Dense Intra-sample’; ‘Ablation3’\\r\\n     is ‘+ Soft Dense Intra-sample’; ‘Ablation4’ is ‘+ Sparse Structure-aware’. All results in [%].\\r\\n\\r\\n     Methods        Ratio    bg     right hip right knee right foot left hip left knee left foot left shoulder left elbow left hand right shoulder right elbow right hand crotch right thigh right calf left thigh left calf lower spine upper spine   head    left arm left forearm right arm right forearm mIoU\\r\\n     From Scratch   100%    99.53    29.66      32.28      52.94    25.86     30.71    49.59       35.14        20.78     29.46        34.33         19.01       30.61    37.24     54.53      65.77      54.35     64.12      50.56       57.39       77.57   40.71      35.83      38.14       36.99     44.13\\r\\n     CMC* [45]      100%    99.55    44.95      48.96      54.05    37.57     41.55    53.41       47.98        33.35     37.05        46.01         32.28       38.18    50.81     64.84      69.95      62.22     68.16      67.76       71.78       81.23   56.32      46.75      55.92       47.53     54.33\\r\\n     MMV* [1]       100%    99.54    39.92      45.88      53.18    34.04     34.77    52.97       45.35        31.22     34.10        45.08         31.92       35.26    53.11     63.96      69.41      60.81     66.78      67.53       72.35       79.96   55.63      43.48      56.07       44.99     52.69\\r\\n     Ours*          100%    99.61    51.09      58.38      61.22    40.81     48.02    59.65       54.21        44.75     48.57        52.78         43.66       48.68    56.70     71.43      75.89      67.98     73.73      71.63       76.79       83.89   64.64      57.51      64.47       57.97     61.36\\r\\n     From Scratch   20%     99.52    29.07      32.15      50.17    23.11     24.90    48.12       34.98        17.26     23.58        33.20         17.75       25.39    34.76     54.76      65.22      51.44     62.03      55.16       60.63       78.27   39.47      30.30      37.31       31.84     42.41\\r\\n     CMC* [45]      20%     99.53    41.21      42.49      49.99    39.44     39.37    49.24       46.58        30.02     33.19        45.35         30.08       34.48    53.06     62.33      66.06      61.24     64.88      67.02       71.59       80.35   54.80      42.67      54.03       43.61     52.10\\r\\n     MMV* [1]       20%     99.51    38.19      43.71      49.45    34.44     32.47    49.59       42.88        28.15     30.74        43.23         28.81       32.13    52.26     62.83      66.82      59.55     63.94      66.44       71.66       79.42   54.10      40.48      54.03       41.73     50.66\\r\\n     Ours*          20%     99.59    46.76      51.17      56.35    43.52     48.66    55.20       53.79        42.58     43.76        52.41         42.29       44.58    60.19     68.57      71.95      67.80     70.67      70.56       75.74       82.68   62.90      52.71      62.44       52.48     59.17\\r\\n     From Scratch   10%     99.39    18.84      19.95      35.96    19.55     15.23    32.38       23.23        12.43     15.57        20.73         12.46       18.64    28.46     40.41      45.51      37.85     42.10      51.25       58.41       74.93   25.77      20.47      23.29       22.50     32.61\\r\\n     CMC* [45]      10%     99.49    35.71      37.05      44.08    36.65     33.37    42.36       44.43        23.56     29.01        43.30         24.51       30.05    52.29     60.05      60.32      58.81     58.03      65.01       70.68       79.19   52.11      37.89      51.56       39.64     48.37\\r\\n     MMV* [1]       10%     99.46    33.84      34.77      42.58    33.47     27.94    41.83       37.89        21.06     26.17        38.65         23.36       27.40    50.05     58.14      59.71      56.35     57.01      64.47       69.99       77.38   50.43      35.00      51.34       37.35     46.23\\r\\n     Ours*          10%     99.56    42.65      48.60      52.82    44.08     45.44    51.58       52.21        40.08     41.70        51.70         40.35       42.73    59.06     66.85      69.09      66.78     67.47      68.73       74.82       81.39   61.15      49.08      60.39       48.64     57.08\\r\\n     From Scratch   1%      98.39    0.00       0.00       0.00      0.00     0.00      0.00        0.00        0.00       0.00         0.00         0.00        0.00      0.00     18.12       0.00      15.94      0.00      18.71       30.70        0.00    0.00       0.00       0.00        0.00      7.27\\r\\n     CMC* [45]      1%      98.95    0.00       0.00       4.08      0.00     0.00      2.69        0.00        0.00       0.00         0.00         0.00        0.00      7.38     22.13      20.40      22.19     17.72      48.99       52.65       68.09    0.00       0.00       0.00        0.00     14.61\\r\\n     MMV* [1]       1%      98.62    0.00       0.00       0.00      0.00     0.00      0.00        0.00        0.00       0.00         0.00         0.00        0.00      0.00     22.03      12.54      18.33      9.34      46.48       54.31       59.75    0.00       0.00       0.00        0.00     12.86\\r\\n     Ours*          1%      99.02    0.00       0.00       9.95      0.00     0.00      8.70        0.00        0.00       0.00         0.00         0.00        0.00      9.97     26.15      25.71      24.97     24.00      57.31       58.61       69.24    0.00       0.00       0.00        0.00     16.55\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n16\\r\\n     IN Pre-train   100%    99.60    37.98      52.94      56.49    32.24     46.97    56.29       46.05        47.55     42.50        42.81         47.05       42.88    46.57     66.69      75.49      64.51     74.23      61.85       67.29       82.69   58.41      57.98      56.56       58.90     56.90\\r\\n     CMC† [45]      100%    99.60    44.36      51.60      57.77    41.26     51.46    54.51       53.20        42.49     42.28        51.82         41.57       41.95    55.85     68.72      72.62      67.34     72.18      69.21       74.36       82.78   63.95      54.88      63.38       54.21     58.93\\r\\n     MMV† [1]       100%    99.60    44.58      54.97      59.49    40.01     48.30    56.47       51.56        43.01     43.60        50.11         42.12       42.97    56.61     69.42      74.69      67.35     72.90      68.54       73.46       82.52   63.02      55.04      62.54       54.13     59.08\\r\\n     Ours†          100%    99.62    50.34      57.06      60.78    42.96     50.52    57.75       56.66        48.56     48.11        56.50         47.64       47.86    60.06     72.46      75.23      70.20     73.58      73.16       78.24       84.36   66.48      58.91      66.96       58.60     62.50\\r\\n     IN Pre-train   20%     99.53    36.33      37.57      49.56    32.73     33.11    47.01       42.29        29.79     34.31        39.52         29.84       33.40    46.71     55.73      60.20      54.27     58.36      64.67       68.74       81.83   49.21      43.69      48.51       44.49     48.86\\r\\n     CMC† [45]      20%     99.58    44.54      49.05      54.20    42.95     47.77    52.52       50.89        40.65     40.08        49.67         39.46       39.85    56.26     68.09      70.69      66.45     69.76      69.23       73.95       81.87   62.20      52.63      61.35       51.45     57.41\\r\\n     MMV† [1]       20%     99.58    44.94      51.80      56.68    38.96     45.12    53.29       50.16        40.35     40.29        48.96         39.99       39.31    56.14     68.21      72.31      66.00     70.07      68.50       73.38       81.62   61.54      52.27      61.23       51.28     57.28\\r\\n     Ours†          20%     99.60    48.34      54.49      58.57    43.35     47.57    55.65       55.46        45.50     46.55        55.34         44.65       46.47    61.25     71.12      73.61      69.45     71.37      71.44       76.74       83.52   64.86      56.40      64.23       55.73     60.85\\r\\n     IN Pre-train   10%     99.52    31.06      30.86      42.62    31.49     28.61    39.59       40.24        24.58     28.15        38.36         25.25       26.65    47.38     50.07      53.83      49.90     52.64      63.70       68.21       80.72   44.98      35.24      44.19       35.80     44.55\\r\\n     CMC† [45]      10%     99.55    38.50      45.93      48.87    42.27     42.37    47.59       48.30        37.07     36.52        47.46         36.72       36.07    55.22     65.35      66.68      65.23     64.92      67.31       72.76       81.25   59.06      48.16      58.33       47.14     54.35\\r\\n     MMV† [1]       10%     99.54    38.12      45.86      50.23    37.23     40.96    48.24       47.57        36.17     36.33        46.99         36.13       35.34    55.36     64.36      66.91      63.40     64.91      67.40       72.44       80.88   58.72      47.61      58.71       46.98     53.86\\r\\n     Ours†          10%     99.57    43.01      49.27      53.44    46.22     46.13    52.20       53.68        40.73     43.25        54.63         41.18       43.80    59.88     67.81      69.59      69.08     67.91      69.02       75.36       82.30   62.99      51.10      63.35       51.42     58.28\\r\\n     IN Pre-train   1%      99.07    0.00       0.00        8.97     0.00     0.00      6.92        0.00        0.00       0.00         0.00         0.00        0.00      0.00     19.38      24.79      19.34     23.11      43.39       48.80       72.37   0.00        0.00      0.00         0.00     14.65\\r\\n     CMC† [45]      1%      99.08    0.00       0.00       17.72     0.00     0.00     14.43        0.00        0.00       0.00         0.00         0.00        0.00     33.14     26.55      24.95      25.94     22.73      48.98       52.21       72.88   2.65        0.00      2.99         0.00     17.77\\r\\n     MMV† [1]       1%      99.10    0.00       0.00       12.83     0.00     0.00     10.97        0.00        0.00       0.00         0.00         0.00        0.00     23.62     26.53      24.72      26.40     24.54      51.18       53.27       72.29   8.15        0.00      7.96         0.00     17.66\\r\\n     Ours†          1%      99.15    0.00       0.00       18.29     0.00     0.00     16.99        0.07        0.00       0.00         0.07         0.00        0.00     46.56     28.73      27.59      29.13     26.38      58.59       61.46       74.41   16.32       0.00      15.65        0.00     20.78\\r\\n     Ablation1      10%     99.55    38.50      45.93      48.87    42.27     42.37    47.59       48.30        37.07     36.52        47.46         36.72       36.07    55.22     65.35      66.68      65.23     64.92      67.31       72.76       81.25   59.06      48.16      58.33       47.14     54.35\\r\\n     Ablation2      10%     99.56    40.06      45.94      50.88    40.18     43.91    49.06       50.41        36.73     37.65        49.50         37.33       36.94    57.19     66.32      68.69      67.28     66.98      68.08       73.37       81.95   60.25      48.45      59.80       47.54     55.36\\r\\n     Ablation3      10%     99.57    44.49      46.38      52.15    44.00     43.85    50.25       51.73        37.19     38.12        51.30         37.24       37.61    57.38     67.16      69.08      66.91     67.32      69.42       74.45       82.12   61.36      49.91      60.69       48.97     56.35\\r\\n     Ablation4      10%     99.57    43.01      49.27      53.44    46.22     46.13    52.20       53.68        40.73     43.25        54.63         41.18       43.80    59.88     67.81      69.59      69.08     67.91      69.02       75.36       82.30   62.99      51.10      63.35       51.42     58.28\\r\\n\\x0c\\n\\n     Table 11. Detailed Human Parsing Results on NTURGBD-Parsing-4K. ‘Ratio’ stands for the ratio of training data for downstream tasks transfer. * randomly initializes the model\\r\\n     before pre-training. † initializes the model by ImageNet pre-train before pre-training. ‘Ablation1’ is ‘Sample-level Mod-invariant’; ‘Abation2’ is ‘+ Hard Dense Intra-sample’;\\r\\n     ‘Ablation3’ is ‘+ Soft Dense Intra-sample’; ‘Ablation4’ is ‘+ Sparse Structure-aware’. All results in [%].\\r\\n\\r\\n     Methods                Ratio    bg     right hip right knee right foot left hip left knee left foot left shoulder left elbow left hand right shoulder right elbow right hand crotch right thigh right calf left thigh left calf lower spine upper spine    head    left arm left forearm right arm right forearm mIoU\\r\\n     IN Pre-train           100%    99.24    21.99      19.79      39.62    23.95     20.98    39.61       22.61        14.14     22.11        23.05         12.24       22.52    25.66     47.00      46.85      46.81     48.53      53.26       61.51        61.11    43.45     36.13      46.50       38.56     37.49\\r\\n     CMC† [45]              100%    99.26    22.50      19.49      40.15    24.81     20.65    39.96       24.64        14.68     21.79        25.50         13.04       23.46    26.12     48.93      46.21      49.28     47.68      54.26       62.52        59.24    45.02     37.22      48.32       40.25     38.20\\r\\n     MMV† [1]               100%    99.23    22.64      19.68      39.02    24.66     21.38    38.77       24.40        13.92     22.36        25.15         12.83       23.65    25.78     47.87      46.62      48.12     48.45      53.32       62.46        59.89    45.03     37.80      48.17       40.98     38.09\\r\\n     Ours†                  100%    99.32    22.95      21.25      41.26    25.70     22.59    40.99       24.47        15.17     23.61        25.11         14.26       24.87    25.75     49.46      47.90      49.97     49.88      54.45       62.88        61.97    47.24     39.06      50.48       42.29     39.32\\r\\n     IN Pre-train           20%     99.13    12.39      17.07      31.82    14.08     19.28    32.72       13.68         2.61     12.58        14.59          2.96       10.87    18.16     35.01      33.36      38.41     35.82      46.81       54.45        57.49    31.95     23.39      32.65       22.60     28.56\\r\\n     CMC† [45]              20%     98.98    13.16      14.06      29.82    16.28     16.13    30.99       17.65        10.09     14.69        17.99         8.23        15.92    18.08     38.79      34.57      41.26     36.41      46.21       54.68        54.60    37.80     26.75      39.11       27.66     30.40\\r\\n     MMV† [1]               20%     99.03    12.92      16.15      30.81    15.62     18.80    32.06       17.75         7.46     15.27        18.73          5.52       15.72    19.48     38.29      34.88      40.95     37.82      45.78       54.42        55.21    36.83     25.66      37.85       27.15     30.41\\r\\n     Ours†                  20%     99.40    12.18      18.25      36.74    15.35     21.15    38.91       18.78        11.99     20.92        20.31         10.99       20.84    18.05     44.73      44.45      47.86     46.87      51.41       59.57        65.76    42.43     31.07      44.67       32.65     35.01\\r\\n     Ablation1              20%     99.13    12.39      17.07      31.82    14.08     19.28    32.72       13.68         2.61     12.58        14.59          2.96       10.87    18.16     35.01      33.36      38.41     35.82      46.81       54.45        57.49    31.95     23.39      32.65       22.60     28.56\\r\\n     Ablation2              20%     99.12    12.85      13.79      32.69    16.99     16.20    33.29       17.38        8.73      15.12        19.13         7.67        16.71    19.07     39.46      36.15      42.08     38.08      48.69       55.53        58.94    38.49     27.62      40.04       27.55     31.26\\r\\n     Ablation3              20%     99.22    13.40      14.18      31.37    16.46     17.71    32.88       19.72        9.33      16.13        20.19         8.95        17.49    17.48     39.12      37.41      41.55     39.13      49.67       58.23        61.41    39.71     30.95      41.16       32.13     32.20\\r\\n     Ablation4              20%     99.40    12.18      18.25      36.74    15.35     21.15    38.91       18.78        11.99     20.92        20.31         10.99       20.84    18.05     44.73      44.45      47.86     46.87      51.41       59.57        65.76    42.43     31.07      44.67       32.65     35.01\\r\\n     From Scratch w/ PN++   20%       -      21.43      30.84      67.53    21.52     29.85    66.76       32.82        23.17     38.37        36.74         23.42       36.23    25.19     55.25      60.25      55.11     60.37      53.34       65.85        88.22    50.77     47.05      52.70       45.88     45.36\\r\\n     CMC* [45] w/ PN++      20%       -      24.12      32.89      73.11    23.84     32.67    73.20       33.43        27.55     44.62        38.40         27.59       42.11    26.55     57.82      65.60      57.73     65.02      54.53       66.31        89.16    55.04     51.00      56.66       50.82     48.74\\r\\n     Ours* w/ PN++          20%       -      23.96      32.90      73.30    24.16     32.44    73.10       34.81        29.54     45.43        37.79         28.16       42.89    27.83     58.25      66.16      58.64     65.51      55.60       66.92        89.51    56.33     53.05      57.87       52.29     49.43\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n17\\r\\n                                          Table 12. Detailed Cross-Modality Supervision and Missing-Modality Inference Results on NTURGBD-Parsing-4K. All results in [%].\\r\\n\\r\\n     Methods           Setting       bg     right hip right knee right foot left hip left knee left foot left shoulder left elbow left hand right shoulder right elbow right hand crotch right thigh right calf left thigh left calf lower spine upper spine head left arm left forearm right arm right forearm mIoU\\r\\n     No Contrastive RGB→Depth 92.87           0.00       0.00       0.00     0.00      0.00     0.00        0.00        0.00       0.22        0.00          0.00        1.08      0.00     0.00       0.25       0.00     0.00       0.00         0.00         2.62     0.00     0.45        0.00       1.11       3.94\\r\\n     CMC [45]       RGB→Depth 89.79           0.00       0.00       0.00     0.00      0.02     0.01        0.00        1.51       0.79        0.00          0.11        0.43      0.00     0.00       0.00       0.01     0.00       1.02         0.20         0.42     1.64     0.37        0.03       0.14       3.86\\r\\n     Ours           RGB→Depth 96.78          12.50      23.26      37.60    16.45     21.43    40.51       22.41       19.59      22.86       21.35         17.19       25.67     17.40    36.48      35.62      37.43    34.04      49.83        59.37        61.07    33.91    24.55       36.15      26.37      33.19\\r\\n     No Contrastive Depth→RGB 91.79           0.00       0.00       0.00     0.00      0.00     0.00        0.00        0.00       0.23        0.00          0.00        0.07      0.00     0.01       0.00       0.00     0.00       0.10         0.00         0.02     0.00     0.50        0.00       0.02       3.71\\r\\n     CMC [45]       Depth→RGB 91.96           0.00       0.00       0.00     0.00      0.00     0.01        0.00        0.68       0.32        0.00          0.00        0.21      0.00     0.00       0.00       0.00     0.00       0.46         0.23         0.05     0.49     1.84        0.00       0.01       3.85\\r\\n     Ours           Depth→RGB 95.15          13.70      16.04      28.70    16.59     12.15    28.12       18.11       12.78      10.51       21.06         11.12       14.46     11.02    33.71      30.90      34.03    22.93      42.62        50.99        56.12    23.51    19.89       27.29      18.50      26.80\\r\\n     No Contrastive   Only RGB      93.55     8.97       6.66       0.42     4.28      0.75     0.02        0.98        1.19      15.22        0.28          2.69       23.56      0.08     0.30      12.35       0.07     0.88      25.48         5.27        57.54    12.63    26.38        7.73      28.90      13.45\\r\\n     CMC [45]         Only RGB      93.80     0.00      11.94      47.69     0.00     12.00    38.76       21.43        0.01       9.58       24.32         13.56       15.38      1.15     1.84      32.30       1.07    18.12       0.59         3.13        36.86    27.04    15.63       42.37      21.90      19.62\\r\\n     Ours             Only RGB      97.80    29.12      27.49      57.93    27.09     26.41    57.31       28.22       27.62      32.70       29.51         26.16       33.41     25.66    52.42      49.60      52.38    54.91      52.40        52.75        75.69    47.98    41.69       50.68      40.08      43.88\\r\\n     No Contrastive   Only Depth    96.46    27.81       7.16      1.46     33.36     10.60    2.30        28.64       4.72       1.27        11.49          0.11        7.86     26.16    33.67      39.55      33.21    27.40      46.05        63.16        47.50    25.38     9.99       19.89       5.04      24.41\\r\\n     CMC [45]         Only Depth    94.81     7.25       1.08      0.06     6.82      0.05     0.10        25.47        3.11      2.95        20.80          0.39       2.73      26.79    17.11      17.84      33.96    4.58       10.64        11.32        37.01    41.41    11.08       33.69      3.52       16.58\\r\\n     Ours             Only Depth    97.89    23.09      32.97      54.55    26.44     32.55    57.28       34.08       19.83      25.23       32.33         21.33       28.71     30.57    54.91      59.33      53.16    59.89      56.50        65.09        61.87    49.43    36.47       50.34      35.75      43.98\\r\\n\\x0c\\n\\nRGB             GT                   IN               CMC                 MMV                 Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  Figure 7. Qualitative Results of RGB Human Parsing on Human3.6M with 10% of the Training Set.\\r\\n\\r\\n\\r\\n\\r\\n                                               18\\r\\n\\x0c\\n\\nRGB                GT                  IN                 CMC                MMV                    Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      Figure 8. Qualitative Results of RGB Human Parsing on Human3.6M with the Full Training Set.\\r\\n\\r\\n\\r\\n\\r\\n                                                  19\\r\\n\\x0c\\n\\nDepth               GT                  IN                CMC                MMV                  Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n Figure 9. Qualitative Results of Depth Human Parsing on NTURGBD-Parsing-4K with the Full Training Set.\\r\\n\\r\\n\\r\\n\\r\\n                                                  20\\r\\n\\x0c',\n",
       " \"                                              Eur. Phys. J. A manuscript No.\\r\\n                                              (will be inserted by the editor)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          Femtoscopic study on DD∗ and DD̄∗ interactions for Tcc and X(3872) a\\r\\n                                          Yuki Kamiyab,1,2 , Tetsuo Hyodoc,3,2 , Akira Ohnishid,4\\r\\n                                          1\\r\\n                                            Helmholtz Institut für Strahlen- und Kernphysik and Bethe Center for Theoretical Physics, Universität Bonn, D-53115 Bonn, Germany\\r\\n                                          2\\r\\n                                            RIKEN Interdisciplinary Theoretical and Mathematical Science Program (iTHEMS), Wako 351-0198, Japan\\r\\n                                          3\\r\\n                                            Department of Physics, Tokyo Metropolitan University, Hachioji 192-0397, Japan\\r\\n                                          4\\r\\n                                            Yukawa Institute for Theoretical Physics, Kyoto University, Kyoto 606-8502, Japan\\r\\narXiv:2203.13814v1 [hep-ph] 25 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          Received: date / Accepted: date\\r\\n\\r\\n\\r\\n                                          Abstract We investigate DD∗ and DD̄∗ momentum corre-                     valence quark components (ccūd).   ¯ Although the X(3872)\\r\\n                                          lations in high-energy collisions to elucidate the nature of             and Tcc are in different sectors, there is one similarity be-\\r\\n                                          Tcc and X(3872) exotic hadrons. Single range Gaussian po-                tween them, i.e., the existence of a nearby two-meson thresh-\\r\\n                                          tentials with the channel couplings to the isospin partners              old. Namely, the Tcc peak is also found just below the DD∗\\r\\n                                          are constructed based on the empirical data. The momen-                  threshold. The proximity with the DD̄∗ and DD∗ thresholds\\r\\n                                          tum correlation functions of the D0 D∗+ , D+ D∗0 , D0 D̄∗0 ,             would imply the molecular nature of these states. It should\\r\\n                                          and D+ D∗− pairs are computed with including the coupled-                be noted, however, that the structure of X(3872) is still under\\r\\n                                          channel effects. We discuss how the nature of the exotic                 debate. In the study of Ref. [10], it is shown that the con-\\r\\n                                          states are reflected in the behaviors of the correlation results.        tribution of the cc̄ component is important by the analysis\\r\\n                                                                                                                   of the prompt production cross section. On the other hand,\\r\\n                                          PACS 25.75.Gz · 21.30.Fe · 13.75.Lb · 14.40.Rt\\r\\n                                                                                                                   the enhancement of the production yield in AA collisions\\r\\n                                                                                                                   observed in CMS [11] seems to imply that X(3872) con-\\r\\n                                                                                                                   tains a significant fraction of the hadronic molecule compo-\\r\\n                                          1 Introduction                                                           nent [12]. In order to discriminate the possible structures of\\r\\n                                                                                                                   X(3872), it is desirable to experimentally access the DD̄∗\\r\\n                                          The study of various exotic resonances in heavy quark sec-               interaction.\\r\\n                                          tors has been one of the most interesting subjects in recent\\r\\n                                          hadron physics [1, 2, 3]. The most extensively studied state                 For the study of the near-threshold resonances, the fem-\\r\\n                                          is the X(3872) lying just below the DD̄∗ threshold, which                toscopy using the two-particle momentum correlation func-\\r\\n                                          is listed as χc1 (3872) in the current PDG paper [4]. Ever               tion in high-energy collisions is a helpful technique because\\r\\n                                          since its first observation in 2003 [5], this exotic hadron has          the correlation function is sensitive to the low-energy hadron\\r\\n                                          attracted huge interest of researchers and a bunch of the ex-            interactions. With the femtoscopy, various interactions in the\\r\\n                                          perimental and theoretical studies have been devoted to un-              strangeness sector have been investigated theoretically [13,\\r\\n                                          derstand this state. Nevertheless, its nature still remains to           14, 15, 16, 17, 18, 19, 20, 21] and experimentally [22, 23, 24, 25,\\r\\n                                          be elucidated.                                                           26, 27, 28, 29, 30, 31, 32, 33]. It turns out that the source size\\r\\n                                               Recently, the LHCb Collaboration reported a clear sig-              dependence of the correlation function is useful to distin-\\r\\n                                          nal of the doubly charmed tetraquark state Tcc   +\\r\\n                                                                                              in the mass          guish the existence or non-existence of hadronic bound states [19,\\r\\n                                                            0 0 +\\r\\n                                          spectrum of D D π [6, 7]. Such exotic states with two                    20]. Recently, the D− p correlation function has been mea-\\r\\n                                          heavy quarks and two light antiquarks are theoretically pre-             sured by the ALICE collaboration [34], which paves the way\\r\\n                                          dicted with the quark model in Refs. [8, 9] more than thirty             to the femtoscopy in the charm sector.\\r\\n                                          years ago. In contrast to the X(3872), this Tcc state is found               In this study, we discuss the correlation functions of the\\r\\n                                          in the genuine exotic channel, which requires at least four              DD∗ and DD̄∗ channels towards the understanding of the\\r\\n                                          a\\r\\n                                            Report No.: YITP-22-26\\r\\n                                                                                                                   nature of the Tcc and X(3872) states. To this end, we con-\\r\\n                                          b\\r\\n                                            e-mail: kamiya@hiskp.uni-bonn.de                                       struct one-range Gaussian potentials for the DD∗ and DD̄∗\\r\\n                                          c\\r\\n                                            e-mail: hyodo@tmu.ac.jp                                                channels which reproduce the empirical information in these\\r\\n                                          d\\r\\n                                            e-mail: ohnishi@yukawa.kyoto-u.ac.jp                                   channels. Including the coupled-channel effects with the isospin\\r\\n\\x0c\\n\\n2\\r\\n\\r\\n\\r\\npartners and the decay channels, we compute the correlation         where V0 is the interaction strength and m is the parame-\\r\\nfunctions.                                                          ter of the dimension of mass to control the range of the in-\\r\\n    This paper is organized as follows. In Sect. 2, we con-         teraction. Here we use the charged (isospin averaged) pion\\r\\nstruct the DD∗ and DD̄∗ potentials from the empirical data          mass mπ± (mπ ) for the DD∗ (DD̄∗ ) interactions because\\r\\nand summarize the method to calculate the correlation func-         the lightest exchangeable meson, pion, determines the in-\\r\\ntion with coupled-channel source effect. In Sect. 3, we show        teraction range. Thus, in this formulation, we are left with a\\r\\nthe results of the correlation functions of the D0 D∗+ , D+ D∗0 ,   single parameter V0 for each DD∗ /DD̄∗ potential. Note that\\r\\nD0 D̄∗0 , and D+ D∗− channels and discuss how the exotic            V0 takes a complex number, in order to express the decay ef-\\r\\nstates can be studied in the future femtoscopy experiments.         fects into the lower energy channels. While the DD∗ poten-\\r\\nSection 4 is devoted to summarize this study.                       tial is free from the Coulomb interaction, for the {D+D∗−}\\r\\n                                                                    channel we should include the Coulomb force:\\r\\n                                                                                   \\x12        \\x13\\r\\n                                                                       c             0 0\\r\\n2 Method                                                            VDD̄∗ (r) =               ,                                (5)\\r\\n                                                                                     0 −α/r\\r\\n\\r\\nLet us first summarize the relevant channels which couple to        with the fine structure constant α. This potential is added to\\r\\nthe system of interest. For the Tcc and X(3872) states, we          Eq. (1) for the DD̄∗ potential.\\r\\ncannot neglect the mass difference among the isospin mul-                Here we determine the potential strength V0 so as to re-\\r\\ntiplets, because the deviation of the eigenenergy from the          produce the empirical data for these systems. For the DD∗\\r\\n                                                                                                                 0 ∗+\\r\\nthreshold is comparable or smaller than the isospin breaking        potential, we use the scattering length a0D D = −7.16 +\\r\\neffect. The Tcc locates just below the D0 D∗+ threshold, and        i1.85 fm, given in the experimental analysis in Ref. [7].1 For\\r\\n                                                                                                                        {D 0 D̄ ∗0}\\r\\nit also couples to the D+ D∗0 channel whose threshold lies          the DD̄∗ potential, we use the scattering length a0             =\\r\\nslightly above that of the D0 D∗+ channel. At energies lower        −4.23 + i3.95 fm which is determined by the eigenenergy\\r\\nthan the Tcc , the three-body DDπ channels are open, which          Eh = −0.04 − i0.60 MeV in PDG [4] measured from the\\r\\n                                                                                             {D 0 D̄ ∗0}      √\\r\\nprovide the finite decay width of Tcc . The X(3872)\\r\\n                                               √      lies just     D0 D̄∗0 threshold, as a0             = −i/ 2µEh with the re-\\r\\nbelow the {D0 D̄∗0} = (D0 D̄∗0 + D̄0 D∗0 )/ 2 (C = +)               duced mass µ. We notice that these scattering lengths have\\r\\n                                                     + ∗−\\r\\n                         √ to the higher energy {D D } =\\r\\nthreshold and couples also\\r\\n    + ∗−        − ∗+\\r\\n                                                                    a much larger magnitude than the typical length scale of the\\r\\n(D D +D D )/ 2 (C = +) channel. At much lower                       strong interaction ∼ 1 fm. The obtained potential strengths\\r\\nenergies, the decay channels such as ππJ/ψ couple to the            are summarized in Table 1. For the later use, the scattering\\r\\nX(3872). In the following, we explicitly treat the D0 D∗+           lengths of the higher channels (D+ D∗0 and {D+D∗−}) cal-\\r\\nand D+ D∗0 channels for Tcc and {D0 D̄∗0} and {D+D∗−}               culated with the same potentials are also listed. Note that\\r\\nchannels for X(3872), and the decay effect to the other chan-       all these calculations are performed in the coupled-channel\\r\\nnels are renormalized in the imaginary part of the potential.       scheme.\\r\\nThus, the Hamiltonian of the system is expressed by a 2 × 2              To calculate the correlation functions C(q) with the coupled-\\r\\nmatrix in the channel basis.                                        channel effects, we employ the Koonin-Pratt-Lednicky-Lyuboshitz-\\r\\n     Next, we construct the DD∗ and DD̄∗ potentials. As-            Lyuboshitz formula (KPLLL) formula [35, 18, 21] given by\\r\\nsuming that the interaction is isospin symmetric, the strong\\r\\ninteraction part of the coupled-channel potentials can be given              Z          2\\r\\n                                                                                                         (−)\\r\\n                                                                                        X\\r\\nby the I = 0 and I = 1 components as                                C(q) =       d3 r         ωi Si (r)|Ψi     (q; r)|2 ,             (6)\\r\\n                   \\x12                             \\x13                                      i=1\\r\\n                 1 VI=1 + VI=0 VI=1 − VI=0                                                            (−)\\r\\nVDD∗ /DD̄∗ =                                        ,       (1)     where the wave function Ψi in the i-th channel is written\\r\\n                 2 VI=1 − VI=0 VI=1 + VI=0\\r\\n                                                                    as a function of the relative coordinate r, with imposing the\\r\\nwhere we assign channel i = 1 and 2 to D+ D∗0 and D0 D∗+            outgoing boundary condition on the measured channel. We\\r\\nfor the DD∗ system and {D0 D̄∗0} and {D+D∗−} for the                consider the small momentum region and assume that only\\r\\n                                                                                                                      (−)\\r\\nDD̄∗ system, respectively. Because the Tcc and X(3872)              the s-wave component of the wave function Ψi is modi-\\r\\ncouples to the I = 0 channel, we assume that the I = 0              fied by the strong interaction. The wave function is calcu-\\r\\ncomponent gives the dominant contribution, and set                  lated by solving the Schrödinger equation with the hermite\\r\\n                                                                    conjugated potential V † , which gives the appropriate bound-\\r\\nVI=0 = V (r),                                              (2)      ary condition for the eliminated decay channels (See Ap-\\r\\nVI=1 = 0,                                                  (3)      pendix A). We adopt a common static Gaussian source func-\\r\\n                                                                    tion for all the channels Si (r) = exp(−r2 /4R2 )/(4πR2 )3/2\\r\\nwhere V (r) is a spherical Gaussian potential:                      1\\r\\n                                                                     Here we use the the high-energy physics convention for the scatter-\\r\\n                     2 2                                            ing length where the positive (negative) real value corresponds to the\\r\\nV (r) = V0 exp(−m r ),                                     (4)      weakly attractive (repulsive or strongly attractive) interaction.\\r\\n\\x0c\\n\\n                                                                                                                                    3\\r\\n\\r\\n\\r\\nTable 1 Strength parameters V0 for the DD∗ and DD̄∗ potentials and the scattering lengths in the DD∗ and DD̄∗ channels. The scattering\\r\\nlengths of the lower channels (third column) are the empirical inputs.\\r\\n\\r\\n\\r\\n                                                                       0   ∗+                 +   ∗0\\r\\n                                   DD∗           V0 [MeV]          aD\\r\\n                                                                    0\\r\\n                                                                      D\\r\\n                                                                                  [fm]   aD\\r\\n                                                                                          0\\r\\n                                                                                            D\\r\\n                                                                                                       [fm]\\r\\n                                             −36.569 − i1.243     −7.16 + i1.85          −1.75 + i1.82\\r\\n                                                                    {D 0 D̄ ∗0}           {D+D ∗−}\\r\\n                                   {DD̄∗}        V0 [MeV]         a0              [fm]   a0            [fm]\\r\\n                                             −43.265 − i6.091     −4.23 + i3.95          −0.41 + i1.47\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n          8                                                          with the source size R, and the weight factor ωi is taken as\\r\\n                                              1 fm\\r\\n                                                                     unity for all channels. The weight factor ωi represents the\\r\\n                                              2 fm\\r\\n          7                                                          ratio of the pair production yield in the ith channel with re-\\r\\n                                              3 fm\\r\\n                                              5 fm                   spect to the measured channel. Since we only include the\\r\\n          6                                                          coupled-channel effect of the isospin partners, they are con-\\r\\n                                                                     sidered to have the equivalent emitting source. The source\\r\\n          5                                                          size R ranges from ∼ 1 fm for the high-multiplicity events\\r\\n                                                                     in pp collisions to ∼ (5 − 6) fm for the central PbPb colli-\\r\\nCD0 D∗+\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n          4                                                          sions.\\r\\n                                                                         While we construct the DD̄∗ potential in the charge con-\\r\\n          3                                                          jugation C = + combination which couples to the X(3872),\\r\\n                                                                     the experimental measurement of the correlation function\\r\\n          2                                                          will be done with fixed charge states, i.e., either D0 D̄∗0\\r\\n                                                                     or D̄0 D∗0 . To obtain the correlation functions of the fixed\\r\\n          1                                                          charge states, the correlation functions in the C = − sector\\r\\n                                                                     are also needed to take an average of the C = + and C = −\\r\\n          0                                                          contributions. In this exploratory study, we assume that the\\r\\n              0   50   100      150    200        250       300      C = − interaction is small and can be neglected with respect\\r\\n                             q [MeV/c]\\r\\n                                                                     to the dominant C = + contribution. In this case, we obtain\\r\\n          8\\r\\n                                              1 fm                   the experimentally accessible correlation functions from the\\r\\n                                              2 fm                   correlation function calculated by the C = + potential as\\r\\n          7\\r\\n                                              3 fm\\r\\n                                                                                         1\\r\\n                                              5 fm\\r\\n                                                                                                     \\x01\\r\\n                                                                       CD0 D̄∗0 = CD̄0 D∗0 =\\r\\n                                                                                           C 0 ∗0 + 1 ,                           (7)\\r\\n          6                                                                              2 {D D̄ }\\r\\n                                                                                         1                     \\x01\\r\\n                                                                     CD+ D∗− = CD− D∗+ =   C + ∗− + Cpure Coul. ,                 (8)\\r\\n          5                                                                              2 {D D }\\r\\n                                                                     where Cpure Coul. is calculated only with the Coulomb inter-\\r\\nCD+ D∗0\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n          4                                                          action by switching off the strong interaction contribution.\\r\\n\\r\\n          3\\r\\n                                                                     3 Results\\r\\n          2\\r\\n                                                                     Now we calculate the correlation functions with the con-\\r\\n          1                                                          structed potentials. First we show the DD∗ sector coupled\\r\\n                                                                     with the Tcc state. The correlation function of the D0 D∗+\\r\\n          0                                                          and the D+ D∗0 pairs with source sizes R = 1, 2, 3, and 5\\r\\n              0   50   100      150    200        250       300      fm are shown in Fig. 1. We can see that the source size de-\\r\\n                             q [MeV/c]\\r\\n                                                                     pendence typical to the system with a shallow bound state\\r\\nFig. 1 The correlation functions of the D0 D∗+ (top) and D+ D∗0      for both correlation functions; the enhancement in the small\\r\\n(bottom) pair with the source size R = 1, 2, 3, and 5 fm.\\r\\n                                                                     source case turns to the suppression for the large source\\r\\n                                                                     case [21]. The stronger correlation is found in the D0 D∗+\\r\\n                                                                     channel, whose threshold is closer to the Tcc pole. The cusp\\r\\n                                                                     structure is seen at the D+ D∗0 threshold (q ' 52 MeV/c) in\\r\\n\\x0c\\n\\n4\\r\\n\\r\\n\\r\\nthe D0 D∗+ correlation, while the strength is not very promi-\\r\\n                                                                              2.4                                                      1 fm\\r\\nnent.\\r\\n                                                                                                                                       2 fm\\r\\n    Next we show the results of the DD̄∗ correlation func-                    2.2                                                      3 fm\\r\\ntion coupled with the X(3872) in Fig. 2. Here we plot the                                                                              5 fm\\r\\ncorrelation functions of the fixed charges states in Eqs. (7)                  2\\r\\nand (8) which can be compared with the experimental mea-                      1.8\\r\\nsurements. The characteristic strong source size dependence\\r\\nwith the shallow bound state is found in CD0 D̄∗0 . We can                    1.6\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                   CD0 D̄∗0\\r\\nalso see the cusp structure at the D+ D∗− threshold (q '\\r\\n                                                                              1.4\\r\\n126 MeV/c). The cusp structure is more prominent for the\\r\\nsmaller source case. This is because the coupled-channel                      1.2\\r\\nsource effect by the D+ D∗− channel is stronger for the\\r\\n                                                                               1\\r\\nsmaller source case [20]. On the other hand, due to the at-\\r\\ntractive Coulomb force, the CD+ D∗− correlations show a                       0.8\\r\\nstrong enhancement at small q. To extract the contribution\\r\\nby the strong interaction, we show the difference from the                    0.6\\r\\npure Coulomb case ∆C = CD+ D∗− − Cpure Coul. . We can                         0.4\\r\\nsee that the effect of the strong interaction emerges mainly                        0              50           100       150    200       250     300\\r\\n                                                                                                                       q [MeV/c]\\r\\nas the suppression compared to the pure Coulomb case. How-\\r\\never, the deviation |∆C| is less than 0.2 for the momentum\\r\\nregion q > 50 MeV/c. Thus, the correlation of D+ D∗− pair                      4\\r\\n                                                                                                                                  1 fm\\r\\nis expected to be dominated by the Coulomb contribution.\\r\\n                                                                                                        0                         2 fm\\r\\n    In this study, we used the empirically determined scat-                   3.5                                                 3 fm\\r\\n                                                                                        ∆CD+ D∗−\\r\\n\\r\\n\\r\\n\\r\\ntering lengths as input to calculate the correlation functions.                                −0.5\\r\\n                                                                                                                                  5 fm\\r\\nGiven the correlation data obtained from the precise future                    3                   −1\\r\\nmeasurement, we can independently determine the scatter-\\r\\ning lengths a0 because the correlation functions are sensitive                                 −1.5\\r\\n                                                                   CD+ D∗−\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                              2.5\\r\\nto the low-energy interaction. According to the Weinberg’s                                         −2\\r\\n                                                                                                            0    100 200 300\\r\\nweak-binding relation [36, 37, 38], the compositeness, which                                                     q [MeV/c]\\r\\nis defined as the probability of finding molecular state in the                2\\r\\neigenstate, is directly related to the ratio of the a0 /Rh where\\r\\nRh is the length scale determined with the eigenenergy Eh                     1.5\\r\\n              √\\r\\nas Rh = 1/ −2µEh . Thus, combined with the informa-\\r\\ntion of the pole position, to measure the these correlation\\r\\n                                                                               1\\r\\nfunctions leads to understand the nature of Tcc and X(3872)\\r\\nstates.                                                                             0              50           100      150    200      250     300\\r\\n                                                                                                                      q [MeV/c]\\r\\n\\r\\n                                                                   Fig. 2 The correlation functions of the D0 D̄∗0 (top) and D+ D∗−\\r\\n4 Summary                                                          (bottom) pair with the source size R = 1, 2, 3, and 5 fm. For D+ D∗−\\r\\n                                                                   pair, the difference from the pure Coulomb case ∆C is shown in sub\\r\\nWe have studied the correlation functions of the DD∗ and           figure.\\r\\nDD̄∗ pairs for the purpose of the investigation of the Tcc and\\r\\nX(3872) exotic states. With the assumption of the molecu-\\r\\nlar nature of these states, one-range Gaussian potentials are\\r\\nconstructed for the DD∗ and DD̄∗ channels from the em-             by the ALICE collaboration [34], we expect that the mea-\\r\\npirical data. Due to the large scattering lengths, the calcu-      surements of the DD∗ and DD̄∗ correlations in future will\\r\\nlated correlation functions in the lower channels (D0 D∗+          bring new insights of the exotic hadrons from the viewpoint\\r\\nand D0 D̄∗0 ), which are closer to the exotic states, show the     of the femtoscopy.\\r\\ncharacteristic behavior of the bound state below the thresh-           In this study, we have introduced the potentials in the\\r\\nold. On the other hand, the correlation function of the D+ D∗0     channels that couple to the exotic states (isospin I = 0 and\\r\\nchannel shows less prominent behavior due to the energy            charge conjugation C = +), and have neglected the interac-\\r\\ndifference from the Tcc pole, and the correlation in the D+ D∗−    tions in the other channels. This is because the existence of\\r\\nchannel is mainly caused by the Coulomb interaction. Given         near-threshold states implies the strong interaction, which is\\r\\nthe successful measurement of the D− p correlation function        considered to give the dominant contribution for the corre-\\r\\n\\x0c\\n\\n                                                                                                                            5\\r\\n\\r\\n\\r\\nlation function. For more quantitative discussion of the cor-     G0 = diag.(G01 , G02 ),                               (A.5)\\r\\nrelation functions, these subleading effects should also be\\r\\nconsidered. In particular, the cusp structure may be sensitive    with the free propagator\\r\\nto the isospin I = 1 interaction, because the coupling be-\\r\\ntween the isospin partners are given by the difference of the     G0i (z) = (z − Hii0 )−1 .                             (A.6)\\r\\ntwo isospin components. The DD̄∗ interaction in the C = −\\r\\nsector is still unclear at this moment, but the neutral partner   With the Feshbach projection [40, 41] for channel 2, the Lippmann-\\r\\nof Zc (3900) [39] may play an important role in this channel.     Schwinger equation for channel 1 can be written with the\\r\\nThese effect should be discussed in the future studies.           effective potential Veff as\\r\\n\\r\\n                                                                  T11 (z) = Veff (z) + Veff (z)G01 (z)T11 (z),          (A.7)\\r\\nAppendix A: Outgoing boundary condition for the                   Veff (z) = V11 + V12 G2 (z)V21 ,                      (A.8)\\r\\noptical complex potential\\r\\n                                                                  where Gi (z) is the full propagator given as\\r\\nThe wave function for the KPLLL formula (6) must sat-\\r\\nisfy the outgoing boundary condition where the flux of the        Gi (z) = (z − Hii )−1 .                               (A.9)\\r\\noutgoing wave of the reference channel is normalized to be\\r\\nunity. On the other hand, the complex optical potentials are                                                (0)\\r\\n                                                                  The contour of the time integration of Gi (z) can be chosen\\r\\nconstructed based on the scattering problem with the incom-       by taking z → E + i\\x0f for the scattering problem. On the\\r\\ning boundary condition where the flux of the incoming wave        other hand, that of the time-reversed system can be given\\r\\nis normalized. This boundary condition is applied to the in-      as z → E − i\\x0f. This effective potential is complex due to\\r\\ntegrated channels, whose coupling to referenced channels          the pole term included in G2 (E − i\\x0f). Then the effective\\r\\n(D+ D∗0 and D0 D∗+ in the case of DD∗ sector) give the            potential in the time reversed system is given as\\r\\nimaginary part of the potential. Thus, we cannot obtain the\\r\\ncorrect wave function ψ by solving the Schrödinger equa-         Veff (E − i\\x0f) = V11 + V12 G2 (E − i\\x0f)V21\\r\\ntion                                                                                †     † †            †\\r\\n                                                                                = V11 + V21 G2 (E + i\\x0f)V12\\r\\nHψ = [H0 + V ]ψ = Eψ,                                    (A.1)                                                    †\\r\\n                                                                                = [V11 + V12 G2 (E + i\\x0f)V21 ]\\r\\nwith the boundary condition only with the referenced chan-                          †\\r\\n                                                                                = Veff (E + i\\x0f).                       (A.10)\\r\\nnels.\\r\\n    We claim that we can just take the hermite conjugate of       Here we assumed that the full Hamiltonian is hermitian and\\r\\nthe potential V and solve the Schrödinger equation in or-        the potential V is real. Thus, the hermite conjugated effec-\\r\\nder to obtain the wave function which satisfies the boundary      tive potential corresponds to that in the time reversed sys-\\r\\nconditions for all the channels,                                  tem. Remembering that the time reversal operator T acts on\\r\\n[H0 + V † ]ψ = Eψ,                                       (A.2)    the wave function as T ψ = ψ ∗ [42], the system obtained\\r\\n                                                                  from Eq. (A.2) with the outgoing boundary condition cor-\\r\\nwith the outgoing boundary condition. One can easily check        responds to the time-reversed system written with Eq. (A.1)\\r\\nthat ψ ∗ satisfies the original Schrödinger equation with in-    with incoming boundary condition.\\r\\ncoming boundary condition.                                            The imaginary part of the optical potential causes the\\r\\n    Taking the hermite conjugate of the potential V corre-        suppression or the enhancement of the wave function com-\\r\\nsponds to consider the time reversal of the system. This can      ponent of the referenced channel depending on its sign. In\\r\\nbe understood as follows. Let us consider the two chan-           the scattering problem of the coupled-channel system, the\\r\\nnel scattering problem with spinless particles where channel      asymptotic form of the s-wave component of the scattering\\r\\n1 (2) has higher threshold energy and is measured (lower          wave function of channel 1 is given with the S matrix com-\\r\\nthreshold energy and is not measured). The Hamiltonian for        ponent as\\r\\nthis system is given as\\r\\n                                                                                 1\\r\\n                                                                                     e−iqr − S11 eiqr .\\r\\n                                                                                                     \\x01\\r\\n                                                                  ψ1 (q; r) →\\r\\n      \\x12            \\x13 \\x12 0                          \\x13\\r\\n        H11 H12            H11 + V11       V12                                                                         (A.11)\\r\\nH=                    =                  0          ,   (A.3)                   2iqr\\r\\n        H21 H22               V21      H22  + V22\\r\\n          0                                                       Due to the coupling to channel 2, the absolute value of the\\r\\nwhere Hij   and Vij are the free Hamiltonian and the interac-\\r\\n                                                                  S matrix component S11 is less than unity, which leads the\\r\\ntion potential, respectively. The Lippmann-Schwinger equa-\\r\\n                                                                  reduced outgoing wave (eiqr ) compared to the normalized\\r\\ntion for the T matrix is given as\\r\\n                                                                  incoming wave (e−iqr ). When we use the complex optical\\r\\nT = V + V G0 T,                                          (A.4)    potential V with negative imaginary part, this reduction of\\r\\n\\x0c\\n\\n6\\r\\n\\r\\n\\r\\nthe wave function is caused by the imaginary part of the po-             20. Y. Kamiya, T. Hyodo, K. Morita, A. Ohnishi, and W. Weise, Phys.\\r\\ntential. On the other hand, the outgoing boundary condition,                 Rev. Lett. 124, 132501 (2020).\\r\\n                                                                         21. Y. Kamiya, K. Sasaki, T. Fukui, T. Hyodo, K. Morita, K. Ogata,\\r\\nwhich is used for the correlation study, is given as\\r\\n                                                                             A. Ohnishi and T. Hatsuda, Phys. Rev. C 105, no.1, 014915\\r\\n               1 \\x10 iqr               \\x11                                       (2022).\\r\\n                             † −iqr\\r\\nψ1 (q; r) →         e − S11    e       .              (A.12)             22. L. Adamczyk et al. [STAR], Phys. Rev. Lett. 114, no.2, 022301\\r\\n             2iqr                                                            (2015).\\r\\nIn this case, the flux of the outgoing wave (1) is larger than           23. S. Acharya et al. [ALICE], Phys. Lett. B 774, 64-77 (2017).\\r\\n                                †                                        24. J. Adam et al. [STAR], Phys. Lett. B 790, 490-497 (2019).\\r\\nthat of the incoming wave (|S11   |). This is because the wave           25. S. Acharya et al. [ALICE], Phys. Rev. C 99, no.2, 024001 (2019).\\r\\nfunction of channel 2 flows into channel 1 by the coupling               26. S. Acharya et al. [ALICE], Phys. Rev. Lett. 123, no.11, 112002\\r\\npotential to give the normalized outgoing wave. When we                      (2019).\\r\\nuse the hermite conjugated optical potential V † with positive           27. S. Acharya et al. [ALICE], Phys. Lett. B 805, 135419 (2020).\\r\\n                                                                         28. S. Acharya et al. [ALICE], Phys. Lett. B 797, 134822 (2019).\\r\\nimaginary part, its imaginary part causes the enhancement                29. S. Acharya et al. [ALICE], Phys. Rev. Lett. 124, no.9, 092301\\r\\nof the channel 1 component. We also note that the result-                    (2020).\\r\\ning wave function can also be obtained by solving Eq. (A.1)              30. S. Acharya et al. [ALICE], Nature 588, 232-238 (2020) [erratum:\\r\\nwith the (standard) incoming boundary condition and taking                   Nature 590, E13 (2021)].\\r\\n                                                                         31. S. Acharya et al. [ALICE], Phys. Lett. B 822, 136708 (2021).\\r\\nthe complex conjugate of the wave function.                              32. S. Acharya et al. [ALICE], Phys. Rev. Lett. 127, no.17, 172301\\r\\n                                                                             (2021).\\r\\nAcknowledgements The authors thank Laura Fabbietti and Fabrizio          33. L. Fabbietti, V. Mantovani Sarti and O. Vazquez Doce, Ann. Rev.\\r\\nGrosa for useful discussions. This work has been supported in part by        Nucl. Part. Sci. 71, 377-402 (2021).\\r\\nthe Grants-in-Aid for Scientific Research from JSPS (Grant numbers       34. S. Acharya et al. [ALICE], [arXiv:2201.05352 [nucl-ex]].\\r\\nJP21H00121, JP19H05150, JP19H05151, JP19H01898, JP18H05402,              35. R. Lednicky, V. V. Lyuboshits, and V. L. Lyuboshits, Phys. Atomic\\r\\nand JP16K17694), by the Yukawa International Program for Quark-              Nuclei 61, 2950 (1998).\\r\\nhadron Sciences (YIPQS), by the Deutsche Forschungsgemeinschaft          36. S. Weinberg, Phys. Rev. 137, B672 (1965).\\r\\n(DFG) and the National Natural Science Foundation of China (NSFC)        37. Y. Kamiya and T. Hyodo, Phys. Rev. C93, 035203 (2016).\\r\\nthrough the funds provided to the Sino-German Collaborative Research     38. Y. Kamiya and T. Hyodo, PTEP 2017, 023D02 (2017).\\r\\nCenter “Symmetries and the Emergence of Structure in QCD” (NSFC          39. M. Ablikim et al. [BESIII], Phys. Rev. Lett. 115, no.11, 112003\\r\\nGrant No. 12070131001, DFG Project-ID 196253076 – TRR 110).                  (2015).\\r\\n                                                                         40. H. Feshbach, Annals Phys. 5, 357-390 (1958).\\r\\n                                                                         41. H. Feshbach, Annals Phys. 19, 287-313 (1962).\\r\\n                                                                         42. J.R. Taylor, Scattering Theory: The Quantum Theory on Nonrela-\\r\\nReferences                                                                   tivistic Collisions, Wiley, New York, 1972.\\r\\n\\r\\n 1. A. Hosaka, T. Iijima, K. Miyabayashi, Y. Sakai and S. Yasui, PTEP\\r\\n    2016, no.6, 062C01 (2016).\\r\\n 2. F. K. Guo, C. Hanhart, U. G. Meißner, Q. Wang, Q. Zhao and\\r\\n    B. S. Zou, Rev. Mod. Phys. 90, no.1, 015004 (2018).\\r\\n 3. N. Brambilla, S. Eidelman, C. Hanhart, A. Nefediev, C. P. Shen,\\r\\n    C. E. Thomas, A. Vairo and C. Z. Yuan, Phys. Rept. 873, 1-154\\r\\n    (2020).\\r\\n 4. Particle Data Group, P. A. Zyla et al., PTEP 2020, 083C01 (2020).\\r\\n 5. Belle, S. K. Choi et al., Phys. Rev. Lett. 91, 262001 (2003), hep-\\r\\n    ex/0309032.\\r\\n 6. LHCb, R. Aaij et al., (2021), 2109.01038.\\r\\n 7. LHCb, R. Aaij et al., (2021), 2109.01056.\\r\\n 8. J. l. Ballot and J. M. Richard, Phys. Lett. B 123, 449-451 (1983).\\r\\n 9. S. Zouzou, B. Silvestre-Brac, C. Gignoux and J. M. Richard, Z.\\r\\n    Phys. C 30, 457 (1986).\\r\\n10. C. Bignamini, B. Grinstein, F. Piccinini, A. D. Polosa and\\r\\n    C. Sabelli, Phys. Rev. Lett. 103, 162001 (2009).\\r\\n11. A. M. Sirunyan et al. [CMS], Phys. Rev. Lett. 128, no.3, 032001\\r\\n    (2022).\\r\\n12. S. Cho et al. [ExHIC], Prog. Part. Nucl. Phys. 95, 279-322 (2017).\\r\\n13. K. Morita, T. Furumoto, and A. Ohnishi, Phys. Rev. C91, 024916\\r\\n    (2015).\\r\\n14. A. Ohnishi, K. Morita, K. Miyahara, and T. Hyodo, Nucl. Phys.\\r\\n    A954, 294 (2016).\\r\\n15. K. Morita, A. Ohnishi, F. Etminan, and T. Hatsuda, Phys. Rev.\\r\\n    C94, 031901 (2016).\\r\\n16. T. Hatsuda, K. Morita, A. Ohnishi, and K. Sasaki, Nucl. Phys.\\r\\n    A967, 856 (2017).\\r\\n17. D. L. Mihaylov et al., Eur. Phys. J. C78, 394 (2018).\\r\\n18. J. Haidenbauer, Nucl. Phys. A981, 1 (2019).\\r\\n19. K. Morita et al., Phys. Rev. C 101, 015201 (2020).\\r\\n\\x0c\",\n",
       " '    Spatially Multi-conditional Image Generation\\r\\n\\r\\n          Ritika Chakraborty⋆1 Nikola Popovic⋆1 , Danda Pani Paudel1 ,\\r\\n                       Thomas Probst1 , Luc Van Gool1,2\\r\\n               1\\r\\n                Computer Vision Laboratory, ETH Zurich, Switzerland\\r\\n                    2\\r\\n                      VISICS, ESAT/PSI, KU Leuven, Belgium\\r\\n       {critika, nipopovic, paudel, probstt, vangool}@vision.ee.ethz.ch\\r\\n\\r\\n\\r\\n\\r\\n        Abstract. In most scenarios, conditional image generation can be thought\\r\\n        of as an inversion of the image understanding process. Since generic\\r\\n        image understanding involves the solving of multiple tasks, it is natu-\\r\\n        ral to aim at the generation of images via multi-conditioning. However,\\r\\n        multi-conditional image generation is a very challenging problem due to\\r\\n        the heterogeneity and the sparsity of the (in practice) available condi-\\r\\n        tioning labels. In this work, we propose a novel neural architecture to\\r\\n        address the problem of heterogeneity and sparsity of the spatially multi-\\r\\n        conditional labels. Our choice of spatial conditioning, such as by seman-\\r\\n        tics and depth, is driven by the promise it holds for better control of the\\r\\n        image generation process. The proposed method uses a transformer-like\\r\\n        architecture operating pixel-wise, which receives the available labels as\\r\\n        input tokens to merge them in a learned homogeneous space of labels.\\r\\n        The merged labels are then used for image generation via conditional\\r\\n        generative adversarial training. In this process, the sparsity of the labels\\r\\n        is handled by simply dropping the input tokens corresponding to the\\r\\n        missing labels at the desired locations, thanks to the proposed pixel-wise\\r\\n        operating architecture. Our experiments on three benchmark datasets\\r\\n        demonstrate the clear superiority of our method over the state-of-the-\\r\\n        art and the compared baselines.\\r\\n\\r\\n\\r\\n1     Introduction\\r\\n\\r\\nIn recent years, automated image generation under user control has become\\r\\nmore and more of a reality. Such processes are typically based on so-called con-\\r\\nditional image generation methods [34,21]. One could see these as an intermedi-\\r\\nate between fully unconditional [15] and purely rendering based [7] generation,\\r\\nrespectively. Methods belonging to these two extreme cases either offer no user\\r\\ncontrol (the former) or need to get all necessary information of image formation\\r\\nsupplied by the user (the latter). In many cases, neither extreme is desirable.\\r\\nTherefore, several conditional image generation methods, that circumvent the\\r\\nrendering process altogether, have been proposed. These methods usually re-\\r\\nceive the image descriptions either in the form of text [34,45] or as spatially\\r\\nlocalized semantic classes [21,39,47].\\r\\n⋆\\r\\n    Equal contributions.\\r\\n\\x0c\\n\\n2       R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n    In this paper, we aim to condition the image generation process beyond text\\r\\ndescriptions and the desired semantics. In this regard, we make a generic practi-\\r\\ncal assumption that any semantic or geometric aspects of the desired image may\\r\\nbe available for conditioning. For example, an augmented reality application may\\r\\nrequire to render a known object using its 3D model and specified pose, but with\\r\\nmissing texture and lighting details. In such cases, attributes such as semantic\\r\\nmask, depth, normal, curvature at that object’s location, become instantly avail-\\r\\nable for the image generation process, offering us the multi-conditioning inputs.\\r\\nThe geometric and semantic aspects of the other parts of the same image may\\r\\nalso be available partially or completely. Incorporating all such information in a\\r\\nmulti-conditional manner to generate the desired image, is the main challenge\\r\\nthat we undertake. In some sense, our approach bridges the gap between gener-\\r\\native and rendering based image synthesis.\\r\\n    Two major challenges of multi-conditional image generation, in the context\\r\\nof this paper, are the heterogeneity and the sparsity of the available labels. The\\r\\nheterogeneity refers to the difference in representation of different labels, for e.g.\\r\\ndepth and semantics. On the other hand, the sparsity is either simply caused by\\r\\nthe label definition (e.g. sky has no normal) or due to missing annotations [42]. It\\r\\nis important to note that some geometric aspects of images, such as an object’s\\r\\ndepth and orientation, can be introduced manually (e.g. by sketching), without\\r\\nrequiring any 3D model with its pose. This allows users to geometrically control\\r\\nimages (beyond the semantics based control) both in the the presence or absence\\r\\nof 3D models. It goes without saying that the geometric manipulation of any\\r\\nimage can be carried out by first inferring its geometric attributes using existing\\r\\nmethods [42,49,61], followed by generation after manipulation.\\r\\n    To address the problems of both heterogeneity and diversity, we propose a\\r\\nlabel merging network that learns to merge the provided conditioning labels\\r\\npixel-wise. To this end, we introduce a novel transformer-based architecture,\\r\\nthat is designed to operate on each pixel individually. The provided labels are\\r\\nfirst processed by label-specific multilayer perceptrons (MLPs) to generate a to-\\r\\nken for each label. The tokens are then processed by the transformer module.\\r\\nIn contrast to popular vision transformers that perform spatial attention [9,62],\\r\\nour transformer module applies self-attention across the label dimension, thus\\r\\navoiding the high computational complexity. The pixel-wise interaction of avail-\\r\\nable labels homogenizes the different labels to a common representation for the\\r\\noutput tokens. The output tokes are then averaged to obtain the fused labels in\\r\\nthe homogeneous space to form the local concept. This is performed efficiently\\r\\nfor all pixels in parallel by sliding the pixel-wise transformer over the input label\\r\\nmaps. Finally, the concepts are used for the image generation via conditional\\r\\ngenerative adversarial training, using a state-of-the-art method [47]. During the\\r\\nprocess of label merging, the spatial alignment is always preserved. The sparsity\\r\\nof the labels is handled by simply dropping the input tokens of the missing la-\\r\\nbels, at the corresponding pixel locations. This way, the transformer learns to\\r\\nreconstruct the concept for each pixel, also in the case when not all labels are\\r\\navailable.\\r\\n\\x0c\\n\\n                                   Spatially Multi-conditional Image Generation       3\\r\\n\\r\\n                    Input labels                 Ours               Generated image\\r\\n\\r\\n\\r\\n                                      Label\\r\\n                                                        Generator\\r\\n                                     merging\\r\\n\\r\\n\\r\\n\\r\\n                    Input label                Standard             Generated image\\r\\n\\r\\n\\r\\n                                               Generator\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 1. Spatially multi-conditional image generation. Our model uses multiple\\r\\nlabels to generate an image, compared to standard approaches which use only the\\r\\nsemantic segmentation label. Multiple input labels, coming from different sources, are\\r\\nhandled by the proposed label merging block.\\r\\n\\r\\n\\r\\n    We study the influence of several spatially conditioning labels including se-\\r\\nmantics, depth, normal, curvature, edges, in three different benchmark datasets.\\r\\nThe influence of the labels is studied both the case of sparse and dense label\\r\\navailability. In both cases, the proposed method provides outstanding results by\\r\\nclearly demonstrating the benefit of conditioning labels beyond the commonly\\r\\nused image semantics. The major contribution of this paper can be summarized\\r\\nas follow:\\r\\n\\r\\n 1. We study the problem of spatially multi-conditional image generation, for\\r\\n    the first time.\\r\\n 2. We propose a novel neural network architecture to fuse the heterogeneous\\r\\n    multi-conditioning labels provided for the task at hand, while also handling\\r\\n    the sparsity of the labels at the same time.\\r\\n 3. We analyse the utility of various conditioning types for image generation and\\r\\n    present outstanding results obtained by the proposed method in benchmark\\r\\n    datasets.\\r\\n\\r\\n\\r\\n\\r\\n2    Related Work\\r\\n\\r\\nConditional Image Synthesis. A description of the desired image to be gener-\\r\\nated can be provided in various forms, from class conditions [35,3], text [34,45],\\r\\nspatially localized semantic classes [21,39,47], sketches [21], style information\\r\\n[13,23,24] to human poses [33]. Recently, the handling of different data structures\\r\\n(e.g. text sequences and images) has received attention in the literature [64,59].\\r\\nThe problem of spatially multi-conditional image generation is orthogonal to the\\r\\nproblem of unifying different (non-spatial) modalities, as it seeks to fuse heteroge-\\r\\nneous spatially localized labels into concepts, while preserving the spatial layout\\r\\nfor image generation. In the area of image-to-image translation, Ronneberger\\r\\n\\x0c\\n\\n4       R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2. Pixel-wise Transformer Label Merging block (TLAM). The heteroge-\\r\\nneous labels of each pixel are first projected into the same dimensionality, and then\\r\\npassed to the concept generation block. A transformer module promotes the interaction\\r\\nbetween labels, before finally distilling them to a concept representation by averaging\\r\\nthe homogeneous label embeddings at every pixel location.\\r\\n\\r\\n\\r\\n\\r\\net al. first introduced the UNet [46] architecture, which has been used in sev-\\r\\neral important follow up works. Isola et al. [21] later introduced the Pix2Pix\\r\\nparadigm that leveraged the UNet backbone for a generator and combined it\\r\\nwith a convolutional discriminator, to convert sketches into photo-realistic im-\\r\\nages. This work was improved by Wang et al. to support high resolution image\\r\\ntranslation in Pix2PixHD [57] and video translation in Vid2Vid [56]. Recently,\\r\\nShaham et al. introduced the ASAP-Net [47] which achieves a superior trade-off\\r\\nof inference time and performance on several image translation tasks. We employ\\r\\nthe ASAP-Net as one component in our architecture.\\r\\nConditioning Mechanisms. The conditioning mechanism is at the core of\\r\\nsemantically controllable neural networks, and is often realized in conjunction\\r\\nwith normalization techniques [10,19]. Perez et al. introduced a simple feature-\\r\\nwise linear modulation FiLM [41] for visual reasoning. In the context of neu-\\r\\nral style transfer [12], Huang et al. introduced Adaptive Instance Normalization\\r\\nAdaIN [20]. Park et al. extended AdaIN for spatial control in SPADE [39], where\\r\\nthe normalization parameters are derived from a semantic segmentation. Zhu et\\r\\nal. [65] further extend SPADE to allow for independent application of global\\r\\nand local styles. Finally, generic normalisation schemes that make use of ker-\\r\\nnel prediction networks to achieve arbitrarily global and local control include\\r\\nDynamic Instance Normalization (DIN) [22] and Adaptive Convolutions (Ada-\\r\\nConv) [5]. While being greatly flexible, they also increase the resulting inference\\r\\ntime, unlike ASAP-Net that uses adaptive implicit functions [48] for efficiency.\\r\\nDifferentiable Rendering Methods. Since rendering is a complex and com-\\r\\nputationally expensive process, several approximations were proposed to facil-\\r\\nitate its use for training neural networks. Methods starting from simple ap-\\r\\nproximations of the rasterization function to produce silhouettes [26,31,28] to\\r\\nmore complex approximations modelling indirect lighting effects [29,37,32] have\\r\\nbeen proposed. These algorithms have also been incorporated into popular deep\\r\\nlearning frameworks [44,29,38]. Differentiable renderers have been successfully\\r\\n\\x0c\\n\\n                                                                     Spatially Multi-conditional Image Generation                                                                       5\\r\\n\\r\\nused in several neural networks including those used for face [52,51] and hu-\\r\\nman body [30,40] reconstruction. We refer the interested reader to the excellent\\r\\nsurveys of Tewari et al. [50] and Kato et al. [25] for more details. In contrast\\r\\nto rendering approaches which require setting numerous scene parameters, our\\r\\nmethod directly generates realistic images from only a sparse set of chosen labels.\\r\\n\\r\\n\\r\\n3         Method\\r\\nWe start by introducing a few formal notations. As an input, we have a collection\\r\\nof labels X = {X1 , X2 , ..., XN }, where each label Xk ∈ RH×W ×Ck has height H,\\r\\nwidth W and Ck channels. The element of the label Xk corresponding to the\\r\\npixel location (i, j) is denoted as xij k ∈ R\\r\\n                                              Ck\\r\\n                                                 . Hence, elements of the label set\\r\\nX corresponding to the pixel location (i, j) form a set X ij = {xij       ij        ij\\r\\n                                                                     1 , x2 , ..., xN }.\\r\\nThe model takes the set of labels X as an input and produces I = ϕ(X ), where\\r\\nI ∈ RH×W ×3 is the generated image.\\r\\n\\r\\n3.1           Label Merging\\r\\nWe describe the mechanism of the label merging component, as illustrated in\\r\\nFigure 2. This component processes different spatial locations of the input la-\\r\\nbels X ij independently, and is efficiently performed on all pixels in parallel. We\\r\\nempirically found this to be sufficient. To this end, we first collapse the spatial\\r\\ndimensions of every label Xk to obtain Ek ∈ RHW ×d , which contains HW embed-\\r\\nding vectors eij     d\\r\\n               k ∈ R . In this process, we are interested to merge all embedding\\r\\n           ij\\r\\nvectors {ek } into a common concept vector zij , for each pixel location. The la-\\r\\nbel merging is performed by two blocks: the projection block and the concept\\r\\ngeneration block, which we described in the following.\\r\\n\\r\\nProjection Block This block projects every heterogeneous input label Xk into\\r\\nan embedding space Ek ∈ RH×W ×d , where d is the dimensionality of the embed-\\r\\nding space. It does so by transforming every token xij                                                                                                      k with the projection func-\\r\\ntion fk , to project it into the embedding space eij                                                                                    k         =           fk (xij ), where eij   d\\r\\n                                                                                                                                                                                k ∈ R . All\\r\\nthe elements of a given label Xk share the same projection function fk . Different\\r\\ninput labels have different projection functions fk . We use an affine transfor-\\r\\nmation followed by the GeLU nonlinearity a(·) [17] to serve as the embedding\\r\\nfunction,\\r\\n                              \\\\label {eq:projection_block} \\\\mathsf {e}_{k}^{ij} = f_k (\\\\mathsf {x}_{k}^{ij}) = a(\\\\mathsf {A}_{k}\\\\mathsf {x}_{k}^{ij} + \\\\mathsf {b}_k),                  (1)\\r\\nwhere Ak ∈ Rd×Ck and bk ∈ Rd . We implement this by using fully connected\\r\\nlayers as projection functions for each input label Xk , followed by the GeLU\\r\\nactivation function. Our method also works with sparse input labels, which can\\r\\nbe caused by label definition or missing labels. When the value of a label Xk is\\r\\nmissing at a certain spatial location, its element token xij\\r\\n                                                          k is dropped by setting\\r\\nit to a zero vector. Thus, a token with the embedding eij   k = a(bk ) will send a\\r\\nsignal to the concept generation block that the label k is not present at this\\r\\n\\x0c\\n\\n6       R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\nlocation, and that it should extract the information from other labels. Also, the\\r\\nabsence of the whole label Xk is handled similarly.\\r\\n\\r\\nConcept Generation Block The collection of the embedding vectors corre-\\r\\nsponding to different labels E = {e1 , .., ek , ..., eN } serve as input tokens for our\\r\\nconcept generation block. This block uses a novel attention mechanism to model\\r\\nthe interaction across labels, which is shared across all spatial locations. It there-\\r\\nfore does not require expensive spatial attention as used in vision transformer\\r\\narchitectures [53,9]. In other words, we apply the same label-transformer on each\\r\\npixel individually. Transformers are naturally suited here to encourage interac-\\r\\ntion between the embedded label tokens to share their label specific information\\r\\nwhile merging it into the final label representation. Before giving E to the trans-\\r\\nformer, we add label specific encodings pk to the embedded tokens ek to obtain\\r\\n (0)                                        (0) (0)         (0)\\r\\nzk = ek + pk . Then we pass Z (0) = {z1 , z2 , ..., zN } through l transformer\\r\\nblocks Bm . Each block Bm takes the output Z (m−1) of the previous block and\\r\\n                                                        (m) (m)       (m)\\r\\nproduces Z (m) = Bm (Z (m−1) ), where Z (m) = {z1 , z2 , . . . , zN }.\\r\\n     Each transformer block Bm is composed of a Multi-head Self Attention block\\r\\n(MSA), followed by a Multilayer Perceptron block (MLP) [53,9]. Self-attention\\r\\nis a global operation, where every token interacts with every other token and\\r\\nthus information is shared across them. Multiple heads in the MSA block are\\r\\nused for more efficient computation and for extracting richer and more diverse\\r\\nfeatures. The MSA block executes the following operations:\\r\\n                         \\\\label {eq:MSA} \\\\mathcal {\\\\hat {Z}}^{(m)} = \\\\mathsf {MSA}(\\\\mathsf {LN}(\\\\mathcal {Z}^{(m-1)})) + \\\\mathcal {Z}^{(m-1)},            (2)\\r\\nwhere LN represents Layer Normalization [2]. The MSA block is followed by the\\r\\nMLP block, which processes each token separately using the same Multilayer\\r\\nPerceptron. This block further processes the token features, after their global\\r\\ninteractions in the MSA block, by sharing and refining the representations of each\\r\\ntoken across all their channels. The MLP block executes the following operations:\\r\\n\\r\\n                              \\\\label {eq:MLP} \\\\mathcal {Z}^{(m)} = \\\\mathsf {MLP}(\\\\mathsf {LN}(\\\\mathcal {\\\\hat {Z}}^{(m)})) + \\\\mathcal {\\\\hat {Z}}^{(m)}.    (3)\\r\\n   Note that, for vision transformers [9] operating on M spatial elements (e.g.\\r\\npixels/patches), the computational complexity of the self-attention is O(M 2 ).\\r\\nOur pixel-wise transformer, operating only on N label tokens, reduces the com-\\r\\nplexity of self-attention to O(N 2 ) (per pixel) with the number of labels N ≪ M.\\r\\n   Finally, all the elements of the output set produced by the final transformer\\r\\n               (l) (l)     (l)\\r\\nblock Z = {z1 , z2 , ..., zN } are averaged to obtain z = N1 k=1:N zk . They are\\r\\n                                                             P\\r\\nreshaped back into the original spatial dimensions to obtain the Concept Tensor\\r\\nZ ∈ RH×W ×d . We name this label merging block as the Transformer LAbel\\r\\nMerging (TLAM) block.\\r\\n\\r\\n3.2   Network Overview\\r\\nThe proposed model is divided into two components: the label merging compo-\\r\\nnent and the image generation component. This is depicted in Figure 3.\\r\\n\\x0c\\n\\n                                 Spatially Multi-conditional Image Generation          7\\r\\n\\r\\n               Input labels                          Generated image\\r\\n\\r\\n                               Label\\r\\n                                         Generator                     Discriminator\\r\\n                              merging\\r\\n\\r\\n\\r\\n\\r\\n              Real image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3. Network overview. Different input labels are embedded into a homogeneous\\r\\nspace with the label merging module. The generator uses this embedding to produce\\r\\nthe output image. During training, the discriminator takes the input labels, real and\\r\\ngenerated images, and uses them to optimize the label merging and generator modules.\\r\\n\\r\\n\\r\\nLabel Merging. The label merging block takes the set of heterogeneous labels\\r\\nX as the input and merges them into a homogeneous space Z = ψ(X ), where\\r\\nZ ∈ RH×W ×d is the Concept Tensor. It is important to note that the label\\r\\nmerging block does not require the number of input labels to be the same for\\r\\nevery input. Input labels X are heterogeneous because they come from different\\r\\nsources and can have different numbers of channels Ck , as well as different ranges\\r\\nof values. For example, semantic segmentation labels are represented using dis-\\r\\ncrete values, while surface normals are continuous. The label merging block first\\r\\ntranslates all the provided labels Xk into the same dimensional embeddings Ek ,\\r\\nusing the projection block. Then, it uses the concept generation block to trans-\\r\\nlate embeddings Ek to the concept tensor Z. In short, the label merging block\\r\\nfuses the heterogeneous set of input labels X into a homogeneous Concept Tensor\\r\\nZ. This block is depicted in Figure 2.\\r\\nGenerator. The task of the generator is to take the produced Concept Tensor\\r\\nZ and generate the image I = g(Z), as shown in Figure 3. As the generator,\\r\\nwe employ the state-of-the-art ASAP model [47]. This model first synthesizes\\r\\nthe high-resolution pixels using lightweight and highly parallelizable operators.\\r\\nASAP performs most computationally expensive image analysis at a very coarse\\r\\nresolution. We modify the input to ASAP, by providing the Concept Tensor Z\\r\\nas an input, instead of giving only one specific input labels Xk . The generator\\r\\ncan therefore exploit the merged information from all available input labels X .\\r\\nAdversarial Training for Multi-conditioning. We follow the optimization\\r\\nprotocol of ASAP-Net [47]. We train our generator model adversarially with a\\r\\nmulti-scale patch discriminator, as suggested by pix2pixHD [57]. To achieve this,\\r\\nwe modify the input to the discriminator by using the Concept Tensor Z instead\\r\\nof a specific label Xk , for example semantics used by the most existing works.\\r\\n\\r\\n\\r\\n4   Experiments\\r\\nImplementation Details. We follow the optimization protocol of ASAP-Net [47].\\r\\nWe train our generator model adversarially with a multi-scale patch discrimina-\\r\\n\\x0c\\n\\n8       R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n                                                           TLAM          Sparse-TLAM\\r\\n     All-dense   All-sparse SPADE [39] ASAP [47]\\r\\n                                                           (Ours)           (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 4. Visual comparisons on the Taskonomy dataset. Column 1 & 2: five\\r\\ndifferent labels tiled horizontally for dense and 50% sparse cases. Column 5 & 6: images\\r\\ngenerated by our method using the labels from column 1 & 2. Column 3 & 4: images\\r\\ngenerated by the dense semantic methods. Our method generates more realistic images\\r\\nwith fine geometric and visual details from both dense and sparse labels.\\r\\n\\r\\n\\r\\n                                                    Label sparsity\\r\\n                    Backbone         Method                        FID\\r\\n                                                    S E C D N\\r\\n                                     Regular                      72.3\\r\\n                   SPADE [39]\\r\\n                                  Naive-baseline                  66.1\\r\\n                                   Regular                       73.8\\r\\n                                Naive-baseline                   74.6\\r\\n                   ASAP [47] CLAM-baseline (Ours)                43.8\\r\\n                                TLAM (Ours)                      37.9\\r\\n                             CLAM-baseline (Ours)                37.1\\r\\n                                TLAM (Ours)                      30.6\\r\\nTable 1. FID scores on the Taskonomy dataset. Our TLAM method generates\\r\\nimages with significantly better visual quality for both sparse and dense labels. Symbol\\r\\n  corresponds to dense labels, while     corresponds to 50% label sparsity. S stands for\\r\\nsemantics, E for edges, C for curvature, D for depth and N for normals. Please, refer\\r\\nFigure 4 for corresponding images.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ntor, as suggested by pix2pixHD [57]. The training includes an adversarial hinge-\\r\\nloss, a perceptual loss and a discriminator feature matching loss. The learning\\r\\nrates for the generator and discriminator are 0.0001 and 0.0004, respectively. We\\r\\nuse the ADAM [27] solver with β1 = 0 and β2 = 0.999, following [39,47]. To gen-\\r\\nerate sparse labels, we look at spatial areas corresponding to distinct semantic\\r\\nsegmentation instances. For the sparsity of S, we randomly drop the labels with\\r\\nS% probability, independently for every label inside every area. Please turn to\\r\\nthe supplementary material for a detailed visualization of sparse labels.\\r\\n\\x0c\\n\\n                               Spatially Multi-conditional Image Generation       9\\r\\n\\r\\n4.1   Datasets\\r\\n\\r\\nWe conduct experiments on three different datasets to demonstrate the versatil-\\r\\nity and effectiveness of our approach.\\r\\nTaskonomy dataset [63] is a multi-label annotated dataset of indoor scenes.\\r\\nThe entire dataset consists of over 4 Million images from around 500 different\\r\\nbuildings with high resolution RGB images, segmentation masks and other la-\\r\\nbels. We chose this dataset for our main experiments because of its wide selection\\r\\nof both semantic and geometric labels. In our experiments, we use the following\\r\\nlabels: semantic segmentation, depth, surface normals, edges and curvature. We\\r\\nselected two buildings from the large dataset resulting a total of 18,246 images,\\r\\nsplit into 14,630/3,619 training/validation images.\\r\\nCityscapes dataset [8] contains images of urban street scenes from 50 different\\r\\ncities and their dense pixel annotations for 30 classes. The training and validation\\r\\nsplit contains 3000 and 500 samples respectively. To obtain further labels, we\\r\\nuse a state-of-the art depth estimation network [14] for depth. The estimated\\r\\ndepth, along with the camera intrinsics were used to compute the local patch-\\r\\nwise surface normals. Additionally, we use canny filters for edge detection. This\\r\\nresulted four labels for Cityscape.\\r\\nNYU depth v2 dataset [36] consists of 1449 densely labeled pairs of aligned\\r\\nRGB images and depth, normals, semantic segmentation and edges maps of\\r\\nindoor scenes. We split the data into 1200/249 training/validation sets.\\r\\n    The presented qualitative and quantitative results are generated on the re-\\r\\nspective hold-out test sets, with the exception of Figure 9, where we follow the\\r\\nprotocol of [57] for comparison with the other methods.\\r\\n\\r\\n\\r\\n4.2   Baselines and Metrics\\r\\n\\r\\nSince this is the first work for spatially multi-conditional image generation, we\\r\\nconstruct our own baselines.\\r\\nNaive-baseline takes all the available inputs and concatenates them along the\\r\\nchannel dimension to create the input, which is then fed as an input to the\\r\\nASAP-Net or SPADE backbone. This also serves as an ablation to study the\\r\\nefficacy of the concept generation component in TLAM.\\r\\nConvolutional Label Merging (CLAM-baseline) is another baseline that\\r\\nstacks multiple consecutive blocks similar to the projection block from Sec-\\r\\ntion 3.1. The first block is exactly the projection block (1), while the following\\r\\nl blocks perform the same operation with Alk ∈ Rd×d , blk ∈ Rd , preserving the\\r\\ndimensionality d. After the final block, all output elements corresponding to the\\r\\nsame spatial location are averaged to produce the Concept Tensor Z, just like\\r\\nat the end of the TLAM block.\\r\\nSotA semantic-only methods. We also compare compare our method with\\r\\nstate-of-the-art semantic image synthesis models. These methods include SPADE\\r\\n[39], ASAP-Net [47], CRN [6], SIMS [43], Pix2Pix [21] and Pix2PixHD [57].\\r\\nPerformance Metrics. We follow the evaluation protocol of previous methods\\r\\n[47,39]. We measure the quality of generated images using the Fréchet Inception\\r\\n\\x0c\\n\\n10     R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n                                                       Taskonomy dataset performance\\r\\n                                    73.8                         74.6\\r\\n                            72.3                                                                                                      Semantic\\r\\n                      70                                                                                                              Sparse\\r\\n                                                66.1\\r\\n                                                                                                                                      Dense\\r\\n                      60\\r\\n\\r\\n\\r\\n                      50\\r\\n                                                                                  43.8\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                FID\\r\\n                      40                                                                           37.9            37.1\\r\\n\\r\\n                                                                                                                               30.6\\r\\n                      30\\r\\n\\r\\n\\r\\n                      20\\r\\n\\r\\n\\r\\n                      10\\r\\n\\r\\n\\r\\n                       0\\r\\n                           SPADE   ASAP       SPADE            ASAP          CLAM-baseline       TLAM         CLAM-baseline   TLAM\\r\\n                                           Naive-baseline   Naive-baseline    sparse (ours)   sparse (ours)      (ours)       (ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 5. Model comparison. Our TLAM method preforms better compared to the\\r\\nestablished baselines as well as to the SotA models which use semantic segmentation\\r\\nlabels only.\\r\\n\\r\\n\\r\\nDistance (FID) [18], which compares the distribution between generated data\\r\\nand real data. The FID summarizes how similar two groups of images are in\\r\\nterms of visual feature statistics. The second metric is the segmentation score,\\r\\nobtained by evaluating mean-Intersection-over-Union (mIoU) and pixel accuracy\\r\\n(accu) of a semantic segmentation model applied to the generated images. We use\\r\\nstate-of-the-art semantic segmentation networks DRN-D-105 [60] for Cityscapes\\r\\nand DeepLabv3plus [4] for NYU depth v2 dataset.\\r\\n\\r\\n4.3   Quantitative Results\\r\\nImage Generation. Table 1 reports the FID scores obtained on the Taskonomy\\r\\ndataset. We compare our method with several baselines and with different label\\r\\nsparsity. Our TLAM significantly outpreforms SPADE and ASAP SotA meth-\\r\\nods, which use only semantic segmentation maps as inputs. This shows that\\r\\nusing different spatial input labels can indeed improve the generation quality.\\r\\nMoreover, when SPADE and ASAP merge multiple labels as an input, they do\\r\\nnot preform significantly better than when using just the semantics. This empha-\\r\\nsizes the difficulty of merging multiple spatial labels, which are heterogeneous in\\r\\nnature. Some labels represent semantic image properties, while others represent\\r\\ngeometric properties. Also, some labels are continuous, while others are discrete.\\r\\nFurthermore, our TLAM preforms better than the CLAM-baseline, showing the\\r\\nvalue of having a better label merging network to deal with the heterogeneity\\r\\npresent in the input labels. Finally, we compare TLAM and the CLAM-baseline\\r\\nwith dense and sparse labels. As expected, having dense labels achieves better\\r\\nimage quality. The visual quality when using 50% sparse labels is close to when\\r\\nusing dense labels. This is interesting and desirable, since in practical scenarios\\r\\none often ends up having sparse labels [42].\\r\\n    The results on Cityscapes and NYU are summarized in Tables 2a and 2b.\\r\\nOn the Cityscape dataset, we compare our method with the SotA methods and\\r\\nreport FID, and segmentation mIoU and accuracy. Our method achieves better\\r\\naccuracy compared to the other methods. As reported in [39] the SIMS model\\r\\nproduces a lower FID, but has poor segmentation accuracy on the Cityscapes.\\r\\nThis is because SIMS synthesizes an image by first stitching image patches from\\r\\nthe training dataset. On the NYU dataset, our method achieves better mIoU\\r\\n\\x0c\\n\\n                               Spatially Multi-conditional Image Generation      11\\r\\n\\r\\nand accuracy. Unfortunately, we are unable to report the FID, due to the small\\r\\nsize of only 249 images in the validation set.\\r\\nTraining Convergence. Figure 6a shows the evolution of FID during training\\r\\non Taskonomy. We evaluate the FID on the validation set every 20 epochs for\\r\\nTLAM, and the ASAP naive and CLAM baselines. One can observe that TLAM\\r\\nquickly achieves a very good FID, even after 20 epochs. At this instance, TLAM\\r\\nhas 2.5 and 1.3 times better FID than those of the naive and CLAM baseline,\\r\\nrespectively. We can also see that TLAM converges faster than the other models.\\r\\nOne training epoch in Figure 6a takes about 1.7hrs for our method on one\\r\\nGeForce GTX TITAN X GPU.\\r\\n\\r\\n\\r\\n4.4   Qualitative Results\\r\\n\\r\\nTo visually compare our TLAM label merging, we present qualitative results\\r\\nfor both dense and sparse labels on the Taskonomy dataset in Figure 4, to-\\r\\ngether with semantic-only baselines. TLAM with all-dense and all-sparse labels\\r\\ngenerates high-fidelity images. Our method is able to generate fine structural de-\\r\\ntails such as lights, decorations, and even mirror reflections clearly better than\\r\\nSPADE and ASAP. The results on the Cityscapes dataset are depicted in Fig-\\r\\nure 8. Notably, our method with sparse labels achieves similar visual quality as\\r\\nother methods. Figure 9 shows qualitative results on the NYU dataset, where\\r\\nPix2PixHD also renders images with good quality. However, Pix2PixHD fails to\\r\\ncapture the lighting conditions, in contrast to our method. Notably, our method\\r\\ncaptures the rich geometric structures (such as on the ceiling), thanks to the\\r\\ngeometric labels. Please, refer to our supplementary material for more visual\\r\\nresults. Overall, the qualitative results demonstrate the effectiveness of TLAM,\\r\\nwhich exploits novel pixel-wise label transformers, even for sparse labels.\\r\\n\\r\\n\\r\\n4.5   Label Sensitivity Study\\r\\n\\r\\nWe analyse the sensitivity of our method with regards to the provided labels on\\r\\nthe Taskonomy dataset.\\r\\nLabel Sparsity. Figure 6b shows how the FID is affected by label sparsity.\\r\\nExperiments conducted on increasing sparsity from 10% to 70%, show a steady\\r\\ndegradation of FID with less available labels. Note that our model already achieves\\r\\ngood FID using only 30% of available labels. The experiments were conducted\\r\\nusing a single model trained with 50% sparsity. It also shows the generalizability\\r\\nof our method across various levels of sparsity.\\r\\nRemoval of Labels. In Figure 6c, we plot the FID after removing each label\\r\\nfrom the input, using the TLAM model trained on Taskonomy with 50% sparsity.\\r\\nWe observe that among the five labels, edges play the most significant role. On\\r\\nthe other hand, the semantics and depth are the most dispensable. Nevertheless,\\r\\nthe removal of any label results into a worsening of the FID at least by a factor of\\r\\n1.3. This suggests that all labels provide different information crucial for image\\r\\ngeneration, while being mutually complimentary.\\r\\n\\x0c\\n\\n12                 R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n                FID convergence of different models                                        Effects of different label sparsity                                   Effects of dropping input labels\\r\\n                                              CLAM-baseline (ours)            55        Sparse labels\\r\\n                                              ASAP Naive-baseline                       All labels\\r\\n                                                                                                                                                80\\r\\n                                              TLAM (ours)\\r\\n      100\\r\\n                                                                              50\\r\\n\\r\\n                                                                                                                                                60\\r\\n       80                                                                     45\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                        FID\\r\\nFID\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                          FID\\r\\n                                                                                                                                                40\\r\\n       60                                                                     40\\r\\n\\r\\n\\r\\n\\r\\n                                                                              35                                                                20\\r\\n       40\\r\\n\\r\\n\\r\\n                                                                              30\\r\\n            0      20       40           60         80         100                 10     20            30      40      50   60     70           0\\r\\n                                                                                                                                                     Semantics      Edges     Normals    Depth      Curvature\\r\\n                                 Epoch                                                                  Label sparsity [%]                            dropped      dropped    dropped   dropped      dropped\\r\\n\\r\\n\\r\\n\\r\\n(a) Convergence        of                                               (b) Effects of differ-                                            (c) Effects of dropping\\r\\ndifferent models. We                                                    ent label sparsity. Our                                           input labels. We exam-\\r\\ncompare FID scores dur-                                                 model achieves good FID                                           ine how dropping a spe-\\r\\ning training of our TLAM                                                score even when a frac-                                           cific label affects the im-\\r\\nmethod to the Naive and                                                 tion of labels is pre-                                            age quality of our TLAM\\r\\nCLAM baselines. Our                                                     sented during inference.                                          model. Dropping any label\\r\\nmethod converges faster                                                 With more labels present,                                         degrades FID which leads\\r\\nand achieves better FID                                                 our model achieves better                                         to the conclusion that all\\r\\ncompared to those of the                                                performance, even though                                          labels provide useful infor-\\r\\nbaselines at every point                                                it was trained for 50% la-                                        mation for image synthe-\\r\\nduring training.                                                        bel sparsity.                                                     sis.\\r\\n\\r\\nFig. 6. Training convergence and label sensitivity. We analyse behavioural as-\\r\\npects of our models with regards to training and labels.\\r\\n\\r\\n       Concept                       Image                           Semantics                    Normals                         Edges                  Depth                    Curvature\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 7. Concept Tensor visualization. From left to right: Concept Tensor projected\\r\\nto RGB; original image; five different input labels.\\r\\n\\r\\n\\r\\n   We conclude that the proposed label merging block can successfully deal with\\r\\nincomplete labels and is able to exploit information from all available labels.\\r\\n\\r\\n4.6             Concept Visualization and Image Editing\\r\\nConcept Visualization. To visualize the concept tensor Z ∈ RH×W ×96 , we\\r\\nproject it to 3 channels, using Principal Component Analysis [11], and present\\r\\nit in Figure 7, along with the corresponding image and labels. One can observe\\r\\nthat the visualized concept tensor indeed resembles different aspects of the input\\r\\nlabels (e.g. edges and normal orientations).\\r\\nGeometric Image Editing with User Inputs. In order to demonstrate an\\r\\nintuitive application of our method, we perform image editing by inserting a new\\r\\nobject into the scene. Table 10 shows how our method can mimic rendering while\\r\\nallowing the geometric manipulation of an image. In this application, we render\\r\\n\\x0c\\n\\n                              Spatially Multi-conditional Image Generation     13\\r\\n\\r\\n                                                      TLAM      Sparse-TLAM\\r\\n      Label      Reference SPADE [39] ASAP [47]\\r\\n                                                      (Ours)       (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 8. Visual comparison on the Cityscapes. Our approach achieves a visual\\r\\nquality on par with the compared methods.\\r\\n\\r\\nReference Pix2pix [21] CRN [6] pix2pixHD [57] SPADE [39] ASAP [47] TLAM (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 9. Visual comparison on NYU. Our method generates images that better\\r\\ncapture the lighting and geometry with more details.\\r\\n\\r\\n\\r\\n\\r\\na table in the given image, by simply augmenting different labels to include label\\r\\ninformation derived from a 3D model. Our method is able to render the table\\r\\nrealistically within the image, while ASAP and SPADE perform unsatisfactorily.\\r\\nFigure 11 shows another example of inserting an object of interest into the\\r\\nimage, as well as one example of removing a certain object from the image by\\r\\naugmenting the labels.\\r\\n\\r\\n\\r\\n5   Conclusion\\r\\nIn this work, we offer a new perspective on image generation as inverse of image\\r\\nunderstanding. In the same way as image understanding involves the solving\\r\\nof multiple different tasks, we desire the control over the generation process to\\r\\ninclude multiple input labels of various kinds. To this end, we design a neural\\r\\nnetwork architecture that is capable of handling sparse and heterogeneous labels,\\r\\nby mapping them to a homogeneous concept space in a pixel-wise fashion. With\\r\\nour proposed module, we can equip spatially conditioned generators with the\\r\\ndesired properties. From our experiments on challenging datasets, we conclude\\r\\nthat the benefits and flexibility of this additional layer of control gives way to\\r\\nexciting results beyond the state-of-the-art.\\r\\n\\x0c\\n\\n14       R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n            Method mIoU Accuracy FID                    Method mIoU Accuracy\\r\\n            CRN [6] 52.4  77.1   104.7               SPADE [39] 33.1  47.4\\r\\n          SIMS [43] 47.2  75.5   49.7                 ASAP [47] 36.2  49.1\\r\\n        Pix2Pix [21] 39.5 78.3   80.7               TLAM (Ours) 38.3  53.1\\r\\n     Pix2PixHD [57] 58.3  81.4   95.0\\r\\n        SPADE [39] 62.3   81.9   71.8\\r\\n         ASAP [47] 44.9   78.6   72.5        (b) NYU dataset. Our method demon-\\r\\n      TLAM (Ours) 45.5    85.3   68.3        strates effective of multi-conditioning\\r\\n(a) Cityscapes dataset. Our method is        with ASAP. Note that SPADE and ASAP\\r\\nusing the ASAP backbone.                     are the SotA in semantic conditioning.\\r\\n\\r\\n Table 2. Quantitative results. Our method enables successful multi-conditioning.\\r\\n\\r\\n\\r\\n     Original    Curvature     Normals         SPADE           ASAP          TLAM\\r\\n      image     user-provided mask by user       [39]           [47]         (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 10. Geometric image editing with user inputs. From left to right: original\\r\\nimage; curvature label (provided among five) of the table to be inserted into the image;\\r\\nsemantics of the table on the normal map of the scene; generated image using SPADE,\\r\\nASAP and our method, respectively. During editing, we use dense labels of the original\\r\\nimage, where we introduce five labels derived from a texture-less 3D model of a table\\r\\n(object of interest)– labels are first edited to introduce the object, followed by the\\r\\nimage generation using the proposed TLAM method.\\r\\n\\r\\n           Original       Mask       SPADE [39]      ASAP [47]    TLAM (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 11. Object removal and insertion. From left to right: original image; mask of\\r\\nthe inserted/removed object; generated image using SPADE, ASAP and our method,\\r\\nrespectively. Top row shows removal of chairs from the image, by removing their labels.\\r\\nBottom row shows insertion of a TV, by inserting the labels provided by the user. The\\r\\nmask for removal/insertion is manually chosen by the user. Note that our TLAM\\r\\ngenerates consistent images, unlike the compared methods.\\r\\n\\x0c\\n\\n    Spatially Multi-conditional Image Generation\\r\\n               Supplementary material\\r\\n\\r\\n          Ritika Chakraborty⋆1 Nikola Popovic⋆1 , Danda Pani Paudel1 ,\\r\\n                       Thomas Probst1 , Luc Van Gool1,2\\r\\n               1\\r\\n                Computer Vision Laboratory, ETH Zurich, Switzerland\\r\\n                    2\\r\\n                      VISICS, ESAT/PSI, KU Leuven, Belgium\\r\\n       {critika, nipopovic, paudel, probstt, vangool}@vision.ee.ethz.ch\\r\\n\\r\\n    This document provides additional details, which complement the main pa-\\r\\nper. We provide the complete network diagram of our generator in Section A.\\r\\nMore implementation details are contained in in Section B. The visualization\\r\\nof input labels under different sparsity can be found in Section C. Additional\\r\\nqualitative results are presented in Section D. Finally, a discussion about ethical\\r\\nand societal impacts is contained in Section E. Also, please refer to our video\\r\\nsupplementary for the example of geometric editing.\\r\\n\\r\\n\\r\\nA      Generator Overview\\r\\n\\r\\nWhole Network. The network diagram of our generator is presented in Fig-\\r\\nure 1. The figure shows how our proposed Label Merging Block TLAM is con-\\r\\nnected to the ASAP-Net [47] generator, which we use as the backbone.\\r\\nASAP-Net Generator. The generator takes the Concept Tensor Z ∈ RH×W ×d\\r\\nas an input, similar to taking the semantic labels in the original design. It then\\r\\noutputs a tensor of weights, which parameterize the pixelwise spatially-varying\\r\\nmulti-layer perceptrons (MLPs). The MLPs, with infered weights, compute the\\r\\nfinal output image by taking the Concept Tensor Z as their input.\\r\\n\\r\\n\\r\\nB      Implementation Details\\r\\n\\r\\nProjection Block. The Projection Block projects each input label into an\\r\\nembedding space with the same dimensionality. Every input label is processed\\r\\nby one 1x1 convolution layer, followed by a nonlinear activation. The projection\\r\\nis a 96-dimensional tensor for each label.\\r\\nConcept generation Block. The Concept Generation Block takes the pro-\\r\\njected tensors as input, and operates over them in a pixel-wise fashion. For each\\r\\npixel, we thus have an embedding vector for each task. We add a label specific-\\r\\nencoding to the embedding vector of each label, as a way of signalizing which\\r\\ntask the embedding corresponds to. For every pixel, the concept generation block\\r\\nprocesses each set of encoded labels with a transformer. The transformer consists\\r\\nof 3 layers, where each uses 3 heads.\\r\\n⋆\\r\\n    Equal contributions.\\r\\n\\x0c\\n\\n16      R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n\\r\\n                                                       ASAP-Net Backbone\\r\\n                                                                                  Generated Image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            p\\r\\n                                                                           MPL at p\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 1. Label Merging TLAM block and ASAP-Net Backbone. The input labels\\r\\nare processed by the label merging network to output the Concept Tensor. The ASAP\\r\\ngenerator takes the Concept Tensor Z ∈ RH×W ×d as an input and outputs a tensor\\r\\nof weights. Those weights are parameters of pixelwise, spatially-varying, MLPs, which\\r\\ncompute the final output image from the Concept Tensor Z.\\r\\n\\r\\n\\r\\nC    Visualization of input Labels\\r\\nIn Figure 2 we show examples of the input labels from the Taskonomy dataset.\\r\\nThese labels include semantic segmentation, normals, depth, edges and curva-\\r\\nture. Furthermore, in Figure 3, 4 and 5 we show examples of labels from the\\r\\nTaskonomy dataset, with 70%, 50% and 30% label sparsity. To generate sparse\\r\\nlabels, we look at spatial areas corresponding to distinct semantic segmentation\\r\\ninstances. For the sparsity of S, we randomly drop the labels with S% probabil-\\r\\nity, independently for every label inside every area. Higher sparsity means there\\r\\nis a higher probability that a semantic region will be masked out for all labels.\\r\\n\\r\\n\\r\\nD    Additional Qualitative Results\\r\\nIn Figure 6 we show images synthesized with our TLAM model using dense\\r\\ninput labels (semantics, curvature, edges, normals and depth), on the Taskonomy\\r\\ndataset. In Figure 7 we show images synthesized with our TLAM model using\\r\\nsparse input labels (50% sparsity), on the Taskonomy dataset. Finally, Figure 8\\r\\nshows additional visual comparison on the Cityscapes dataset, where we compare\\r\\nour TLAM and Sparse-TLAM methods with SPADE [39] and ASAP-Net [47].\\r\\n\\r\\n\\r\\nE    Ethical and Societal Impact\\r\\nThis work is going one step further into the image generation. While bringing\\r\\ngreat potential artistic value to the general public, such technology can be mis-\\r\\nused for fraudulent purposes. Despite the generated images looking realistic, this\\r\\nissue can be partially mitigated by learning-based fake detection [16,55,54].\\r\\n    In regards to limitations, our method is not designed with a particular focus\\r\\non balanced representation of appearance and labels. Therefore, image genera-\\r\\ntion may behave unexpected on certain conditions, groups of objects or people.\\r\\nWe recommend application-specific strategies for data collection and training to\\r\\nensure the desired outcome [1,58].\\r\\n\\x0c\\n\\n                                   Spatially Multi-conditional Image Generation   17\\r\\n\\r\\n         Image         Semseg      Normals      Depth      Edges    Curvature\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2. Examples of dense input labels. Samples of the Taskonomy dataset with\\r\\nall input labels visualized.\\r\\n\\r\\n                                             Image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                  Semseg        Normals       Depth     Edges      Curvature\\r\\n       sparsity\\r\\n        70 %\\r\\n       sparsity\\r\\n        50 %\\r\\n       sparsity\\r\\n        30 %\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3. One example of sparse input labels. One sample from the Taskonomy\\r\\ndataset where input labels have the sparsity of 70%, 50% and 30%.\\r\\n\\x0c\\n\\n18    R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n\\r\\n                                    Image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                 Semseg   Normals     Depth        Edges      Curvature\\r\\n      sparsity\\r\\n       70 %\\r\\n      sparsity\\r\\n       50 %\\r\\n      sparsity\\r\\n       30 %\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 4. One example of sparse input labels. One sample from the Taskonomy\\r\\ndataset where input labels have the sparsity of 70%, 50% and 30%.\\r\\n\\r\\n                                    Image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                 Semseg   Normals     Depth        Edges      Curvature\\r\\n      sparsity\\r\\n       70 %\\r\\n      sparsity\\r\\n       50 %\\r\\n      sparsity\\r\\n       30 %\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 5. One example of sparse input labels. One sample from the Taskonomy\\r\\ndataset where input labels have the sparsity of 70%, 50% and 30%.\\r\\n\\x0c\\n\\n                             Spatially Multi-conditional Image Generation   19\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 6. Images generated using dense input labels. Here we see images generated\\r\\nwith our proposed TLAM label merging model, using dense input labels from the\\r\\nTaskonomy dataset.\\r\\n\\x0c\\n\\n20     R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 7. Images generated using 50% sparse input labels. Here we see images\\r\\ngenerated with our proposed TLAM label merging model, using input labels from the\\r\\nTaskonomy dataset with 50% sparsity.\\r\\n\\x0c\\n\\n                            Spatially Multi-conditional Image Generation     21\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                           SPADE        ASAP         TLAM      Sparse-TLAM\\r\\n     Label      Image\\r\\n                             [39]        [47]        (Ours)       (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 8. Image generation of different methods on Cityscapes. Here we compare\\r\\ngenerated results of our methods TLAM and Sparse-TLAM to SPADE and ASAP-Net.\\r\\n\\x0c\\n\\n22      R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\nReferences\\r\\n\\r\\n 1. Alvi, M.S., Zisserman, A., Nellåker, C.: Turning a blind eye: Explicit removal of\\r\\n    biases and variation from deep neural network embeddings. In: ECCV Workshops\\r\\n    (2018)\\r\\n 2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization (2016)\\r\\n 3. Brock, A., Donahue, J., Simonyan, K.: Large scale gan training for high fidelity\\r\\n    natural image synthesis. arXiv preprint arXiv:1809.11096 (2018)\\r\\n 4. Cao, J., Leng, H., Lischinski, D., Cohen-Or, D., Tu, C., Li, Y.: Shapeconv: Shape-\\r\\n    aware convolutional layer for indoor rgb-d semantic segmentation. arXiv preprint\\r\\n    arXiv:2108.10528 (2021)\\r\\n 5. Chandran, P., Zoss, G., Gotardo, P., Gross, M., Bradley, D.: Adaptive convolutions\\r\\n    for structure-aware style transfer. In: Proceedings of the IEEE/CVF Conference\\r\\n    on Computer Vision and Pattern Recognition (CVPR). pp. 7972–7981 (June 2021)\\r\\n 6. Chen, Q., Koltun, V.: Photographic image synthesis with cascaded refinement\\r\\n    networks. pp. 1520–1529 (10 2017). https://doi.org/10.1109/ICCV.2017.168\\r\\n 7. Cook, R.L., Carpenter, L., Catmull, E.: The reyes image rendering architecture.\\r\\n    ACM SIGGRAPH Computer Graphics 21(4), 95–102 (1987)\\r\\n 8. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\\r\\n    Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\\r\\n    understanding. In: Proceedings of the IEEE Conference on Computer Vision and\\r\\n    Pattern Recognition (CVPR) (June 2016)\\r\\n 9. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\\r\\n    T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\\r\\n    An image is worth 16x16 words: Transformers for image recognition at scale. ICLR\\r\\n    (2021)\\r\\n10. Dumoulin, V., Perez, E., Schucher, N., Strub, F., Vries, H.d.,\\r\\n    Courville, A., Bengio, Y.: Feature-wise transformations. Distill (2018).\\r\\n    https://doi.org/10.23915/distill.00011\\r\\n11. F.R.S., K.P.: Liii. on lines and planes of closest fit to systems of points in space.\\r\\n    Philosophical Magazine Series 1 2, 559–572\\r\\n12. Gatys, L.A., Ecker, A.S., Bethge, M.: A neural algorithm of artistic style (2015)\\r\\n13. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neu-\\r\\n    ral networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition\\r\\n    (CVPR) pp. 2414–2423 (2016)\\r\\n14. Godard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocular depth esti-\\r\\n    mation with left-right consistency. In: CVPR (2017)\\r\\n15. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\\r\\n    Courville, A., Bengio, Y.: Generative adversarial nets. Advances in neural infor-\\r\\n    mation processing systems 27 (2014)\\r\\n16. Guarnera, L., Giudice, O., Battiato, S.: Deepfake detection by analyzing convolu-\\r\\n    tional traces. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recog-\\r\\n    nition Workshops (CVPRW) pp. 2841–2850 (2020)\\r\\n17. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus) (2020)\\r\\n18. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained\\r\\n    by a two time-scale update rule converge to a local nash equilibrium. In: NIPS\\r\\n    (2017)\\r\\n19. Huang, L., Qin, J., Zhou, Y., Zhu, F., Liu, L., Shao, L.: Normalization techniques\\r\\n    in training dnns: Methodology, analysis and application (2020)\\r\\n\\x0c\\n\\n                                  Spatially Multi-conditional Image Generation           23\\r\\n\\r\\n20. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance\\r\\n    normalization. In: Proceedings of the IEEE International Conference on Computer\\r\\n    Vision (ICCV) (Oct 2017)\\r\\n21. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-\\r\\n    tional adversarial networks. CVPR (2017)\\r\\n22. Jing, Y., Liu, X., Ding, Y., Wang, X., Ding, E., Song, M., Wen, S.: Dynamic\\r\\n    instance normalization for arbitrary style transfer (2019)\\r\\n23. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and\\r\\n    super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) Computer\\r\\n    Vision – ECCV 2016. pp. 694–711. Springer International Publishing, Cham (2016)\\r\\n24. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative\\r\\n    adversarial networks (2019)\\r\\n25. Kato, H., Beker, D., Morariu, M., Ando, T., Matsuoka, T., Kehl, W., Gaidon, A.:\\r\\n    Differentiable rendering: A survey (2020)\\r\\n26. Kato, H., Ushiku, Y., Harada, T.: Neural 3d mesh renderer. In: The IEEE Confer-\\r\\n    ence on Computer Vision and Pattern Recognition (CVPR) (2018)\\r\\n27. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR\\r\\n    abs/1412.6980 (2015)\\r\\n28. Laine, S., Hellsten, J., Karras, T., Seol, Y., Lehtinen, J., Aila, T.: Modular primi-\\r\\n    tives for high-performance differentiable rendering. ACM Transactions on Graphics\\r\\n    39(6) (2020)\\r\\n29. Li, T.M., Aittala, M., Durand, F., Lehtinen, J.: Differentiable monte carlo ray\\r\\n    tracing through edge sampling. ACM Trans. Graph. (Proc. SIGGRAPH Asia)\\r\\n    37(6), 222:1–222:11 (2018)\\r\\n30. Lin, K., Wang, L., Liu, Z.: End-to-end human pose and mesh reconstruction with\\r\\n    transformers (2021)\\r\\n31. Liu, S., Li, T., Chen, W., Li, H.: Soft rasterizer: A differentiable renderer for image-\\r\\n    based 3d reasoning (2019)\\r\\n32. Loubet, G., Holzschuch, N., Jakob, W.: Reparameterizing discontinuous integrands\\r\\n    for differentiable rendering. Transactions on Graphics (Proceedings of SIGGRAPH\\r\\n    Asia) 38(6) (Dec 2019). https://doi.org/10.1145/3355089.3356510\\r\\n33. Ma, L., Jia, X., Sun, Q., Schiele, B., Tuytelaars, T., Gool, L.V.: Pose guided person\\r\\n    image generation. In: NIPS (2017)\\r\\n34. Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint\\r\\n    arXiv:1411.1784 (2014)\\r\\n35. Mirza, M., Osindero, S.: Conditional generative adversarial nets. ArXiv\\r\\n    abs/1411.1784 (2014)\\r\\n36. Nathan Silberman, Derek Hoiem, P.K., Fergus, R.: Indoor segmentation and sup-\\r\\n    port inference from rgbd images. In: ECCV (2012)\\r\\n37. Nimier-David, M., Speierer, S., Ruiz, B., Jakob, W.: Radiative back-\\r\\n    propagation: An adjoint method for lightning-fast differentiable rendering.\\r\\n    Transactions on Graphics (Proceedings of SIGGRAPH) 39(4) (Jul 2020).\\r\\n    https://doi.org/10.1145/3386569.3392406\\r\\n38. Nimier-David, M., Vicini, D., Zeltner, T., Jakob, W.: Mitsuba 2: A retargetable for-\\r\\n    ward and inverse renderer. Transactions on Graphics (Proceedings of SIGGRAPH\\r\\n    Asia) 38(6) (Dec 2019). https://doi.org/10.1145/3355089.3356498\\r\\n39. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with\\r\\n    spatially-adaptive normalization (2019)\\r\\n40. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas,\\r\\n    D., Black, M.J.: Expressive body capture: 3d hands, face, and body from a single\\r\\n    image (2019)\\r\\n\\x0c\\n\\n24      R. Chakraborty⋆ , N. Popovic⋆ , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n41. Perez, E., Strub, F., de Vries, H., Dumoulin, V., Courville, A.: Film: Visual rea-\\r\\n    soning with a general conditioning layer (2017)\\r\\n42. Popovic, N., Paudel, D.P., Probst, T., Sun, G., Van Gool, L.: Compositetask-\\r\\n    ing: Understanding images by spatial composition of tasks. In: Proceedings of the\\r\\n    IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6870–\\r\\n    6880 (2021)\\r\\n43. Qi, X., Chen, Q., Jia, J., Koltun, V.: Semi-parametric image synthesis. pp. 8808–\\r\\n    8816 (06 2018). https://doi.org/10.1109/CVPR.2018.00918\\r\\n44. Ravi, N., Reizenstein, J., Novotny, D., Gordon, T., Lo, W.Y., Johnson, J., Gkioxari,\\r\\n    G.: Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501 (2020)\\r\\n45. Reddy, M.D.M., Basha, M.S.M., Hari, M.M.C., Penchalaiah, M.N.: Dall-e: Creating\\r\\n    images from text\\r\\n46. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\\r\\n    ical image segmentation (2015)\\r\\n47. Rott Shaham, T., Gharbi, M., Zhang, R., Shechtman, E., Michaeli, T.: Spatially-\\r\\n    adaptive pixelwise networks for fast image translation. In: Computer Vision and\\r\\n    Pattern Recognition (CVPR) (2021)\\r\\n48. Sitzmann, V., Zollhöfer, M., Wetzstein, G.: Scene representation networks:\\r\\n    Continuous 3d-structure-aware neural scene representations. arXiv preprint\\r\\n    arXiv:1906.01618 (2019)\\r\\n49. Sun, G., Probst, T., Paudel, D.P., Popovic, N., Kanakis, M., Patel, J., Dai, D.,\\r\\n    Van Gool, L.: Task switching network for multi-task learning. In: Proceedings\\r\\n    of the IEEE/CVF International Conference on Computer Vision. pp. 8291–8300\\r\\n    (2021)\\r\\n50. Tewari, A., Fried, O., Thies, J., Sitzmann, V., Lombardi, S., Sunkavalli, K., Martin-\\r\\n    Brualla, R., Simon, T., Saragih, J., Nießner, M., Pandey, R., Fanello, S., Wet-\\r\\n    zstein, G., Zhu, J.Y., Theobalt, C., Agrawala, M., Shechtman, E., Goldman, D.B.,\\r\\n    Zollhöfer, M.: State of the art on neural rendering (2020)\\r\\n51. Tewari, A., Zollöfer, M., Bernard, F., Garrido, P., Kim, H., Perez, P., Theobalt, C.:\\r\\n    High-fidelity monocular face reconstruction based on an unsupervised model-based\\r\\n    face autoencoder. IEEE Transactions on Pattern Analysis and Machine Intelligence\\r\\n    pp. 1–1 (2018). https://doi.org/10.1109/TPAMI.2018.2876842\\r\\n52. Tewari, A., Zollöfer, M., Kim, H., Garrido, P., Bernard, F., Perez, P., Christian,\\r\\n    T.: MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised\\r\\n    Monocular Reconstruction. In: The IEEE International Conference on Computer\\r\\n    Vision (ICCV) (2017)\\r\\n53. Vaswani, A., Shazeer, N.M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\\r\\n    Kaiser, L., Polosukhin, I.: Attention is all you need. ArXiv abs/1706.03762 (2017)\\r\\n54. Verdoliva, L.: Media forensics and deepfakes: An overview. IEEE Journal of Se-\\r\\n    lected Topics in Signal Processing 14, 910–932 (2020)\\r\\n55. Wang, S., Wang, O., Zhang, R., Owens, A., Efros, A.A.: Cnn-generated images are\\r\\n    surprisingly easy to spot. . . for now. 2020 IEEE/CVF Conference on Computer\\r\\n    Vision and Pattern Recognition (CVPR) pp. 8692–8701 (2020)\\r\\n56. Wang, T.C., Liu, M.Y., Zhu, J.Y., Liu, G., Tao, A., Kautz, J., Catanzaro, B.:\\r\\n    Video-to-video synthesis. In: Advances in Neural Information Processing Systems\\r\\n    (NeurIPS) (2018)\\r\\n57. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-\\r\\n    resolution image synthesis and semantic manipulation with conditional gans. In:\\r\\n    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\r\\n    (2018)\\r\\n\\x0c\\n\\n                                Spatially Multi-conditional Image Generation        25\\r\\n\\r\\n58. Wang, Z., Qinami, K., Karakozis, Y., Genova, K., Nair, P.Q., Hata, K., Rus-\\r\\n    sakovsky, O.: Towards fairness in visual recognition: Effective strategies for bias\\r\\n    mitigation. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recog-\\r\\n    nition (CVPR) pp. 8916–8925 (2020)\\r\\n59. Xia, W., Yang, Y., Xue, J., Wu, B.: Tedigan: Text-guided diverse face image gen-\\r\\n    eration and manipulation. 2021 IEEE/CVF Conference on Computer Vision and\\r\\n    Pattern Recognition (CVPR) pp. 2256–2265 (2021)\\r\\n60. Yu, F., Koltun, V., Funkhouser, T.: Dilated residual networks. In: Computer Vision\\r\\n    and Pattern Recognition (CVPR) (2017)\\r\\n61. Yu, Y., Smith, W.A.: Inverserendernet: Learning single image inverse rendering.\\r\\n    In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n    Recognition. pp. 3155–3164 (2019)\\r\\n62. Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F.E.H., Feng, J., Yan, S.:\\r\\n    Tokens-to-token vit: Training vision transformers from scratch on imagenet. 2021\\r\\n    IEEE/CVF International Conference on Computer Vision (ICCV) pp. 538–547\\r\\n    (2021)\\r\\n63. Zamir, A.R., Sax, A., Shen, W.B., Guibas, L.J., Malik, J., Savarese, S.: Taskonomy:\\r\\n    Disentangling task transfer learning. In: IEEE Conference on Computer Vision and\\r\\n    Pattern Recognition (CVPR). IEEE (2018)\\r\\n64. Zhang, Z., Ma, J., Zhou, C., Men, R., Li, Z., Ding, M., Tang, J., Zhou, J., Yang,\\r\\n    H.: M6-ufc: Unifying multi-modal controls for conditional image synthesis. arXiv\\r\\n    preprint arXiv:2105.14211 (2021)\\r\\n65. Zhu, P., Abdal, R., Qin, Y., Wonka, P.: Sean: Image synthesis with semantic region-\\r\\n    adaptive normalization. In: Proceedings of the IEEE/CVF Conference on Com-\\r\\n    puter Vision and Pattern Recognition (CVPR) (June 2020)\\r\\n\\x0c',\n",
       " '                                                                                                                                                                              1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                DOTS: An Open Testbed for Industrial Swarm\\r\\n                                                           Robotic Solutions\\r\\n                                                                Simon Jones, Emma Milner, Mahesh Sooriyabandara, and Sabine Hauert\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            Abstract—We present DOTS, a new open access testbed for\\r\\n                                         industrial swarm robotics experimentation. It consists of 20 fast\\r\\n                                         agile robots with high sensing and computational performance,\\r\\n                                         and real-world payload capability. They are housed in an arena\\r\\narXiv:2203.13809v1 [cs.RO] 25 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                         equipped with private 5G, motion capture, multiple cameras,\\r\\n                                         and openly accessible via an online portal. We reduce barriers\\r\\n                                         to entry by providing a complete platform-agnostic pipeline\\r\\n                                         to develop, simulate, and deploy experimental applications to\\r\\n                                         the swarm. We showcase the testbed capabilities with a swarm\\r\\n                                         logistics application, autonomously and reliably searching for and\\r\\n                                         retrieving multiple cargo carriers.\\r\\n                                           Index Terms—Swarm robotics, intralogistics, open testbed,\\r\\n                                         industrial swarm\\r\\n\\r\\n\\r\\n                                                                I. I NTRODUCTION\\r\\n                                                                                                               Fig. 1: DOTS robot, showing two forward-facing cameras and\\r\\n                                         M       ANY robots (10-1000+) working together to facilitate\\r\\n                                                 intralogistics in real-world settings promise to improve\\r\\n                                         productivity through the automatic transport, storage, inspec-\\r\\n                                                                                                               lifting platform on top.\\r\\n\\r\\n                                         tion, and retrieval of goods. Yet, existing systems typically\\r\\n                                                                                                               enabling easy scalability, adaptation and robustness. While\\r\\n                                         require sophisticated robots, carefully engineered infrastruc-\\r\\n                                                                                                               traditional logistics solutions prioritise throughput, speed and\\r\\n                                         ture to support the operations of the robots, and often central\\r\\n                                                                                                               operating cost, swarm logistics solutions will be measured\\r\\n                                         planners to coordinate the many-robot system (e.g. Amazon,\\r\\n                                                                                                               against additional ‘Swarm Ideals’, targeting zero setup, recon-\\r\\n                                         Ocado). Once operational, these solutions offer speed and\\r\\n                                                                                                               figuration time, and scaling effort, zero infrastructure, zero\\r\\n                                         precision, but the pre-operational investment cost is often high\\r\\n                                                                                                               training, and zero failure modes. It is these attributes that\\r\\n                                         and the flexibility can be low.\\r\\n                                                                                                               make swarm robotics the natural solution for logistics in more\\r\\n                                            Consequently, there is a critical unmet need in scenarios\\r\\n                                                                                                               dynamic and varied scenarios.\\r\\n                                         that would benefit from logistics solutions that are low-cost\\r\\n                                         and usable out-of-the-box. These robot solutions must adapt              To explore this potential, we present a new 5G-enabled\\r\\n                                         to evolving user requirements, scale to meet varying demand,          testbed for industrial swarm robotic solutions. The testbed\\r\\n                                         and be robust to the messiness of real-world deployments.             hosts 20 custom-built 250 mm robots called DOTS (Distributed\\r\\n                                         Examples include small and medium enterprises, local retail           Organisation and Transport System) that move fast, have long\\r\\n                                         shops, pop-up or flexible warehouses (COVID-19 distribution           battery life (6 hours), are 5G enabled, house a GPU, can sense\\r\\n                                         centres, food banks, refugee camps, airport luggage storage),         the environment locally with cameras and distance sensors,\\r\\n                                         and manufacturing of products with high variance or variation         as well as lift and transport payloads (2 kg per robot). The\\r\\n                                         (e.g. small series, personalised manufacturing). Flexible solu-       platform is modular and so can easily be augmented with\\r\\n                                         tions for such scenarios also have the potential to translate         new capabilities based on the scenarios to be explored (e.g. to\\r\\n                                         to other applications and settings such as construction or            manipulate items). To monitor experiments, the arena is fitted\\r\\n                                         inspection.                                                           with overhead cameras and a motion capture system, allowing\\r\\n                                            Swarm robotics offers a solution to this unmet need. Large         for precise telemetry and replay capabilities. The testbed is\\r\\n                                         numbers of robots, following distributed rules that react to          accessible remotely through custom-built cloud infrastructure,\\r\\n                                         local interactions with other robots or local perception of           allowing for experiments to be run from anywhere in the world\\r\\n                                         their environment, can give rise to efficient, flexible, and          and enabling future fast integration between the digital and\\r\\n                                         coordinated behaviours. A swarm engineering approach has              physical word through digital twinning. In addition we present\\r\\n                                         the potential to be flexible to the number of robots involved,        an integrated development environment useable on Windows,\\r\\n                                                                                                               Linux and OSX lowering the barriers to entry for users of the\\r\\n                                           S. Jones, E. Milner, and S. Hauert are at the University of         system.\\r\\n                                         Bristol, UK. simon2.jones@bristol.ac.uk, emma.milner@bristol.ac.uk,      Beyond characterising the testbed, making use of the de-\\r\\n                                         sabine.hauert@bristol.ac.uk\\r\\n                                           M. Sooriyabandara is at Toshiba Research, Bristol, UK,              velopment pipeline illustrated in Figure 2, we demonstrate the\\r\\n                                         Mahesh.Sooriyabandara@toshiba-bril.com                                steps and processes required to take a conceptually simple\\r\\n\\x0c\\n\\n                                                                                                                                      2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                    Develop              Simulate             Validate              Deploy                Analyse\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2: Using the DOTS IDE, controllers are developed and tested locally in simulation. When ready, controllers are validated\\r\\nusing the online portal and deployed to the physical testbed for an experimental run. Experimental data captured is available\\r\\nfor download from the portal and subsequent analysis.\\r\\n\\r\\n\\r\\nswarm logistics algorithm and successfully run it on real-world       IML Loadrunner [9] swarm is technically sophisticated and\\r\\nrobots to reliably find, collect, and deposit five payload carriers   physically considerably larger than our design but does not\\r\\nin an entirely decentralised way.                                     appear to be open.\\r\\n   This paper is structured in the following way; in Section II\\r\\nwe provide background and discuss related work. In Section\\r\\nIII we detail the physical, electronic, and software systems of                     III. M ATERIALS AND M ETHODS\\r\\nthe testbed, with characterisation of the performance of various\\r\\n                                                                        In the following sections, we detail the robots, the arena,\\r\\nsubsystems. In Section IV we build a complete demonstration\\r\\n                                                                      the online portal for remote experimentation, and the cross-\\r\\nof a distributed logistics task. Section V concludes the article.\\r\\n                                                                      platform integrated development environment.\\r\\n           II. BACKGROUND AND RELATED WORK\\r\\n   Two recent reviews of multi-robot technologies for ware-           A. DOTS robots\\r\\nhouse automation show that amongst over 100 papers in the\\r\\narea, only very few considered decentralised, distributed, or            The robots were designed from the start to be low-cost,\\r\\nswarm approaches [1], [2]. Of those that did, solutions were          around £1000 per robot, simple to construct, to allow relatively\\r\\nonly partially decentralised [3]. This suggests swarm solutions       large numbers to be built, and high capability, by using\\r\\nfor logistics have largely remained unexplored, although the          commodity parts in innovative ways. Each robot is 250 mm in\\r\\ninterest in distributed multi-robot systems for intralogistics is     diameter with a holonomic drive capable of rapid omnidirec-\\r\\ngrowing as shown in a recent review by [4]. At the same time,         tional movement, up to 2 ms−1 and typically 6 hours of battery\\r\\nswarm robotics [5] has 30 years of history, mostly focussing          life. They are equipped with multiple sensors; 360° vision with\\r\\non conceptual problems in the laboratory with translation to          four cameras and a further camera looking vertically upwards,\\r\\napplications increasing in the last couple of years [6].              laser time-of-flight sensors for accurate distance sensing of\\r\\n   We now need fresh strategies to design and deploy swarm            surrounding obstacles, and multiple environment sensors. Each\\r\\nsolutions that eschew the mantra of emergence from interac-           robot has a lifting platform, allowing the transport of payloads.\\r\\ntion of simple agents and embrace the use of high specification       Onboard computation is provided with a high-specification\\r\\nrobots at the agent level to enhance the performance of a             Single Board Computer (SBC), with six ARM CPUs and a\\r\\nrobot swarm without sacrificing its inherent scalability and          GPU. And there are multiple forms of communication avail-\\r\\nadaptability in real-world applications. This is made possible        able - WiFi, two Bluetooth Low Energy (BLE5) programmable\\r\\nnow due to the convergence of technologies including high             radios, an ultrawideband (UWB) distance ranging radio, and\\r\\nindividual robot specifications (fast motion, high-computation,       the ability to add a 5G modem.\\r\\nand high-precision sensing), 5G networking to power commu-               1) Cost minimisation: A central driving factor in the design\\r\\nnication between humans and interconnected robots, access to          process was cost minimisation while still achieving good\\r\\nhigh onboard computational power that allows for sophisti-            performance. The rapid progress in mobile phone capabilities\\r\\ncated local perception of the world and new algorithms for            means that high performance sensors such as cameras are\\r\\nswarm control. Swarm testbeds for research do exist. The              now available at very low cost. Communications advances\\r\\nRobotarium [7] is a complete system of small robots, with             means that fully programmable Bluetooth Low Energy (BLE5)\\r\\nassociated simulator, online access, tracking, and automated          modules cost only a few pounds. Single board computers based\\r\\ncharging and management. The individual robots are not                on mobile phone SoCs are widely available, high definition\\r\\nautonomous though, with controller code executes on a central         cameras cost £4. And the market for consumer drones has\\r\\nserver, with the robots acting as peripherals. Duckietown [8]         made very high power-to-weight ratio motors available at\\r\\nis an open source platform of small cheap autonomous robots           much lower cost than specialised servo motors. We leverage\\r\\ndesigned for teaching autonomous self-driving, but the em-            these advances to reduce the cost per robot to £1000, so the\\r\\nphasis is on users building their own testbed. The Fraunhofer         complete testbed can have many robots.\\r\\n\\x0c\\n\\n                                                                                                                                       3\\r\\n\\r\\n\\r\\n\\r\\n                                                                           2) Robot chassis: The mechanical chassis, shown in Figure\\r\\n              TABLE I: Features of the DOTS robots                      3, is designed to be easy to fabricate in volumes of a few\\r\\n                                                                        10s. Custom parts are made using 3D printing or simple\\r\\n  Specification             Value        Notes                          milling operations. It is constructed around two disks of 2 mm\\r\\n  Diameter                  250 mm                                      thick aluminium, 250 mm in diameter, for the base and top\\r\\n  Height                    145 mm       Lifter lowered\\r\\n                            197mm        Lifter raised                  surfaces. Between the base and top plates and mechanically\\r\\n  Weight                    3 kg                                        joining them 100 mm apart are three omniwheel assemblies,\\r\\n  Lifter max load           2 kg                                        positioned at 120° separation. Each wheel assembly consists of\\r\\n  Max velocity              2 ms−1\\r\\n  Max acceleration          4 ms−2                                      an omniwheel, a drone motor with rotational position encoder,\\r\\n  Wheel torque              0.35 Nm                                     and a 5:1 timing belt reduction drive between motor and wheel.\\r\\n  Battery capacity          100 Wh       2x Toshiba SCiB 23 Ah          Mounted on the base plate are the single board computer\\r\\n  Endurance                 6 hours      All cameras enabled,\\r\\n                                         moderate vision processing     (SBC) and the power supply PCBs, in both cases thermally\\r\\n                                         and movement                   bonded to the base plate to provide passive cooling. The\\r\\n                            14 hours     No cameras, low level          battery pack sits in the central area of the base plate. On\\r\\n                                         processing, occasional\\r\\n                                         movement                       55 mm standoffs above the base plate is the mainboard PCB,\\r\\n  Carriers                  330x330 mm   175 mm clearance               which contains all the sensors, four cameras, motor drives, and\\r\\n  Processor                 RockPi4      6x ARM CPUs, ARM Mali          associated support electronics. The top plate holds the payload\\r\\n                                         T860MP4 GPU\\r\\n  Cameras                   OV5647       5x 120°FOV video up to         lifting mechanism and an upward-facing camera, connecting\\r\\n                                         1080p30                        via USB and power leads to the mainboard. Alternate top\\r\\n  Proximity                 VL53L1CX     16x IR laser time-of-flight,   plates, with different actuators, for example a small robot arm,\\r\\n                                         3 m range, 10 mm precision\\r\\n  IMU                       LSM6DSM      6DoF accelerometer and         can easily be fitted.\\r\\n                                         gyroscope                         3) System architecture: Many subsystems within the robot\\r\\n  Magnetometer              LIS2MDL      3DoF                           need to be integrated and connected to the single board\\r\\n  Temperature,              Si7021-A20   Temperature accuracy\\r\\n  humidity                               ±0.4 °C, relative humidity     computer. The system architecture is shown in Figure 4.\\r\\n                                         ±3%                            Communications between sensors, actuators, and the single\\r\\n  Absolute pressure         LPS22HB      260 hPa- 1260 hPa, accuracy    board computer takes place on three bus types; USB for\\r\\n                                         ±0.1 hPa\\r\\n  Microphones               IM69D130     MEMS sensor, FPGA-based        high bandwidth, mostly MJPEG compressed video from the\\r\\n                                         CIC filter                     cameras, I2 C for low bandwidth, mostly various sensors, and\\r\\n                                                                        SPI for medium bandwidth deterministic latency, used for\\r\\n                                                                        controlling the motor movements. An FPGA is used for glue\\r\\n                                                                        logic, for example to interface to the MEMS microphones\\r\\n                                                                        and the programmable LEDs. The BLE5 and UWB radios\\r\\n                                                                        are reprogrammable from the SBC via their single wire debug\\r\\n                Camera                                                  (SWD) interfaces [10].\\r\\n                                                                           4) Vision: Vision is the primary modality by which the\\r\\n          Top plate                                                     robot can understand its environment. Miniature cameras have\\r\\n              Lifter                                                    been revolutionised by progress in mobile devices. It is now\\r\\n     BLE5 radios                                                        possible to buy a 5 megapixel camera also capable of multiple\\r\\n                                                                        video resolutions up to 1080p for £4. A central design issue\\r\\nRaspberry Pi Zero                                                       is how to best use this cheap data - image processing is\\r\\n     Camera x4                                                          computationally heavy. We designed the system to have all\\r\\n   RGB LED x16                                                          round vision for the robot, achieved by using four cameras\\r\\n                                                                        with a 120° field of view (FOV) around the perimeter of the\\r\\nIRToF sensor x16\\r\\n                                                                        robot. The two front-facing cameras have overlapping fields\\r\\n     Mainboard\\r\\n     UWB radio\\r\\n                                                                        of view to allow for the possibility of stereo depth extraction.\\r\\n                                                                        A fifth camera is fitted in the centre of the lifting platform\\r\\n        Stand-off\\r\\n                                                                        looking upwards so that visual navigation under load carriers\\r\\n     Battery pack\\r\\n                                                                        can be achieved with suitable targeting patterns.\\r\\n    RockPi 4 SBC                                                           The amount of data in an uncompressed video stream\\r\\n        Drone motor\\r\\n                                                                        is high, 640x480 at 30 fps takes around 18 MBytes/s of\\r\\n                                                                        bandwidth. For five cameras, this is close to 100 MBytes/s,\\r\\n               Base plate\\r\\n                                                                        which would require a high performance interconnect such\\r\\n                       Omniwheel                                        a gigabit ethernet, impractical at low cost and indicating a\\r\\n                                   PSU\\r\\n                                                                        need for image compression. Rather than using fixed function\\r\\nFig. 3: Exploded view of the robot chassis showing major                webcam hardware to perform this compression, we looked\\r\\ncomponents.                                                             for flexibility. The approach we took was to use a local\\r\\n                                                                        Raspberry Pi Zero computer for each camera to perform image\\r\\n                                                                        compression and then send the data over USB. This gives a\\r\\n\\x0c\\n\\n                                                                                                                                                                             4\\r\\n\\r\\n\\r\\n\\r\\n                            RPI            Cam 0\\r\\n                                                                     Locations of cameras              TABLE II: Vision system latency, from photon arrival to\\r\\n                           Zero 0                                        and motors\\r\\n                                                                                                       change in memory buffer (glass-to-algorithm) under different\\r\\n                            RPI            Cam 3                              0       3\\r\\n                           Zero 3                                                                      camera refresh rates and utilisation of the USB fabric. Results\\r\\n                            RPI                                                                        are averaged over 1000 frames.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                       0\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                              2\\r\\n                           Zero 4          Cam 4\\r\\n                                                                                  4\\r\\n                            RPI\\r\\n                           RPI Zero 3      Cam 2                                                           Cameras     Latency x̄ (ms)   Latency σ (ms)   Latency (frames)\\r\\n                           Zero 2\\r\\n                                                                                                           30 Hz refresh rate\\r\\n                            RPI\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                        1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                             2\\r\\n                           Zero 1          Cam 1                                                              1              78.2             9.23             2.35\\r\\n                                                                                  1\\r\\n            USB     USB     BLE                                                                               2              72.8             10.7             2.18\\r\\n                    hub    radio 0                                                                            3              74.6             11.7             2.24\\r\\n RockPi 4                   BLE                SWI                                                            4              73.4             11.5             2.20\\r\\n  SBC                      radio 1                                                                            5              78.5             11.5             2.36\\r\\n                                                                    Motor                                  60 Hz refresh rate\\r\\n                                                                    drive 0\\r\\n                                                                                                              1              53.2             15.8             3.19\\r\\n                                                                              Motor                           2              45.2             9.55             2.71\\r\\n                                                                              drive 1\\r\\n                                                                                                              3              45.3             9.82             2.72\\r\\n                                                                                            Motor\\r\\n                                                                                            drive 2           4              58.3             12.9             3.50\\r\\n                                                                                                              5              58.4             14.6             3.50\\r\\n                           FT2232\\r\\n                                                             Step/Dir Step/Dir Step/Dir\\r\\n\\r\\n            Serial/GPIO\\r\\n            SPI                                               FPGA\\r\\n            I2C                                                                                        from each.\\r\\n                                                                                                             a) Vision latency: A basic performance metric when\\r\\n                                Serial\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                          I2C            SWD      I2C                Custom           I2C\\r\\n                                                           8x                                          using vision for motion control is the time delay between\\r\\n  12V       5V     3.3V                                   ADC\\r\\n   4A       4A      2A                    UWB            temp         IM69D130              VL53L1CX   photons arriving at the camera, and when data corresponding\\r\\n                                          radio         humidity         mic                   x16\\r\\n                                                                                                       to those photons is available in a memory buffer on the single\\r\\n          PSU                                           barometer     IM69D130\\r\\n                                                                         mic                  RGB\\r\\n                                                                                             LEDx16\\r\\n                                                                                                       board computer, known as Glass-to-Algorithm [13]. Measuring\\r\\n   2x SCiB cell battery                            magnetometer                                        the vision latency is non-trivial, see [14]. We used a set of eight\\r\\n      pack 100Wh\\r\\n     protection and                                       6DoF                                         LEDs in approximately the centre of the field of view of one\\r\\n       monitoring                                          IMU\\r\\n                                                                                                       camera, and performed image processing on the destination\\r\\nFig. 4: Architecture of the robot. Green for computation and                                           buffer to extract the binary data displayed on the LEDs. By\\r\\ncommunication, red for power, orange for sensors, and blue                                             setting a code on the LEDs then timing how long before the\\r\\nfor interconnect fabric.                                                                               code was visible in the buffer data we can measure the latency.\\r\\n                                                                                                       After each code is detected, the next in a Gray-code sequence\\r\\n                                                                                                       is displayed to accumulate multiple timings.\\r\\ncost per camera of around £15.                                                                            Data from each camera is transmitted to the single board\\r\\n   The Raspberry Pi Zero is a low cost small form factor                                               computer via a USB hub on the mainboard. We load the system\\r\\nSingle Board Computer with a camera interface. Although the                                            by running a varying number of cameras simultaneously.\\r\\nCPU has quite a low performance (ARM1176 700MHz), it is                                                Measurements are taken with different camera refresh rates.\\r\\nnot generally appreciated how powerful the associated VPU                                              The results are shown in Table II for different refresh rates\\r\\n(Video Processing Unit) and GPU (Graphics Processing Unit)                                             with varying numbers of other cameras running and streaming\\r\\nis. This allows for the possibility of utilising this power to                                         data at the same time. Times averaged over 1000 samples.\\r\\nperform local image processing, e.g. to perform fiducial recog-                                           Because the LEDs are positioned at approximately the\\r\\nnition locally, rather than loading the main central processor.                                        centre vertically of the camera field of view, and the camera\\r\\nIn this paper, we use standard applications to stream MJPEG                                            performs a raster scan to read the sensed image, there is\\r\\ncompressed frames with low latency from the photon arrival                                             a minimum of half a frame before data is available to the\\r\\nat the camera, to the presence of data in a buffer at the central                                      Raspberry Pi for processing and compression. Since the LEDs\\r\\nprocessor.                                                                                             are changed uncorrelated to the frame rate, there may be\\r\\n   Although cameras are cheap, processing image streams to                                             between zero and one additional frame of delay uniformly\\r\\ngive useful information is not, and although the single board                                          distributed, giving a mean of one frame delay before the start\\r\\ncomputer is quite capable, it is not in the same class as a                                            of processing, compression, transmission, and then decompres-\\r\\ndesktop PC. This means we are not free to easily run more                                              sion into the destination image buffer.\\r\\ncomplex algorithms without considerable optimisation efforts                                              The worst-case figures of 79 ms at 30 Hz and 58 ms at 60 Hz\\r\\nto, for example, utilise GPU processing. As an example, most                                           for the most heavily loaded cases of five cameras streaming\\r\\nvisual SLAM algorithms are evaluated on PC-class systems.                                              image data compare well with state-of-the-art systems, which\\r\\nORB-SLAM2 was evaluated in a system with an Intel Core                                                 is reported as 50-80 ms by [13].\\r\\ni7-4790 and this is just fast enough to run at real-time with a                                              b) Camera calibration: In order that the perimeter cam-\\r\\nsingle camera at 640x480 30 Hz [11]. With this in mind, we                                             eras can be used to extract pose information from fiducial\\r\\ncan ease the task of understanding the environment by using                                            markers, it is necessary that they be calibrated so that their\\r\\nubiquitous fiducial markers - future systems will be able to                                           intrinsic parameters are known. We initially calibrated several\\r\\nuse greater processing power. The ArUco library [12] has a far                                         cameras using the ArUco calibration target and software\\r\\nlower computational load than visual SLAM, and this allows                                             tools. The process involves taking multiple images of the\\r\\nus to process all five camera streams and extract marker poses                                         calibration target then using the software tool to find intrinsics\\r\\n\\x0c\\n\\n                                                                                                                                           5\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            Sensor\\r\\n                                                            Comms fabric\\r\\n                                                            ROS node\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nRPI Zero    RPI Zero    RPI Zero     RPI Zero\\r\\nMJPEG       MJPEG       MJPEG        MJPEG           IMU         Odometry\\r\\n\\r\\n\\r\\n  USB        USB          USB            USB          I2C\\r\\n\\r\\nDecomp      Decomp      Decomp        Decomp\\r\\n\\r\\n Aruco       Aruco       Aruco           Aruco\\r\\n detect      detect      detect          detect\\r\\n\\r\\n              Extended Kalman Filter (robot_localization)                   Fig. 6: Ground truth X and Y positions (left axis), and visual\\r\\n                                                                            tracking position error (right axis), of robot while moving\\r\\n                                                                            along a square path.\\r\\n                                  Pose\\r\\n\\r\\n                                                                            IMU and wheel odometry information. The output of the EKF\\r\\nFig. 5: Visual tracking system. Each camera video stream is                 is a stream of poses in the map frame.\\r\\nsend as compressed MJPEG to the single board computer, then                    The robot was commanded to move twice around a square\\r\\ndecompressed and processed for fiducial marker poses. The                   of sides 1.6m, with velocities of 0.3ms−1 and 0.5ms−1 using\\r\\nposes, together with IMU and wheel odometry information                     ground truth as the position feedback. Ground truth from the\\r\\nare fed to an EKF to output robot pose.                                     motion capture system and estimated position were recorded.\\r\\n                                                                            Figure 6 shows the actual positions and the absolute error in\\r\\n                                                                            x and y axes. Maximum positional error in either axis never\\r\\nthat minimise the reprojection error. This showed that each                 exceeds 62mm, with σ(x) = 22.3mm and σ(y) = 16.1mm.\\r\\ncamera had significant differences in their intrinsic parameters,              5) Sensors: In addition to the vision system, the are a wide\\r\\nperhaps not surprising given their very low cost, but meaning               variety of sensors to apprehend the state of the environment\\r\\nwe had to calibrate each camera individually, rather than using             and the robot itself.\\r\\na standard calibration. We needed an automated approach - 20                     a) Proximity sensors: Surrounding the perimeter of the\\r\\nrobots, each with four cameras to calibrate would require many              robot are 16 equally-spaced ST Microelectronics VL53L1CX\\r\\nhundreds of images captured from different angles.                          infra-red laser time-of-flight distance sensors (IRToF). These\\r\\n   By attaching a calibration target to the arena wall at a fixed           are capable of measuring distance to several metres with\\r\\nand known location, we could execute a calibration process                  a precision of around 10 mm at an update rate of 50 Hz,\\r\\nautomatically taking multiple pictures per camera in about 5                giving a pointcloud of the environment around the robot. Each\\r\\nminutes per robot. Because the only angle that can be varied                detector has a field of view of approximately 22°, which\\r\\nbetween the camera and the target is the rotation about robot Z,            can be partitioned into smaller regions, allowing for a higher\\r\\nthe calibration is less complete, but sufficient given the robot            resolution pointcloud at the cost of lower update rate.\\r\\nwill always be horizontal on the arena floor.                                  As well as returning distance measurements, the\\r\\n      c) Vision processing for localisation: To demonstrate the             VL53L1CX devices are capable of being operated in a\\r\\nability to usefully process vision from multiple cameras we                 mode that returns raw photon counts per temporal bin.\\r\\nbuilt a simple localisation system, shown in Figure 5, and                  This opens up intriguing possibilities - it is possible to\\r\\nbased on ArUco markermaps. This is a feature of the ArUco                   classify materials by their temporal response to incident\\r\\nlibrary that allows the specification of a map of the locations             illumination, and this is demonstrated in [16] with a custom\\r\\nof an arbitrary number of arbitrarily placed fiducial markers,              built time-of-flight camera. [17] show that this also possible\\r\\nwith library functions to return the 3D pose of a calibrated                with the VL53L1CX device, despite the much lower cost\\r\\ncamera which can see at least one of the markers in the map.                and limitations on output power and temporal resolution.\\r\\nWe fixed twelve fiducial markers around the walls of the arena,             They demonstrate successful identification of five different\\r\\nand encoded the locations in a markermap.                                   materials. There is no reason in principle that this robot could\\r\\n   Each camera was calibrated automatically, as described                   not function as a mobile material identification platform, for\\r\\nabove. The video stream from each of the four perimeter                     example in inspection applications.\\r\\ncameras is analysed and if there are any visible fiducials in                  There is little published data on the performance of the\\r\\nthe correct ID range, the pose of the robot in the global map               VL53L1CX sensors, outside the manufacturers specifications,\\r\\nframe is generated. This stream of poses is fed to an Extended              so we wanted to characterise this. The sensors were set\\r\\nKalman Filter filter (EKF, robot localization [15]), along with             up according to the manufacturers recommendations using\\r\\n\\x0c\\n\\n                                                                                                                                                                       6\\r\\n\\r\\n                                                                      Time-of-flight measurement accuracy\\r\\n                                          3.5                                                                                            350\\r\\n                                                      IRToF measurement (left scale)\\r\\n                                                      Ideal sensor (slope 1)\\r\\n                                          3.0         Multipath (slope 2)                                                                300\\r\\n                                                      Multipath (slope 3)\\r\\n\\r\\n                                          2.5                                                                                            250\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                              Measurement error (mm)\\r\\n                  Measured distance (m)\\r\\n\\r\\n                                          2.0                                                                                            200\\r\\n\\r\\n                                          1.5                                                                                            150\\r\\n\\r\\n                                          1.0                                                                                            100\\r\\n\\r\\n                                          0.5                                                                                            50\\r\\n\\r\\n                                          0.0                                                                                            0\\r\\n                                                                                                               Error: x, (right scale)\\r\\n                                          0.5                                                                                                50\\r\\n                                                0.0           0.5         1.0         1.5           2.0       2.5          3.0\\r\\n                                                                                  Actual distance (m)\\r\\nFig. 7: Plot of 2455 points of IRToF measured distance vs ground truth (left scale) and measurement error in 0.125 m bins\\r\\n(right scale). Bars are ±σ. The inset shows the data in the range 0.05 m to 0.4 m. There are signs of multipath effects at close\\r\\nrange (less than 0.25 m) indicated by the green and red lines of slope 2 and 3.\\r\\n\\r\\n\\r\\nthe VL53L1X Ultra Lite driver1 for ‘short’ distance mode                                     other device setup possibilities to see if it possible to mitigate\\r\\nand update rate of 50 Hz. This notionally gives a maximum                                    this behaviour.\\r\\ndistance measurement of 1.3 m. We commanded a robot to\\r\\n                                                                                                  b) Additional sensors: In addition to the cameras and\\r\\nmove repeatedly between locations 0 m and 3.5 m from a\\r\\n                                                                                             proximity sensors, the robot is equipped with other sensors to\\r\\nvertical wall made of polypropylene blocks at a speed no\\r\\n                                                                                             understand the robot state or the state of the environment - 9\\r\\ngreater than 0.5 ms−1 along a path at right angles to the wall.\\r\\n                                                                                             degree-of-freedom (DoF) Inertial Measurement Unit (IMU),\\r\\nThe robot pose was such that one sensor was directly pointing\\r\\n                                                                                             temperature, pressure, humidity, two MEMS microphones,\\r\\nat the wall. The ground truth of the robot position and the\\r\\n                                                                                             robot health, and auxiliary ADCs for future use.\\r\\ndistance readings from the IRToF sensor orthogonal to the wall\\r\\nwere captured. A total of 2455 measurement samples were                                        The IMU consists of a 6 DoF LSM6DSM accelerometer\\r\\ncollected. Figure 7 shows the results. Each measured distance                                and gyroscope and a 3 DoF LIS2MDL magnetometer. The\\r\\nis plotted against the ground truth distance of the sensor from                              IMU, together with information about wheel position, is the\\r\\nthe wall. We divided the measurements into bins 0.125 m wide                                 main way the robot performs odometry to estimate its local\\r\\nand calculated the mean and standard deviation of the error in                               movement. We currently acquire samples at 100Hz and filter\\r\\neach bin.                                                                                    with an EKF to match the motion control update rate.\\r\\n   The performance of the sensor is remarkably good over                                       The temperature, pressure and humidity sensors allow us\\r\\nmost of its range. Once above 0.25 m, up to 2.7 m, the mean                                  to monitor the state of the robots environment - useful in\\r\\nmeasurement error does not exceed 12 mm and is mostly less                                   a warehouse scenario, for example, noting locations with\\r\\nthan 5 mm. Even out to 3.2 m, way beyond the notional range                                  excessive humidity or temperatures outside desired limits.\\r\\nfor its mode of operation, the mean error never exceeds 30 mm.\\r\\n                                                                                                The two IM69D130 MEMS microphones are processed by\\r\\n   Of some interest is the behaviour of the sensor at close\\r\\n                                                                                             the on-board FPGA to a I2 S stream which is sent to the\\r\\ndistances, less than 0.25 m. Here, the mean error is much\\r\\n                                                                                             SBC. The audio is available using the standard Linux audio\\r\\nhigher, up to 75 mm, and examining the measured points in\\r\\n                                                                                             framework. We intend to use these to experiment with audio-\\r\\nmore detail (see inset in Figure 7) we can see that many of\\r\\n                                                                                             based localisation - sending ultrasonic signals at known times\\r\\nthe points do not fall on a line of slope 1, which would be\\r\\n                                                                                             and measuring time-of-arrival to estimate distances.\\r\\nexpected from a perfect sensor, but on lines of slope 2, and\\r\\npossibly slope 3. This would be characteristic of multipath                                    Robot health sensors monitor the state of various subsys-\\r\\nreflections e.g. reflecting off the wall, then the robot body, then                          tems on board. We measure the voltages and currents of each\\r\\nthe wall again before being sensed. We intend to investigate                                 power supply, the state of each cell of the battery pack, and the\\r\\n                                                                                             temperatures of the battery charging system, power supplies,\\r\\n  1 https://www.st.com/en/embedded-software/stsw-img009.html                                 motor drives, and single board computer. All this information\\r\\n\\x0c\\n\\n                                                                                                                                                                7\\r\\n\\r\\n\\r\\n\\r\\nis available through the Linux hwmon sensor framework2 .                 TABLE III: Motor drive cost comparison. Maxxon parts\\r\\n   6) Communication: Local communication is important for                chosen to give broadly similar performance\\r\\nswarm robotics, and an area where there is much potential                 Part             Maxxon                   Cost    Commodity                    Cost\\r\\nfor novel approaches. To facilitate experimentation, we in-               Motor            EC45 flat 120W            129    Sunnysky X2814                22\\r\\n                                                                                           P/N 608148                       900Kv\\r\\nclude multiple different radios. As well as the WiFi built in             Encoder          MILE 2048 P/N             97     Sensor PCB and                 10\\r\\nto the single board computer, each robot is equipped with                                  673027                           magnet\\r\\ntwo nRF52840-based BLE5 (Bluetooth Low Energy) USB-                       Controller       ESCON P/N                127     Custom drive                   25\\r\\n                                                                                           438725                           electronics\\r\\nconnected radio modules, and a DWM1001 UWB (Ultra wide                                                              £353                                  £57\\r\\nband) module. A private 5G modem can be added. All of\\r\\nthese modules can be reprogrammed with custom firmware\\r\\nunder the control of the single board computer, allowing for                                    Encoder PCB\\r\\n                                                                                              support, 3D printed\\r\\non-the-fly installation of novel communication protocols.\\r\\n   We currently have firmware for the BLE radios that contin-                                                                                       Encoder PCB\\r\\nually advertises the robots unique name and a small amount of\\r\\ndata, and scans for other BLE radios. Each scan result contains\\r\\nthe name of any nearby robot sensed, along with its received\\r\\nsignal strength (RSSI), which can be used as a proxy measure\\r\\nfor distance.                                                                 12T drive pully                                                Diametrically\\r\\n   The DWM1001 radio is designed to perform two-way                                                                                        magnetised magnet\\r\\n\\r\\nranging between devices, measuring the distance between a\\r\\npair of radios to a claimed accuracy of around 0.1m. It is                 GT2 toothed belt                                              SunnySky X2814\\r\\n                                                                                                                                        900Kv drone motor\\r\\ninterfaced to the SBC using an SPI bus.\\r\\n   7) Mobility: For the collective transport of large loads, it\\r\\nis necessary for multiple robots to be able to move together\\r\\nin a fixed geometric relationship with each other. The only\\r\\nway to achieve this with arbitrary trajectories is for the robots                                                           Aluminium motor support\\r\\n\\r\\nto have holonomic motion. We use a three omniwheel system\\r\\nwith the wheels equally spaced 120° apart. The kinematics for\\r\\nthis type of drive are well known [18] and are shown below:                                                                60T pulley, 3D printed\\r\\n            \\uf8eb \\uf8f6 \\uf8eb √3                     \\uf8f6\\uf8eb \\uf8f6\\r\\n               v1        − 2      0.5 1        vx\\r\\n                                                                                                                      Omniwheel\\r\\n            \\uf8ed v2 \\uf8f8 = \\uf8ed 0\\r\\n                           √\\r\\n                                  −1    1\\uf8f8 \\uf8ed vy \\uf8f8\\r\\n               v3            3                Rω\\r\\n                              2  −0.5 1\\r\\nwhere v1 , v2 , v3 are the tangential wheel velocities, vx , vy , ω\\r\\nthe robot body linear and angular velocities, and R the radius           Fig. 8: Single drive assembly, showing commodity drone\\r\\nfrom centre of robot to wheels.                                          motor with magnetic angle encoder PCB driving omniwheel\\r\\n   Another important requirement for the robots is that they             via 5:1 toothed belt reduction.\\r\\ncan move fast and accurately. A limitation of other lab-\\r\\nbased swarm systems is that the locomotion is often based\\r\\non stepper motors or small geared DC motors. These are                   and the motors often use lower voltages and higher currents\\r\\nrelatively cheap and accurate but are slow and heavy. Much               than typical servos, but these deficiencies can be compensated\\r\\nhigher performance is possible with Brushless DC (BLDC)                  for with clever software, e.g. [23]. We designed drive circuitry\\r\\nservo motors. These servo motors are paired with a position              suitable for running the Odrive Robotics3 open source BLDC\\r\\nencoder and drive electronics that modulates the coil current            servo controller and tested various cheap drone motors, select-\\r\\nto achieve accurate torque, velocity and position control with           ing for the motor with the least cogging that met the required\\r\\nmuch higher performance to weight and size ratios than is                form factor and price. We replaced the costly traditional optical\\r\\ntypically possible with stepper motors. This comes at a cost,            position encoder with a high resolution magnetic field angle\\r\\na typical motor, encoder, and driver with the performance we             encoder IC. By glueing a small diametrically magnetised disc\\r\\nrequire costs around £350.                                               magnet to the end of a motor shaft and positioning the encoder\\r\\n   There has been recent interest in using commodity drone               IC correctly in line with the motor axis, we can sense absolute\\r\\nmotors in place of dedicated servo motors [19]–[22]. The                 motor shaft angle with high precision and low cost.\\r\\nhigh power-to-weight ratios and low costs due to the large                  These innovations reduced the cost per robot of the motor\\r\\nmarket size make this an interesting alternative. There are              drives from over £1000 to less than £200 with comparable\\r\\ndisadvantages - the motors are not designed for use as servos            performance. Table III illustrates this, and Figure 8 shows a\\r\\nand certain parameters which are more important in servo                 single complete wheel assembly.\\r\\nmotors are not controlled, e.g. the amount of torque cogging,               Many systems aimed at swarm robotics research e.g. epucks,\\r\\n  2 https://www.kernel.org/doc/html/latest/hwmon/hwmon-kernel-api.html     3 https://odriverobotics.com/\\r\\n\\x0c\\n\\n                                                                                                                                                              8\\r\\n\\r\\n\\r\\n\\r\\n                                                      ẍimu\\r\\n                                                                                         TABLE IV: Subsystem power consumption. The table indi-\\r\\n                                                      ÿimu\\r\\n                                                                IMU                      cates the power consumption of some subsystems and battery\\r\\n                                                      ωimu\\r\\n                                                                              Motor      life under various conditions.\\r\\n                                              EKF                           encoder\\r\\n                                                      ẋenc                  velocities    Subsystem       Power (W)     Life1 (h)    Notes\\r\\n                                                      ẏenc\\r\\n                                                      ωenc     motorT\\r\\n                                                                     base                 Mainboard          2.5                     Includes all sensors except\\r\\n                                                                                                                                     vision\\r\\n                                                                                          RockPi4 SBC          2.5                   Basic telemetry and WiFi\\r\\n                                                                                                                                     networking\\r\\n                                       pos                                      M0                             9.5                   All 6 CPUs and GPU busy\\r\\n         req                                        ẋbase                                 Vision system        7.0                   5x cameras streaming MJPEG\\r\\n                Path                                ẏ\\r\\n         fb    action      Traj gen goal     PID x3 ωbase  baseT\\r\\n                                                                motor           M1                                                   640x480 @30fps\\r\\n         res                                          base\\r\\n               server                                                                     Motor system          0                    Off\\r\\n                                     Position                                                                   2                    Random walk 0.5 ms−1 (avg)\\r\\n        Path       Waypoint            goal         Velocity         Motor M2                                  15                    Full torque on all wheels\\r\\n p={wp1,..,wpn}   wpi=(x,y,θ,ẋ,ẏ,ω)T g=(x,y,θ)                      velocity Motor\\r\\n                                                                                          Total                 6           14       Basic sensing and comms,\\r\\n                                                                             drives\\r\\n                                                                                                                                     occasional movement\\r\\n                                                 100Hz                                                         14            6       Sensing, comm, and cameras,\\r\\n                                                                                                                                     moderate vision processing\\r\\nFig. 9: Motion control system. A ROS action server accepts                                                                           and movement\\r\\nmulti-waypoint paths which get played out by the constrained                                                   34           2.5      All systems at maximum\\r\\ntrajectory generator at 100 Hz. The PID control loop operates                                                                        power consumption\\r\\n                                                                                                   1 Assuming 100 Wh battery capacity @85% efficiency\\r\\nin the robot local frame, with robot velocities transformed into\\r\\nthe appropriate motor velocities.\\r\\n                                                                                         monitored with INA219 power sensors.\\r\\npheno, kilobots etc (cite) are physically quite small and move                              A battery charger circuit appropriate to the chemistry of\\r\\nat low speeds. Collisions are not of high enough energy to                               the SCiB cells is integrated into the PSU module, enabling\\r\\ncause damage and the mass of the robots is low enough that                               the robot battery to be recharged with a commodity AC-DC\\r\\nlittle attention has to be paid to concerns that are important                           adaptor capable of providing 12 V@5 A.\\r\\nin larger robots such as trajectory generation respecting ac-                               Table IV shows the power consumption of various sub-\\r\\nceleration limits. The DOTS robots, however, have a mass of                              systems. The baseline idle consumption gives access to all\\r\\n3 kg, with a potential payload of 2 kg, so a proper trajectory                           sensors except for the vision system, and includes WiFi and\\r\\ngeneration and motion control loop is important. We want                                 Bluetooth communication. At this level, the endurance of a\\r\\nto be able to generate new trajectory points in real time                                robot with a full charge is around 16 hours. We can see that the\\r\\nwith low computational cost, and also to be able to revise                               consumption increases considerably when using the camera\\r\\nthe goals on-the-fly. This capability is important for agile                             vision system. The motor system figure is for all wheels\\r\\nquadrotor drones, so there has been recent work in this area.                            delivering maximum torque so would not be a continuous\\r\\nWe use the Ruckig trajectory generator described in [24],                                state. Average motor power when performing a random walk\\r\\nwhich is available as C++ library4 . The on-board motion                                 with a maximum speed of 0.5 ms−1 was around 2 W. With full\\r\\ncontrol system is shown in Figure 9. The action server can                               camera usage and moderate movement and processing we get\\r\\naccept a path consisting of multiple waypoints with required                             an endurance of around 6 hours.\\r\\npositions and velocities, this is played out by the trajectory                              Battery life is limited to the point the output voltage falls\\r\\ngenerator producing intermediate goal positions at a rate of                             below the undervoltage protection limit. The largest dynamic\\r\\n100 Hz. Three PID controllers generate velocity commands                                 load component is the motor drives, so we compromise on the\\r\\nin the robot frame to satisfy the current goal on the path.                              maximum allowed acceleration in order to limit voltage drop\\r\\nPosition feedback uses the Inertial Measurement Unit (IMU)                               and extend effective endurance. The available torque implies\\r\\nand wheel odometry fed through an EKF filter. At any point,                              a maximum loaded acceleration of around 4 ms−2 but we limit\\r\\nthe action server can accept a new trajectory or the cancellation                        this to 2 ms−2 .\\r\\nof the existing trajectory and goals are updated on-the-fly                                 9) Manipulation: In order to demonstrate novel algorithms\\r\\nwhile respecting acceleration and jerk limits. Cancellation of                           for logistics and collective object transport, the robots need a\\r\\na trajectory results in maximum acceleration braking to zero                             means of carrying payloads. For this purpose, each robot is\\r\\nvelocity. Supplementary material ?? shows various examples                               equipped with a lifter - a platform on top of the robot that can\\r\\nof agile trajectory following.                                                           be raised and lowered by the use of a miniature scissor lift\\r\\n    8) Power: The power system consists of two Toshiba SCiB                              mechanism actuated by a high-powered model servo. In the\\r\\n23Ah Lithium Titanate cells, with a total energy capacity of                             centre of the lifter platform there is an upwards-facing camera\\r\\n100 Wh. These are combined into a battery pack, with a                                   that can be used to visually navigate.\\r\\nbattery management system to provide under- and overvoltage                                 To hold payloads, there are carriers, square trays with four\\r\\nprotection and monitoring. This is converted by the PSU                                  legs that the robot can manoeuvre underneath before raising\\r\\nmodule to the required voltages 3.3 V@2 A, 5 V@4A, and                                   the lifter. In order that a robot can position itself correctly to\\r\\n12 V@4 A. All voltages, currents, and the battery state are                              lift the carrier, we place a visual target on the underside.\\r\\n                                                                                            Each carrier weighs about 200 g, so with the lifting capacity\\r\\n  4 https://github.com/pantor/ruckig                                                     of 2 kg we can take a payload of around 1 8kg. In order to\\r\\n\\x0c\\n\\n                                                                                                                                    9\\r\\n\\r\\n\\r\\n\\r\\ndemonstrate collective transport, the carriers can be joined         more suitable protocol, this is the approach we used; Zenoh6\\r\\ntogether along their edges to form arbitrary shapes, to be           is an open-source project supported Eclipse Foundation7 . The\\r\\nmoved with an equal number of robots, one under each carrier.        Zenoh approach is agnostic of the particular DDS middleware\\r\\n                                                                     used, and is intended to be part of the next release of ROS2.\\r\\nB. Industrial Swarm Testbed                                          The Zenoh protocol is designed to overcome the deficiencies\\r\\n   The industrial swarm testbed is the collection of DOTS            of DDS regarding discovery scalability [32]. By using a\\r\\nrobots, the physical arena - a 5 m x 5 m main room overlooked        Zenoh-DDS bridge on each agent (robots and other partici-\\r\\nby an adjacent control room, the communications infrastruc-          pants) and disallowing DDS traffic off robot, we can achieve\\r\\nture comprising WiFi and private 5G network, motion tracking         transparent ROS2 connectivity with far lower discovery traffic\\r\\nsystem, video cameras, and the system software architecture.         levels, granular control over the topics that can seen outside\\r\\n   It also has an associated high fidelity simulation system,        the robot, true decentralisation, and no use of problematic\\r\\nbased on the widely used Gazebo simulator. Controllers can           multicast.\\r\\nbe safely developed in simulation and transferred without               A number of ROS nodes are always running on each robot,\\r\\nmodification to run on the real robots.                              interfacing to the hardware and providing a set of ROS topics\\r\\n   This section details the individual components making up          that constitute the robot API. This API is available on both\\r\\nthe testbed, before tying them together with a description of        the simulated and real robot, and is intended to be the only\\r\\nthe complete system architecture.                                    way a controller has access to the hardware.\\r\\n   1) Robot Operating System (ROS): We use ROS2 Galactic                2) Coordinate frames: We adhere to the ROS standard for\\r\\n[25] for the testbed. ROS2 is the next generation of ROS             coordinate frames REP-105 [33], see the arena diagram Figure\\r\\n[26], and, at first glance, would appear to have several             13, with the extension that each frame name that is related to\\r\\nadvantages over ROS1 for the type of decentralised swarm             an individual robot is prefixed with the unique robot name. The\\r\\ntestbed we describe here. Firstly, there is no ROS master,           global frame map is fixed to the centre of the arena, odom\\r\\nROS2 is completely decentralised. With multiple, potentially         is fixed to the starting position of the robot, and base link\\r\\ncommunicating, robots, we don’t need to choose one to be             is attached to the robot body, with +x pointing forward,\\r\\nthe master, nor do we need to use multimaster techniques             between the overlapping cameras. The base link frames are\\r\\n(e.g, [27]) as were necessary with ROS1. This is facilitated         updated relative to the odom frames using each robots inertial\\r\\nby the second major difference - the communications fabric of        measurements and wheel velocities. There is no transform\\r\\nROS2 uses middleware conforming to the DDS standard [28]             between map and any odom unless some form of localisation\\r\\nof which there are several suppliers. DDS supports automatic         is used on a robot, which is not necessary for many swarm\\r\\ndiscovery of the ROS graph and supports a rich set of Quality        applications. The ROS transform library TF [34] is used to\\r\\nof Service (QoS) policies that can be applied to different ROS       manage transforms. Each robot has its own transform tree,\\r\\ntopics, or communication channels. This maps onto a swarm            and a transform relay node is used to broadcast a subset of\\r\\nperspective quite naturally; communications within the ROS           the local transforms to global topics for use in visualisation\\r\\ngraph of a single robot may be marked as reliable meaning            and data gathering.\\r\\ndata will always be delivered, even if retries are necessary,           3) Containerisation: Controllers for the robots are de-\\r\\nwhereas inter-robot communications could be marked as best           ployed as Docker [35] containers. Docker provides a\\r\\neffort meaning data may be lost and dropped with no errors           lightweight virtualised environment making it easy to package\\r\\nor retries if the physical channel is unreliable.                    an application and associated dependencies into a container,\\r\\n   However, this enticing vision has yet to be borne out in          which can then be deployed to computation resources using\\r\\nreality. The DDS discovery process generates network traffic         an orchestration framework such as Kubernetes, we use the\\r\\nthat increases approximately O(n) where n is the number of           lightweight implementation K3S8 .\\r\\nparticipants when using UDP multicast, or O(n2 ) when using             Containers only have controlled access to the resources of\\r\\nunicast [29] [30]. A participant is typically a ROS2 process of      the compute node, facilitating security and abstraction - as far\\r\\nwhich there might be many per robot. Complete information            as the container is concerned, it has no knowledge of whether\\r\\nis maintained for all participants at every location, even if they   it has been deployed onto a real robot, or a compute node\\r\\nwill never communicate. Additionally, although the standard          communicating with a simulated environment.\\r\\ndiscovery protocol uses UDP multicast, this is very unreliable          4) Real world: The arena consists of a 5 m x 5 m main\\r\\non WiFi networks [31] forcing the use of unicast discovery.          room, overlooked by an adjacent control room. The main room\\r\\nBuilding robot systems with many mobile agents linked using          is equipped with an OptiTrack [36] motion capture system with\\r\\nwireless communication is becoming more common, and the              8x Flex 13 120Hz cameras for analysis of experiments. There\\r\\nlimitations of ROS2 in this regard have resulted in several          are separate high resolution video cameras. Communications\\r\\npossible solutions. One approach, the discovery server, uses         with the robots are provided with a dedicated 5 GHz WiFi\\r\\na central server to hold information about all participants,         access point and a private 5G base station. A Linux server\\r\\ngreatly reducing traffic. This so far is limited to a single DDS     is used for data collection, Kubernetes orchestration, video\\r\\nvendor, eProsima5 , and is the opposite of the decentralised\\r\\n                                                                       6 https://zenoh.io/\\r\\nvision. Another approach uses bridges between DDS and a\\r\\n                                                                       7 https://www.eclipse.org/\\r\\n  5 https://www.eprosima.com/                                          8 https://k3s.io/\\r\\n\\x0c\\n\\n                                                                                                                                                                  10\\r\\n\\r\\n\\r\\n\\r\\nstreaming, and general experiment management. The arena                                            Server\\r\\nserver and 5G base station are connected to the UMBRELLA                                                              Umbrella\\r\\n\\r\\nnetwork [37] with a fibre link.                                                                                      Arena ROS management\\r\\n   5) Simulation: The standard robot simulator Gazebo is                                              Container\\r\\n                                                                                                                        DDS middleware\\r\\nused. The simulation environment consists of a configurable                                          orchestration\\r\\n\\r\\nmodel of the real arena with various props such as carriers the                                                      Zenoh-DDS Optitrack-\\r\\n                                                                                                                       bridge  ROS bridge\\r\\nrobots can pick up, and obstacles, a Unified Robot Description\\r\\nFormat (URDF) model of the robot and some of its senses, and\\r\\n                                                                     Comms fabric\\r\\na ROS node that provides the same topic API that is present          (WiFi, private 5G, ethernet)\\r\\nin the real robots.\\r\\n   The robot is modelled taking into account the trade-off be-\\r\\ntween speed and fidelity. Rather than modelling the physically\\r\\ncomplex holonomic drive omniwheels we instead modelled               Robot                                           Robot                        Optitrack PC\\r\\n                                                                         Zenoh                                           Zenoh\\r\\nthe motion of the robot as a disc with friction sliding over the         bridge                                          bridge\\r\\n\\r\\nfloor, with a custom plugin using a PID loop to apply force               DDS\\r\\n                                                                       middleware\\r\\n                                                                                                                          DDS\\r\\n                                                                                                                       middleware\\r\\nand torque to the robot body in order meet velocity goals. This            ROS\\r\\n                                                                                     ROS user\\r\\n                                                                                                                          ROS\\r\\n                                                                                                                                    ROS user\\r\\n                                                                                     application                                    application\\r\\nhas the advantage over directly applying velocity of avoiding             nodes\\r\\n                                                                                      container\\r\\n                                                                                                                         nodes\\r\\n                                                                                                                                     container\\r\\n                                                                        Hardware                                        Hardware\\r\\nunphysical effects such as infinite acceleration, and results in        interface                                       interface\\r\\nrealistic behaviour at low computational cost.                           Sensors                                        Sensors\\r\\n                                                                         actuators                                      actuators                     Optitrack\\r\\n   The cameras, time-of-flight proximity sensors, and IMU (In-                                                                                        cameras\\r\\nertial Measurement Unit) are modelled using standard Gazebo\\r\\nROS plugins and the ROS node emulates hardware sensors                                                                Reality\\r\\nsuch as the battery state and controls actuation of the simulated\\r\\nlifting platform, presenting the same interface as the real          Fig. 10: System architecture. Each robot has a stack of low-\\r\\nhardware.                                                            level software interfacing to the sensors and actuators, with\\r\\n   Simulation is not reality - there are always differences. The     some ROS nodes running natively and a Zenoh-DDS bridge\\r\\nreality-gap [38] means that keeping in mind the limitations of       to handle DDS traffic over the arena communications fabric.\\r\\nsimulation when designing robot behaviours that will transfer        Docker containers with ROS user applications are deployed\\r\\nwell to real robots is important. For example, collisions are        by container orchestration from the server, which also handles\\r\\nhard to simulate, and bad for robots, so behaviours relying on       Optitrack, cameras, and Umbrella integration.\\r\\nthem in simulation should be avoided.\\r\\n   6) Online portal for remote experimentation: The physical\\r\\narena and the simulation environment are designed to be              capture system and video from the arena cameras to be\\r\\nuseable remotely. The UMBRELLA Platform, detailed in [37]            streamed to the UMBRELLA cloud system and made available\\r\\nprovides cloud infrastructure to facilitate this. An online portal   to through the online portal.\\r\\nis used for managing experiments through which users may                7) System Architecture: Figure 10 shows the testbed system\\r\\nupload controller containers and simulator and experiment            architecture. Robots in the arena all run a stack of hardware\\r\\nconfiguration files - controlling for example the number and         interface code, housekeeping and low-level ROS nodes that\\r\\nstarting positions or the robots, and download data generated        provide the robot API, and communication bridges. User\\r\\nduring an experimental run.                                          applications or controllers are deployed to the robots as Docker\\r\\n   The user can schedule or queue experiments to be run in           containers using Kubernetes orchestration. A dedicated PC\\r\\nsimulation or on the real robots. Because the real robots could      runs an Optitrack motion capture system. A Linux server\\r\\nbe damaged by collisions, the controller containers used for         PC is responsible for running container orchestration, arena\\r\\nexperiments on them need to be verified in a simulation run          system control, camera, ROS, and Optitrack data capture, and\\r\\nthat checks robot trajectories for dangerous approach velocities     interface to the UMBRELLA Platform.\\r\\nand other potential indicators of hazard. This is an open area\\r\\nof research [39]–[41].                                               C. DOTS integrated development environment\\r\\n   Controller containers for simulation are run on cloud ma-            In order to facilitate wider use of the DOTS system, we cre-\\r\\nchines of the same processor architecture as the robots (ARM         ated a Docker-based development and simulation environment,\\r\\n64 bit), communicating over ROS topics on a virtual network          capable of running on Windows, Linux, and OSX operating\\r\\nwith a server running a Gazebo simulator instance. When the          systems. There is a large barrier to entry in learning to\\r\\ncontroller container has been verified and is to be run on           program ROS-based robots - versions of ROS are closely tied\\r\\nthe real robots, it is passed to the testbed Linux server and        to particular releases of Ubuntu, dependencies are not always\\r\\nqueued to be run by the testbed technician, who ensures the          straightforward to install, custom plugins for the simulator\\r\\nphysical arena is correctly set up and the right number of           need to be compiled, and there are few public examples of\\r\\nrobots positioned.                                                   complete systems.\\r\\n   As well as data collected by the experiment containers,              Because Docker containers can run on multiple operating\\r\\nthe testbed server collects ground truth data from the motion        systems, we can package up the particular version of Ubuntu,\\r\\n\\x0c\\n\\n                                                                                                                                       11\\r\\n\\r\\n\\r\\n\\r\\nROS, and all the difficult to install dependencies and provide                                 Arena width 3.7m\\r\\neasy access to a complete ROS2 DOTS code development\\r\\n                                                                                Search zone 1.85m                     Drop zone 0.6m\\r\\nand simulation environment. All access is provided via web\\r\\ninterfaces using open-source technologies developed for cloud\\r\\napplications. The local file system is mounted within the\\r\\nDocker container.\\r\\n   1) Code-server: Code-server9 takes the popular open-\\r\\nsource Microsoft editor VSCode10 and makes it available via a\\r\\n                                                                                                        y\\r\\nbrowser window. We install this within the Docker container.\\r\\nBy connecting to a particular port on the localhost, a standard\\r\\nVSCode interface is available in the browser. This not only                                                       x\\r\\n\\r\\nallows browsing and editing of files, but also provides a\\r\\nterminal for command line access to the environment.\\r\\n   2) Gazebo: The Gazebo simulator is architecturally split\\r\\ninto gzserver which runs the simulation, and gzclient which\\r\\npresents the standard user interface on a Linux desktop. We\\r\\nmake available GZWeb, which is another client for gzserver\\r\\nthat presents a browser-based interface using WebGL [42].\\r\\nThis allows browser access and is more performant than using                        carriers                            robots\\r\\nthe standard interface via VNC.\\r\\n   3) VNC: VNC is a technology for allowing remote desk-             Fig. 11: Intralogistics task, robots starting on the right must\\r\\ntop access. We run a virtual framebuffer within the Docker           search for and pick up carriers in the search zone on the left.\\r\\ncontainer together with a simple window manager and a                Once picked up, the carriers must be moved to the drop zone\\r\\nstandard VNC server, and use the noVNC11 client to make              and deposited.\\r\\nthis available through a browser window. This gives access to\\r\\na standard Linux desktop and allows the use of other graphical\\r\\napplications such as the ROS data visualiser rviz.\\r\\n   These three applications packaged in a Docker container\\r\\n                                                                     described in Section III-A4c but could use, for example, the\\r\\nenable rapid access to a complete DOTS development and\\r\\n                                                                     magnetometer and Bluetooth beacons.\\r\\nsimulation environment in a platform agnostic way. Once a\\r\\ncontroller application has been prototyped, it can be converted         Although conceptually simple, for a robot to detect, ma-\\r\\ninto a Docker container for upload to the UMBRELLA online            noeuvre underneath a carrier and position itself correctly in the\\r\\nportal to be verified and for deployment to the real robots.         centre, then lift it, is non-trivial. Many swarm foraging tasks\\r\\n                                                                     demonstrated with real robots have used virtual objects [44],\\r\\n                              IV. R ESULTS                           objects that can be moved just by collision and pushing [45],\\r\\nA. Intralogistics use case                                           or have an actuator that mates with a particular structure [46].\\r\\n   In order to demonstrate the whole system, we implemented          These approaches abstract the difficulty of an arbitrary object\\r\\na conceptually simple swarm intralogistics task. Imagine a           collection task to various extents. We argue the system we\\r\\ncloakroom, similar to the scenario described in [43], where          demonstrate here is closer to real-world practical use because\\r\\nusers can deposit and collect bags and jackets, and a swarm          the location and lifting of the carriers require the sorts of\\r\\nof robots will move these to and from a storage area. It is          sensory processing and navigation typical of many non-trivial\\r\\npossible to perform this search and retrieval task in an entirely    robotics tasks.\\r\\ndistributed, decentralised manner. Here we implement one                We develop using the pipeline shown in Figure 2. The\\r\\naspect of the task, item retrieval, in this decentralised fashion.   DOTS integrated development environment is used to write\\r\\n   The arena, shown in Figure 17, has two regions, the search        controller code, in a mixture of Python and C++, which can be\\r\\nzone on the left-hand side, everywhere with x < 0, and the           quickly iterated to run on the local Gazebo simulator. Simple\\r\\ndrop zone on the rightmost 0.6m, everywhere with x > 1.25.           sub-behaviours are built up into the complete controller until\\r\\nRobots start randomly placed in the drop zone, and carriers          multiple robots are running successfully in simulation. Once\\r\\nin the search zone. The task is for the swarm to find and            satisfied with simulation the controllers are packaged into\\r\\nmove the carriers into the drop zone. Apart from the earlier         Docker containers for validation on the remote UMBRELLA\\r\\ndescribed sensors, we make available two synthetic senses: a         cloud service portal. This allows both larger simulations, with\\r\\ncompass that gives the absolute orientation of a robot, and          resources limited only by cloud availability, and state-of-the-\\r\\na zone sense, indicating if the robot is in either of the two        art validation of the simulated controller for safety. Once\\r\\nzones. Currently these are derived from the visual localisation      validated, the controller may be deployed onto the real robots\\r\\n  9 https://github.com/cdr/code-server                               in the testbed for an experiment run. During the run, multiple\\r\\n  10 https://code.visualstudio.com/                                  data sources are captured and these are then made available\\r\\n  11 https://novnc.com/info.html                                     for download from the portal.\\r\\n\\x0c\\n\\n                                                                                                                                                12\\r\\n\\r\\n\\r\\n\\r\\n                                                                                collision is detected, a new direction is chosen and the robot\\r\\n                                                                                moves in that direction at 0.5ms−1 . The direction chosen is\\r\\n                                                                                randomly picked from a Gaussian distribution:\\r\\n                                                                                                          (\\r\\n                                                                                                            σ = 3.0 if x ∈ searchzone\\r\\n                                                                                   θ ∼ N (π, σ 2 ) where                                   (1)\\r\\n                                                                                                            σ = 1.0 otherwise\\r\\n                                                        Take to                 This has probability p ≈ 0.9 of moving in −x direction when\\r\\n              Explore       Pick up\\r\\n                                                       drop zone                outside the search zone, and p ≈ 0.6 when in the search zone,\\r\\n                                                                                so robots will tend to move towards the search zone then\\r\\nFig. 12: Top level of Behaviour Tree controller. Robots explore                 randomly within it. The robot continues in the same direction\\r\\nuntil they find a carrier, the pick up the carrier, then take to                until another potential collision is detected. If, at any point, a\\r\\ndrop zone the carrier.                                                          carrier ID fiducial (on the four sides), or a carrier markermap\\r\\n                                                                                (on the underneath) are detected, the robot motion is stopped\\r\\n                                                                                and the explore behaviour returns success, moving to the pick\\r\\n                                    cfiducial\\r\\n                                                        carrier                 up behaviour, otherwise it returns running.\\r\\n                                                                                     b) Pick up: This behaviour is started if the robot has seen\\r\\n                                                                                an ID or markermap. In order of priority, the robot will try to\\r\\n                                                dock\\r\\n                                                                    z           move to the centre position under the carrier if a markermap\\r\\n              base_link\\r\\n                                                                        y\\r\\n                                                                                has been seen, or move underneath the carrier to the dock\\r\\n                          predock                                 map\\r\\n                                                                                (see frame diagram, Figure 13) position if an ID has been\\r\\n                                                                            x\\r\\n       odom                                                                     seen and robot is close to predock position or move to the\\r\\n                                                                                predock position if an ID has been seen. If the centre is reached\\r\\n                                                                                successfully, the lifting platform is raised and the behaviour\\r\\nFig. 13: Frames in use within the testbed. map is fixed to                      returns success and starts the take to drop zone behaviour.\\r\\nthe arena, odom to the robot starting position. base link is                    If the vision system loses track of the fiducial ID or the\\r\\nthe robot frame, with +x being forward. Each carrier has four                   markermap, the behaviour returns failure, otherwise it returns\\r\\nfiducials around the sides, with frame cfiducial, and the carrier               running. The consequence of failure is reversion to the explore\\r\\nas a whole has the frame carrier, centred in the markermap on                   behaviour.\\r\\nits underside. predock and dock are frames relative to cfiducial                     c) Take to drop zone: This behaviour is started if the\\r\\nthat the robot uses when navigating under a carrier.                            pick up behaviour has succeeded. It is similar to the explore\\r\\n                                                                                behaviour but biassed to move towards the drop zone. Move-\\r\\n                                                                                ment is slower at 0.3ms−1 and collision detection suited to\\r\\nB. Robot behaviour                                                              the situation where the legs of the carrier can obscure some\\r\\n   We use a Behaviour Tree controller [47]–[51]. The top                        of the IRToF sensors. This behaviour will return running until\\r\\nlevel is shown in Figure 12. There are a sequence of actions                    the robot reaches the drop zone with no collision detection,\\r\\nthat are performed, once each action has been performed                         at which point it will lower the lifting platform and return\\r\\nsuccessfully, the next is started. Firstly, the robot explores the              success and revert to explore behaviour.\\r\\nenvironment, searching for a cargo carrier. If one is found, the                   2) Cargo carrier detection: Carriers have an ArUco fiducial\\r\\npick up behaviour is started. This involves manoeuvring under                   marker on each side, labelled cfiducial in Figure 13. These\\r\\nthe carrier, positioning itself centrally, then raising the lifting             markers has an ID unique to the carrier, and can be detected\\r\\nplatform. Finally, the robot does take to drop zone, moving in                  by the perimeter cameras of the robot from about 1m away. On\\r\\nthe direction of the nest region and once there, lowering the                   the underside of the carrier is a markermap, shown in Figure\\r\\nlifting platform. If there are any failures, or once a robot has                14, so the robot can locate the centre of the carrier using its\\r\\nsucceeded in all tasks, the behaviour reverts to explore.                       upwards-facing camera. The size and density of the fiducials\\r\\n   1) Behaviour trees: Behaviour Trees are hierarchical struc-                  making up the map are chosen such that there are always at\\r\\ntures that combine leaf nodes that interact with the environ-                   least one complete marker visible in the camera field-of-view\\r\\nment into subtrees of progressively more complex behaviours                     for all physically possible positions of the robot under the\\r\\nusing various composition nodes such as sequence and selec-                     carrier.\\r\\ntor, see [52] for more information. The whole tree is ticked at a                  When a carrier fiducial has been seen by any camera, the\\r\\nregular rate, in this case 10Hz, corresponding to the controller                transform from the robot to that fiducial is made available on\\r\\nupdate rate, with nodes returning success, failure, or running.                 the blackboard. The detection system then focusses on that\\r\\nA blackboard is used to interface between the tree and the                      ID for a short time (3s) and ignores all other IDs, this is to\\r\\nrobot, holding conditioned and abstracted senses and means of                   prevent flipping of attention when multiple IDs are visible. The\\r\\nactuation. The three top-level behaviours are described below                   fiducial transform can be used by the controller to navigate to\\r\\nin more detail.                                                                 the predock and dock locations relative to the carrier.\\r\\n      a) Explore: The explore behaviour sends the robot on a                       When a carrier markermap is visible, the transform to the\\r\\nballistic random walk. At the start, or whenever a potential                    centre of the carrier is made available on the blackboard. This\\r\\n\\x0c\\n\\n                                                                                                                                      13\\r\\n\\r\\n\\r\\n\\r\\n                                                                             TABLE V: Performance in intralogistics task\\r\\n                                                                                     Run    Carriers retrieved   Time (s)\\r\\n                                                                                      1             5              136\\r\\n                                                                                      2             5               99\\r\\n                                                                                      3             5              213\\r\\n                                                                                      4             5              116\\r\\n                                                                                      5             5              327\\r\\n                                                                                                    x̄             178\\r\\n\\r\\n\\r\\n\\r\\n                                                                        Static objects result in corresponding cells in the map\\r\\n                                                                     reaching the upper limit of 1.0, transient or false returns tend\\r\\n                                                                     towards zero.\\r\\n                                                                        Collisions are predicted by projecting the current velocity\\r\\n                                                                     vector forward by a lookahead time, then taking the average\\r\\n                                                                     value of the cell contents over a circular area related to the\\r\\n                                                                     robot size. The scalar value is a collision likelihood which is\\r\\nFig. 14: Underside of a carrier, showing the markermap and           made available on the blackboard.\\r\\nthe frame. Legs in blue, and the grey areas show regions where          4) Navigation: The various behaviours need to navigate\\r\\nthe centreline of the upwards-facing camera cannot physically        and choose a path to follow. All navigation is performed in the\\r\\nreach.                                                               local frame of the robot - there is no global map. For example,\\r\\n                                                                     when exploring, a direction is chosen as described above, then\\r\\n                                                                     a lower-level navigation behaviour is started that creates a\\r\\n                                                                     trajectory, then continually monitors the collision map of the\\r\\n                                                                     locality to abort the trajectory playout if necessary. If there are\\r\\n                                                                     no paths available that don’t trigger a collision warning, for\\r\\n                                                                     example, if a robot is under a carrier, then a fallback behaviour\\r\\n                                                                     of picking a least-worst (lowest sum of nearby collision cells)\\r\\n                                                                     direction and moving slowly for short distances is used to\\r\\n                                                                     safely emerge into more open space.\\r\\n\\r\\n\\r\\nFig. 15: Example collision map as visualised by Rviz, with a         C. Performance\\r\\nframe indicator at the location of the robot, and the grey area         We ran the task five times, in each case the five robots were\\r\\nthe robot-relative map, showing two of the arena walls.              positioned with random orientation in the drop zone, and the\\r\\n                                                                     carriers in the search zone. Robots were allowed to run until\\r\\n                                                                     the task was completed and all robots had escaped the drop\\r\\ncan be used to navigate the robot to correct central location        zone, or more than 10 minutes elapsed. The minimum time\\r\\nunder the carrier in order to safely lift it.                        the task could take, assuming each robot goes directly to the\\r\\n   3) Collision: Information from the IRToF sensors is used          nearest carrier, the distance from robot to carrier is 3 m, search\\r\\nto build a dynamic robot-relative map of obstacles. The map is       speed is 0.5 ms−1 , carry speed 0.3 ms−1 , pickup and drop times\\r\\nan array of cells, centred on the robot, with values in the range    5 s, is 26 s Obviously, this assumes each robot moves directly\\r\\n[0, 1]. The value of a cell continually reduces by exponential       underneath a separate carrier, there are no collisions, and they\\r\\ndecay rate λ, and is increased when a cell is affected by            move directly to the drop zone.\\r\\na sensor return. Sensor returns increase a cell location by             Table V shows the results. In every case, the swarm was\\r\\na Gaussian based on distance between each return location            able to successfully retrieve all the carriers within the allowed\\r\\n(x0i , yi0 ) and cell location (x, y):                               10 minutes. The average time to complete the task was 178\\r\\n                             q                                       seconds. All the runs are shown in the Supplementary video\\r\\n                        ri = (x0i − x)2 + (yi0 − y)2           (2)   material logistics task runs.\\r\\n                          n      r2\\r\\n                          X  − 2 i\\r\\n              S(x, y) =     e 2σsensor                        (3)                           V. C ONCLUSION\\r\\n                          i=1\\r\\n                                                                       In this work, we have introduced DOTS, our new open\\r\\nwhere S(x, y) is the sensor return function, showing the total       access industrial swarm testbed. With a purpose-build arena,\\r\\neffect all n sensor returns have at this location.                   20 high specification robots that move fast, have long en-\\r\\n   Each map location M (x, y) changes over time as:                  durance and high computational power, a platform-agnostic\\r\\n                                                                     development environment, and a cloud portal, this system\\r\\n Mt+∆t (x, y) = min(Mt (x, y)(1 − λ∆t) + S(x, y), 1.0) (4)           breaks new ground in enabling experimentation.\\r\\n                                                                       The intralogistics scenario demonstrates the abilities of\\r\\nwith the decay rate λ = 1s, and σsensor = 25mm.                      the robot swarm to successfully complete a non-trivial task.\\r\\n\\x0c\\n\\n                                                                                                                                                  14\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                       A                                        B                                  C\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                       D                                        E                                   F\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    Fig. 16: Sequence showing a robot picking up a carrier. (A) Overall View. (B) While exploring, a robot detects a carrier fiducial\\r\\n    (blue arrow) and starts the pick up behaviour. It moves to the pre-dock position (orange arrow). (C) The robot is now facing\\r\\n    the carrier and is in the optimal position for accurate pose estimation of the carrier fiducial so (D) moves under the carrier and\\r\\n    (E) centres itself using upwards-facing camera and raises the lifting platform. Finally (F) the robot moves towards the drop\\r\\n    zone.\\r\\n\\r\\n\\r\\n                              Robot paths during logistics task                 and deposit multiple cargo carriers, is considerable. More\\r\\n                                                                      Robot 1   importantly, we demonstrate a process. The DOTS Testbed\\r\\n                 1.5                                                  Robot 2   pipeline reduces barriers to entry by making it easy to get\\r\\n                                                                      Robot 3\\r\\n                                                                      Robot 4   started, using the cross-platform IDE to experiment with ideas\\r\\n                                                                      Robot 5   in simulation, then making available a real physical robot\\r\\n                 1.0\\r\\n                                                                                swarm to see these ideas translated into reality.\\r\\n                                                                                   There are many directions this research can go in. Currently,\\r\\n                 0.5                                                            there is necessarily some manual management of the robots,\\r\\n                                                                                they need to be manually charged, set up for experimental runs,\\r\\nY position (m)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                and watched over. We are currently designing automatic charg-\\r\\n                 0.0\\r\\n                                                                                ing stations to aid this. Validation of controllers in simulation\\r\\n                                                                                is difficult, although the simulator is relatively high-fidelity, it\\r\\n                 0.5                                                            still suffers from differences to reality, and it is not possible\\r\\n                                                                                to guarantee the safety of a general-purpose controller; even\\r\\n                                                                                defining safety for a swarm is an open question. At the\\r\\n                 1.0                                                            moment, we are not utilising the full processing power of the\\r\\n                                                                                robotic platform - we intend to accelerate image processing,\\r\\n                 1.5                                                            moving it onto the GPUs, both on the single board computer\\r\\n                                                                                and on the peripheral Raspberry Pi Zeros. We look forward\\r\\n                       1.5   1.0     0.5        0.0       0.5   1.0    1.5      to exploring the possibilities for localisation and short-range\\r\\n                                           X position (m)                       communication of the BLE5 and UWE radios, and the two\\r\\n    Fig. 17: Tracks of the robots in one run of the logistics task,             microphones. The possibilities for decentralised stochastic\\r\\n    with thicker tracks when a robot is carrying a load. loads are              swarm algorithms to reliably perform logistics tasks is touched\\r\\n    carried from left to right. In this example, robot 3 never picks            upon here, and is open to further work. Designing toolkits of\\r\\n    up a load.                                                                  useful, working, and validated sub-behaviours from which to\\r\\n                                                                                more easily construct full applications is another step towards\\r\\n                                                                                wider real-world use of swarm techniques.\\r\\n                                                                                   Building swarm robotic applications for the real world is\\r\\n    Although conceptually simple, the underlying processing nec-                hard, and one of the biggest obstacles is the lack of actually\\r\\n    essary to handle the five concurrent video streams, time-of-                existing robot swarms with the types of capabilities necessary\\r\\n    flight pointcloud, and other senses, and entirely autonomously              to experiment with ideas. By making available such a swarm,\\r\\n    search for, find, manoeuvre under with precision, lift, carry               together with an accessible development pipeline, we hope to\\r\\n\\x0c\\n\\n                                                                                                                                                               15\\r\\n\\r\\n\\r\\n\\r\\nreduce the barriers to entry and stimulate further research.                      [16] S. Su, F. Heide, R. Swanson, J. Klein, C. Callenberg, M. Hullin, and\\r\\n                                                                                       W. Heidrich, “Material classification using raw time-of-flight measure-\\r\\n                                                                                       ments,” in Proceedings of the IEEE Conference on Computer Vision and\\r\\n                         ACKNOWLEDGMENT                                                Pattern Recognition, 2016, pp. 3503–3511.\\r\\n                                                                                  [17] C. Callenberg, Z. Shi, F. Heide, and M. B. Hullin, “Low-cost spad\\r\\n  S.J. was supported by EPSRC DTP Doctoral Prize Award                                 sensing for non-line-of-sight tracking, material classification and depth\\r\\n                                                                                       imaging,” ACM Transactions on Graphics (TOG), vol. 40, no. 4, pp.\\r\\nEP/T517872/1. S.J. and S.H. were supported by EPSRC IAA                                1–12, 2021.\\r\\nAward EP/R511663/1. E.M. was supported by the EPSRC                               [18] R. Rojas and A. G. Förster, “Holonomic control of a robot with an\\r\\nCentre for Doctoral Training in Future Autonomous and                                  omnidirectional drive,” KI-Künstliche Intelligenz, vol. 20, no. 2, pp. 12–\\r\\n                                                                                       17, 2006.\\r\\nRobotic Systems (FARSCOPE) and an EPSRC ICASE Award.                              [19] B. Katz, J. Di Carlo, and S. Kim, “Mini cheetah: A platform for\\r\\nToshiba Bristol Research and Innovation Laboratory (BRIL)                              pushing the limits of dynamic quadruped control,” in 2019 international\\r\\nprovided additional support.                                                           conference on robotics and automation (ICRA). IEEE, 2019, pp. 6295–\\r\\n                                                                                       6301.\\r\\n                                                                                  [20] U. H. Lee, C.-W. Pan, and E. J. Rouse, “Empirical characterization of\\r\\n                                                                                       a high-performance exterior-rotor type brushless dc motor and drive,”\\r\\n                              R EFERENCES                                              in 2019 IEEE/RSJ International Conference on Intelligent Robots and\\r\\n                                                                                       Systems (IROS). IEEE, 2019, pp. 8018–8025.\\r\\n [1] L. Custodio and R. Machado, “Flexible automated warehouse: a liter-          [21] F. Grimminger, A. Meduri, M. Khadiv, J. Viereck, M. Wüthrich,\\r\\n     ature review and an innovative framework,” The International Journal              M. Naveau, V. Berenz, S. Heim, F. Widmaier, T. Flayols et al., “An\\r\\n     of Advanced Manufacturing Technology, vol. 106, no. 1, pp. 533–558,               open torque-controlled modular robot architecture for legged locomotion\\r\\n     2020.                                                                             research,” IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.\\r\\n [2] Y. Jaghbeer, R. Hanson, and M. I. Johansson, “Automated order picking             3650–3657, 2020.\\r\\n     systems and the links between design and performance: a systematic           [22] N. Kau, A. Schultz, N. Ferrante, and P. Slade, “Stanford doggo: An open-\\r\\n     literature review,” International Journal of Production Research, vol. 58,        source, quasi-direct-drive quadruped,” in 2019 International conference\\r\\n     no. 15, pp. 4489–4505, 2020.                                                      on robotics and automation (ICRA). IEEE, 2019, pp. 6309–6315.\\r\\n [3] I. Draganjac, D. Miklić, Z. Kovačić, G. Vasiljević, and S. Bogdan,       [23] P. Dini and S. Saponara, “Cogging torque reduction in brushless motors\\r\\n     “Decentralized control of multi-agv systems in autonomous warehousing             by a nonlinear control technique,” Energies, vol. 12, no. 11, p. 2224,\\r\\n     applications,” IEEE Transactions on Automation Science and Engineer-              2019.\\r\\n     ing, vol. 13, no. 4, pp. 1433–1447, 2016.                                    [24] L. Berscheid and T. Kröger, “Jerk-limited real-time trajectory generation\\r\\n [4] G. Fragapane, R. de Koster, F. Sgarbossa, and J. O. Strandhagen,                  with arbitrary target states,” arXiv preprint arXiv:2105.04830, 2021.\\r\\n     “Planning and control of autonomous mobile robots for intralogistics:        [25] D. Thomas, W. Woodall, and E. Fernandez, “Next-generation\\r\\n     Literature review and research agenda,” European Journal of Opera-                ROS: Building on DDS,” in ROSCon Chicago 2014. Mountain\\r\\n     tional Research, 2021.                                                            View, CA: Open Robotics, sep 2014. [Online]. Available: https:\\r\\n [5] E. Şahin, “Swarm robotics: From sources of inspiration to domains of             //vimeo.com/106992622\\r\\n     application,” in International Workshop on Swarm robotics (SR 2004),         [26] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,\\r\\n     S. W. Şahin E., Ed. Santa Monica, CA, USA: Springer, 2005, pp.                   R. Wheeler, and A. Y. Ng, “ROS: an open-source Robot Operating\\r\\n     10–20.                                                                            System,” in IEEE International Conference on Robotics and Automation\\r\\n [6] M. Schranz, G. A. Di Caro, T. Schmickl, W. Elmenreich, F. Arvin,                  (ICRA 2009) Workshop on Open Source Robotics, vol. 3, no. 3.2. Kobe,\\r\\n     A. Şekercioğlu, and M. Sende, “Swarm intelligence and cyber-physical            Japan: IEEE, 2009, p. 5.\\r\\n     systems: concepts, challenges and future trends,” Swarm and Evolution-       [27] S. H. Juan and F. H. Cotarelo, “Multi-master ros systems,” Institut de\\r\\n     ary Computation, vol. 60, p. 100762, 2021.                                        Robotics and Industrial Informatics, pp. 1–18, 2015.\\r\\n [7] D. Pickem, P. Glotfelter, L. Wang, M. Mote, A. Ames, E. Feron, and           [28] G. Pardo-Castellote, “Omg data-distribution service: Architectural\\r\\n     M. Egerstedt, “The robotarium: A remotely accessible swarm robotics               overview,” in 23rd International Conference on Distributed Computing\\r\\n     research testbed,” in 2017 IEEE International Conference on Robotics              Systems Workshops, 2003. Proceedings. IEEE, 2003, pp. 200–206.\\r\\n     and Automation (ICRA). IEEE, 2017, pp. 1699–1706.                            [29] K. An, A. Gokhale, D. Schmidt, S. Tambe, P. Pazandak, and G. Pardo-\\r\\n [8] L. Paull, J. Tani, H. Ahn, J. Alonso-Mora, L. Carlone, M. Cap, Y. F.              Castellote, “Content-based filtering discovery protocol (cfdp) scalable\\r\\n     Chen, C. Choi, J. Dusek, Y. Fang et al., “Duckietown: an open,                    and efficient omg dds discovery protocol,” in Proceedings of the 8th\\r\\n     inexpensive and flexible platform for autonomy education and research,”           ACM International Conference on Distributed Event-Based Systems,\\r\\n     in 2017 IEEE International Conference on Robotics and Automation                  2014, pp. 130–141.\\r\\n     (ICRA). IEEE, 2017, pp. 1497–1504.                                           [30] J. Sanchez-Monedero, J. Povedano-Molina, J. M. Lopez-Vega, and\\r\\n [9] M. ten Hompel, H. Bayhan, J. Behling, L. Benkenstein, J. Emmerich,                J. M. Lopez-Soler, “Bloom filter-based discovery protocol for dds\\r\\n     G. Follert, M. Grzenia, C. Hammermeister, H. Hasse, D. Hoening                    middleware,” Journal of Parallel and Distributed Computing, vol. 71,\\r\\n     et al., “Technical report: Loadrunner®, a new platform approach                   no. 10, pp. 1305–1317, 2011.\\r\\n     on collaborative logistics services,” Logistics Journal: nicht referierte    [31] W. Kumari, C. E. Perkins, M. McBride, D. Stanley, and J. C. Zúñiga,\\r\\n     Veröffentlichungen, vol. 2020, no. 10, 2020.                                     “Rfc 9119-multicast considerations over ieee 802 wireless media,” 2021.\\r\\n[10] M. Williams, “Low pin-count debug interfaces for multi-device sys-           [32] Eclipse Foundation. (2021) Minimizing discovery overhead in ROS2.\\r\\n     tems,” 2009.                                                                      [Online]. Available: https://zenoh.io/blog/2021-03-23-discovery/\\r\\n[11] R. Mur-Artal and J. D. Tardós, “Orb-slam2: An open-source slam              [33] W. Meeussen. (2010, October) Coordinate frames for mobile platforms.\\r\\n     system for monocular, stereo, and rgb-d cameras,” IEEE transactions               [Online]. Available: https://ros.org/reps/rep-0105.html\\r\\n     on robotics, vol. 33, no. 5, pp. 1255–1262, 2017.                            [34] T. Foote, “tf: The transform library,” in 2013 IEEE Conference on\\r\\n[12] F. J. Romero-Ramirez, R. Muñoz-Salinas, and R. Medina-Carnicer,                  Technologies for Practical Robot Applications (TePRA). IEEE, 2013,\\r\\n     “Speeded up detection of squared fiducial markers,” Image and vision              pp. 1–6.\\r\\n     Computing, vol. 76, pp. 38–47, 2018.                                         [35] D. Merkel, “Docker: lightweight linux containers for consistent devel-\\r\\n[13] C. Bachhuber, E. Steinbach, M. Freundl, and M. Reisslein, “On the                 opment and deployment,” Linux journal, vol. 2014, no. 239, p. 2, 2014.\\r\\n     minimization of glass-to-glass and glass-to-algorithm delay in video         [36] NaturalPoint, Inc. DBA OptiTrack. (2022) Motion capture systems.\\r\\n     communication,” IEEE Transactions on Multimedia, vol. 20, no. 1, pp.              [Online]. Available: https://optitrack.com/\\r\\n     238–252, 2017.                                                               [37] T. Farnham, S. Jones, A. Aijaz, Y. Jin, I. Mavromatis, U. Raza,\\r\\n[14] C. Bachhuber and E. Steinbach, “Are today’s video communication                   A. Portelli, A. Stanoev, and M. Sooriyabandara, “Umbrella collaborative\\r\\n     solutions ready for the tactile internet?” in 2017 IEEE Wireless Com-             robotics testbed and iot platform,” in 2021 IEEE 18th Annual Consumer\\r\\n     munications and Networking Conference Workshops (WCNCW). IEEE,                    Communications & Networking Conference (CCNC). IEEE, 2021, pp.\\r\\n     2017, pp. 1–6.                                                                    1–7.\\r\\n[15] T. Moore and D. Stouch, “A generalized extended kalman filter imple-         [38] J.-B. Mouret and K. Chatzilygeroudis, “20 years of reality gap: a\\r\\n     mentation for the robot operating system,” in Proceedings of the 13th             few thoughts about simulators in evolutionary robotics,” in Workshop\\r\\n     International Conference on Intelligent Autonomous Systems (IAS-13).              ”Simulation in Evolutionary Robotics”, Genetic and Evolutionary Com-\\r\\n     Springer, July 2014.                                                              putation Conference (GECCO 2017). Berlin, Germany: ACM, 2017.\\r\\n\\x0c\\n\\n                                                                                 16\\r\\n\\r\\n\\r\\n\\r\\n[39] D. Pickem, L. Wang, P. Glotfelter, Y. Diaz-Mercado, M. Mote, A. Ames,\\r\\n     E. Feron, and M. Egerstedt, “Safe, remote-access swarm robotics re-\\r\\n     search on the robotarium,” arXiv preprint arXiv:1604.00640, 2016.\\r\\n[40] G. Beltrame, E. Merlo, J. Panerati, and C. Pinciroli, “Engineering safety\\r\\n     in swarm robotics,” in Proceedings of the 1st International Workshop\\r\\n     on Robotics Software Engineering, 2018, pp. 36–39.\\r\\n[41] K. I. Eder, W.-l. Huang, and J. Peleska, “Complete agent-driven\\r\\n     model-based system testing for autonomous systems,” arXiv preprint\\r\\n     arXiv:2110.12586, 2021.\\r\\n[42] Khronos Group, WebGL 1.0 Specification, Khronos Group, 2011.\\r\\n[43] S. Jones, E. Milner, M. Sooriyabandara, and S. Hauert, “Distributed\\r\\n     situational awareness in robot swarms,” Advanced Intelligent Systems,\\r\\n     vol. 2, no. 11, p. 2000110, 2020.\\r\\n[44] L. Pitonakova, R. Crowder, and S. Bullock, “Information exchange\\r\\n     design patterns for robot swarm foraging and their application in robot\\r\\n     control algorithms,” Frontiers in Robotics and AI, vol. 5, p. 47, 2018.\\r\\n[45] S. Jones, A. F. Winfield, S. Hauert, and M. Studley, “Onboard evolution\\r\\n     of understandable swarm behaviors,” Advanced Intelligent Systems,\\r\\n     vol. 1, 2019.\\r\\n[46] M. Dorigo, D. Floreano, L. M. Gambardella, F. Mondada, S. Nolfi,\\r\\n     T. Baaboura, M. Birattari, M. Bonani, M. Brambilla, A. Brutschy et al.,\\r\\n     “Swarmanoid: a novel concept for the study of heterogeneous robotic\\r\\n     swarms,” IEEE Robotics & Automation Magazine, vol. 20, no. 4, pp.\\r\\n     60–71, 2013.\\r\\n[47] A. Champandard, “Behavior trees for next-gen game AI,” in Game\\r\\n     developers conference, audio lecture, 2007.\\r\\n[48] P. Ogren, “Increasing modularity of UAV control systems using com-\\r\\n     puter game behavior trees,” in AIAA Guidance, Navigation and Control\\r\\n     Conference. Minneapolis, MN, USA: AIAA, 2012.\\r\\n[49] A. Klöckner, “Behavior trees for uav mission management.” in IN-\\r\\n     FORMATIK 2013 Informatik angepasst an Mensch, Organisation und\\r\\n     Umwelt. Koblenz, Germany: Springer, 2013, pp. 57–68.\\r\\n[50] S. Jones, M. Studley, S. Hauert, and A. F. Winfield, “Evolving be-\\r\\n     haviour trees for swarm robotics,” in 13th International Symposium\\r\\n     on Distributed Autonomous Robotic Systems (DARS 2016), R. Groß,\\r\\n     A. Kolling, S. Berman, E. Frazzoli, A. Martinoli, F. Matsuno, and\\r\\n     M. Gauci, Eds. London, UK: Springer, 2016.\\r\\n[51] M. Colledanchise, “Behavior trees in robotics,” 2017.\\r\\n[52] A. Marzinotto, M. Colledanchise, C. Smith, and P. Ogren, “Towards a\\r\\n     unified behavior trees framework for robot control,” in IEEE Interna-\\r\\n     tional Conference on Robotics and Automation (ICRA 2014). Hong\\r\\n     Kong, China: IEEE, 2014, pp. 5420–5427.\\r\\n\\x0c']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the text into one string\n",
    "\n",
    "array_pdf_text=[]\n",
    "def pdf_to_text():\n",
    "    for i in array_link:\n",
    "        array_pdf_text.append(\"\\n\\n\".join(load_pdf(i[21:])))\n",
    "    return array_pdf_text\n",
    "pdf_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc82c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ziqian.Bai', 'Timur.Bagautdinov', 'Javier.Romero', 'Michael.Zollh.fer', 'Ping.Tan', 'Shunsuke.Saito']\n",
      "                AutoAvatar: Autoregressive Neural Fields\r\n",
      "                     for Dynamic Avatar Modeling\r\n",
      "\r\n",
      "        Ziqian Bai1,2∗        Timur Bagautdinov2 Javier Romero2             Michael Zollhöfer2\r\n",
      "                                    Ping Tan1 Shunsuke Saito2\r\n",
      "                         1                              2\r\n",
      "                             Simon Fraser University        Reality Labs Research\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "           Abstract. Neural fields such as implicit surfaces have recently enabled avatar\r\n",
      "           modeling from raw scans without explicit temporal correspondences. In this work,\r\n",
      "           we exploit autoregressive modeling to further extend this notion to capture dynamic\r\n",
      "           effects, such as soft-tissue deformations. Although autoregressive models are\r\n",
      "           naturally capable of handling dynamics, it is non-trivial to apply them to implicit\r\n",
      "           representations, as explicit state decoding is infeasible due to prohibitive memory\r\n",
      "           requirements. In this work, for the first time, we enable autoregressive modeling of\r\n",
      "           implicit avatars. To reduce the memory bottleneck and efficiently model dynamic\r\n",
      "           implicit surfaces, we introduce the notion of articulated observer points, which\r\n",
      "           relate implicit states to the explicit surface of a parametric human body model.\r\n",
      "           We demonstrate that encoding implicit surfaces as a set of height fields defined on\r\n",
      "           articulated observer points leads to significantly better generalization compared\r\n",
      "           to a latent representation. The experiments show that our approach outperforms\r\n",
      "           the state of the art, achieving plausible dynamic deformations even for unseen\r\n",
      "           motions. https://zqbai-jeremy.github.io/autoavatar.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "1       Introduction\r\n",
      "Animatable 3D human body models are key enablers for various applications ranging\r\n",
      "from virtual try-on to social telepresence [4]. While modeling of human avatars from\r\n",
      "3D scans without surface registration is gaining more and more attention in recent\r\n",
      "years [43,24,48,8,26], complex temporal dynamics are often completely ignored and the\r\n",
      "resulting deformations are often treated exclusively as a function of the pose parameters.\r\n",
      "However, the body shape is not uniquely determined by the current pose of the human,\r\n",
      "but also depends on the history of shape deformations due to secondary motion effects.\r\n",
      "The goal of our work is to realistically model these history-dependent dynamic effects\r\n",
      "for human bodies without requiring precise surface registration.\r\n",
      "    To this end, we propose AutoAvatar, a novel autoregressive model for dynamically\r\n",
      "deforming human bodies. AutoAvatar models body geometry implicitly - using a signed\r\n",
      "distance field (SDF) - and is able to directly learn from raw scans without requiring\r\n",
      "temporal correspondences for supervision. In addition, akin to physics-based simulation,\r\n",
      "AutoAvatar infers the complete shape of an avatar given history of shape and motion. The\r\n",
      "    ∗\r\n",
      "        Work done while Ziqian Bai was an intern at Reality Labs Research, Pittsburgh, PA, USA.\r\n",
      "\f",
      "\n",
      "\n",
      "2        Z. Bai et al.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Fig. 1: AutoAvatar. Given raw 4D scans with self-intersections, holes, and noise (grey meshes)\r\n",
      "and fitted SMPL models (blue meshes), AutoAvatar automatically learns highly detailed animatable\r\n",
      "body models with plausible secondary motion dynamics without requiring a personalized template\r\n",
      "or surface registration (right).\r\n",
      "\r\n",
      "aforementioned properties lead to a generalizable method that models complex dynamic\r\n",
      "effects including inertia and elastic deformations without requiring a personalized\r\n",
      "template or precise temporal correspondences across training frames.\r\n",
      "    To model temporal dependencies in the data, prior work has typically resorted to\r\n",
      "autoregressive models [39,22,28,45]. While the autoregressive framework naturally\r\n",
      "allows for incorporation of temporal information, combining it with neural implicit\r\n",
      "surface representations [29,36,9] for modeling human bodies is non-trivial. Unlike\r\n",
      "explicit shape representations, such neural representations implicitly encode the shape in\r\n",
      "the parameters of the neural network and latent codes. Thus, in practice, producing the\r\n",
      "actual shape requires expensive neural network evaluation at each voxel of a dense spatial\r\n",
      "grid [36]. This aspect is particularly problematic for autoregressive modeling, since\r\n",
      "most of the successful autoregressive models rely on rollout training [28,19] to ensure\r\n",
      "stability of both training and inference. Unfortunately, rollout training requires multiple\r\n",
      "evaluations of the model for each time step, and thus becomes prohibitively expensive\r\n",
      "both in terms of memory and compute as the resolution of the spatial grid grows. Another\r\n",
      "approach would be learning an autoregressive model using latent embeddings that encode\r\n",
      "dynamic shape information [15]. However, it is infeasible to observe the entire span of\r\n",
      "possible surface deformations from limited real-world scans, which makes the model\r\n",
      "prone to overfitting and leads to worse generalization at test time.\r\n",
      "    By addressing these limitations, we, for the first time, enable autoregressive training\r\n",
      "of a full-body geometry model represented by a neural implicit surface. To tackle\r\n",
      "the scalability issues of rollout training for implicit representations, we introduce the\r\n",
      "novel notion of articulated observer points. Intuitively, articulated observer points are\r\n",
      "temporally coherent locations on the human body surface which store the dynamically\r\n",
      "changing state of the implicit function. In practice, we parameterize the observer points\r\n",
      "using the underlying body model [22], and then represent the state of the implicit surface\r\n",
      "as signed heights with respect to the vertices of the pose-dependent geometry produced by\r\n",
      "the articulated model (see Fig. 3a). The number of query points is significantly lower than\r\n",
      "the number of voxels in a high-resolution grid, which allows for a significant reduction\r\n",
      "\f",
      "\n",
      "\n",
      "              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling     3\r\n",
      "\r\n",
      "in terms of memory and compute requirements, making rollout training tractable for\r\n",
      "implicit surfaces. In addition, we demonstrate that explicitly encoding shapes as signed\r\n",
      "height fields is less prone to overfitting compared to latent embeddings, a common way\r\n",
      "to represent autoregressive states [19,52].\r\n",
      "    Our main contributions are the following:\r\n",
      "    – The first autoregressive approach for modeling history-dependent implicit surfaces\r\n",
      "      of human bodies,\r\n",
      "    – Articulated observer points to enable autogressive training of neural fields, and\r\n",
      "    – Extensive experiments showing that our approach outperforms existing methods\r\n",
      "      both on shape interpolation and extrapolation tasks.\r\n",
      "\r\n",
      "\r\n",
      "2     Related Work\r\n",
      "Parametric Human Models Since the anatomical structure of humans is shared across\r\n",
      "identities, various methods have been proposed to parameterize shape and pose of\r\n",
      "human bodies from large-scale 3D scan data [3,13,22,33,54,1]. SCAPE [3,13] learns\r\n",
      "statistical human model models using triangle deformations. The pioneering work\r\n",
      "by Allen et al. [2] used a vertex-based representation enhanced with pose-dependent\r\n",
      "deformations, but the model was complex and trained with insufficient data, resulting\r\n",
      "in overfitting. SMPL [22] improved the generalizability of [2] by training on more data\r\n",
      "and removing the shape dependency in the pose-dependent deformations. More recent\r\n",
      "works show that sparsity in the pose correctives reduces spurious correlations [33],\r\n",
      "and that non-linear deformation bases parameterized by neural networks achieve better\r\n",
      "modeling accuracy [54]. While most works focus on modeling static human bodies\r\n",
      "under different poses, Dyna [39] and DMPL (Dynamic SMPL) [22] enable parametric\r\n",
      "modeling of dynamic deformations by learning a linear autoregressive model. Kim\r\n",
      "et al. [16] combine a volumetric parametric model, VSMPL, with an external layer\r\n",
      "driven by the finite element method to enable soft tissue dynamics. SoftSMPL [45]\r\n",
      "learns a more powerful recurrent neural network to achieve better generalization to\r\n",
      "unseen subjects. Xiang et al. [52] model dynamically moving clothing from a history\r\n",
      "of poses. Importantly, the foundation of the aforementioned works is accurate surface\r\n",
      "registration of a template body mesh [6,7], which remains non-trivial. Habermann et\r\n",
      "al. [12] also model dynamic deformations from a history of poses. While they relax the\r\n",
      "need of registration by leveraging image-based supervision, a personalized template is\r\n",
      "still required as a preprocessing step.\r\n",
      "     Recently, neural networks promise to enable the modeling of animatable bodies\r\n",
      "without requiring surface registration or a personalized template [10,24,43,48,8]. These\r\n",
      "methods leverage structured point clouds [24,26,56] or 3D neural fields [53] to learn\r\n",
      "animatable avatars. Approaches based on neural fields parameterize human bodies as\r\n",
      "compositional articulated occupancy networks [10] or implicit surface in canonical space\r\n",
      "with linear blend skinning [30,43,8,51] and deformation fields [48,35]. Since implicit\r\n",
      "surfaces do not require surface correspondences for training, avatars can be learned from\r\n",
      "raw scans. Similarly, neural radiance fields [31] have been applied to body modeling to\r\n",
      "build animatable avatars from multi-view images [37,20]. However, these approaches\r\n",
      "represent avatars as a function of only pose parameters, and thus are unable to model\r\n",
      "\f",
      "\n",
      "\n",
      "4              Z. Bai et al.\r\n",
      "\r\n",
      "                     Shape Encoding                                   Dynamic Feature Encoding                     Articulation-Aware Shape Decoding\r\n",
      "    SMPL Poses                                              Temporal Derivatives          Localized Pose\r\n",
      "                                                             · dH\r\n",
      "                                                                          p· ≈\r\n",
      "                                                                               dp                                      Query Point\r\n",
      "                      Articulated                            H≈                       L(p) = (W ⋅ ωi) ∘ p\r\n",
      "                       Observer                                 dt             dt                                         q\r\n",
      "                                                                                                                              k(q)\r\n",
      "                        Points        Signed Heights\r\n",
      "\r\n",
      "pt−2, pt−1, pt, pt+1\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                             Resample\r\n",
      "                                                                         UNet                                              SDF decoding\r\n",
      "    Implicit Shapes                                                                                                            f(q)\r\n",
      "\r\n",
      "                                                              ·                                                                              St+1\r\n",
      "                                      Ht−2, Ht−1, Ht     Ht, {Ht+i}                 Zuv                 Zt+1        Features in\r\n",
      "                                                       L(pt+1), {L( p· t+i)}\r\n",
      "                      SDF values                                                                                   Posed Space\r\n",
      "                                                                                                  Dynamic Latent\r\n",
      "                                                                                                   Embeddings\r\n",
      "    St−2, St−1, St\r\n",
      "\r\n",
      "\r\n",
      "Fig. 2: Overview. AutoAvatar learns a pose-driven animatable human body model with plausible\r\n",
      "dynamics including secondary motions. Notice that our approach takes the history of implicit\r\n",
      "shapes in an autoregressive manner for learning dynamics.\r\n",
      "𝒩\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "dynamics. While our approach is also based on 3D neural fields to eliminate the need\r\n",
      "for surface registration, our approach learns not only pose-dependent deformations but\r\n",
      "also history-dependent dynamics by enabling autoregressive training of neural implicit\r\n",
      "surfaces.\r\n",
      "\r\n",
      "Learning Dynamics Traditionally, physics-based simulation [46] is used to model\r\n",
      "dynamics of objects. While material parameters of physics simulation can be estimated\r\n",
      "from real data [5,50,47,55], accurately simulating dynamic behavior of objects remains\r\n",
      "an open question. In addition, authenticity of physics-based simulation is bounded by the\r\n",
      "underlying model, and complex anisotropic materials such as the human body are still\r\n",
      "challenging to model accurately. For this reason, several works attempt to substitute a\r\n",
      "deterministic physics-based simulation with a learnable module parameterized by neural\r\n",
      "networks [14,57,44,38]. Such approaches have been applied to cloth simulation [14,38],\r\n",
      "fluid [44], and elastic bodies [57]. Subspace Neural Physics [14] learns a recurrent\r\n",
      "neural network from offline simulation to predict the simulation state in a subspace.\r\n",
      "Deep Emulator [57] first learns an autoregressive model to predict deformations using a\r\n",
      "simple primitive (sphere), and applies the learned function to more complex characters.\r\n",
      "While we share the same spirit with the aforementioned works by learning dynamic\r\n",
      "deformations in an autoregressive manner, our approach fundamentally differs from\r\n",
      "them. The aforementioned approaches all assume that physical quantities such as vertex\r\n",
      "positions are observable with perfect correspondence in time, and thus results are only\r\n",
      "demonstrated on synthetic data. In contrast, we learn dynamic deformations from real-\r\n",
      "world observation while requiring only coarse temporal guidance by the fitted SMPL\r\n",
      "models. This property is essential to model faithful dynamics of real humans.\r\n",
      "\r\n",
      "\r\n",
      "3        Method\r\n",
      "Our approach is an autoregressive model, which takes as inputs human poses and a shape\r\n",
      "history and produces the implicit surface for a future frame. Fig. 2 shows the overview\r\n",
      "of our approach. Given a sequence of T implicitly encoded shapes {St−T +1 , ..., St }\r\n",
      "and T + 1 poses {pt−T +1 , ..., pt+1 } with t being the current time frame, our model\r\n",
      "predicts the implicit surface St+1 of the future frame t + 1. The output shape St+1 is\r\n",
      "\f",
      "\n",
      "\n",
      "                   AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling                                                             5\r\n",
      "\r\n",
      "\r\n",
      "                                                                           ci = cos(q − vi, ni)\r\n",
      "                        Articulated\r\n",
      "                         Observer\r\n",
      "                                                                           di = ∥q − vi∥2\r\n",
      "                          Points\r\n",
      "\r\n",
      "                                                                                           [zit+1, di, ci]\r\n",
      "                                                                                 vi ni           …\r\n",
      "                           Signed\r\n",
      "                                    Height\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                                                             PointNet\r\n",
      "                                                                                           [zjt+1, dj, cj]\r\n",
      "                                                                                     q\r\n",
      "                                                                                           [zkt+1, dk, ck]\r\n",
      "                                                                    v{i,j,k} ∈      k(q)\r\n",
      "                                                         Zt+1                                                                    St+1\r\n",
      "      Posed SMPL                                 Features in Posed Space                                                Signed Distance Functions\r\n",
      "\r\n",
      "         (a) Shape Encoding.                               (b) Articulation-Aware SDF Decoding.\r\n",
      "                                             𝒩\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Fig. 3: Shape Encoding/Decoding. Our novel shape encoding via articulated observer points and\r\n",
      "articulated-aware SDF decoding lead to faithful modeling of dynamics.\r\n",
      "\r\n",
      "then passed as an input to the next frame prediction in an autoregressive manner. Our\r\n",
      "model is supervised directly with raw body scans, and requires a training dataset of 4D\r\n",
      "scans (sequences of 3D scans) along with fitted SMPL body models [22]. Unfortunately,\r\n",
      "explicitly representing shapes St as levelsets of implicit surface is prohibitively expensive\r\n",
      "for end-to-end training. To this end, we introduce the concept of articulated observer\r\n",
      "points - vertex locations on the underlying articulated model - which are used as a local\r\n",
      "reference for defining the full body geometry. The underlying implicit surface is encoded\r\n",
      "as a height field with respect to the articulated observer points (Sec. 3.1). Given a history\r\n",
      "of height fields and pose parameters, we convert those to dynamic latent feature maps\r\n",
      "in UV space (Sec. 3.2). Finally, we map the resulting features to SDFs by associating\r\n",
      "continuous 3D space with the learned features on the SMPL vertices, which are directly\r\n",
      "supervised by point clouds with surface normals (Sec. 3.3).\r\n",
      "\r\n",
      "3.1     Shape Encoding via Articulated Observer Points\r\n",
      "The core of our approach is an autoregressive model that operates on implicit neural\r\n",
      "surfaces, allowing us to incorporate temporal shape information necessary for modeling\r\n",
      "challenging dynamics. The key challenge that arises when training such autoregressive\r\n",
      "models is finding a way to encode the shape - parameterized implicitly as a neural field -\r\n",
      "into a representation that can be efficiently computed and fed back into the model. The\r\n",
      "most straightforward way is to extract an explicit geometry representation by evaluating\r\n",
      "the neural field on a dense spatial grid and running marching cubes [23]. However, in\r\n",
      "practice this approach is infeasible due to prohibitive memory and computational costs,\r\n",
      "in particular due to the cubic scaling with respect to the grid dimensions. Instead, we\r\n",
      "propose to encode the state of the implicit surface into a set of observer points.\r\n",
      "    Encoding geometry into discrete point sets has been shown to be efficient and\r\n",
      "effective for learning shape representations from point clouds [40]. Prokudin et al. [40]\r\n",
      "relies on a fixed set of randomly sampled observer points in global world coordinates,\r\n",
      "which is not suitable for modeling dynamic humans due to the articulated nature of human\r\n",
      "motion. Namely, a model relying on observer points with a fixed 3D location needs to\r\n",
      "account for extremely large shape variations including rigid transformations, making\r\n",
      "the learning task difficult. Moreover, associating randomly sampled 3D points with a\r\n",
      "parametric human body is non-trivial. To address these limitations, we further extend the\r\n",
      "\f",
      "\n",
      "\n",
      "6       Z. Bai et al.\r\n",
      "\r\n",
      "notion of observer points to an articulated template represented by the SMPL model [22],\r\n",
      "which provides several advantages for modeling dynamic articulated geometries. In\r\n",
      "particular, soft-tissue dynamic deformations appear only around the minimally clothed\r\n",
      "body, and we can rely on this notion as an explicit prior to effectively allocate observer\r\n",
      "points only to the relevant regions. In addition, the SMPL model provides a mapping of\r\n",
      "3D vertices to a common UV parameterization, allowing us to effectively process shape\r\n",
      "information using 2D CNNs in a temporally consistent manner.\r\n",
      "     More specifically, to encode the neural implicit surface into the articulated observer\r\n",
      "points, we compute “signed heights” H = {hi }M      i=1 ∈ R\r\n",
      "                                                            M\r\n",
      "                                                               from M vertices on a fitted\r\n",
      "SMPL model. For each vertex, the signed height hi is the signed distance from the\r\n",
      "vertex to the zero-crossing of the implicit surface along the vertex normal (see Fig. 3a).\r\n",
      "We use the iterative secant method as in [32] to compute the zero-crossings. Note that\r\n",
      "there can be multiple valid signed heights per vertex since the line along the normal can\r\n",
      "hit the zero-crossing multiple times. Based on the observation that the SMPL vertices\r\n",
      "are usually close to the actual surface with their normals roughly facing into the same\r\n",
      "direction, we use the minimum signed height within a predefined range [hmin , hmax ]\r\n",
      "(in our experiments, we use hmin = −2cm, hmax = 8cm). If no zero-crossing is found\r\n",
      "inside this range, we set the signed height to hmin . Note that the computed heights\r\n",
      "are signed because the fitted SMPL can go beyond the actual surface due to its limited\r\n",
      "expressiveness and inaccuracy in the fitting stage.\r\n",
      "\r\n",
      "3.2   Dynamic Feature Encoding\r\n",
      "The essence of AutoAvatar is an animatable autoregressive model. In other words,\r\n",
      "a reconstructed avatar is driven by pose parameters, while secondary dynamics is\r\n",
      "automatically synthesized from the history of shapes and poses. To enable this, we learn\r\n",
      "a mapping that encodes the history of shape and pose information to latent embeddings\r\n",
      "containing the shape information of the future frame. More specifically, denoting the\r\n",
      "current time frame as t, we take as input T + 1 poses {pt−T +1 , ..., pt+1 } and T signed\r\n",
      "heights vectors {Ht−T +1 , ..., Ht }, and produce dynamic features Zt+1 ∈ RM ×C .\r\n",
      "Given these inputs, we also compute the temporal derivatives of poses {ṗt+i }1i=−T +2\r\n",
      "and signed heights {Ḣt+i }0i=−T +2 as follows:\r\n",
      "\r\n",
      "                                               \\begin {aligned} \\dot {\\bm {p}}_{k} &= \\bm {p}_{k} \\bm {p}_{k-1}^{-1} \\\\ \\dot {\\bm {H}}_{k} &= \\bm {H}_{k} - \\bm {H}_{k-1}. \\end {aligned} \r\n",
      "                                                                                                                                                                                             (1)\r\n",
      "\r\n",
      "Note that in practice p∗ are represented as quaternions, and pk p−1\r\n",
      "                                                                 k−1 is computed by first\r\n",
      "converting multipliers to rotation matrices, multiplying those, and then converting the\r\n",
      "product back to quaternions. To emphasize small values in Ḣ, we apply the following\r\n",
      "transformation g(x) = sign(x) · ln(α|x| + 1) · β, where α = 1000 and β = 0.25.\r\n",
      "Following prior works [43,4], we also localize pose parameters to reduce long range\r\n",
      "spurious correlations as follows:\r\n",
      "                                  \\begin {aligned} L(\\bm {p}) &= (W \\cdot \\omega _i) \\circ \\bm {p}, \\end {aligned}                                                                           (2)\r\n",
      "where ◦ denotes the element-wise product, i is the vertex index, W ∈ RJ×J is an\r\n",
      "association matrix of J joints, and ωi ∈ RJ×1 is the skinning weights of the i-th vertex.\r\n",
      "\f",
      "\n",
      "\n",
      "             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling                                                                     7\r\n",
      "\r\n",
      "We set Wn,m = 1 if the n-th joint is within the 1-ring neighborhood of the m-th joint\r\n",
      "(otherwise Wn,m = 0). Note that the derivative of the root transformation is included in\r\n",
      "{L(ṗt+i )} without localization. Finally, we map Ht , {Ḣt+i }, L(pt+1 ), and {L(ṗt+i )}\r\n",
      "to UV space using barycentric interpolation. The concatenated features are fed into a\r\n",
      "UNet [42] to generate a feature map Zuv . We then resample Zuv on the UV coordinates\r\n",
      "corresponding to SMPL vertices to obtain the per-vertex dynamic latent embeddings\r\n",
      "Z. We empirically found that incorporating temporal derivatives further improves the\r\n",
      "realism of dynamics (see Supp. Mat. video for comparison).\r\n",
      "\r\n",
      "3.3   Articulation-Aware Shape Decoding\r\n",
      "Given the dynamic feature Zt+1 = {z1t+1 , ..., zM   t+1\r\n",
      "                                                        } and a query point q, we decode\r\n",
      "signed distance fields f (q) to obtain the surface geometry of the dynamic avatar. Several\r\n",
      "methods model the implicit surface in canonical space by jointly learning a warping\r\n",
      "function from the posed space to the canonical space [43,8]. However, we observe that\r\n",
      "the canonicalization step is very sensitive to small fitting error in the SMPL model, and\r\n",
      "further amplifies the error in the canonical space, making it difficult to learn dynamics\r\n",
      "(see discussion in Sec. 4). Therefore, we directly model the implicit surface in a posed\r\n",
      "space while being robust to pose changes. Inspired by Neural Actor [20], we associate\r\n",
      "a queried 3D point with a human body model and pose-agnostic spatial information.\r\n",
      "Specifically, Neural Actor uses height from the closest surface point on the SMPL\r\n",
      "model to the query location together with a feature vector sampled on the same surface\r\n",
      "point. However, we find that their approach based on the single closest point leads\r\n",
      "to artifacts around body joints (e.g., armpits) for unseen poses. To better distinguish\r\n",
      "regions with multiple body parts, we instead use k-nearest neighbor vertices. Fig. 3b\r\n",
      "shows the illustration of our SDF decoding approach. Given a query point q, we first\r\n",
      "compute the k-nearest SMPL vertices {vi }i∈Nk (q) , where Nk (q) is a set of indices\r\n",
      "of k-nearest neighbor vertices. To encode pose-agnostic spatial information, we use\r\n",
      "rotation-invariant features. Specifically, we compute the distance di = ∥q − vi ∥2 and\r\n",
      "cosine value ci = cos(xi , ni ), where xi is the vertex-to-query vector \u0002 xi = q −\u0003vi , and\r\n",
      "ni is the surface normal on vi . We feed the concatenated vector zit+1 , di , ci into a\r\n",
      "PointNet-like [41] architecture to compute the final SDFs with the max pooling replaced\r\n",
      "by a weighted average pooling based on jointly predicted weights for better continuity.\r\n",
      "    As in [43], we employ implicit geometric regularization (IGR) [11] to train our\r\n",
      "model directly from raw scans without requiring watertight meshes. Note that in contrast,\r\n",
      "other methods [30,8,48] require watertight meshes to compute ground-truth occupancy\r\n",
      "or signed distance values for training. Our final objective function L is the following:\r\n",
      "                             \\label {eq:igr} \\begin {aligned} L &= L_{s} + L_{n} + \\lambda _{igr} L_{igr} + \\lambda _{o} L_{o}, \\\\ \\end {aligned}    (3)\r\n",
      "where λigr = 1.0, λo = 0.1. Ls promotes SDFs which vanish on the ground    Ptruth surface,\r\n",
      "while LPn encourages that its normal align with the ones from data: Ls =     q∈Qs |f (q)|,\r\n",
      "Ln =      q∈Qs  ∥∇ q f (q) −  n(q)∥  2 , where Q s is the surface of the input raw scans.\r\n",
      "Ligr is the Eikonal regularization term [11] that encourages the function f to satisfy\r\n",
      "the Eikonal equation: Ligr = Eq (∥∇q f (q)∥2 − 1)2 , and Lo prevents off-surface SDF\r\n",
      "values from being too close to the zero-crossings as follows: Lo = Eq (exp(−γ ·|f (q)|)),\r\n",
      "where γ = 50.\r\n",
      "\f",
      "\n",
      "\n",
      "8       Z. Bai et al.\r\n",
      "\r\n",
      "3.4   Implementation Details\r\n",
      "Network Architectures. In our experiments, we use a UV map of resolution 256 × 256,\r\n",
      "T = 3, and k = 20. To reduce the imbalance of SMPL vertex density for k-NN\r\n",
      "computation, we use 3928 points subsampled by poisson-disk sampling on the SMPL\r\n",
      "mesh. Before being fed into the UNet, L(pk+1 ) and {L(ṗt+i )} are compressed to 32\r\n",
      "channels using 1×1 convolutions. The UNet uses convolution and transposed convolution\r\n",
      "layers with untied biases, a kernel size of 3, no normalization, and LeakyReLU with a\r\n",
      "slope of 0.2 as the non-linear activation, except for the last layer which uses TanH. The\r\n",
      "SDF decoder is implemented as an MLP, which takes as input 64-dim features from the\r\n",
      "UNet, positional encoded di and ci up to 4-th order Fourier features. The number of\r\n",
      "intermediate neurons in the first part of the MLP is (128, 128, 129), where the output\r\n",
      "is split into a 128-dim feature vector and a 1-dim scalar, which is converted into non-\r\n",
      "negative weights by softmax across the k-NN samples. After weighted average pooling,\r\n",
      "the aggregated feature is fed into another MLP with a neuron size of (128, 128, 1) to\r\n",
      "predict the SDF values. The MLPs use Softplus with β = 100 and a threshold of 20 as\r\n",
      "non-linear activation except for the last layer which does not apply any activation.\r\n",
      "Training. Our training consists of two stages. First, we train our model using ground-\r\n",
      "truth signed heights without rollout for 90000 iterations. Then, we finetune the model\r\n",
      "using a rollout of 2 frames for another 7500 iterations to reduce error accumulation for\r\n",
      "both training and inference. We use the Adam optimizer with a learning rate of 1.0×10−4\r\n",
      "(1.0 × 10−5 ) at the first (second) stage. To compute Ln , we sample 10000(1000) points\r\n",
      "on the scan surface. Similarly, for Ligr , we sample 10000(1000) points around the scan\r\n",
      "surface by adding Gaussian noise with standard deviation of 10cm to uniformly sampled\r\n",
      "surface points, and sample 2000(500) points within the bounding box around the raw\r\n",
      "scans. The points uniformly sampled inside the bounding box are also used to compute\r\n",
      "Lo . Both stages are trained with a batch size of 1.\r\n",
      "Inference. At the beginning of the animations, we assume ground-truth raw scans are\r\n",
      "available for the previous T frames for initialization. If no ground truth initial shape is\r\n",
      "available, we initialize the first T frames with our baseline model conditioned only on\r\n",
      "pose parameters. Note that the scan data is extremely noisy around the hand and foot\r\n",
      "areas, and the SMPL fitting of the head region is especially inaccurate. Therefore, we fix\r\n",
      "the dynamic features on the face, hands, and feet to the ones of the first frame.\r\n",
      "\r\n",
      "\r\n",
      "4     Experimental Results\r\n",
      "4.1   Datasets and Metrics\r\n",
      "Datasets. We use the DFaust dataset [7] for both training and quantitative evaluation,\r\n",
      "and AIST++ [49,18] for qualitative evaluation on unseen motions. For the DFaust\r\n",
      "dataset, we choose 2 subjects (50002 and 50004), who exhibit the most soft-tissue\r\n",
      "deformations. The interpolation test evaluates the fidelity of dynamics under the same\r\n",
      "type of motions as in training but at different time instance, and the extrapolation\r\n",
      "test evaluates performance on unseen motion. For 50002, we use the 2nd half of\r\n",
      "chicken wings and running on spot for the interpolation test, one leg jump\r\n",
      "for the extrapolation test, and the rest for training. For 50004, we use the 2nd half of\r\n",
      "\f",
      "\n",
      "\n",
      "             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling        9\r\n",
      "\r\n",
      "chicken wings and running on spot for interpolation, one leg loose for\r\n",
      "extrapolation, and the rest for training. The fitted SMPL parameters in DFaust are\r\n",
      "provided by the AMASS [27] dataset that uses sparse points on the registered data as\r\n",
      "approximated mocap marker locations and computes the parameters using MoSh [21].\r\n",
      "Note that more accurate pose can be obtained by using all the registration vertices (see\r\n",
      "Appendix A), but this is not required by our method to recover soft-tissue deformation.\r\n",
      "Metrics. For evaluation, we extract the 0-level set surface at each time step using\r\n",
      "Marching Cubes [23] with a resolution of 2563 . We also use simplified scans with\r\n",
      "around 10000 vertices and outlier points (distance to the nearest SMPL vertex larger\r\n",
      "than 10cm) have been removed. We evaluate the accuracy of the predicted surface in\r\n",
      "terms of its position and dynamics accuracy. The surface position accuracy is measured\r\n",
      "by averaging the distance from each simplified scan vertex to the closest prediction\r\n",
      "surface point. Evaluating the dynamics accuracy of the implicit surface efficiently is more\r\n",
      "challenging. We approximate the local occupied volume as a scalar per registration vertex\r\n",
      "representing the ratio of surrounding points contained in the interior of the (ground-truth\r\n",
      "or inferred) surface. We use 10 points uniformly sampled inside a 5cm cube centered at\r\n",
      "the vertex. The head, hands and feet vertices are ignored due to their high noise levels.\r\n",
      "The temporal difference of this scalar across adjacent frames can be interpreted as a\r\n",
      "dynamic measure of the local volume evolution. We report the mean square difference\r\n",
      "between this dynamic descriptor as computed with the ground truth simplified scan\r\n",
      "and the inferred implicit surface. Since a small phase shift in dynamics may lead to\r\n",
      "large cumulative error, reporting only the averaged errors from the entire frames can\r\n",
      "be misleading. Therefore, we report errors along the progression of rollout predictions.\r\n",
      "For each evaluation sequence, we start prediction every 20 frames (i.e., 20th frame, 40th\r\n",
      "frame, ...), and use the ground truth pose and shape history only for the first frame,\r\n",
      "followed by the autoregressive predictions for the error computation. In Tab. 1, we\r\n",
      "report the averaged errors for both metrics after 1, 2, 4, 8, 16, and 30 rollouts. The\r\n",
      "errors for small number of rollouts evaluate the accuracy of future shape prediction\r\n",
      "given the ground-truth shape history, whereas the errors with longer rollouts evaluate\r\n",
      "the accumulated errors by autoregressively taking as input the predictions of previous\r\n",
      "frames. We discuss the limitation of error metrics with longer rollouts in Sec. 4.2.\r\n",
      "\r\n",
      "\r\n",
      "4.2   Evaluation\r\n",
      "\r\n",
      "In this section, we provide comprehensive analysis to validate our design choices and\r\n",
      "highlight the limitations of alternative approaches and SoTA methods based on both\r\n",
      "implicit and explicit shape representations. Note that all approaches use the same training\r\n",
      "set, and are trained with the same number of iterations as our method for fair comparison.\r\n",
      "Effectiveness of Autoregressive Modeling. While autoregressive modeling is a widely\r\n",
      "used technique for learning dynamics [39,57,38], several recent methods still employ\r\n",
      "only the history of poses for modeling dynamic avatars [52,12]. Thus, to evaluate the\r\n",
      "effectiveness of autoregressive modeling we compare AutoAvatar with pose-dependent\r\n",
      "alternatives that use neural implicit surfaces. More specifically, we design the following\r\n",
      "3 non-autoregressive baselines:\r\n",
      "\f",
      "\n",
      "\n",
      "10        Z. Bai et al.\r\n",
      "\r\n",
      "\r\n",
      "Table 1: Quantitative Comparison with Baseline Methods. Our method produces the most\r\n",
      "accurate predictions of the future frames given the ground-truth shape history among all baseline\r\n",
      "methods (see rollout 1-4). For longer rollouts, more dynamic predictions lead to higher error than\r\n",
      "less dynamic results due to high sensitivity to initial conditions in dynamic systems [34] (see\r\n",
      "discussion in Sec. 4.2).\r\n",
      "\r\n",
      "                      (a) Mean Scan-to-Prediction Distance (mm) ↓ on DFaust.\r\n",
      "\r\n",
      "                                                          Rollout (# of frames)\r\n",
      "                                               1       2       4         8      16      30\r\n",
      "                                        Interpolation Set\r\n",
      "                              SNARF [8]      7.428 7.372 7.337 7.476 7.530             7.656\r\n",
      "                              Pose           4.218 4.202 4.075 4.240 4.409             4.426\r\n",
      "      Non-Autoregressive\r\n",
      "                              PoseTCN        4.068 4.118 4.086 4.228 4.405             4.411\r\n",
      "                              Pose + dPose 3.852 3.841 3.764 3.972 4.164               4.156\r\n",
      "                              G-embed       2.932 3.006      3.131   3.462     3.756   3.793\r\n",
      "      Autoregressive          L-embed       1.784 2.138      2.863   4.250     5.448   5.916\r\n",
      "                              Ours          1.569 1.914      2.587   3.627     4.736   5.255\r\n",
      "                                        Extrapolation Set\r\n",
      "                              SNARF [8]     7.264 7.287      7.321   7.387     7.308   7.251\r\n",
      "                              Pose          4.303 4.306      4.308   4.299     4.385   4.398\r\n",
      "      Non-Autoregressive\r\n",
      "                              PoseTCN       4.090 4.091      4.105   4.119     4.233   4.257\r\n",
      "                              Pose + dPose 3.984 3.991       4.017   4.063     4.162   4.190\r\n",
      "                              G-embed        2.884   2.926   3.043   3.258     3.577   3.787\r\n",
      "      Autoregressive          L-embed        1.329   1.539   2.079   3.326     4.578   5.192\r\n",
      "                              Ours           1.150   1.361   1.834   2.689     3.789   4.526\r\n",
      "\r\n",
      "                      (b) Mean Squared Error of Volume Change ↓ on DFaust.\r\n",
      "\r\n",
      "                                                          Rollout (# of frames)\r\n",
      "                                              2         4           8         16         30\r\n",
      "                                        Interpolation Set\r\n",
      "                            SNARF [8]     0.01582 0.01552 0.01610 0.01658              0.01682\r\n",
      "                            Pose          0.01355 0.01305 0.01341 0.01367              0.01387\r\n",
      "     Non-Autoregressive\r\n",
      "                            PoseTCN       0.01364 0.01323 0.01350 0.01399              0.01416\r\n",
      "                            Pose + dPose 0.01288 0.01247 0.01273 0.01311               0.01321\r\n",
      "                            G-embed       0.01179 0.01168       0.01199    0.01248     0.01265\r\n",
      "     Autoregressive         L-embed       0.01003 0.01180       0.01466    0.01716     0.01844\r\n",
      "                            Ours          0.00902 0.01053       0.01258    0.01456     0.01565\r\n",
      "                                        Extrapolation Set\r\n",
      "                            SNARF [8]     0.01178 0.01194       0.01251    0.01228     0.01206\r\n",
      "                            Pose          0.01027 0.01039       0.01074    0.01052     0.01039\r\n",
      "     Non-Autoregressive\r\n",
      "                            PoseTCN       0.01020 0.01038       0.01064    0.01040     0.01029\r\n",
      "                            Pose + dPose 0.00992 0.01014        0.01048    0.01029     0.01013\r\n",
      "                            G-embed        0.00936   0.00959    0.00995    0.00996     0.00998\r\n",
      "     Autoregressive         L-embed        0.00648   0.00821    0.01100    0.01308     0.01402\r\n",
      "                            Ours           0.00567   0.00715    0.00915    0.01039     0.01107\r\n",
      "\f",
      "\n",
      "\n",
      "                   AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling      11\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    SMPL\r\n",
      "    Pose\r\n",
      "    PoseTCN\r\n",
      "    Pose + dPose\r\n",
      "    Ours\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                 Time\r\n",
      "\r\n",
      "\r\n",
      "Fig. 4: Qualitative Comparison with Non-Autoregressive Baselines. In contrast to the rigid\r\n",
      "results in non-autoregressive baselines, our approach produces high quality non-rigid dynamics.\r\n",
      "\r\n",
      " 1. Pose: We only feed pose parameters of the next frame L(pt+1 ) in our architec-\r\n",
      "     ture. Prior avatar modeling methods based on neural fields employ this pose-only\r\n",
      "     parameterization [43,48,8].\r\n",
      " 2. PoseTCN: Temporal convolutional networks (TCN) [17] support the incorporation\r\n",
      "     of a long-range history for learning tasks, and have been used in several avatar\r\n",
      "     modeling methods [52,12]. Thus, we use a TCN that takes as input the sequence\r\n",
      "     of poses with the length of 16. We first compute localized pose parameters, as in\r\n",
      "     our method, for each frame and apply the TCN to obtain 64-dim features for each\r\n",
      "     SMPL vertex. The features are then fed into the UNet and SDF decoders identical\r\n",
      "     to our method.\r\n",
      " 3. Pose+dPose: Our approach without autoregressive components (Ht , {Ḣt+i }).\r\n",
      "    Tab. 1 shows that our approach outperforms the baseline methods for the first 8\r\n",
      "frames for interpolation, and first 16 frames for extrapolation. In particular, there is a\r\n",
      "significantly large margin for the first 4-8 frames, indicating that our method achieves\r\n",
      "the most accurate prediction of the future frames given the ground-truth shape history.\r\n",
      "We also observe that the non-autoregressive methods tend to collapse to predicting the\r\n",
      "“mean” shape under each pose without faithful dynamics for unseen motions (see Fig. 4\r\n",
      "and Supp. Mat. video). Since the accumulation of small errors in each frame may lead\r\n",
      "to large deviations from the ground-truth due to high sensitivity to initial conditions\r\n",
      "in dynamic systems [34], for longer rollouts mean predictions without any dynamics\r\n",
      "can produce lower errors than more dynamic predictions. In fact, although our method\r\n",
      "leads to slightly higher errors on longer rollouts, Fig. 4 clearly shows that our approach\r\n",
      "produces the most visually plausible dynamics on the AIST++ sequences. Importantly,\r\n",
      "we do not observe any instability or explosions in our autoregressive model for longer\r\n",
      "rollouts, as can be seen from the error behavior shown in Fig. 7. We also highly encourage\r\n",
      "\f",
      "\n",
      "\n",
      "12             Z. Bai et al.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "     SMPL\r\n",
      "     G-embed\r\n",
      "     L-embed\r\n",
      "     Ours\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                              Time\r\n",
      "\r\n",
      "\r\n",
      "Fig. 5: Qualitative Comparison with Latent-space Autoregression. While the latent space-\r\n",
      "based autoregression approaches suffer either from overfitting to pose parameters (G-embed)\r\n",
      "or instability (L-embed, see Supp. Mat. video), our approach based on a physically meaningful\r\n",
      "quantity (signed height) achieves the most stable and expressive synthesis of dynamics.\r\n",
      "\r\n",
      "readers to see Supp. Mat. video for qualitative comparison in animation. In summary,\r\n",
      "our results confirm that autoregressive modeling plays a critical role for generalization\r\n",
      "to unseen motions and improving the realism of dynamics.\r\n",
      "Explicit Shape Encoding vs. Latent Encoding. Efficiently encoding the geometry of\r\n",
      "implicit surfaces is non-trivial. While our proposed approach encodes the geometry via\r\n",
      "signed heights on articulated observer points, prior approaches have demonstrated shape\r\n",
      "encoding based on a learned latent space [36,4]. Therefore, we also investigate different\r\n",
      "encoding methods for autoregressive modeling with the following 2 baselines:\r\n",
      " 1. G-embed: Inspired by DeepSDF [36], we first learn per-frame global embeddings\r\n",
      "    lg ∈ R512 with the UNet and SDF decoder by replacing Ht , {Ḣt+i }, {L(ṗt+i )}\r\n",
      "    with repeated global embeddings. Then, we train a small MLP with three 512-dim\r\n",
      "    hidden layers using Softplus except for the last layer, taking as input pt+1 , {ṗt+i },\r\n",
      "    and 3 embeddings of previous frames to predict the global embedding at time t + 1.\r\n",
      " 2. L-embed: For modeling mesh-based body avatars, localized embeddings are shown\r\n",
      "    to be effective [4]. Inspired by this, we also train a model with localized embeddings\r\n",
      "    ll ∈ R16×64×64 . We first learn per-frame local embeddings ll together with the UNet\r\n",
      "    and SDF decoder by replacing Ht , {Ḣt+i }, {L(ṗt+i )} with bilinearly upsampled\r\n",
      "    ll . Then we train another UNet that takes as input L(pt+1 ), {L(ṗt+i )}, and 3\r\n",
      "    embeddings of previous frames to predict the localized embeddings at time t + 1.\r\n",
      "    Note that for evaluation, we optimize per-frame embeddings for test sequences using\r\n",
      "Eq. (3) such that the baseline methods can use the best possible history of embeddings\r\n",
      "for autoregression. Tab. 1 shows that our method outperforms L-embed in all cases\r\n",
      "because L-embed becomes unstable for the test sequences. For G-embed, we observe the\r\n",
      "same trend as for the non-autoregressive baselines: our approach achieves significantly\r\n",
      "\f",
      "\n",
      "\n",
      "              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling              13\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                 Jump to top\r\n",
      "                                                 Fall to bottom\r\n",
      "              Scan       SNARF       Ours                         Scan    SoftSMPL   Ours\r\n",
      "\r\n",
      "\r\n",
      "Fig. 6: Qualitative Comparison with SoTA Methods. Our approach produces significantly more\r\n",
      "faithful shapes and dynamics than the state-of-the-art implicit avatar modeling method [8], and\r\n",
      "shows comparable dynamics with prior art dependent on registrations with fixed topology [45].\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "       (a) Scan-to-Prediction Distance ↓                           (b) MSE of Volume Change ↓\r\n",
      "\r\n",
      "Fig. 7: Comparison with SoftSMPL [45]. We plot the errors on the sequence of\r\n",
      "one leg loose for subject 50004. Surprisingly, our registration-free approach mostly out-\r\n",
      "performs this baseline that has to rely on registered data with fixed topology.\r\n",
      "\r\n",
      "more accurate predictions of the future frames given the ground-truth trajectories (see the\r\n",
      "errors for 1-4 rollouts), and G-embed tends to predict “mean” shapes without plausible\r\n",
      "dynamics. The qualitative comparison in Fig. 5 confirms that our approach produces\r\n",
      "more plausible dynamics. Please refer to Supp. Mat. video for detailed visual comparison\r\n",
      "in animation. We summarize that physically meaningful shape encodings (e.g., signed\r\n",
      "heights) enable more stable learning of dynamics via autoregression than methods relying\r\n",
      "on latent space.\r\n",
      "Comparison to SoTA Methods.. We compare our approach with state-of-the-art meth-\r\n",
      "ods for both implicit surface representations and mesh-based representations. As a\r\n",
      "method using neural implicit surfaces, we choose SNARF [8], which jointly learns a\r\n",
      "pose-conditioned implicit surface in a canonical T-pose and a forward skinning network\r\n",
      "for reposing. Similar to ours, SNARF does not require temporal correspondences other\r\n",
      "than the fitted SMPL models. We use the training code released by the authors using\r\n",
      "the same training data and the fitted SMPL parameters as in our method. Note that in\r\n",
      "the DFaust experiment in [8], SNARF is trained using only the fitted SMPL models\r\n",
      "to DFaust as ground-truth geometry, which do not contain any dynamic deformations.\r\n",
      "\f",
      "\n",
      "\n",
      "14      Z. Bai et al.\r\n",
      "\r\n",
      "Tab. 1 shows that our approach significantly outperforms SNARF for any number of\r\n",
      "rollouts. Interestingly, SNARF can produce dynamic effects for training data by severely\r\n",
      "overfitting to the pose parameters, but this does not generalize to unseen poses as the\r\n",
      "learned dynamics is the results of spurious correlations. As mentioned in Sec. 3.3, we\r\n",
      "also observe that the performance of SNARF heavily relies on the accuracy of the\r\n",
      "SMPL fitting for canonicalization, and any small alignment errors in the underlying\r\n",
      "SMPL registration deteriorates their test-time performance (see Fig. 6). Therefore, this\r\n",
      "experiment demonstrates not only the importance of autoregressive dynamic avatar\r\n",
      "modeling, but also the efficacy of our articulation-aware shape decoding approach given\r\n",
      "the quality of available SMPL fitting for real-world scans.\r\n",
      "    We also compare against SoftSMPL [45], a state-of-the-art mesh-based method that\r\n",
      "learns dynamically deforming human bodies from registered meshes. The authors of\r\n",
      "SoftSMPL kindly provide their predictions on the sequence of one leg loose for\r\n",
      "subject 50004, which is excluded from training for both our method and SoftSMPL for\r\n",
      "fair comparison. To our surprise, Fig. 7 show that our results are slightly better on both\r\n",
      "metrics for the majority of frames, although we tackle a significantly harder problem\r\n",
      "because our approach learns dynamic bodies directly from raw scans, whereas SoftSMPL\r\n",
      "learns from the carefully registered data. We speculate that the lower error may be mainly\r\n",
      "attributed to the higher resolution of our geometry using implicit surfaces in contrast\r\n",
      "to their predictions on the coarse SMPL topology (see Fig. 6). Nevertheless, this result\r\n",
      "is highly encouraging as our approach achieves comparable performance on dynamics\r\n",
      "modeling without having to rely on surface registration.\r\n",
      "\r\n",
      "\r\n",
      "5    Conclusion\r\n",
      "We have introduced AutoAvatar, an autoregressive approach for modeling high-fidelity\r\n",
      "dynamic deformations of human bodies directly from raw 4D scans using neural\r\n",
      "implicit surfaces. The reconstructed avatars can be driven by pose parameters, and\r\n",
      "automatically incorporate secondary dynamic effects that depend on the history of\r\n",
      "shapes. Our experiments indicate that modeling dynamic avatars without relying on\r\n",
      "accurate registrations is made possible by choosing an efficient representation for our\r\n",
      "autoregressive model.\r\n",
      "Limitations and Future Work. While our method has shown to be effective in modeling\r\n",
      "the elastic deformations of real humans, we observe that it remains challenging, yet\r\n",
      "promising, to model clothing deformations that involve high-frequency wrinkles (see\r\n",
      "Appendix C for details). Our evaluation also suggests that ground-truth comparison\r\n",
      "with longer rollouts may not reliably reflect the plausibility of dynamics. Quantitative\r\n",
      "metrics that handle the high sensitivity to initial conditions in dynamics could be further\r\n",
      "investigated. Currently, AutoAvatar models subject-specific dynamic human bodies,\r\n",
      "but generalizing it to multiple identities, as demonstrated in registration-based shape\r\n",
      "modeling [39,22,45], is an interesting direction for future work. The most exciting venue\r\n",
      "for future work is to extend the notion of dynamics to image-based avatars [37,20]. In\r\n",
      "contrast to implicit surfaces, neural radiance fields [31] do not have an explicit “surface”\r\n",
      "as they model geometry using density fields. While this remains an open question, we\r\n",
      "believe that our contributions in this work such as efficiently modeling the state of shapes\r\n",
      "via articulated observer points might be useful to unlock this application.\r\n",
      "\f",
      "\n",
      "\n",
      "              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling             15\r\n",
      "\r\n",
      "References\r\n",
      " 1. Alldieck, T., Xu, H., Sminchisescu, C.: imghum: Implicit generative models of 3d human\r\n",
      "    shape and articulated pose. In: Proc. of International Conference on Computer Vision (ICCV).\r\n",
      "    pp. 5461–5470 (2021) 3\r\n",
      " 2. Allen, B., Curless, B., Popović, Z., Hertzmann, A.: Learning a correlated model of identity\r\n",
      "    and pose-dependent body shape variation for real-time synthesis. In: Proceedings of the 2006\r\n",
      "    ACM SIGGRAPH/Eurographics symposium on Computer animation. pp. 147–156 (2006) 3\r\n",
      " 3. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: SCAPE: shape\r\n",
      "    completion and animation of people. ACM Trans. on Graphics (TOG) 24(3), 408–416 (2005)\r\n",
      "    3\r\n",
      " 4. Bagautdinov, T., Wu, C., Simon, T., Prada, F., Shiratori, T., Wei, S.E., Xu, W., Sheikh, Y.,\r\n",
      "    Saragih, J.: Driving-signal aware full-body avatars. ACM Trans. on Graphics (TOG) 40(4),\r\n",
      "    1–17 (2021) 1, 6, 12\r\n",
      " 5. Bhat, K.S., Twigg, C.D., Hodgins, J.K., Khosla, P., Popovic, Z., Seitz, S.M.: Estimating cloth\r\n",
      "    simulation parameters from video (2003) 4\r\n",
      " 6. Bogo, F., Romero, J., Loper, M., Black, M.J.: Faust: Dataset and evaluation for 3d mesh\r\n",
      "    registration. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 3794–3801\r\n",
      "    (2014) 3\r\n",
      " 7. Bogo, F., Romero, J., Pons-Moll, G., Black, M.J.: Dynamic faust: Registering human bodies\r\n",
      "    in motion. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 6233–6242\r\n",
      "    (2017) 3, 8, 19\r\n",
      " 8. Chen, X., Zheng, Y., Black, M.J., Hilliges, O., Geiger, A.: Snarf: Differentiable forward\r\n",
      "    skinning for animating non-rigid neural implicit shapes. In: Proc. of International Conference\r\n",
      "    on Computer Vision (ICCV) (2021) 1, 3, 7, 10, 11, 13, 19, 20\r\n",
      " 9. Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling. In: Proc.\r\n",
      "    of Computer Vision and Pattern Recognition (CVPR). pp. 5939–5948. Computer Vision\r\n",
      "    Foundation / IEEE (2019) 2\r\n",
      "10. Deng, B., Lewis, J.P., Jeruzalski, T., Pons-Moll, G., Hinton, G.E., Norouzi, M., Tagliasacchi,\r\n",
      "    A.: NASA neural articulated shape approximation. In: Proc. of European Conference on\r\n",
      "    Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12352, pp. 612–628.\r\n",
      "    Springer (2020) 3\r\n",
      "11. Gropp, A., Yariv, L., Haim, N., Atzmon, M., Lipman, Y.: Implicit geometric regularization for\r\n",
      "    learning shapes. In: Proceedings of the 37th International Conference on Machine Learning\r\n",
      "    (ICML). Proceedings of Machine Learning Research, vol. 119, pp. 3789–3799. PMLR (2020)\r\n",
      "    7\r\n",
      "12. Habermann, M., Liu, L., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.: Real-time deep\r\n",
      "    dynamic characters. ACM Trans. on Graphics (TOG) 40(4), 1–16 (2021) 3, 9, 11\r\n",
      "13. Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., Seidel, H.: A statistical model of human\r\n",
      "    pose and body shape. Computer Graphics Forum 28(2), 337–346 (2009) 3\r\n",
      "14. Holden, D., Duong, B.C., Datta, S., Nowrouzezahrai, D.: Subspace neural physics:\r\n",
      "    Fast data-driven interactive simulation. In: Proceedings of the 18th annual ACM SIG-\r\n",
      "    GRAPH/Eurographics Symposium on Computer Animation. pp. 1–12 (2019) 4\r\n",
      "15. Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3d human dynamics from video. In:\r\n",
      "    Proc. of Computer Vision and Pattern Recognition (CVPR) (2019) 2\r\n",
      "16. Kim, M., Pons-Moll, G., Pujades, S., Bang, S., Kim, J., Black, M.J., Lee, S.: Data-driven\r\n",
      "    physics for human soft tissue animation. ACM Trans. on Graphics (TOG) 36(4), 54:1–54:12\r\n",
      "    (2017) 3\r\n",
      "17. Lea, C., Vidal, R., Reiter, A., Hager, G.D.: Temporal convolutional networks: A unified\r\n",
      "    approach to action segmentation. In: Proc. of European Conference on Computer Vision\r\n",
      "    (ECCV). pp. 47–54. Springer (2016) 11\r\n",
      "\f",
      "\n",
      "\n",
      "16       Z. Bai et al.\r\n",
      "\r\n",
      "18. Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance\r\n",
      "    generation with aist++. In: Proc. of International Conference on Computer Vision (ICCV). pp.\r\n",
      "    13401–13412 (2021) 8\r\n",
      "19. Ling, H.Y., Zinno, F., Cheng, G., Van De Panne, M.: Character controllers using motion vaes.\r\n",
      "    ACM Trans. on Graphics (TOG) 39(4), 40–1 (2020) 2, 3\r\n",
      "20. Liu, L., Habermann, M., Rudnev, V., Sarkar, K., Gu, J., Theobalt, C.: Neural actor: Neural\r\n",
      "    free-view synthesis of human actors with pose control. ACM Trans. on Graphics (TOG) 40(6),\r\n",
      "    1–16 (2021) 3, 7, 14, 19\r\n",
      "21. Loper, M., Mahmood, N., Black, M.J.: Mosh: Motion and shape capture from sparse markers.\r\n",
      "    ACM Trans. on Graphics (TOG) 33(6), 1–13 (2014) 9, 19\r\n",
      "22. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-\r\n",
      "    person linear model. ACM Trans. on Graphics (TOG) 34(6), 248:1–248:16 (2015) 2, 3, 5, 6,\r\n",
      "    14\r\n",
      "23. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface construction\r\n",
      "    algorithm. ACM siggraph computer graphics 21(4), 163–169 (1987) 5, 9\r\n",
      "24. Ma, Q., Saito, S., Yang, J., Tang, S., Black, M.J.: SCALE: Modeling clothed humans with\r\n",
      "    a surface codec of articulated local elements. In: Proc. of Computer Vision and Pattern\r\n",
      "    Recognition (CVPR) (Jun 2021) 1, 3\r\n",
      "25. Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., Black, M.J.: Learning to\r\n",
      "    dress 3d people in generative clothing. In: Proc. of Computer Vision and Pattern Recognition\r\n",
      "    (CVPR). pp. 6469–6478 (2020) 19\r\n",
      "26. Ma, Q., Yang, J., Tang, S., Black, M.J.: The power of points for modeling humans in clothing.\r\n",
      "    In: Proc. of International Conference on Computer Vision (ICCV) (Oct 2021) 1, 3\r\n",
      "27. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of\r\n",
      "    motion capture as surface shapes. In: Proc. of International Conference on Computer Vision\r\n",
      "    (ICCV). pp. 5442–5451 (2019) 9, 19, 20\r\n",
      "28. Martinez, J., Black, M.J., Romero, J.: On human motion prediction using recurrent neural\r\n",
      "    networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 2891–2900\r\n",
      "    (2017) 2\r\n",
      "29. Mescheder, L.M., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\r\n",
      "    Learning 3D reconstruction in function space. In: Proc. of Computer Vision and Pattern\r\n",
      "    Recognition (CVPR). pp. 4460–4470. Computer Vision Foundation / IEEE (2019) 2\r\n",
      "30. Mihajlovic, M., Zhang, Y., Black, M.J., Tang, S.: LEAP: Learning articulated occupancy of\r\n",
      "    people. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun 2021) 3, 7\r\n",
      "31. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF:\r\n",
      "    Representing scenes as neural radiance fields for view synthesis. In: Proc. of European\r\n",
      "    Conference on Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12346,\r\n",
      "    pp. 405–421. Springer (2020) 3, 14\r\n",
      "32. Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: Differentiable volumetric rendering:\r\n",
      "    Learning implicit 3d representations without 3d supervision. In: Proc. of Computer Vision\r\n",
      "    and Pattern Recognition (CVPR) (2020) 6\r\n",
      "33. Osman, A.A.A., Bolkart, T., Black, M.J.: STAR: sparse trained articulated human body\r\n",
      "    regressor. In: Proc. of European Conference on Computer Vision (ECCV). Lecture Notes in\r\n",
      "    Computer Science, vol. 12351, pp. 598–613. Springer (2020) 3\r\n",
      "34. Ott, E., Grebogi, C., Yorke, J.A.: Controlling chaos. Physical review letters 64(11), 1196\r\n",
      "    (1990) 10, 11\r\n",
      "35. Palafox, P., Božič, A., Thies, J., Nießner, M., Dai, A.: Npms: Neural parametric models for\r\n",
      "    3d deformable shapes. In: Proc. of International Conference on Computer Vision (ICCV). pp.\r\n",
      "    12695–12705 (2021) 3\r\n",
      "\f",
      "\n",
      "\n",
      "               AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling               17\r\n",
      "\r\n",
      "36. Park, J.J., Florence, P., Straub, J., Newcombe, R.A., Lovegrove, S.: DeepSDF: Learning\r\n",
      "    continuous signed distance functions for shape representation. In: Proc. of Computer Vision\r\n",
      "    and Pattern Recognition (CVPR). pp. 165–174. Computer Vision Foundation / IEEE (2019)\r\n",
      "    2, 12\r\n",
      "37. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit\r\n",
      "    neural representations with structured latent codes for novel view synthesis of dynamic\r\n",
      "    humans. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 9054–9063\r\n",
      "    (2021) 3, 14\r\n",
      "38. Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., Battaglia, P.: Learning mesh-based simulation\r\n",
      "    with graph networks. In: International Conference on Learning Representations (2021) 4, 9\r\n",
      "39. Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J.: Dyna: A model of dynamic human\r\n",
      "    shape in motion. ACM Trans. on Graphics (TOG) 34(4), 1–14 (2015) 2, 3, 9, 14\r\n",
      "40. Prokudin, S., Lassner, C., Romero, J.: Efficient learning on point clouds with basis point sets.\r\n",
      "    In: Proc. of International Conference on Computer Vision (ICCV). pp. 4332–4341 (2019) 5\r\n",
      "41. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d\r\n",
      "    classification and segmentation. In: Proc. of Computer Vision and Pattern Recognition (CVPR).\r\n",
      "    pp. 652–660 (2017) 7\r\n",
      "42. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image\r\n",
      "    segmentation. In: International Conference on Medical image computing and computer-\r\n",
      "    assisted intervention. pp. 234–241. Springer (2015) 7\r\n",
      "43. Saito, S., Yang, J., Ma, Q., Black, M.J.: SCANimate: Weakly supervised learning of skinned\r\n",
      "    clothed avatar networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun\r\n",
      "    2021) 1, 3, 6, 7, 11\r\n",
      "44. Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., Battaglia, P.: Learning\r\n",
      "    to simulate complex physics with graph networks. In: International Conference on Machine\r\n",
      "    Learning. pp. 8459–8468. PMLR (2020) 4\r\n",
      "45. Santesteban, I., Garces, E., Otaduy, M.A., Casas, D.: Softsmpl: Data-driven modeling of\r\n",
      "    nonlinear soft-tissue dynamics for parametric humans. In: Computer Graphics Forum. vol. 39,\r\n",
      "    pp. 65–75. Wiley Online Library (2020) 2, 3, 13, 14\r\n",
      "46. Sifakis, E., Barbic, J.: Fem simulation of 3d deformable solids: a practitioner’s guide to theory,\r\n",
      "    discretization and model reduction. In: Acm siggraph 2012 courses, pp. 1–50 (2012) 4\r\n",
      "47. Srinivasan, S.G., Wang, Q., Rojas, J., Klár, G., Kavan, L., Sifakis, E.: Learning active\r\n",
      "    quasistatic physics-based models from data. ACM Trans. on Graphics (TOG) 40(4), 1–14\r\n",
      "    (2021) 4\r\n",
      "48. Tiwari, G., Sarafianos, N., Tung, T., Pons-Moll, G.: Neural-gif: Neural generalized implicit\r\n",
      "    functions for animating people in clothing. In: Proc. of International Conference on Computer\r\n",
      "    Vision (ICCV). pp. 11708–11718 (2021) 1, 3, 7, 11\r\n",
      "49. Tsuchida, S., Fukayama, S., Hamasaki, M., Goto, M.: Aist dance video database: Multi-genre,\r\n",
      "    multi-dancer, and multi-camera database for dance information processing. In: ISMIR. vol. 1,\r\n",
      "    p. 6 (2019) 8\r\n",
      "50. Wang, H., O’Brien, J.F., Ramamoorthi, R.: Data-driven elastic models for cloth: modeling\r\n",
      "    and measurement. ACM Trans. on Graphics (TOG) 30(4), 1–12 (2011) 4\r\n",
      "51. Wang, S., Mihajlovic, M., Ma, Q., Geiger, A., Tang, S.: Metaavatar: Learning animatable\r\n",
      "    clothed human models from few depth images. Proc. of Advances in Neural Information\r\n",
      "    Processing Systems (NeurIPS) 34 (2021) 3\r\n",
      "52. Xiang, D., Prada, F., Bagautdinov, T., Xu, W., Dong, Y., Wen, H., Hodgins, J., Wu, C.:\r\n",
      "    Modeling clothing as a separate layer for an animatable human avatar. ACM Trans. on\r\n",
      "    Graphics (TOG) 40(6), 1–15 (2021) 3, 9, 11\r\n",
      "53. Xie, Y., Takikawa, T., Saito, S., Litany, O., Yan, S., Khan, N., Tombari, F., Tompkin, J.,\r\n",
      "    Sitzmann, V., Sridhar, S.: Neural fields in visual computing and beyond. arXiv preprint\r\n",
      "    arXiv:2111.11426 (2021) 3\r\n",
      "\f",
      "\n",
      "\n",
      "18       Z. Bai et al.\r\n",
      "\r\n",
      "54. Xu, H., Bazavan, E.G., Zanfir, A., Freeman, W.T., Sukthankar, R., Sminchisescu, C.: GHUM\r\n",
      "    & GHUML: generative 3D human shape and articulated pose models. In: Proc. of Computer\r\n",
      "    Vision and Pattern Recognition (CVPR). pp. 6183–6192. IEEE (2020) 3\r\n",
      "55. Yang, S., Liang, J., Lin, M.C.: Learning-based cloth material recovery from video. In: Proc.\r\n",
      "    of International Conference on Computer Vision (ICCV). pp. 4393–4403. IEEE Computer\r\n",
      "    Society (2017) 4\r\n",
      "56. Zakharkin, I., Mazur, K., Grigorev, A., Lempitsky, V.: Point-based modeling of human\r\n",
      "    clothing. In: Proc. of International Conference on Computer Vision (ICCV). pp. 14718–14727\r\n",
      "    (2021) 3\r\n",
      "57. Zheng, M., Zhou, Y., Ceylan, D., Barbic, J.: A deep emulator for secondary motion of 3d\r\n",
      "    characters. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 5932–5940\r\n",
      "    (2021) 4, 9\r\n",
      "\f",
      "\n",
      "\n",
      "             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling      19\r\n",
      "\r\n",
      "Appendix\r\n",
      "\r\n",
      "A    Analysis on Input Pose Accuracy\r\n",
      "We investigate how the accuracy of the input SMPL fitting influences the results on\r\n",
      "subject 50002 of DFaust [7]. As discussed in Sec. 4.1, the fitted SMPL parameters in\r\n",
      "DFaust are provided by the AMASS [27] dataset that uses sparse points on the registered\r\n",
      "data as approximated motion capture marker locations and computes the parameters\r\n",
      "using MoSh [21]. We observe that the provided pose parameters sometimes exhibit small\r\n",
      "misalignment with respect to the input scans. While the fitting quality in the AMASS\r\n",
      "dataset is sufficient for our approach, we also evaluate the performance on more accurate\r\n",
      "pose parameters by using all the vertices on the registered meshes. More specifically, we\r\n",
      "first compute a better template by unposing the registered meshes in the first frame of\r\n",
      "each sequence using the LBS skinning weights of the SMPL template, and averaging\r\n",
      "over all the sequences. Using this new template, we optimize pose parameters for each\r\n",
      "frame with an L2-loss on all the registered vertices. Note that in this experiment, we use\r\n",
      "the original template with the refined pose parameters instead of the refined template in\r\n",
      "order not to unfairly favor our method over SNARF [8].\r\n",
      "     In Tab. 1, we report the mean absolute error of scan-to-prediction distance (mm) and\r\n",
      "the mean squared error of volume change for our method and SNARF. Tab. 1 shows that\r\n",
      "SNARF has a large error reduction with refined poses, indicating that SNARF is highly\r\n",
      "sensitive to the accuracy of the SMPL fit. We also observe that after pose refinement,\r\n",
      "SNARF overfits more to training poses (e.g., interpolation) as SNARF cannot model\r\n",
      "history-dependent dynamic deformations. In contrast, our method is more robust to the\r\n",
      "fitting errors, and significantly outperforms SNARF in most settings except for 16-30\r\n",
      "rollouts in the interpolation set. Note that the results with longer rollouts favor “mean”\r\n",
      "predictions over more dynamic predictions, and do not inform us of the plausibility of\r\n",
      "the synthesized dynamics (see the discussion in Sec. 4.2).\r\n",
      "\r\n",
      "\r\n",
      "B    k-NN vs. Closest Surface Projection\r\n",
      "As discussed in Sec. 3.3, our SDF decoding approach uses k-nearest neighbors (k-NN)\r\n",
      "of the SMPL vertices instead of closest surface projection [20]. Fig. H illustrates the\r\n",
      "limitation of this alternative approach proposed in Neural Actor [20]. As shown in Fig. H,\r\n",
      "we observe that associating a query location with a single closest point on the surface\r\n",
      "leads to poor generalization to unseen poses around regions with multiple body parts in\r\n",
      "close proximity (e.g. around armpits). In contrast, our approach, which associates query\r\n",
      "points with multiple k-NN vertices, produces more plausible surface geometry even for\r\n",
      "unseen poses.\r\n",
      "\r\n",
      "\r\n",
      "C    Limitation: Clothing Deformations\r\n",
      "We also apply our method on the CAPE [25] dataset that contains 4D scans of clothed\r\n",
      "humans. We select the subject 03375 longlong, which exhibits the most visible\r\n",
      "\f",
      "\n",
      "\n",
      "20       Z. Bai et al.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Table B: Quantitative Evaluation on Input Pose Accuracy on Subject 50002. We show the\r\n",
      "results of our approach and SNARF [8] using the poses provided by the AMASS [27] dataset\r\n",
      "and the ones after refinement using all vertices in the registered meshes. While SNARF is greatly\r\n",
      "influenced by the accuracy of pose parameters, the slight improvement in our method illustrates its\r\n",
      "robustness to SMPL fitting errors. In addition, our approach significantly outperforms SNARF\r\n",
      "even after pose refinement in most settings except for the 16-30 rollouts in the interpolation set.\r\n",
      "\r\n",
      "                         (a) Mean Scan-to-Prediction Distance (mm) ↓\r\n",
      "\r\n",
      "                                                 Rollout (# of frames)\r\n",
      "                                       1       2      4         8      16            30\r\n",
      "                                    Interpolation Set\r\n",
      "                          SNARF [8] 7.898 7.715 7.588 7.840 7.898                  8.238\r\n",
      "         AMASS [27]\r\n",
      "                          Ours       1.731 2.127 2.953 4.325 5.606                 6.455\r\n",
      "                          SNARF [8]  3.982 4.001 3.964            4.068    4.029   4.158\r\n",
      "         Refined Poses\r\n",
      "                          Ours       1.417 1.703 2.259            3.241    4.044   4.601\r\n",
      "                                    Extrapolation Set\r\n",
      "                          SNARF [8] 8.083 8.126 8.160             8.246    8.050   8.025\r\n",
      "         AMASS [27]\r\n",
      "                          Ours       1.259 1.479 1.984            2.883    4.023   4.867\r\n",
      "                          SNARF [8]      4.624   4.632    4.672   4.749    4.548   4.447\r\n",
      "         Refined Poses\r\n",
      "                          Ours           1.149   1.329    1.745   2.486    3.313   3.855\r\n",
      "\r\n",
      "                          (b) Mean Squared Error of Volume Change ↓\r\n",
      "\r\n",
      "                                                      Rollout (# of frames)\r\n",
      "                                           2         4          8         16          30\r\n",
      "                                         Interpolation Set\r\n",
      "                         SNARF [8]     0.01623 0.01590 0.01688 0.01703             0.01829\r\n",
      "       AMASS [27]\r\n",
      "                         Ours          0.00990 0.01135 0.01417 0.01597             0.01815\r\n",
      "                         SNARF [8]     0.01401 0.01349       0.01430    0.01426    0.01524\r\n",
      "       Refined Poses\r\n",
      "                         Ours          0.00849 0.01002       0.01248    0.01389    0.01558\r\n",
      "                                        Extrapolation Set\r\n",
      "                         SNARF [8]     0.01228 0.01244       0.01333    0.01292    0.01264\r\n",
      "       AMASS [27]\r\n",
      "                         Ours          0.00602 0.00756       0.00977    0.01082    0.01140\r\n",
      "                         SNARF [8]     0.01094    0.01092    0.01148    0.01099    0.01080\r\n",
      "       Refined Poses\r\n",
      "                         Ours          0.00559    0.00691    0.00871    0.00953    0.01000\r\n",
      "\f",
      "\n",
      "\n",
      "             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling            21\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    Scan         Projection        Ours                 Scan         Projection       Ours\r\n",
      "\r\n",
      "Fig. H: k-NN vs. Closest Surface Projection. While the closest surface projection suffers from\r\n",
      "artifacts around armpits, our SDF decoding based on k-NN produces more plausible surface\r\n",
      "geometry for unseen poses.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "dynamic deformations for clothing. We exclude 6 sequences (athletics, frisbee,\r\n",
      "volleyball, box trial1, swim trial1, twist tilt trial1) from train-\r\n",
      "ing, and use them for testing. We employ as input the template and SMPL poses provided\r\n",
      "by the CAPE dataset for training our model. Note that we approximate raw scans by\r\n",
      "sampling point clouds with surface normals computed on the registered meshes as the\r\n",
      "CAPE dataset only provides registered meshes for 03375 longlong.\r\n",
      "    Please refer to the supplementary video for qualitative results. While our approach\r\n",
      "produces plausible short-term clothing deformations, it remains challenging to model\r\n",
      "dynamically deforming clothing with longer rollouts. Compared to soft-tissue deforma-\r\n",
      "tions, dynamics on clothed humans involve high-frequency deformations and topology\r\n",
      "change, making the learning of clothing dynamics more difficult. We leave this for future\r\n",
      "work.\r\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "\n",
    "print(array_authors[0])\n",
    "print(array_pdf_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1220700",
   "metadata": {},
   "source": [
    "### Extract all word after the term: \"References\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e6468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                AutoAvatar: Autoregressive Neural Fields\\r\\n                     for Dynamic Avatar Modeling\\r\\n\\r\\n        Ziqian Bai1,2∗        Timur Bagautdinov2 Javier Romero2             Michael Zollhöfer2\\r\\n                                    Ping Tan1 Shunsuke Saito2\\r\\n                         1                              2\\r\\n                             Simon Fraser University        Reality Labs Research\\r\\n\\r\\n\\r\\n\\r\\n           Abstract. Neural fields such as implicit surfaces have recently enabled avatar\\r\\n           modeling from raw scans without explicit temporal correspondences. In this work,\\r\\n           we exploit autoregressive modeling to further extend this notion to capture dynamic\\r\\n           effects, such as soft-tissue deformations. Although autoregressive models are\\r\\n           naturally capable of handling dynamics, it is non-trivial to apply them to implicit\\r\\n           representations, as explicit state decoding is infeasible due to prohibitive memory\\r\\n           requirements. In this work, for the first time, we enable autoregressive modeling of\\r\\n           implicit avatars. To reduce the memory bottleneck and efficiently model dynamic\\r\\n           implicit surfaces, we introduce the notion of articulated observer points, which\\r\\n           relate implicit states to the explicit surface of a parametric human body model.\\r\\n           We demonstrate that encoding implicit surfaces as a set of height fields defined on\\r\\n           articulated observer points leads to significantly better generalization compared\\r\\n           to a latent representation. The experiments show that our approach outperforms\\r\\n           the state of the art, achieving plausible dynamic deformations even for unseen\\r\\n           motions. https://zqbai-jeremy.github.io/autoavatar.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n1       Introduction\\r\\nAnimatable 3D human body models are key enablers for various applications ranging\\r\\nfrom virtual try-on to social telepresence [4]. While modeling of human avatars from\\r\\n3D scans without surface registration is gaining more and more attention in recent\\r\\nyears [43,24,48,8,26], complex temporal dynamics are often completely ignored and the\\r\\nresulting deformations are often treated exclusively as a function of the pose parameters.\\r\\nHowever, the body shape is not uniquely determined by the current pose of the human,\\r\\nbut also depends on the history of shape deformations due to secondary motion effects.\\r\\nThe goal of our work is to realistically model these history-dependent dynamic effects\\r\\nfor human bodies without requiring precise surface registration.\\r\\n    To this end, we propose AutoAvatar, a novel autoregressive model for dynamically\\r\\ndeforming human bodies. AutoAvatar models body geometry implicitly - using a signed\\r\\ndistance field (SDF) - and is able to directly learn from raw scans without requiring\\r\\ntemporal correspondences for supervision. In addition, akin to physics-based simulation,\\r\\nAutoAvatar infers the complete shape of an avatar given history of shape and motion. The\\r\\n    ∗\\r\\n        Work done while Ziqian Bai was an intern at Reality Labs Research, Pittsburgh, PA, USA.\\r\\n\\x0c\\n\\n2        Z. Bai et al.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 1: AutoAvatar. Given raw 4D scans with self-intersections, holes, and noise (grey meshes)\\r\\nand fitted SMPL models (blue meshes), AutoAvatar automatically learns highly detailed animatable\\r\\nbody models with plausible secondary motion dynamics without requiring a personalized template\\r\\nor surface registration (right).\\r\\n\\r\\naforementioned properties lead to a generalizable method that models complex dynamic\\r\\neffects including inertia and elastic deformations without requiring a personalized\\r\\ntemplate or precise temporal correspondences across training frames.\\r\\n    To model temporal dependencies in the data, prior work has typically resorted to\\r\\nautoregressive models [39,22,28,45]. While the autoregressive framework naturally\\r\\nallows for incorporation of temporal information, combining it with neural implicit\\r\\nsurface representations [29,36,9] for modeling human bodies is non-trivial. Unlike\\r\\nexplicit shape representations, such neural representations implicitly encode the shape in\\r\\nthe parameters of the neural network and latent codes. Thus, in practice, producing the\\r\\nactual shape requires expensive neural network evaluation at each voxel of a dense spatial\\r\\ngrid [36]. This aspect is particularly problematic for autoregressive modeling, since\\r\\nmost of the successful autoregressive models rely on rollout training [28,19] to ensure\\r\\nstability of both training and inference. Unfortunately, rollout training requires multiple\\r\\nevaluations of the model for each time step, and thus becomes prohibitively expensive\\r\\nboth in terms of memory and compute as the resolution of the spatial grid grows. Another\\r\\napproach would be learning an autoregressive model using latent embeddings that encode\\r\\ndynamic shape information [15]. However, it is infeasible to observe the entire span of\\r\\npossible surface deformations from limited real-world scans, which makes the model\\r\\nprone to overfitting and leads to worse generalization at test time.\\r\\n    By addressing these limitations, we, for the first time, enable autoregressive training\\r\\nof a full-body geometry model represented by a neural implicit surface. To tackle\\r\\nthe scalability issues of rollout training for implicit representations, we introduce the\\r\\nnovel notion of articulated observer points. Intuitively, articulated observer points are\\r\\ntemporally coherent locations on the human body surface which store the dynamically\\r\\nchanging state of the implicit function. In practice, we parameterize the observer points\\r\\nusing the underlying body model [22], and then represent the state of the implicit surface\\r\\nas signed heights with respect to the vertices of the pose-dependent geometry produced by\\r\\nthe articulated model (see Fig. 3a). The number of query points is significantly lower than\\r\\nthe number of voxels in a high-resolution grid, which allows for a significant reduction\\r\\n\\x0c\\n\\n              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling     3\\r\\n\\r\\nin terms of memory and compute requirements, making rollout training tractable for\\r\\nimplicit surfaces. In addition, we demonstrate that explicitly encoding shapes as signed\\r\\nheight fields is less prone to overfitting compared to latent embeddings, a common way\\r\\nto represent autoregressive states [19,52].\\r\\n    Our main contributions are the following:\\r\\n    – The first autoregressive approach for modeling history-dependent implicit surfaces\\r\\n      of human bodies,\\r\\n    – Articulated observer points to enable autogressive training of neural fields, and\\r\\n    – Extensive experiments showing that our approach outperforms existing methods\\r\\n      both on shape interpolation and extrapolation tasks.\\r\\n\\r\\n\\r\\n2     Related Work\\r\\nParametric Human Models Since the anatomical structure of humans is shared across\\r\\nidentities, various methods have been proposed to parameterize shape and pose of\\r\\nhuman bodies from large-scale 3D scan data [3,13,22,33,54,1]. SCAPE [3,13] learns\\r\\nstatistical human model models using triangle deformations. The pioneering work\\r\\nby Allen et al. [2] used a vertex-based representation enhanced with pose-dependent\\r\\ndeformations, but the model was complex and trained with insufficient data, resulting\\r\\nin overfitting. SMPL [22] improved the generalizability of [2] by training on more data\\r\\nand removing the shape dependency in the pose-dependent deformations. More recent\\r\\nworks show that sparsity in the pose correctives reduces spurious correlations [33],\\r\\nand that non-linear deformation bases parameterized by neural networks achieve better\\r\\nmodeling accuracy [54]. While most works focus on modeling static human bodies\\r\\nunder different poses, Dyna [39] and DMPL (Dynamic SMPL) [22] enable parametric\\r\\nmodeling of dynamic deformations by learning a linear autoregressive model. Kim\\r\\net al. [16] combine a volumetric parametric model, VSMPL, with an external layer\\r\\ndriven by the finite element method to enable soft tissue dynamics. SoftSMPL [45]\\r\\nlearns a more powerful recurrent neural network to achieve better generalization to\\r\\nunseen subjects. Xiang et al. [52] model dynamically moving clothing from a history\\r\\nof poses. Importantly, the foundation of the aforementioned works is accurate surface\\r\\nregistration of a template body mesh [6,7], which remains non-trivial. Habermann et\\r\\nal. [12] also model dynamic deformations from a history of poses. While they relax the\\r\\nneed of registration by leveraging image-based supervision, a personalized template is\\r\\nstill required as a preprocessing step.\\r\\n     Recently, neural networks promise to enable the modeling of animatable bodies\\r\\nwithout requiring surface registration or a personalized template [10,24,43,48,8]. These\\r\\nmethods leverage structured point clouds [24,26,56] or 3D neural fields [53] to learn\\r\\nanimatable avatars. Approaches based on neural fields parameterize human bodies as\\r\\ncompositional articulated occupancy networks [10] or implicit surface in canonical space\\r\\nwith linear blend skinning [30,43,8,51] and deformation fields [48,35]. Since implicit\\r\\nsurfaces do not require surface correspondences for training, avatars can be learned from\\r\\nraw scans. Similarly, neural radiance fields [31] have been applied to body modeling to\\r\\nbuild animatable avatars from multi-view images [37,20]. However, these approaches\\r\\nrepresent avatars as a function of only pose parameters, and thus are unable to model\\r\\n\\x0c\\n\\n4              Z. Bai et al.\\r\\n\\r\\n                     Shape Encoding                                   Dynamic Feature Encoding                     Articulation-Aware Shape Decoding\\r\\n    SMPL Poses                                              Temporal Derivatives          Localized Pose\\r\\n                                                             · dH\\r\\n                                                                          p· ≈\\r\\n                                                                               dp                                      Query Point\\r\\n                      Articulated                            H≈                       L(p) = (W ⋅ ωi) ∘ p\\r\\n                       Observer                                 dt             dt                                         q\\r\\n                                                                                                                              k(q)\\r\\n                        Points        Signed Heights\\r\\n\\r\\npt−2, pt−1, pt, pt+1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                             Resample\\r\\n                                                                         UNet                                              SDF decoding\\r\\n    Implicit Shapes                                                                                                            f(q)\\r\\n\\r\\n                                                              ·                                                                              St+1\\r\\n                                      Ht−2, Ht−1, Ht     Ht, {Ht+i}                 Zuv                 Zt+1        Features in\\r\\n                                                       L(pt+1), {L( p· t+i)}\\r\\n                      SDF values                                                                                   Posed Space\\r\\n                                                                                                  Dynamic Latent\\r\\n                                                                                                   Embeddings\\r\\n    St−2, St−1, St\\r\\n\\r\\n\\r\\nFig. 2: Overview. AutoAvatar learns a pose-driven animatable human body model with plausible\\r\\ndynamics including secondary motions. Notice that our approach takes the history of implicit\\r\\nshapes in an autoregressive manner for learning dynamics.\\r\\n𝒩\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ndynamics. While our approach is also based on 3D neural fields to eliminate the need\\r\\nfor surface registration, our approach learns not only pose-dependent deformations but\\r\\nalso history-dependent dynamics by enabling autoregressive training of neural implicit\\r\\nsurfaces.\\r\\n\\r\\nLearning Dynamics Traditionally, physics-based simulation [46] is used to model\\r\\ndynamics of objects. While material parameters of physics simulation can be estimated\\r\\nfrom real data [5,50,47,55], accurately simulating dynamic behavior of objects remains\\r\\nan open question. In addition, authenticity of physics-based simulation is bounded by the\\r\\nunderlying model, and complex anisotropic materials such as the human body are still\\r\\nchallenging to model accurately. For this reason, several works attempt to substitute a\\r\\ndeterministic physics-based simulation with a learnable module parameterized by neural\\r\\nnetworks [14,57,44,38]. Such approaches have been applied to cloth simulation [14,38],\\r\\nfluid [44], and elastic bodies [57]. Subspace Neural Physics [14] learns a recurrent\\r\\nneural network from offline simulation to predict the simulation state in a subspace.\\r\\nDeep Emulator [57] first learns an autoregressive model to predict deformations using a\\r\\nsimple primitive (sphere), and applies the learned function to more complex characters.\\r\\nWhile we share the same spirit with the aforementioned works by learning dynamic\\r\\ndeformations in an autoregressive manner, our approach fundamentally differs from\\r\\nthem. The aforementioned approaches all assume that physical quantities such as vertex\\r\\npositions are observable with perfect correspondence in time, and thus results are only\\r\\ndemonstrated on synthetic data. In contrast, we learn dynamic deformations from real-\\r\\nworld observation while requiring only coarse temporal guidance by the fitted SMPL\\r\\nmodels. This property is essential to model faithful dynamics of real humans.\\r\\n\\r\\n\\r\\n3        Method\\r\\nOur approach is an autoregressive model, which takes as inputs human poses and a shape\\r\\nhistory and produces the implicit surface for a future frame. Fig. 2 shows the overview\\r\\nof our approach. Given a sequence of T implicitly encoded shapes {St−T +1 , ..., St }\\r\\nand T + 1 poses {pt−T +1 , ..., pt+1 } with t being the current time frame, our model\\r\\npredicts the implicit surface St+1 of the future frame t + 1. The output shape St+1 is\\r\\n\\x0c\\n\\n                   AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling                                                             5\\r\\n\\r\\n\\r\\n                                                                           ci = cos(q − vi, ni)\\r\\n                        Articulated\\r\\n                         Observer\\r\\n                                                                           di = ∥q − vi∥2\\r\\n                          Points\\r\\n\\r\\n                                                                                           [zit+1, di, ci]\\r\\n                                                                                 vi ni           …\\r\\n                           Signed\\r\\n                                    Height\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                             PointNet\\r\\n                                                                                           [zjt+1, dj, cj]\\r\\n                                                                                     q\\r\\n                                                                                           [zkt+1, dk, ck]\\r\\n                                                                    v{i,j,k} ∈      k(q)\\r\\n                                                         Zt+1                                                                    St+1\\r\\n      Posed SMPL                                 Features in Posed Space                                                Signed Distance Functions\\r\\n\\r\\n         (a) Shape Encoding.                               (b) Articulation-Aware SDF Decoding.\\r\\n                                             𝒩\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3: Shape Encoding/Decoding. Our novel shape encoding via articulated observer points and\\r\\narticulated-aware SDF decoding lead to faithful modeling of dynamics.\\r\\n\\r\\nthen passed as an input to the next frame prediction in an autoregressive manner. Our\\r\\nmodel is supervised directly with raw body scans, and requires a training dataset of 4D\\r\\nscans (sequences of 3D scans) along with fitted SMPL body models [22]. Unfortunately,\\r\\nexplicitly representing shapes St as levelsets of implicit surface is prohibitively expensive\\r\\nfor end-to-end training. To this end, we introduce the concept of articulated observer\\r\\npoints - vertex locations on the underlying articulated model - which are used as a local\\r\\nreference for defining the full body geometry. The underlying implicit surface is encoded\\r\\nas a height field with respect to the articulated observer points (Sec. 3.1). Given a history\\r\\nof height fields and pose parameters, we convert those to dynamic latent feature maps\\r\\nin UV space (Sec. 3.2). Finally, we map the resulting features to SDFs by associating\\r\\ncontinuous 3D space with the learned features on the SMPL vertices, which are directly\\r\\nsupervised by point clouds with surface normals (Sec. 3.3).\\r\\n\\r\\n3.1     Shape Encoding via Articulated Observer Points\\r\\nThe core of our approach is an autoregressive model that operates on implicit neural\\r\\nsurfaces, allowing us to incorporate temporal shape information necessary for modeling\\r\\nchallenging dynamics. The key challenge that arises when training such autoregressive\\r\\nmodels is finding a way to encode the shape - parameterized implicitly as a neural field -\\r\\ninto a representation that can be efficiently computed and fed back into the model. The\\r\\nmost straightforward way is to extract an explicit geometry representation by evaluating\\r\\nthe neural field on a dense spatial grid and running marching cubes [23]. However, in\\r\\npractice this approach is infeasible due to prohibitive memory and computational costs,\\r\\nin particular due to the cubic scaling with respect to the grid dimensions. Instead, we\\r\\npropose to encode the state of the implicit surface into a set of observer points.\\r\\n    Encoding geometry into discrete point sets has been shown to be efficient and\\r\\neffective for learning shape representations from point clouds [40]. Prokudin et al. [40]\\r\\nrelies on a fixed set of randomly sampled observer points in global world coordinates,\\r\\nwhich is not suitable for modeling dynamic humans due to the articulated nature of human\\r\\nmotion. Namely, a model relying on observer points with a fixed 3D location needs to\\r\\naccount for extremely large shape variations including rigid transformations, making\\r\\nthe learning task difficult. Moreover, associating randomly sampled 3D points with a\\r\\nparametric human body is non-trivial. To address these limitations, we further extend the\\r\\n\\x0c\\n\\n6       Z. Bai et al.\\r\\n\\r\\nnotion of observer points to an articulated template represented by the SMPL model [22],\\r\\nwhich provides several advantages for modeling dynamic articulated geometries. In\\r\\nparticular, soft-tissue dynamic deformations appear only around the minimally clothed\\r\\nbody, and we can rely on this notion as an explicit prior to effectively allocate observer\\r\\npoints only to the relevant regions. In addition, the SMPL model provides a mapping of\\r\\n3D vertices to a common UV parameterization, allowing us to effectively process shape\\r\\ninformation using 2D CNNs in a temporally consistent manner.\\r\\n     More specifically, to encode the neural implicit surface into the articulated observer\\r\\npoints, we compute “signed heights” H = {hi }M      i=1 ∈ R\\r\\n                                                            M\\r\\n                                                               from M vertices on a fitted\\r\\nSMPL model. For each vertex, the signed height hi is the signed distance from the\\r\\nvertex to the zero-crossing of the implicit surface along the vertex normal (see Fig. 3a).\\r\\nWe use the iterative secant method as in [32] to compute the zero-crossings. Note that\\r\\nthere can be multiple valid signed heights per vertex since the line along the normal can\\r\\nhit the zero-crossing multiple times. Based on the observation that the SMPL vertices\\r\\nare usually close to the actual surface with their normals roughly facing into the same\\r\\ndirection, we use the minimum signed height within a predefined range [hmin , hmax ]\\r\\n(in our experiments, we use hmin = −2cm, hmax = 8cm). If no zero-crossing is found\\r\\ninside this range, we set the signed height to hmin . Note that the computed heights\\r\\nare signed because the fitted SMPL can go beyond the actual surface due to its limited\\r\\nexpressiveness and inaccuracy in the fitting stage.\\r\\n\\r\\n3.2   Dynamic Feature Encoding\\r\\nThe essence of AutoAvatar is an animatable autoregressive model. In other words,\\r\\na reconstructed avatar is driven by pose parameters, while secondary dynamics is\\r\\nautomatically synthesized from the history of shapes and poses. To enable this, we learn\\r\\na mapping that encodes the history of shape and pose information to latent embeddings\\r\\ncontaining the shape information of the future frame. More specifically, denoting the\\r\\ncurrent time frame as t, we take as input T + 1 poses {pt−T +1 , ..., pt+1 } and T signed\\r\\nheights vectors {Ht−T +1 , ..., Ht }, and produce dynamic features Zt+1 ∈ RM ×C .\\r\\nGiven these inputs, we also compute the temporal derivatives of poses {ṗt+i }1i=−T +2\\r\\nand signed heights {Ḣt+i }0i=−T +2 as follows:\\r\\n\\r\\n                                               \\\\begin {aligned} \\\\dot {\\\\bm {p}}_{k} &= \\\\bm {p}_{k} \\\\bm {p}_{k-1}^{-1} \\\\\\\\ \\\\dot {\\\\bm {H}}_{k} &= \\\\bm {H}_{k} - \\\\bm {H}_{k-1}. \\\\end {aligned} \\r\\n                                                                                                                                                                                             (1)\\r\\n\\r\\nNote that in practice p∗ are represented as quaternions, and pk p−1\\r\\n                                                                 k−1 is computed by first\\r\\nconverting multipliers to rotation matrices, multiplying those, and then converting the\\r\\nproduct back to quaternions. To emphasize small values in Ḣ, we apply the following\\r\\ntransformation g(x) = sign(x) · ln(α|x| + 1) · β, where α = 1000 and β = 0.25.\\r\\nFollowing prior works [43,4], we also localize pose parameters to reduce long range\\r\\nspurious correlations as follows:\\r\\n                                  \\\\begin {aligned} L(\\\\bm {p}) &= (W \\\\cdot \\\\omega _i) \\\\circ \\\\bm {p}, \\\\end {aligned}                                                                           (2)\\r\\nwhere ◦ denotes the element-wise product, i is the vertex index, W ∈ RJ×J is an\\r\\nassociation matrix of J joints, and ωi ∈ RJ×1 is the skinning weights of the i-th vertex.\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling                                                                     7\\r\\n\\r\\nWe set Wn,m = 1 if the n-th joint is within the 1-ring neighborhood of the m-th joint\\r\\n(otherwise Wn,m = 0). Note that the derivative of the root transformation is included in\\r\\n{L(ṗt+i )} without localization. Finally, we map Ht , {Ḣt+i }, L(pt+1 ), and {L(ṗt+i )}\\r\\nto UV space using barycentric interpolation. The concatenated features are fed into a\\r\\nUNet [42] to generate a feature map Zuv . We then resample Zuv on the UV coordinates\\r\\ncorresponding to SMPL vertices to obtain the per-vertex dynamic latent embeddings\\r\\nZ. We empirically found that incorporating temporal derivatives further improves the\\r\\nrealism of dynamics (see Supp. Mat. video for comparison).\\r\\n\\r\\n3.3   Articulation-Aware Shape Decoding\\r\\nGiven the dynamic feature Zt+1 = {z1t+1 , ..., zM   t+1\\r\\n                                                        } and a query point q, we decode\\r\\nsigned distance fields f (q) to obtain the surface geometry of the dynamic avatar. Several\\r\\nmethods model the implicit surface in canonical space by jointly learning a warping\\r\\nfunction from the posed space to the canonical space [43,8]. However, we observe that\\r\\nthe canonicalization step is very sensitive to small fitting error in the SMPL model, and\\r\\nfurther amplifies the error in the canonical space, making it difficult to learn dynamics\\r\\n(see discussion in Sec. 4). Therefore, we directly model the implicit surface in a posed\\r\\nspace while being robust to pose changes. Inspired by Neural Actor [20], we associate\\r\\na queried 3D point with a human body model and pose-agnostic spatial information.\\r\\nSpecifically, Neural Actor uses height from the closest surface point on the SMPL\\r\\nmodel to the query location together with a feature vector sampled on the same surface\\r\\npoint. However, we find that their approach based on the single closest point leads\\r\\nto artifacts around body joints (e.g., armpits) for unseen poses. To better distinguish\\r\\nregions with multiple body parts, we instead use k-nearest neighbor vertices. Fig. 3b\\r\\nshows the illustration of our SDF decoding approach. Given a query point q, we first\\r\\ncompute the k-nearest SMPL vertices {vi }i∈Nk (q) , where Nk (q) is a set of indices\\r\\nof k-nearest neighbor vertices. To encode pose-agnostic spatial information, we use\\r\\nrotation-invariant features. Specifically, we compute the distance di = ∥q − vi ∥2 and\\r\\ncosine value ci = cos(xi , ni ), where xi is the vertex-to-query vector \\x02 xi = q −\\x03vi , and\\r\\nni is the surface normal on vi . We feed the concatenated vector zit+1 , di , ci into a\\r\\nPointNet-like [41] architecture to compute the final SDFs with the max pooling replaced\\r\\nby a weighted average pooling based on jointly predicted weights for better continuity.\\r\\n    As in [43], we employ implicit geometric regularization (IGR) [11] to train our\\r\\nmodel directly from raw scans without requiring watertight meshes. Note that in contrast,\\r\\nother methods [30,8,48] require watertight meshes to compute ground-truth occupancy\\r\\nor signed distance values for training. Our final objective function L is the following:\\r\\n                             \\\\label {eq:igr} \\\\begin {aligned} L &= L_{s} + L_{n} + \\\\lambda _{igr} L_{igr} + \\\\lambda _{o} L_{o}, \\\\\\\\ \\\\end {aligned}    (3)\\r\\nwhere λigr = 1.0, λo = 0.1. Ls promotes SDFs which vanish on the ground    Ptruth surface,\\r\\nwhile LPn encourages that its normal align with the ones from data: Ls =     q∈Qs |f (q)|,\\r\\nLn =      q∈Qs  ∥∇ q f (q) −  n(q)∥  2 , where Q s is the surface of the input raw scans.\\r\\nLigr is the Eikonal regularization term [11] that encourages the function f to satisfy\\r\\nthe Eikonal equation: Ligr = Eq (∥∇q f (q)∥2 − 1)2 , and Lo prevents off-surface SDF\\r\\nvalues from being too close to the zero-crossings as follows: Lo = Eq (exp(−γ ·|f (q)|)),\\r\\nwhere γ = 50.\\r\\n\\x0c\\n\\n8       Z. Bai et al.\\r\\n\\r\\n3.4   Implementation Details\\r\\nNetwork Architectures. In our experiments, we use a UV map of resolution 256 × 256,\\r\\nT = 3, and k = 20. To reduce the imbalance of SMPL vertex density for k-NN\\r\\ncomputation, we use 3928 points subsampled by poisson-disk sampling on the SMPL\\r\\nmesh. Before being fed into the UNet, L(pk+1 ) and {L(ṗt+i )} are compressed to 32\\r\\nchannels using 1×1 convolutions. The UNet uses convolution and transposed convolution\\r\\nlayers with untied biases, a kernel size of 3, no normalization, and LeakyReLU with a\\r\\nslope of 0.2 as the non-linear activation, except for the last layer which uses TanH. The\\r\\nSDF decoder is implemented as an MLP, which takes as input 64-dim features from the\\r\\nUNet, positional encoded di and ci up to 4-th order Fourier features. The number of\\r\\nintermediate neurons in the first part of the MLP is (128, 128, 129), where the output\\r\\nis split into a 128-dim feature vector and a 1-dim scalar, which is converted into non-\\r\\nnegative weights by softmax across the k-NN samples. After weighted average pooling,\\r\\nthe aggregated feature is fed into another MLP with a neuron size of (128, 128, 1) to\\r\\npredict the SDF values. The MLPs use Softplus with β = 100 and a threshold of 20 as\\r\\nnon-linear activation except for the last layer which does not apply any activation.\\r\\nTraining. Our training consists of two stages. First, we train our model using ground-\\r\\ntruth signed heights without rollout for 90000 iterations. Then, we finetune the model\\r\\nusing a rollout of 2 frames for another 7500 iterations to reduce error accumulation for\\r\\nboth training and inference. We use the Adam optimizer with a learning rate of 1.0×10−4\\r\\n(1.0 × 10−5 ) at the first (second) stage. To compute Ln , we sample 10000(1000) points\\r\\non the scan surface. Similarly, for Ligr , we sample 10000(1000) points around the scan\\r\\nsurface by adding Gaussian noise with standard deviation of 10cm to uniformly sampled\\r\\nsurface points, and sample 2000(500) points within the bounding box around the raw\\r\\nscans. The points uniformly sampled inside the bounding box are also used to compute\\r\\nLo . Both stages are trained with a batch size of 1.\\r\\nInference. At the beginning of the animations, we assume ground-truth raw scans are\\r\\navailable for the previous T frames for initialization. If no ground truth initial shape is\\r\\navailable, we initialize the first T frames with our baseline model conditioned only on\\r\\npose parameters. Note that the scan data is extremely noisy around the hand and foot\\r\\nareas, and the SMPL fitting of the head region is especially inaccurate. Therefore, we fix\\r\\nthe dynamic features on the face, hands, and feet to the ones of the first frame.\\r\\n\\r\\n\\r\\n4     Experimental Results\\r\\n4.1   Datasets and Metrics\\r\\nDatasets. We use the DFaust dataset [7] for both training and quantitative evaluation,\\r\\nand AIST++ [49,18] for qualitative evaluation on unseen motions. For the DFaust\\r\\ndataset, we choose 2 subjects (50002 and 50004), who exhibit the most soft-tissue\\r\\ndeformations. The interpolation test evaluates the fidelity of dynamics under the same\\r\\ntype of motions as in training but at different time instance, and the extrapolation\\r\\ntest evaluates performance on unseen motion. For 50002, we use the 2nd half of\\r\\nchicken wings and running on spot for the interpolation test, one leg jump\\r\\nfor the extrapolation test, and the rest for training. For 50004, we use the 2nd half of\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling        9\\r\\n\\r\\nchicken wings and running on spot for interpolation, one leg loose for\\r\\nextrapolation, and the rest for training. The fitted SMPL parameters in DFaust are\\r\\nprovided by the AMASS [27] dataset that uses sparse points on the registered data as\\r\\napproximated mocap marker locations and computes the parameters using MoSh [21].\\r\\nNote that more accurate pose can be obtained by using all the registration vertices (see\\r\\nAppendix A), but this is not required by our method to recover soft-tissue deformation.\\r\\nMetrics. For evaluation, we extract the 0-level set surface at each time step using\\r\\nMarching Cubes [23] with a resolution of 2563 . We also use simplified scans with\\r\\naround 10000 vertices and outlier points (distance to the nearest SMPL vertex larger\\r\\nthan 10cm) have been removed. We evaluate the accuracy of the predicted surface in\\r\\nterms of its position and dynamics accuracy. The surface position accuracy is measured\\r\\nby averaging the distance from each simplified scan vertex to the closest prediction\\r\\nsurface point. Evaluating the dynamics accuracy of the implicit surface efficiently is more\\r\\nchallenging. We approximate the local occupied volume as a scalar per registration vertex\\r\\nrepresenting the ratio of surrounding points contained in the interior of the (ground-truth\\r\\nor inferred) surface. We use 10 points uniformly sampled inside a 5cm cube centered at\\r\\nthe vertex. The head, hands and feet vertices are ignored due to their high noise levels.\\r\\nThe temporal difference of this scalar across adjacent frames can be interpreted as a\\r\\ndynamic measure of the local volume evolution. We report the mean square difference\\r\\nbetween this dynamic descriptor as computed with the ground truth simplified scan\\r\\nand the inferred implicit surface. Since a small phase shift in dynamics may lead to\\r\\nlarge cumulative error, reporting only the averaged errors from the entire frames can\\r\\nbe misleading. Therefore, we report errors along the progression of rollout predictions.\\r\\nFor each evaluation sequence, we start prediction every 20 frames (i.e., 20th frame, 40th\\r\\nframe, ...), and use the ground truth pose and shape history only for the first frame,\\r\\nfollowed by the autoregressive predictions for the error computation. In Tab. 1, we\\r\\nreport the averaged errors for both metrics after 1, 2, 4, 8, 16, and 30 rollouts. The\\r\\nerrors for small number of rollouts evaluate the accuracy of future shape prediction\\r\\ngiven the ground-truth shape history, whereas the errors with longer rollouts evaluate\\r\\nthe accumulated errors by autoregressively taking as input the predictions of previous\\r\\nframes. We discuss the limitation of error metrics with longer rollouts in Sec. 4.2.\\r\\n\\r\\n\\r\\n4.2   Evaluation\\r\\n\\r\\nIn this section, we provide comprehensive analysis to validate our design choices and\\r\\nhighlight the limitations of alternative approaches and SoTA methods based on both\\r\\nimplicit and explicit shape representations. Note that all approaches use the same training\\r\\nset, and are trained with the same number of iterations as our method for fair comparison.\\r\\nEffectiveness of Autoregressive Modeling. While autoregressive modeling is a widely\\r\\nused technique for learning dynamics [39,57,38], several recent methods still employ\\r\\nonly the history of poses for modeling dynamic avatars [52,12]. Thus, to evaluate the\\r\\neffectiveness of autoregressive modeling we compare AutoAvatar with pose-dependent\\r\\nalternatives that use neural implicit surfaces. More specifically, we design the following\\r\\n3 non-autoregressive baselines:\\r\\n\\x0c\\n\\n10        Z. Bai et al.\\r\\n\\r\\n\\r\\nTable 1: Quantitative Comparison with Baseline Methods. Our method produces the most\\r\\naccurate predictions of the future frames given the ground-truth shape history among all baseline\\r\\nmethods (see rollout 1-4). For longer rollouts, more dynamic predictions lead to higher error than\\r\\nless dynamic results due to high sensitivity to initial conditions in dynamic systems [34] (see\\r\\ndiscussion in Sec. 4.2).\\r\\n\\r\\n                      (a) Mean Scan-to-Prediction Distance (mm) ↓ on DFaust.\\r\\n\\r\\n                                                          Rollout (# of frames)\\r\\n                                               1       2       4         8      16      30\\r\\n                                        Interpolation Set\\r\\n                              SNARF [8]      7.428 7.372 7.337 7.476 7.530             7.656\\r\\n                              Pose           4.218 4.202 4.075 4.240 4.409             4.426\\r\\n      Non-Autoregressive\\r\\n                              PoseTCN        4.068 4.118 4.086 4.228 4.405             4.411\\r\\n                              Pose + dPose 3.852 3.841 3.764 3.972 4.164               4.156\\r\\n                              G-embed       2.932 3.006      3.131   3.462     3.756   3.793\\r\\n      Autoregressive          L-embed       1.784 2.138      2.863   4.250     5.448   5.916\\r\\n                              Ours          1.569 1.914      2.587   3.627     4.736   5.255\\r\\n                                        Extrapolation Set\\r\\n                              SNARF [8]     7.264 7.287      7.321   7.387     7.308   7.251\\r\\n                              Pose          4.303 4.306      4.308   4.299     4.385   4.398\\r\\n      Non-Autoregressive\\r\\n                              PoseTCN       4.090 4.091      4.105   4.119     4.233   4.257\\r\\n                              Pose + dPose 3.984 3.991       4.017   4.063     4.162   4.190\\r\\n                              G-embed        2.884   2.926   3.043   3.258     3.577   3.787\\r\\n      Autoregressive          L-embed        1.329   1.539   2.079   3.326     4.578   5.192\\r\\n                              Ours           1.150   1.361   1.834   2.689     3.789   4.526\\r\\n\\r\\n                      (b) Mean Squared Error of Volume Change ↓ on DFaust.\\r\\n\\r\\n                                                          Rollout (# of frames)\\r\\n                                              2         4           8         16         30\\r\\n                                        Interpolation Set\\r\\n                            SNARF [8]     0.01582 0.01552 0.01610 0.01658              0.01682\\r\\n                            Pose          0.01355 0.01305 0.01341 0.01367              0.01387\\r\\n     Non-Autoregressive\\r\\n                            PoseTCN       0.01364 0.01323 0.01350 0.01399              0.01416\\r\\n                            Pose + dPose 0.01288 0.01247 0.01273 0.01311               0.01321\\r\\n                            G-embed       0.01179 0.01168       0.01199    0.01248     0.01265\\r\\n     Autoregressive         L-embed       0.01003 0.01180       0.01466    0.01716     0.01844\\r\\n                            Ours          0.00902 0.01053       0.01258    0.01456     0.01565\\r\\n                                        Extrapolation Set\\r\\n                            SNARF [8]     0.01178 0.01194       0.01251    0.01228     0.01206\\r\\n                            Pose          0.01027 0.01039       0.01074    0.01052     0.01039\\r\\n     Non-Autoregressive\\r\\n                            PoseTCN       0.01020 0.01038       0.01064    0.01040     0.01029\\r\\n                            Pose + dPose 0.00992 0.01014        0.01048    0.01029     0.01013\\r\\n                            G-embed        0.00936   0.00959    0.00995    0.00996     0.00998\\r\\n     Autoregressive         L-embed        0.00648   0.00821    0.01100    0.01308     0.01402\\r\\n                            Ours           0.00567   0.00715    0.00915    0.01039     0.01107\\r\\n\\x0c\\n\\n                   AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling      11\\r\\n\\r\\n\\r\\n\\r\\n    SMPL\\r\\n    Pose\\r\\n    PoseTCN\\r\\n    Pose + dPose\\r\\n    Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 Time\\r\\n\\r\\n\\r\\nFig. 4: Qualitative Comparison with Non-Autoregressive Baselines. In contrast to the rigid\\r\\nresults in non-autoregressive baselines, our approach produces high quality non-rigid dynamics.\\r\\n\\r\\n 1. Pose: We only feed pose parameters of the next frame L(pt+1 ) in our architec-\\r\\n     ture. Prior avatar modeling methods based on neural fields employ this pose-only\\r\\n     parameterization [43,48,8].\\r\\n 2. PoseTCN: Temporal convolutional networks (TCN) [17] support the incorporation\\r\\n     of a long-range history for learning tasks, and have been used in several avatar\\r\\n     modeling methods [52,12]. Thus, we use a TCN that takes as input the sequence\\r\\n     of poses with the length of 16. We first compute localized pose parameters, as in\\r\\n     our method, for each frame and apply the TCN to obtain 64-dim features for each\\r\\n     SMPL vertex. The features are then fed into the UNet and SDF decoders identical\\r\\n     to our method.\\r\\n 3. Pose+dPose: Our approach without autoregressive components (Ht , {Ḣt+i }).\\r\\n    Tab. 1 shows that our approach outperforms the baseline methods for the first 8\\r\\nframes for interpolation, and first 16 frames for extrapolation. In particular, there is a\\r\\nsignificantly large margin for the first 4-8 frames, indicating that our method achieves\\r\\nthe most accurate prediction of the future frames given the ground-truth shape history.\\r\\nWe also observe that the non-autoregressive methods tend to collapse to predicting the\\r\\n“mean” shape under each pose without faithful dynamics for unseen motions (see Fig. 4\\r\\nand Supp. Mat. video). Since the accumulation of small errors in each frame may lead\\r\\nto large deviations from the ground-truth due to high sensitivity to initial conditions\\r\\nin dynamic systems [34], for longer rollouts mean predictions without any dynamics\\r\\ncan produce lower errors than more dynamic predictions. In fact, although our method\\r\\nleads to slightly higher errors on longer rollouts, Fig. 4 clearly shows that our approach\\r\\nproduces the most visually plausible dynamics on the AIST++ sequences. Importantly,\\r\\nwe do not observe any instability or explosions in our autoregressive model for longer\\r\\nrollouts, as can be seen from the error behavior shown in Fig. 7. We also highly encourage\\r\\n\\x0c\\n\\n12             Z. Bai et al.\\r\\n\\r\\n\\r\\n\\r\\n     SMPL\\r\\n     G-embed\\r\\n     L-embed\\r\\n     Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              Time\\r\\n\\r\\n\\r\\nFig. 5: Qualitative Comparison with Latent-space Autoregression. While the latent space-\\r\\nbased autoregression approaches suffer either from overfitting to pose parameters (G-embed)\\r\\nor instability (L-embed, see Supp. Mat. video), our approach based on a physically meaningful\\r\\nquantity (signed height) achieves the most stable and expressive synthesis of dynamics.\\r\\n\\r\\nreaders to see Supp. Mat. video for qualitative comparison in animation. In summary,\\r\\nour results confirm that autoregressive modeling plays a critical role for generalization\\r\\nto unseen motions and improving the realism of dynamics.\\r\\nExplicit Shape Encoding vs. Latent Encoding. Efficiently encoding the geometry of\\r\\nimplicit surfaces is non-trivial. While our proposed approach encodes the geometry via\\r\\nsigned heights on articulated observer points, prior approaches have demonstrated shape\\r\\nencoding based on a learned latent space [36,4]. Therefore, we also investigate different\\r\\nencoding methods for autoregressive modeling with the following 2 baselines:\\r\\n 1. G-embed: Inspired by DeepSDF [36], we first learn per-frame global embeddings\\r\\n    lg ∈ R512 with the UNet and SDF decoder by replacing Ht , {Ḣt+i }, {L(ṗt+i )}\\r\\n    with repeated global embeddings. Then, we train a small MLP with three 512-dim\\r\\n    hidden layers using Softplus except for the last layer, taking as input pt+1 , {ṗt+i },\\r\\n    and 3 embeddings of previous frames to predict the global embedding at time t + 1.\\r\\n 2. L-embed: For modeling mesh-based body avatars, localized embeddings are shown\\r\\n    to be effective [4]. Inspired by this, we also train a model with localized embeddings\\r\\n    ll ∈ R16×64×64 . We first learn per-frame local embeddings ll together with the UNet\\r\\n    and SDF decoder by replacing Ht , {Ḣt+i }, {L(ṗt+i )} with bilinearly upsampled\\r\\n    ll . Then we train another UNet that takes as input L(pt+1 ), {L(ṗt+i )}, and 3\\r\\n    embeddings of previous frames to predict the localized embeddings at time t + 1.\\r\\n    Note that for evaluation, we optimize per-frame embeddings for test sequences using\\r\\nEq. (3) such that the baseline methods can use the best possible history of embeddings\\r\\nfor autoregression. Tab. 1 shows that our method outperforms L-embed in all cases\\r\\nbecause L-embed becomes unstable for the test sequences. For G-embed, we observe the\\r\\nsame trend as for the non-autoregressive baselines: our approach achieves significantly\\r\\n\\x0c\\n\\n              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling              13\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 Jump to top\\r\\n                                                 Fall to bottom\\r\\n              Scan       SNARF       Ours                         Scan    SoftSMPL   Ours\\r\\n\\r\\n\\r\\nFig. 6: Qualitative Comparison with SoTA Methods. Our approach produces significantly more\\r\\nfaithful shapes and dynamics than the state-of-the-art implicit avatar modeling method [8], and\\r\\nshows comparable dynamics with prior art dependent on registrations with fixed topology [45].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n       (a) Scan-to-Prediction Distance ↓                           (b) MSE of Volume Change ↓\\r\\n\\r\\nFig. 7: Comparison with SoftSMPL [45]. We plot the errors on the sequence of\\r\\none leg loose for subject 50004. Surprisingly, our registration-free approach mostly out-\\r\\nperforms this baseline that has to rely on registered data with fixed topology.\\r\\n\\r\\nmore accurate predictions of the future frames given the ground-truth trajectories (see the\\r\\nerrors for 1-4 rollouts), and G-embed tends to predict “mean” shapes without plausible\\r\\ndynamics. The qualitative comparison in Fig. 5 confirms that our approach produces\\r\\nmore plausible dynamics. Please refer to Supp. Mat. video for detailed visual comparison\\r\\nin animation. We summarize that physically meaningful shape encodings (e.g., signed\\r\\nheights) enable more stable learning of dynamics via autoregression than methods relying\\r\\non latent space.\\r\\nComparison to SoTA Methods.. We compare our approach with state-of-the-art meth-\\r\\nods for both implicit surface representations and mesh-based representations. As a\\r\\nmethod using neural implicit surfaces, we choose SNARF [8], which jointly learns a\\r\\npose-conditioned implicit surface in a canonical T-pose and a forward skinning network\\r\\nfor reposing. Similar to ours, SNARF does not require temporal correspondences other\\r\\nthan the fitted SMPL models. We use the training code released by the authors using\\r\\nthe same training data and the fitted SMPL parameters as in our method. Note that in\\r\\nthe DFaust experiment in [8], SNARF is trained using only the fitted SMPL models\\r\\nto DFaust as ground-truth geometry, which do not contain any dynamic deformations.\\r\\n\\x0c\\n\\n14      Z. Bai et al.\\r\\n\\r\\nTab. 1 shows that our approach significantly outperforms SNARF for any number of\\r\\nrollouts. Interestingly, SNARF can produce dynamic effects for training data by severely\\r\\noverfitting to the pose parameters, but this does not generalize to unseen poses as the\\r\\nlearned dynamics is the results of spurious correlations. As mentioned in Sec. 3.3, we\\r\\nalso observe that the performance of SNARF heavily relies on the accuracy of the\\r\\nSMPL fitting for canonicalization, and any small alignment errors in the underlying\\r\\nSMPL registration deteriorates their test-time performance (see Fig. 6). Therefore, this\\r\\nexperiment demonstrates not only the importance of autoregressive dynamic avatar\\r\\nmodeling, but also the efficacy of our articulation-aware shape decoding approach given\\r\\nthe quality of available SMPL fitting for real-world scans.\\r\\n    We also compare against SoftSMPL [45], a state-of-the-art mesh-based method that\\r\\nlearns dynamically deforming human bodies from registered meshes. The authors of\\r\\nSoftSMPL kindly provide their predictions on the sequence of one leg loose for\\r\\nsubject 50004, which is excluded from training for both our method and SoftSMPL for\\r\\nfair comparison. To our surprise, Fig. 7 show that our results are slightly better on both\\r\\nmetrics for the majority of frames, although we tackle a significantly harder problem\\r\\nbecause our approach learns dynamic bodies directly from raw scans, whereas SoftSMPL\\r\\nlearns from the carefully registered data. We speculate that the lower error may be mainly\\r\\nattributed to the higher resolution of our geometry using implicit surfaces in contrast\\r\\nto their predictions on the coarse SMPL topology (see Fig. 6). Nevertheless, this result\\r\\nis highly encouraging as our approach achieves comparable performance on dynamics\\r\\nmodeling without having to rely on surface registration.\\r\\n\\r\\n\\r\\n5    Conclusion\\r\\nWe have introduced AutoAvatar, an autoregressive approach for modeling high-fidelity\\r\\ndynamic deformations of human bodies directly from raw 4D scans using neural\\r\\nimplicit surfaces. The reconstructed avatars can be driven by pose parameters, and\\r\\nautomatically incorporate secondary dynamic effects that depend on the history of\\r\\nshapes. Our experiments indicate that modeling dynamic avatars without relying on\\r\\naccurate registrations is made possible by choosing an efficient representation for our\\r\\nautoregressive model.\\r\\nLimitations and Future Work. While our method has shown to be effective in modeling\\r\\nthe elastic deformations of real humans, we observe that it remains challenging, yet\\r\\npromising, to model clothing deformations that involve high-frequency wrinkles (see\\r\\nAppendix C for details). Our evaluation also suggests that ground-truth comparison\\r\\nwith longer rollouts may not reliably reflect the plausibility of dynamics. Quantitative\\r\\nmetrics that handle the high sensitivity to initial conditions in dynamics could be further\\r\\ninvestigated. Currently, AutoAvatar models subject-specific dynamic human bodies,\\r\\nbut generalizing it to multiple identities, as demonstrated in registration-based shape\\r\\nmodeling [39,22,45], is an interesting direction for future work. The most exciting venue\\r\\nfor future work is to extend the notion of dynamics to image-based avatars [37,20]. In\\r\\ncontrast to implicit surfaces, neural radiance fields [31] do not have an explicit “surface”\\r\\nas they model geometry using density fields. While this remains an open question, we\\r\\nbelieve that our contributions in this work such as efficiently modeling the state of shapes\\r\\nvia articulated observer points might be useful to unlock this application.\\r\\n\\x0c\\n\\n              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling             15\\r\\n\\r\\nReferences\\r\\n 1. Alldieck, T., Xu, H., Sminchisescu, C.: imghum: Implicit generative models of 3d human\\r\\n    shape and articulated pose. In: Proc. of International Conference on Computer Vision (ICCV).\\r\\n    pp. 5461–5470 (2021) 3\\r\\n 2. Allen, B., Curless, B., Popović, Z., Hertzmann, A.: Learning a correlated model of identity\\r\\n    and pose-dependent body shape variation for real-time synthesis. In: Proceedings of the 2006\\r\\n    ACM SIGGRAPH/Eurographics symposium on Computer animation. pp. 147–156 (2006) 3\\r\\n 3. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: SCAPE: shape\\r\\n    completion and animation of people. ACM Trans. on Graphics (TOG) 24(3), 408–416 (2005)\\r\\n    3\\r\\n 4. Bagautdinov, T., Wu, C., Simon, T., Prada, F., Shiratori, T., Wei, S.E., Xu, W., Sheikh, Y.,\\r\\n    Saragih, J.: Driving-signal aware full-body avatars. ACM Trans. on Graphics (TOG) 40(4),\\r\\n    1–17 (2021) 1, 6, 12\\r\\n 5. Bhat, K.S., Twigg, C.D., Hodgins, J.K., Khosla, P., Popovic, Z., Seitz, S.M.: Estimating cloth\\r\\n    simulation parameters from video (2003) 4\\r\\n 6. Bogo, F., Romero, J., Loper, M., Black, M.J.: Faust: Dataset and evaluation for 3d mesh\\r\\n    registration. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 3794–3801\\r\\n    (2014) 3\\r\\n 7. Bogo, F., Romero, J., Pons-Moll, G., Black, M.J.: Dynamic faust: Registering human bodies\\r\\n    in motion. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 6233–6242\\r\\n    (2017) 3, 8, 19\\r\\n 8. Chen, X., Zheng, Y., Black, M.J., Hilliges, O., Geiger, A.: Snarf: Differentiable forward\\r\\n    skinning for animating non-rigid neural implicit shapes. In: Proc. of International Conference\\r\\n    on Computer Vision (ICCV) (2021) 1, 3, 7, 10, 11, 13, 19, 20\\r\\n 9. Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling. In: Proc.\\r\\n    of Computer Vision and Pattern Recognition (CVPR). pp. 5939–5948. Computer Vision\\r\\n    Foundation / IEEE (2019) 2\\r\\n10. Deng, B., Lewis, J.P., Jeruzalski, T., Pons-Moll, G., Hinton, G.E., Norouzi, M., Tagliasacchi,\\r\\n    A.: NASA neural articulated shape approximation. In: Proc. of European Conference on\\r\\n    Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12352, pp. 612–628.\\r\\n    Springer (2020) 3\\r\\n11. Gropp, A., Yariv, L., Haim, N., Atzmon, M., Lipman, Y.: Implicit geometric regularization for\\r\\n    learning shapes. In: Proceedings of the 37th International Conference on Machine Learning\\r\\n    (ICML). Proceedings of Machine Learning Research, vol. 119, pp. 3789–3799. PMLR (2020)\\r\\n    7\\r\\n12. Habermann, M., Liu, L., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.: Real-time deep\\r\\n    dynamic characters. ACM Trans. on Graphics (TOG) 40(4), 1–16 (2021) 3, 9, 11\\r\\n13. Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., Seidel, H.: A statistical model of human\\r\\n    pose and body shape. Computer Graphics Forum 28(2), 337–346 (2009) 3\\r\\n14. Holden, D., Duong, B.C., Datta, S., Nowrouzezahrai, D.: Subspace neural physics:\\r\\n    Fast data-driven interactive simulation. In: Proceedings of the 18th annual ACM SIG-\\r\\n    GRAPH/Eurographics Symposium on Computer Animation. pp. 1–12 (2019) 4\\r\\n15. Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3d human dynamics from video. In:\\r\\n    Proc. of Computer Vision and Pattern Recognition (CVPR) (2019) 2\\r\\n16. Kim, M., Pons-Moll, G., Pujades, S., Bang, S., Kim, J., Black, M.J., Lee, S.: Data-driven\\r\\n    physics for human soft tissue animation. ACM Trans. on Graphics (TOG) 36(4), 54:1–54:12\\r\\n    (2017) 3\\r\\n17. Lea, C., Vidal, R., Reiter, A., Hager, G.D.: Temporal convolutional networks: A unified\\r\\n    approach to action segmentation. In: Proc. of European Conference on Computer Vision\\r\\n    (ECCV). pp. 47–54. Springer (2016) 11\\r\\n\\x0c\\n\\n16       Z. Bai et al.\\r\\n\\r\\n18. Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance\\r\\n    generation with aist++. In: Proc. of International Conference on Computer Vision (ICCV). pp.\\r\\n    13401–13412 (2021) 8\\r\\n19. Ling, H.Y., Zinno, F., Cheng, G., Van De Panne, M.: Character controllers using motion vaes.\\r\\n    ACM Trans. on Graphics (TOG) 39(4), 40–1 (2020) 2, 3\\r\\n20. Liu, L., Habermann, M., Rudnev, V., Sarkar, K., Gu, J., Theobalt, C.: Neural actor: Neural\\r\\n    free-view synthesis of human actors with pose control. ACM Trans. on Graphics (TOG) 40(6),\\r\\n    1–16 (2021) 3, 7, 14, 19\\r\\n21. Loper, M., Mahmood, N., Black, M.J.: Mosh: Motion and shape capture from sparse markers.\\r\\n    ACM Trans. on Graphics (TOG) 33(6), 1–13 (2014) 9, 19\\r\\n22. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-\\r\\n    person linear model. ACM Trans. on Graphics (TOG) 34(6), 248:1–248:16 (2015) 2, 3, 5, 6,\\r\\n    14\\r\\n23. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface construction\\r\\n    algorithm. ACM siggraph computer graphics 21(4), 163–169 (1987) 5, 9\\r\\n24. Ma, Q., Saito, S., Yang, J., Tang, S., Black, M.J.: SCALE: Modeling clothed humans with\\r\\n    a surface codec of articulated local elements. In: Proc. of Computer Vision and Pattern\\r\\n    Recognition (CVPR) (Jun 2021) 1, 3\\r\\n25. Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., Black, M.J.: Learning to\\r\\n    dress 3d people in generative clothing. In: Proc. of Computer Vision and Pattern Recognition\\r\\n    (CVPR). pp. 6469–6478 (2020) 19\\r\\n26. Ma, Q., Yang, J., Tang, S., Black, M.J.: The power of points for modeling humans in clothing.\\r\\n    In: Proc. of International Conference on Computer Vision (ICCV) (Oct 2021) 1, 3\\r\\n27. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of\\r\\n    motion capture as surface shapes. In: Proc. of International Conference on Computer Vision\\r\\n    (ICCV). pp. 5442–5451 (2019) 9, 19, 20\\r\\n28. Martinez, J., Black, M.J., Romero, J.: On human motion prediction using recurrent neural\\r\\n    networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 2891–2900\\r\\n    (2017) 2\\r\\n29. Mescheder, L.M., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\\r\\n    Learning 3D reconstruction in function space. In: Proc. of Computer Vision and Pattern\\r\\n    Recognition (CVPR). pp. 4460–4470. Computer Vision Foundation / IEEE (2019) 2\\r\\n30. Mihajlovic, M., Zhang, Y., Black, M.J., Tang, S.: LEAP: Learning articulated occupancy of\\r\\n    people. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun 2021) 3, 7\\r\\n31. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF:\\r\\n    Representing scenes as neural radiance fields for view synthesis. In: Proc. of European\\r\\n    Conference on Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12346,\\r\\n    pp. 405–421. Springer (2020) 3, 14\\r\\n32. Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: Differentiable volumetric rendering:\\r\\n    Learning implicit 3d representations without 3d supervision. In: Proc. of Computer Vision\\r\\n    and Pattern Recognition (CVPR) (2020) 6\\r\\n33. Osman, A.A.A., Bolkart, T., Black, M.J.: STAR: sparse trained articulated human body\\r\\n    regressor. In: Proc. of European Conference on Computer Vision (ECCV). Lecture Notes in\\r\\n    Computer Science, vol. 12351, pp. 598–613. Springer (2020) 3\\r\\n34. Ott, E., Grebogi, C., Yorke, J.A.: Controlling chaos. Physical review letters 64(11), 1196\\r\\n    (1990) 10, 11\\r\\n35. Palafox, P., Božič, A., Thies, J., Nießner, M., Dai, A.: Npms: Neural parametric models for\\r\\n    3d deformable shapes. In: Proc. of International Conference on Computer Vision (ICCV). pp.\\r\\n    12695–12705 (2021) 3\\r\\n\\x0c\\n\\n               AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling               17\\r\\n\\r\\n36. Park, J.J., Florence, P., Straub, J., Newcombe, R.A., Lovegrove, S.: DeepSDF: Learning\\r\\n    continuous signed distance functions for shape representation. In: Proc. of Computer Vision\\r\\n    and Pattern Recognition (CVPR). pp. 165–174. Computer Vision Foundation / IEEE (2019)\\r\\n    2, 12\\r\\n37. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit\\r\\n    neural representations with structured latent codes for novel view synthesis of dynamic\\r\\n    humans. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 9054–9063\\r\\n    (2021) 3, 14\\r\\n38. Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., Battaglia, P.: Learning mesh-based simulation\\r\\n    with graph networks. In: International Conference on Learning Representations (2021) 4, 9\\r\\n39. Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J.: Dyna: A model of dynamic human\\r\\n    shape in motion. ACM Trans. on Graphics (TOG) 34(4), 1–14 (2015) 2, 3, 9, 14\\r\\n40. Prokudin, S., Lassner, C., Romero, J.: Efficient learning on point clouds with basis point sets.\\r\\n    In: Proc. of International Conference on Computer Vision (ICCV). pp. 4332–4341 (2019) 5\\r\\n41. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d\\r\\n    classification and segmentation. In: Proc. of Computer Vision and Pattern Recognition (CVPR).\\r\\n    pp. 652–660 (2017) 7\\r\\n42. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image\\r\\n    segmentation. In: International Conference on Medical image computing and computer-\\r\\n    assisted intervention. pp. 234–241. Springer (2015) 7\\r\\n43. Saito, S., Yang, J., Ma, Q., Black, M.J.: SCANimate: Weakly supervised learning of skinned\\r\\n    clothed avatar networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun\\r\\n    2021) 1, 3, 6, 7, 11\\r\\n44. Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., Battaglia, P.: Learning\\r\\n    to simulate complex physics with graph networks. In: International Conference on Machine\\r\\n    Learning. pp. 8459–8468. PMLR (2020) 4\\r\\n45. Santesteban, I., Garces, E., Otaduy, M.A., Casas, D.: Softsmpl: Data-driven modeling of\\r\\n    nonlinear soft-tissue dynamics for parametric humans. In: Computer Graphics Forum. vol. 39,\\r\\n    pp. 65–75. Wiley Online Library (2020) 2, 3, 13, 14\\r\\n46. Sifakis, E., Barbic, J.: Fem simulation of 3d deformable solids: a practitioner’s guide to theory,\\r\\n    discretization and model reduction. In: Acm siggraph 2012 courses, pp. 1–50 (2012) 4\\r\\n47. Srinivasan, S.G., Wang, Q., Rojas, J., Klár, G., Kavan, L., Sifakis, E.: Learning active\\r\\n    quasistatic physics-based models from data. ACM Trans. on Graphics (TOG) 40(4), 1–14\\r\\n    (2021) 4\\r\\n48. Tiwari, G., Sarafianos, N., Tung, T., Pons-Moll, G.: Neural-gif: Neural generalized implicit\\r\\n    functions for animating people in clothing. In: Proc. of International Conference on Computer\\r\\n    Vision (ICCV). pp. 11708–11718 (2021) 1, 3, 7, 11\\r\\n49. Tsuchida, S., Fukayama, S., Hamasaki, M., Goto, M.: Aist dance video database: Multi-genre,\\r\\n    multi-dancer, and multi-camera database for dance information processing. In: ISMIR. vol. 1,\\r\\n    p. 6 (2019) 8\\r\\n50. Wang, H., O’Brien, J.F., Ramamoorthi, R.: Data-driven elastic models for cloth: modeling\\r\\n    and measurement. ACM Trans. on Graphics (TOG) 30(4), 1–12 (2011) 4\\r\\n51. Wang, S., Mihajlovic, M., Ma, Q., Geiger, A., Tang, S.: Metaavatar: Learning animatable\\r\\n    clothed human models from few depth images. Proc. of Advances in Neural Information\\r\\n    Processing Systems (NeurIPS) 34 (2021) 3\\r\\n52. Xiang, D., Prada, F., Bagautdinov, T., Xu, W., Dong, Y., Wen, H., Hodgins, J., Wu, C.:\\r\\n    Modeling clothing as a separate layer for an animatable human avatar. ACM Trans. on\\r\\n    Graphics (TOG) 40(6), 1–15 (2021) 3, 9, 11\\r\\n53. Xie, Y., Takikawa, T., Saito, S., Litany, O., Yan, S., Khan, N., Tombari, F., Tompkin, J.,\\r\\n    Sitzmann, V., Sridhar, S.: Neural fields in visual computing and beyond. arXiv preprint\\r\\n    arXiv:2111.11426 (2021) 3\\r\\n\\x0c\\n\\n18       Z. Bai et al.\\r\\n\\r\\n54. Xu, H., Bazavan, E.G., Zanfir, A., Freeman, W.T., Sukthankar, R., Sminchisescu, C.: GHUM\\r\\n    & GHUML: generative 3D human shape and articulated pose models. In: Proc. of Computer\\r\\n    Vision and Pattern Recognition (CVPR). pp. 6183–6192. IEEE (2020) 3\\r\\n55. Yang, S., Liang, J., Lin, M.C.: Learning-based cloth material recovery from video. In: Proc.\\r\\n    of International Conference on Computer Vision (ICCV). pp. 4393–4403. IEEE Computer\\r\\n    Society (2017) 4\\r\\n56. Zakharkin, I., Mazur, K., Grigorev, A., Lempitsky, V.: Point-based modeling of human\\r\\n    clothing. In: Proc. of International Conference on Computer Vision (ICCV). pp. 14718–14727\\r\\n    (2021) 3\\r\\n57. Zheng, M., Zhou, Y., Ceylan, D., Barbic, J.: A deep emulator for secondary motion of 3d\\r\\n    characters. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 5932–5940\\r\\n    (2021) 4, 9\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling      19\\r\\n\\r\\nAppendix\\r\\n\\r\\nA    Analysis on Input Pose Accuracy\\r\\nWe investigate how the accuracy of the input SMPL fitting influences the results on\\r\\nsubject 50002 of DFaust [7]. As discussed in Sec. 4.1, the fitted SMPL parameters in\\r\\nDFaust are provided by the AMASS [27] dataset that uses sparse points on the registered\\r\\ndata as approximated motion capture marker locations and computes the parameters\\r\\nusing MoSh [21]. We observe that the provided pose parameters sometimes exhibit small\\r\\nmisalignment with respect to the input scans. While the fitting quality in the AMASS\\r\\ndataset is sufficient for our approach, we also evaluate the performance on more accurate\\r\\npose parameters by using all the vertices on the registered meshes. More specifically, we\\r\\nfirst compute a better template by unposing the registered meshes in the first frame of\\r\\neach sequence using the LBS skinning weights of the SMPL template, and averaging\\r\\nover all the sequences. Using this new template, we optimize pose parameters for each\\r\\nframe with an L2-loss on all the registered vertices. Note that in this experiment, we use\\r\\nthe original template with the refined pose parameters instead of the refined template in\\r\\norder not to unfairly favor our method over SNARF [8].\\r\\n     In Tab. 1, we report the mean absolute error of scan-to-prediction distance (mm) and\\r\\nthe mean squared error of volume change for our method and SNARF. Tab. 1 shows that\\r\\nSNARF has a large error reduction with refined poses, indicating that SNARF is highly\\r\\nsensitive to the accuracy of the SMPL fit. We also observe that after pose refinement,\\r\\nSNARF overfits more to training poses (e.g., interpolation) as SNARF cannot model\\r\\nhistory-dependent dynamic deformations. In contrast, our method is more robust to the\\r\\nfitting errors, and significantly outperforms SNARF in most settings except for 16-30\\r\\nrollouts in the interpolation set. Note that the results with longer rollouts favor “mean”\\r\\npredictions over more dynamic predictions, and do not inform us of the plausibility of\\r\\nthe synthesized dynamics (see the discussion in Sec. 4.2).\\r\\n\\r\\n\\r\\nB    k-NN vs. Closest Surface Projection\\r\\nAs discussed in Sec. 3.3, our SDF decoding approach uses k-nearest neighbors (k-NN)\\r\\nof the SMPL vertices instead of closest surface projection [20]. Fig. H illustrates the\\r\\nlimitation of this alternative approach proposed in Neural Actor [20]. As shown in Fig. H,\\r\\nwe observe that associating a query location with a single closest point on the surface\\r\\nleads to poor generalization to unseen poses around regions with multiple body parts in\\r\\nclose proximity (e.g. around armpits). In contrast, our approach, which associates query\\r\\npoints with multiple k-NN vertices, produces more plausible surface geometry even for\\r\\nunseen poses.\\r\\n\\r\\n\\r\\nC    Limitation: Clothing Deformations\\r\\nWe also apply our method on the CAPE [25] dataset that contains 4D scans of clothed\\r\\nhumans. We select the subject 03375 longlong, which exhibits the most visible\\r\\n\\x0c\\n\\n20       Z. Bai et al.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTable B: Quantitative Evaluation on Input Pose Accuracy on Subject 50002. We show the\\r\\nresults of our approach and SNARF [8] using the poses provided by the AMASS [27] dataset\\r\\nand the ones after refinement using all vertices in the registered meshes. While SNARF is greatly\\r\\ninfluenced by the accuracy of pose parameters, the slight improvement in our method illustrates its\\r\\nrobustness to SMPL fitting errors. In addition, our approach significantly outperforms SNARF\\r\\neven after pose refinement in most settings except for the 16-30 rollouts in the interpolation set.\\r\\n\\r\\n                         (a) Mean Scan-to-Prediction Distance (mm) ↓\\r\\n\\r\\n                                                 Rollout (# of frames)\\r\\n                                       1       2      4         8      16            30\\r\\n                                    Interpolation Set\\r\\n                          SNARF [8] 7.898 7.715 7.588 7.840 7.898                  8.238\\r\\n         AMASS [27]\\r\\n                          Ours       1.731 2.127 2.953 4.325 5.606                 6.455\\r\\n                          SNARF [8]  3.982 4.001 3.964            4.068    4.029   4.158\\r\\n         Refined Poses\\r\\n                          Ours       1.417 1.703 2.259            3.241    4.044   4.601\\r\\n                                    Extrapolation Set\\r\\n                          SNARF [8] 8.083 8.126 8.160             8.246    8.050   8.025\\r\\n         AMASS [27]\\r\\n                          Ours       1.259 1.479 1.984            2.883    4.023   4.867\\r\\n                          SNARF [8]      4.624   4.632    4.672   4.749    4.548   4.447\\r\\n         Refined Poses\\r\\n                          Ours           1.149   1.329    1.745   2.486    3.313   3.855\\r\\n\\r\\n                          (b) Mean Squared Error of Volume Change ↓\\r\\n\\r\\n                                                      Rollout (# of frames)\\r\\n                                           2         4          8         16          30\\r\\n                                         Interpolation Set\\r\\n                         SNARF [8]     0.01623 0.01590 0.01688 0.01703             0.01829\\r\\n       AMASS [27]\\r\\n                         Ours          0.00990 0.01135 0.01417 0.01597             0.01815\\r\\n                         SNARF [8]     0.01401 0.01349       0.01430    0.01426    0.01524\\r\\n       Refined Poses\\r\\n                         Ours          0.00849 0.01002       0.01248    0.01389    0.01558\\r\\n                                        Extrapolation Set\\r\\n                         SNARF [8]     0.01228 0.01244       0.01333    0.01292    0.01264\\r\\n       AMASS [27]\\r\\n                         Ours          0.00602 0.00756       0.00977    0.01082    0.01140\\r\\n                         SNARF [8]     0.01094    0.01092    0.01148    0.01099    0.01080\\r\\n       Refined Poses\\r\\n                         Ours          0.00559    0.00691    0.00871    0.00953    0.01000\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling            21\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    Scan         Projection        Ours                 Scan         Projection       Ours\\r\\n\\r\\nFig. H: k-NN vs. Closest Surface Projection. While the closest surface projection suffers from\\r\\nartifacts around armpits, our SDF decoding based on k-NN produces more plausible surface\\r\\ngeometry for unseen poses.\\r\\n\\r\\n\\r\\n\\r\\ndynamic deformations for clothing. We exclude 6 sequences (athletics, frisbee,\\r\\nvolleyball, box trial1, swim trial1, twist tilt trial1) from train-\\r\\ning, and use them for testing. We employ as input the template and SMPL poses provided\\r\\nby the CAPE dataset for training our model. Note that we approximate raw scans by\\r\\nsampling point clouds with surface normals computed on the registered meshes as the\\r\\nCAPE dataset only provides registered meshes for 03375 longlong.\\r\\n    Please refer to the supplementary video for qualitative results. While our approach\\r\\nproduces plausible short-term clothing deformations, it remains challenging to model\\r\\ndynamically deforming clothing with longer rollouts. Compared to soft-tissue deforma-\\r\\ntions, dynamics on clothed humans involve high-frequency deformations and topology\\r\\nchange, making the learning of clothing dynamics more difficult. We leave this for future\\r\\nwork.\\r\\n\\x0c'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with one pdf first\n",
    "\n",
    "mypdftext=array_pdf_text[0]\n",
    "mypdftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a99fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 1. Alldieck, T., Xu, H., Sminchisescu, C.: imghum: Implicit generative models of 3d human\r\n",
      "    shape and articulated pose. In: Proc. of International Conference on Computer Vision (ICCV).\r\n",
      "    pp. 5461–5470 (2021) 3\r\n",
      " 2. Allen, B., Curless, B., Popović, Z., Hertzmann, A.: Learning a correlated model of identity\r\n",
      "    and pose-dependent body shape variation for real-time synthesis. In: Proceedings of the 2006\r\n",
      "    ACM SIGGRAPH/Eurographics symposium on Computer animation. pp. 147–156 (2006) 3\r\n",
      " 3. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: SCAPE: shape\r\n",
      "    completion and animation of people. ACM Trans. on Graphics (TOG) 24(3), 408–416 (2005)\r\n",
      "    3\r\n",
      " 4. Bagautdinov, T., Wu, C., Simon, T., Prada, F., Shiratori, T., Wei, S.E., Xu, W., Sheikh, Y.,\r\n",
      "    Saragih, J.: Driving-signal aware full-body avatars. ACM Trans. on Graphics (TOG) 40(4),\r\n",
      "    1–17 (2021) 1, 6, 12\r\n",
      " 5. Bhat, K.S., Twigg, C.D., Hodgins, J.K., Khosla, P., Popovic, Z., Seitz, S.M.: Estimating cloth\r\n",
      "    simulation parameters from video (2003) 4\r\n",
      " 6. Bogo, F., Romero, J., Loper, M., Black, M.J.: Faust: Dataset and evaluation for 3d mesh\r\n",
      "    registration. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 3794–3801\r\n",
      "    (2014) 3\r\n",
      " 7. Bogo, F., Romero, J., Pons-Moll, G., Black, M.J.: Dynamic faust: Registering human bodies\r\n",
      "    in motion. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 6233–6242\r\n",
      "    (2017) 3, 8, 19\r\n",
      " 8. Chen, X., Zheng, Y., Black, M.J., Hilliges, O., Geiger, A.: Snarf: Differentiable forward\r\n",
      "    skinning for animating non-rigid neural implicit shapes. In: Proc. of International Conference\r\n",
      "    on Computer Vision (ICCV) (2021) 1, 3, 7, 10, 11, 13, 19, 20\r\n",
      " 9. Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling. In: Proc.\r\n",
      "    of Computer Vision and Pattern Recognition (CVPR). pp. 5939–5948. Computer Vision\r\n",
      "    Foundation / IEEE (2019) 2\r\n",
      "10. Deng, B., Lewis, J.P., Jeruzalski, T., Pons-Moll, G., Hinton, G.E., Norouzi, M., Tagliasacchi,\r\n",
      "    A.: NASA neural articulated shape approximation. In: Proc. of European Conference on\r\n",
      "    Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12352, pp. 612–628.\r\n",
      "    Springer (2020) 3\r\n",
      "11. Gropp, A., Yariv, L., Haim, N., Atzmon, M., Lipman, Y.: Implicit geometric regularization for\r\n",
      "    learning shapes. In: Proceedings of the 37th International Conference on Machine Learning\r\n",
      "    (ICML). Proceedings of Machine Learning Research, vol. 119, pp. 3789–3799. PMLR (2020)\r\n",
      "    7\r\n",
      "12. Habermann, M., Liu, L., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.: Real-time deep\r\n",
      "    dynamic characters. ACM Trans. on Graphics (TOG) 40(4), 1–16 (2021) 3, 9, 11\r\n",
      "13. Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., Seidel, H.: A statistical model of human\r\n",
      "    pose and body shape. Computer Graphics Forum 28(2), 337–346 (2009) 3\r\n",
      "14. Holden, D., Duong, B.C., Datta, S., Nowrouzezahrai, D.: Subspace neural physics:\r\n",
      "    Fast data-driven interactive simulation. In: Proceedings of the 18th annual ACM SIG-\r\n",
      "    GRAPH/Eurographics Symposium on Computer Animation. pp. 1–12 (2019) 4\r\n",
      "15. Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3d human dynamics from video. In:\r\n",
      "    Proc. of Computer Vision and Pattern Recognition (CVPR) (2019) 2\r\n",
      "16. Kim, M., Pons-Moll, G., Pujades, S., Bang, S., Kim, J., Black, M.J., Lee, S.: Data-driven\r\n",
      "    physics for human soft tissue animation. ACM Trans. on Graphics (TOG) 36(4), 54:1–54:12\r\n",
      "    (2017) 3\r\n",
      "17. Lea, C., Vidal, R., Reiter, A., Hager, G.D.: Temporal convolutional networks: A unified\r\n",
      "    approach to action segmentation. In: Proc. of European Conference on Computer Vision\r\n",
      "    (ECCV). pp. 47–54. Springer (2016) 11\r\n",
      "\f",
      "\n",
      "\n",
      "16       Z. Bai et al.\r\n",
      "\r\n",
      "18. Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance\r\n",
      "    generation with aist++. In: Proc. of International Conference on Computer Vision (ICCV). pp.\r\n",
      "    13401–13412 (2021) 8\r\n",
      "19. Ling, H.Y., Zinno, F., Cheng, G., Van De Panne, M.: Character controllers using motion vaes.\r\n",
      "    ACM Trans. on Graphics (TOG) 39(4), 40–1 (2020) 2, 3\r\n",
      "20. Liu, L., Habermann, M., Rudnev, V., Sarkar, K., Gu, J., Theobalt, C.: Neural actor: Neural\r\n",
      "    free-view synthesis of human actors with pose control. ACM Trans. on Graphics (TOG) 40(6),\r\n",
      "    1–16 (2021) 3, 7, 14, 19\r\n",
      "21. Loper, M., Mahmood, N., Black, M.J.: Mosh: Motion and shape capture from sparse markers.\r\n",
      "    ACM Trans. on Graphics (TOG) 33(6), 1–13 (2014) 9, 19\r\n",
      "22. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-\r\n",
      "    person linear model. ACM Trans. on Graphics (TOG) 34(6), 248:1–248:16 (2015) 2, 3, 5, 6,\r\n",
      "    14\r\n",
      "23. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface construction\r\n",
      "    algorithm. ACM siggraph computer graphics 21(4), 163–169 (1987) 5, 9\r\n",
      "24. Ma, Q., Saito, S., Yang, J., Tang, S., Black, M.J.: SCALE: Modeling clothed humans with\r\n",
      "    a surface codec of articulated local elements. In: Proc. of Computer Vision and Pattern\r\n",
      "    Recognition (CVPR) (Jun 2021) 1, 3\r\n",
      "25. Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., Black, M.J.: Learning to\r\n",
      "    dress 3d people in generative clothing. In: Proc. of Computer Vision and Pattern Recognition\r\n",
      "    (CVPR). pp. 6469–6478 (2020) 19\r\n",
      "26. Ma, Q., Yang, J., Tang, S., Black, M.J.: The power of points for modeling humans in clothing.\r\n",
      "    In: Proc. of International Conference on Computer Vision (ICCV) (Oct 2021) 1, 3\r\n",
      "27. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of\r\n",
      "    motion capture as surface shapes. In: Proc. of International Conference on Computer Vision\r\n",
      "    (ICCV). pp. 5442–5451 (2019) 9, 19, 20\r\n",
      "28. Martinez, J., Black, M.J., Romero, J.: On human motion prediction using recurrent neural\r\n",
      "    networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 2891–2900\r\n",
      "    (2017) 2\r\n",
      "29. Mescheder, L.M., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\r\n",
      "    Learning 3D reconstruction in function space. In: Proc. of Computer Vision and Pattern\r\n",
      "    Recognition (CVPR). pp. 4460–4470. Computer Vision Foundation / IEEE (2019) 2\r\n",
      "30. Mihajlovic, M., Zhang, Y., Black, M.J., Tang, S.: LEAP: Learning articulated occupancy of\r\n",
      "    people. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun 2021) 3, 7\r\n",
      "31. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF:\r\n",
      "    Representing scenes as neural radiance fields for view synthesis. In: Proc. of European\r\n",
      "    Conference on Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12346,\r\n",
      "    pp. 405–421. Springer (2020) 3, 14\r\n",
      "32. Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: Differentiable volumetric rendering:\r\n",
      "    Learning implicit 3d representations without 3d supervision. In: Proc. of Computer Vision\r\n",
      "    and Pattern Recognition (CVPR) (2020) 6\r\n",
      "33. Osman, A.A.A., Bolkart, T., Black, M.J.: STAR: sparse trained articulated human body\r\n",
      "    regressor. In: Proc. of European Conference on Computer Vision (ECCV). Lecture Notes in\r\n",
      "    Computer Science, vol. 12351, pp. 598–613. Springer (2020) 3\r\n",
      "34. Ott, E., Grebogi, C., Yorke, J.A.: Controlling chaos. Physical review letters 64(11), 1196\r\n",
      "    (1990) 10, 11\r\n",
      "35. Palafox, P., Božič, A., Thies, J., Nießner, M., Dai, A.: Npms: Neural parametric models for\r\n",
      "    3d deformable shapes. In: Proc. of International Conference on Computer Vision (ICCV). pp.\r\n",
      "    12695–12705 (2021) 3\r\n",
      "\f",
      "\n",
      "\n",
      "               AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling               17\r\n",
      "\r\n",
      "36. Park, J.J., Florence, P., Straub, J., Newcombe, R.A., Lovegrove, S.: DeepSDF: Learning\r\n",
      "    continuous signed distance functions for shape representation. In: Proc. of Computer Vision\r\n",
      "    and Pattern Recognition (CVPR). pp. 165–174. Computer Vision Foundation / IEEE (2019)\r\n",
      "    2, 12\r\n",
      "37. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit\r\n",
      "    neural representations with structured latent codes for novel view synthesis of dynamic\r\n",
      "    humans. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 9054–9063\r\n",
      "    (2021) 3, 14\r\n",
      "38. Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., Battaglia, P.: Learning mesh-based simulation\r\n",
      "    with graph networks. In: International Conference on Learning Representations (2021) 4, 9\r\n",
      "39. Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J.: Dyna: A model of dynamic human\r\n",
      "    shape in motion. ACM Trans. on Graphics (TOG) 34(4), 1–14 (2015) 2, 3, 9, 14\r\n",
      "40. Prokudin, S., Lassner, C., Romero, J.: Efficient learning on point clouds with basis point sets.\r\n",
      "    In: Proc. of International Conference on Computer Vision (ICCV). pp. 4332–4341 (2019) 5\r\n",
      "41. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d\r\n",
      "    classification and segmentation. In: Proc. of Computer Vision and Pattern Recognition (CVPR).\r\n",
      "    pp. 652–660 (2017) 7\r\n",
      "42. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image\r\n",
      "    segmentation. In: International Conference on Medical image computing and computer-\r\n",
      "    assisted intervention. pp. 234–241. Springer (2015) 7\r\n",
      "43. Saito, S., Yang, J., Ma, Q., Black, M.J.: SCANimate: Weakly supervised learning of skinned\r\n",
      "    clothed avatar networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun\r\n",
      "    2021) 1, 3, 6, 7, 11\r\n",
      "44. Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., Battaglia, P.: Learning\r\n",
      "    to simulate complex physics with graph networks. In: International Conference on Machine\r\n",
      "    Learning. pp. 8459–8468. PMLR (2020) 4\r\n",
      "45. Santesteban, I., Garces, E., Otaduy, M.A., Casas, D.: Softsmpl: Data-driven modeling of\r\n",
      "    nonlinear soft-tissue dynamics for parametric humans. In: Computer Graphics Forum. vol. 39,\r\n",
      "    pp. 65–75. Wiley Online Library (2020) 2, 3, 13, 14\r\n",
      "46. Sifakis, E., Barbic, J.: Fem simulation of 3d deformable solids: a practitioner’s guide to theory,\r\n",
      "    discretization and model reduction. In: Acm siggraph 2012 courses, pp. 1–50 (2012) 4\r\n",
      "47. Srinivasan, S.G., Wang, Q., Rojas, J., Klár, G., Kavan, L., Sifakis, E.: Learning active\r\n",
      "    quasistatic physics-based models from data. ACM Trans. on Graphics (TOG) 40(4), 1–14\r\n",
      "    (2021) 4\r\n",
      "48. Tiwari, G., Sarafianos, N., Tung, T., Pons-Moll, G.: Neural-gif: Neural generalized implicit\r\n",
      "    functions for animating people in clothing. In: Proc. of International Conference on Computer\r\n",
      "    Vision (ICCV). pp. 11708–11718 (2021) 1, 3, 7, 11\r\n",
      "49. Tsuchida, S., Fukayama, S., Hamasaki, M., Goto, M.: Aist dance video database: Multi-genre,\r\n",
      "    multi-dancer, and multi-camera database for dance information processing. In: ISMIR. vol. 1,\r\n",
      "    p. 6 (2019) 8\r\n",
      "50. Wang, H., O’Brien, J.F., Ramamoorthi, R.: Data-driven elastic models for cloth: modeling\r\n",
      "    and measurement. ACM Trans. on Graphics (TOG) 30(4), 1–12 (2011) 4\r\n",
      "51. Wang, S., Mihajlovic, M., Ma, Q., Geiger, A., Tang, S.: Metaavatar: Learning animatable\r\n",
      "    clothed human models from few depth images. Proc. of Advances in Neural Information\r\n",
      "    Processing Systems (NeurIPS) 34 (2021) 3\r\n",
      "52. Xiang, D., Prada, F., Bagautdinov, T., Xu, W., Dong, Y., Wen, H., Hodgins, J., Wu, C.:\r\n",
      "    Modeling clothing as a separate layer for an animatable human avatar. ACM Trans. on\r\n",
      "    Graphics (TOG) 40(6), 1–15 (2021) 3, 9, 11\r\n",
      "53. Xie, Y., Takikawa, T., Saito, S., Litany, O., Yan, S., Khan, N., Tombari, F., Tompkin, J.,\r\n",
      "    Sitzmann, V., Sridhar, S.: Neural fields in visual computing and beyond. arXiv preprint\r\n",
      "    arXiv:2111.11426 (2021) 3\r\n",
      "\f",
      "\n",
      "\n",
      "18       Z. Bai et al.\r\n",
      "\r\n",
      "54. Xu, H., Bazavan, E.G., Zanfir, A., Freeman, W.T., Sukthankar, R., Sminchisescu, C.: GHUM\r\n",
      "    & GHUML: generative 3D human shape and articulated pose models. In: Proc. of Computer\r\n",
      "    Vision and Pattern Recognition (CVPR). pp. 6183–6192. IEEE (2020) 3\r\n",
      "55. Yang, S., Liang, J., Lin, M.C.: Learning-based cloth material recovery from video. In: Proc.\r\n",
      "    of International Conference on Computer Vision (ICCV). pp. 4393–4403. IEEE Computer\r\n",
      "    Society (2017) 4\r\n",
      "56. Zakharkin, I., Mazur, K., Grigorev, A., Lempitsky, V.: Point-based modeling of human\r\n",
      "    clothing. In: Proc. of International Conference on Computer Vision (ICCV). pp. 14718–14727\r\n",
      "    (2021) 3\r\n",
      "57. Zheng, M., Zhou, Y., Ceylan, D., Barbic, J.: A deep emulator for secondary motion of 3d\r\n",
      "    characters. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 5932–5940\r\n",
      "    (2021) 4, 9\r\n",
      "\f",
      "\n",
      "\n",
      "             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling      19\r\n",
      "\r\n",
      "Appendix\r\n",
      "\r\n",
      "A    Analysis on Input Pose Accuracy\r\n",
      "We investigate how the accuracy of the input SMPL fitting influences the results on\r\n",
      "subject 50002 of DFaust [7]. As discussed in Sec. 4.1, the fitted SMPL parameters in\r\n",
      "DFaust are provided by the AMASS [27] dataset that uses sparse points on the registered\r\n",
      "data as approximated motion capture marker locations and computes the parameters\r\n",
      "using MoSh [21]. We observe that the provided pose parameters sometimes exhibit small\r\n",
      "misalignment with respect to the input scans. While the fitting quality in the AMASS\r\n",
      "dataset is sufficient for our approach, we also evaluate the performance on more accurate\r\n",
      "pose parameters by using all the vertices on the registered meshes. More specifically, we\r\n",
      "first compute a better template by unposing the registered meshes in the first frame of\r\n",
      "each sequence using the LBS skinning weights of the SMPL template, and averaging\r\n",
      "over all the sequences. Using this new template, we optimize pose parameters for each\r\n",
      "frame with an L2-loss on all the registered vertices. Note that in this experiment, we use\r\n",
      "the original template with the refined pose parameters instead of the refined template in\r\n",
      "order not to unfairly favor our method over SNARF [8].\r\n",
      "     In Tab. 1, we report the mean absolute error of scan-to-prediction distance (mm) and\r\n",
      "the mean squared error of volume change for our method and SNARF. Tab. 1 shows that\r\n",
      "SNARF has a large error reduction with refined poses, indicating that SNARF is highly\r\n",
      "sensitive to the accuracy of the SMPL fit. We also observe that after pose refinement,\r\n",
      "SNARF overfits more to training poses (e.g., interpolation) as SNARF cannot model\r\n",
      "history-dependent dynamic deformations. In contrast, our method is more robust to the\r\n",
      "fitting errors, and significantly outperforms SNARF in most settings except for 16-30\r\n",
      "rollouts in the interpolation set. Note that the results with longer rollouts favor “mean”\r\n",
      "predictions over more dynamic predictions, and do not inform us of the plausibility of\r\n",
      "the synthesized dynamics (see the discussion in Sec. 4.2).\r\n",
      "\r\n",
      "\r\n",
      "B    k-NN vs. Closest Surface Projection\r\n",
      "As discussed in Sec. 3.3, our SDF decoding approach uses k-nearest neighbors (k-NN)\r\n",
      "of the SMPL vertices instead of closest surface projection [20]. Fig. H illustrates the\r\n",
      "limitation of this alternative approach proposed in Neural Actor [20]. As shown in Fig. H,\r\n",
      "we observe that associating a query location with a single closest point on the surface\r\n",
      "leads to poor generalization to unseen poses around regions with multiple body parts in\r\n",
      "close proximity (e.g. around armpits). In contrast, our approach, which associates query\r\n",
      "points with multiple k-NN vertices, produces more plausible surface geometry even for\r\n",
      "unseen poses.\r\n",
      "\r\n",
      "\r\n",
      "C    Limitation: Clothing Deformations\r\n",
      "We also apply our method on the CAPE [25] dataset that contains 4D scans of clothed\r\n",
      "humans. We select the subject 03375 longlong, which exhibits the most visible\r\n",
      "\f",
      "\n",
      "\n",
      "20       Z. Bai et al.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Table B: Quantitative Evaluation on Input Pose Accuracy on Subject 50002. We show the\r\n",
      "results of our approach and SNARF [8] using the poses provided by the AMASS [27] dataset\r\n",
      "and the ones after refinement using all vertices in the registered meshes. While SNARF is greatly\r\n",
      "influenced by the accuracy of pose parameters, the slight improvement in our method illustrates its\r\n",
      "robustness to SMPL fitting errors. In addition, our approach significantly outperforms SNARF\r\n",
      "even after pose refinement in most settings except for the 16-30 rollouts in the interpolation set.\r\n",
      "\r\n",
      "                         (a) Mean Scan-to-Prediction Distance (mm) ↓\r\n",
      "\r\n",
      "                                                 Rollout (# of frames)\r\n",
      "                                       1       2      4         8      16            30\r\n",
      "                                    Interpolation Set\r\n",
      "                          SNARF [8] 7.898 7.715 7.588 7.840 7.898                  8.238\r\n",
      "         AMASS [27]\r\n",
      "                          Ours       1.731 2.127 2.953 4.325 5.606                 6.455\r\n",
      "                          SNARF [8]  3.982 4.001 3.964            4.068    4.029   4.158\r\n",
      "         Refined Poses\r\n",
      "                          Ours       1.417 1.703 2.259            3.241    4.044   4.601\r\n",
      "                                    Extrapolation Set\r\n",
      "                          SNARF [8] 8.083 8.126 8.160             8.246    8.050   8.025\r\n",
      "         AMASS [27]\r\n",
      "                          Ours       1.259 1.479 1.984            2.883    4.023   4.867\r\n",
      "                          SNARF [8]      4.624   4.632    4.672   4.749    4.548   4.447\r\n",
      "         Refined Poses\r\n",
      "                          Ours           1.149   1.329    1.745   2.486    3.313   3.855\r\n",
      "\r\n",
      "                          (b) Mean Squared Error of Volume Change ↓\r\n",
      "\r\n",
      "                                                      Rollout (# of frames)\r\n",
      "                                           2         4          8         16          30\r\n",
      "                                         Interpolation Set\r\n",
      "                         SNARF [8]     0.01623 0.01590 0.01688 0.01703             0.01829\r\n",
      "       AMASS [27]\r\n",
      "                         Ours          0.00990 0.01135 0.01417 0.01597             0.01815\r\n",
      "                         SNARF [8]     0.01401 0.01349       0.01430    0.01426    0.01524\r\n",
      "       Refined Poses\r\n",
      "                         Ours          0.00849 0.01002       0.01248    0.01389    0.01558\r\n",
      "                                        Extrapolation Set\r\n",
      "                         SNARF [8]     0.01228 0.01244       0.01333    0.01292    0.01264\r\n",
      "       AMASS [27]\r\n",
      "                         Ours          0.00602 0.00756       0.00977    0.01082    0.01140\r\n",
      "                         SNARF [8]     0.01094    0.01092    0.01148    0.01099    0.01080\r\n",
      "       Refined Poses\r\n",
      "                         Ours          0.00559    0.00691    0.00871    0.00953    0.01000\r\n",
      "\f",
      "\n",
      "\n",
      "             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling            21\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "    Scan         Projection        Ours                 Scan         Projection       Ours\r\n",
      "\r\n",
      "Fig. H: k-NN vs. Closest Surface Projection. While the closest surface projection suffers from\r\n",
      "artifacts around armpits, our SDF decoding based on k-NN produces more plausible surface\r\n",
      "geometry for unseen poses.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "dynamic deformations for clothing. We exclude 6 sequences (athletics, frisbee,\r\n",
      "volleyball, box trial1, swim trial1, twist tilt trial1) from train-\r\n",
      "ing, and use them for testing. We employ as input the template and SMPL poses provided\r\n",
      "by the CAPE dataset for training our model. Note that we approximate raw scans by\r\n",
      "sampling point clouds with surface normals computed on the registered meshes as the\r\n",
      "CAPE dataset only provides registered meshes for 03375 longlong.\r\n",
      "    Please refer to the supplementary video for qualitative results. While our approach\r\n",
      "produces plausible short-term clothing deformations, it remains challenging to model\r\n",
      "dynamically deforming clothing with longer rollouts. Compared to soft-tissue deforma-\r\n",
      "tions, dynamics on clothed humans involve high-frequency deformations and topology\r\n",
      "change, making the learning of clothing dynamics more difficult. We leave this for future\r\n",
      "work.\r\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#First filter to have only the References from a pdf\n",
    "\n",
    "def after_references(mypdftext): \n",
    "    keyword1 = 'References'\n",
    "    keyword2 = 'REFERENCES'\n",
    "    keyword3 = 'R EFERENCES'\n",
    "    keyword4 = 'Reference'\n",
    "    keyword5='[1]' \n",
    "\n",
    "    if keyword1 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword1)\n",
    "    elif keyword2 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword2)\n",
    "    elif keyword3 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword3)\n",
    "    elif keyword4 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword4)\n",
    "    elif keyword5 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword5)\n",
    "    else:\n",
    "        after_keyword = mypdftext[:10000]\n",
    "    return after_keyword\n",
    "\n",
    "#All references in a variable\n",
    "\n",
    "references=after_references(mypdftext)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439d489",
   "metadata": {},
   "source": [
    "## Preprocess to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e325730d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First cleaning\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns): \n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s) \n",
    "        return s\n",
    "\n",
    "replacer=RegexpReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e46e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess a text before using IA\n",
    "\n",
    "def preprocess_text(test):\n",
    "\n",
    "    #Removing Numbers\n",
    "    test=re.sub(r'\\d+','',test)\n",
    "\n",
    "    #Removing Letter alone\n",
    "    test = re.sub(r'\\b\\w\\b', ' ', test)\n",
    "\n",
    "    #Removing white spaces\n",
    "    test=test.strip()\n",
    "    \n",
    "    #Replacer replace\n",
    "    text_replaced = replacer.replace(test)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text_replaced)\n",
    "\n",
    "    #Tokenize words\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = tokenizer.tokenize(sentences[i])\n",
    "\n",
    "    #Remove stop words\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stops=set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [word for word in sentences[i] if word not in stops]\n",
    "\n",
    "    #Lemmatize\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer_output=WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            sentences[i][j] = lemmatizer_output.lemmatize(sentences[i][j])\n",
    "\n",
    "\n",
    "    #Join the words back into a sentence.\n",
    "    a=[' '.join(s) for s in sentences]\n",
    "    b=', '.join(a)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c077ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Alldieck Xu Sminchisescu, imghum Implicit generative model human shape articulated pose, In Proc, International Conference Computer Vision ICCV, pp, , Allen Curless Popovic Hertzmann, Learning correlated model identity pose dependent body shape variation real time synthesis, In Proceedings ACM SIGGRAPH Eurographics symposium Computer animation, pp, , Anguelov Srinivasan Koller Thrun Rodgers Davis, SCAPE shape completion animation people, ACM Trans, Graphics TOG, Bagautdinov Wu Simon Prada Shiratori Wei, Xu Sheikh Saragih, Driving signal aware full body avatar, ACM Trans, Graphics TOG, Bhat, Twigg, Hodgins, Khosla Popovic Seitz Estimating cloth simulation parameter video, Bogo Romero Loper Black Faust Dataset evaluation mesh registration, In Proc, Computer Vision Pattern Recognition CVPR, pp, , Bogo Romero Pons Moll Black Dynamic faust Registering human body motion, In Proc, Computer Vision Pattern Recognition CVPR, pp, , Chen Zheng Black, Hilliges Geiger, Snarf Differentiable forward skinning animating non rigid neural implicit shape, In Proc, International Conference Computer Vision ICCV, Chen Zhang, Learning implicit field generative shape modeling, In Proc, Computer Vision Pattern Recognition CVPR, pp, , Computer Vision Foundation IEEE, Deng Lewis, Jeruzalski Pons Moll Hinton, Norouzi Tagliasacchi, NASA neural articulated shape approximation, In Proc, European Conference Computer Vision ECCV, Lecture Notes Computer Science vol, pp, , Springer, Gropp Yariv Haim Atzmon Lipman, Implicit geometric regularization learning shape, In Proceedings th International Conference Machine Learning ICML, Proceedings Machine Learning Research vol, pp, , PMLR, Habermann Liu Xu Zollhoefer Pons Moll Theobalt, Real time deep dynamic character, ACM Trans, Graphics TOG, Hasler Stoll Sunkel Rosenhahn Seidel, statistical model human pose body shape, Computer Graphics Forum, Holden Duong, Datta Nowrouzezahrai, Subspace neural physic Fast data driven interactive simulation, In Proceedings th annual ACM SIG GRAPH Eurographics Symposium Computer Animation, pp, , Kanazawa Zhang, Felsen Malik, Learning human dynamic video, In Proc, Computer Vision Pattern Recognition CVPR, Kim Pons Moll Pujades Bang Kim Black, Lee, Data driven physic human soft tissue animation, ACM Trans, Graphics TOG, Lea Vidal Reiter Hager Temporal convolutional network unified approach action segmentation, In Proc, European Conference Computer Vision ECCV, pp, , Springer, Bai et al Li Yang Ross, Kanazawa, Ai choreographer Music conditioned dance generation aist, In Proc, International Conference Computer Vision ICCV, pp, , Ling, Zinno Cheng Van De Panne, Character controller using motion vaes, ACM Trans, Graphics TOG, Liu Habermann Rudnev Sarkar Gu Theobalt, Neural actor Neural free view synthesis human actor pose control, ACM Trans, Graphics TOG, Loper Mahmood Black Mosh Motion shape capture sparse marker, ACM Trans, Graphics TOG, Loper Mahmood Romero Pons Moll Black SMPL skinned multi person linear model, ACM Trans, Graphics TOG, Lorensen, Cline Marching cube high resolution surface construction algorithm, ACM siggraph computer graphic, Ma Saito Yang Tang Black SCALE Modeling clothed human surface codec articulated local element, In Proc, Computer Vision Pattern Recognition CVPR Jun, Ma Yang Ranjan Pujades Pons Moll Tang Black Learning dress people generative clothing, In Proc, Computer Vision Pattern Recognition CVPR, pp, , Ma Yang Tang Black The power point modeling human clothing, In Proc, International Conference Computer Vision ICCV Oct, Mahmood Ghorbani Troje, Pons Moll Black Amass Archive motion capture surface shape, In Proc, International Conference Computer Vision ICCV, pp, , Martinez Black, Romero, On human motion prediction using recurrent neural network, In Proc, Computer Vision Pattern Recognition CVPR, pp, , Mescheder, Oechsle Niemeyer Nowozin Geiger, Occupancy network Learning reconstruction function space, In Proc, Computer Vision Pattern Recognition CVPR, pp, , Computer Vision Foundation IEEE, Mihajlovic Zhang Black, Tang, LEAP Learning articulated occupancy people, In Proc, Computer Vision Pattern Recognition CVPR Jun, Mildenhall Srinivasan, Tancik Barron, Ramamoorthi Ng, NeRF Representing scene neural radiance field view synthesis, In Proc, European Conference Computer Vision ECCV, Lecture Notes Computer Science vol, pp, , Springer, Niemeyer Mescheder Oechsle Geiger, Differentiable volumetric rendering Learning implicit representation without supervision, In Proc, Computer Vision Pattern Recognition CVPR, Osman, Bolkart Black STAR sparse trained articulated human body regressor, In Proc, European Conference Computer Vision ECCV, Lecture Notes Computer Science vol, pp, , Springer, Ott Grebogi Yorke Controlling chaos, Physical review letter, Palafox Boz ic Thies Nießner Dai, Npms Neural parametric model deformable shape, In Proc, International Conference Computer Vision ICCV, pp, AutoAvatar Autoregressive Neural Fields Dynamic Avatar Modeling, Park, Florence Straub Newcombe, Lovegrove, DeepSDF Learning continuous signed distance function shape representation, In Proc, Computer Vision Pattern Recognition CVPR, pp, , Computer Vision Foundation IEEE, Peng Zhang Xu Wang Shuai Bao Zhou, Neural body Implicit neural representation structured latent code novel view synthesis dynamic human, In Proc, Computer Vision Pattern Recognition CVPR, pp, , Pfaff Fortunato Sanchez Gonzalez Battaglia, Learning mesh based simulation graph network, In International Conference Learning Representations, Pons Moll Romero Mahmood Black Dyna model dynamic human shape motion, ACM Trans, Graphics TOG, Prokudin Lassner Romero, Efficient learning point cloud basis point set, In Proc, International Conference Computer Vision ICCV, pp, , Qi, Su Mo Guibas Pointnet Deep learning point set classification segmentation, In Proc, Computer Vision Pattern Recognition CVPR, pp, , Ronneberger Fischer Brox, net Convolutional network biomedical image segmentation, In International Conference Medical image computing computer assisted intervention, pp, , Springer, Saito Yang Ma Black SCANimate Weakly supervised learning skinned clothed avatar network, In Proc, Computer Vision Pattern Recognition CVPR Jun, Sanchez Gonzalez Godwin Pfaff Ying Leskovec Battaglia, Learning simulate complex physic graph network, In International Conference Machine Learning, pp, , PMLR, Santesteban Garces Otaduy, Casas, Softsmpl Data driven modeling nonlinear soft tissue dynamic parametric human, In Computer Graphics Forum, vol, pp, , Wiley Online Library, Sifakis Barbic, Fem simulation deformable solid practitioner guide theory discretization model reduction, In Acm siggraph course pp, , Srinivasan, Wang Rojas Kla Kavan Sifakis, Learning active quasistatic physic based model data, ACM Trans, Graphics TOG, Tiwari Sarafianos Tung Pons Moll, Neural gif Neural generalized implicit function animating people clothing, In Proc, International Conference Computer Vision ICCV, pp, , Tsuchida Fukayama Hamasaki Goto, Aist dance video database Multi genre multi dancer multi camera database dance information processing, In ISMIR, vol, , , Wang Brien, Ramamoorthi, Data driven elastic model cloth modeling measurement, ACM Trans, Graphics TOG, Wang Mihajlovic Ma Geiger Tang, Metaavatar Learning animatable clothed human model depth image, Proc, Advances Neural Information Processing Systems NeurIPS, Xiang Prada Bagautdinov Xu Dong Wen Hodgins Wu, Modeling clothing separate layer animatable human avatar, ACM Trans, Graphics TOG, Xie Takikawa Saito Litany Yan Khan Tombari Tompkin Sitzmann Sridhar, Neural field visual computing beyond, arXiv preprint arXiv, , Bai et al Xu Bazavan, Zanfir Freeman, Sukthankar Sminchisescu, GHUM GHUML generative human shape articulated pose model, In Proc, Computer Vision Pattern Recognition CVPR, pp, , IEEE, Yang Liang Lin Learning based cloth material recovery video, In Proc, International Conference Computer Vision ICCV, pp, , IEEE Computer Society, Zakharkin Mazur Grigorev Lempitsky, Point based modeling human clothing, In Proc, International Conference Computer Vision ICCV, pp, , Zheng Zhou Ceylan Barbic, deep emulator secondary motion character, In Proc, Computer Vision Pattern Recognition CVPR, pp, AutoAvatar Autoregressive Neural Fields Dynamic Avatar Modeling Appendix Analysis Input Pose Accuracy We investigate accuracy input SMPL fitting influence result subject DFaust, As discussed Sec, fitted SMPL parameter DFaust provided AMASS dataset us sparse point registered data approximated motion capture marker location computes parameter using MoSh, We observe provided pose parameter sometimes exhibit small misalignment respect input scan, While fitting quality AMASS dataset sufficient approach also evaluate performance accurate pose parameter using vertex registered mesh, More specifically first compute better template unposing registered mesh first frame sequence using LBS skinning weight SMPL template averaging sequence, Using new template optimize pose parameter frame loss registered vertex, Note experiment use original template refined pose parameter instead refined template order unfairly favor method SNARF, In Tab, report mean absolute error scan prediction distance mm mean squared error volume change method SNARF, Tab, show SNARF large error reduction refined pose indicating SNARF highly sensitive accuracy SMPL fit, We also observe pose refinement SNARF overfits training pose, interpolation SNARF cannot model history dependent dynamic deformation, In contrast method robust fitting error significantly outperforms SNARF setting except rollouts interpolation set, Note result longer rollouts favor mean prediction dynamic prediction inform u plausibility synthesized dynamic see discussion Sec, , NN v, Closest Surface Projection As discussed Sec, SDF decoding approach us nearest neighbor NN SMPL vertex instead closest surface projection, Fig, illustrates limitation alternative approach proposed Neural Actor, As shown Fig, observe associating query location single closest point surface lead poor generalization unseen pose around region multiple body part close proximity around armpit, In contrast approach associate query point multiple NN vertex produce plausible surface geometry even unseen pose, Limitation Clothing Deformations We also apply method CAPE dataset contains scan clothed human, We select subject longlong exhibit visible, Bai et al, Table Quantitative Evaluation Input Pose Accuracy Subject, We show result approach SNARF using pose provided AMASS dataset one refinement using vertex registered mesh, While SNARF greatly influenced accuracy pose parameter slight improvement method illustrates robustness SMPL fitting error, In addition approach significantly outperforms SNARF even pose refinement setting except rollouts interpolation set, Mean Scan Prediction Distance mm Rollout frame Interpolation Set SNARF AMASS Ours SNARF, Refined Poses Ours, Extrapolation Set SNARF AMASS Ours SNARF, Refined Poses Ours Mean Squared Error Volume Change Rollout frame Interpolation Set SNARF, AMASS Ours, SNARF, , Refined Poses Ours, , Extrapolation Set SNARF, AMASS Ours, SNARF, , Refined Poses Ours, AutoAvatar Autoregressive Neural Fields Dynamic Avatar Modeling Scan Projection Ours Scan Projection Ours Fig, NN v, Closest Surface Projection, While closest surface projection suffers artifact around armpit SDF decoding based NN produce plausible surface geometry unseen pose, dynamic deformation clothing, We exclude sequence athletics frisbee volleyball box trial swim trial twist tilt trial train ing use testing, We employ input template SMPL pose provided CAPE dataset training model, Note approximate raw scan sampling point cloud surface normal computed registered mesh CAPE dataset provides registered mesh longlong, Please refer supplementary video qualitative result, While approach produce plausible short term clothing deformation remains challenging model dynamically deforming clothing longer rollouts, Compared soft tissue deforma tions dynamic clothed human involve high frequency deformation topology change making learning clothing dynamic difficult, We leave future work\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "\n",
    "references_clean= preprocess_text(references)\n",
    "print(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5af5e9",
   "metadata": {},
   "source": [
    "## Get_human_names Algorithme using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa296d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    \n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    for person in person_list:\n",
    "        person_split = person.split(\" \")\n",
    "        for name in person_split:\n",
    "            if wordnet.synsets(name):\n",
    "                if(name in person):\n",
    "                    person_names.remove(person)\n",
    "                    break\n",
    "    return person_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864d30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 54 and without 41\n",
      "Ex: Alldieck Xu Sminchisescu\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "\n",
    "print(\"Len with preprocess\",len(get_human_names(references_clean)), \"and without\",len(get_human_names(references)))\n",
    "print(\"Ex:\",get_human_names(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8dd9ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alldieck Xu Sminchisescu',\n",
       " 'Khosla Popovic Seitz',\n",
       " 'Norouzi Tagliasacchi',\n",
       " 'Gropp Yariv Haim Atzmon Lipman',\n",
       " 'Holden Duong',\n",
       " 'Datta Nowrouzezahrai',\n",
       " 'Kanazawa Zhang',\n",
       " 'Mahmood Ghorbani Troje',\n",
       " 'Mildenhall Srinivasan',\n",
       " 'Tancik Barron',\n",
       " 'Ott Grebogi Yorke',\n",
       " 'Palafox Boz',\n",
       " 'Thies Nießner Dai',\n",
       " 'Prokudin Lassner Romero',\n",
       " 'Leskovec Battaglia',\n",
       " 'Santesteban Garces Otaduy',\n",
       " 'Sifakis Barbic',\n",
       " 'Wang Rojas Kla Kavan Sifakis',\n",
       " 'Tsuchida Fukayama Hamasaki Goto',\n",
       " 'Wang Brien',\n",
       " 'Xie Takikawa',\n",
       " 'Sukthankar Sminchisescu',\n",
       " 'Zakharkin Mazur Grigorev Lempitsky',\n",
       " 'Machine Learning',\n",
       " 'Poses Ours',\n",
       " 'Graphics TOG',\n",
       " 'Chen Zheng Black',\n",
       " 'Lecture Notes Computer Science',\n",
       " 'Kim Pons Moll Pujades Bang',\n",
       " 'Zinno Cheng Van De Panne',\n",
       " 'Ma Yang Ranjan Pujades Pons Moll Tang Black Learning',\n",
       " 'Oechsle Niemeyer Nowozin Geiger',\n",
       " 'Npms Neural',\n",
       " 'Pons Moll Romero Mahmood Black Dyna',\n",
       " 'Softsmpl Data',\n",
       " 'Advances Neural',\n",
       " 'Yang Liang Lin',\n",
       " 'Table Quantitative Evaluation Input Pose Accuracy Subject',\n",
       " 'Extrapolation Set SNARF',\n",
       " 'Bagautdinov Wu Simon Prada Shiratori Wei',\n",
       " 'Bogo Romero Pons Moll Black Dynamic',\n",
       " 'Snarf Differentiable',\n",
       " 'Habermann Liu Xu Zollhoefer Pons Moll Theobalt',\n",
       " 'Kim Black',\n",
       " 'Yang Ross',\n",
       " 'Cline Marching',\n",
       " 'Ma Yang Tang',\n",
       " 'Mihajlovic Zhang Black',\n",
       " 'Bolkart Black',\n",
       " 'Peng Zhang Xu Wang Shuai Bao Zhou',\n",
       " 'Su Mo Guibas Pointnet Deep',\n",
       " 'Sanchez Gonzalez Godwin Pfaff',\n",
       " 'Wang Mihajlovic Ma Geiger Tang',\n",
       " 'Zanfir Freeman',\n",
       " 'Closest Surface',\n",
       " 'Mean Scan',\n",
       " 'Fields Dynamic Avatar Modeling Scan']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the result\n",
    "get_human_names(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fcb76",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "656d73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "\n",
    "def nlp_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    PROPN=[token.lemma_ for token in doc if token.pos_ == \"PROPN\"]\n",
    "    \n",
    "#     Remove duplicate\n",
    "    PROPN = list(dict.fromkeys(PROPN))\n",
    "#     Remove word with first letter as lowercase \n",
    "    for word in PROPN:\n",
    "        if word[0].islower():\n",
    "            PROPN.remove(word)\n",
    "    return PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4230cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n"
     ]
    }
   ],
   "source": [
    "final_names_prep_spacy=nlp_entities(references_clean)\n",
    "final_names_spacy=nlp_entities(references)\n",
    "print(len(final_names_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d123b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An other function with more accuracy\n",
    "\n",
    "def extract(text:str) :\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    doc = spacy_nlp(text.strip())\n",
    "    named_entities = []\n",
    "    \n",
    "    for i in doc.ents:\n",
    "        entry = str(i.lemma_).lower()\n",
    "        text = text.replace(str(i).lower(), \"\")\n",
    "        if i.label_ in [\"ART\", \"EVE\", \"NAT\", \"PERSON\"]:\n",
    "            named_entities.append(entry.title().replace(\" \", \"_\").replace(\"\\n\",\"_\"))\n",
    "        named_entities = list(dict.fromkeys(named_entities))\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f8b14fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 85 and without 45\n",
      "Ex: Alldieck_Xu_Sminchisescu\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "\n",
    "print(\"Len with preprocess\",len(extract(references_clean)), \"and without\",len(extract(references)))\n",
    "print(\"Ex:\",extract(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "167b9906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alldieck_Xu_Sminchisescu',\n",
       " 'Imghum_Implicit',\n",
       " 'Allen_Curless',\n",
       " 'Popovic_Hertzmann',\n",
       " 'Anguelov_Srinivasan_Koller',\n",
       " 'Thrun_Rodgers_Davis',\n",
       " 'Bagautdinov_Wu_Simon',\n",
       " 'Shiratori_Wei',\n",
       " 'Xu_Sheikh_Saragih',\n",
       " 'Khosla_Popovic',\n",
       " 'Register',\n",
       " 'Chen_Zheng_Black',\n",
       " 'Hilliges_Geiger',\n",
       " 'Chen_Zhang',\n",
       " 'Learn',\n",
       " 'Deng_Lewis',\n",
       " 'Norouzi_Tagliasacchi',\n",
       " 'Atzmon_Lipman',\n",
       " 'Habermann_Liu',\n",
       " 'Xu_Zollhoefer_Pons_Moll_Theobalt',\n",
       " 'Datta_Nowrouzezahrai',\n",
       " 'Proceedings',\n",
       " 'Kanazawa_Zhang',\n",
       " 'Felsen_Malik',\n",
       " 'Kim_Pons_Moll_Pujades',\n",
       " 'Kim_Black',\n",
       " 'Lee',\n",
       " 'Springer',\n",
       " 'Bai',\n",
       " 'Al_Li_Yang_Ross',\n",
       " 'Ling',\n",
       " 'Zinno_Cheng_Van_De_Panne',\n",
       " 'Liu_Habermann_Rudnev',\n",
       " 'Sarkar_Gu_Theobalt',\n",
       " 'Neural',\n",
       " 'Cline_Marching',\n",
       " 'Ma_Saito_Yang',\n",
       " 'Tang_Black',\n",
       " 'Jun',\n",
       " 'Ma_Yang',\n",
       " 'Ghorbani_Troje',\n",
       " 'Martinez_Black',\n",
       " 'Mihajlovic_Zhang_Black',\n",
       " 'Mildenhall_Srinivasan',\n",
       " 'Tancik_Barron',\n",
       " 'Ramamoorthi_Ng',\n",
       " 'Oechsle_Geiger',\n",
       " 'Osman',\n",
       " 'Bolkart_Black_Star',\n",
       " 'Palafox_Boz',\n",
       " 'Nießner_Dai',\n",
       " 'Florence_Straub_Newcombe',\n",
       " 'Peng_Zhang',\n",
       " 'Wang_Shuai_Bao_Zhou',\n",
       " 'Pfaff_Fortunato_Sanchez',\n",
       " 'Gonzalez_Battaglia',\n",
       " 'Prokudin_Lassner_Romero',\n",
       " 'Efficient',\n",
       " 'Su_Mo_Guibas',\n",
       " 'Ronneberger_Fischer_Brox',\n",
       " 'Saito_Yang',\n",
       " 'Sanchez_Gonzalez',\n",
       " 'Godwin_Pfaff',\n",
       " 'Wiley_Online_Library',\n",
       " 'Sifakis_Barbic',\n",
       " 'Wang_Rojas_Kla_Kavan_Sifakis',\n",
       " 'Tiwari_Sarafianos_Tung_Pons_Moll',\n",
       " 'Tsuchida_Fukayama_Hamasaki_Goto',\n",
       " 'Wang_Brien',\n",
       " 'Wang_Mihajlovic',\n",
       " 'Ma_Geiger_Tang',\n",
       " 'Metaavatar_Learning',\n",
       " 'Xiang_Prada',\n",
       " 'Bagautdinov_Xu_Dong_Wen_Hodgins_Wu',\n",
       " 'Al_Xu_Bazavan',\n",
       " 'Zanfir_Freeman',\n",
       " 'Sukthankar_Sminchisescu',\n",
       " 'Yang_Liang_Lin_Learning',\n",
       " 'Zakharkin_Mazur_Grigorev_Lempitsky',\n",
       " 'Zheng_Zhou_Ceylan_Barbic',\n",
       " 'Pose_Accuracy',\n",
       " 'Tab',\n",
       " 'Fig',\n",
       " 'Pose_Accuracy_Subject',\n",
       " 'Ours_Fig']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Result\n",
    "\n",
    "extract(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dbd17",
   "metadata": {},
   "source": [
    "## TextBlob : TextBlob est une bibliothèque python et propose une API simple pour accéder à ses méthodes et effectuer des tâches NLP de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86155dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_names_TextBlob= TextBlob(references_clean)\n",
    "PROPN=[words for words, tag in final_names_TextBlob.tags if tag == \"NNP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cc696d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 814\n",
      "Ex: Alldieck\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess\n",
    "final_names_TextBlob=PROPN\n",
    "#With preprocess\n",
    "print(\"Len with preprocess\",len(final_names_TextBlob))\n",
    "print(\"Ex:\",final_names_TextBlob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60bbf0",
   "metadata": {},
   "source": [
    "Also efficence but not useful here because spacy hasa better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb67165",
   "metadata": {},
   "source": [
    "## Apply Spacy functions on all pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b585bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'                AutoAvatar: Autoregressive Neural Fields\\r\\n                     for Dynamic Avatar Modeling\\r\\n\\r\\n        Ziqian Bai1,2\\xe2\\x88\\x97        Timur Bagautdinov2 Javier Romero2             Michael Zollho\\xcc\\x88fer2\\r\\n                                    Ping Tan1 Shunsuke Saito2\\r\\n                         1                              2\\r\\n                             Simon Fraser University        Reality Labs Research\\r\\n\\r\\n\\r\\n\\r\\n           Abstract. Neural fields such as implicit surfaces have recently enabled avatar\\r\\n           modeling from raw scans without explicit temporal correspondences. In this work,\\r\\n           we exploit autoregressive modeling to further extend this notion to capture dynamic\\r\\n           effects, such as soft-tissue deformations. Although autoregressive models are\\r\\n           naturally capable of handling dynamics, it is non-trivial to apply them to implicit\\r\\n           representations, as explicit state decoding is infeasible due to prohibitive memory\\r\\n           requirements. In this work, for the first time, we enable autoregressive modeling of\\r\\n           implicit avatars. To reduce the memory bottleneck and efficiently model dynamic\\r\\n           implicit surfaces, we introduce the notion of articulated observer points, which\\r\\n           relate implicit states to the explicit surface of a parametric human body model.\\r\\n           We demonstrate that encoding implicit surfaces as a set of height fields defined on\\r\\n           articulated observer points leads to significantly better generalization compared\\r\\n           to a latent representation. The experiments show that our approach outperforms\\r\\n           the state of the art, achieving plausible dynamic deformations even for unseen\\r\\n           motions. https://zqbai-jeremy.github.io/autoavatar.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n1       Introduction\\r\\nAnimatable 3D human body models are key enablers for various applications ranging\\r\\nfrom virtual try-on to social telepresence [4]. While modeling of human avatars from\\r\\n3D scans without surface registration is gaining more and more attention in recent\\r\\nyears [43,24,48,8,26], complex temporal dynamics are often completely ignored and the\\r\\nresulting deformations are often treated exclusively as a function of the pose parameters.\\r\\nHowever, the body shape is not uniquely determined by the current pose of the human,\\r\\nbut also depends on the history of shape deformations due to secondary motion effects.\\r\\nThe goal of our work is to realistically model these history-dependent dynamic effects\\r\\nfor human bodies without requiring precise surface registration.\\r\\n    To this end, we propose AutoAvatar, a novel autoregressive model for dynamically\\r\\ndeforming human bodies. AutoAvatar models body geometry implicitly - using a signed\\r\\ndistance field (SDF) - and is able to directly learn from raw scans without requiring\\r\\ntemporal correspondences for supervision. In addition, akin to physics-based simulation,\\r\\nAutoAvatar infers the complete shape of an avatar given history of shape and motion. The\\r\\n    \\xe2\\x88\\x97\\r\\n        Work done while Ziqian Bai was an intern at Reality Labs Research, Pittsburgh, PA, USA.\\r\\n\\x0c\\n\\n2        Z. Bai et al.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 1: AutoAvatar. Given raw 4D scans with self-intersections, holes, and noise (grey meshes)\\r\\nand fitted SMPL models (blue meshes), AutoAvatar automatically learns highly detailed animatable\\r\\nbody models with plausible secondary motion dynamics without requiring a personalized template\\r\\nor surface registration (right).\\r\\n\\r\\naforementioned properties lead to a generalizable method that models complex dynamic\\r\\neffects including inertia and elastic deformations without requiring a personalized\\r\\ntemplate or precise temporal correspondences across training frames.\\r\\n    To model temporal dependencies in the data, prior work has typically resorted to\\r\\nautoregressive models [39,22,28,45]. While the autoregressive framework naturally\\r\\nallows for incorporation of temporal information, combining it with neural implicit\\r\\nsurface representations [29,36,9] for modeling human bodies is non-trivial. Unlike\\r\\nexplicit shape representations, such neural representations implicitly encode the shape in\\r\\nthe parameters of the neural network and latent codes. Thus, in practice, producing the\\r\\nactual shape requires expensive neural network evaluation at each voxel of a dense spatial\\r\\ngrid [36]. This aspect is particularly problematic for autoregressive modeling, since\\r\\nmost of the successful autoregressive models rely on rollout training [28,19] to ensure\\r\\nstability of both training and inference. Unfortunately, rollout training requires multiple\\r\\nevaluations of the model for each time step, and thus becomes prohibitively expensive\\r\\nboth in terms of memory and compute as the resolution of the spatial grid grows. Another\\r\\napproach would be learning an autoregressive model using latent embeddings that encode\\r\\ndynamic shape information [15]. However, it is infeasible to observe the entire span of\\r\\npossible surface deformations from limited real-world scans, which makes the model\\r\\nprone to overfitting and leads to worse generalization at test time.\\r\\n    By addressing these limitations, we, for the first time, enable autoregressive training\\r\\nof a full-body geometry model represented by a neural implicit surface. To tackle\\r\\nthe scalability issues of rollout training for implicit representations, we introduce the\\r\\nnovel notion of articulated observer points. Intuitively, articulated observer points are\\r\\ntemporally coherent locations on the human body surface which store the dynamically\\r\\nchanging state of the implicit function. In practice, we parameterize the observer points\\r\\nusing the underlying body model [22], and then represent the state of the implicit surface\\r\\nas signed heights with respect to the vertices of the pose-dependent geometry produced by\\r\\nthe articulated model (see Fig. 3a). The number of query points is significantly lower than\\r\\nthe number of voxels in a high-resolution grid, which allows for a significant reduction\\r\\n\\x0c\\n\\n              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling     3\\r\\n\\r\\nin terms of memory and compute requirements, making rollout training tractable for\\r\\nimplicit surfaces. In addition, we demonstrate that explicitly encoding shapes as signed\\r\\nheight fields is less prone to overfitting compared to latent embeddings, a common way\\r\\nto represent autoregressive states [19,52].\\r\\n    Our main contributions are the following:\\r\\n    \\xe2\\x80\\x93 The first autoregressive approach for modeling history-dependent implicit surfaces\\r\\n      of human bodies,\\r\\n    \\xe2\\x80\\x93 Articulated observer points to enable autogressive training of neural fields, and\\r\\n    \\xe2\\x80\\x93 Extensive experiments showing that our approach outperforms existing methods\\r\\n      both on shape interpolation and extrapolation tasks.\\r\\n\\r\\n\\r\\n2     Related Work\\r\\nParametric Human Models Since the anatomical structure of humans is shared across\\r\\nidentities, various methods have been proposed to parameterize shape and pose of\\r\\nhuman bodies from large-scale 3D scan data [3,13,22,33,54,1]. SCAPE [3,13] learns\\r\\nstatistical human model models using triangle deformations. The pioneering work\\r\\nby Allen et al. [2] used a vertex-based representation enhanced with pose-dependent\\r\\ndeformations, but the model was complex and trained with insufficient data, resulting\\r\\nin overfitting. SMPL [22] improved the generalizability of [2] by training on more data\\r\\nand removing the shape dependency in the pose-dependent deformations. More recent\\r\\nworks show that sparsity in the pose correctives reduces spurious correlations [33],\\r\\nand that non-linear deformation bases parameterized by neural networks achieve better\\r\\nmodeling accuracy [54]. While most works focus on modeling static human bodies\\r\\nunder different poses, Dyna [39] and DMPL (Dynamic SMPL) [22] enable parametric\\r\\nmodeling of dynamic deformations by learning a linear autoregressive model. Kim\\r\\net al. [16] combine a volumetric parametric model, VSMPL, with an external layer\\r\\ndriven by the finite element method to enable soft tissue dynamics. SoftSMPL [45]\\r\\nlearns a more powerful recurrent neural network to achieve better generalization to\\r\\nunseen subjects. Xiang et al. [52] model dynamically moving clothing from a history\\r\\nof poses. Importantly, the foundation of the aforementioned works is accurate surface\\r\\nregistration of a template body mesh [6,7], which remains non-trivial. Habermann et\\r\\nal. [12] also model dynamic deformations from a history of poses. While they relax the\\r\\nneed of registration by leveraging image-based supervision, a personalized template is\\r\\nstill required as a preprocessing step.\\r\\n     Recently, neural networks promise to enable the modeling of animatable bodies\\r\\nwithout requiring surface registration or a personalized template [10,24,43,48,8]. These\\r\\nmethods leverage structured point clouds [24,26,56] or 3D neural fields [53] to learn\\r\\nanimatable avatars. Approaches based on neural fields parameterize human bodies as\\r\\ncompositional articulated occupancy networks [10] or implicit surface in canonical space\\r\\nwith linear blend skinning [30,43,8,51] and deformation fields [48,35]. Since implicit\\r\\nsurfaces do not require surface correspondences for training, avatars can be learned from\\r\\nraw scans. Similarly, neural radiance fields [31] have been applied to body modeling to\\r\\nbuild animatable avatars from multi-view images [37,20]. However, these approaches\\r\\nrepresent avatars as a function of only pose parameters, and thus are unable to model\\r\\n\\x0c\\n\\n4              Z. Bai et al.\\r\\n\\r\\n                     Shape Encoding                                   Dynamic Feature Encoding                     Articulation-Aware Shape Decoding\\r\\n    SMPL Poses                                              Temporal Derivatives          Localized Pose\\r\\n                                                             \\xc2\\xb7 dH\\r\\n                                                                          p\\xc2\\xb7 \\xe2\\x89\\x88\\r\\n                                                                               dp                                      Query Point\\r\\n                      Articulated                            H\\xe2\\x89\\x88                       L(p) = (W \\xe2\\x8b\\x85 \\xcf\\x89i) \\xe2\\x88\\x98 p\\r\\n                       Observer                                 dt             dt                                         q\\r\\n                                                                                                                              k(q)\\r\\n                        Points        Signed Heights\\r\\n\\r\\npt\\xe2\\x88\\x922, pt\\xe2\\x88\\x921, pt, pt+1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                             Resample\\r\\n                                                                         UNet                                              SDF decoding\\r\\n    Implicit Shapes                                                                                                            f(q)\\r\\n\\r\\n                                                              \\xc2\\xb7                                                                              St+1\\r\\n                                      Ht\\xe2\\x88\\x922, Ht\\xe2\\x88\\x921, Ht     Ht, {Ht+i}                 Zuv                 Zt+1        Features in\\r\\n                                                       L(pt+1), {L( p\\xc2\\xb7 t+i)}\\r\\n                      SDF values                                                                                   Posed Space\\r\\n                                                                                                  Dynamic Latent\\r\\n                                                                                                   Embeddings\\r\\n    St\\xe2\\x88\\x922, St\\xe2\\x88\\x921, St\\r\\n\\r\\n\\r\\nFig. 2: Overview. AutoAvatar learns a pose-driven animatable human body model with plausible\\r\\ndynamics including secondary motions. Notice that our approach takes the history of implicit\\r\\nshapes in an autoregressive manner for learning dynamics.\\r\\n\\xf0\\x9d\\x92\\xa9\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ndynamics. While our approach is also based on 3D neural fields to eliminate the need\\r\\nfor surface registration, our approach learns not only pose-dependent deformations but\\r\\nalso history-dependent dynamics by enabling autoregressive training of neural implicit\\r\\nsurfaces.\\r\\n\\r\\nLearning Dynamics Traditionally, physics-based simulation [46] is used to model\\r\\ndynamics of objects. While material parameters of physics simulation can be estimated\\r\\nfrom real data [5,50,47,55], accurately simulating dynamic behavior of objects remains\\r\\nan open question. In addition, authenticity of physics-based simulation is bounded by the\\r\\nunderlying model, and complex anisotropic materials such as the human body are still\\r\\nchallenging to model accurately. For this reason, several works attempt to substitute a\\r\\ndeterministic physics-based simulation with a learnable module parameterized by neural\\r\\nnetworks [14,57,44,38]. Such approaches have been applied to cloth simulation [14,38],\\r\\nfluid [44], and elastic bodies [57]. Subspace Neural Physics [14] learns a recurrent\\r\\nneural network from offline simulation to predict the simulation state in a subspace.\\r\\nDeep Emulator [57] first learns an autoregressive model to predict deformations using a\\r\\nsimple primitive (sphere), and applies the learned function to more complex characters.\\r\\nWhile we share the same spirit with the aforementioned works by learning dynamic\\r\\ndeformations in an autoregressive manner, our approach fundamentally differs from\\r\\nthem. The aforementioned approaches all assume that physical quantities such as vertex\\r\\npositions are observable with perfect correspondence in time, and thus results are only\\r\\ndemonstrated on synthetic data. In contrast, we learn dynamic deformations from real-\\r\\nworld observation while requiring only coarse temporal guidance by the fitted SMPL\\r\\nmodels. This property is essential to model faithful dynamics of real humans.\\r\\n\\r\\n\\r\\n3        Method\\r\\nOur approach is an autoregressive model, which takes as inputs human poses and a shape\\r\\nhistory and produces the implicit surface for a future frame. Fig. 2 shows the overview\\r\\nof our approach. Given a sequence of T implicitly encoded shapes {St\\xe2\\x88\\x92T +1 , ..., St }\\r\\nand T + 1 poses {pt\\xe2\\x88\\x92T +1 , ..., pt+1 } with t being the current time frame, our model\\r\\npredicts the implicit surface St+1 of the future frame t + 1. The output shape St+1 is\\r\\n\\x0c\\n\\n                   AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling                                                             5\\r\\n\\r\\n\\r\\n                                                                           ci = cos(q \\xe2\\x88\\x92 vi, ni)\\r\\n                        Articulated\\r\\n                         Observer\\r\\n                                                                           di = \\xe2\\x88\\xa5q \\xe2\\x88\\x92 vi\\xe2\\x88\\xa52\\r\\n                          Points\\r\\n\\r\\n                                                                                           [zit+1, di, ci]\\r\\n                                                                                 vi ni           \\xe2\\x80\\xa6\\r\\n                           Signed\\r\\n                                    Height\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                             PointNet\\r\\n                                                                                           [zjt+1, dj, cj]\\r\\n                                                                                     q\\r\\n                                                                                           [zkt+1, dk, ck]\\r\\n                                                                    v{i,j,k} \\xe2\\x88\\x88      k(q)\\r\\n                                                         Zt+1                                                                    St+1\\r\\n      Posed SMPL                                 Features in Posed Space                                                Signed Distance Functions\\r\\n\\r\\n         (a) Shape Encoding.                               (b) Articulation-Aware SDF Decoding.\\r\\n                                             \\xf0\\x9d\\x92\\xa9\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3: Shape Encoding/Decoding. Our novel shape encoding via articulated observer points and\\r\\narticulated-aware SDF decoding lead to faithful modeling of dynamics.\\r\\n\\r\\nthen passed as an input to the next frame prediction in an autoregressive manner. Our\\r\\nmodel is supervised directly with raw body scans, and requires a training dataset of 4D\\r\\nscans (sequences of 3D scans) along with fitted SMPL body models [22]. Unfortunately,\\r\\nexplicitly representing shapes St as levelsets of implicit surface is prohibitively expensive\\r\\nfor end-to-end training. To this end, we introduce the concept of articulated observer\\r\\npoints - vertex locations on the underlying articulated model - which are used as a local\\r\\nreference for defining the full body geometry. The underlying implicit surface is encoded\\r\\nas a height field with respect to the articulated observer points (Sec. 3.1). Given a history\\r\\nof height fields and pose parameters, we convert those to dynamic latent feature maps\\r\\nin UV space (Sec. 3.2). Finally, we map the resulting features to SDFs by associating\\r\\ncontinuous 3D space with the learned features on the SMPL vertices, which are directly\\r\\nsupervised by point clouds with surface normals (Sec. 3.3).\\r\\n\\r\\n3.1     Shape Encoding via Articulated Observer Points\\r\\nThe core of our approach is an autoregressive model that operates on implicit neural\\r\\nsurfaces, allowing us to incorporate temporal shape information necessary for modeling\\r\\nchallenging dynamics. The key challenge that arises when training such autoregressive\\r\\nmodels is finding a way to encode the shape - parameterized implicitly as a neural field -\\r\\ninto a representation that can be efficiently computed and fed back into the model. The\\r\\nmost straightforward way is to extract an explicit geometry representation by evaluating\\r\\nthe neural field on a dense spatial grid and running marching cubes [23]. However, in\\r\\npractice this approach is infeasible due to prohibitive memory and computational costs,\\r\\nin particular due to the cubic scaling with respect to the grid dimensions. Instead, we\\r\\npropose to encode the state of the implicit surface into a set of observer points.\\r\\n    Encoding geometry into discrete point sets has been shown to be efficient and\\r\\neffective for learning shape representations from point clouds [40]. Prokudin et al. [40]\\r\\nrelies on a fixed set of randomly sampled observer points in global world coordinates,\\r\\nwhich is not suitable for modeling dynamic humans due to the articulated nature of human\\r\\nmotion. Namely, a model relying on observer points with a fixed 3D location needs to\\r\\naccount for extremely large shape variations including rigid transformations, making\\r\\nthe learning task difficult. Moreover, associating randomly sampled 3D points with a\\r\\nparametric human body is non-trivial. To address these limitations, we further extend the\\r\\n\\x0c\\n\\n6       Z. Bai et al.\\r\\n\\r\\nnotion of observer points to an articulated template represented by the SMPL model [22],\\r\\nwhich provides several advantages for modeling dynamic articulated geometries. In\\r\\nparticular, soft-tissue dynamic deformations appear only around the minimally clothed\\r\\nbody, and we can rely on this notion as an explicit prior to effectively allocate observer\\r\\npoints only to the relevant regions. In addition, the SMPL model provides a mapping of\\r\\n3D vertices to a common UV parameterization, allowing us to effectively process shape\\r\\ninformation using 2D CNNs in a temporally consistent manner.\\r\\n     More specifically, to encode the neural implicit surface into the articulated observer\\r\\npoints, we compute \\xe2\\x80\\x9csigned heights\\xe2\\x80\\x9d H = {hi }M      i=1 \\xe2\\x88\\x88 R\\r\\n                                                            M\\r\\n                                                               from M vertices on a fitted\\r\\nSMPL model. For each vertex, the signed height hi is the signed distance from the\\r\\nvertex to the zero-crossing of the implicit surface along the vertex normal (see Fig. 3a).\\r\\nWe use the iterative secant method as in [32] to compute the zero-crossings. Note that\\r\\nthere can be multiple valid signed heights per vertex since the line along the normal can\\r\\nhit the zero-crossing multiple times. Based on the observation that the SMPL vertices\\r\\nare usually close to the actual surface with their normals roughly facing into the same\\r\\ndirection, we use the minimum signed height within a predefined range [hmin , hmax ]\\r\\n(in our experiments, we use hmin = \\xe2\\x88\\x922cm, hmax = 8cm). If no zero-crossing is found\\r\\ninside this range, we set the signed height to hmin . Note that the computed heights\\r\\nare signed because the fitted SMPL can go beyond the actual surface due to its limited\\r\\nexpressiveness and inaccuracy in the fitting stage.\\r\\n\\r\\n3.2   Dynamic Feature Encoding\\r\\nThe essence of AutoAvatar is an animatable autoregressive model. In other words,\\r\\na reconstructed avatar is driven by pose parameters, while secondary dynamics is\\r\\nautomatically synthesized from the history of shapes and poses. To enable this, we learn\\r\\na mapping that encodes the history of shape and pose information to latent embeddings\\r\\ncontaining the shape information of the future frame. More specifically, denoting the\\r\\ncurrent time frame as t, we take as input T + 1 poses {pt\\xe2\\x88\\x92T +1 , ..., pt+1 } and T signed\\r\\nheights vectors {Ht\\xe2\\x88\\x92T +1 , ..., Ht }, and produce dynamic features Zt+1 \\xe2\\x88\\x88 RM \\xc3\\x97C .\\r\\nGiven these inputs, we also compute the temporal derivatives of poses {p\\xcc\\x87t+i }1i=\\xe2\\x88\\x92T +2\\r\\nand signed heights {H\\xcc\\x87t+i }0i=\\xe2\\x88\\x92T +2 as follows:\\r\\n\\r\\n                                               \\\\begin {aligned} \\\\dot {\\\\bm {p}}_{k} &= \\\\bm {p}_{k} \\\\bm {p}_{k-1}^{-1} \\\\\\\\ \\\\dot {\\\\bm {H}}_{k} &= \\\\bm {H}_{k} - \\\\bm {H}_{k-1}. \\\\end {aligned} \\r\\n                                                                                                                                                                                             (1)\\r\\n\\r\\nNote that in practice p\\xe2\\x88\\x97 are represented as quaternions, and pk p\\xe2\\x88\\x921\\r\\n                                                                 k\\xe2\\x88\\x921 is computed by first\\r\\nconverting multipliers to rotation matrices, multiplying those, and then converting the\\r\\nproduct back to quaternions. To emphasize small values in H\\xcc\\x87, we apply the following\\r\\ntransformation g(x) = sign(x) \\xc2\\xb7 ln(\\xce\\xb1|x| + 1) \\xc2\\xb7 \\xce\\xb2, where \\xce\\xb1 = 1000 and \\xce\\xb2 = 0.25.\\r\\nFollowing prior works [43,4], we also localize pose parameters to reduce long range\\r\\nspurious correlations as follows:\\r\\n                                  \\\\begin {aligned} L(\\\\bm {p}) &= (W \\\\cdot \\\\omega _i) \\\\circ \\\\bm {p}, \\\\end {aligned}                                                                           (2)\\r\\nwhere \\xe2\\x97\\xa6 denotes the element-wise product, i is the vertex index, W \\xe2\\x88\\x88 RJ\\xc3\\x97J is an\\r\\nassociation matrix of J joints, and \\xcf\\x89i \\xe2\\x88\\x88 RJ\\xc3\\x971 is the skinning weights of the i-th vertex.\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling                                                                     7\\r\\n\\r\\nWe set Wn,m = 1 if the n-th joint is within the 1-ring neighborhood of the m-th joint\\r\\n(otherwise Wn,m = 0). Note that the derivative of the root transformation is included in\\r\\n{L(p\\xcc\\x87t+i )} without localization. Finally, we map Ht , {H\\xcc\\x87t+i }, L(pt+1 ), and {L(p\\xcc\\x87t+i )}\\r\\nto UV space using barycentric interpolation. The concatenated features are fed into a\\r\\nUNet [42] to generate a feature map Zuv . We then resample Zuv on the UV coordinates\\r\\ncorresponding to SMPL vertices to obtain the per-vertex dynamic latent embeddings\\r\\nZ. We empirically found that incorporating temporal derivatives further improves the\\r\\nrealism of dynamics (see Supp. Mat. video for comparison).\\r\\n\\r\\n3.3   Articulation-Aware Shape Decoding\\r\\nGiven the dynamic feature Zt+1 = {z1t+1 , ..., zM   t+1\\r\\n                                                        } and a query point q, we decode\\r\\nsigned distance fields f (q) to obtain the surface geometry of the dynamic avatar. Several\\r\\nmethods model the implicit surface in canonical space by jointly learning a warping\\r\\nfunction from the posed space to the canonical space [43,8]. However, we observe that\\r\\nthe canonicalization step is very sensitive to small fitting error in the SMPL model, and\\r\\nfurther amplifies the error in the canonical space, making it difficult to learn dynamics\\r\\n(see discussion in Sec. 4). Therefore, we directly model the implicit surface in a posed\\r\\nspace while being robust to pose changes. Inspired by Neural Actor [20], we associate\\r\\na queried 3D point with a human body model and pose-agnostic spatial information.\\r\\nSpecifically, Neural Actor uses height from the closest surface point on the SMPL\\r\\nmodel to the query location together with a feature vector sampled on the same surface\\r\\npoint. However, we find that their approach based on the single closest point leads\\r\\nto artifacts around body joints (e.g., armpits) for unseen poses. To better distinguish\\r\\nregions with multiple body parts, we instead use k-nearest neighbor vertices. Fig. 3b\\r\\nshows the illustration of our SDF decoding approach. Given a query point q, we first\\r\\ncompute the k-nearest SMPL vertices {vi }i\\xe2\\x88\\x88Nk (q) , where Nk (q) is a set of indices\\r\\nof k-nearest neighbor vertices. To encode pose-agnostic spatial information, we use\\r\\nrotation-invariant features. Specifically, we compute the distance di = \\xe2\\x88\\xa5q \\xe2\\x88\\x92 vi \\xe2\\x88\\xa52 and\\r\\ncosine value ci = cos(xi , ni ), where xi is the vertex-to-query vector \\x02 xi = q \\xe2\\x88\\x92\\x03vi , and\\r\\nni is the surface normal on vi . We feed the concatenated vector zit+1 , di , ci into a\\r\\nPointNet-like [41] architecture to compute the final SDFs with the max pooling replaced\\r\\nby a weighted average pooling based on jointly predicted weights for better continuity.\\r\\n    As in [43], we employ implicit geometric regularization (IGR) [11] to train our\\r\\nmodel directly from raw scans without requiring watertight meshes. Note that in contrast,\\r\\nother methods [30,8,48] require watertight meshes to compute ground-truth occupancy\\r\\nor signed distance values for training. Our final objective function L is the following:\\r\\n                             \\\\label {eq:igr} \\\\begin {aligned} L &= L_{s} + L_{n} + \\\\lambda _{igr} L_{igr} + \\\\lambda _{o} L_{o}, \\\\\\\\ \\\\end {aligned}    (3)\\r\\nwhere \\xce\\xbbigr = 1.0, \\xce\\xbbo = 0.1. Ls promotes SDFs which vanish on the ground    Ptruth surface,\\r\\nwhile LPn encourages that its normal align with the ones from data: Ls =     q\\xe2\\x88\\x88Qs |f (q)|,\\r\\nLn =      q\\xe2\\x88\\x88Qs  \\xe2\\x88\\xa5\\xe2\\x88\\x87 q f (q) \\xe2\\x88\\x92  n(q)\\xe2\\x88\\xa5  2 , where Q s is the surface of the input raw scans.\\r\\nLigr is the Eikonal regularization term [11] that encourages the function f to satisfy\\r\\nthe Eikonal equation: Ligr = Eq (\\xe2\\x88\\xa5\\xe2\\x88\\x87q f (q)\\xe2\\x88\\xa52 \\xe2\\x88\\x92 1)2 , and Lo prevents off-surface SDF\\r\\nvalues from being too close to the zero-crossings as follows: Lo = Eq (exp(\\xe2\\x88\\x92\\xce\\xb3 \\xc2\\xb7|f (q)|)),\\r\\nwhere \\xce\\xb3 = 50.\\r\\n\\x0c\\n\\n8       Z. Bai et al.\\r\\n\\r\\n3.4   Implementation Details\\r\\nNetwork Architectures. In our experiments, we use a UV map of resolution 256 \\xc3\\x97 256,\\r\\nT = 3, and k = 20. To reduce the imbalance of SMPL vertex density for k-NN\\r\\ncomputation, we use 3928 points subsampled by poisson-disk sampling on the SMPL\\r\\nmesh. Before being fed into the UNet, L(pk+1 ) and {L(p\\xcc\\x87t+i )} are compressed to 32\\r\\nchannels using 1\\xc3\\x971 convolutions. The UNet uses convolution and transposed convolution\\r\\nlayers with untied biases, a kernel size of 3, no normalization, and LeakyReLU with a\\r\\nslope of 0.2 as the non-linear activation, except for the last layer which uses TanH. The\\r\\nSDF decoder is implemented as an MLP, which takes as input 64-dim features from the\\r\\nUNet, positional encoded di and ci up to 4-th order Fourier features. The number of\\r\\nintermediate neurons in the first part of the MLP is (128, 128, 129), where the output\\r\\nis split into a 128-dim feature vector and a 1-dim scalar, which is converted into non-\\r\\nnegative weights by softmax across the k-NN samples. After weighted average pooling,\\r\\nthe aggregated feature is fed into another MLP with a neuron size of (128, 128, 1) to\\r\\npredict the SDF values. The MLPs use Softplus with \\xce\\xb2 = 100 and a threshold of 20 as\\r\\nnon-linear activation except for the last layer which does not apply any activation.\\r\\nTraining. Our training consists of two stages. First, we train our model using ground-\\r\\ntruth signed heights without rollout for 90000 iterations. Then, we finetune the model\\r\\nusing a rollout of 2 frames for another 7500 iterations to reduce error accumulation for\\r\\nboth training and inference. We use the Adam optimizer with a learning rate of 1.0\\xc3\\x9710\\xe2\\x88\\x924\\r\\n(1.0 \\xc3\\x97 10\\xe2\\x88\\x925 ) at the first (second) stage. To compute Ln , we sample 10000(1000) points\\r\\non the scan surface. Similarly, for Ligr , we sample 10000(1000) points around the scan\\r\\nsurface by adding Gaussian noise with standard deviation of 10cm to uniformly sampled\\r\\nsurface points, and sample 2000(500) points within the bounding box around the raw\\r\\nscans. The points uniformly sampled inside the bounding box are also used to compute\\r\\nLo . Both stages are trained with a batch size of 1.\\r\\nInference. At the beginning of the animations, we assume ground-truth raw scans are\\r\\navailable for the previous T frames for initialization. If no ground truth initial shape is\\r\\navailable, we initialize the first T frames with our baseline model conditioned only on\\r\\npose parameters. Note that the scan data is extremely noisy around the hand and foot\\r\\nareas, and the SMPL fitting of the head region is especially inaccurate. Therefore, we fix\\r\\nthe dynamic features on the face, hands, and feet to the ones of the first frame.\\r\\n\\r\\n\\r\\n4     Experimental Results\\r\\n4.1   Datasets and Metrics\\r\\nDatasets. We use the DFaust dataset [7] for both training and quantitative evaluation,\\r\\nand AIST++ [49,18] for qualitative evaluation on unseen motions. For the DFaust\\r\\ndataset, we choose 2 subjects (50002 and 50004), who exhibit the most soft-tissue\\r\\ndeformations. The interpolation test evaluates the fidelity of dynamics under the same\\r\\ntype of motions as in training but at different time instance, and the extrapolation\\r\\ntest evaluates performance on unseen motion. For 50002, we use the 2nd half of\\r\\nchicken wings and running on spot for the interpolation test, one leg jump\\r\\nfor the extrapolation test, and the rest for training. For 50004, we use the 2nd half of\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling        9\\r\\n\\r\\nchicken wings and running on spot for interpolation, one leg loose for\\r\\nextrapolation, and the rest for training. The fitted SMPL parameters in DFaust are\\r\\nprovided by the AMASS [27] dataset that uses sparse points on the registered data as\\r\\napproximated mocap marker locations and computes the parameters using MoSh [21].\\r\\nNote that more accurate pose can be obtained by using all the registration vertices (see\\r\\nAppendix A), but this is not required by our method to recover soft-tissue deformation.\\r\\nMetrics. For evaluation, we extract the 0-level set surface at each time step using\\r\\nMarching Cubes [23] with a resolution of 2563 . We also use simplified scans with\\r\\naround 10000 vertices and outlier points (distance to the nearest SMPL vertex larger\\r\\nthan 10cm) have been removed. We evaluate the accuracy of the predicted surface in\\r\\nterms of its position and dynamics accuracy. The surface position accuracy is measured\\r\\nby averaging the distance from each simplified scan vertex to the closest prediction\\r\\nsurface point. Evaluating the dynamics accuracy of the implicit surface efficiently is more\\r\\nchallenging. We approximate the local occupied volume as a scalar per registration vertex\\r\\nrepresenting the ratio of surrounding points contained in the interior of the (ground-truth\\r\\nor inferred) surface. We use 10 points uniformly sampled inside a 5cm cube centered at\\r\\nthe vertex. The head, hands and feet vertices are ignored due to their high noise levels.\\r\\nThe temporal difference of this scalar across adjacent frames can be interpreted as a\\r\\ndynamic measure of the local volume evolution. We report the mean square difference\\r\\nbetween this dynamic descriptor as computed with the ground truth simplified scan\\r\\nand the inferred implicit surface. Since a small phase shift in dynamics may lead to\\r\\nlarge cumulative error, reporting only the averaged errors from the entire frames can\\r\\nbe misleading. Therefore, we report errors along the progression of rollout predictions.\\r\\nFor each evaluation sequence, we start prediction every 20 frames (i.e., 20th frame, 40th\\r\\nframe, ...), and use the ground truth pose and shape history only for the first frame,\\r\\nfollowed by the autoregressive predictions for the error computation. In Tab. 1, we\\r\\nreport the averaged errors for both metrics after 1, 2, 4, 8, 16, and 30 rollouts. The\\r\\nerrors for small number of rollouts evaluate the accuracy of future shape prediction\\r\\ngiven the ground-truth shape history, whereas the errors with longer rollouts evaluate\\r\\nthe accumulated errors by autoregressively taking as input the predictions of previous\\r\\nframes. We discuss the limitation of error metrics with longer rollouts in Sec. 4.2.\\r\\n\\r\\n\\r\\n4.2   Evaluation\\r\\n\\r\\nIn this section, we provide comprehensive analysis to validate our design choices and\\r\\nhighlight the limitations of alternative approaches and SoTA methods based on both\\r\\nimplicit and explicit shape representations. Note that all approaches use the same training\\r\\nset, and are trained with the same number of iterations as our method for fair comparison.\\r\\nEffectiveness of Autoregressive Modeling. While autoregressive modeling is a widely\\r\\nused technique for learning dynamics [39,57,38], several recent methods still employ\\r\\nonly the history of poses for modeling dynamic avatars [52,12]. Thus, to evaluate the\\r\\neffectiveness of autoregressive modeling we compare AutoAvatar with pose-dependent\\r\\nalternatives that use neural implicit surfaces. More specifically, we design the following\\r\\n3 non-autoregressive baselines:\\r\\n\\x0c\\n\\n10        Z. Bai et al.\\r\\n\\r\\n\\r\\nTable 1: Quantitative Comparison with Baseline Methods. Our method produces the most\\r\\naccurate predictions of the future frames given the ground-truth shape history among all baseline\\r\\nmethods (see rollout 1-4). For longer rollouts, more dynamic predictions lead to higher error than\\r\\nless dynamic results due to high sensitivity to initial conditions in dynamic systems [34] (see\\r\\ndiscussion in Sec. 4.2).\\r\\n\\r\\n                      (a) Mean Scan-to-Prediction Distance (mm) \\xe2\\x86\\x93 on DFaust.\\r\\n\\r\\n                                                          Rollout (# of frames)\\r\\n                                               1       2       4         8      16      30\\r\\n                                        Interpolation Set\\r\\n                              SNARF [8]      7.428 7.372 7.337 7.476 7.530             7.656\\r\\n                              Pose           4.218 4.202 4.075 4.240 4.409             4.426\\r\\n      Non-Autoregressive\\r\\n                              PoseTCN        4.068 4.118 4.086 4.228 4.405             4.411\\r\\n                              Pose + dPose 3.852 3.841 3.764 3.972 4.164               4.156\\r\\n                              G-embed       2.932 3.006      3.131   3.462     3.756   3.793\\r\\n      Autoregressive          L-embed       1.784 2.138      2.863   4.250     5.448   5.916\\r\\n                              Ours          1.569 1.914      2.587   3.627     4.736   5.255\\r\\n                                        Extrapolation Set\\r\\n                              SNARF [8]     7.264 7.287      7.321   7.387     7.308   7.251\\r\\n                              Pose          4.303 4.306      4.308   4.299     4.385   4.398\\r\\n      Non-Autoregressive\\r\\n                              PoseTCN       4.090 4.091      4.105   4.119     4.233   4.257\\r\\n                              Pose + dPose 3.984 3.991       4.017   4.063     4.162   4.190\\r\\n                              G-embed        2.884   2.926   3.043   3.258     3.577   3.787\\r\\n      Autoregressive          L-embed        1.329   1.539   2.079   3.326     4.578   5.192\\r\\n                              Ours           1.150   1.361   1.834   2.689     3.789   4.526\\r\\n\\r\\n                      (b) Mean Squared Error of Volume Change \\xe2\\x86\\x93 on DFaust.\\r\\n\\r\\n                                                          Rollout (# of frames)\\r\\n                                              2         4           8         16         30\\r\\n                                        Interpolation Set\\r\\n                            SNARF [8]     0.01582 0.01552 0.01610 0.01658              0.01682\\r\\n                            Pose          0.01355 0.01305 0.01341 0.01367              0.01387\\r\\n     Non-Autoregressive\\r\\n                            PoseTCN       0.01364 0.01323 0.01350 0.01399              0.01416\\r\\n                            Pose + dPose 0.01288 0.01247 0.01273 0.01311               0.01321\\r\\n                            G-embed       0.01179 0.01168       0.01199    0.01248     0.01265\\r\\n     Autoregressive         L-embed       0.01003 0.01180       0.01466    0.01716     0.01844\\r\\n                            Ours          0.00902 0.01053       0.01258    0.01456     0.01565\\r\\n                                        Extrapolation Set\\r\\n                            SNARF [8]     0.01178 0.01194       0.01251    0.01228     0.01206\\r\\n                            Pose          0.01027 0.01039       0.01074    0.01052     0.01039\\r\\n     Non-Autoregressive\\r\\n                            PoseTCN       0.01020 0.01038       0.01064    0.01040     0.01029\\r\\n                            Pose + dPose 0.00992 0.01014        0.01048    0.01029     0.01013\\r\\n                            G-embed        0.00936   0.00959    0.00995    0.00996     0.00998\\r\\n     Autoregressive         L-embed        0.00648   0.00821    0.01100    0.01308     0.01402\\r\\n                            Ours           0.00567   0.00715    0.00915    0.01039     0.01107\\r\\n\\x0c\\n\\n                   AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling      11\\r\\n\\r\\n\\r\\n\\r\\n    SMPL\\r\\n    Pose\\r\\n    PoseTCN\\r\\n    Pose + dPose\\r\\n    Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 Time\\r\\n\\r\\n\\r\\nFig. 4: Qualitative Comparison with Non-Autoregressive Baselines. In contrast to the rigid\\r\\nresults in non-autoregressive baselines, our approach produces high quality non-rigid dynamics.\\r\\n\\r\\n 1. Pose: We only feed pose parameters of the next frame L(pt+1 ) in our architec-\\r\\n     ture. Prior avatar modeling methods based on neural fields employ this pose-only\\r\\n     parameterization [43,48,8].\\r\\n 2. PoseTCN: Temporal convolutional networks (TCN) [17] support the incorporation\\r\\n     of a long-range history for learning tasks, and have been used in several avatar\\r\\n     modeling methods [52,12]. Thus, we use a TCN that takes as input the sequence\\r\\n     of poses with the length of 16. We first compute localized pose parameters, as in\\r\\n     our method, for each frame and apply the TCN to obtain 64-dim features for each\\r\\n     SMPL vertex. The features are then fed into the UNet and SDF decoders identical\\r\\n     to our method.\\r\\n 3. Pose+dPose: Our approach without autoregressive components (Ht , {H\\xcc\\x87t+i }).\\r\\n    Tab. 1 shows that our approach outperforms the baseline methods for the first 8\\r\\nframes for interpolation, and first 16 frames for extrapolation. In particular, there is a\\r\\nsignificantly large margin for the first 4-8 frames, indicating that our method achieves\\r\\nthe most accurate prediction of the future frames given the ground-truth shape history.\\r\\nWe also observe that the non-autoregressive methods tend to collapse to predicting the\\r\\n\\xe2\\x80\\x9cmean\\xe2\\x80\\x9d shape under each pose without faithful dynamics for unseen motions (see Fig. 4\\r\\nand Supp. Mat. video). Since the accumulation of small errors in each frame may lead\\r\\nto large deviations from the ground-truth due to high sensitivity to initial conditions\\r\\nin dynamic systems [34], for longer rollouts mean predictions without any dynamics\\r\\ncan produce lower errors than more dynamic predictions. In fact, although our method\\r\\nleads to slightly higher errors on longer rollouts, Fig. 4 clearly shows that our approach\\r\\nproduces the most visually plausible dynamics on the AIST++ sequences. Importantly,\\r\\nwe do not observe any instability or explosions in our autoregressive model for longer\\r\\nrollouts, as can be seen from the error behavior shown in Fig. 7. We also highly encourage\\r\\n\\x0c\\n\\n12             Z. Bai et al.\\r\\n\\r\\n\\r\\n\\r\\n     SMPL\\r\\n     G-embed\\r\\n     L-embed\\r\\n     Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              Time\\r\\n\\r\\n\\r\\nFig. 5: Qualitative Comparison with Latent-space Autoregression. While the latent space-\\r\\nbased autoregression approaches suffer either from overfitting to pose parameters (G-embed)\\r\\nor instability (L-embed, see Supp. Mat. video), our approach based on a physically meaningful\\r\\nquantity (signed height) achieves the most stable and expressive synthesis of dynamics.\\r\\n\\r\\nreaders to see Supp. Mat. video for qualitative comparison in animation. In summary,\\r\\nour results confirm that autoregressive modeling plays a critical role for generalization\\r\\nto unseen motions and improving the realism of dynamics.\\r\\nExplicit Shape Encoding vs. Latent Encoding. Efficiently encoding the geometry of\\r\\nimplicit surfaces is non-trivial. While our proposed approach encodes the geometry via\\r\\nsigned heights on articulated observer points, prior approaches have demonstrated shape\\r\\nencoding based on a learned latent space [36,4]. Therefore, we also investigate different\\r\\nencoding methods for autoregressive modeling with the following 2 baselines:\\r\\n 1. G-embed: Inspired by DeepSDF [36], we first learn per-frame global embeddings\\r\\n    lg \\xe2\\x88\\x88 R512 with the UNet and SDF decoder by replacing Ht , {H\\xcc\\x87t+i }, {L(p\\xcc\\x87t+i )}\\r\\n    with repeated global embeddings. Then, we train a small MLP with three 512-dim\\r\\n    hidden layers using Softplus except for the last layer, taking as input pt+1 , {p\\xcc\\x87t+i },\\r\\n    and 3 embeddings of previous frames to predict the global embedding at time t + 1.\\r\\n 2. L-embed: For modeling mesh-based body avatars, localized embeddings are shown\\r\\n    to be effective [4]. Inspired by this, we also train a model with localized embeddings\\r\\n    ll \\xe2\\x88\\x88 R16\\xc3\\x9764\\xc3\\x9764 . We first learn per-frame local embeddings ll together with the UNet\\r\\n    and SDF decoder by replacing Ht , {H\\xcc\\x87t+i }, {L(p\\xcc\\x87t+i )} with bilinearly upsampled\\r\\n    ll . Then we train another UNet that takes as input L(pt+1 ), {L(p\\xcc\\x87t+i )}, and 3\\r\\n    embeddings of previous frames to predict the localized embeddings at time t + 1.\\r\\n    Note that for evaluation, we optimize per-frame embeddings for test sequences using\\r\\nEq. (3) such that the baseline methods can use the best possible history of embeddings\\r\\nfor autoregression. Tab. 1 shows that our method outperforms L-embed in all cases\\r\\nbecause L-embed becomes unstable for the test sequences. For G-embed, we observe the\\r\\nsame trend as for the non-autoregressive baselines: our approach achieves significantly\\r\\n\\x0c\\n\\n              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling              13\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 Jump to top\\r\\n                                                 Fall to bottom\\r\\n              Scan       SNARF       Ours                         Scan    SoftSMPL   Ours\\r\\n\\r\\n\\r\\nFig. 6: Qualitative Comparison with SoTA Methods. Our approach produces significantly more\\r\\nfaithful shapes and dynamics than the state-of-the-art implicit avatar modeling method [8], and\\r\\nshows comparable dynamics with prior art dependent on registrations with fixed topology [45].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n       (a) Scan-to-Prediction Distance \\xe2\\x86\\x93                           (b) MSE of Volume Change \\xe2\\x86\\x93\\r\\n\\r\\nFig. 7: Comparison with SoftSMPL [45]. We plot the errors on the sequence of\\r\\none leg loose for subject 50004. Surprisingly, our registration-free approach mostly out-\\r\\nperforms this baseline that has to rely on registered data with fixed topology.\\r\\n\\r\\nmore accurate predictions of the future frames given the ground-truth trajectories (see the\\r\\nerrors for 1-4 rollouts), and G-embed tends to predict \\xe2\\x80\\x9cmean\\xe2\\x80\\x9d shapes without plausible\\r\\ndynamics. The qualitative comparison in Fig. 5 confirms that our approach produces\\r\\nmore plausible dynamics. Please refer to Supp. Mat. video for detailed visual comparison\\r\\nin animation. We summarize that physically meaningful shape encodings (e.g., signed\\r\\nheights) enable more stable learning of dynamics via autoregression than methods relying\\r\\non latent space.\\r\\nComparison to SoTA Methods.. We compare our approach with state-of-the-art meth-\\r\\nods for both implicit surface representations and mesh-based representations. As a\\r\\nmethod using neural implicit surfaces, we choose SNARF [8], which jointly learns a\\r\\npose-conditioned implicit surface in a canonical T-pose and a forward skinning network\\r\\nfor reposing. Similar to ours, SNARF does not require temporal correspondences other\\r\\nthan the fitted SMPL models. We use the training code released by the authors using\\r\\nthe same training data and the fitted SMPL parameters as in our method. Note that in\\r\\nthe DFaust experiment in [8], SNARF is trained using only the fitted SMPL models\\r\\nto DFaust as ground-truth geometry, which do not contain any dynamic deformations.\\r\\n\\x0c\\n\\n14      Z. Bai et al.\\r\\n\\r\\nTab. 1 shows that our approach significantly outperforms SNARF for any number of\\r\\nrollouts. Interestingly, SNARF can produce dynamic effects for training data by severely\\r\\noverfitting to the pose parameters, but this does not generalize to unseen poses as the\\r\\nlearned dynamics is the results of spurious correlations. As mentioned in Sec. 3.3, we\\r\\nalso observe that the performance of SNARF heavily relies on the accuracy of the\\r\\nSMPL fitting for canonicalization, and any small alignment errors in the underlying\\r\\nSMPL registration deteriorates their test-time performance (see Fig. 6). Therefore, this\\r\\nexperiment demonstrates not only the importance of autoregressive dynamic avatar\\r\\nmodeling, but also the efficacy of our articulation-aware shape decoding approach given\\r\\nthe quality of available SMPL fitting for real-world scans.\\r\\n    We also compare against SoftSMPL [45], a state-of-the-art mesh-based method that\\r\\nlearns dynamically deforming human bodies from registered meshes. The authors of\\r\\nSoftSMPL kindly provide their predictions on the sequence of one leg loose for\\r\\nsubject 50004, which is excluded from training for both our method and SoftSMPL for\\r\\nfair comparison. To our surprise, Fig. 7 show that our results are slightly better on both\\r\\nmetrics for the majority of frames, although we tackle a significantly harder problem\\r\\nbecause our approach learns dynamic bodies directly from raw scans, whereas SoftSMPL\\r\\nlearns from the carefully registered data. We speculate that the lower error may be mainly\\r\\nattributed to the higher resolution of our geometry using implicit surfaces in contrast\\r\\nto their predictions on the coarse SMPL topology (see Fig. 6). Nevertheless, this result\\r\\nis highly encouraging as our approach achieves comparable performance on dynamics\\r\\nmodeling without having to rely on surface registration.\\r\\n\\r\\n\\r\\n5    Conclusion\\r\\nWe have introduced AutoAvatar, an autoregressive approach for modeling high-fidelity\\r\\ndynamic deformations of human bodies directly from raw 4D scans using neural\\r\\nimplicit surfaces. The reconstructed avatars can be driven by pose parameters, and\\r\\nautomatically incorporate secondary dynamic effects that depend on the history of\\r\\nshapes. Our experiments indicate that modeling dynamic avatars without relying on\\r\\naccurate registrations is made possible by choosing an efficient representation for our\\r\\nautoregressive model.\\r\\nLimitations and Future Work. While our method has shown to be effective in modeling\\r\\nthe elastic deformations of real humans, we observe that it remains challenging, yet\\r\\npromising, to model clothing deformations that involve high-frequency wrinkles (see\\r\\nAppendix C for details). Our evaluation also suggests that ground-truth comparison\\r\\nwith longer rollouts may not reliably reflect the plausibility of dynamics. Quantitative\\r\\nmetrics that handle the high sensitivity to initial conditions in dynamics could be further\\r\\ninvestigated. Currently, AutoAvatar models subject-specific dynamic human bodies,\\r\\nbut generalizing it to multiple identities, as demonstrated in registration-based shape\\r\\nmodeling [39,22,45], is an interesting direction for future work. The most exciting venue\\r\\nfor future work is to extend the notion of dynamics to image-based avatars [37,20]. In\\r\\ncontrast to implicit surfaces, neural radiance fields [31] do not have an explicit \\xe2\\x80\\x9csurface\\xe2\\x80\\x9d\\r\\nas they model geometry using density fields. While this remains an open question, we\\r\\nbelieve that our contributions in this work such as efficiently modeling the state of shapes\\r\\nvia articulated observer points might be useful to unlock this application.\\r\\n\\x0c\\n\\n              AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling             15\\r\\n\\r\\nReferences\\r\\n 1. Alldieck, T., Xu, H., Sminchisescu, C.: imghum: Implicit generative models of 3d human\\r\\n    shape and articulated pose. In: Proc. of International Conference on Computer Vision (ICCV).\\r\\n    pp. 5461\\xe2\\x80\\x935470 (2021) 3\\r\\n 2. Allen, B., Curless, B., Popovic\\xcc\\x81, Z., Hertzmann, A.: Learning a correlated model of identity\\r\\n    and pose-dependent body shape variation for real-time synthesis. In: Proceedings of the 2006\\r\\n    ACM SIGGRAPH/Eurographics symposium on Computer animation. pp. 147\\xe2\\x80\\x93156 (2006) 3\\r\\n 3. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: SCAPE: shape\\r\\n    completion and animation of people. ACM Trans. on Graphics (TOG) 24(3), 408\\xe2\\x80\\x93416 (2005)\\r\\n    3\\r\\n 4. Bagautdinov, T., Wu, C., Simon, T., Prada, F., Shiratori, T., Wei, S.E., Xu, W., Sheikh, Y.,\\r\\n    Saragih, J.: Driving-signal aware full-body avatars. ACM Trans. on Graphics (TOG) 40(4),\\r\\n    1\\xe2\\x80\\x9317 (2021) 1, 6, 12\\r\\n 5. Bhat, K.S., Twigg, C.D., Hodgins, J.K., Khosla, P., Popovic, Z., Seitz, S.M.: Estimating cloth\\r\\n    simulation parameters from video (2003) 4\\r\\n 6. Bogo, F., Romero, J., Loper, M., Black, M.J.: Faust: Dataset and evaluation for 3d mesh\\r\\n    registration. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 3794\\xe2\\x80\\x933801\\r\\n    (2014) 3\\r\\n 7. Bogo, F., Romero, J., Pons-Moll, G., Black, M.J.: Dynamic faust: Registering human bodies\\r\\n    in motion. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 6233\\xe2\\x80\\x936242\\r\\n    (2017) 3, 8, 19\\r\\n 8. Chen, X., Zheng, Y., Black, M.J., Hilliges, O., Geiger, A.: Snarf: Differentiable forward\\r\\n    skinning for animating non-rigid neural implicit shapes. In: Proc. of International Conference\\r\\n    on Computer Vision (ICCV) (2021) 1, 3, 7, 10, 11, 13, 19, 20\\r\\n 9. Chen, Z., Zhang, H.: Learning implicit fields for generative shape modeling. In: Proc.\\r\\n    of Computer Vision and Pattern Recognition (CVPR). pp. 5939\\xe2\\x80\\x935948. Computer Vision\\r\\n    Foundation / IEEE (2019) 2\\r\\n10. Deng, B., Lewis, J.P., Jeruzalski, T., Pons-Moll, G., Hinton, G.E., Norouzi, M., Tagliasacchi,\\r\\n    A.: NASA neural articulated shape approximation. In: Proc. of European Conference on\\r\\n    Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12352, pp. 612\\xe2\\x80\\x93628.\\r\\n    Springer (2020) 3\\r\\n11. Gropp, A., Yariv, L., Haim, N., Atzmon, M., Lipman, Y.: Implicit geometric regularization for\\r\\n    learning shapes. In: Proceedings of the 37th International Conference on Machine Learning\\r\\n    (ICML). Proceedings of Machine Learning Research, vol. 119, pp. 3789\\xe2\\x80\\x933799. PMLR (2020)\\r\\n    7\\r\\n12. Habermann, M., Liu, L., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.: Real-time deep\\r\\n    dynamic characters. ACM Trans. on Graphics (TOG) 40(4), 1\\xe2\\x80\\x9316 (2021) 3, 9, 11\\r\\n13. Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., Seidel, H.: A statistical model of human\\r\\n    pose and body shape. Computer Graphics Forum 28(2), 337\\xe2\\x80\\x93346 (2009) 3\\r\\n14. Holden, D., Duong, B.C., Datta, S., Nowrouzezahrai, D.: Subspace neural physics:\\r\\n    Fast data-driven interactive simulation. In: Proceedings of the 18th annual ACM SIG-\\r\\n    GRAPH/Eurographics Symposium on Computer Animation. pp. 1\\xe2\\x80\\x9312 (2019) 4\\r\\n15. Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3d human dynamics from video. In:\\r\\n    Proc. of Computer Vision and Pattern Recognition (CVPR) (2019) 2\\r\\n16. Kim, M., Pons-Moll, G., Pujades, S., Bang, S., Kim, J., Black, M.J., Lee, S.: Data-driven\\r\\n    physics for human soft tissue animation. ACM Trans. on Graphics (TOG) 36(4), 54:1\\xe2\\x80\\x9354:12\\r\\n    (2017) 3\\r\\n17. Lea, C., Vidal, R., Reiter, A., Hager, G.D.: Temporal convolutional networks: A unified\\r\\n    approach to action segmentation. In: Proc. of European Conference on Computer Vision\\r\\n    (ECCV). pp. 47\\xe2\\x80\\x9354. Springer (2016) 11\\r\\n\\x0c\\n\\n16       Z. Bai et al.\\r\\n\\r\\n18. Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance\\r\\n    generation with aist++. In: Proc. of International Conference on Computer Vision (ICCV). pp.\\r\\n    13401\\xe2\\x80\\x9313412 (2021) 8\\r\\n19. Ling, H.Y., Zinno, F., Cheng, G., Van De Panne, M.: Character controllers using motion vaes.\\r\\n    ACM Trans. on Graphics (TOG) 39(4), 40\\xe2\\x80\\x931 (2020) 2, 3\\r\\n20. Liu, L., Habermann, M., Rudnev, V., Sarkar, K., Gu, J., Theobalt, C.: Neural actor: Neural\\r\\n    free-view synthesis of human actors with pose control. ACM Trans. on Graphics (TOG) 40(6),\\r\\n    1\\xe2\\x80\\x9316 (2021) 3, 7, 14, 19\\r\\n21. Loper, M., Mahmood, N., Black, M.J.: Mosh: Motion and shape capture from sparse markers.\\r\\n    ACM Trans. on Graphics (TOG) 33(6), 1\\xe2\\x80\\x9313 (2014) 9, 19\\r\\n22. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-\\r\\n    person linear model. ACM Trans. on Graphics (TOG) 34(6), 248:1\\xe2\\x80\\x93248:16 (2015) 2, 3, 5, 6,\\r\\n    14\\r\\n23. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface construction\\r\\n    algorithm. ACM siggraph computer graphics 21(4), 163\\xe2\\x80\\x93169 (1987) 5, 9\\r\\n24. Ma, Q., Saito, S., Yang, J., Tang, S., Black, M.J.: SCALE: Modeling clothed humans with\\r\\n    a surface codec of articulated local elements. In: Proc. of Computer Vision and Pattern\\r\\n    Recognition (CVPR) (Jun 2021) 1, 3\\r\\n25. Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., Black, M.J.: Learning to\\r\\n    dress 3d people in generative clothing. In: Proc. of Computer Vision and Pattern Recognition\\r\\n    (CVPR). pp. 6469\\xe2\\x80\\x936478 (2020) 19\\r\\n26. Ma, Q., Yang, J., Tang, S., Black, M.J.: The power of points for modeling humans in clothing.\\r\\n    In: Proc. of International Conference on Computer Vision (ICCV) (Oct 2021) 1, 3\\r\\n27. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of\\r\\n    motion capture as surface shapes. In: Proc. of International Conference on Computer Vision\\r\\n    (ICCV). pp. 5442\\xe2\\x80\\x935451 (2019) 9, 19, 20\\r\\n28. Martinez, J., Black, M.J., Romero, J.: On human motion prediction using recurrent neural\\r\\n    networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 2891\\xe2\\x80\\x932900\\r\\n    (2017) 2\\r\\n29. Mescheder, L.M., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\\r\\n    Learning 3D reconstruction in function space. In: Proc. of Computer Vision and Pattern\\r\\n    Recognition (CVPR). pp. 4460\\xe2\\x80\\x934470. Computer Vision Foundation / IEEE (2019) 2\\r\\n30. Mihajlovic, M., Zhang, Y., Black, M.J., Tang, S.: LEAP: Learning articulated occupancy of\\r\\n    people. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun 2021) 3, 7\\r\\n31. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF:\\r\\n    Representing scenes as neural radiance fields for view synthesis. In: Proc. of European\\r\\n    Conference on Computer Vision (ECCV). Lecture Notes in Computer Science, vol. 12346,\\r\\n    pp. 405\\xe2\\x80\\x93421. Springer (2020) 3, 14\\r\\n32. Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: Differentiable volumetric rendering:\\r\\n    Learning implicit 3d representations without 3d supervision. In: Proc. of Computer Vision\\r\\n    and Pattern Recognition (CVPR) (2020) 6\\r\\n33. Osman, A.A.A., Bolkart, T., Black, M.J.: STAR: sparse trained articulated human body\\r\\n    regressor. In: Proc. of European Conference on Computer Vision (ECCV). Lecture Notes in\\r\\n    Computer Science, vol. 12351, pp. 598\\xe2\\x80\\x93613. Springer (2020) 3\\r\\n34. Ott, E., Grebogi, C., Yorke, J.A.: Controlling chaos. Physical review letters 64(11), 1196\\r\\n    (1990) 10, 11\\r\\n35. Palafox, P., Boz\\xcc\\x8cic\\xcc\\x8c, A., Thies, J., Nie\\xc3\\x9fner, M., Dai, A.: Npms: Neural parametric models for\\r\\n    3d deformable shapes. In: Proc. of International Conference on Computer Vision (ICCV). pp.\\r\\n    12695\\xe2\\x80\\x9312705 (2021) 3\\r\\n\\x0c\\n\\n               AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling               17\\r\\n\\r\\n36. Park, J.J., Florence, P., Straub, J., Newcombe, R.A., Lovegrove, S.: DeepSDF: Learning\\r\\n    continuous signed distance functions for shape representation. In: Proc. of Computer Vision\\r\\n    and Pattern Recognition (CVPR). pp. 165\\xe2\\x80\\x93174. Computer Vision Foundation / IEEE (2019)\\r\\n    2, 12\\r\\n37. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit\\r\\n    neural representations with structured latent codes for novel view synthesis of dynamic\\r\\n    humans. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 9054\\xe2\\x80\\x939063\\r\\n    (2021) 3, 14\\r\\n38. Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., Battaglia, P.: Learning mesh-based simulation\\r\\n    with graph networks. In: International Conference on Learning Representations (2021) 4, 9\\r\\n39. Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J.: Dyna: A model of dynamic human\\r\\n    shape in motion. ACM Trans. on Graphics (TOG) 34(4), 1\\xe2\\x80\\x9314 (2015) 2, 3, 9, 14\\r\\n40. Prokudin, S., Lassner, C., Romero, J.: Efficient learning on point clouds with basis point sets.\\r\\n    In: Proc. of International Conference on Computer Vision (ICCV). pp. 4332\\xe2\\x80\\x934341 (2019) 5\\r\\n41. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d\\r\\n    classification and segmentation. In: Proc. of Computer Vision and Pattern Recognition (CVPR).\\r\\n    pp. 652\\xe2\\x80\\x93660 (2017) 7\\r\\n42. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image\\r\\n    segmentation. In: International Conference on Medical image computing and computer-\\r\\n    assisted intervention. pp. 234\\xe2\\x80\\x93241. Springer (2015) 7\\r\\n43. Saito, S., Yang, J., Ma, Q., Black, M.J.: SCANimate: Weakly supervised learning of skinned\\r\\n    clothed avatar networks. In: Proc. of Computer Vision and Pattern Recognition (CVPR) (Jun\\r\\n    2021) 1, 3, 6, 7, 11\\r\\n44. Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., Battaglia, P.: Learning\\r\\n    to simulate complex physics with graph networks. In: International Conference on Machine\\r\\n    Learning. pp. 8459\\xe2\\x80\\x938468. PMLR (2020) 4\\r\\n45. Santesteban, I., Garces, E., Otaduy, M.A., Casas, D.: Softsmpl: Data-driven modeling of\\r\\n    nonlinear soft-tissue dynamics for parametric humans. In: Computer Graphics Forum. vol. 39,\\r\\n    pp. 65\\xe2\\x80\\x9375. Wiley Online Library (2020) 2, 3, 13, 14\\r\\n46. Sifakis, E., Barbic, J.: Fem simulation of 3d deformable solids: a practitioner\\xe2\\x80\\x99s guide to theory,\\r\\n    discretization and model reduction. In: Acm siggraph 2012 courses, pp. 1\\xe2\\x80\\x9350 (2012) 4\\r\\n47. Srinivasan, S.G., Wang, Q., Rojas, J., Kla\\xcc\\x81r, G., Kavan, L., Sifakis, E.: Learning active\\r\\n    quasistatic physics-based models from data. ACM Trans. on Graphics (TOG) 40(4), 1\\xe2\\x80\\x9314\\r\\n    (2021) 4\\r\\n48. Tiwari, G., Sarafianos, N., Tung, T., Pons-Moll, G.: Neural-gif: Neural generalized implicit\\r\\n    functions for animating people in clothing. In: Proc. of International Conference on Computer\\r\\n    Vision (ICCV). pp. 11708\\xe2\\x80\\x9311718 (2021) 1, 3, 7, 11\\r\\n49. Tsuchida, S., Fukayama, S., Hamasaki, M., Goto, M.: Aist dance video database: Multi-genre,\\r\\n    multi-dancer, and multi-camera database for dance information processing. In: ISMIR. vol. 1,\\r\\n    p. 6 (2019) 8\\r\\n50. Wang, H., O\\xe2\\x80\\x99Brien, J.F., Ramamoorthi, R.: Data-driven elastic models for cloth: modeling\\r\\n    and measurement. ACM Trans. on Graphics (TOG) 30(4), 1\\xe2\\x80\\x9312 (2011) 4\\r\\n51. Wang, S., Mihajlovic, M., Ma, Q., Geiger, A., Tang, S.: Metaavatar: Learning animatable\\r\\n    clothed human models from few depth images. Proc. of Advances in Neural Information\\r\\n    Processing Systems (NeurIPS) 34 (2021) 3\\r\\n52. Xiang, D., Prada, F., Bagautdinov, T., Xu, W., Dong, Y., Wen, H., Hodgins, J., Wu, C.:\\r\\n    Modeling clothing as a separate layer for an animatable human avatar. ACM Trans. on\\r\\n    Graphics (TOG) 40(6), 1\\xe2\\x80\\x9315 (2021) 3, 9, 11\\r\\n53. Xie, Y., Takikawa, T., Saito, S., Litany, O., Yan, S., Khan, N., Tombari, F., Tompkin, J.,\\r\\n    Sitzmann, V., Sridhar, S.: Neural fields in visual computing and beyond. arXiv preprint\\r\\n    arXiv:2111.11426 (2021) 3\\r\\n\\x0c\\n\\n18       Z. Bai et al.\\r\\n\\r\\n54. Xu, H., Bazavan, E.G., Zanfir, A., Freeman, W.T., Sukthankar, R., Sminchisescu, C.: GHUM\\r\\n    & GHUML: generative 3D human shape and articulated pose models. In: Proc. of Computer\\r\\n    Vision and Pattern Recognition (CVPR). pp. 6183\\xe2\\x80\\x936192. IEEE (2020) 3\\r\\n55. Yang, S., Liang, J., Lin, M.C.: Learning-based cloth material recovery from video. In: Proc.\\r\\n    of International Conference on Computer Vision (ICCV). pp. 4393\\xe2\\x80\\x934403. IEEE Computer\\r\\n    Society (2017) 4\\r\\n56. Zakharkin, I., Mazur, K., Grigorev, A., Lempitsky, V.: Point-based modeling of human\\r\\n    clothing. In: Proc. of International Conference on Computer Vision (ICCV). pp. 14718\\xe2\\x80\\x9314727\\r\\n    (2021) 3\\r\\n57. Zheng, M., Zhou, Y., Ceylan, D., Barbic, J.: A deep emulator for secondary motion of 3d\\r\\n    characters. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 5932\\xe2\\x80\\x935940\\r\\n    (2021) 4, 9\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling      19\\r\\n\\r\\nAppendix\\r\\n\\r\\nA    Analysis on Input Pose Accuracy\\r\\nWe investigate how the accuracy of the input SMPL fitting influences the results on\\r\\nsubject 50002 of DFaust [7]. As discussed in Sec. 4.1, the fitted SMPL parameters in\\r\\nDFaust are provided by the AMASS [27] dataset that uses sparse points on the registered\\r\\ndata as approximated motion capture marker locations and computes the parameters\\r\\nusing MoSh [21]. We observe that the provided pose parameters sometimes exhibit small\\r\\nmisalignment with respect to the input scans. While the fitting quality in the AMASS\\r\\ndataset is sufficient for our approach, we also evaluate the performance on more accurate\\r\\npose parameters by using all the vertices on the registered meshes. More specifically, we\\r\\nfirst compute a better template by unposing the registered meshes in the first frame of\\r\\neach sequence using the LBS skinning weights of the SMPL template, and averaging\\r\\nover all the sequences. Using this new template, we optimize pose parameters for each\\r\\nframe with an L2-loss on all the registered vertices. Note that in this experiment, we use\\r\\nthe original template with the refined pose parameters instead of the refined template in\\r\\norder not to unfairly favor our method over SNARF [8].\\r\\n     In Tab. 1, we report the mean absolute error of scan-to-prediction distance (mm) and\\r\\nthe mean squared error of volume change for our method and SNARF. Tab. 1 shows that\\r\\nSNARF has a large error reduction with refined poses, indicating that SNARF is highly\\r\\nsensitive to the accuracy of the SMPL fit. We also observe that after pose refinement,\\r\\nSNARF overfits more to training poses (e.g., interpolation) as SNARF cannot model\\r\\nhistory-dependent dynamic deformations. In contrast, our method is more robust to the\\r\\nfitting errors, and significantly outperforms SNARF in most settings except for 16-30\\r\\nrollouts in the interpolation set. Note that the results with longer rollouts favor \\xe2\\x80\\x9cmean\\xe2\\x80\\x9d\\r\\npredictions over more dynamic predictions, and do not inform us of the plausibility of\\r\\nthe synthesized dynamics (see the discussion in Sec. 4.2).\\r\\n\\r\\n\\r\\nB    k-NN vs. Closest Surface Projection\\r\\nAs discussed in Sec. 3.3, our SDF decoding approach uses k-nearest neighbors (k-NN)\\r\\nof the SMPL vertices instead of closest surface projection [20]. Fig. H illustrates the\\r\\nlimitation of this alternative approach proposed in Neural Actor [20]. As shown in Fig. H,\\r\\nwe observe that associating a query location with a single closest point on the surface\\r\\nleads to poor generalization to unseen poses around regions with multiple body parts in\\r\\nclose proximity (e.g. around armpits). In contrast, our approach, which associates query\\r\\npoints with multiple k-NN vertices, produces more plausible surface geometry even for\\r\\nunseen poses.\\r\\n\\r\\n\\r\\nC    Limitation: Clothing Deformations\\r\\nWe also apply our method on the CAPE [25] dataset that contains 4D scans of clothed\\r\\nhumans. We select the subject 03375 longlong, which exhibits the most visible\\r\\n\\x0c\\n\\n20       Z. Bai et al.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTable B: Quantitative Evaluation on Input Pose Accuracy on Subject 50002. We show the\\r\\nresults of our approach and SNARF [8] using the poses provided by the AMASS [27] dataset\\r\\nand the ones after refinement using all vertices in the registered meshes. While SNARF is greatly\\r\\ninfluenced by the accuracy of pose parameters, the slight improvement in our method illustrates its\\r\\nrobustness to SMPL fitting errors. In addition, our approach significantly outperforms SNARF\\r\\neven after pose refinement in most settings except for the 16-30 rollouts in the interpolation set.\\r\\n\\r\\n                         (a) Mean Scan-to-Prediction Distance (mm) \\xe2\\x86\\x93\\r\\n\\r\\n                                                 Rollout (# of frames)\\r\\n                                       1       2      4         8      16            30\\r\\n                                    Interpolation Set\\r\\n                          SNARF [8] 7.898 7.715 7.588 7.840 7.898                  8.238\\r\\n         AMASS [27]\\r\\n                          Ours       1.731 2.127 2.953 4.325 5.606                 6.455\\r\\n                          SNARF [8]  3.982 4.001 3.964            4.068    4.029   4.158\\r\\n         Refined Poses\\r\\n                          Ours       1.417 1.703 2.259            3.241    4.044   4.601\\r\\n                                    Extrapolation Set\\r\\n                          SNARF [8] 8.083 8.126 8.160             8.246    8.050   8.025\\r\\n         AMASS [27]\\r\\n                          Ours       1.259 1.479 1.984            2.883    4.023   4.867\\r\\n                          SNARF [8]      4.624   4.632    4.672   4.749    4.548   4.447\\r\\n         Refined Poses\\r\\n                          Ours           1.149   1.329    1.745   2.486    3.313   3.855\\r\\n\\r\\n                          (b) Mean Squared Error of Volume Change \\xe2\\x86\\x93\\r\\n\\r\\n                                                      Rollout (# of frames)\\r\\n                                           2         4          8         16          30\\r\\n                                         Interpolation Set\\r\\n                         SNARF [8]     0.01623 0.01590 0.01688 0.01703             0.01829\\r\\n       AMASS [27]\\r\\n                         Ours          0.00990 0.01135 0.01417 0.01597             0.01815\\r\\n                         SNARF [8]     0.01401 0.01349       0.01430    0.01426    0.01524\\r\\n       Refined Poses\\r\\n                         Ours          0.00849 0.01002       0.01248    0.01389    0.01558\\r\\n                                        Extrapolation Set\\r\\n                         SNARF [8]     0.01228 0.01244       0.01333    0.01292    0.01264\\r\\n       AMASS [27]\\r\\n                         Ours          0.00602 0.00756       0.00977    0.01082    0.01140\\r\\n                         SNARF [8]     0.01094    0.01092    0.01148    0.01099    0.01080\\r\\n       Refined Poses\\r\\n                         Ours          0.00559    0.00691    0.00871    0.00953    0.01000\\r\\n\\x0c\\n\\n             AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling            21\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    Scan         Projection        Ours                 Scan         Projection       Ours\\r\\n\\r\\nFig. H: k-NN vs. Closest Surface Projection. While the closest surface projection suffers from\\r\\nartifacts around armpits, our SDF decoding based on k-NN produces more plausible surface\\r\\ngeometry for unseen poses.\\r\\n\\r\\n\\r\\n\\r\\ndynamic deformations for clothing. We exclude 6 sequences (athletics, frisbee,\\r\\nvolleyball, box trial1, swim trial1, twist tilt trial1) from train-\\r\\ning, and use them for testing. We employ as input the template and SMPL poses provided\\r\\nby the CAPE dataset for training our model. Note that we approximate raw scans by\\r\\nsampling point clouds with surface normals computed on the registered meshes as the\\r\\nCAPE dataset only provides registered meshes for 03375 longlong.\\r\\n    Please refer to the supplementary video for qualitative results. While our approach\\r\\nproduces plausible short-term clothing deformations, it remains challenging to model\\r\\ndynamically deforming clothing with longer rollouts. Compared to soft-tissue deforma-\\r\\ntions, dynamics on clothed humans involve high-frequency deformations and topology\\r\\nchange, making the learning of clothing dynamics more difficult. We leave this for future\\r\\nwork.\\r\\n\\x0c', b'              Versatile Multi-Modal Pre-Training for Human-Centric Perception\\r\\n\\r\\n                  Fangzhou Hong1 , Liang Pan1 , Zhongang Cai1,2,3 , Ziwei Liu1\\r\\n     1\\r\\n       S-Lab, Nanyang Technological University 2 SenseTime Research 3 Shanghai AI Laboratory\\r\\n                 {fangzhou001, liang.pan, ziwei.liu}@ntu.edu.sg                                       caizhongang@sensetime.com\\r\\n                                                             IN Pre-train          Ours        IN Pre-train      Ours        IN Pre-train      Ours        IN-Pretrain       Ours\\r\\n                                                        56                     54.5       80                            55                   52.6     90\\r\\n                                                                                                               74.2                                   80\\r\\n                                                                                                                                                               76.3\\r\\n                                                        54                                75\\r\\n                                                                                                                        50              46.8\\r\\n                                                                 51.5                                                                                 70                 62.4\\r\\n                                                        52                                70\\r\\n                                     1)         3)                          49.1          65\\r\\n                                                                                                                        45                            60   54.3\\r\\n                                                        50                                         60.9   60.8                                        50\\r\\n                                                        48                                60                            40\\r\\n                                                                                                                                 35.0                 40\\r\\n  Dense Representations                                 46   44.3                         55\\r\\n                                                                                                                        35                            30\\r\\n                                                                                               48.9\\r\\n                                                        44                                50\\r\\n                                                                                                                             28.6                     20              14.2\\r\\n                                                                                                                        30\\r\\n                                                        42                                45                                                          10\\r\\n                                                        40                                40                            25                             0\\r\\n                                                              GPS AP        GPSM AP              mIoU         mAcc             mIoU         mAcc           0.5% Acc 0.1% Acc\\r\\n                                     2)         4)      1) DensePose Estimation           2) RGB Human Parsing          3) Depth Human Parsing        4) Depth 3D Pose Est.\\r\\n  Sparse Representations\\r\\n                           a) System Overview                                                  b) Performance on Downstream Tasks\\r\\nFigure 1. An Overview of HCMoCo. a) We present HCMoCo, a versatile multi-modal pre-training framework that takes multi-modal\\r\\nobservations of human body as input for human-centric perception. The pre-train models can be transferred to various human-centric\\r\\ndownstream tasks with different modalities. b) Our HCMoCo shows superior performance on all four downstream tasks, especially for\\r\\ndata-efficient settings (10% DensePose, 20% RGB/depth human parsing, 0.5/0.1% 3D pose estimation). \\xe2\\x80\\x98IN\\xe2\\x80\\x99 stands for ImageNet.\\r\\n                                                                              are available at https://github.com/hongfz16/\\r\\n                                  Abstract                                    HCMoCo.\\r\\n\\r\\n   Human-centric perception plays a vital role in vision and                  1. Introduction\\r\\ngraphics. But their data annotations are prohibitively ex-\\r\\npensive. Therefore, it is desirable to have a versatile pre-                      As a long-standing problem, human-centric perception\\r\\ntrain model that serves as a foundation for data-efficient                    has been studied for decades, ranging from sparse predic-\\r\\ndownstream tasks transfer. To this end, we propose the                        tion tasks, such as human action recognition [8, 27, 42, 50],\\r\\nHuman-Centric Multi-Modal Contrastive Learning frame-                         2D keypoints detection [2, 26, 43, 48] and 3D pose estima-\\r\\nwork HCMoCo that leverages the multi-modal nature of hu-                      tion [22, 31, 40], to dense prediction tasks, such as human\\r\\nman data (e.g. RGB, depth, 2D keypoints) for effective rep-                   parsing [7, 11, 12, 25] and DensePose prediction [14]. Un-\\r\\nresentation learning. The objective comes with two main                       fortunately, to train a model with reasonable generalizabil-\\r\\nchallenges: dense pre-train for multi-modality data, effi-                    ity and robustness, an enormous amount of labeled real data\\r\\ncient usage of sparse human priors. To tackle the chal-                       is necessary, which is extremely expensive to collect and an-\\r\\nlenges, we design the novel Dense Intra-sample Contrastive                    notate. Therefore, it is desirable to have a versatile pre-train\\r\\nLearning and Sparse Structure-aware Contrastive Learning                      model that can serve as a foundation for all the aforemen-\\r\\ntargets by hierarchically learning a modal-invariant latent                   tioned human-centric perception tasks.\\r\\nspace featured with continuous and ordinal feature distri-                        With the development of sensors, the human body can\\r\\nbution and structure-aware semantic consistency. HCMoCo                       be more conveniently perceived and represented in multi-\\r\\nprovides pre-train for different modalities by combining het-                 ple modalities, such as RGB, depth, and infrared. In this\\r\\nerogeneous datasets, which allows efficient usage of exist-                   work, we argue that the multi-modality nature of human-\\r\\ning task-specific human data. Extensive experiments on four                   centric data can induce effective representations that trans-\\r\\ndownstream tasks of different modalities demonstrate the ef-                  fer well to various downstream tasks, due to three major\\r\\nfectiveness of HCMoCo, especially under data-efficient set-                   advantages: 1) Learning a modal-invariant latent space\\r\\ntings (7.16% and 12% improvement on DensePose Estima-                         through pre-training helps efficient task-relevant mutual in-\\r\\ntion and Human Parsing). Moreover, we demonstrate the                         formation extraction. 2) A single versatile pre-train model\\r\\nversatility of HCMoCo by exploring cross-modality super-                      on multi-modal data facilitates multiple downstream tasks\\r\\nvision and missing-modality inference, validating its strong                  using various modalities. 3) Our multi-modal pre-train set-\\r\\nability in cross-modal association and reasoning. Codes                       ting bridges heterogeneous human-centric datasets through\\r\\n                                                                              their common modality, which benefits the generalizability\\r\\n       Corresponding author                                                   of pre-train models.\\r\\n\\r\\n\\r\\n                                                                        1\\r\\n\\x0c\\n\\n    We mainly explore two groups of modalities as shown in             timation (RGB) [14], human parsing using RGB [22] or\\r\\nFig. 1 a): dense representations (e.g. RGB, depth, infrared)           depth frames, and 3D pose estimation (depth) [16]. Un-\\r\\nand sparse representations (e.g. 2D keypoints, 3D pose).               der full set and data-efficient training settings, HCMoCo\\r\\nDense representations can provide rich texture and/or 3D               constantly achieves better performance than training from\\r\\ngeometry information. But they are mostly low-level and                scratch or pre-train on ImageNet. To name a few, as shown\\r\\nnoisy. On the contrary, sparse representations obtained by             in Fig. 1 b), we achieve 7.16% improvement in terms\\r\\noff-the-shelf tools [4, 9] are semantic and structured. But            of GPS AP on 10% training data of DensePose estima-\\r\\nthe sparsity results in insufficient details. We highlight that        tion; 12% improvement in terms of mIoU on 20% training\\r\\nit is non-trivial to integrate these heterogeneous modalities          data of Human3.6M human parsing. Moreover, we eval-\\r\\ninto a unified pre-training framework for the following two            uate the modal-invariance of the latent space learned by\\r\\nmain challenges: 1) learning representations suitable for              HCMoCo for dense prediction on NTURGBD-Parsing-4K\\r\\ndense prediction tasks in the multi-modality setting; 2) us-           with two settings: cross-modality supervision and missing-\\r\\ning weak priors from sparse representations effectively for            modality inference. Compared against conventional con-\\r\\npre-training.                                                          trastive learning targets, our method improves the segmen-\\r\\n    Challenge 1: Dense Targets. Existing methods [21, 30]              tation mIoU by 29% and 24% for the two settings, respec-\\r\\nperform contrastive learning densely on pixel-level features           tively. To the best of our knowledge, we are the first to study\\r\\nto achieve view-invariance for dense prediction tasks. How-            multi-modal pre-training for human-centric perception.\\r\\never, those methods require multiple views of a static 3D                  The main contributions are summarized below: 1) As\\r\\nscene [10], which is inapplicable for human-centric appli-             the first endeavor, we provide an in-depth analysis for\\r\\ncations with only single view. Furthermore, it is preferable           human-centric pre-training, which is formulated as a chal-\\r\\nto learn representations that are continuously and orderly             lenging multi-modal contrastive learning problem. 2) To-\\r\\ndistributed over the human body. In light of this, we gener-           gether with the novel hierarchical contrastive learning ob-\\r\\nalize the widely used InfoNCE [33] and propose a dense                 jectives, a comprehensive framework HCMoCo is pro-\\r\\nintra-sample contrastive learning objective that applies a             posed for effective pre-training for human-centric tasks. 3)\\r\\nsoft pixel-level contrastive target, which can facilitate learn-       Through extensive experiments, HCMoCo achieves supe-\\r\\ning ordinal and continuous dense feature distributions.                rior performance than existing methods, and meanwhile\\r\\n    Challenge 2: Sparse Priors. To employ priors in con-               shows promising modal-invariance properties. 4) To bene-\\r\\ntrastive learning, previous works [3, 23, 46] mainly use the           fit multi-modal human-centric perception, we contribute an\\r\\nsupervision to generate semantically positive pairs. How-              RGB-D human parsing dataset, NTURGBD-Parsing-4K.\\r\\never, these methods only focus on the sample-level con-\\r\\ntrastive learning, which means each sample is encoded to a             2. Related Work\\r\\nglobal embedding. It is not optimal for human dense predic-            Human-Centric Perception. Many efforts have been put\\r\\ntion tasks. To this end, we propose a sparse structure-aware           into human-centric perception in decades. Lots of work in\\r\\ncontrastive learning target, which uses semantic correspon-            2D keypoint detection [2,26,43,48] has achieved robust and\\r\\ndences across samples as positive pairs to complement pos-             accurate performance. 3D pose estimation has long been a\\r\\nitive intra-sample pairs. Particularly, leveraging sparse hu-          challenging problem and is approached from two aspects,\\r\\nman priors leads to an embedding space where semantically              lifting from 2D keypoints [22, 31, 40] and predicting from\\r\\ncorresponding parts are aligned more closely.                          depth map [16, 49]. Human parsing can be defined in two\\r\\n    To sum up, we propose HCMoCo, a Human-Centric                      ways. The first one parses garments together with visible\\r\\nmulti-Modal Contrastive learning framework for versatile               body parts [11, 12, 25]. The second one only focuses on\\r\\nmulti-modal pre-training. To fully leverage multi-modal                parsing human parts [7, 20, 22]. In this work, we focus\\r\\nobservations, HCMoCo effectively utilizes both dense mea-              on the second setting because the depth and 2D keypoints\\r\\nsurements and sparse priors using the following three-levels           do not contain the texture information needed for garment\\r\\nhierarchical contrastive learning objectives: 1) sample-               parsing. There are a few works [19, 32] about human pars-\\r\\nlevel modality-invariant representation learning; 2) dense             ing on depth maps. However, the data and annotations are\\r\\nintra-sample contrastive learning; 3) sparse structure-aware           too coarse or unavailable. To further push the accuracy of\\r\\ncontrastive learning. As an effort towards establishing                human-centric perception, DensePose [14, 44] is proposed\\r\\na comprehensive multi-modal human parsing benchmark                    to densely model each human body surface point. The cost\\r\\ndataset, we label human segments for RGB-D images from                 of DensePose annotation is enormous. Therefore, we also\\r\\nNTU RGB+D dataset [42], and contribute the NTURGBD-                    explore data-efficient learning of DensePose.\\r\\nParsing-4K dataset. To evaluate HCMoCo, we trans-                      Multi-Modal Contrastive Learning. Multi-modality nat-\\r\\nfer our pre-train model to four human-centric downstream               urally provides different views of the same sample which\\r\\ntasks using different modalities, including DensePose es-              fits well into the contrastive learning framework. CMC [45]\\r\\n\\r\\n                                                                   2\\r\\n\\x0c\\n\\n                                 #            #       #                        representations for downstream tasks transfer.\\r\\n                               \\xf0\\x9d\\x91\\x80!\\xe2\\x88\\x97 \\xe2\\x88\\x98 \\xe2\\x84\\xb3      \\xf0\\x9d\\x91\\x93!$     \\xf0\\x9d\\x91\\x93!%\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                              (a) Global\\r\\n  \\xf0\\x9d\\x90\\xbc$\"   \\xf0\\x9d\\x90\\xb8$\"       \\xf0\\x9d\\x90\\xb8!$ (\\xf0\\x9d\\x90\\xbc!$ )                                                       To support dense downstream tasks, other than the usual\\r\\n                                 #            #       #                        sample-level global embeddings used in [5,6,13,18,28,45],\\r\\n                               \\xf0\\x9d\\x91\\x80&\\xe2\\x88\\x97 \\xe2\\x88\\x98 \\xe2\\x84\\xb3      \\xf0\\x9d\\x91\\x93&$     \\xf0\\x9d\\x91\\x93&%\\r\\n                                                                               we propose to consider different levels of embeddings i.e.\\r\\n  \\xf0\\x9d\\x90\\xbc$#   \\xf0\\x9d\\x90\\xb8$#       \\xf0\\x9d\\x90\\xb8!% (\\xf0\\x9d\\x90\\xbc!% )                                                   global embeddings f g , sparse embeddings f s and dense\\r\\n              \\xe2\\x80\\xa6\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                              (b) Dense\\r\\n                                                                               embeddings f d 1 , which are defined as follows: 1) For\\r\\n                                              !         !\\r\\n                                  !\\r\\n                                 \\xf0\\x9d\\x91\\x80!\\xe2\\x88\\x97         \\xf0\\x9d\\x91\\x93!$       \\xf0\\x9d\\x91\\x93!%                     dense representations Id , the global embedding is obtained\\r\\n  \\xf0\\x9d\\x90\\xbc!\"   \\xf0\\x9d\\x90\\xb8!\"       \\xf0\\x9d\\x90\\xb8&$ (\\xf0\\x9d\\x90\\xbc&$ )                                                   by applying a mapper network Mdg to the mean pooling M\\r\\n                                              &        &\\r\\n                                                                               of the corresponding feature map, which is formulated as\\r\\n                                &\\r\\n                               \\xf0\\x9d\\x91\\x80!\\xe2\\x88\\x97 \\xe2\\x88\\x98\\xf0\\x9d\\x92\\xa2       \\xf0\\x9d\\x91\\x93!$      \\xf0\\x9d\\x91\\x93!%\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                              (c) Sparse\\r\\n                                                                               fdg = Mdg \\xe2\\x97\\xa6 M \\xe2\\x97\\xa6 Ed (Id ). Similarly, for sparse representa-\\r\\n  \\xf0\\x9d\\x90\\xbc!#   \\xf0\\x9d\\x90\\xb8!#       \\xf0\\x9d\\x90\\xb8&% (\\xf0\\x9d\\x90\\xbc&% )\\r\\n                                              &        &\\r\\n                                                                               tions Is , the global embedding is defined as fsg = Msg \\xe2\\x97\\xa6M\\xe2\\x97\\xa6\\r\\n                                  &         \\xf0\\x9d\\x91\\x93&$      \\xf0\\x9d\\x91\\x93&$\\r\\n                                 \\xf0\\x9d\\x91\\x80&\\xe2\\x88\\x97\\r\\n              \\xe2\\x80\\xa6                                                                Es (Is ). 2) Sparse embeddings have the same size as that\\r\\n                                                                               of sparse representations. Formally, for sparse representa-\\r\\nFigure 2. Illustration of the general paradigm of HCMoCo.                      tions Is = G(V, E), where V \\xe2\\x88\\x88 RJ\\xc3\\x97K , the corresponding\\r\\nWe group modalities of human data into dense Id\\xe2\\x88\\x97 and sparse rep-\\r\\n                                                                               sparse embedding is defined as fss = Mss \\xe2\\x97\\xa6 Es (Is ), where\\r\\nresentations Is\\xe2\\x88\\x97 . Three levels of embeddings are extracted (3.1).                             \\xe2\\x80\\xb2\\r\\n\\r\\nCombining the nature of human data and tasks (3.2), we present\\r\\n                                                                               fss \\xe2\\x88\\x88 RJ\\xc3\\x97K , Mss is a mapper network. For dense rep-\\r\\ncontrastive learning targets for each level of embedding (3.3).                resentations, the corresponding sparse features are pooled\\r\\n                                                                               from the dense feature map using the correspondences G.\\r\\nproposes the first multi-view contrastive learning paradigm                    Then the sparse features are mapped to sparse embeddings\\r\\nwhich takes any number of views. CLIP [39] learns a                            as fds = Mds \\xe2\\x97\\xa6 G \\xe2\\x97\\xa6 Ed (Id ). 3) Dense embeddings are only\\r\\njoint latent space from large-scale paired image-language                      defined on dense representations, which is formulated as\\r\\ndataset. Extensive studies [1, 15, 17, 34, 35, 41] focus on                    fdd = Mdd \\xe2\\x97\\xa6 Ed (Id ). With three levels of embeddings de-\\r\\nvideo-audio contrastive learning. Recently, 2D-3D con-                         fined, we formulate the overall learning objective as\\r\\ntrastive learning [21, 29, 30] has also been studied with the\\r\\n                                                                                           \\\\label {eq:whole} \\\\mathcal {L} = \\\\lambda _g\\\\mathcal {L}_g(f^g) + \\\\lambda _d\\\\mathcal {L}_d(f^d) + \\\\lambda _s\\\\mathcal {L}_s(f^s),    (1)\\r\\ndevelopment in 3D computer vision. In this work, aside\\r\\nfrom commonly used modalities, we also explore the poten-                      which is analyzed and explained as follows.\\r\\ntial of 2D keypoints in human-centric contrastive learning.\\r\\n                                                                               3.2. Principles of Learning Targets Design\\r\\n                                                                                   In this subsection, we analyze the intuitions when de-\\r\\n3. Our Approach                                                                signing learning targets, which makes the following three\\r\\n   In this section, we first introduce the general paradigm                    principles. 1) Mutual Information Maximization: In-\\r\\nof HCMoCo (3.1). Following the design principles (3.2),                        spired by [36,47], we propose to maximize the lower bound\\r\\nhierarchical contrastive learning targets are formally intro-                  on mutual information, which has been proved by many pre-\\r\\nduced (3.3). Next, an instantiation of HCMoCo is intro-                        vious works [5, 6, 18, 45] to be able to produce strong pre-\\r\\nduced (3.4). Finally, we propose two applications of HC-                       train models. 2) Continuous and Ordinal Feature Distri-\\r\\nMoCo to show the versatility (3.5).                                            bution: Inspired by the property of human-centric percep-\\r\\n                                                                               tion, it is desirable for the feature maps of the human body\\r\\n3.1. HCMoCo                                                                    to be continuous and ordinal. The human body is a struc-\\r\\n   As shown in Fig. 2, HCMoCo takes multiple modalities                        tural and continuous surface. The dense predictions, e.g.\\r\\nof perceived human body as input. The target is to learn                       human parsing [11, 12, 25], DensePose [14], are also con-\\r\\nhuman-centric representations, which can be transferred to                     tinuous. Therefore, such property should also be reflected\\r\\ndownstream tasks. The input modalities can be categorized                      in the learned representations. Besides, for an anchor point\\r\\ninto dense and sparse representations. Dense representa-                       on human surfaces, closer points have higher probabilities\\r\\ntions Id\\xe2\\x88\\x97 are the direct output of imaging sensors, e.g. RGB,                  of sharing similar semantics with the anchor point than that\\r\\ndepth, infrared. They typically contain rich information but                   of far away points. Therefore, the learned dense repre-\\r\\nare low-level and noisy. Sparse representations are struc-                     sentations should also align with such ordinal relationship.\\r\\ntured abstractions of the human body, e.g. 2D keypoints, 3D                    3) Structure-Aware Semantic Consistency: Sparse repre-\\r\\npose, which can be formulated as graph Is\\xe2\\x88\\x97 = G(V, E). Dif-                     sentations are abstractions of the human body, which con-\\r\\nferent representations of the same view of a human should                      tains valuable structural semantics about the human body.\\r\\nbe spatially aligned, which means intra-sample correspon-                      Instead of identity information, the human pose and struc-\\r\\ndences can be obtained for dense contrastive learning. HC-                        1 For easier understanding of the notations, the superscripts of f and\\r\\nMoCo aims to pre-train multiple encoders Ed\\xe2\\x88\\x97 and Es\\xe2\\x88\\x97 that                      M stand for the kind of embeddings. The subscripts stand for the kind of\\r\\nproduce embeddings of dense representations and sparse                         representations (\\xe2\\x80\\x98g\\xe2\\x80\\x99 for \\xe2\\x80\\x98global\\xe2\\x80\\x99; \\xe2\\x80\\x98d\\xe2\\x80\\x99 for \\xe2\\x80\\x98dense\\xe2\\x80\\x99; \\xe2\\x80\\x98s\\xe2\\x80\\x99 for \\xe2\\x80\\x98sparse\\xe2\\x80\\x99).\\r\\n\\r\\n\\r\\n                                                                           3\\r\\n\\x0c\\n\\n                                                                                                                                                                                                                                         positive pair                                                  soft positive pair                                                                                          negative pair\\r\\n\\r\\n\\r\\n                                                                      \\xf0\\x9d\\x90\\xb8!\"\\r\\n                                                                                                                                                                                                                                                                  %\\r\\n                                                                                                                                                                                                                                                          \\xf0\\x9d\\x91\\x93!\"                                                                                                                                                                                                                                                                                                  $\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\xf0\\x9d\\x91\\x93!\"\\r\\n                                                                                                                                                                                                                                                                         !\\r\\n                                                                                                                                                                                                                                                                        \\xf0\\x9d\\x91\\x93!\"\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                w=0.5\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                       w=0.1\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                               w=0.01\\r\\n                                                                      \\xf0\\x9d\\x90\\xb8!#                                                                                                                                                                                         %                                                                                                                                                                                                                                                                                             $\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\xf0\\x9d\\x91\\x93!#\\r\\n                                                                                                                                                                                                                                                           \\xf0\\x9d\\x91\\x93!#\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                         !\\r\\n                                                                                                                                                                                                                                                                  %     \\xf0\\x9d\\x91\\x93!#\\r\\n                                                                       \\xf0\\x9d\\x90\\xb8$                                                                                                                                                                                 \\xf0\\x9d\\x91\\x93$                                                                                                                                                                                                                                                                                                  \\xf0\\x9d\\x91\\x93$$\\r\\n\\r\\n                                                                                                                       a) Sample-level Modality-invariant                                                                                                                            b) Dense Intra-sample                                                                                                            c) Sparse Structure-aware\\r\\n                               Encoding                                                                                     Representation Learning                                                                                                                                  Contrastive Learning                                                                                                                Contrastive Learning\\r\\n\\r\\nFigure 3. Our Proposed Instantiation of HCMoCo. For dense representations, we choose to use RGB and depth. For sparse represen-\\r\\ntations, 2D keypoints are used for its convenience to obtain. a) At sample-level, the global embeddings are used for modality-invariant\\r\\nrepresentation learning. b) Between paired dense embeddings, soft contrastive learning target is proposed for continuous and ordinal feature\\r\\nlearning. c) Using human prior provided by sparse representations, intra- and inter-sample contrastive learning targets are proposed.\\r\\n\\r\\nture understanding are the keys to our target downstream                                                                                                                                                                                                                                 sentation, 1 \\xe2\\x89\\xa4 x, x\\xe2\\x80\\xb2 , m \\xe2\\x89\\xa4 H, 1 \\xe2\\x89\\xa4 y, y \\xe2\\x80\\xb2 , n \\xe2\\x89\\xa4 W . The\\r\\ntasks. Therefore, it is reasonable to eliminate the iden-                                                                                                                                                                                                                                above equation is a generalized version of InfoNCE [33].\\r\\n                                                                                                                                                                                                                                                                                                                           mn\\r\\ntity information and enhance the structure information by                                                                                                                                                                                                                                InfoNCE is a special case when Wxy    is set to 1 if x = m\\r\\nenforcing structure-aware semantic consistency where se-                                                                                                                                                                                                                                 and y = n else 0. We use the normalized distances as the\\r\\nmantically close embeddings (e.g. embeddings of left hands                                                                                                                                                                                                                               weights, which is formulated as\\r\\nfrom different samples) are pulled close and vice versa.\\r\\n                                                                                                                                                                                                                                                                                                               \\\\mathcal {W}_{xy}^{mn} = \\\\frac {\\\\text {exp}(\\\\sqrt {(x - m)^2 + (y - n)^2})}{\\\\sum _{x\\', y\\'}\\\\text {exp}(\\\\sqrt {(x - x\\')^2 + (y - y\\')^2})}.                                                                                                                (4)\\r\\n3.3. Hierarchical Contrastive Learning Targets\\r\\n                                                                                                                                                                                                                                                                                         For each pair of dense representations, the above learning\\r\\n   Based on the above three principles, we formally define                                                                                                                                                                                                                               target is calculated between each pair of dense embeddings.\\r\\nhierarchical contrastive learning targets in this subsection.                                                                                                                                                                                                                            Therefore, the whole learning target is defined as\\r\\nSample-level modality-invariant representation learn-                                                                                                                                                                                                                                                                                                    \\\\mathcal {L}_d = \\\\underset {\\\\underset {f_{d1}^d, f_{d2}^d \\\\in F_1^d, F_2^d}{F_1^d, F_2^d \\\\in S_d}}{\\\\mathbb {E}} \\\\mathcal {L}_d^{12}(f_{d1}^d, f_{d2}^d),                                                      (5)\\r\\ning aims at learning a joint latent space at the sample level\\r\\nusing global embeddings, which fulfills the first principle.\\r\\nInspired by [45], the learning target can be formulated as                                                                                                                                                                                                                               where F\\xe2\\x88\\x97d is a set of dense embeddings of one modality, Sd\\r\\n                                                                                                                                                                                                                                                                                                                  d\\r\\n                                                                                                                                                                                                                                                                                         is the set of all F\\xe2\\x88\\x97d , fd1      d\\r\\n                                                                                                                                                                                                                                                                                                                     and fd2 are two paired embeddings.\\r\\n               \\\\mathcal {L}_g = -\\\\underset {\\\\underset {f_1^g \\\\in F_1^g}{F_1^g, F_2^g \\\\in S_g}}{\\\\mathbb {E}} \\\\left [ \\\\text {log}\\\\frac {\\\\text {exp}(f_1^g \\\\cdot \\\\bar {f_2^g} / \\\\tau )}{\\\\sum _{f_2^g \\\\in F_2^g} \\\\text {exp}(f_1^g \\\\cdot f_2^g / \\\\tau )} \\\\right ],        (2)                It should be noticed that the \\xe2\\x80\\x98soft\\xe2\\x80\\x99 learning target cannot\\r\\n                                                                                                                                                                                                                                                                                         guarantee an ordinal feature distribution. Instead, it serves\\r\\n                                                                                                                                                                                                                                                                                         as a computationally efficient relaxation of the requirement\\r\\nwhere F\\xe2\\x88\\x97g is a set of global embeddings of one modality, Sg                                                                                                                                                                                                                              of ordinal distribution.\\r\\nis the set of F\\xe2\\x88\\x97g of all modalities, f\\xc2\\xaf2g is the embedding of the\\r\\npaired view of that of f1g , \\xcf\\x84 is the temperature. It should be                                                                                                                                                                                                                          Sparse structure-aware contrastive learning takes two\\r\\nnoticed that f1g can be sampled from the global embeddings                                                                                                                                                                                                                               sparse representations f1s and f2s as inputs. The paired fea-\\r\\nof either dense or sparse representations.                                                                                                                                                                                                                                                       s      s\\r\\n                                                                                                                                                                                                                                                                                         tures f1j and f2j  (i.e. features of the j-th joint) should be\\r\\nDense intra-sample contrastive learning is operated on                                                                                                                                                                                                                                   pulled close while unpaired features are pushed away. The\\r\\nthe paired dense representations. For any two paired dense                                                                                                                                                                                                                               two sparse representations can be sampled from the same\\r\\n                                         \\xe2\\x80\\xb2\\r\\nembeddings fd1   d    d\\r\\n                   , fd2 \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97K , to simultaneously sat-                                                                                                                                                                                                                              or different modalities, intra- or inter-sample. The intra-\\r\\nisfy the first and the second principle, the dense intra-                                                                                                                                                                                                                                sample alignment satisfies the first principle. The inter-\\r\\nsample contrastive learning target between them is defined                                                                                                                                                                                                                               sample alignment follows the third principle. The sparse\\r\\nin a \\xe2\\x80\\x98soft\\xe2\\x80\\x99 way as                                                                                                                                                                                                                                                                       structure-aware contrastive learning target is formulated as\\r\\n\\r\\n\\r\\n  \\\\mathcal {L}_d^{12} = -\\\\underset {\\\\underset {m, n}{x, y}}{\\\\mathbb {E}} \\\\left [ \\\\mathcal {W}_{xy}^{mn} \\\\text {log} \\\\frac {\\\\text {exp}(f_{d1}^d(x, y) \\\\cdot f_{d2}^d(m, n)) / \\\\tau )}{\\\\sum \\\\limits _{x\\', y\\'} \\\\text {exp}(f_{d1}^d(x,y) \\\\cdot f_{d2}^d(x\\', y\\') / \\\\tau )} \\\\right ],          \\\\mathcal {L}_s = -\\\\underset {\\\\underset {j; f_1^s, f_2^s \\\\in \\\\{F_1^s, F_2^s\\\\}}{F_1^s, F_2^s \\\\in S_s}}{\\\\mathbb {E}} \\\\left [\\\\text {log}\\\\frac {\\\\text {exp}(f_{1j}^{s}\\\\cdot f_{2j}^{s} / \\\\tau )}{\\\\sum \\\\limits _{j\\'; f_i^s \\\\in \\\\{F_1^s, F_2^s\\\\}} \\\\text {exp}(f_{1j}^s \\\\cdot f_{ij\\'}^s / \\\\tau )} \\\\right ], \\r\\n\\r\\n                                                            (3)                                                                                                                                                                                                                                                                                       (6)\\r\\n           mn\\r\\nwhere Wxy        is the weight, \\xcf\\x84 is the temperature,                                                                                                                                                                                                                                    where F\\xe2\\x88\\x97s is a set of sparse embeddings of one modality,\\r\\n(x, y), (m, n), (x\\xe2\\x80\\xb2 , y \\xe2\\x80\\xb2 ) are coordinates on the dense repre-                                                                                                                                                                                                                          Ss is the set of F\\xe2\\x88\\x97s , \\xcf\\x84 is the temperature, f1s , f2s are sam-\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                     4\\r\\n\\x0c\\n\\n                                                     \\xe2\\x84\\x92\\xe2\\x80\\xb2 GT                                                           Label Distribution\\r\\n                    \\xf0\\x9d\\x90\\xb8!\"                      \\xf0\\x9d\\x90\\xb7\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                       RGB\\r\\n                                                     Training\\r\\n                                \\xe2\\x84\\x92\\r\\n                    \\xf0\\x9d\\x90\\xb8!#                      \\xf0\\x9d\\x90\\xb7\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                       Depth\\r\\n                                                     Inference\\r\\n                   (a) Cross-Modality Supervision\\r\\n\\r\\n                                                     \\xe2\\x84\\x92\\xe2\\x80\\xb2 GT\\r\\n                    \\xf0\\x9d\\x90\\xb8!\"                      \\xf0\\x9d\\x90\\xb7\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                       Annotation\\r\\n                                                                                                               right hip        right knee    right foot\\r\\n                                                                                                               left hip         left knee     left foot\\r\\n                                                                                                               left shoulder    left elbow    left hand\\r\\n                                                      Training                                                 right shoulder   right elbow   right hand\\r\\n\\r\\n                            \\xe2\\x84\\x92                                                                                  crotch\\r\\n                                                                                                               left thigh\\r\\n                                                                                                                                right thigh\\r\\n                                                                                                                                left calf\\r\\n                                                                                                                                              right calf\\r\\n                                                                                                                                              lower spine\\r\\n                                                                                                               upper spine      head          left arm\\r\\n\\r\\n                    \\xf0\\x9d\\x90\\xb8!#                       \\xf0\\x9d\\x90\\xb7\\r\\n                                                                                                               left forearm     right arm     right forearm\\r\\n                                     OR\\r\\n                                                     Inference         Figure 5. Illustration of the RGB-D human parsing dataset\\r\\n                   (b) Missing-Modality Inference                      NTURGBD-Parsing-4K.\\r\\n    Figure 4. Pipelines of Two Applications of HCMoCo.\\r\\n                                                                       intra-sample contrastive learning target. With the feature\\r\\npled from the union of F1s and F2s . To conclude, the overall          maps of different modalities aligned, it is straightforward to\\r\\nlearning target is formulated as Eq. 1, where \\xce\\xbb\\xe2\\x88\\x97 are the               implement the two extensions, which are shown in Fig. 4.\\r\\nweights to balance the targets.                                        Cross-Modality Supervision is a novel task where we train\\r\\n                                                                       the network on the source modality, while test on the target\\r\\n3.4. Instantiation of HCMoCo                                           modality. This is a practical scenario where people trans-\\r\\n     In this section, we introduce an instantiation of HC-             fer the knowledge of some single modality dataset to other\\r\\nMoCo. As shown in Fig. 3, for dense representations,                   modalities. At training time, an additional downstream task\\r\\nwe use RGB and depth. Large-scale paired human RGB                     head (e.g. segmentation head) D is attached to the backbone\\r\\nand depth data is easy to obtain with affordable sensors e.g.          of the source modality. The hierarchical contrastive learn-\\r\\nKinect. These two modalities are the most commonly en-                 ing targets L together with downstream task loss L\\xe2\\x80\\xb2 are used\\r\\ncountered in human-centric tasks [7, 11, 12, 22, 25]. More-            for end-to-end training. At inference time, D is attached to\\r\\nover, a proper pre-train model for depth is highly desired.            the backbone of the target modality. The extracted feature\\r\\nTherefore, RGB and depth are reasonable choices of hu-                 maps of the target modality are passed to D for prediction.\\r\\nman dense representations, both of which are easy to ac-               Missing-Modality Inference is another novel task where\\r\\nquire and important to downstream tasks. For sparse repre-             we train the network using multi-modal data and inference\\r\\nsentations, 2D keypoints are used, which provide positions             on single modality. Multi-modal data collection in practice\\r\\nof human body joints in the image coordinate. Off-the-shelf            would inevitably result in data with incomplete modalities,\\r\\ntools [4, 9] are available to quickly and robustly extract hu-         which brings the requirement of missing-modality infer-\\r\\nman 2D keypoints given RGB images. Using 2D keypoints                  ence. At training time, the feature maps of multiple modal-\\r\\nas the sparse representation is a good balance between the             ities are fused using max-pooling and fed to a downstream\\r\\namount of human prior and acquisition difficulty.                      task head D. Similarly, hierarchical contrastive learning tar-\\r\\n     For RGB inputs Id1 , an image encoder Ed1 [43] is applied         gets L and downstream task loss L\\xe2\\x80\\xb2 are used for co-training.\\r\\nto obtain feature maps Ed1 (Id1 ). Similarly, for depth inputs         At inference time, the feature map of a single modality is\\r\\nId2 , an image encoder [43] or 3D encoder [37, 38] Ed2 can             passed to D for missing-modality inference.\\r\\nbe applied to extract feature maps Ed2 (Id2 ). 2D keypoints Is\\r\\nare encoded by a GCN-based encoder [51] Es to produce\\r\\nsparse features Es (Is ). Mapper networks comprise a single\\r\\nlinear layer and a normalization operation.                            4. NTURGBD-Parsing-4K Dataset\\r\\n     As for the implementation of contrastive learning targets,           Although RGB human parsing has been well studied [7,\\r\\nwe choose to use a memory pool to store all the global em-             11, 12, 25], human parsing on depth [19, 32] or RGB-D data\\r\\nbeddings which are updated in a momentum way. Sparse                   has not been fully addressed due to the lack of labeled data.\\r\\nand dense embeddings cannot all fit in memory. Therefore,              Therefore, we contribute the first RGB-D human parsing\\r\\nfor the last two types of contrastive learning targets, the neg-       dataset: NTURGBD-Parsing-4K. The RGB and depth are\\r\\native samples are sampled within a mini-batch.                         uniformly sampled from NTU RGB+D (60/120) [27, 42].\\r\\n                                                                       As shown in Fig. 5, we annotate 24 human parts for paired\\r\\n3.5. Versatility of HCMoCo\\r\\n                                                                       RGB-D data. The partition protocols follow that of [22].\\r\\n   On top of the pre-train framework HCMoCo, we pro-                   The train and test set both have 1963 samples. The whole\\r\\npose to further extend it on two direct applications: cross-           dataset contains 3926 samples. Hopefully, by contributing\\r\\nmodality supervision and missing-modality inference. The               this dataset, we could promote the development of both hu-\\r\\nextensions are based on the key design of HCMoCo: dense                man perception and multi-modality learning.\\r\\n\\r\\n\\r\\n                                                                   5\\r\\n\\x0c\\n\\nTable 1. DensePose Estimation Results on COCO. * randomly initializes the model before pre-training. \\xe2\\x80\\xa0 initializes the model by ImageNet\\r\\npre-train before pre-training. All results in [%].\\r\\n                                                        Full Data                    10% Data\\r\\n     Method           Pre-train Datasets\\r\\n                                            BBox AP GPS AP GPSM AP IOU AP BBox AP GPS AP GPSM AP IoU AP\\r\\n     From Scratch          -                 57.27       62.03        63.61        65.88      39.38       35.75       41.62    49.92\\r\\n     CMC* [45]        NTURGBD+MPII           60.33       64.97        65.66        66.96      44.92       43.84       47.94    54.00\\r\\n     MMV* [1]         NTURGBD+MPII           59.89       64.23        65.47        67.03      43.24       41.40       45.99    52.52\\r\\n     Ours*            NTURGBD+MPII           61.33       65.89        66.92        67.66      47.76       48.47       51.65    56.15\\r\\n     IN Pre-train          -                 62.66       66.48        67.42        68.63      48.28       44.34       49.11    56.11\\r\\n     CMC\\xe2\\x80\\xa0 [45]        NTURGBD+MPII           62.76       66.16        67.30        68.06      49.21       48.82       52.57    57.94\\r\\n     MMV\\xe2\\x80\\xa0 [1]         NTURGBD+MPII           62.97       66.67        67.51        68.29      50.16       50.28       53.54    58.32\\r\\n     Ours\\xe2\\x80\\xa0            NTURGBD+MPII           63.11       67.33        68.12        68.72      50.29       51.50       54.47    58.66\\r\\n     CMC\\xe2\\x80\\xa0 [45]        NTURGBD+COCO           63.58       67.22        67.77        68.46      51.77       53.53       56.18    59.37\\r\\n     Ours\\xe2\\x80\\xa0            NTURGBD+COCO           62.95       67.77        68.29        68.63      52.18       54.01       56.64    59.93\\r\\n\\r\\nTable 2. Human Parsing Results on Human3.6M. * randomly initializes the model before pre-training. \\xe2\\x80\\xa0 initializes the model by ImageNet\\r\\npre-train before pre-training. All results in [%].\\r\\n                               Full Data                     20% Data                      10% Data                   1% Data\\r\\n       Method\\r\\n                       mIoU     mAcc aAcc            mIoU      mAcc aAcc          mIoU       mAcc aAcc        mIoU     mAcc aAcc\\r\\n       From Scratch    44.13     58.88     98.82     42.41    56.25       98.81   32.61     43.76     98.52   7.27     10.97   97.45\\r\\n       CMC* [45]       54.33     68.01     99.09     52.10    65.65       99.03   48.37     61.18     98.95   14.61    20.07   98.07\\r\\n       MMV* [1]        52.69     65.82     99.06     50.66    63.55       99.01   46.23     58.52     98.90   12.86    17.10   97.94\\r\\n       Ours*           61.36     75.09     99.25     59.17    73.44       99.19   57.08     71.75     99.13   16.55    22.27   98.18\\r\\n       IN Pre-train    56.90     69.94     99.14     48.86    60.75       98.97   44.55     56.86     98.87   14.65    20.22   98.09\\r\\n       CMC\\xe2\\x80\\xa0 [45]       58.93     71.70     99.20     57.41    70.13       99.17   54.35     67.47     99.09   17.77    23.77   98.20\\r\\n       MMV\\xe2\\x80\\xa0 [1]        59.08     71.57     99.20     57.28    69.69       99.17   53.86     66.46     99.08   17.66    23.54   98.20\\r\\n       Ours\\xe2\\x80\\xa0           62.50     75.84     99.27     60.85    74.23       99.23   58.28     71.99     99.17   20.78    27.52   98.34\\r\\n\\r\\n5. Experiments                                                             to use general multi-modal contrastive learning methods\\r\\n                                                                           CMC [45] and MMV [1] as the baselines. Although there\\r\\n5.1. Experimental Setup                                                    are other multi-modal contrastive learning works, they ei-\\r\\nImplementation Details. The default RGB and depth en-                      ther require the multi-view calibration [21] or focus on\\r\\ncoders are HRNet-W18 [43]. The default datasets for pre-                   multi-modal downstream tasks [17, 29] and therefore are\\r\\ntrain are NTU RGB+D [27] and MPII [2]. The former pro-                     not suitable for comparison. In addition, for RGB tasks,\\r\\nvides paired indoor human RGB, depth, and 2D keypoints,                    we also experiment under two settings, one initializes en-\\r\\nThe latter provides in-the-wild human RGB and 2D key-                      coders with supervised ImageNet [24] (IN) pre-train while\\r\\npoints. Mixing human data from different domains helps                     the other does not.\\r\\nour pre-train models adapt to a wilder domain.\\r\\n                                                                           5.2. Performance on Downstream Tasks\\r\\nDownstream Tasks. We test our pre-train models on four\\r\\ndifferent human-centric downstream tasks, two on RGB                       DensePose Estimation. As shown in Tab. 1, we test Dense-\\r\\nimages and two on depth. 1) DensePose estimation on                        Pose estimation [14] under two settings: full and 10% of the\\r\\nCOCO [14]: DensePose aims at mapping pixels of the ob-                     training data. The trained models are tested on the full val-\\r\\nserved human body to the surface of a 3D human body,                       idation set of DensePose. Firstly, if not using IN pre-train,\\r\\nwhich is a highly challenging task. 2) RGB human parsing                   our pre-train model significantly outperforms both \\xe2\\x80\\x98From\\r\\non Human3.6M [22]. Human3.6M provides pure human                           Scratch\\xe2\\x80\\x99 and two baseline methods. Especially under 10%\\r\\npart segmentation, which aligns with our objectives. We                    of training data, 12.7% improvement in terms of GPS AP\\r\\nuniformly sample 2fps of the video for training and evalu-                 is observed. And our pre-train model even outperforms that\\r\\nation. 3) Depth human parsing on NTURGBD-Parsing-4K.                       using IN pre-train by 4.13% in terms of GPS AP. When\\r\\n4) 3D pose estimation from depth maps on ITOP [16] (only                   we use IN pre-train as initialization, which is a common\\r\\nside view). For all the above downstream tasks, we use the                 practice for 2D tasks, our method still outperforms all the\\r\\npre-train backbones for end-to-end fine-tune.                              baselines. Our method surpasses IN pre-train by 7.2% and\\r\\nComparison Methods. Since there are few previous                           5.4% in terms of GPS/GPSM AP under 10% setting. To\\r\\nhuman-centric multi-modal pre-train methods, we propose                    further test the performance of in-domain transfer, we also\\r\\n\\r\\n\\r\\n                                                                      6\\r\\n\\x0c\\n\\n                 Table 3. Ablation Study on Densepose/ Human3.6M/ ITOP/ NTURGBD-Parsing-4K. All results in [%].\\r\\n                                             DensePose 10%                ITOP 0.1%/ 0.2%      Human3.6M 10%       NTURGBD 20%\\r\\n    Method\\r\\n                                   BBox      GPS    GPSM          IoU      Acc     Acc         mIoU    mAcc        mIoU  mAcc\\r\\n    Sample-level Mod-invariant     49.21     48.82    52.57       57.94   57.73      50.08     54.35     67.47     30.40     51.54\\r\\n    + Hard Dense Intra-sample      49.40     49.14    52.49       57.30   56.43      54.05     55.36     68.43     31.26     51.54\\r\\n    + Soft Dense Intra-sample      50.21     50.25    53.42       57.70   62.33      51.50     56.35     69.26     32.20     51.06\\r\\n    + Sparse Structure-aware       50.29     51.50    54.47       58.66   65.83      62.36     58.28     71.99     35.01     52.55\\r\\n\\r\\npre-train models using training sets of NTU RGB+D and                     5.3. Ablation Study\\r\\nCOCO. The performance gain under 10% setting further\\r\\n                                                                             In this subsection, we perform a thorough ablation study\\r\\nimproves to 9.7% and 7.5% in terms of GPS/GPSM AP.\\r\\n                                                                          on HCMoCo to justify the design choices. As shown\\r\\nRGB Human Parsing. As shown in Tab. 2, we test four                       in Tab. 3, we firstly report the results of only apply-\\r\\nsettings on Human3.6M [22]: full, 20%, 10% and 1% train-                  ing sample-level modality-invariant representation learning.\\r\\ning data. In all settings, our method outperforms all base-               Then we add dense intra-sample contrastive learning and\\r\\nlines in all metrics. On full training data, we outperform                sparse structure-aware contrastive learning in order. To fur-\\r\\nIN pre-train by 5.6% in terms of mIoU. The performance                    ther demonstrate the effect of the \\xe2\\x80\\x98soft\\xe2\\x80\\x99 design in dense\\r\\ngain increases with the amount of training data decreases.                intra-sample contrastive learning, we also report results of\\r\\nIt is worth noticing that with only 10% of training data, our             the \\xe2\\x80\\x98hard\\xe2\\x80\\x99 learning target, which takes the form of a classic\\r\\nmethod outperforms IN pre-train with full training data.                  InfoNCE [33]. We report the results of the ablation study\\r\\n                                                                          on all four downstream tasks under data-efficient settings.\\r\\nTable 4. Human Parsing Results on NTURGBD-Parsing-4K [%].                    For DensePose estimation, it is important to learn fea-\\r\\n                     Full Data               20% Data                     ture maps that are continuously and ordinally distributed,\\r\\n  Method                                                                  which is the expected result of soft dense intra-sample con-\\r\\n                 mIoU mAcc aAcc          mIoU mAcc aAcc\\r\\n  IN Pre-train   37.49   57.52   98.36   28.56    46.81   98.10           trastive learning. The performance gain of the soft learn-\\r\\n  CMC [45]       38.20   58.73   98.39   30.40    51.54   98.02           ing target over the hard counterpart justifies the observation\\r\\n  MMV [1]        38.09   58.49   98.37   30.41    50.62   98.07           and the learning target design. The dense intra-sample con-\\r\\n  Ours           39.32   58.79   98.47   35.01    52.55   98.53           trastive learning also shows superiority on three other down-\\r\\n                                                                          stream tasks, which shows the importance of fine-grained\\r\\n                                                                          contrastive learning targets for dense prediction tasks.\\r\\nDepth Human Parsing. As shown in Tab. 4, we test\\r\\n                                                                             Explicitly injecting human prior into the network\\r\\nthe pre-train depth backbone on our proposed Dataset\\r\\n                                                                          through sparse structure-aware contrastive learning also\\r\\nNTURGBD-Parsing-4K with all training data and 20%\\r\\n                                                                          proves its effectiveness by further improving the perfor-\\r\\ntraining data. We outperform all baselines on two settings.\\r\\n                                                                          mance on DensePose. Thanks to the strong hints provided\\r\\nEspecially, only using 20% of training data, we surpass IN\\r\\n                                                                          by 2D keypoints, the performance of 3D pose estimation is\\r\\npre-train by 6.4% and MMV [1] by 4.6% in terms of mIoU.\\r\\n                                                                          improved. Moreover, the sparse structure-aware contrastive\\r\\n                                                                          learning boosts the performance of human parsing both on\\r\\nTable 5. 3D Pose Estimation Results on ITOP. All results in [%].\\r\\n                                                                          RGB and depth maps by 1.9% and 2.8% respectively in\\r\\n  Method         100%    10%      1%       0.5%   0.2%    0.1%            terms of mIoU. Although 2D keypoints are sparse priors,\\r\\n  IN Pre-train   85.19   83.44 77.20 54.31 13.27 14.21                    they still provide the rough location of each part of the hu-\\r\\n  CMC [45]       87.08   85.36 79.49 75.07 57.73 50.08                    man body, which facilitate the feature alignment of same\\r\\n  MMV [1]        86.13   83.49 79.70 71.70 60.83 54.44                    body parts. To summarize, the sparse and dense learning\\r\\n  Ours           87.19   85.49 78.71 76.34 65.83 62.36                    targets both contribute to the performance of our methods,\\r\\n                                                                          which is in line with our analysis.\\r\\n3D Pose Estimation. As shown in Tab. 5, we test the pre-\\r\\n                                                                          5.4. Performance on HCMoCo Versatility\\r\\ntrain depth backbone on ITOP [16] with six different ra-\\r\\ntios of training data. Our pre-train model outperforms all                Cross-Modality Supervision. We test the cross-modality\\r\\nbaselines on most settings. With only 10% training data,                  supervision pipeline on the task of human parsing on\\r\\nthe accuracy of our method outperforms that of IN pre-train               NTURGBD-Parsing-4K because it has two modalities and\\r\\nwith all training data. It is also worth noticing that 0.1% of            respective dense annotations. Two baseline methods are\\r\\ntraining data are 17 samples, which makes this a few-shot                 adopted: 1) using CMC [45] contrastive learning target; 2)\\r\\nlearning setting. With such limited training data, IN pre-                no contrastive learning target. For a fair comparison, the\\r\\ntrain barely produce meaningful results, while our method                 backbones of all methods are initialized by CMC [45] pre-\\r\\nimproves the accuracy by 48.2%.                                           train. At training time, the target modality of training data\\r\\n\\r\\n\\r\\n                                                                     7\\r\\n\\x0c\\n\\nTable 6. Cross-Modality Supervised Human Parsing Results on           Table 8. Experiments on changing the backbone. * stands for\\r\\nNTURGBD-Parsing-4K. All results in [%].                               \\xe2\\x80\\x98IN Pre-train\\xe2\\x80\\x99 for DensePose and \\xe2\\x80\\x98From Scratch\\xe2\\x80\\x99 for NTURGBD-\\r\\n                                                                      Parsing-4K. All results in [%].\\r\\n                        RGB \\xe2\\x86\\x92 Depth             Depth \\xe2\\x86\\x92 RGB\\r\\n Method\\r\\n                      mIoU mAcc aAcc          mIoU mAcc aAcc                             DensePose 10%          NTURGBD 20%\\r\\n                                                                       Method\\r\\n No Contrastive 3.94 4.36 92.24 3.71 4.03 91.63                                   BBox    GPS GPSM IoU          mIoU  mAcc\\r\\n CMC [45]        3.86 5.59 86.81 3.85 4.27 91.75                       *        55.10 54.60      57.60   61.73 45.36     59.51\\r\\n Ours           33.19 54.38 94.70 26.80 48.80 92.84                    CMC [45] 53.88 54.62      57.46   61.14 48.74     62.94\\r\\n                                                                       Ours     54.55 55.80      58.36   61.75 49.43     63.52\\r\\nis not available. We experiment on two settings where we\\r\\nsupervise on RGB, test on depth (RGB \\xe2\\x86\\x92 Depth), and vice               both the full training data and data-efficient settings.\\r\\nversa (Depth \\xe2\\x86\\x92 RGB). As shown in Tab. 6, our method                   Changing Backbone. So far our experiments are all\\r\\noutperforms both baselines under two settings. Specifically,          performed on HRNet-W18. To further demonstrate HC-\\r\\nour method improves the mIoU of both settings by 29.2%                MoCo\\xe2\\x80\\x99s performance on other backbones, for the 2D back-\\r\\nand 23.0%, respectively. Even compared to methods with                bone, we also experiment with HRNet-W32 [43]. For the\\r\\ndirect supervision, we can achieve comparable results.                depth backbone, we choose to test with PointNet++ [38].\\r\\nTable 7. Missing-Modality Human Parsing Results on                    For the RGB pre-train model, we experiment on the 10%\\r\\nNTURGBD-Depth. All results in [%].                                    DensePose estimation. For the depth pre-train model, we\\r\\n                                                                      experiment on the 20% NTURGBD-Parsing-4K. As shown\\r\\n                          Only RGB                Only Depth\\r\\n Method                                                               in Tab. 8, our method outperforms its pre-train counterparts\\r\\n                      mIoU mAcc aAcc          mIoU mAcc aAcc\\r\\n                                                                      by a reasonable margin, which is in line with our previous\\r\\n No Contrastive 13.45 14.77 93.35 24.41 30.49 95.27\\r\\n                                                                      experimental results.\\r\\n CMC [45]       19.62 28.19 92.94 16.58 19.83 93.94\\r\\n Ours           43.88 64.27 96.15 43.98 63.66 96.34\\r\\n                                                                      6. Discussion and Conclusion\\r\\nMissing-Modality Inference. For missing-modality in-                      In this work, we propose the first versatile multi-modal\\r\\nference, we report the experiments on the same dataset                pre-training framework HCMoCo specifically designed for\\r\\nand same baselines as above. As shown in Tab. 7, with                 human-centric perception tasks. Hierarchical contrastive\\r\\nno pixel-level alignment, the two baseline methods strug-             learning targets are designed based on the nature of hu-\\r\\ngle in two missing-modality settings i.e. \\xe2\\x80\\x98Only RGB\\xe2\\x80\\x99 and              man datasets and the requirements of human-centric down-\\r\\n\\xe2\\x80\\x98Only Depth\\xe2\\x80\\x99. While our method improves the segmenta-                 stream tasks. Extensive experiments on four different hu-\\r\\ntion mIoU by 24.3% and 19.6% on two settings.                         man downstream tasks of different modalities demonstrated\\r\\n                                                                      the effectiveness of our pre-training framework. We con-\\r\\n                      IN Pre-train    CMC           Ours              tribute a new RGB-D human parsing dataset NTURGBD-\\r\\n        65                           60                               Parsing-4K to support research of human perception on\\r\\n        55                           50                               RGB-D data. Besides downstream task transfer, we also\\r\\n                                     40                               propose two novel applications of HCMoCo to show its ver-\\r\\n mIoU\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n        45\\r\\n                                     30                               satility and ability in cross-modal reasoning.\\r\\n        35\\r\\n                                     20                               Potential Negative Impacts & Limitations. Usage of\\r\\n        25                           10                               large amounts of data and long training time might nega-\\r\\n        15                            0                               tively impact the environment. Moreover, even though we\\r\\n             1   10 19 28 37 46           0   30    60 90 120\\r\\n                    epochs                         epochs             did not collect any new human data in this work, human data\\r\\n                                                                      collection could happen if our framework is used in other\\r\\nFigure 6. Validation mIoU Changes with Training Epochs In-\\r\\ncrease. Left: Human3.6M human parsing full training set. Right:       applications, which potentially raises privacy concerns. As\\r\\nHuman3.6M human parsing 20% training set.                             for the limitations, due to limited resources, we could only\\r\\n                                                                      experiment with one possible instantiation of HCMoCo.\\r\\n5.5. Further Analysis                                                 And for the same reason, even though the theoretical possi-\\r\\n                                                                      bility exists, we do not have the chance to further scale up\\r\\nFaster Convergence. One of the advantages of pre-training             the amount of human dataset and network size.\\r\\nis the fast convergence speed when transferred to down-               Acknowledgments           This work is supported by NTU\\r\\nstream tasks. Our HCMoCo also shows superiority in this               NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the\\r\\nfeature. We log the validation mIoU of Human3.6M hu-                  RIE2020 Industry Alignment Fund \\xe2\\x80\\x93 Industry Collabora-\\r\\nman parsing at different training epochs. As shown in Fig.            tion Projects (IAF-ICP) Funding Initiative, as well as cash\\r\\n6, compared with IN pre-train and CMC [45], our pre-train             and in-kind contribution from the industry partner(s).\\r\\nmodel is able to converge within a few training epochs in\\r\\n\\r\\n\\r\\n                                                                  8\\r\\n\\x0c\\n\\nReferences                                                                    ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\\r\\n                                                                              mad Gheshlaghi Azar, et al. Bootstrap your own latent: A\\r\\n [1] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider,                new approach to self-supervised learning. arXiv preprint\\r\\n     Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lu-                arXiv:2006.07733, 2020. 3\\r\\n     cas Smaira, Sander Dieleman, and Andrew Zisserman. Self-            [14] R\\xc4\\xb1za Alp Gu\\xcc\\x88ler, Natalia Neverova, and Iasonas Kokkinos.\\r\\n     supervised multimodal versatile networks. NeurIPS, 2(6):7,               Densepose: Dense human pose estimation in the wild. In\\r\\n     2020. 3, 6, 7, 15, 16, 17                                                Proceedings of the IEEE Conference on Computer Vision\\r\\n [2] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and                 and Pattern Recognition, pages 7297\\xe2\\x80\\x937306, 2018. 1, 2, 3,\\r\\n     Bernt Schiele. 2d human pose estimation: New benchmark                   6, 12, 13\\r\\n     and state of the art analysis. In IEEE Conference on Com-           [15] Tengda Han, Weidi Xie, and Andrew Zisserman. Self-\\r\\n     puter Vision and Pattern Recognition (CVPR), June 2014. 1,               supervised co-training for video representation learning.\\r\\n     2, 6                                                                     arXiv preprint arXiv:2010.09709, 2020. 3\\r\\n [3] Mahmoud Assran, Nicolas Ballas, Lluis Castrejon, and                [16] Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Ser-\\r\\n     Michael Rabbat. Supervision accelerates pre-training in con-             ena Yeung, and Li Fei-Fei. Towards viewpoint invariant 3d\\r\\n     trastive semi-supervised learning of visual representations.             human pose estimation. In European Conference on Com-\\r\\n     arXiv preprint arXiv:2006.10803, 2020. 2                                 puter Vision, pages 160\\xe2\\x80\\x93177. Springer, 2016. 2, 6, 7, 12\\r\\n [4] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.            [17] Devamanyu Hazarika, Roger Zimmermann, and Soujanya\\r\\n     Sheikh. Openpose: Realtime multi-person 2d pose estima-                  Poria. Misa: Modality-invariant and-specific representations\\r\\n     tion using part affinity fields. IEEE Transactions on Pattern            for multimodal sentiment analysis. In Proceedings of the\\r\\n     Analysis and Machine Intelligence, 2019. 2, 5                            28th ACM International Conference on Multimedia, pages\\r\\n [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-                    1122\\xe2\\x80\\x931131, 2020. 3, 6\\r\\n     offrey Hinton. A simple framework for contrastive learning          [18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\r\\n     of visual representations. In International conference on ma-            Girshick. Momentum contrast for unsupervised visual rep-\\r\\n     chine learning, pages 1597\\xe2\\x80\\x931607. PMLR, 2020. 3                           resentation learning. In Proceedings of the IEEE/CVF Con-\\r\\n [6] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.                   ference on Computer Vision and Pattern Recognition, pages\\r\\n     Improved baselines with momentum contrastive learning.                   9729\\xe2\\x80\\x939738, 2020. 3\\r\\n     arXiv preprint arXiv:2003.04297, 2020. 3                            [19] Antonio Herna\\xcc\\x81ndez-Vela, Nadezhda Zlateva, Alexander\\r\\n [7] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fi-                   Marinov, Miguel Reyes, Petia Radeva, Dimo Dimov, and\\r\\n     dler, Raquel Urtasun, and Alan Yuille. Detect what you                   Sergio Escalera. Graph cuts optimization for multi-limb hu-\\r\\n     can: Detecting and representing objects using holistic mod-              man segmentation in depth maps. In 2012 IEEE Conference\\r\\n     els and body parts. In Proceedings of the IEEE conference on             on Computer Vision and Pattern Recognition, pages 726\\xe2\\x80\\x93\\r\\n     computer vision and pattern recognition, pages 1971\\xe2\\x80\\x931978,                732. IEEE, 2012. 2, 5\\r\\n     2014. 1, 2, 5                                                       [20] Fangzhou Hong, Liang Pan, Zhongang Cai, and Ziwei Liu.\\r\\n [8] Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying                     Garment4d: Garment reconstruction from point cloud se-\\r\\n     Deng, and Weiming Hu. Channel-wise topology refinement                   quences. In Thirty-Fifth Conference on Neural Information\\r\\n     graph convolution for skeleton-based action recognition. In              Processing Systems, 2021. 2\\r\\n     Proceedings of the IEEE/CVF International Conference on             [21] Ji Hou, Saining Xie, Benjamin Graham, Angela Dai, and\\r\\n     Computer Vision, pages 13359\\xe2\\x80\\x9313368, 2021. 1                              Matthias Nie\\xc3\\x9fner. Pri3d: Can 3d priors help 2d represen-\\r\\n [9] MMPose Contributors. Openmmlab pose estimation tool-                     tation learning? arXiv preprint arXiv:2104.11225, 2021. 2,\\r\\n     box and benchmark. https://github.com/open-                              3, 6\\r\\n     mmlab/mmpose, 2020. 2, 5                                            [22] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\\r\\n[10] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-                    Sminchisescu. Human3.6m: Large scale datasets and predic-\\r\\n     ber, Thomas Funkhouser, and Matthias Nie\\xc3\\x9fner. Scannet:                   tive methods for 3d human sensing in natural environments.\\r\\n     Richly-annotated 3d reconstructions of indoor scenes. In                 IEEE Transactions on Pattern Analysis and Machine Intel-\\r\\n     Proceedings of the IEEE conference on computer vision and                ligence, 36(7):1325\\xe2\\x80\\x931339, jul 2014. 1, 2, 5, 6, 7, 12, 13,\\r\\n     pattern recognition, pages 5828\\xe2\\x80\\x935839, 2017. 2                            14\\r\\n[11] Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming                [23] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\\r\\n     Yang, and Liang Lin. Instance-level human parsing via part               Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\\r\\n     grouping network. In Proceedings of the European Confer-                 Dilip Krishnan. Supervised contrastive learning. arXiv\\r\\n     ence on Computer Vision (ECCV), pages 770\\xe2\\x80\\x93785, 2018. 1,                  preprint arXiv:2004.11362, 2020. 2\\r\\n     2, 3, 5                                                             [24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\r\\n[12] Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen,                      Imagenet classification with deep convolutional neural net-\\r\\n     and Liang Lin. Look into person: Self-supervised structure-              works. Advances in neural information processing systems,\\r\\n     sensitive learning and a new benchmark for human parsing.                25:1097\\xe2\\x80\\x931105, 2012. 6\\r\\n     In Proceedings of the IEEE Conference on Computer Vision            [25] Jianshu Li, Jian Zhao, Yunchao Wei, Congyan Lang, Yidong\\r\\n     and Pattern Recognition, pages 932\\xe2\\x80\\x93940, 2017. 1, 2, 3, 5                 Li, Terence Sim, Shuicheng Yan, and Jiashi Feng. Multiple-\\r\\n[13] Jean-Bastien Grill, Florian Strub, Florent Altche\\xcc\\x81, Corentin             human parsing in the wild. arXiv preprint arXiv:1705.07206,\\r\\n     Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-                  2017. 1, 2, 3, 5\\r\\n\\r\\n\\r\\n                                                                     9\\r\\n\\x0c\\n\\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,                    Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\r\\n     Pietro Perona, Deva Ramanan, Piotr Dolla\\xcc\\x81r, and C Lawrence                  ing transferable visual models from natural language super-\\r\\n     Zitnick. Microsoft coco: Common objects in context. In                      vision. arXiv preprint arXiv:2103.00020, 2021. 3\\r\\n     European conference on computer vision, pages 740\\xe2\\x80\\x93755.                 [40] N Dinesh Reddy, Laurent Guigues, Leonid Pishchulin, Jayan\\r\\n     Springer, 2014. 1, 2                                                        Eledath, and Srinivasa G Narasimhan. Tessetrack: End-to-\\r\\n[27] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,                         end learnable multi-person articulated 3d pose tracking. In\\r\\n     Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-                      Proceedings of the IEEE/CVF Conference on Computer Vi-\\r\\n     scale benchmark for 3d human activity understanding. IEEE                   sion and Pattern Recognition, pages 15190\\xe2\\x80\\x9315200, 2021. 1,\\r\\n     transactions on pattern analysis and machine intelligence,                  2\\r\\n     42(10):2684\\xe2\\x80\\x932701, 2019. 1, 5, 6                                        [41] Andrew Rouditchenko, Angie Boggust, David Harwath,\\r\\n[28] Songtao Liu, Zeming Li, and Jian Sun. Self-emd: Self-                       Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Au-\\r\\n     supervised object detection without imagenet. arXiv preprint                dhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris,\\r\\n     arXiv:2011.13677, 2020. 3                                                   et al. Avlnet: Learning audio-visual language representations\\r\\n[29] Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong,                          from instructional videos. arXiv preprint arXiv:2006.09199,\\r\\n     Thomas Funkhouser, and Li Yi. Contrastive multimodal fu-                    2020. 3\\r\\n     sion with tupleinfonce. In Proceedings of the IEEE/CVF In-             [42] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\\r\\n     ternational Conference on Computer Vision, pages 754\\xe2\\x80\\x93763,                   Ntu rgb+ d: A large scale dataset for 3d human activity anal-\\r\\n     2021. 3, 6                                                                  ysis. In Proceedings of the IEEE conference on computer\\r\\n[30] Yunze Liu, Li Yi, Shanghang Zhang, Qingnan Fan, Thomas                      vision and pattern recognition, pages 1010\\xe2\\x80\\x931019, 2016. 1,\\r\\n     Funkhouser, and Hao Dong. P4contrast: Contrastive learn-                    2, 5, 12\\r\\n     ing with pairs of point-pixel pairs for rgb-d scene understand-        [43] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\\r\\n     ing. arXiv preprint arXiv:2012.13089, 2020. 2, 3                            high-resolution representation learning for human pose es-\\r\\n[31] Julieta Martinez, Rayat Hossain, Javier Romero, and James J                 timation. In Proceedings of the IEEE/CVF Conference\\r\\n     Little. A simple yet effective baseline for 3d human pose es-               on Computer Vision and Pattern Recognition, pages 5693\\xe2\\x80\\x93\\r\\n     timation. In Proceedings of the IEEE International Confer-                  5703, 2019. 1, 2, 5, 6, 8, 12\\r\\n     ence on Computer Vision, pages 2640\\xe2\\x80\\x932649, 2017. 1, 2                   [44] Feitong Tan, Danhang Tang, Mingsong Dou, Kaiwen Guo,\\r\\n[32] Kaichiro Nishi and Jun Miura. Generation of human depth                     Rohit Pandey, Cem Keskin, Ruofei Du, Deqing Sun, Sofien\\r\\n     images with body part labels for complex human pose recog-                  Bouaziz, Sean Fanello, et al. Humangps: Geodesic preserv-\\r\\n     nition. Pattern Recognition, 71:402\\xe2\\x80\\x93413, 2017. 2, 5                         ing feature for dense human correspondences. In Proceed-\\r\\n[33] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-                     ings of the IEEE/CVF Conference on Computer Vision and\\r\\n     sentation learning with contrastive predictive coding. arXiv                Pattern Recognition, pages 1820\\xe2\\x80\\x931830, 2021. 2\\r\\n     preprint arXiv:1807.03748, 2018. 2, 4, 7                               [45] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-\\r\\n[34] Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth                      trastive multiview coding. In Computer Vision\\xe2\\x80\\x93ECCV 2020:\\r\\n     Fong, Joao F Henriques, Geoffrey Zweig, and Andrea                          16th European Conference, Glasgow, UK, August 23\\xe2\\x80\\x9328,\\r\\n     Vedaldi. Multi-modal self-supervision from generalized data                 2020, Proceedings, Part XI 16, pages 776\\xe2\\x80\\x93794. Springer,\\r\\n     transformations. arXiv preprint arXiv:2003.04298, 2020. 3                   2020. 2, 3, 4, 6, 7, 8, 13, 15, 16, 17\\r\\n[35] Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth                 [46] Longhui Wei, Lingxi Xie, Jianzhong He, Jianlong Chang,\\r\\n     Fong, Joa\\xcc\\x83o F Henriques, Geoffrey Zweig, and Andrea                         Xiaopeng Zhang, Wengang Zhou, Houqiang Li, and Qi Tian.\\r\\n     Vedaldi. On compositions of transformations in contrastive                  Can semantic labels assist self-supervised visual representa-\\r\\n     self-supervised learning. In Proceedings of the IEEE/CVF                    tion learning? arXiv preprint arXiv:2011.08621, 2020. 2\\r\\n     International Conference on Computer Vision, pages 9577\\xe2\\x80\\x93               [47] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\r\\n     9587, 2021. 3                                                               Unsupervised feature learning via non-parametric instance\\r\\n[36] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi,                   discrimination. In Proceedings of the IEEE conference on\\r\\n     and George Tucker. On variational bounds of mutual infor-                   computer vision and pattern recognition, pages 3733\\xe2\\x80\\x933742,\\r\\n     mation. In International Conference on Machine Learning,                    2018. 3\\r\\n     pages 5171\\xe2\\x80\\x935180. PMLR, 2019. 3                                         [48] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines\\r\\n[37] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.                    for human pose estimation and tracking. In Proceedings of\\r\\n     Pointnet: Deep learning on point sets for 3d classification                 the European conference on computer vision (ECCV), pages\\r\\n     and segmentation. In Proceedings of the IEEE conference                     466\\xe2\\x80\\x93481, 2018. 1, 2\\r\\n     on computer vision and pattern recognition, pages 652\\xe2\\x80\\x93660,             [49] Fu Xiong, Boshen Zhang, Yang Xiao, Zhiguo Cao, Taidong\\r\\n     2017. 5                                                                     Yu, Joey Tianyi Zhou, and Junsong Yuan. A2j: Anchor-to-\\r\\n[38] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J                       joint regression network for 3d articulated pose estimation\\r\\n     Guibas. Pointnet++: Deep hierarchical feature learning on                   from a single depth image. In Proceedings of the IEEE/CVF\\r\\n     point sets in a metric space. In Advances in neural informa-                International Conference on Computer Vision, pages 793\\xe2\\x80\\x93\\r\\n     tion processing systems, pages 5099\\xe2\\x80\\x935108, 2017. 5, 8, 13                    802, 2019. 2, 12, 13\\r\\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya                     [50] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-\\r\\n     Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,                       ral graph convolutional networks for skeleton-based action\\r\\n\\r\\n\\r\\n                                                                       10\\r\\n\\x0c\\n\\n     recognition. In Thirty-second AAAI conference on artificial\\r\\n     intelligence, 2018. 1\\r\\n[51] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dim-\\r\\n     itris N Metaxas. Semantic graph convolutional networks for\\r\\n     3d human pose regression. In Proceedings of the IEEE/CVF\\r\\n     Conference on Computer Vision and Pattern Recognition,\\r\\n     pages 3425\\xe2\\x80\\x933435, 2019. 5\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                   11\\r\\n\\x0c\\n\\n              Supplementary Material                                  takes around 80 hours to train. For the 10% training set, we\\r\\n                                                                      train the network for 13000 iterations with a learning rate\\r\\n    In this supplementary material, we provide more imple-            of 0.005 and other settings the same, which takes 9 hours to\\r\\nmentation details of HCMoCo, downstream tasks and two                 train. The 10% training set is uniformly sampled from the\\r\\napplications (Sec. 1). More detailed quantitative and qual-           default ordered training set.\\r\\nitative results of downstream tasks are also illustrated (Sec.\\r\\n2 and Sec. 3).                                                        1.3. RGB Human Parsing\\r\\n                                                                          For the RGB human parsing on Human3.6M [22], we\\r\\n1. Implementation Details                                             use the official HRNet [43] semantic segmentation imple-\\r\\n1.1. HCMoCo                                                           mentation 4 . Different ratios of training settings are uni-\\r\\n                                                                      formly sampled from the default ordered full training set.\\r\\nNetwork Details. For the proposed instantiation of HC-                For the full training set, we train the network for 50 epochs\\r\\nMoCo, we implement the sample-level modality-invariant                with a learning rate of 0.007, a batch size of 40 on 2\\r\\nrepresentation learning target by maintaining a memory                NVIDIA V100 GPUs. For other data-efficient settings, we\\r\\npool, which is adapted from an open-sourced implemen-                 train the network for 150 epochs with other settings the\\r\\ntation 2 . The memory pool is updated in a momentum                   same. We use the the standard dataset split protocol, where\\r\\nstyle with the momentum of 0.5. For global embeddings,                the subjects 1, 5, 6, 7, 8 are for training and the subjects 9\\r\\nwe sample 16384 negative samples from the memory pool.                and 11 are for evaluation.\\r\\nFor other hyper-parameters, we use a batch size of 224, a\\r\\nlearning rate of 0.03, a temperature of 0.07 for all three            1.4. Depth Human Parsing\\r\\ncontrastive learning targets. For the pre-train, 4 NVIDIA                 For the depth human parsing on NTURGBD-Parsing-\\r\\nV100 GPUs are used. The training process is divided in two            4K, we use the same implementation as that of RGB human\\r\\nsteps. The first step only pre-train the model using sample-          parsing. To use the HRNet to encode depth maps, we repeat\\r\\nlevel modality-invariant representation learning target for           the depth dimension for three times to fit the RGB input,\\r\\n100 epochs. The second stage adds the other two learn-                which is also how HCMoCo deals with depth inputs. For all\\r\\ning targets and trains for another 100 epochs. The whole              training settings, we train the network for 150 epochs with\\r\\ntraining process takes approximately 48 hours.                        a learning rate of 0.007, a batch size of 80 on 2 NVIDIA\\r\\nMixing Heterogeneous Datasets. Since we mix several                   V100 GPUs. Even though the encoder is used to deal with\\r\\nheterogeneous human datasets for pre-train, we need to                depth inputs, we still initialize it using ImageNet pre-train\\r\\nmask out the missing modalities. For example, when we                 for that it might help with the performance proved by some\\r\\nuse NTU RGB+D and MPII for pre-train. The former                      previous works [49].\\r\\ndataset has all the required modalities, while the latter one\\r\\nmisses depth maps. Therefore, for the hierarchical con-               1.5. 3D Pose Estimation on Depth\\r\\ntrastive learning targets, we mask out the missing depth em-              For the 3D pose estimation from depth maps on\\r\\nbeddings of MPII for all the positive pairs sampling. By              ITOP [16], we choose to adapt the official implementation\\r\\nusing the masking technique, it is possible to combine mul-           5\\r\\n                                                                        of A2J [49]. The original implementation uses ResNet as\\r\\ntiple heterogeneous datasets into this pre-train paradigm as          the backbone. And we switch to HRNet. Since the origi-\\r\\nlong as there are at least two common modalities.                     nal implementation only provides validation scripts, we re-\\r\\nDatasets for Pre-train. For NTU RGB+D, we only use                    implement the whole training pipeline. We change the orig-\\r\\nthe version with 60 actions [42]. With the provided RGB-              inal normalization method where a global mean and vari-\\r\\nD videos, we uniformly sample one frame from every 30                 ance is counted for a global normalization. Instead, we per-\\r\\nframes, which makes 143648 samples. The RGB and depth                 form an online instance normalization where we only cen-\\r\\nframes are calibrated by the correspondences provided by              tralize each depth pixel to zero mean but do not normalize\\r\\nthe 2D keypoints positions on RGB and depths. For MPII                its variance, since its a better way to prevent the over-fitting\\r\\nand COCO, we use the full training sets for pre-train.                to the relatively small dataset. We train the network for 50\\r\\n1.2. DensePose Estimation                                             epochs with a learning rate of 0.00035 and a batch size of\\r\\n                                                                      12 on one NVIDIA V100 GPU. As for the dataset, we use\\r\\n    For the DensePose [14] estimation, we use the official            the side-view of ITOP since the depth maps in pre-train are\\r\\nopen-sourced implementation 3 . For the full training set, we         side views. Following the official dataset split, there are\\r\\ntrain the network for 130000 iterations with a batch size of          17991 samples for training and 4863 for testing. Following\\r\\n16, a learning rate of 0.01 on 4 NVIDIA V100 GPUs, which\\r\\n                                                                        4 https : / / github . com / HRNet / HRNet - Semantic -\\r\\n  2 https://github.com/HobbitLong/PyContrast                          Segmentation\\r\\n  3 https://github.com/facebookresearch/detectron2                      5 https://github.com/zhangboshen/A2J\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 12\\r\\n\\x0c\\n\\nthe practice of A2J [49], we initialize the encoders using            dense embeddings of RGB and depth are aligned and the\\r\\nImageNet pre-train.                                                   segmentation header is trained with the fusion of both em-\\r\\n                                                                      beddings, missing one of them will still produce reasonable\\r\\n1.6. Cross-Modality Supervision                                       predictions.\\r\\n   To experiment with the cross-modality supervision,\\r\\nwe choose the downstream task of human parsing on                     2. More Quantitative Results\\r\\nNTURGBD-Parsing-4K. The modalities to experiment with                 DensePose Estimation. Due to the page limitation, we\\r\\nare RGB and depth. To make the experiment fair and the                could not report all metrics for the DensePose [14] esti-\\r\\nnetworks to converge faster, the backbones are initialized            mation. Therefore, we report them in this supplementary\\r\\nby CMC [45] pre-train. The following descriptions are for             material. As shown in Tab. 9, detailed results of all set-\\r\\nthe setting of \\xe2\\x80\\x98RGB\\xe2\\x86\\x92Depth\\xe2\\x80\\x99, where the source modality                 tings mentioned in the main paper are listed. Specifically,\\r\\nis RGB and the target modality is depth. To implement                 for the initialization of the network, we test with the net-\\r\\n\\xe2\\x80\\x98Depth\\xe2\\x86\\x92RGB\\xe2\\x80\\x99, one can simply switch the source and tar-                work randomly initialized (\\xe2\\x80\\x98From Scratch\\xe2\\x80\\x99) and the network\\r\\nget modalities. At training time, a randomly initialized              initialized by ImageNet pre-train (\\xe2\\x80\\x98IN Pre-train\\xe2\\x80\\x99). As for\\r\\nsegmentation header, which is the same one used for hu-               the ratio of training data, we test with the full training set\\r\\nman parsing experiments, is attached to the dense mapper              and 10% of the training set. As for the pre-train datasets,\\r\\nnetwork of RGB. Then the network is trained with both                 we test with two combinations: NTU RGB+D + MPII and\\r\\nthe hierarchical contrastive learning targets L and a cross-          NTU RGB+D + COCO. As for the backbone, we test with\\r\\nentropy loss L\\xe2\\x80\\xb2 for the supervision of the segmentation. For          HRNet-W18 and HRNet-W32. Compared with the base-\\r\\nthe \\xe2\\x80\\x98No Contrastive\\xe2\\x80\\x99 baseline, we only train with L\\xe2\\x80\\xb2 . As             line and two other state-of-the-art pre-train counterparts,\\r\\nfor the \\xe2\\x80\\x98CMC\\xe2\\x80\\x99 baseline, the network is supervised by both             our method outperforms them in most of the metrics. Espe-\\r\\nthe learning target proposed by CMC [45] L and the seg-               cially, our method has advantages in GPS and GPSM, which\\r\\nmentation loss L\\xe2\\x80\\xb2 . Note that, during the whole training              are two critical metrics for DensePose quality. Additionally,\\r\\ntime, including the CMC pre-train, the target modality of             we also report full results of the ablation study. The detailed\\r\\nNTURGBD-Parsing-4K is not exposed to better simulate                  results further validates the analysis in the main paper.\\r\\nthe application scenario. In order to build the connection            RGB Human Parsing. We further report detailed RGB hu-\\r\\nbetween RGB and depth during training time, we mix the                man parsing results on Human3.6M [22] that could not fit\\r\\nNTURGBD-Parsing-4K with NTU RGB+D which is the                        into the main paper. As shown in Tab. 10, we report the\\r\\nsame one used for our pre-train. At inference time, we at-            per-class IoU for all the settings reported in the main paper.\\r\\ntach the trained segmentation head to the mapper network              Similarly, for the initialization of the network, we test with\\r\\nof depth. Since the dense embeddings of RGB and depth                 the network randomly initialized (\\xe2\\x80\\x98From Scratch\\xe2\\x80\\x99) and the\\r\\nare aligned thanks to our hierarchical contrastive learning           network initialized by ImageNet pre-train (\\xe2\\x80\\x98IN Pre-train\\xe2\\x80\\x99).\\r\\ntargets, it is reasonable for the segmentation head to be able        As for the ratio of training data, we test with the full train-\\r\\nto handle the dense embeddings of depth.                              ing set, 20%, 10% and 1% of the training set. The pre-\\r\\n                                                                      train datasets are NTU RGB+D + MPII. In most classes,\\r\\n1.7. Missing-Modality Inference\\r\\n                                                                      our method outperforms comparison methods. Moreover,\\r\\n    We also use human parsing on NTURGBD-Parsing-4K                   we also report per-class IoU for the four settings in ablation\\r\\nto experiment with our extension of missing-modality in-              study, which are in line with our analysis in the main paper.\\r\\nference. The basic setup is the same as that of the cross-            Depth Human Parsing. We report detailed depth human\\r\\nmodality supervision experiments. At training time, we take           parsing results on NTURGBD-Parsing-4K. As shown in\\r\\nthe dense embeddings of both RGB and depth together for               Tab. 11, we report the per-class IoU for all the settings re-\\r\\na max pooling operation for a simple feature-level fusion.            ported in the main paper. We initialize the networks using\\r\\nThen the fused dense embedding is passed to a segmen-                 ImageNet pre-train. Two ratios of the training set, i.e. full\\r\\ntation header, which is the same one used by the human                and 20%, are tested. We also change the backbone to Point-\\r\\nparsing experiment, to produce the segmentation predic-               Net++ [38] (\\xe2\\x80\\x98PN++\\xe2\\x80\\x99). Since it is a point-based backbone,\\r\\ntion. The network is supervised with both the hierarchi-              the \\xe2\\x80\\x98background\\xe2\\x80\\x99 class is ignored and not included in the\\r\\ncal contrastive learning targets L and a cross-entropy loss           calculation of mIoU. The per-class IoU results also agree\\r\\nL\\xe2\\x80\\xb2 for segmentation supervision. Similarly, the \\xe2\\x80\\x98No con-              with the conclusion in the main paper that our method is\\r\\ntrastive\\xe2\\x80\\x99 baseline does not use any contrastive learning tar-         superior than other comparison methods.\\r\\ngets. The \\xe2\\x80\\x98CMC\\xe2\\x80\\x99 baseline uses the contrastive learning tar-           Cross-Modality Supervision. As shown in Tab. 12,\\r\\nget proposed in CMC [45] as L. At inference time, if RGB              we report detailed per-class IoU for the experiments of\\r\\nis missing, then the dense embedding of depth is passed to            cross-modality supervision. In both \\xe2\\x80\\x98RGB\\xe2\\x86\\x92Depth\\xe2\\x80\\x99 and\\r\\nthe trained segmentation header for prediction. Since the             \\xe2\\x80\\x98Depth\\xe2\\x86\\x92RGB\\xe2\\x80\\x99 settings, our method outperforms other\\r\\n\\r\\n\\r\\n                                                                 13\\r\\n\\x0c\\n\\nbaseline methods in all classes. Especially, other baseline\\r\\nmethods barely make correct predictions while ours makes\\r\\na huge improvement.\\r\\nMissing-Modality Inference. As shown in Tab. 12, we\\r\\nlist detailed per-class IoU for the experiments of missing-\\r\\nmodality inference. In both \\xe2\\x80\\x98Only RGB\\xe2\\x80\\x99 and \\xe2\\x80\\x98Only Depth\\xe2\\x80\\x99\\r\\nsettings, our method outperforms baseline methods in most\\r\\nclasses. Therefore, the detailed results further validates the\\r\\nconclusions made in the main paper.\\r\\n\\r\\n3. More Qualitative Results\\r\\n   More qualitative results of RGB human parsing on Hu-\\r\\nman3.6M [22] and depth human parsing on NTURGBD-\\r\\nParsing-4K are shown in Fig. 7, Fig 8 and Fig. 9. We\\r\\nchoose to visualize both the full training set and 10% train-\\r\\ning set for RGB human parsing. The segmentation results\\r\\nproduced by our pre-train model are superior than those of\\r\\nother comparison methods, especially in data-efficient set-\\r\\ntings. For challenging classes like hands and elbows, our\\r\\nmethod is capable of producing correct predictions con-\\r\\nstantly while other methods struggle. The depth map is a\\r\\nchallenging modality for the dense prediction task like se-\\r\\nmantic segmentation. Our method manages to produce rea-\\r\\nsonable predictions that are better than those of other com-\\r\\nparison methods.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 14\\r\\n\\x0c\\n\\n     Table 9. Detailed DensePose Estimation Results on COCO. \\xe2\\x80\\x98Ratio\\xe2\\x80\\x99 stands for the ratio of training data for downstream tasks transfer. * randomly initializes the model before\\r\\n     pre-training. \\xe2\\x80\\xa0 initializes the model by ImageNet pre-train before pre-training. \\xe2\\x80\\x98Ablation1\\xe2\\x80\\x99 is \\xe2\\x80\\x98Sample-level Mod-invariant\\xe2\\x80\\x99; \\xe2\\x80\\x98Abation2\\xe2\\x80\\x99 is \\xe2\\x80\\x98+ Hard Dense Intra-sample\\xe2\\x80\\x99; \\xe2\\x80\\x98Ablation3\\xe2\\x80\\x99\\r\\n     is \\xe2\\x80\\x98+ Soft Dense Intra-sample\\xe2\\x80\\x99; \\xe2\\x80\\x98Ablation4\\xe2\\x80\\x99 is \\xe2\\x80\\x98+ Sparse Structure-aware\\xe2\\x80\\x99. \\xe2\\x80\\xa1 uses HRNet-W32 while others all use HRNet-W18. All results in [%].\\r\\n\\r\\n                                                                 BBox                                                                GPS                                                                            GPSM                                                                                IoU\\r\\n     Methods         Pre-train Datasets   Ratio\\r\\n                                                   AP     AP50 AP75 APs           APm     APl      AP     AP50 AP75 APm           APl   AR        AR50 AR75 ARm           ARl      AP     AP50 AP75 APm           APl  AR         AR50 AR75 ARm           ARl      AP     AP50 AP75 APm           APl      AR     AR50 AR75 ARm           ARl\\r\\n     From Scratch         -               100%    57.27   85.44   61.37   28.31   53.64   72.46   62.03   89.93   69.57   58.29   62.89   69.00   93.05   76.06   60.78   69.55   63.61   91.89   74.56   57.18   64.71   69.24   95.05   79.36   59.72   69.88   65.88   93.86   77.95   58.43   67.04   71.11   96.12   82.97   60.35   71.83\\r\\n     CMC* [45]       NTURGBD+MPII         100%    57.27   85.44   61.37   28.31   53.64   72.46   62.03   89.93   69.57   58.29   62.89   69.00   93.05   76.06   60.78   69.55   63.61   91.89   74.56   57.18   64.71   69.24   95.05   79.36   59.72   69.88   65.88   93.86   77.95   58.43   67.04   71.11   96.12   82.97   60.35   71.83\\r\\n     MMV* [1]        NTURGBD+MPII         100%    60.33   86.82   66.23   30.37   57.29   75.59   64.97   91.10   72.99   59.59   65.90   71.57   93.80   79.00   61.42   72.25   65.66   92.22   77.31   57.81   66.81   70.80   94.96   81.77   59.72   71.54   66.96   93.80   79.14   59.73   67.97   71.83   95.72   83.68   61.42   72.52\\r\\n     Ours*           NTURGBD+MPII         100%    61.33   87.81   66.48   31.80   58.27   76.30   65.89   92.20   75.36   61.12   66.87   72.52   94.69   81.01   62.91   73.17   66.92   93.27   78.72   59.75   67.97   71.87   95.50   82.92   61.63   72.56   67.66   94.41   80.14   61.08   68.71   72.62   96.21   84.80   62.62   73.29\\r\\n     IN Pre-train         -               100%    62.66   89.28   68.47   35.78   59.59   76.47   66.49   92.11   75.47   64.45   67.43   73.41   94.87   81.28   66.24   73.89   67.42   93.20   79.72   62.11   68.52   72.63   95.90   83.82   63.97   73.20   68.63   94.64   81.81   62.56   69.72   73.45   96.30   85.64   64.11   74.08\\r\\n     CMC\\xe2\\x80\\xa0 [45]       NTURGBD+MPII         100%    62.76   88.32   68.32   33.67   60.22   76.95   66.17   92.45   74.79   62.79   66.96   72.75   94.69   80.34   64.68   73.29   67.31   93.51   79.71   61.55   68.27   72.23   95.68   83.73   63.55   72.80   68.07   94.68   81.41   61.89   69.01   72.90   96.30   85.51   63.62   73.53\\r\\n     MMV\\xe2\\x80\\xa0 [1]        NTURGBD+MPII         100%    62.97   88.75   68.91   34.62   60.83   76.87   66.67   92.42   76.24   63.09   67.64   73.03   94.78   81.28   64.04   73.63   67.51   93.44   79.72   60.91   68.50   72.29   95.81   83.10   61.99   72.97   68.29   94.52   81.84   61.04   69.20   72.95   96.34   85.42   62.20   73.67\\r\\n     Ours\\xe2\\x80\\xa0           NTURGBD+MPII         100%    63.11   88.66   69.64   34.53   60.80   77.01   67.33   93.20   76.27   62.63   68.20   73.77   95.23   81.59   63.83   74.44   68.12   94.09   79.90   61.35   68.99   72.94   96.08   83.95   62.48   73.64   68.72   94.74   81.67   61.50   69.77   73.47   96.26   85.73   62.70   74.19\\r\\n\\r\\n     CMC\\xe2\\x80\\xa0 [45]       NTURGBD+COCO 100% 63.58 88.94 69.69 35.24 61.37 77.46 67.22 92.68 76.27 64.61 68.20 73.75 95.10 81.59 65.60 74.29 67.77 93.65 79.56 62.62 68.81 72.78 95.99 83.55 63.83 73.38 68.46 93.93 81.80 62.60 69.50 73.37 95.94 85.78 63.76 74.02\\r\\n     Ours\\xe2\\x80\\xa0           NTURGBD+COCO 100% 62.95 88.78 68.83 34.77 60.59 77.02 67.77 93.18 77.13 64.02 68.63 74.15 95.23 82.39 65.18 74.75 68.29 93.60 80.53 62.77 69.23 73.21 95.99 84.44 64.11 73.81 68.63 94.53 81.53 62.36 69.51 73.30 96.17 85.69 63.33 73.97\\r\\n     From Scratch         -               10%     39.38   72.29   37.98   12.03   35.59   55.16   35.75   73.78   30.07   27.19   37.28   45.32   81.41   43.69   31.49   46.25   41.62   80.25   38.81   30.71   43.33   49.67   87.34   50.74   35.60   50.61   49.92   85.96   54.21   38.56   51.61   57.90   91.62   65.14   43.19   58.88\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n15\\r\\n     CMC* [45]       NTURGBD+MPII         10%     44.92   76.41   46.34   16.09   41.58   60.88   43.84   79.94   42.97   40.31   45.00   52.25   85.69   54.21   42.55   52.90   47.94   84.22   50.23   41.42   49.33   55.00   89.43   59.61   43.90   55.74   54.00   88.65   61.20   46.16   55.45   60.96   92.73   69.82   48.94   61.77\\r\\n     MMV* [1]        NTURGBD+MPII         10%     43.24   74.53   44.27   13.49   40.43   59.22   41.40   77.54   39.26   34.54   42.93   50.45   84.22   51.67   37.02   51.35   45.99   81.83   48.11   37.22   47.64   53.58   88.19   58.18   39.72   54.51   52.52   87.73   59.10   43.45   54.01   59.80   92.15   68.26   46.17   60.71\\r\\n     Ours*           NTURGBD+MPII         10%     47.76   78.66   50.56   18.59   45.19   63.12   48.47   82.74   51.27   45.21   49.51   56.41   87.56   61.21   47.73   56.99   51.65   85.50   56.06   45.86   52.76   58.27   90.33   64.20   48.44   58.91   56.15   89.63   64.34   49.27   57.43   62.92   93.36   72.54   51.13   63.71\\r\\n     IN Pre-train         -               10%     48.29   80.07   50.90   19.55   44.54   63.64   44.34   78.77   44.83   36.85   46.49   54.54   86.49   57.38   39.29   55.56   49.11   84.02   51.91   40.57   51.38   57.78   90.82   63.17   43.40   58.73   56.11   88.10   63.14   46.91   58.16   63.73   92.56   72.94   49.50   64.69\\r\\n     CMC\\xe2\\x80\\xa0 [45]       NTURGBD+MPII         10%     49.21   80.43   52.01   21.66   47.22   63.71   48.82   83.85   51.39   43.97   50.05   57.21   88.72   61.35   46.67   57.92   52.57   86.67   57.21   45.69   53.89   59.52   91.26   66.21   48.65   60.25   57.94   90.82   66.11   50.11   59.30   64.80   94.07   74.50   52.34   65.64\\r\\n     MMV\\xe2\\x80\\xa0 [1]        NTURGBD+MPII         10%     50.16   81.33   53.68   22.17   47.73   64.76   50.28   84.02   53.95   44.06   51.68   58.43   89.03   63.40   45.74   59.28   53.54   86.66   59.25   44.91   55.06   60.35   91.22   67.59   46.74   61.25   58.32   90.15   67.10   49.21   59.89   64.94   93.62   74.90   50.92   65.88\\r\\n     Ours\\xe2\\x80\\xa0           NTURGBD+MPII         10%     50.29   80.94   53.62   22.74   47.91   65.25   51.50   84.89   54.64   45.63   52.68   59.03   89.17   63.58   47.80   59.78   54.47   87.13   60.26   46.82   55.71   60.66   91.44   67.99   49.29   61.42   58.66   90.32   68.82   50.63   59.93   64.94   93.89   75.57   52.48   65.77\\r\\n\\r\\n     CMC\\xe2\\x80\\xa0 [45]       NTURGBD+COCO         10%     51.77 81.86 55.93 23.64 49.54 66.50 53.53 86.23 58.30 46.79 54.78 60.81 90.15 66.47 50.21 61.52 56.18 88.58 64.41 48.38 57.39 62.35 92.33 71.73 52.13 63.04 59.37 90.74 68.92 52.20 60.62 65.83 93.85 76.42 54.68 66.58\\r\\n     Ours\\xe2\\x80\\xa0           NTURGBD+COCO         10%     52.18 82.71 56.57 24.92 49.96 66.86 54.01 86.41 58.31 48.15 55.14 61.53 90.55 67.10 50.99 62.24 56.64 88.67 64.79 48.78 57.88 63.05 92.69 72.18 51.70 63.81 59.93 91.31 69.99 52.01 61.08 66.50 94.34 77.40 54.54 67.29\\r\\n     Ablation1       NTURGBD+MPII         10%     49.21   80.43   52.01   21.66   47.22   63.71   48.82   83.85   51.39   43.97   50.05   57.21   88.72   61.35   46.67   57.92   52.57   86.67   57.21   45.69   53.89   59.52   91.26   66.21   48.65   60.25   57.94   90.82   66.11   50.11   59.30   64.80   94.07   74.50   52.34   65.64\\r\\n     Ablation2       NTURGBD+MPII         10%     49.40   80.93   51.61   21.94   47.56   63.72   49.14   82.17   51.83   44.53   50.43   57.65   87.83   62.42   46.88   58.37   52.49   85.69   58.42   46.56   53.84   59.61   90.68   66.87   49.15   60.31   57.30   89.93   65.38   50.58   58.74   64.29   93.80   73.96   53.12   65.03\\r\\n     Ablation3       NTURGBD+MPII         10%     50.21   81.02   53.39   22.86   47.78   64.75   50.25   84.20   53.58   44.74   51.41   57.95   88.50   62.91   47.66   58.64   53.42   86.41   58.95   46.27   54.68   59.85   90.68   66.87   49.01   60.56   57.70   89.75   67.40   50.95   58.89   64.40   93.31   74.72   52.91   65.17\\r\\n     Ablation4       NTURGBD+MPII         10%     50.29   80.94   53.62   22.74   47.91   65.25   51.50   84.89   54.64   45.63   52.68   59.03   89.17   63.58   47.80   59.78   54.47   87.13   60.26   46.82   55.71   60.66   91.44   67.99   49.29   61.42   58.66   90.32   68.82   50.63   59.93   64.94   93.89   75.57   52.48   65.77\\r\\n\\r\\n     IN Pre-train\\xe2\\x80\\xa1        -               10%     55.10 84.95 59.81 27.33 52.01 69.65 54.60 86.42 59.80 48.63 55.93 62.48 90.68 68.12 50.43 63.29 57.60 88.62 66.39 49.61 59.15 64.09 92.55 72.49 51.63 64.92 61.73 91.74 72.52 53.24 63.10 67.99 94.69 78.73 55.11 68.86\\r\\n     CMC\\xe2\\x80\\xa1 [45]       NTURGBD+MPII         10%     53.88 83.61 58.47 26.16 51.24 68.64 54.62 86.83 58.93 47.45 55.86 62.05 90.73 67.19 49.01 62.93 57.46 88.57 65.55 49.59 58.71 63.57 92.33 72.05 51.21 64.39 61.14 91.65 72.58 53.46 62.39 67.21 94.25 78.73 54.96 68.03\\r\\n     Ours\\xe2\\x80\\xa1           NTURGBD+MPII         10%     54.55 83.77 58.83 26.94 52.56 68.78 55.80 87.37 61.22 51.83 56.75 62.70 90.91 68.48 53.12 63.34 58.36 89.29 67.31 52.03 59.39 64.11 92.87 73.52 53.55 64.82 61.75 91.59 72.79 54.77 62.87 67.72 94.38 79.00 56.38 68.48\\r\\n\\x0c\\n\\n     Table 10. Detailed Human Parsing Results on Human3.6M. \\xe2\\x80\\x98Ratio\\xe2\\x80\\x99 stands for the ratio of training data for downstream tasks transfer. * randomly initializes the model before\\r\\n     pre-training. \\xe2\\x80\\xa0 initializes the model by ImageNet pre-train before pre-training. \\xe2\\x80\\x98Ablation1\\xe2\\x80\\x99 is \\xe2\\x80\\x98Sample-level Mod-invariant\\xe2\\x80\\x99; \\xe2\\x80\\x98Abation2\\xe2\\x80\\x99 is \\xe2\\x80\\x98+ Hard Dense Intra-sample\\xe2\\x80\\x99; \\xe2\\x80\\x98Ablation3\\xe2\\x80\\x99\\r\\n     is \\xe2\\x80\\x98+ Soft Dense Intra-sample\\xe2\\x80\\x99; \\xe2\\x80\\x98Ablation4\\xe2\\x80\\x99 is \\xe2\\x80\\x98+ Sparse Structure-aware\\xe2\\x80\\x99. All results in [%].\\r\\n\\r\\n     Methods        Ratio    bg     right hip right knee right foot left hip left knee left foot left shoulder left elbow left hand right shoulder right elbow right hand crotch right thigh right calf left thigh left calf lower spine upper spine   head    left arm left forearm right arm right forearm mIoU\\r\\n     From Scratch   100%    99.53    29.66      32.28      52.94    25.86     30.71    49.59       35.14        20.78     29.46        34.33         19.01       30.61    37.24     54.53      65.77      54.35     64.12      50.56       57.39       77.57   40.71      35.83      38.14       36.99     44.13\\r\\n     CMC* [45]      100%    99.55    44.95      48.96      54.05    37.57     41.55    53.41       47.98        33.35     37.05        46.01         32.28       38.18    50.81     64.84      69.95      62.22     68.16      67.76       71.78       81.23   56.32      46.75      55.92       47.53     54.33\\r\\n     MMV* [1]       100%    99.54    39.92      45.88      53.18    34.04     34.77    52.97       45.35        31.22     34.10        45.08         31.92       35.26    53.11     63.96      69.41      60.81     66.78      67.53       72.35       79.96   55.63      43.48      56.07       44.99     52.69\\r\\n     Ours*          100%    99.61    51.09      58.38      61.22    40.81     48.02    59.65       54.21        44.75     48.57        52.78         43.66       48.68    56.70     71.43      75.89      67.98     73.73      71.63       76.79       83.89   64.64      57.51      64.47       57.97     61.36\\r\\n     From Scratch   20%     99.52    29.07      32.15      50.17    23.11     24.90    48.12       34.98        17.26     23.58        33.20         17.75       25.39    34.76     54.76      65.22      51.44     62.03      55.16       60.63       78.27   39.47      30.30      37.31       31.84     42.41\\r\\n     CMC* [45]      20%     99.53    41.21      42.49      49.99    39.44     39.37    49.24       46.58        30.02     33.19        45.35         30.08       34.48    53.06     62.33      66.06      61.24     64.88      67.02       71.59       80.35   54.80      42.67      54.03       43.61     52.10\\r\\n     MMV* [1]       20%     99.51    38.19      43.71      49.45    34.44     32.47    49.59       42.88        28.15     30.74        43.23         28.81       32.13    52.26     62.83      66.82      59.55     63.94      66.44       71.66       79.42   54.10      40.48      54.03       41.73     50.66\\r\\n     Ours*          20%     99.59    46.76      51.17      56.35    43.52     48.66    55.20       53.79        42.58     43.76        52.41         42.29       44.58    60.19     68.57      71.95      67.80     70.67      70.56       75.74       82.68   62.90      52.71      62.44       52.48     59.17\\r\\n     From Scratch   10%     99.39    18.84      19.95      35.96    19.55     15.23    32.38       23.23        12.43     15.57        20.73         12.46       18.64    28.46     40.41      45.51      37.85     42.10      51.25       58.41       74.93   25.77      20.47      23.29       22.50     32.61\\r\\n     CMC* [45]      10%     99.49    35.71      37.05      44.08    36.65     33.37    42.36       44.43        23.56     29.01        43.30         24.51       30.05    52.29     60.05      60.32      58.81     58.03      65.01       70.68       79.19   52.11      37.89      51.56       39.64     48.37\\r\\n     MMV* [1]       10%     99.46    33.84      34.77      42.58    33.47     27.94    41.83       37.89        21.06     26.17        38.65         23.36       27.40    50.05     58.14      59.71      56.35     57.01      64.47       69.99       77.38   50.43      35.00      51.34       37.35     46.23\\r\\n     Ours*          10%     99.56    42.65      48.60      52.82    44.08     45.44    51.58       52.21        40.08     41.70        51.70         40.35       42.73    59.06     66.85      69.09      66.78     67.47      68.73       74.82       81.39   61.15      49.08      60.39       48.64     57.08\\r\\n     From Scratch   1%      98.39    0.00       0.00       0.00      0.00     0.00      0.00        0.00        0.00       0.00         0.00         0.00        0.00      0.00     18.12       0.00      15.94      0.00      18.71       30.70        0.00    0.00       0.00       0.00        0.00      7.27\\r\\n     CMC* [45]      1%      98.95    0.00       0.00       4.08      0.00     0.00      2.69        0.00        0.00       0.00         0.00         0.00        0.00      7.38     22.13      20.40      22.19     17.72      48.99       52.65       68.09    0.00       0.00       0.00        0.00     14.61\\r\\n     MMV* [1]       1%      98.62    0.00       0.00       0.00      0.00     0.00      0.00        0.00        0.00       0.00         0.00         0.00        0.00      0.00     22.03      12.54      18.33      9.34      46.48       54.31       59.75    0.00       0.00       0.00        0.00     12.86\\r\\n     Ours*          1%      99.02    0.00       0.00       9.95      0.00     0.00      8.70        0.00        0.00       0.00         0.00         0.00        0.00      9.97     26.15      25.71      24.97     24.00      57.31       58.61       69.24    0.00       0.00       0.00        0.00     16.55\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n16\\r\\n     IN Pre-train   100%    99.60    37.98      52.94      56.49    32.24     46.97    56.29       46.05        47.55     42.50        42.81         47.05       42.88    46.57     66.69      75.49      64.51     74.23      61.85       67.29       82.69   58.41      57.98      56.56       58.90     56.90\\r\\n     CMC\\xe2\\x80\\xa0 [45]      100%    99.60    44.36      51.60      57.77    41.26     51.46    54.51       53.20        42.49     42.28        51.82         41.57       41.95    55.85     68.72      72.62      67.34     72.18      69.21       74.36       82.78   63.95      54.88      63.38       54.21     58.93\\r\\n     MMV\\xe2\\x80\\xa0 [1]       100%    99.60    44.58      54.97      59.49    40.01     48.30    56.47       51.56        43.01     43.60        50.11         42.12       42.97    56.61     69.42      74.69      67.35     72.90      68.54       73.46       82.52   63.02      55.04      62.54       54.13     59.08\\r\\n     Ours\\xe2\\x80\\xa0          100%    99.62    50.34      57.06      60.78    42.96     50.52    57.75       56.66        48.56     48.11        56.50         47.64       47.86    60.06     72.46      75.23      70.20     73.58      73.16       78.24       84.36   66.48      58.91      66.96       58.60     62.50\\r\\n     IN Pre-train   20%     99.53    36.33      37.57      49.56    32.73     33.11    47.01       42.29        29.79     34.31        39.52         29.84       33.40    46.71     55.73      60.20      54.27     58.36      64.67       68.74       81.83   49.21      43.69      48.51       44.49     48.86\\r\\n     CMC\\xe2\\x80\\xa0 [45]      20%     99.58    44.54      49.05      54.20    42.95     47.77    52.52       50.89        40.65     40.08        49.67         39.46       39.85    56.26     68.09      70.69      66.45     69.76      69.23       73.95       81.87   62.20      52.63      61.35       51.45     57.41\\r\\n     MMV\\xe2\\x80\\xa0 [1]       20%     99.58    44.94      51.80      56.68    38.96     45.12    53.29       50.16        40.35     40.29        48.96         39.99       39.31    56.14     68.21      72.31      66.00     70.07      68.50       73.38       81.62   61.54      52.27      61.23       51.28     57.28\\r\\n     Ours\\xe2\\x80\\xa0          20%     99.60    48.34      54.49      58.57    43.35     47.57    55.65       55.46        45.50     46.55        55.34         44.65       46.47    61.25     71.12      73.61      69.45     71.37      71.44       76.74       83.52   64.86      56.40      64.23       55.73     60.85\\r\\n     IN Pre-train   10%     99.52    31.06      30.86      42.62    31.49     28.61    39.59       40.24        24.58     28.15        38.36         25.25       26.65    47.38     50.07      53.83      49.90     52.64      63.70       68.21       80.72   44.98      35.24      44.19       35.80     44.55\\r\\n     CMC\\xe2\\x80\\xa0 [45]      10%     99.55    38.50      45.93      48.87    42.27     42.37    47.59       48.30        37.07     36.52        47.46         36.72       36.07    55.22     65.35      66.68      65.23     64.92      67.31       72.76       81.25   59.06      48.16      58.33       47.14     54.35\\r\\n     MMV\\xe2\\x80\\xa0 [1]       10%     99.54    38.12      45.86      50.23    37.23     40.96    48.24       47.57        36.17     36.33        46.99         36.13       35.34    55.36     64.36      66.91      63.40     64.91      67.40       72.44       80.88   58.72      47.61      58.71       46.98     53.86\\r\\n     Ours\\xe2\\x80\\xa0          10%     99.57    43.01      49.27      53.44    46.22     46.13    52.20       53.68        40.73     43.25        54.63         41.18       43.80    59.88     67.81      69.59      69.08     67.91      69.02       75.36       82.30   62.99      51.10      63.35       51.42     58.28\\r\\n     IN Pre-train   1%      99.07    0.00       0.00        8.97     0.00     0.00      6.92        0.00        0.00       0.00         0.00         0.00        0.00      0.00     19.38      24.79      19.34     23.11      43.39       48.80       72.37   0.00        0.00      0.00         0.00     14.65\\r\\n     CMC\\xe2\\x80\\xa0 [45]      1%      99.08    0.00       0.00       17.72     0.00     0.00     14.43        0.00        0.00       0.00         0.00         0.00        0.00     33.14     26.55      24.95      25.94     22.73      48.98       52.21       72.88   2.65        0.00      2.99         0.00     17.77\\r\\n     MMV\\xe2\\x80\\xa0 [1]       1%      99.10    0.00       0.00       12.83     0.00     0.00     10.97        0.00        0.00       0.00         0.00         0.00        0.00     23.62     26.53      24.72      26.40     24.54      51.18       53.27       72.29   8.15        0.00      7.96         0.00     17.66\\r\\n     Ours\\xe2\\x80\\xa0          1%      99.15    0.00       0.00       18.29     0.00     0.00     16.99        0.07        0.00       0.00         0.07         0.00        0.00     46.56     28.73      27.59      29.13     26.38      58.59       61.46       74.41   16.32       0.00      15.65        0.00     20.78\\r\\n     Ablation1      10%     99.55    38.50      45.93      48.87    42.27     42.37    47.59       48.30        37.07     36.52        47.46         36.72       36.07    55.22     65.35      66.68      65.23     64.92      67.31       72.76       81.25   59.06      48.16      58.33       47.14     54.35\\r\\n     Ablation2      10%     99.56    40.06      45.94      50.88    40.18     43.91    49.06       50.41        36.73     37.65        49.50         37.33       36.94    57.19     66.32      68.69      67.28     66.98      68.08       73.37       81.95   60.25      48.45      59.80       47.54     55.36\\r\\n     Ablation3      10%     99.57    44.49      46.38      52.15    44.00     43.85    50.25       51.73        37.19     38.12        51.30         37.24       37.61    57.38     67.16      69.08      66.91     67.32      69.42       74.45       82.12   61.36      49.91      60.69       48.97     56.35\\r\\n     Ablation4      10%     99.57    43.01      49.27      53.44    46.22     46.13    52.20       53.68        40.73     43.25        54.63         41.18       43.80    59.88     67.81      69.59      69.08     67.91      69.02       75.36       82.30   62.99      51.10      63.35       51.42     58.28\\r\\n\\x0c\\n\\n     Table 11. Detailed Human Parsing Results on NTURGBD-Parsing-4K. \\xe2\\x80\\x98Ratio\\xe2\\x80\\x99 stands for the ratio of training data for downstream tasks transfer. * randomly initializes the model\\r\\n     before pre-training. \\xe2\\x80\\xa0 initializes the model by ImageNet pre-train before pre-training. \\xe2\\x80\\x98Ablation1\\xe2\\x80\\x99 is \\xe2\\x80\\x98Sample-level Mod-invariant\\xe2\\x80\\x99; \\xe2\\x80\\x98Abation2\\xe2\\x80\\x99 is \\xe2\\x80\\x98+ Hard Dense Intra-sample\\xe2\\x80\\x99;\\r\\n     \\xe2\\x80\\x98Ablation3\\xe2\\x80\\x99 is \\xe2\\x80\\x98+ Soft Dense Intra-sample\\xe2\\x80\\x99; \\xe2\\x80\\x98Ablation4\\xe2\\x80\\x99 is \\xe2\\x80\\x98+ Sparse Structure-aware\\xe2\\x80\\x99. All results in [%].\\r\\n\\r\\n     Methods                Ratio    bg     right hip right knee right foot left hip left knee left foot left shoulder left elbow left hand right shoulder right elbow right hand crotch right thigh right calf left thigh left calf lower spine upper spine    head    left arm left forearm right arm right forearm mIoU\\r\\n     IN Pre-train           100%    99.24    21.99      19.79      39.62    23.95     20.98    39.61       22.61        14.14     22.11        23.05         12.24       22.52    25.66     47.00      46.85      46.81     48.53      53.26       61.51        61.11    43.45     36.13      46.50       38.56     37.49\\r\\n     CMC\\xe2\\x80\\xa0 [45]              100%    99.26    22.50      19.49      40.15    24.81     20.65    39.96       24.64        14.68     21.79        25.50         13.04       23.46    26.12     48.93      46.21      49.28     47.68      54.26       62.52        59.24    45.02     37.22      48.32       40.25     38.20\\r\\n     MMV\\xe2\\x80\\xa0 [1]               100%    99.23    22.64      19.68      39.02    24.66     21.38    38.77       24.40        13.92     22.36        25.15         12.83       23.65    25.78     47.87      46.62      48.12     48.45      53.32       62.46        59.89    45.03     37.80      48.17       40.98     38.09\\r\\n     Ours\\xe2\\x80\\xa0                  100%    99.32    22.95      21.25      41.26    25.70     22.59    40.99       24.47        15.17     23.61        25.11         14.26       24.87    25.75     49.46      47.90      49.97     49.88      54.45       62.88        61.97    47.24     39.06      50.48       42.29     39.32\\r\\n     IN Pre-train           20%     99.13    12.39      17.07      31.82    14.08     19.28    32.72       13.68         2.61     12.58        14.59          2.96       10.87    18.16     35.01      33.36      38.41     35.82      46.81       54.45        57.49    31.95     23.39      32.65       22.60     28.56\\r\\n     CMC\\xe2\\x80\\xa0 [45]              20%     98.98    13.16      14.06      29.82    16.28     16.13    30.99       17.65        10.09     14.69        17.99         8.23        15.92    18.08     38.79      34.57      41.26     36.41      46.21       54.68        54.60    37.80     26.75      39.11       27.66     30.40\\r\\n     MMV\\xe2\\x80\\xa0 [1]               20%     99.03    12.92      16.15      30.81    15.62     18.80    32.06       17.75         7.46     15.27        18.73          5.52       15.72    19.48     38.29      34.88      40.95     37.82      45.78       54.42        55.21    36.83     25.66      37.85       27.15     30.41\\r\\n     Ours\\xe2\\x80\\xa0                  20%     99.40    12.18      18.25      36.74    15.35     21.15    38.91       18.78        11.99     20.92        20.31         10.99       20.84    18.05     44.73      44.45      47.86     46.87      51.41       59.57        65.76    42.43     31.07      44.67       32.65     35.01\\r\\n     Ablation1              20%     99.13    12.39      17.07      31.82    14.08     19.28    32.72       13.68         2.61     12.58        14.59          2.96       10.87    18.16     35.01      33.36      38.41     35.82      46.81       54.45        57.49    31.95     23.39      32.65       22.60     28.56\\r\\n     Ablation2              20%     99.12    12.85      13.79      32.69    16.99     16.20    33.29       17.38        8.73      15.12        19.13         7.67        16.71    19.07     39.46      36.15      42.08     38.08      48.69       55.53        58.94    38.49     27.62      40.04       27.55     31.26\\r\\n     Ablation3              20%     99.22    13.40      14.18      31.37    16.46     17.71    32.88       19.72        9.33      16.13        20.19         8.95        17.49    17.48     39.12      37.41      41.55     39.13      49.67       58.23        61.41    39.71     30.95      41.16       32.13     32.20\\r\\n     Ablation4              20%     99.40    12.18      18.25      36.74    15.35     21.15    38.91       18.78        11.99     20.92        20.31         10.99       20.84    18.05     44.73      44.45      47.86     46.87      51.41       59.57        65.76    42.43     31.07      44.67       32.65     35.01\\r\\n     From Scratch w/ PN++   20%       -      21.43      30.84      67.53    21.52     29.85    66.76       32.82        23.17     38.37        36.74         23.42       36.23    25.19     55.25      60.25      55.11     60.37      53.34       65.85        88.22    50.77     47.05      52.70       45.88     45.36\\r\\n     CMC* [45] w/ PN++      20%       -      24.12      32.89      73.11    23.84     32.67    73.20       33.43        27.55     44.62        38.40         27.59       42.11    26.55     57.82      65.60      57.73     65.02      54.53       66.31        89.16    55.04     51.00      56.66       50.82     48.74\\r\\n     Ours* w/ PN++          20%       -      23.96      32.90      73.30    24.16     32.44    73.10       34.81        29.54     45.43        37.79         28.16       42.89    27.83     58.25      66.16      58.64     65.51      55.60       66.92        89.51    56.33     53.05      57.87       52.29     49.43\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n17\\r\\n                                          Table 12. Detailed Cross-Modality Supervision and Missing-Modality Inference Results on NTURGBD-Parsing-4K. All results in [%].\\r\\n\\r\\n     Methods           Setting       bg     right hip right knee right foot left hip left knee left foot left shoulder left elbow left hand right shoulder right elbow right hand crotch right thigh right calf left thigh left calf lower spine upper spine head left arm left forearm right arm right forearm mIoU\\r\\n     No Contrastive RGB\\xe2\\x86\\x92Depth 92.87           0.00       0.00       0.00     0.00      0.00     0.00        0.00        0.00       0.22        0.00          0.00        1.08      0.00     0.00       0.25       0.00     0.00       0.00         0.00         2.62     0.00     0.45        0.00       1.11       3.94\\r\\n     CMC [45]       RGB\\xe2\\x86\\x92Depth 89.79           0.00       0.00       0.00     0.00      0.02     0.01        0.00        1.51       0.79        0.00          0.11        0.43      0.00     0.00       0.00       0.01     0.00       1.02         0.20         0.42     1.64     0.37        0.03       0.14       3.86\\r\\n     Ours           RGB\\xe2\\x86\\x92Depth 96.78          12.50      23.26      37.60    16.45     21.43    40.51       22.41       19.59      22.86       21.35         17.19       25.67     17.40    36.48      35.62      37.43    34.04      49.83        59.37        61.07    33.91    24.55       36.15      26.37      33.19\\r\\n     No Contrastive Depth\\xe2\\x86\\x92RGB 91.79           0.00       0.00       0.00     0.00      0.00     0.00        0.00        0.00       0.23        0.00          0.00        0.07      0.00     0.01       0.00       0.00     0.00       0.10         0.00         0.02     0.00     0.50        0.00       0.02       3.71\\r\\n     CMC [45]       Depth\\xe2\\x86\\x92RGB 91.96           0.00       0.00       0.00     0.00      0.00     0.01        0.00        0.68       0.32        0.00          0.00        0.21      0.00     0.00       0.00       0.00     0.00       0.46         0.23         0.05     0.49     1.84        0.00       0.01       3.85\\r\\n     Ours           Depth\\xe2\\x86\\x92RGB 95.15          13.70      16.04      28.70    16.59     12.15    28.12       18.11       12.78      10.51       21.06         11.12       14.46     11.02    33.71      30.90      34.03    22.93      42.62        50.99        56.12    23.51    19.89       27.29      18.50      26.80\\r\\n     No Contrastive   Only RGB      93.55     8.97       6.66       0.42     4.28      0.75     0.02        0.98        1.19      15.22        0.28          2.69       23.56      0.08     0.30      12.35       0.07     0.88      25.48         5.27        57.54    12.63    26.38        7.73      28.90      13.45\\r\\n     CMC [45]         Only RGB      93.80     0.00      11.94      47.69     0.00     12.00    38.76       21.43        0.01       9.58       24.32         13.56       15.38      1.15     1.84      32.30       1.07    18.12       0.59         3.13        36.86    27.04    15.63       42.37      21.90      19.62\\r\\n     Ours             Only RGB      97.80    29.12      27.49      57.93    27.09     26.41    57.31       28.22       27.62      32.70       29.51         26.16       33.41     25.66    52.42      49.60      52.38    54.91      52.40        52.75        75.69    47.98    41.69       50.68      40.08      43.88\\r\\n     No Contrastive   Only Depth    96.46    27.81       7.16      1.46     33.36     10.60    2.30        28.64       4.72       1.27        11.49          0.11        7.86     26.16    33.67      39.55      33.21    27.40      46.05        63.16        47.50    25.38     9.99       19.89       5.04      24.41\\r\\n     CMC [45]         Only Depth    94.81     7.25       1.08      0.06     6.82      0.05     0.10        25.47        3.11      2.95        20.80          0.39       2.73      26.79    17.11      17.84      33.96    4.58       10.64        11.32        37.01    41.41    11.08       33.69      3.52       16.58\\r\\n     Ours             Only Depth    97.89    23.09      32.97      54.55    26.44     32.55    57.28       34.08       19.83      25.23       32.33         21.33       28.71     30.57    54.91      59.33      53.16    59.89      56.50        65.09        61.87    49.43    36.47       50.34      35.75      43.98\\r\\n\\x0c\\n\\nRGB             GT                   IN               CMC                 MMV                 Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  Figure 7. Qualitative Results of RGB Human Parsing on Human3.6M with 10% of the Training Set.\\r\\n\\r\\n\\r\\n\\r\\n                                               18\\r\\n\\x0c\\n\\nRGB                GT                  IN                 CMC                MMV                    Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      Figure 8. Qualitative Results of RGB Human Parsing on Human3.6M with the Full Training Set.\\r\\n\\r\\n\\r\\n\\r\\n                                                  19\\r\\n\\x0c\\n\\nDepth               GT                  IN                CMC                MMV                  Ours\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n Figure 9. Qualitative Results of Depth Human Parsing on NTURGBD-Parsing-4K with the Full Training Set.\\r\\n\\r\\n\\r\\n\\r\\n                                                  20\\r\\n\\x0c', b\"                                              Eur. Phys. J. A manuscript No.\\r\\n                                              (will be inserted by the editor)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          Femtoscopic study on DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97 interactions for Tcc and X(3872) a\\r\\n                                          Yuki Kamiyab,1,2 , Tetsuo Hyodoc,3,2 , Akira Ohnishid,4\\r\\n                                          1\\r\\n                                            Helmholtz Institut fu\\xcc\\x88r Strahlen- und Kernphysik and Bethe Center for Theoretical Physics, Universita\\xcc\\x88t Bonn, D-53115 Bonn, Germany\\r\\n                                          2\\r\\n                                            RIKEN Interdisciplinary Theoretical and Mathematical Science Program (iTHEMS), Wako 351-0198, Japan\\r\\n                                          3\\r\\n                                            Department of Physics, Tokyo Metropolitan University, Hachioji 192-0397, Japan\\r\\n                                          4\\r\\n                                            Yukawa Institute for Theoretical Physics, Kyoto University, Kyoto 606-8502, Japan\\r\\narXiv:2203.13814v1 [hep-ph] 25 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          Received: date / Accepted: date\\r\\n\\r\\n\\r\\n                                          Abstract We investigate DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97 momentum corre-                     valence quark components (ccu\\xcc\\x84d).   \\xc2\\xaf Although the X(3872)\\r\\n                                          lations in high-energy collisions to elucidate the nature of             and Tcc are in different sectors, there is one similarity be-\\r\\n                                          Tcc and X(3872) exotic hadrons. Single range Gaussian po-                tween them, i.e., the existence of a nearby two-meson thresh-\\r\\n                                          tentials with the channel couplings to the isospin partners              old. Namely, the Tcc peak is also found just below the DD\\xe2\\x88\\x97\\r\\n                                          are constructed based on the empirical data. The momen-                  threshold. The proximity with the DD\\xcc\\x84\\xe2\\x88\\x97 and DD\\xe2\\x88\\x97 thresholds\\r\\n                                          tum correlation functions of the D0 D\\xe2\\x88\\x97+ , D+ D\\xe2\\x88\\x970 , D0 D\\xcc\\x84\\xe2\\x88\\x970 ,             would imply the molecular nature of these states. It should\\r\\n                                          and D+ D\\xe2\\x88\\x97\\xe2\\x88\\x92 pairs are computed with including the coupled-                be noted, however, that the structure of X(3872) is still under\\r\\n                                          channel effects. We discuss how the nature of the exotic                 debate. In the study of Ref. [10], it is shown that the con-\\r\\n                                          states are reflected in the behaviors of the correlation results.        tribution of the cc\\xcc\\x84 component is important by the analysis\\r\\n                                                                                                                   of the prompt production cross section. On the other hand,\\r\\n                                          PACS 25.75.Gz \\xc2\\xb7 21.30.Fe \\xc2\\xb7 13.75.Lb \\xc2\\xb7 14.40.Rt\\r\\n                                                                                                                   the enhancement of the production yield in AA collisions\\r\\n                                                                                                                   observed in CMS [11] seems to imply that X(3872) con-\\r\\n                                                                                                                   tains a significant fraction of the hadronic molecule compo-\\r\\n                                          1 Introduction                                                           nent [12]. In order to discriminate the possible structures of\\r\\n                                                                                                                   X(3872), it is desirable to experimentally access the DD\\xcc\\x84\\xe2\\x88\\x97\\r\\n                                          The study of various exotic resonances in heavy quark sec-               interaction.\\r\\n                                          tors has been one of the most interesting subjects in recent\\r\\n                                          hadron physics [1, 2, 3]. The most extensively studied state                 For the study of the near-threshold resonances, the fem-\\r\\n                                          is the X(3872) lying just below the DD\\xcc\\x84\\xe2\\x88\\x97 threshold, which                toscopy using the two-particle momentum correlation func-\\r\\n                                          is listed as \\xcf\\x87c1 (3872) in the current PDG paper [4]. Ever               tion in high-energy collisions is a helpful technique because\\r\\n                                          since its first observation in 2003 [5], this exotic hadron has          the correlation function is sensitive to the low-energy hadron\\r\\n                                          attracted huge interest of researchers and a bunch of the ex-            interactions. With the femtoscopy, various interactions in the\\r\\n                                          perimental and theoretical studies have been devoted to un-              strangeness sector have been investigated theoretically [13,\\r\\n                                          derstand this state. Nevertheless, its nature still remains to           14, 15, 16, 17, 18, 19, 20, 21] and experimentally [22, 23, 24, 25,\\r\\n                                          be elucidated.                                                           26, 27, 28, 29, 30, 31, 32, 33]. It turns out that the source size\\r\\n                                               Recently, the LHCb Collaboration reported a clear sig-              dependence of the correlation function is useful to distin-\\r\\n                                          nal of the doubly charmed tetraquark state Tcc   +\\r\\n                                                                                              in the mass          guish the existence or non-existence of hadronic bound states [19,\\r\\n                                                            0 0 +\\r\\n                                          spectrum of D D \\xcf\\x80 [6, 7]. Such exotic states with two                    20]. Recently, the D\\xe2\\x88\\x92 p correlation function has been mea-\\r\\n                                          heavy quarks and two light antiquarks are theoretically pre-             sured by the ALICE collaboration [34], which paves the way\\r\\n                                          dicted with the quark model in Refs. [8, 9] more than thirty             to the femtoscopy in the charm sector.\\r\\n                                          years ago. In contrast to the X(3872), this Tcc state is found               In this study, we discuss the correlation functions of the\\r\\n                                          in the genuine exotic channel, which requires at least four              DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97 channels towards the understanding of the\\r\\n                                          a\\r\\n                                            Report No.: YITP-22-26\\r\\n                                                                                                                   nature of the Tcc and X(3872) states. To this end, we con-\\r\\n                                          b\\r\\n                                            e-mail: kamiya@hiskp.uni-bonn.de                                       struct one-range Gaussian potentials for the DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97\\r\\n                                          c\\r\\n                                            e-mail: hyodo@tmu.ac.jp                                                channels which reproduce the empirical information in these\\r\\n                                          d\\r\\n                                            e-mail: ohnishi@yukawa.kyoto-u.ac.jp                                   channels. Including the coupled-channel effects with the isospin\\r\\n\\x0c\\n\\n2\\r\\n\\r\\n\\r\\npartners and the decay channels, we compute the correlation         where V0 is the interaction strength and m is the parame-\\r\\nfunctions.                                                          ter of the dimension of mass to control the range of the in-\\r\\n    This paper is organized as follows. In Sect. 2, we con-         teraction. Here we use the charged (isospin averaged) pion\\r\\nstruct the DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97 potentials from the empirical data          mass m\\xcf\\x80\\xc2\\xb1 (m\\xcf\\x80 ) for the DD\\xe2\\x88\\x97 (DD\\xcc\\x84\\xe2\\x88\\x97 ) interactions because\\r\\nand summarize the method to calculate the correlation func-         the lightest exchangeable meson, pion, determines the in-\\r\\ntion with coupled-channel source effect. In Sect. 3, we show        teraction range. Thus, in this formulation, we are left with a\\r\\nthe results of the correlation functions of the D0 D\\xe2\\x88\\x97+ , D+ D\\xe2\\x88\\x970 ,   single parameter V0 for each DD\\xe2\\x88\\x97 /DD\\xcc\\x84\\xe2\\x88\\x97 potential. Note that\\r\\nD0 D\\xcc\\x84\\xe2\\x88\\x970 , and D+ D\\xe2\\x88\\x97\\xe2\\x88\\x92 channels and discuss how the exotic            V0 takes a complex number, in order to express the decay ef-\\r\\nstates can be studied in the future femtoscopy experiments.         fects into the lower energy channels. While the DD\\xe2\\x88\\x97 poten-\\r\\nSection 4 is devoted to summarize this study.                       tial is free from the Coulomb interaction, for the {D+D\\xe2\\x88\\x97\\xe2\\x88\\x92}\\r\\n                                                                    channel we should include the Coulomb force:\\r\\n                                                                                   \\x12        \\x13\\r\\n                                                                       c             0 0\\r\\n2 Method                                                            VDD\\xcc\\x84\\xe2\\x88\\x97 (r) =               ,                                (5)\\r\\n                                                                                     0 \\xe2\\x88\\x92\\xce\\xb1/r\\r\\n\\r\\nLet us first summarize the relevant channels which couple to        with the fine structure constant \\xce\\xb1. This potential is added to\\r\\nthe system of interest. For the Tcc and X(3872) states, we          Eq. (1) for the DD\\xcc\\x84\\xe2\\x88\\x97 potential.\\r\\ncannot neglect the mass difference among the isospin mul-                Here we determine the potential strength V0 so as to re-\\r\\ntiplets, because the deviation of the eigenenergy from the          produce the empirical data for these systems. For the DD\\xe2\\x88\\x97\\r\\n                                                                                                                 0 \\xe2\\x88\\x97+\\r\\nthreshold is comparable or smaller than the isospin breaking        potential, we use the scattering length a0D D = \\xe2\\x88\\x927.16 +\\r\\neffect. The Tcc locates just below the D0 D\\xe2\\x88\\x97+ threshold, and        i1.85 fm, given in the experimental analysis in Ref. [7].1 For\\r\\n                                                                                                                        {D 0 D\\xcc\\x84 \\xe2\\x88\\x970}\\r\\nit also couples to the D+ D\\xe2\\x88\\x970 channel whose threshold lies          the DD\\xcc\\x84\\xe2\\x88\\x97 potential, we use the scattering length a0             =\\r\\nslightly above that of the D0 D\\xe2\\x88\\x97+ channel. At energies lower        \\xe2\\x88\\x924.23 + i3.95 fm which is determined by the eigenenergy\\r\\nthan the Tcc , the three-body DD\\xcf\\x80 channels are open, which          Eh = \\xe2\\x88\\x920.04 \\xe2\\x88\\x92 i0.60 MeV in PDG [4] measured from the\\r\\n                                                                                             {D 0 D\\xcc\\x84 \\xe2\\x88\\x970}      \\xe2\\x88\\x9a\\r\\nprovide the finite decay width of Tcc . The X(3872)\\r\\n                                               \\xe2\\x88\\x9a      lies just     D0 D\\xcc\\x84\\xe2\\x88\\x970 threshold, as a0             = \\xe2\\x88\\x92i/ 2\\xc2\\xb5Eh with the re-\\r\\nbelow the {D0 D\\xcc\\x84\\xe2\\x88\\x970} = (D0 D\\xcc\\x84\\xe2\\x88\\x970 + D\\xcc\\x840 D\\xe2\\x88\\x970 )/ 2 (C = +)               duced mass \\xc2\\xb5. We notice that these scattering lengths have\\r\\n                                                     + \\xe2\\x88\\x97\\xe2\\x88\\x92\\r\\n                         \\xe2\\x88\\x9a to the higher energy {D D } =\\r\\nthreshold and couples also\\r\\n    + \\xe2\\x88\\x97\\xe2\\x88\\x92        \\xe2\\x88\\x92 \\xe2\\x88\\x97+\\r\\n                                                                    a much larger magnitude than the typical length scale of the\\r\\n(D D +D D )/ 2 (C = +) channel. At much lower                       strong interaction \\xe2\\x88\\xbc 1 fm. The obtained potential strengths\\r\\nenergies, the decay channels such as \\xcf\\x80\\xcf\\x80J/\\xcf\\x88 couple to the            are summarized in Table 1. For the later use, the scattering\\r\\nX(3872). In the following, we explicitly treat the D0 D\\xe2\\x88\\x97+           lengths of the higher channels (D+ D\\xe2\\x88\\x970 and {D+D\\xe2\\x88\\x97\\xe2\\x88\\x92}) cal-\\r\\nand D+ D\\xe2\\x88\\x970 channels for Tcc and {D0 D\\xcc\\x84\\xe2\\x88\\x970} and {D+D\\xe2\\x88\\x97\\xe2\\x88\\x92}               culated with the same potentials are also listed. Note that\\r\\nchannels for X(3872), and the decay effect to the other chan-       all these calculations are performed in the coupled-channel\\r\\nnels are renormalized in the imaginary part of the potential.       scheme.\\r\\nThus, the Hamiltonian of the system is expressed by a 2 \\xc3\\x97 2              To calculate the correlation functions C(q) with the coupled-\\r\\nmatrix in the channel basis.                                        channel effects, we employ the Koonin-Pratt-Lednicky-Lyuboshitz-\\r\\n     Next, we construct the DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97 potentials. As-            Lyuboshitz formula (KPLLL) formula [35, 18, 21] given by\\r\\nsuming that the interaction is isospin symmetric, the strong\\r\\ninteraction part of the coupled-channel potentials can be given              Z          2\\r\\n                                                                                                         (\\xe2\\x88\\x92)\\r\\n                                                                                        X\\r\\nby the I = 0 and I = 1 components as                                C(q) =       d3 r         \\xcf\\x89i Si (r)|\\xce\\xa8i     (q; r)|2 ,             (6)\\r\\n                   \\x12                             \\x13                                      i=1\\r\\n                 1 VI=1 + VI=0 VI=1 \\xe2\\x88\\x92 VI=0                                                            (\\xe2\\x88\\x92)\\r\\nVDD\\xe2\\x88\\x97 /DD\\xcc\\x84\\xe2\\x88\\x97 =                                        ,       (1)     where the wave function \\xce\\xa8i in the i-th channel is written\\r\\n                 2 VI=1 \\xe2\\x88\\x92 VI=0 VI=1 + VI=0\\r\\n                                                                    as a function of the relative coordinate r, with imposing the\\r\\nwhere we assign channel i = 1 and 2 to D+ D\\xe2\\x88\\x970 and D0 D\\xe2\\x88\\x97+            outgoing boundary condition on the measured channel. We\\r\\nfor the DD\\xe2\\x88\\x97 system and {D0 D\\xcc\\x84\\xe2\\x88\\x970} and {D+D\\xe2\\x88\\x97\\xe2\\x88\\x92} for the                consider the small momentum region and assume that only\\r\\n                                                                                                                      (\\xe2\\x88\\x92)\\r\\nDD\\xcc\\x84\\xe2\\x88\\x97 system, respectively. Because the Tcc and X(3872)              the s-wave component of the wave function \\xce\\xa8i is modi-\\r\\ncouples to the I = 0 channel, we assume that the I = 0              fied by the strong interaction. The wave function is calcu-\\r\\ncomponent gives the dominant contribution, and set                  lated by solving the Schro\\xcc\\x88dinger equation with the hermite\\r\\n                                                                    conjugated potential V \\xe2\\x80\\xa0 , which gives the appropriate bound-\\r\\nVI=0 = V (r),                                              (2)      ary condition for the eliminated decay channels (See Ap-\\r\\nVI=1 = 0,                                                  (3)      pendix A). We adopt a common static Gaussian source func-\\r\\n                                                                    tion for all the channels Si (r) = exp(\\xe2\\x88\\x92r2 /4R2 )/(4\\xcf\\x80R2 )3/2\\r\\nwhere V (r) is a spherical Gaussian potential:                      1\\r\\n                                                                     Here we use the the high-energy physics convention for the scatter-\\r\\n                     2 2                                            ing length where the positive (negative) real value corresponds to the\\r\\nV (r) = V0 exp(\\xe2\\x88\\x92m r ),                                     (4)      weakly attractive (repulsive or strongly attractive) interaction.\\r\\n\\x0c\\n\\n                                                                                                                                    3\\r\\n\\r\\n\\r\\nTable 1 Strength parameters V0 for the DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97 potentials and the scattering lengths in the DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97 channels. The scattering\\r\\nlengths of the lower channels (third column) are the empirical inputs.\\r\\n\\r\\n\\r\\n                                                                       0   \\xe2\\x88\\x97+                 +   \\xe2\\x88\\x970\\r\\n                                   DD\\xe2\\x88\\x97           V0 [MeV]          aD\\r\\n                                                                    0\\r\\n                                                                      D\\r\\n                                                                                  [fm]   aD\\r\\n                                                                                          0\\r\\n                                                                                            D\\r\\n                                                                                                       [fm]\\r\\n                                             \\xe2\\x88\\x9236.569 \\xe2\\x88\\x92 i1.243     \\xe2\\x88\\x927.16 + i1.85          \\xe2\\x88\\x921.75 + i1.82\\r\\n                                                                    {D 0 D\\xcc\\x84 \\xe2\\x88\\x970}           {D+D \\xe2\\x88\\x97\\xe2\\x88\\x92}\\r\\n                                   {DD\\xcc\\x84\\xe2\\x88\\x97}        V0 [MeV]         a0              [fm]   a0            [fm]\\r\\n                                             \\xe2\\x88\\x9243.265 \\xe2\\x88\\x92 i6.091     \\xe2\\x88\\x924.23 + i3.95          \\xe2\\x88\\x920.41 + i1.47\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n          8                                                          with the source size R, and the weight factor \\xcf\\x89i is taken as\\r\\n                                              1 fm\\r\\n                                                                     unity for all channels. The weight factor \\xcf\\x89i represents the\\r\\n                                              2 fm\\r\\n          7                                                          ratio of the pair production yield in the ith channel with re-\\r\\n                                              3 fm\\r\\n                                              5 fm                   spect to the measured channel. Since we only include the\\r\\n          6                                                          coupled-channel effect of the isospin partners, they are con-\\r\\n                                                                     sidered to have the equivalent emitting source. The source\\r\\n          5                                                          size R ranges from \\xe2\\x88\\xbc 1 fm for the high-multiplicity events\\r\\n                                                                     in pp collisions to \\xe2\\x88\\xbc (5 \\xe2\\x88\\x92 6) fm for the central PbPb colli-\\r\\nCD0 D\\xe2\\x88\\x97+\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n          4                                                          sions.\\r\\n                                                                         While we construct the DD\\xcc\\x84\\xe2\\x88\\x97 potential in the charge con-\\r\\n          3                                                          jugation C = + combination which couples to the X(3872),\\r\\n                                                                     the experimental measurement of the correlation function\\r\\n          2                                                          will be done with fixed charge states, i.e., either D0 D\\xcc\\x84\\xe2\\x88\\x970\\r\\n                                                                     or D\\xcc\\x840 D\\xe2\\x88\\x970 . To obtain the correlation functions of the fixed\\r\\n          1                                                          charge states, the correlation functions in the C = \\xe2\\x88\\x92 sector\\r\\n                                                                     are also needed to take an average of the C = + and C = \\xe2\\x88\\x92\\r\\n          0                                                          contributions. In this exploratory study, we assume that the\\r\\n              0   50   100      150    200        250       300      C = \\xe2\\x88\\x92 interaction is small and can be neglected with respect\\r\\n                             q [MeV/c]\\r\\n                                                                     to the dominant C = + contribution. In this case, we obtain\\r\\n          8\\r\\n                                              1 fm                   the experimentally accessible correlation functions from the\\r\\n                                              2 fm                   correlation function calculated by the C = + potential as\\r\\n          7\\r\\n                                              3 fm\\r\\n                                                                                         1\\r\\n                                              5 fm\\r\\n                                                                                                     \\x01\\r\\n                                                                       CD0 D\\xcc\\x84\\xe2\\x88\\x970 = CD\\xcc\\x840 D\\xe2\\x88\\x970 =\\r\\n                                                                                           C 0 \\xe2\\x88\\x970 + 1 ,                           (7)\\r\\n          6                                                                              2 {D D\\xcc\\x84 }\\r\\n                                                                                         1                     \\x01\\r\\n                                                                     CD+ D\\xe2\\x88\\x97\\xe2\\x88\\x92 = CD\\xe2\\x88\\x92 D\\xe2\\x88\\x97+ =   C + \\xe2\\x88\\x97\\xe2\\x88\\x92 + Cpure Coul. ,                 (8)\\r\\n          5                                                                              2 {D D }\\r\\n                                                                     where Cpure Coul. is calculated only with the Coulomb inter-\\r\\nCD+ D\\xe2\\x88\\x970\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n          4                                                          action by switching off the strong interaction contribution.\\r\\n\\r\\n          3\\r\\n                                                                     3 Results\\r\\n          2\\r\\n                                                                     Now we calculate the correlation functions with the con-\\r\\n          1                                                          structed potentials. First we show the DD\\xe2\\x88\\x97 sector coupled\\r\\n                                                                     with the Tcc state. The correlation function of the D0 D\\xe2\\x88\\x97+\\r\\n          0                                                          and the D+ D\\xe2\\x88\\x970 pairs with source sizes R = 1, 2, 3, and 5\\r\\n              0   50   100      150    200        250       300      fm are shown in Fig. 1. We can see that the source size de-\\r\\n                             q [MeV/c]\\r\\n                                                                     pendence typical to the system with a shallow bound state\\r\\nFig. 1 The correlation functions of the D0 D\\xe2\\x88\\x97+ (top) and D+ D\\xe2\\x88\\x970      for both correlation functions; the enhancement in the small\\r\\n(bottom) pair with the source size R = 1, 2, 3, and 5 fm.\\r\\n                                                                     source case turns to the suppression for the large source\\r\\n                                                                     case [21]. The stronger correlation is found in the D0 D\\xe2\\x88\\x97+\\r\\n                                                                     channel, whose threshold is closer to the Tcc pole. The cusp\\r\\n                                                                     structure is seen at the D+ D\\xe2\\x88\\x970 threshold (q ' 52 MeV/c) in\\r\\n\\x0c\\n\\n4\\r\\n\\r\\n\\r\\nthe D0 D\\xe2\\x88\\x97+ correlation, while the strength is not very promi-\\r\\n                                                                              2.4                                                      1 fm\\r\\nnent.\\r\\n                                                                                                                                       2 fm\\r\\n    Next we show the results of the DD\\xcc\\x84\\xe2\\x88\\x97 correlation func-                    2.2                                                      3 fm\\r\\ntion coupled with the X(3872) in Fig. 2. Here we plot the                                                                              5 fm\\r\\ncorrelation functions of the fixed charges states in Eqs. (7)                  2\\r\\nand (8) which can be compared with the experimental mea-                      1.8\\r\\nsurements. The characteristic strong source size dependence\\r\\nwith the shallow bound state is found in CD0 D\\xcc\\x84\\xe2\\x88\\x970 . We can                    1.6\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                   CD0 D\\xcc\\x84\\xe2\\x88\\x970\\r\\nalso see the cusp structure at the D+ D\\xe2\\x88\\x97\\xe2\\x88\\x92 threshold (q '\\r\\n                                                                              1.4\\r\\n126 MeV/c). The cusp structure is more prominent for the\\r\\nsmaller source case. This is because the coupled-channel                      1.2\\r\\nsource effect by the D+ D\\xe2\\x88\\x97\\xe2\\x88\\x92 channel is stronger for the\\r\\n                                                                               1\\r\\nsmaller source case [20]. On the other hand, due to the at-\\r\\ntractive Coulomb force, the CD+ D\\xe2\\x88\\x97\\xe2\\x88\\x92 correlations show a                       0.8\\r\\nstrong enhancement at small q. To extract the contribution\\r\\nby the strong interaction, we show the difference from the                    0.6\\r\\npure Coulomb case \\xe2\\x88\\x86C = CD+ D\\xe2\\x88\\x97\\xe2\\x88\\x92 \\xe2\\x88\\x92 Cpure Coul. . We can                         0.4\\r\\nsee that the effect of the strong interaction emerges mainly                        0              50           100       150    200       250     300\\r\\n                                                                                                                       q [MeV/c]\\r\\nas the suppression compared to the pure Coulomb case. How-\\r\\never, the deviation |\\xe2\\x88\\x86C| is less than 0.2 for the momentum\\r\\nregion q > 50 MeV/c. Thus, the correlation of D+ D\\xe2\\x88\\x97\\xe2\\x88\\x92 pair                      4\\r\\n                                                                                                                                  1 fm\\r\\nis expected to be dominated by the Coulomb contribution.\\r\\n                                                                                                        0                         2 fm\\r\\n    In this study, we used the empirically determined scat-                   3.5                                                 3 fm\\r\\n                                                                                        \\xe2\\x88\\x86CD+ D\\xe2\\x88\\x97\\xe2\\x88\\x92\\r\\n\\r\\n\\r\\n\\r\\ntering lengths as input to calculate the correlation functions.                                \\xe2\\x88\\x920.5\\r\\n                                                                                                                                  5 fm\\r\\nGiven the correlation data obtained from the precise future                    3                   \\xe2\\x88\\x921\\r\\nmeasurement, we can independently determine the scatter-\\r\\ning lengths a0 because the correlation functions are sensitive                                 \\xe2\\x88\\x921.5\\r\\n                                                                   CD+ D\\xe2\\x88\\x97\\xe2\\x88\\x92\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                              2.5\\r\\nto the low-energy interaction. According to the Weinberg\\xe2\\x80\\x99s                                         \\xe2\\x88\\x922\\r\\n                                                                                                            0    100 200 300\\r\\nweak-binding relation [36, 37, 38], the compositeness, which                                                     q [MeV/c]\\r\\nis defined as the probability of finding molecular state in the                2\\r\\neigenstate, is directly related to the ratio of the a0 /Rh where\\r\\nRh is the length scale determined with the eigenenergy Eh                     1.5\\r\\n              \\xe2\\x88\\x9a\\r\\nas Rh = 1/ \\xe2\\x88\\x922\\xc2\\xb5Eh . Thus, combined with the informa-\\r\\ntion of the pole position, to measure the these correlation\\r\\n                                                                               1\\r\\nfunctions leads to understand the nature of Tcc and X(3872)\\r\\nstates.                                                                             0              50           100      150    200      250     300\\r\\n                                                                                                                      q [MeV/c]\\r\\n\\r\\n                                                                   Fig. 2 The correlation functions of the D0 D\\xcc\\x84\\xe2\\x88\\x970 (top) and D+ D\\xe2\\x88\\x97\\xe2\\x88\\x92\\r\\n4 Summary                                                          (bottom) pair with the source size R = 1, 2, 3, and 5 fm. For D+ D\\xe2\\x88\\x97\\xe2\\x88\\x92\\r\\n                                                                   pair, the difference from the pure Coulomb case \\xe2\\x88\\x86C is shown in sub\\r\\nWe have studied the correlation functions of the DD\\xe2\\x88\\x97 and           figure.\\r\\nDD\\xcc\\x84\\xe2\\x88\\x97 pairs for the purpose of the investigation of the Tcc and\\r\\nX(3872) exotic states. With the assumption of the molecu-\\r\\nlar nature of these states, one-range Gaussian potentials are\\r\\nconstructed for the DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97 channels from the em-             by the ALICE collaboration [34], we expect that the mea-\\r\\npirical data. Due to the large scattering lengths, the calcu-      surements of the DD\\xe2\\x88\\x97 and DD\\xcc\\x84\\xe2\\x88\\x97 correlations in future will\\r\\nlated correlation functions in the lower channels (D0 D\\xe2\\x88\\x97+          bring new insights of the exotic hadrons from the viewpoint\\r\\nand D0 D\\xcc\\x84\\xe2\\x88\\x970 ), which are closer to the exotic states, show the     of the femtoscopy.\\r\\ncharacteristic behavior of the bound state below the thresh-           In this study, we have introduced the potentials in the\\r\\nold. On the other hand, the correlation function of the D+ D\\xe2\\x88\\x970     channels that couple to the exotic states (isospin I = 0 and\\r\\nchannel shows less prominent behavior due to the energy            charge conjugation C = +), and have neglected the interac-\\r\\ndifference from the Tcc pole, and the correlation in the D+ D\\xe2\\x88\\x97\\xe2\\x88\\x92    tions in the other channels. This is because the existence of\\r\\nchannel is mainly caused by the Coulomb interaction. Given         near-threshold states implies the strong interaction, which is\\r\\nthe successful measurement of the D\\xe2\\x88\\x92 p correlation function        considered to give the dominant contribution for the corre-\\r\\n\\x0c\\n\\n                                                                                                                            5\\r\\n\\r\\n\\r\\nlation function. For more quantitative discussion of the cor-     G0 = diag.(G01 , G02 ),                               (A.5)\\r\\nrelation functions, these subleading effects should also be\\r\\nconsidered. In particular, the cusp structure may be sensitive    with the free propagator\\r\\nto the isospin I = 1 interaction, because the coupling be-\\r\\ntween the isospin partners are given by the difference of the     G0i (z) = (z \\xe2\\x88\\x92 Hii0 )\\xe2\\x88\\x921 .                             (A.6)\\r\\ntwo isospin components. The DD\\xcc\\x84\\xe2\\x88\\x97 interaction in the C = \\xe2\\x88\\x92\\r\\nsector is still unclear at this moment, but the neutral partner   With the Feshbach projection [40, 41] for channel 2, the Lippmann-\\r\\nof Zc (3900) [39] may play an important role in this channel.     Schwinger equation for channel 1 can be written with the\\r\\nThese effect should be discussed in the future studies.           effective potential Veff as\\r\\n\\r\\n                                                                  T11 (z) = Veff (z) + Veff (z)G01 (z)T11 (z),          (A.7)\\r\\nAppendix A: Outgoing boundary condition for the                   Veff (z) = V11 + V12 G2 (z)V21 ,                      (A.8)\\r\\noptical complex potential\\r\\n                                                                  where Gi (z) is the full propagator given as\\r\\nThe wave function for the KPLLL formula (6) must sat-\\r\\nisfy the outgoing boundary condition where the flux of the        Gi (z) = (z \\xe2\\x88\\x92 Hii )\\xe2\\x88\\x921 .                               (A.9)\\r\\noutgoing wave of the reference channel is normalized to be\\r\\nunity. On the other hand, the complex optical potentials are                                                (0)\\r\\n                                                                  The contour of the time integration of Gi (z) can be chosen\\r\\nconstructed based on the scattering problem with the incom-       by taking z \\xe2\\x86\\x92 E + i\\x0f for the scattering problem. On the\\r\\ning boundary condition where the flux of the incoming wave        other hand, that of the time-reversed system can be given\\r\\nis normalized. This boundary condition is applied to the in-      as z \\xe2\\x86\\x92 E \\xe2\\x88\\x92 i\\x0f. This effective potential is complex due to\\r\\ntegrated channels, whose coupling to referenced channels          the pole term included in G2 (E \\xe2\\x88\\x92 i\\x0f). Then the effective\\r\\n(D+ D\\xe2\\x88\\x970 and D0 D\\xe2\\x88\\x97+ in the case of DD\\xe2\\x88\\x97 sector) give the            potential in the time reversed system is given as\\r\\nimaginary part of the potential. Thus, we cannot obtain the\\r\\ncorrect wave function \\xcf\\x88 by solving the Schro\\xcc\\x88dinger equa-         Veff (E \\xe2\\x88\\x92 i\\x0f) = V11 + V12 G2 (E \\xe2\\x88\\x92 i\\x0f)V21\\r\\ntion                                                                                \\xe2\\x80\\xa0     \\xe2\\x80\\xa0 \\xe2\\x80\\xa0            \\xe2\\x80\\xa0\\r\\n                                                                                = V11 + V21 G2 (E + i\\x0f)V12\\r\\nH\\xcf\\x88 = [H0 + V ]\\xcf\\x88 = E\\xcf\\x88,                                    (A.1)                                                    \\xe2\\x80\\xa0\\r\\n                                                                                = [V11 + V12 G2 (E + i\\x0f)V21 ]\\r\\nwith the boundary condition only with the referenced chan-                          \\xe2\\x80\\xa0\\r\\n                                                                                = Veff (E + i\\x0f).                       (A.10)\\r\\nnels.\\r\\n    We claim that we can just take the hermite conjugate of       Here we assumed that the full Hamiltonian is hermitian and\\r\\nthe potential V and solve the Schro\\xcc\\x88dinger equation in or-        the potential V is real. Thus, the hermite conjugated effec-\\r\\nder to obtain the wave function which satisfies the boundary      tive potential corresponds to that in the time reversed sys-\\r\\nconditions for all the channels,                                  tem. Remembering that the time reversal operator T acts on\\r\\n[H0 + V \\xe2\\x80\\xa0 ]\\xcf\\x88 = E\\xcf\\x88,                                       (A.2)    the wave function as T \\xcf\\x88 = \\xcf\\x88 \\xe2\\x88\\x97 [42], the system obtained\\r\\n                                                                  from Eq. (A.2) with the outgoing boundary condition cor-\\r\\nwith the outgoing boundary condition. One can easily check        responds to the time-reversed system written with Eq. (A.1)\\r\\nthat \\xcf\\x88 \\xe2\\x88\\x97 satisfies the original Schro\\xcc\\x88dinger equation with in-    with incoming boundary condition.\\r\\ncoming boundary condition.                                            The imaginary part of the optical potential causes the\\r\\n    Taking the hermite conjugate of the potential V corre-        suppression or the enhancement of the wave function com-\\r\\nsponds to consider the time reversal of the system. This can      ponent of the referenced channel depending on its sign. In\\r\\nbe understood as follows. Let us consider the two chan-           the scattering problem of the coupled-channel system, the\\r\\nnel scattering problem with spinless particles where channel      asymptotic form of the s-wave component of the scattering\\r\\n1 (2) has higher threshold energy and is measured (lower          wave function of channel 1 is given with the S matrix com-\\r\\nthreshold energy and is not measured). The Hamiltonian for        ponent as\\r\\nthis system is given as\\r\\n                                                                                 1\\r\\n                                                                                     e\\xe2\\x88\\x92iqr \\xe2\\x88\\x92 S11 eiqr .\\r\\n                                                                                                     \\x01\\r\\n                                                                  \\xcf\\x881 (q; r) \\xe2\\x86\\x92\\r\\n      \\x12            \\x13 \\x12 0                          \\x13\\r\\n        H11 H12            H11 + V11       V12                                                                         (A.11)\\r\\nH=                    =                  0          ,   (A.3)                   2iqr\\r\\n        H21 H22               V21      H22  + V22\\r\\n          0                                                       Due to the coupling to channel 2, the absolute value of the\\r\\nwhere Hij   and Vij are the free Hamiltonian and the interac-\\r\\n                                                                  S matrix component S11 is less than unity, which leads the\\r\\ntion potential, respectively. The Lippmann-Schwinger equa-\\r\\n                                                                  reduced outgoing wave (eiqr ) compared to the normalized\\r\\ntion for the T matrix is given as\\r\\n                                                                  incoming wave (e\\xe2\\x88\\x92iqr ). When we use the complex optical\\r\\nT = V + V G0 T,                                          (A.4)    potential V with negative imaginary part, this reduction of\\r\\n\\x0c\\n\\n6\\r\\n\\r\\n\\r\\nthe wave function is caused by the imaginary part of the po-             20. Y. Kamiya, T. Hyodo, K. Morita, A. Ohnishi, and W. Weise, Phys.\\r\\ntential. On the other hand, the outgoing boundary condition,                 Rev. Lett. 124, 132501 (2020).\\r\\n                                                                         21. Y. Kamiya, K. Sasaki, T. Fukui, T. Hyodo, K. Morita, K. Ogata,\\r\\nwhich is used for the correlation study, is given as\\r\\n                                                                             A. Ohnishi and T. Hatsuda, Phys. Rev. C 105, no.1, 014915\\r\\n               1 \\x10 iqr               \\x11                                       (2022).\\r\\n                             \\xe2\\x80\\xa0 \\xe2\\x88\\x92iqr\\r\\n\\xcf\\x881 (q; r) \\xe2\\x86\\x92         e \\xe2\\x88\\x92 S11    e       .              (A.12)             22. L. Adamczyk et al. [STAR], Phys. Rev. Lett. 114, no.2, 022301\\r\\n             2iqr                                                            (2015).\\r\\nIn this case, the flux of the outgoing wave (1) is larger than           23. S. Acharya et al. [ALICE], Phys. Lett. B 774, 64-77 (2017).\\r\\n                                \\xe2\\x80\\xa0                                        24. J. Adam et al. [STAR], Phys. Lett. B 790, 490-497 (2019).\\r\\nthat of the incoming wave (|S11   |). This is because the wave           25. S. Acharya et al. [ALICE], Phys. Rev. C 99, no.2, 024001 (2019).\\r\\nfunction of channel 2 flows into channel 1 by the coupling               26. S. Acharya et al. [ALICE], Phys. Rev. Lett. 123, no.11, 112002\\r\\npotential to give the normalized outgoing wave. When we                      (2019).\\r\\nuse the hermite conjugated optical potential V \\xe2\\x80\\xa0 with positive           27. S. Acharya et al. [ALICE], Phys. Lett. B 805, 135419 (2020).\\r\\n                                                                         28. S. Acharya et al. [ALICE], Phys. Lett. B 797, 134822 (2019).\\r\\nimaginary part, its imaginary part causes the enhancement                29. S. Acharya et al. [ALICE], Phys. Rev. Lett. 124, no.9, 092301\\r\\nof the channel 1 component. We also note that the result-                    (2020).\\r\\ning wave function can also be obtained by solving Eq. (A.1)              30. S. Acharya et al. [ALICE], Nature 588, 232-238 (2020) [erratum:\\r\\nwith the (standard) incoming boundary condition and taking                   Nature 590, E13 (2021)].\\r\\n                                                                         31. S. Acharya et al. [ALICE], Phys. Lett. B 822, 136708 (2021).\\r\\nthe complex conjugate of the wave function.                              32. S. Acharya et al. [ALICE], Phys. Rev. Lett. 127, no.17, 172301\\r\\n                                                                             (2021).\\r\\nAcknowledgements The authors thank Laura Fabbietti and Fabrizio          33. L. Fabbietti, V. Mantovani Sarti and O. Vazquez Doce, Ann. Rev.\\r\\nGrosa for useful discussions. This work has been supported in part by        Nucl. Part. Sci. 71, 377-402 (2021).\\r\\nthe Grants-in-Aid for Scientific Research from JSPS (Grant numbers       34. S. Acharya et al. [ALICE], [arXiv:2201.05352 [nucl-ex]].\\r\\nJP21H00121, JP19H05150, JP19H05151, JP19H01898, JP18H05402,              35. R. Lednicky, V. V. Lyuboshits, and V. L. Lyuboshits, Phys. Atomic\\r\\nand JP16K17694), by the Yukawa International Program for Quark-              Nuclei 61, 2950 (1998).\\r\\nhadron Sciences (YIPQS), by the Deutsche Forschungsgemeinschaft          36. S. Weinberg, Phys. Rev. 137, B672 (1965).\\r\\n(DFG) and the National Natural Science Foundation of China (NSFC)        37. Y. Kamiya and T. Hyodo, Phys. Rev. C93, 035203 (2016).\\r\\nthrough the funds provided to the Sino-German Collaborative Research     38. Y. Kamiya and T. Hyodo, PTEP 2017, 023D02 (2017).\\r\\nCenter \\xe2\\x80\\x9cSymmetries and the Emergence of Structure in QCD\\xe2\\x80\\x9d (NSFC          39. M. Ablikim et al. [BESIII], Phys. Rev. Lett. 115, no.11, 112003\\r\\nGrant No. 12070131001, DFG Project-ID 196253076 \\xe2\\x80\\x93 TRR 110).                  (2015).\\r\\n                                                                         40. H. Feshbach, Annals Phys. 5, 357-390 (1958).\\r\\n                                                                         41. H. Feshbach, Annals Phys. 19, 287-313 (1962).\\r\\n                                                                         42. J.R. Taylor, Scattering Theory: The Quantum Theory on Nonrela-\\r\\nReferences                                                                   tivistic Collisions, Wiley, New York, 1972.\\r\\n\\r\\n 1. A. Hosaka, T. Iijima, K. Miyabayashi, Y. Sakai and S. Yasui, PTEP\\r\\n    2016, no.6, 062C01 (2016).\\r\\n 2. F. K. Guo, C. Hanhart, U. G. Mei\\xc3\\x9fner, Q. Wang, Q. Zhao and\\r\\n    B. S. Zou, Rev. Mod. Phys. 90, no.1, 015004 (2018).\\r\\n 3. N. Brambilla, S. Eidelman, C. Hanhart, A. Nefediev, C. P. Shen,\\r\\n    C. E. Thomas, A. Vairo and C. Z. Yuan, Phys. Rept. 873, 1-154\\r\\n    (2020).\\r\\n 4. Particle Data Group, P. A. Zyla et al., PTEP 2020, 083C01 (2020).\\r\\n 5. Belle, S. K. Choi et al., Phys. Rev. Lett. 91, 262001 (2003), hep-\\r\\n    ex/0309032.\\r\\n 6. LHCb, R. Aaij et al., (2021), 2109.01038.\\r\\n 7. LHCb, R. Aaij et al., (2021), 2109.01056.\\r\\n 8. J. l. Ballot and J. M. Richard, Phys. Lett. B 123, 449-451 (1983).\\r\\n 9. S. Zouzou, B. Silvestre-Brac, C. Gignoux and J. M. Richard, Z.\\r\\n    Phys. C 30, 457 (1986).\\r\\n10. C. Bignamini, B. Grinstein, F. Piccinini, A. D. Polosa and\\r\\n    C. Sabelli, Phys. Rev. Lett. 103, 162001 (2009).\\r\\n11. A. M. Sirunyan et al. [CMS], Phys. Rev. Lett. 128, no.3, 032001\\r\\n    (2022).\\r\\n12. S. Cho et al. [ExHIC], Prog. Part. Nucl. Phys. 95, 279-322 (2017).\\r\\n13. K. Morita, T. Furumoto, and A. Ohnishi, Phys. Rev. C91, 024916\\r\\n    (2015).\\r\\n14. A. Ohnishi, K. Morita, K. Miyahara, and T. Hyodo, Nucl. Phys.\\r\\n    A954, 294 (2016).\\r\\n15. K. Morita, A. Ohnishi, F. Etminan, and T. Hatsuda, Phys. Rev.\\r\\n    C94, 031901 (2016).\\r\\n16. T. Hatsuda, K. Morita, A. Ohnishi, and K. Sasaki, Nucl. Phys.\\r\\n    A967, 856 (2017).\\r\\n17. D. L. Mihaylov et al., Eur. Phys. J. C78, 394 (2018).\\r\\n18. J. Haidenbauer, Nucl. Phys. A981, 1 (2019).\\r\\n19. K. Morita et al., Phys. Rev. C 101, 015201 (2020).\\r\\n\\x0c\", b'    Spatially Multi-conditional Image Generation\\r\\n\\r\\n          Ritika Chakraborty\\xe2\\x8b\\x861 Nikola Popovic\\xe2\\x8b\\x861 , Danda Pani Paudel1 ,\\r\\n                       Thomas Probst1 , Luc Van Gool1,2\\r\\n               1\\r\\n                Computer Vision Laboratory, ETH Zurich, Switzerland\\r\\n                    2\\r\\n                      VISICS, ESAT/PSI, KU Leuven, Belgium\\r\\n       {critika, nipopovic, paudel, probstt, vangool}@vision.ee.ethz.ch\\r\\n\\r\\n\\r\\n\\r\\n        Abstract. In most scenarios, conditional image generation can be thought\\r\\n        of as an inversion of the image understanding process. Since generic\\r\\n        image understanding involves the solving of multiple tasks, it is natu-\\r\\n        ral to aim at the generation of images via multi-conditioning. However,\\r\\n        multi-conditional image generation is a very challenging problem due to\\r\\n        the heterogeneity and the sparsity of the (in practice) available condi-\\r\\n        tioning labels. In this work, we propose a novel neural architecture to\\r\\n        address the problem of heterogeneity and sparsity of the spatially multi-\\r\\n        conditional labels. Our choice of spatial conditioning, such as by seman-\\r\\n        tics and depth, is driven by the promise it holds for better control of the\\r\\n        image generation process. The proposed method uses a transformer-like\\r\\n        architecture operating pixel-wise, which receives the available labels as\\r\\n        input tokens to merge them in a learned homogeneous space of labels.\\r\\n        The merged labels are then used for image generation via conditional\\r\\n        generative adversarial training. In this process, the sparsity of the labels\\r\\n        is handled by simply dropping the input tokens corresponding to the\\r\\n        missing labels at the desired locations, thanks to the proposed pixel-wise\\r\\n        operating architecture. Our experiments on three benchmark datasets\\r\\n        demonstrate the clear superiority of our method over the state-of-the-\\r\\n        art and the compared baselines.\\r\\n\\r\\n\\r\\n1     Introduction\\r\\n\\r\\nIn recent years, automated image generation under user control has become\\r\\nmore and more of a reality. Such processes are typically based on so-called con-\\r\\nditional image generation methods [34,21]. One could see these as an intermedi-\\r\\nate between fully unconditional [15] and purely rendering based [7] generation,\\r\\nrespectively. Methods belonging to these two extreme cases either offer no user\\r\\ncontrol (the former) or need to get all necessary information of image formation\\r\\nsupplied by the user (the latter). In many cases, neither extreme is desirable.\\r\\nTherefore, several conditional image generation methods, that circumvent the\\r\\nrendering process altogether, have been proposed. These methods usually re-\\r\\nceive the image descriptions either in the form of text [34,45] or as spatially\\r\\nlocalized semantic classes [21,39,47].\\r\\n\\xe2\\x8b\\x86\\r\\n    Equal contributions.\\r\\n\\x0c\\n\\n2       R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n    In this paper, we aim to condition the image generation process beyond text\\r\\ndescriptions and the desired semantics. In this regard, we make a generic practi-\\r\\ncal assumption that any semantic or geometric aspects of the desired image may\\r\\nbe available for conditioning. For example, an augmented reality application may\\r\\nrequire to render a known object using its 3D model and specified pose, but with\\r\\nmissing texture and lighting details. In such cases, attributes such as semantic\\r\\nmask, depth, normal, curvature at that object\\xe2\\x80\\x99s location, become instantly avail-\\r\\nable for the image generation process, offering us the multi-conditioning inputs.\\r\\nThe geometric and semantic aspects of the other parts of the same image may\\r\\nalso be available partially or completely. Incorporating all such information in a\\r\\nmulti-conditional manner to generate the desired image, is the main challenge\\r\\nthat we undertake. In some sense, our approach bridges the gap between gener-\\r\\native and rendering based image synthesis.\\r\\n    Two major challenges of multi-conditional image generation, in the context\\r\\nof this paper, are the heterogeneity and the sparsity of the available labels. The\\r\\nheterogeneity refers to the difference in representation of different labels, for e.g.\\r\\ndepth and semantics. On the other hand, the sparsity is either simply caused by\\r\\nthe label definition (e.g. sky has no normal) or due to missing annotations [42]. It\\r\\nis important to note that some geometric aspects of images, such as an object\\xe2\\x80\\x99s\\r\\ndepth and orientation, can be introduced manually (e.g. by sketching), without\\r\\nrequiring any 3D model with its pose. This allows users to geometrically control\\r\\nimages (beyond the semantics based control) both in the the presence or absence\\r\\nof 3D models. It goes without saying that the geometric manipulation of any\\r\\nimage can be carried out by first inferring its geometric attributes using existing\\r\\nmethods [42,49,61], followed by generation after manipulation.\\r\\n    To address the problems of both heterogeneity and diversity, we propose a\\r\\nlabel merging network that learns to merge the provided conditioning labels\\r\\npixel-wise. To this end, we introduce a novel transformer-based architecture,\\r\\nthat is designed to operate on each pixel individually. The provided labels are\\r\\nfirst processed by label-specific multilayer perceptrons (MLPs) to generate a to-\\r\\nken for each label. The tokens are then processed by the transformer module.\\r\\nIn contrast to popular vision transformers that perform spatial attention [9,62],\\r\\nour transformer module applies self-attention across the label dimension, thus\\r\\navoiding the high computational complexity. The pixel-wise interaction of avail-\\r\\nable labels homogenizes the different labels to a common representation for the\\r\\noutput tokens. The output tokes are then averaged to obtain the fused labels in\\r\\nthe homogeneous space to form the local concept. This is performed efficiently\\r\\nfor all pixels in parallel by sliding the pixel-wise transformer over the input label\\r\\nmaps. Finally, the concepts are used for the image generation via conditional\\r\\ngenerative adversarial training, using a state-of-the-art method [47]. During the\\r\\nprocess of label merging, the spatial alignment is always preserved. The sparsity\\r\\nof the labels is handled by simply dropping the input tokens of the missing la-\\r\\nbels, at the corresponding pixel locations. This way, the transformer learns to\\r\\nreconstruct the concept for each pixel, also in the case when not all labels are\\r\\navailable.\\r\\n\\x0c\\n\\n                                   Spatially Multi-conditional Image Generation       3\\r\\n\\r\\n                    Input labels                 Ours               Generated image\\r\\n\\r\\n\\r\\n                                      Label\\r\\n                                                        Generator\\r\\n                                     merging\\r\\n\\r\\n\\r\\n\\r\\n                    Input label                Standard             Generated image\\r\\n\\r\\n\\r\\n                                               Generator\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 1. Spatially multi-conditional image generation. Our model uses multiple\\r\\nlabels to generate an image, compared to standard approaches which use only the\\r\\nsemantic segmentation label. Multiple input labels, coming from different sources, are\\r\\nhandled by the proposed label merging block.\\r\\n\\r\\n\\r\\n    We study the influence of several spatially conditioning labels including se-\\r\\nmantics, depth, normal, curvature, edges, in three different benchmark datasets.\\r\\nThe influence of the labels is studied both the case of sparse and dense label\\r\\navailability. In both cases, the proposed method provides outstanding results by\\r\\nclearly demonstrating the benefit of conditioning labels beyond the commonly\\r\\nused image semantics. The major contribution of this paper can be summarized\\r\\nas follow:\\r\\n\\r\\n 1. We study the problem of spatially multi-conditional image generation, for\\r\\n    the first time.\\r\\n 2. We propose a novel neural network architecture to fuse the heterogeneous\\r\\n    multi-conditioning labels provided for the task at hand, while also handling\\r\\n    the sparsity of the labels at the same time.\\r\\n 3. We analyse the utility of various conditioning types for image generation and\\r\\n    present outstanding results obtained by the proposed method in benchmark\\r\\n    datasets.\\r\\n\\r\\n\\r\\n\\r\\n2    Related Work\\r\\n\\r\\nConditional Image Synthesis. A description of the desired image to be gener-\\r\\nated can be provided in various forms, from class conditions [35,3], text [34,45],\\r\\nspatially localized semantic classes [21,39,47], sketches [21], style information\\r\\n[13,23,24] to human poses [33]. Recently, the handling of different data structures\\r\\n(e.g. text sequences and images) has received attention in the literature [64,59].\\r\\nThe problem of spatially multi-conditional image generation is orthogonal to the\\r\\nproblem of unifying different (non-spatial) modalities, as it seeks to fuse heteroge-\\r\\nneous spatially localized labels into concepts, while preserving the spatial layout\\r\\nfor image generation. In the area of image-to-image translation, Ronneberger\\r\\n\\x0c\\n\\n4       R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2. Pixel-wise Transformer Label Merging block (TLAM). The heteroge-\\r\\nneous labels of each pixel are first projected into the same dimensionality, and then\\r\\npassed to the concept generation block. A transformer module promotes the interaction\\r\\nbetween labels, before finally distilling them to a concept representation by averaging\\r\\nthe homogeneous label embeddings at every pixel location.\\r\\n\\r\\n\\r\\n\\r\\net al. first introduced the UNet [46] architecture, which has been used in sev-\\r\\neral important follow up works. Isola et al. [21] later introduced the Pix2Pix\\r\\nparadigm that leveraged the UNet backbone for a generator and combined it\\r\\nwith a convolutional discriminator, to convert sketches into photo-realistic im-\\r\\nages. This work was improved by Wang et al. to support high resolution image\\r\\ntranslation in Pix2PixHD [57] and video translation in Vid2Vid [56]. Recently,\\r\\nShaham et al. introduced the ASAP-Net [47] which achieves a superior trade-off\\r\\nof inference time and performance on several image translation tasks. We employ\\r\\nthe ASAP-Net as one component in our architecture.\\r\\nConditioning Mechanisms. The conditioning mechanism is at the core of\\r\\nsemantically controllable neural networks, and is often realized in conjunction\\r\\nwith normalization techniques [10,19]. Perez et al. introduced a simple feature-\\r\\nwise linear modulation FiLM [41] for visual reasoning. In the context of neu-\\r\\nral style transfer [12], Huang et al. introduced Adaptive Instance Normalization\\r\\nAdaIN [20]. Park et al. extended AdaIN for spatial control in SPADE [39], where\\r\\nthe normalization parameters are derived from a semantic segmentation. Zhu et\\r\\nal. [65] further extend SPADE to allow for independent application of global\\r\\nand local styles. Finally, generic normalisation schemes that make use of ker-\\r\\nnel prediction networks to achieve arbitrarily global and local control include\\r\\nDynamic Instance Normalization (DIN) [22] and Adaptive Convolutions (Ada-\\r\\nConv) [5]. While being greatly flexible, they also increase the resulting inference\\r\\ntime, unlike ASAP-Net that uses adaptive implicit functions [48] for efficiency.\\r\\nDifferentiable Rendering Methods. Since rendering is a complex and com-\\r\\nputationally expensive process, several approximations were proposed to facil-\\r\\nitate its use for training neural networks. Methods starting from simple ap-\\r\\nproximations of the rasterization function to produce silhouettes [26,31,28] to\\r\\nmore complex approximations modelling indirect lighting effects [29,37,32] have\\r\\nbeen proposed. These algorithms have also been incorporated into popular deep\\r\\nlearning frameworks [44,29,38]. Differentiable renderers have been successfully\\r\\n\\x0c\\n\\n                                                                     Spatially Multi-conditional Image Generation                                                                       5\\r\\n\\r\\nused in several neural networks including those used for face [52,51] and hu-\\r\\nman body [30,40] reconstruction. We refer the interested reader to the excellent\\r\\nsurveys of Tewari et al. [50] and Kato et al. [25] for more details. In contrast\\r\\nto rendering approaches which require setting numerous scene parameters, our\\r\\nmethod directly generates realistic images from only a sparse set of chosen labels.\\r\\n\\r\\n\\r\\n3         Method\\r\\nWe start by introducing a few formal notations. As an input, we have a collection\\r\\nof labels X = {X1 , X2 , ..., XN }, where each label Xk \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97Ck has height H,\\r\\nwidth W and Ck channels. The element of the label Xk corresponding to the\\r\\npixel location (i, j) is denoted as xij k \\xe2\\x88\\x88 R\\r\\n                                              Ck\\r\\n                                                 . Hence, elements of the label set\\r\\nX corresponding to the pixel location (i, j) form a set X ij = {xij       ij        ij\\r\\n                                                                     1 , x2 , ..., xN }.\\r\\nThe model takes the set of labels X as an input and produces I = \\xcf\\x95(X ), where\\r\\nI \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x973 is the generated image.\\r\\n\\r\\n3.1           Label Merging\\r\\nWe describe the mechanism of the label merging component, as illustrated in\\r\\nFigure 2. This component processes different spatial locations of the input la-\\r\\nbels X ij independently, and is efficiently performed on all pixels in parallel. We\\r\\nempirically found this to be sufficient. To this end, we first collapse the spatial\\r\\ndimensions of every label Xk to obtain Ek \\xe2\\x88\\x88 RHW \\xc3\\x97d , which contains HW embed-\\r\\nding vectors eij     d\\r\\n               k \\xe2\\x88\\x88 R . In this process, we are interested to merge all embedding\\r\\n           ij\\r\\nvectors {ek } into a common concept vector zij , for each pixel location. The la-\\r\\nbel merging is performed by two blocks: the projection block and the concept\\r\\ngeneration block, which we described in the following.\\r\\n\\r\\nProjection Block This block projects every heterogeneous input label Xk into\\r\\nan embedding space Ek \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97d , where d is the dimensionality of the embed-\\r\\nding space. It does so by transforming every token xij                                                                                                      k with the projection func-\\r\\ntion fk , to project it into the embedding space eij                                                                                    k         =           fk (xij ), where eij   d\\r\\n                                                                                                                                                                                k \\xe2\\x88\\x88 R . All\\r\\nthe elements of a given label Xk share the same projection function fk . Different\\r\\ninput labels have different projection functions fk . We use an affine transfor-\\r\\nmation followed by the GeLU nonlinearity a(\\xc2\\xb7) [17] to serve as the embedding\\r\\nfunction,\\r\\n                              \\\\label {eq:projection_block} \\\\mathsf {e}_{k}^{ij} = f_k (\\\\mathsf {x}_{k}^{ij}) = a(\\\\mathsf {A}_{k}\\\\mathsf {x}_{k}^{ij} + \\\\mathsf {b}_k),                  (1)\\r\\nwhere Ak \\xe2\\x88\\x88 Rd\\xc3\\x97Ck and bk \\xe2\\x88\\x88 Rd . We implement this by using fully connected\\r\\nlayers as projection functions for each input label Xk , followed by the GeLU\\r\\nactivation function. Our method also works with sparse input labels, which can\\r\\nbe caused by label definition or missing labels. When the value of a label Xk is\\r\\nmissing at a certain spatial location, its element token xij\\r\\n                                                          k is dropped by setting\\r\\nit to a zero vector. Thus, a token with the embedding eij   k = a(bk ) will send a\\r\\nsignal to the concept generation block that the label k is not present at this\\r\\n\\x0c\\n\\n6       R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\nlocation, and that it should extract the information from other labels. Also, the\\r\\nabsence of the whole label Xk is handled similarly.\\r\\n\\r\\nConcept Generation Block The collection of the embedding vectors corre-\\r\\nsponding to different labels E = {e1 , .., ek , ..., eN } serve as input tokens for our\\r\\nconcept generation block. This block uses a novel attention mechanism to model\\r\\nthe interaction across labels, which is shared across all spatial locations. It there-\\r\\nfore does not require expensive spatial attention as used in vision transformer\\r\\narchitectures [53,9]. In other words, we apply the same label-transformer on each\\r\\npixel individually. Transformers are naturally suited here to encourage interac-\\r\\ntion between the embedded label tokens to share their label specific information\\r\\nwhile merging it into the final label representation. Before giving E to the trans-\\r\\nformer, we add label specific encodings pk to the embedded tokens ek to obtain\\r\\n (0)                                        (0) (0)         (0)\\r\\nzk = ek + pk . Then we pass Z (0) = {z1 , z2 , ..., zN } through l transformer\\r\\nblocks Bm . Each block Bm takes the output Z (m\\xe2\\x88\\x921) of the previous block and\\r\\n                                                        (m) (m)       (m)\\r\\nproduces Z (m) = Bm (Z (m\\xe2\\x88\\x921) ), where Z (m) = {z1 , z2 , . . . , zN }.\\r\\n     Each transformer block Bm is composed of a Multi-head Self Attention block\\r\\n(MSA), followed by a Multilayer Perceptron block (MLP) [53,9]. Self-attention\\r\\nis a global operation, where every token interacts with every other token and\\r\\nthus information is shared across them. Multiple heads in the MSA block are\\r\\nused for more efficient computation and for extracting richer and more diverse\\r\\nfeatures. The MSA block executes the following operations:\\r\\n                         \\\\label {eq:MSA} \\\\mathcal {\\\\hat {Z}}^{(m)} = \\\\mathsf {MSA}(\\\\mathsf {LN}(\\\\mathcal {Z}^{(m-1)})) + \\\\mathcal {Z}^{(m-1)},            (2)\\r\\nwhere LN represents Layer Normalization [2]. The MSA block is followed by the\\r\\nMLP block, which processes each token separately using the same Multilayer\\r\\nPerceptron. This block further processes the token features, after their global\\r\\ninteractions in the MSA block, by sharing and refining the representations of each\\r\\ntoken across all their channels. The MLP block executes the following operations:\\r\\n\\r\\n                              \\\\label {eq:MLP} \\\\mathcal {Z}^{(m)} = \\\\mathsf {MLP}(\\\\mathsf {LN}(\\\\mathcal {\\\\hat {Z}}^{(m)})) + \\\\mathcal {\\\\hat {Z}}^{(m)}.    (3)\\r\\n   Note that, for vision transformers [9] operating on M spatial elements (e.g.\\r\\npixels/patches), the computational complexity of the self-attention is O(M 2 ).\\r\\nOur pixel-wise transformer, operating only on N label tokens, reduces the com-\\r\\nplexity of self-attention to O(N 2 ) (per pixel) with the number of labels N \\xe2\\x89\\xaa M.\\r\\n   Finally, all the elements of the output set produced by the final transformer\\r\\n               (l) (l)     (l)\\r\\nblock Z = {z1 , z2 , ..., zN } are averaged to obtain z = N1 k=1:N zk . They are\\r\\n                                                             P\\r\\nreshaped back into the original spatial dimensions to obtain the Concept Tensor\\r\\nZ \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97d . We name this label merging block as the Transformer LAbel\\r\\nMerging (TLAM) block.\\r\\n\\r\\n3.2   Network Overview\\r\\nThe proposed model is divided into two components: the label merging compo-\\r\\nnent and the image generation component. This is depicted in Figure 3.\\r\\n\\x0c\\n\\n                                 Spatially Multi-conditional Image Generation          7\\r\\n\\r\\n               Input labels                          Generated image\\r\\n\\r\\n                               Label\\r\\n                                         Generator                     Discriminator\\r\\n                              merging\\r\\n\\r\\n\\r\\n\\r\\n              Real image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3. Network overview. Different input labels are embedded into a homogeneous\\r\\nspace with the label merging module. The generator uses this embedding to produce\\r\\nthe output image. During training, the discriminator takes the input labels, real and\\r\\ngenerated images, and uses them to optimize the label merging and generator modules.\\r\\n\\r\\n\\r\\nLabel Merging. The label merging block takes the set of heterogeneous labels\\r\\nX as the input and merges them into a homogeneous space Z = \\xcf\\x88(X ), where\\r\\nZ \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97d is the Concept Tensor. It is important to note that the label\\r\\nmerging block does not require the number of input labels to be the same for\\r\\nevery input. Input labels X are heterogeneous because they come from different\\r\\nsources and can have different numbers of channels Ck , as well as different ranges\\r\\nof values. For example, semantic segmentation labels are represented using dis-\\r\\ncrete values, while surface normals are continuous. The label merging block first\\r\\ntranslates all the provided labels Xk into the same dimensional embeddings Ek ,\\r\\nusing the projection block. Then, it uses the concept generation block to trans-\\r\\nlate embeddings Ek to the concept tensor Z. In short, the label merging block\\r\\nfuses the heterogeneous set of input labels X into a homogeneous Concept Tensor\\r\\nZ. This block is depicted in Figure 2.\\r\\nGenerator. The task of the generator is to take the produced Concept Tensor\\r\\nZ and generate the image I = g(Z), as shown in Figure 3. As the generator,\\r\\nwe employ the state-of-the-art ASAP model [47]. This model first synthesizes\\r\\nthe high-resolution pixels using lightweight and highly parallelizable operators.\\r\\nASAP performs most computationally expensive image analysis at a very coarse\\r\\nresolution. We modify the input to ASAP, by providing the Concept Tensor Z\\r\\nas an input, instead of giving only one specific input labels Xk . The generator\\r\\ncan therefore exploit the merged information from all available input labels X .\\r\\nAdversarial Training for Multi-conditioning. We follow the optimization\\r\\nprotocol of ASAP-Net [47]. We train our generator model adversarially with a\\r\\nmulti-scale patch discriminator, as suggested by pix2pixHD [57]. To achieve this,\\r\\nwe modify the input to the discriminator by using the Concept Tensor Z instead\\r\\nof a specific label Xk , for example semantics used by the most existing works.\\r\\n\\r\\n\\r\\n4   Experiments\\r\\nImplementation Details. We follow the optimization protocol of ASAP-Net [47].\\r\\nWe train our generator model adversarially with a multi-scale patch discrimina-\\r\\n\\x0c\\n\\n8       R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n                                                           TLAM          Sparse-TLAM\\r\\n     All-dense   All-sparse SPADE [39] ASAP [47]\\r\\n                                                           (Ours)           (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 4. Visual comparisons on the Taskonomy dataset. Column 1 & 2: five\\r\\ndifferent labels tiled horizontally for dense and 50% sparse cases. Column 5 & 6: images\\r\\ngenerated by our method using the labels from column 1 & 2. Column 3 & 4: images\\r\\ngenerated by the dense semantic methods. Our method generates more realistic images\\r\\nwith fine geometric and visual details from both dense and sparse labels.\\r\\n\\r\\n\\r\\n                                                    Label sparsity\\r\\n                    Backbone         Method                        FID\\r\\n                                                    S E C D N\\r\\n                                     Regular                      72.3\\r\\n                   SPADE [39]\\r\\n                                  Naive-baseline                  66.1\\r\\n                                   Regular                       73.8\\r\\n                                Naive-baseline                   74.6\\r\\n                   ASAP [47] CLAM-baseline (Ours)                43.8\\r\\n                                TLAM (Ours)                      37.9\\r\\n                             CLAM-baseline (Ours)                37.1\\r\\n                                TLAM (Ours)                      30.6\\r\\nTable 1. FID scores on the Taskonomy dataset. Our TLAM method generates\\r\\nimages with significantly better visual quality for both sparse and dense labels. Symbol\\r\\n  corresponds to dense labels, while     corresponds to 50% label sparsity. S stands for\\r\\nsemantics, E for edges, C for curvature, D for depth and N for normals. Please, refer\\r\\nFigure 4 for corresponding images.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ntor, as suggested by pix2pixHD [57]. The training includes an adversarial hinge-\\r\\nloss, a perceptual loss and a discriminator feature matching loss. The learning\\r\\nrates for the generator and discriminator are 0.0001 and 0.0004, respectively. We\\r\\nuse the ADAM [27] solver with \\xce\\xb21 = 0 and \\xce\\xb22 = 0.999, following [39,47]. To gen-\\r\\nerate sparse labels, we look at spatial areas corresponding to distinct semantic\\r\\nsegmentation instances. For the sparsity of S, we randomly drop the labels with\\r\\nS% probability, independently for every label inside every area. Please turn to\\r\\nthe supplementary material for a detailed visualization of sparse labels.\\r\\n\\x0c\\n\\n                               Spatially Multi-conditional Image Generation       9\\r\\n\\r\\n4.1   Datasets\\r\\n\\r\\nWe conduct experiments on three different datasets to demonstrate the versatil-\\r\\nity and effectiveness of our approach.\\r\\nTaskonomy dataset [63] is a multi-label annotated dataset of indoor scenes.\\r\\nThe entire dataset consists of over 4 Million images from around 500 different\\r\\nbuildings with high resolution RGB images, segmentation masks and other la-\\r\\nbels. We chose this dataset for our main experiments because of its wide selection\\r\\nof both semantic and geometric labels. In our experiments, we use the following\\r\\nlabels: semantic segmentation, depth, surface normals, edges and curvature. We\\r\\nselected two buildings from the large dataset resulting a total of 18,246 images,\\r\\nsplit into 14,630/3,619 training/validation images.\\r\\nCityscapes dataset [8] contains images of urban street scenes from 50 different\\r\\ncities and their dense pixel annotations for 30 classes. The training and validation\\r\\nsplit contains 3000 and 500 samples respectively. To obtain further labels, we\\r\\nuse a state-of-the art depth estimation network [14] for depth. The estimated\\r\\ndepth, along with the camera intrinsics were used to compute the local patch-\\r\\nwise surface normals. Additionally, we use canny filters for edge detection. This\\r\\nresulted four labels for Cityscape.\\r\\nNYU depth v2 dataset [36] consists of 1449 densely labeled pairs of aligned\\r\\nRGB images and depth, normals, semantic segmentation and edges maps of\\r\\nindoor scenes. We split the data into 1200/249 training/validation sets.\\r\\n    The presented qualitative and quantitative results are generated on the re-\\r\\nspective hold-out test sets, with the exception of Figure 9, where we follow the\\r\\nprotocol of [57] for comparison with the other methods.\\r\\n\\r\\n\\r\\n4.2   Baselines and Metrics\\r\\n\\r\\nSince this is the first work for spatially multi-conditional image generation, we\\r\\nconstruct our own baselines.\\r\\nNaive-baseline takes all the available inputs and concatenates them along the\\r\\nchannel dimension to create the input, which is then fed as an input to the\\r\\nASAP-Net or SPADE backbone. This also serves as an ablation to study the\\r\\nefficacy of the concept generation component in TLAM.\\r\\nConvolutional Label Merging (CLAM-baseline) is another baseline that\\r\\nstacks multiple consecutive blocks similar to the projection block from Sec-\\r\\ntion 3.1. The first block is exactly the projection block (1), while the following\\r\\nl blocks perform the same operation with Alk \\xe2\\x88\\x88 Rd\\xc3\\x97d , blk \\xe2\\x88\\x88 Rd , preserving the\\r\\ndimensionality d. After the final block, all output elements corresponding to the\\r\\nsame spatial location are averaged to produce the Concept Tensor Z, just like\\r\\nat the end of the TLAM block.\\r\\nSotA semantic-only methods. We also compare compare our method with\\r\\nstate-of-the-art semantic image synthesis models. These methods include SPADE\\r\\n[39], ASAP-Net [47], CRN [6], SIMS [43], Pix2Pix [21] and Pix2PixHD [57].\\r\\nPerformance Metrics. We follow the evaluation protocol of previous methods\\r\\n[47,39]. We measure the quality of generated images using the Fre\\xcc\\x81chet Inception\\r\\n\\x0c\\n\\n10     R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n                                                       Taskonomy dataset performance\\r\\n                                    73.8                         74.6\\r\\n                            72.3                                                                                                      Semantic\\r\\n                      70                                                                                                              Sparse\\r\\n                                                66.1\\r\\n                                                                                                                                      Dense\\r\\n                      60\\r\\n\\r\\n\\r\\n                      50\\r\\n                                                                                  43.8\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                FID\\r\\n                      40                                                                           37.9            37.1\\r\\n\\r\\n                                                                                                                               30.6\\r\\n                      30\\r\\n\\r\\n\\r\\n                      20\\r\\n\\r\\n\\r\\n                      10\\r\\n\\r\\n\\r\\n                       0\\r\\n                           SPADE   ASAP       SPADE            ASAP          CLAM-baseline       TLAM         CLAM-baseline   TLAM\\r\\n                                           Naive-baseline   Naive-baseline    sparse (ours)   sparse (ours)      (ours)       (ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 5. Model comparison. Our TLAM method preforms better compared to the\\r\\nestablished baselines as well as to the SotA models which use semantic segmentation\\r\\nlabels only.\\r\\n\\r\\n\\r\\nDistance (FID) [18], which compares the distribution between generated data\\r\\nand real data. The FID summarizes how similar two groups of images are in\\r\\nterms of visual feature statistics. The second metric is the segmentation score,\\r\\nobtained by evaluating mean-Intersection-over-Union (mIoU) and pixel accuracy\\r\\n(accu) of a semantic segmentation model applied to the generated images. We use\\r\\nstate-of-the-art semantic segmentation networks DRN-D-105 [60] for Cityscapes\\r\\nand DeepLabv3plus [4] for NYU depth v2 dataset.\\r\\n\\r\\n4.3   Quantitative Results\\r\\nImage Generation. Table 1 reports the FID scores obtained on the Taskonomy\\r\\ndataset. We compare our method with several baselines and with different label\\r\\nsparsity. Our TLAM significantly outpreforms SPADE and ASAP SotA meth-\\r\\nods, which use only semantic segmentation maps as inputs. This shows that\\r\\nusing different spatial input labels can indeed improve the generation quality.\\r\\nMoreover, when SPADE and ASAP merge multiple labels as an input, they do\\r\\nnot preform significantly better than when using just the semantics. This empha-\\r\\nsizes the difficulty of merging multiple spatial labels, which are heterogeneous in\\r\\nnature. Some labels represent semantic image properties, while others represent\\r\\ngeometric properties. Also, some labels are continuous, while others are discrete.\\r\\nFurthermore, our TLAM preforms better than the CLAM-baseline, showing the\\r\\nvalue of having a better label merging network to deal with the heterogeneity\\r\\npresent in the input labels. Finally, we compare TLAM and the CLAM-baseline\\r\\nwith dense and sparse labels. As expected, having dense labels achieves better\\r\\nimage quality. The visual quality when using 50% sparse labels is close to when\\r\\nusing dense labels. This is interesting and desirable, since in practical scenarios\\r\\none often ends up having sparse labels [42].\\r\\n    The results on Cityscapes and NYU are summarized in Tables 2a and 2b.\\r\\nOn the Cityscape dataset, we compare our method with the SotA methods and\\r\\nreport FID, and segmentation mIoU and accuracy. Our method achieves better\\r\\naccuracy compared to the other methods. As reported in [39] the SIMS model\\r\\nproduces a lower FID, but has poor segmentation accuracy on the Cityscapes.\\r\\nThis is because SIMS synthesizes an image by first stitching image patches from\\r\\nthe training dataset. On the NYU dataset, our method achieves better mIoU\\r\\n\\x0c\\n\\n                               Spatially Multi-conditional Image Generation      11\\r\\n\\r\\nand accuracy. Unfortunately, we are unable to report the FID, due to the small\\r\\nsize of only 249 images in the validation set.\\r\\nTraining Convergence. Figure 6a shows the evolution of FID during training\\r\\non Taskonomy. We evaluate the FID on the validation set every 20 epochs for\\r\\nTLAM, and the ASAP naive and CLAM baselines. One can observe that TLAM\\r\\nquickly achieves a very good FID, even after 20 epochs. At this instance, TLAM\\r\\nhas 2.5 and 1.3 times better FID than those of the naive and CLAM baseline,\\r\\nrespectively. We can also see that TLAM converges faster than the other models.\\r\\nOne training epoch in Figure 6a takes about 1.7hrs for our method on one\\r\\nGeForce GTX TITAN X GPU.\\r\\n\\r\\n\\r\\n4.4   Qualitative Results\\r\\n\\r\\nTo visually compare our TLAM label merging, we present qualitative results\\r\\nfor both dense and sparse labels on the Taskonomy dataset in Figure 4, to-\\r\\ngether with semantic-only baselines. TLAM with all-dense and all-sparse labels\\r\\ngenerates high-fidelity images. Our method is able to generate fine structural de-\\r\\ntails such as lights, decorations, and even mirror reflections clearly better than\\r\\nSPADE and ASAP. The results on the Cityscapes dataset are depicted in Fig-\\r\\nure 8. Notably, our method with sparse labels achieves similar visual quality as\\r\\nother methods. Figure 9 shows qualitative results on the NYU dataset, where\\r\\nPix2PixHD also renders images with good quality. However, Pix2PixHD fails to\\r\\ncapture the lighting conditions, in contrast to our method. Notably, our method\\r\\ncaptures the rich geometric structures (such as on the ceiling), thanks to the\\r\\ngeometric labels. Please, refer to our supplementary material for more visual\\r\\nresults. Overall, the qualitative results demonstrate the effectiveness of TLAM,\\r\\nwhich exploits novel pixel-wise label transformers, even for sparse labels.\\r\\n\\r\\n\\r\\n4.5   Label Sensitivity Study\\r\\n\\r\\nWe analyse the sensitivity of our method with regards to the provided labels on\\r\\nthe Taskonomy dataset.\\r\\nLabel Sparsity. Figure 6b shows how the FID is affected by label sparsity.\\r\\nExperiments conducted on increasing sparsity from 10% to 70%, show a steady\\r\\ndegradation of FID with less available labels. Note that our model already achieves\\r\\ngood FID using only 30% of available labels. The experiments were conducted\\r\\nusing a single model trained with 50% sparsity. It also shows the generalizability\\r\\nof our method across various levels of sparsity.\\r\\nRemoval of Labels. In Figure 6c, we plot the FID after removing each label\\r\\nfrom the input, using the TLAM model trained on Taskonomy with 50% sparsity.\\r\\nWe observe that among the five labels, edges play the most significant role. On\\r\\nthe other hand, the semantics and depth are the most dispensable. Nevertheless,\\r\\nthe removal of any label results into a worsening of the FID at least by a factor of\\r\\n1.3. This suggests that all labels provide different information crucial for image\\r\\ngeneration, while being mutually complimentary.\\r\\n\\x0c\\n\\n12                 R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n                FID convergence of different models                                        Effects of different label sparsity                                   Effects of dropping input labels\\r\\n                                              CLAM-baseline (ours)            55        Sparse labels\\r\\n                                              ASAP Naive-baseline                       All labels\\r\\n                                                                                                                                                80\\r\\n                                              TLAM (ours)\\r\\n      100\\r\\n                                                                              50\\r\\n\\r\\n                                                                                                                                                60\\r\\n       80                                                                     45\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                        FID\\r\\nFID\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                          FID\\r\\n                                                                                                                                                40\\r\\n       60                                                                     40\\r\\n\\r\\n\\r\\n\\r\\n                                                                              35                                                                20\\r\\n       40\\r\\n\\r\\n\\r\\n                                                                              30\\r\\n            0      20       40           60         80         100                 10     20            30      40      50   60     70           0\\r\\n                                                                                                                                                     Semantics      Edges     Normals    Depth      Curvature\\r\\n                                 Epoch                                                                  Label sparsity [%]                            dropped      dropped    dropped   dropped      dropped\\r\\n\\r\\n\\r\\n\\r\\n(a) Convergence        of                                               (b) Effects of differ-                                            (c) Effects of dropping\\r\\ndifferent models. We                                                    ent label sparsity. Our                                           input labels. We exam-\\r\\ncompare FID scores dur-                                                 model achieves good FID                                           ine how dropping a spe-\\r\\ning training of our TLAM                                                score even when a frac-                                           cific label affects the im-\\r\\nmethod to the Naive and                                                 tion of labels is pre-                                            age quality of our TLAM\\r\\nCLAM baselines. Our                                                     sented during inference.                                          model. Dropping any label\\r\\nmethod converges faster                                                 With more labels present,                                         degrades FID which leads\\r\\nand achieves better FID                                                 our model achieves better                                         to the conclusion that all\\r\\ncompared to those of the                                                performance, even though                                          labels provide useful infor-\\r\\nbaselines at every point                                                it was trained for 50% la-                                        mation for image synthe-\\r\\nduring training.                                                        bel sparsity.                                                     sis.\\r\\n\\r\\nFig. 6. Training convergence and label sensitivity. We analyse behavioural as-\\r\\npects of our models with regards to training and labels.\\r\\n\\r\\n       Concept                       Image                           Semantics                    Normals                         Edges                  Depth                    Curvature\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 7. Concept Tensor visualization. From left to right: Concept Tensor projected\\r\\nto RGB; original image; five different input labels.\\r\\n\\r\\n\\r\\n   We conclude that the proposed label merging block can successfully deal with\\r\\nincomplete labels and is able to exploit information from all available labels.\\r\\n\\r\\n4.6             Concept Visualization and Image Editing\\r\\nConcept Visualization. To visualize the concept tensor Z \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x9796 , we\\r\\nproject it to 3 channels, using Principal Component Analysis [11], and present\\r\\nit in Figure 7, along with the corresponding image and labels. One can observe\\r\\nthat the visualized concept tensor indeed resembles different aspects of the input\\r\\nlabels (e.g. edges and normal orientations).\\r\\nGeometric Image Editing with User Inputs. In order to demonstrate an\\r\\nintuitive application of our method, we perform image editing by inserting a new\\r\\nobject into the scene. Table 10 shows how our method can mimic rendering while\\r\\nallowing the geometric manipulation of an image. In this application, we render\\r\\n\\x0c\\n\\n                              Spatially Multi-conditional Image Generation     13\\r\\n\\r\\n                                                      TLAM      Sparse-TLAM\\r\\n      Label      Reference SPADE [39] ASAP [47]\\r\\n                                                      (Ours)       (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 8. Visual comparison on the Cityscapes. Our approach achieves a visual\\r\\nquality on par with the compared methods.\\r\\n\\r\\nReference Pix2pix [21] CRN [6] pix2pixHD [57] SPADE [39] ASAP [47] TLAM (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 9. Visual comparison on NYU. Our method generates images that better\\r\\ncapture the lighting and geometry with more details.\\r\\n\\r\\n\\r\\n\\r\\na table in the given image, by simply augmenting different labels to include label\\r\\ninformation derived from a 3D model. Our method is able to render the table\\r\\nrealistically within the image, while ASAP and SPADE perform unsatisfactorily.\\r\\nFigure 11 shows another example of inserting an object of interest into the\\r\\nimage, as well as one example of removing a certain object from the image by\\r\\naugmenting the labels.\\r\\n\\r\\n\\r\\n5   Conclusion\\r\\nIn this work, we offer a new perspective on image generation as inverse of image\\r\\nunderstanding. In the same way as image understanding involves the solving\\r\\nof multiple different tasks, we desire the control over the generation process to\\r\\ninclude multiple input labels of various kinds. To this end, we design a neural\\r\\nnetwork architecture that is capable of handling sparse and heterogeneous labels,\\r\\nby mapping them to a homogeneous concept space in a pixel-wise fashion. With\\r\\nour proposed module, we can equip spatially conditioned generators with the\\r\\ndesired properties. From our experiments on challenging datasets, we conclude\\r\\nthat the benefits and flexibility of this additional layer of control gives way to\\r\\nexciting results beyond the state-of-the-art.\\r\\n\\x0c\\n\\n14       R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n            Method mIoU Accuracy FID                    Method mIoU Accuracy\\r\\n            CRN [6] 52.4  77.1   104.7               SPADE [39] 33.1  47.4\\r\\n          SIMS [43] 47.2  75.5   49.7                 ASAP [47] 36.2  49.1\\r\\n        Pix2Pix [21] 39.5 78.3   80.7               TLAM (Ours) 38.3  53.1\\r\\n     Pix2PixHD [57] 58.3  81.4   95.0\\r\\n        SPADE [39] 62.3   81.9   71.8\\r\\n         ASAP [47] 44.9   78.6   72.5        (b) NYU dataset. Our method demon-\\r\\n      TLAM (Ours) 45.5    85.3   68.3        strates effective of multi-conditioning\\r\\n(a) Cityscapes dataset. Our method is        with ASAP. Note that SPADE and ASAP\\r\\nusing the ASAP backbone.                     are the SotA in semantic conditioning.\\r\\n\\r\\n Table 2. Quantitative results. Our method enables successful multi-conditioning.\\r\\n\\r\\n\\r\\n     Original    Curvature     Normals         SPADE           ASAP          TLAM\\r\\n      image     user-provided mask by user       [39]           [47]         (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 10. Geometric image editing with user inputs. From left to right: original\\r\\nimage; curvature label (provided among five) of the table to be inserted into the image;\\r\\nsemantics of the table on the normal map of the scene; generated image using SPADE,\\r\\nASAP and our method, respectively. During editing, we use dense labels of the original\\r\\nimage, where we introduce five labels derived from a texture-less 3D model of a table\\r\\n(object of interest)\\xe2\\x80\\x93 labels are first edited to introduce the object, followed by the\\r\\nimage generation using the proposed TLAM method.\\r\\n\\r\\n           Original       Mask       SPADE [39]      ASAP [47]    TLAM (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 11. Object removal and insertion. From left to right: original image; mask of\\r\\nthe inserted/removed object; generated image using SPADE, ASAP and our method,\\r\\nrespectively. Top row shows removal of chairs from the image, by removing their labels.\\r\\nBottom row shows insertion of a TV, by inserting the labels provided by the user. The\\r\\nmask for removal/insertion is manually chosen by the user. Note that our TLAM\\r\\ngenerates consistent images, unlike the compared methods.\\r\\n\\x0c\\n\\n    Spatially Multi-conditional Image Generation\\r\\n               Supplementary material\\r\\n\\r\\n          Ritika Chakraborty\\xe2\\x8b\\x861 Nikola Popovic\\xe2\\x8b\\x861 , Danda Pani Paudel1 ,\\r\\n                       Thomas Probst1 , Luc Van Gool1,2\\r\\n               1\\r\\n                Computer Vision Laboratory, ETH Zurich, Switzerland\\r\\n                    2\\r\\n                      VISICS, ESAT/PSI, KU Leuven, Belgium\\r\\n       {critika, nipopovic, paudel, probstt, vangool}@vision.ee.ethz.ch\\r\\n\\r\\n    This document provides additional details, which complement the main pa-\\r\\nper. We provide the complete network diagram of our generator in Section A.\\r\\nMore implementation details are contained in in Section B. The visualization\\r\\nof input labels under different sparsity can be found in Section C. Additional\\r\\nqualitative results are presented in Section D. Finally, a discussion about ethical\\r\\nand societal impacts is contained in Section E. Also, please refer to our video\\r\\nsupplementary for the example of geometric editing.\\r\\n\\r\\n\\r\\nA      Generator Overview\\r\\n\\r\\nWhole Network. The network diagram of our generator is presented in Fig-\\r\\nure 1. The figure shows how our proposed Label Merging Block TLAM is con-\\r\\nnected to the ASAP-Net [47] generator, which we use as the backbone.\\r\\nASAP-Net Generator. The generator takes the Concept Tensor Z \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97d\\r\\nas an input, similar to taking the semantic labels in the original design. It then\\r\\noutputs a tensor of weights, which parameterize the pixelwise spatially-varying\\r\\nmulti-layer perceptrons (MLPs). The MLPs, with infered weights, compute the\\r\\nfinal output image by taking the Concept Tensor Z as their input.\\r\\n\\r\\n\\r\\nB      Implementation Details\\r\\n\\r\\nProjection Block. The Projection Block projects each input label into an\\r\\nembedding space with the same dimensionality. Every input label is processed\\r\\nby one 1x1 convolution layer, followed by a nonlinear activation. The projection\\r\\nis a 96-dimensional tensor for each label.\\r\\nConcept generation Block. The Concept Generation Block takes the pro-\\r\\njected tensors as input, and operates over them in a pixel-wise fashion. For each\\r\\npixel, we thus have an embedding vector for each task. We add a label specific-\\r\\nencoding to the embedding vector of each label, as a way of signalizing which\\r\\ntask the embedding corresponds to. For every pixel, the concept generation block\\r\\nprocesses each set of encoded labels with a transformer. The transformer consists\\r\\nof 3 layers, where each uses 3 heads.\\r\\n\\xe2\\x8b\\x86\\r\\n    Equal contributions.\\r\\n\\x0c\\n\\n16      R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n\\r\\n                                                       ASAP-Net Backbone\\r\\n                                                                                  Generated Image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            p\\r\\n                                                                           MPL at p\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 1. Label Merging TLAM block and ASAP-Net Backbone. The input labels\\r\\nare processed by the label merging network to output the Concept Tensor. The ASAP\\r\\ngenerator takes the Concept Tensor Z \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97d as an input and outputs a tensor\\r\\nof weights. Those weights are parameters of pixelwise, spatially-varying, MLPs, which\\r\\ncompute the final output image from the Concept Tensor Z.\\r\\n\\r\\n\\r\\nC    Visualization of input Labels\\r\\nIn Figure 2 we show examples of the input labels from the Taskonomy dataset.\\r\\nThese labels include semantic segmentation, normals, depth, edges and curva-\\r\\nture. Furthermore, in Figure 3, 4 and 5 we show examples of labels from the\\r\\nTaskonomy dataset, with 70%, 50% and 30% label sparsity. To generate sparse\\r\\nlabels, we look at spatial areas corresponding to distinct semantic segmentation\\r\\ninstances. For the sparsity of S, we randomly drop the labels with S% probabil-\\r\\nity, independently for every label inside every area. Higher sparsity means there\\r\\nis a higher probability that a semantic region will be masked out for all labels.\\r\\n\\r\\n\\r\\nD    Additional Qualitative Results\\r\\nIn Figure 6 we show images synthesized with our TLAM model using dense\\r\\ninput labels (semantics, curvature, edges, normals and depth), on the Taskonomy\\r\\ndataset. In Figure 7 we show images synthesized with our TLAM model using\\r\\nsparse input labels (50% sparsity), on the Taskonomy dataset. Finally, Figure 8\\r\\nshows additional visual comparison on the Cityscapes dataset, where we compare\\r\\nour TLAM and Sparse-TLAM methods with SPADE [39] and ASAP-Net [47].\\r\\n\\r\\n\\r\\nE    Ethical and Societal Impact\\r\\nThis work is going one step further into the image generation. While bringing\\r\\ngreat potential artistic value to the general public, such technology can be mis-\\r\\nused for fraudulent purposes. Despite the generated images looking realistic, this\\r\\nissue can be partially mitigated by learning-based fake detection [16,55,54].\\r\\n    In regards to limitations, our method is not designed with a particular focus\\r\\non balanced representation of appearance and labels. Therefore, image genera-\\r\\ntion may behave unexpected on certain conditions, groups of objects or people.\\r\\nWe recommend application-specific strategies for data collection and training to\\r\\nensure the desired outcome [1,58].\\r\\n\\x0c\\n\\n                                   Spatially Multi-conditional Image Generation   17\\r\\n\\r\\n         Image         Semseg      Normals      Depth      Edges    Curvature\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2. Examples of dense input labels. Samples of the Taskonomy dataset with\\r\\nall input labels visualized.\\r\\n\\r\\n                                             Image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                  Semseg        Normals       Depth     Edges      Curvature\\r\\n       sparsity\\r\\n        70 %\\r\\n       sparsity\\r\\n        50 %\\r\\n       sparsity\\r\\n        30 %\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3. One example of sparse input labels. One sample from the Taskonomy\\r\\ndataset where input labels have the sparsity of 70%, 50% and 30%.\\r\\n\\x0c\\n\\n18    R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n\\r\\n                                    Image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                 Semseg   Normals     Depth        Edges      Curvature\\r\\n      sparsity\\r\\n       70 %\\r\\n      sparsity\\r\\n       50 %\\r\\n      sparsity\\r\\n       30 %\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 4. One example of sparse input labels. One sample from the Taskonomy\\r\\ndataset where input labels have the sparsity of 70%, 50% and 30%.\\r\\n\\r\\n                                    Image\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                 Semseg   Normals     Depth        Edges      Curvature\\r\\n      sparsity\\r\\n       70 %\\r\\n      sparsity\\r\\n       50 %\\r\\n      sparsity\\r\\n       30 %\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 5. One example of sparse input labels. One sample from the Taskonomy\\r\\ndataset where input labels have the sparsity of 70%, 50% and 30%.\\r\\n\\x0c\\n\\n                             Spatially Multi-conditional Image Generation   19\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 6. Images generated using dense input labels. Here we see images generated\\r\\nwith our proposed TLAM label merging model, using dense input labels from the\\r\\nTaskonomy dataset.\\r\\n\\x0c\\n\\n20     R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 7. Images generated using 50% sparse input labels. Here we see images\\r\\ngenerated with our proposed TLAM label merging model, using input labels from the\\r\\nTaskonomy dataset with 50% sparsity.\\r\\n\\x0c\\n\\n                            Spatially Multi-conditional Image Generation     21\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                           SPADE        ASAP         TLAM      Sparse-TLAM\\r\\n     Label      Image\\r\\n                             [39]        [47]        (Ours)       (Ours)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 8. Image generation of different methods on Cityscapes. Here we compare\\r\\ngenerated results of our methods TLAM and Sparse-TLAM to SPADE and ASAP-Net.\\r\\n\\x0c\\n\\n22      R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\nReferences\\r\\n\\r\\n 1. Alvi, M.S., Zisserman, A., Nella\\xcc\\x8aker, C.: Turning a blind eye: Explicit removal of\\r\\n    biases and variation from deep neural network embeddings. In: ECCV Workshops\\r\\n    (2018)\\r\\n 2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization (2016)\\r\\n 3. Brock, A., Donahue, J., Simonyan, K.: Large scale gan training for high fidelity\\r\\n    natural image synthesis. arXiv preprint arXiv:1809.11096 (2018)\\r\\n 4. Cao, J., Leng, H., Lischinski, D., Cohen-Or, D., Tu, C., Li, Y.: Shapeconv: Shape-\\r\\n    aware convolutional layer for indoor rgb-d semantic segmentation. arXiv preprint\\r\\n    arXiv:2108.10528 (2021)\\r\\n 5. Chandran, P., Zoss, G., Gotardo, P., Gross, M., Bradley, D.: Adaptive convolutions\\r\\n    for structure-aware style transfer. In: Proceedings of the IEEE/CVF Conference\\r\\n    on Computer Vision and Pattern Recognition (CVPR). pp. 7972\\xe2\\x80\\x937981 (June 2021)\\r\\n 6. Chen, Q., Koltun, V.: Photographic image synthesis with cascaded refinement\\r\\n    networks. pp. 1520\\xe2\\x80\\x931529 (10 2017). https://doi.org/10.1109/ICCV.2017.168\\r\\n 7. Cook, R.L., Carpenter, L., Catmull, E.: The reyes image rendering architecture.\\r\\n    ACM SIGGRAPH Computer Graphics 21(4), 95\\xe2\\x80\\x93102 (1987)\\r\\n 8. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\\r\\n    Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\\r\\n    understanding. In: Proceedings of the IEEE Conference on Computer Vision and\\r\\n    Pattern Recognition (CVPR) (June 2016)\\r\\n 9. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\\r\\n    T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\\r\\n    An image is worth 16x16 words: Transformers for image recognition at scale. ICLR\\r\\n    (2021)\\r\\n10. Dumoulin, V., Perez, E., Schucher, N., Strub, F., Vries, H.d.,\\r\\n    Courville, A., Bengio, Y.: Feature-wise transformations. Distill (2018).\\r\\n    https://doi.org/10.23915/distill.00011\\r\\n11. F.R.S., K.P.: Liii. on lines and planes of closest fit to systems of points in space.\\r\\n    Philosophical Magazine Series 1 2, 559\\xe2\\x80\\x93572\\r\\n12. Gatys, L.A., Ecker, A.S., Bethge, M.: A neural algorithm of artistic style (2015)\\r\\n13. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neu-\\r\\n    ral networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition\\r\\n    (CVPR) pp. 2414\\xe2\\x80\\x932423 (2016)\\r\\n14. Godard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocular depth esti-\\r\\n    mation with left-right consistency. In: CVPR (2017)\\r\\n15. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\\r\\n    Courville, A., Bengio, Y.: Generative adversarial nets. Advances in neural infor-\\r\\n    mation processing systems 27 (2014)\\r\\n16. Guarnera, L., Giudice, O., Battiato, S.: Deepfake detection by analyzing convolu-\\r\\n    tional traces. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recog-\\r\\n    nition Workshops (CVPRW) pp. 2841\\xe2\\x80\\x932850 (2020)\\r\\n17. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus) (2020)\\r\\n18. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained\\r\\n    by a two time-scale update rule converge to a local nash equilibrium. In: NIPS\\r\\n    (2017)\\r\\n19. Huang, L., Qin, J., Zhou, Y., Zhu, F., Liu, L., Shao, L.: Normalization techniques\\r\\n    in training dnns: Methodology, analysis and application (2020)\\r\\n\\x0c\\n\\n                                  Spatially Multi-conditional Image Generation           23\\r\\n\\r\\n20. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance\\r\\n    normalization. In: Proceedings of the IEEE International Conference on Computer\\r\\n    Vision (ICCV) (Oct 2017)\\r\\n21. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-\\r\\n    tional adversarial networks. CVPR (2017)\\r\\n22. Jing, Y., Liu, X., Ding, Y., Wang, X., Ding, E., Song, M., Wen, S.: Dynamic\\r\\n    instance normalization for arbitrary style transfer (2019)\\r\\n23. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and\\r\\n    super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) Computer\\r\\n    Vision \\xe2\\x80\\x93 ECCV 2016. pp. 694\\xe2\\x80\\x93711. Springer International Publishing, Cham (2016)\\r\\n24. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative\\r\\n    adversarial networks (2019)\\r\\n25. Kato, H., Beker, D., Morariu, M., Ando, T., Matsuoka, T., Kehl, W., Gaidon, A.:\\r\\n    Differentiable rendering: A survey (2020)\\r\\n26. Kato, H., Ushiku, Y., Harada, T.: Neural 3d mesh renderer. In: The IEEE Confer-\\r\\n    ence on Computer Vision and Pattern Recognition (CVPR) (2018)\\r\\n27. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR\\r\\n    abs/1412.6980 (2015)\\r\\n28. Laine, S., Hellsten, J., Karras, T., Seol, Y., Lehtinen, J., Aila, T.: Modular primi-\\r\\n    tives for high-performance differentiable rendering. ACM Transactions on Graphics\\r\\n    39(6) (2020)\\r\\n29. Li, T.M., Aittala, M., Durand, F., Lehtinen, J.: Differentiable monte carlo ray\\r\\n    tracing through edge sampling. ACM Trans. Graph. (Proc. SIGGRAPH Asia)\\r\\n    37(6), 222:1\\xe2\\x80\\x93222:11 (2018)\\r\\n30. Lin, K., Wang, L., Liu, Z.: End-to-end human pose and mesh reconstruction with\\r\\n    transformers (2021)\\r\\n31. Liu, S., Li, T., Chen, W., Li, H.: Soft rasterizer: A differentiable renderer for image-\\r\\n    based 3d reasoning (2019)\\r\\n32. Loubet, G., Holzschuch, N., Jakob, W.: Reparameterizing discontinuous integrands\\r\\n    for differentiable rendering. Transactions on Graphics (Proceedings of SIGGRAPH\\r\\n    Asia) 38(6) (Dec 2019). https://doi.org/10.1145/3355089.3356510\\r\\n33. Ma, L., Jia, X., Sun, Q., Schiele, B., Tuytelaars, T., Gool, L.V.: Pose guided person\\r\\n    image generation. In: NIPS (2017)\\r\\n34. Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint\\r\\n    arXiv:1411.1784 (2014)\\r\\n35. Mirza, M., Osindero, S.: Conditional generative adversarial nets. ArXiv\\r\\n    abs/1411.1784 (2014)\\r\\n36. Nathan Silberman, Derek Hoiem, P.K., Fergus, R.: Indoor segmentation and sup-\\r\\n    port inference from rgbd images. In: ECCV (2012)\\r\\n37. Nimier-David, M., Speierer, S., Ruiz, B., Jakob, W.: Radiative back-\\r\\n    propagation: An adjoint method for lightning-fast differentiable rendering.\\r\\n    Transactions on Graphics (Proceedings of SIGGRAPH) 39(4) (Jul 2020).\\r\\n    https://doi.org/10.1145/3386569.3392406\\r\\n38. Nimier-David, M., Vicini, D., Zeltner, T., Jakob, W.: Mitsuba 2: A retargetable for-\\r\\n    ward and inverse renderer. Transactions on Graphics (Proceedings of SIGGRAPH\\r\\n    Asia) 38(6) (Dec 2019). https://doi.org/10.1145/3355089.3356498\\r\\n39. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with\\r\\n    spatially-adaptive normalization (2019)\\r\\n40. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas,\\r\\n    D., Black, M.J.: Expressive body capture: 3d hands, face, and body from a single\\r\\n    image (2019)\\r\\n\\x0c\\n\\n24      R. Chakraborty\\xe2\\x8b\\x86 , N. Popovic\\xe2\\x8b\\x86 , D. Pani Paudel, T. Probst, L. Van Gool\\r\\n\\r\\n41. Perez, E., Strub, F., de Vries, H., Dumoulin, V., Courville, A.: Film: Visual rea-\\r\\n    soning with a general conditioning layer (2017)\\r\\n42. Popovic, N., Paudel, D.P., Probst, T., Sun, G., Van Gool, L.: Compositetask-\\r\\n    ing: Understanding images by spatial composition of tasks. In: Proceedings of the\\r\\n    IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6870\\xe2\\x80\\x93\\r\\n    6880 (2021)\\r\\n43. Qi, X., Chen, Q., Jia, J., Koltun, V.: Semi-parametric image synthesis. pp. 8808\\xe2\\x80\\x93\\r\\n    8816 (06 2018). https://doi.org/10.1109/CVPR.2018.00918\\r\\n44. Ravi, N., Reizenstein, J., Novotny, D., Gordon, T., Lo, W.Y., Johnson, J., Gkioxari,\\r\\n    G.: Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501 (2020)\\r\\n45. Reddy, M.D.M., Basha, M.S.M., Hari, M.M.C., Penchalaiah, M.N.: Dall-e: Creating\\r\\n    images from text\\r\\n46. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\\r\\n    ical image segmentation (2015)\\r\\n47. Rott Shaham, T., Gharbi, M., Zhang, R., Shechtman, E., Michaeli, T.: Spatially-\\r\\n    adaptive pixelwise networks for fast image translation. In: Computer Vision and\\r\\n    Pattern Recognition (CVPR) (2021)\\r\\n48. Sitzmann, V., Zollho\\xcc\\x88fer, M., Wetzstein, G.: Scene representation networks:\\r\\n    Continuous 3d-structure-aware neural scene representations. arXiv preprint\\r\\n    arXiv:1906.01618 (2019)\\r\\n49. Sun, G., Probst, T., Paudel, D.P., Popovic, N., Kanakis, M., Patel, J., Dai, D.,\\r\\n    Van Gool, L.: Task switching network for multi-task learning. In: Proceedings\\r\\n    of the IEEE/CVF International Conference on Computer Vision. pp. 8291\\xe2\\x80\\x938300\\r\\n    (2021)\\r\\n50. Tewari, A., Fried, O., Thies, J., Sitzmann, V., Lombardi, S., Sunkavalli, K., Martin-\\r\\n    Brualla, R., Simon, T., Saragih, J., Nie\\xc3\\x9fner, M., Pandey, R., Fanello, S., Wet-\\r\\n    zstein, G., Zhu, J.Y., Theobalt, C., Agrawala, M., Shechtman, E., Goldman, D.B.,\\r\\n    Zollho\\xcc\\x88fer, M.: State of the art on neural rendering (2020)\\r\\n51. Tewari, A., Zollo\\xcc\\x88fer, M., Bernard, F., Garrido, P., Kim, H., Perez, P., Theobalt, C.:\\r\\n    High-fidelity monocular face reconstruction based on an unsupervised model-based\\r\\n    face autoencoder. IEEE Transactions on Pattern Analysis and Machine Intelligence\\r\\n    pp. 1\\xe2\\x80\\x931 (2018). https://doi.org/10.1109/TPAMI.2018.2876842\\r\\n52. Tewari, A., Zollo\\xcc\\x88fer, M., Kim, H., Garrido, P., Bernard, F., Perez, P., Christian,\\r\\n    T.: MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised\\r\\n    Monocular Reconstruction. In: The IEEE International Conference on Computer\\r\\n    Vision (ICCV) (2017)\\r\\n53. Vaswani, A., Shazeer, N.M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\\r\\n    Kaiser, L., Polosukhin, I.: Attention is all you need. ArXiv abs/1706.03762 (2017)\\r\\n54. Verdoliva, L.: Media forensics and deepfakes: An overview. IEEE Journal of Se-\\r\\n    lected Topics in Signal Processing 14, 910\\xe2\\x80\\x93932 (2020)\\r\\n55. Wang, S., Wang, O., Zhang, R., Owens, A., Efros, A.A.: Cnn-generated images are\\r\\n    surprisingly easy to spot. . . for now. 2020 IEEE/CVF Conference on Computer\\r\\n    Vision and Pattern Recognition (CVPR) pp. 8692\\xe2\\x80\\x938701 (2020)\\r\\n56. Wang, T.C., Liu, M.Y., Zhu, J.Y., Liu, G., Tao, A., Kautz, J., Catanzaro, B.:\\r\\n    Video-to-video synthesis. In: Advances in Neural Information Processing Systems\\r\\n    (NeurIPS) (2018)\\r\\n57. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-\\r\\n    resolution image synthesis and semantic manipulation with conditional gans. In:\\r\\n    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\r\\n    (2018)\\r\\n\\x0c\\n\\n                                Spatially Multi-conditional Image Generation        25\\r\\n\\r\\n58. Wang, Z., Qinami, K., Karakozis, Y., Genova, K., Nair, P.Q., Hata, K., Rus-\\r\\n    sakovsky, O.: Towards fairness in visual recognition: Effective strategies for bias\\r\\n    mitigation. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recog-\\r\\n    nition (CVPR) pp. 8916\\xe2\\x80\\x938925 (2020)\\r\\n59. Xia, W., Yang, Y., Xue, J., Wu, B.: Tedigan: Text-guided diverse face image gen-\\r\\n    eration and manipulation. 2021 IEEE/CVF Conference on Computer Vision and\\r\\n    Pattern Recognition (CVPR) pp. 2256\\xe2\\x80\\x932265 (2021)\\r\\n60. Yu, F., Koltun, V., Funkhouser, T.: Dilated residual networks. In: Computer Vision\\r\\n    and Pattern Recognition (CVPR) (2017)\\r\\n61. Yu, Y., Smith, W.A.: Inverserendernet: Learning single image inverse rendering.\\r\\n    In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n    Recognition. pp. 3155\\xe2\\x80\\x933164 (2019)\\r\\n62. Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F.E.H., Feng, J., Yan, S.:\\r\\n    Tokens-to-token vit: Training vision transformers from scratch on imagenet. 2021\\r\\n    IEEE/CVF International Conference on Computer Vision (ICCV) pp. 538\\xe2\\x80\\x93547\\r\\n    (2021)\\r\\n63. Zamir, A.R., Sax, A., Shen, W.B., Guibas, L.J., Malik, J., Savarese, S.: Taskonomy:\\r\\n    Disentangling task transfer learning. In: IEEE Conference on Computer Vision and\\r\\n    Pattern Recognition (CVPR). IEEE (2018)\\r\\n64. Zhang, Z., Ma, J., Zhou, C., Men, R., Li, Z., Ding, M., Tang, J., Zhou, J., Yang,\\r\\n    H.: M6-ufc: Unifying multi-modal controls for conditional image synthesis. arXiv\\r\\n    preprint arXiv:2105.14211 (2021)\\r\\n65. Zhu, P., Abdal, R., Qin, Y., Wonka, P.: Sean: Image synthesis with semantic region-\\r\\n    adaptive normalization. In: Proceedings of the IEEE/CVF Conference on Com-\\r\\n    puter Vision and Pattern Recognition (CVPR) (June 2020)\\r\\n\\x0c', b'                                                                                                                                                                              1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                DOTS: An Open Testbed for Industrial Swarm\\r\\n                                                           Robotic Solutions\\r\\n                                                                Simon Jones, Emma Milner, Mahesh Sooriyabandara, and Sabine Hauert\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            Abstract\\xe2\\x80\\x94We present DOTS, a new open access testbed for\\r\\n                                         industrial swarm robotics experimentation. It consists of 20 fast\\r\\n                                         agile robots with high sensing and computational performance,\\r\\n                                         and real-world payload capability. They are housed in an arena\\r\\narXiv:2203.13809v1 [cs.RO] 25 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                         equipped with private 5G, motion capture, multiple cameras,\\r\\n                                         and openly accessible via an online portal. We reduce barriers\\r\\n                                         to entry by providing a complete platform-agnostic pipeline\\r\\n                                         to develop, simulate, and deploy experimental applications to\\r\\n                                         the swarm. We showcase the testbed capabilities with a swarm\\r\\n                                         logistics application, autonomously and reliably searching for and\\r\\n                                         retrieving multiple cargo carriers.\\r\\n                                           Index Terms\\xe2\\x80\\x94Swarm robotics, intralogistics, open testbed,\\r\\n                                         industrial swarm\\r\\n\\r\\n\\r\\n                                                                I. I NTRODUCTION\\r\\n                                                                                                               Fig. 1: DOTS robot, showing two forward-facing cameras and\\r\\n                                         M       ANY robots (10-1000+) working together to facilitate\\r\\n                                                 intralogistics in real-world settings promise to improve\\r\\n                                         productivity through the automatic transport, storage, inspec-\\r\\n                                                                                                               lifting platform on top.\\r\\n\\r\\n                                         tion, and retrieval of goods. Yet, existing systems typically\\r\\n                                                                                                               enabling easy scalability, adaptation and robustness. While\\r\\n                                         require sophisticated robots, carefully engineered infrastruc-\\r\\n                                                                                                               traditional logistics solutions prioritise throughput, speed and\\r\\n                                         ture to support the operations of the robots, and often central\\r\\n                                                                                                               operating cost, swarm logistics solutions will be measured\\r\\n                                         planners to coordinate the many-robot system (e.g. Amazon,\\r\\n                                                                                                               against additional \\xe2\\x80\\x98Swarm Ideals\\xe2\\x80\\x99, targeting zero setup, recon-\\r\\n                                         Ocado). Once operational, these solutions offer speed and\\r\\n                                                                                                               figuration time, and scaling effort, zero infrastructure, zero\\r\\n                                         precision, but the pre-operational investment cost is often high\\r\\n                                                                                                               training, and zero failure modes. It is these attributes that\\r\\n                                         and the flexibility can be low.\\r\\n                                                                                                               make swarm robotics the natural solution for logistics in more\\r\\n                                            Consequently, there is a critical unmet need in scenarios\\r\\n                                                                                                               dynamic and varied scenarios.\\r\\n                                         that would benefit from logistics solutions that are low-cost\\r\\n                                         and usable out-of-the-box. These robot solutions must adapt              To explore this potential, we present a new 5G-enabled\\r\\n                                         to evolving user requirements, scale to meet varying demand,          testbed for industrial swarm robotic solutions. The testbed\\r\\n                                         and be robust to the messiness of real-world deployments.             hosts 20 custom-built 250 mm robots called DOTS (Distributed\\r\\n                                         Examples include small and medium enterprises, local retail           Organisation and Transport System) that move fast, have long\\r\\n                                         shops, pop-up or flexible warehouses (COVID-19 distribution           battery life (6 hours), are 5G enabled, house a GPU, can sense\\r\\n                                         centres, food banks, refugee camps, airport luggage storage),         the environment locally with cameras and distance sensors,\\r\\n                                         and manufacturing of products with high variance or variation         as well as lift and transport payloads (2 kg per robot). The\\r\\n                                         (e.g. small series, personalised manufacturing). Flexible solu-       platform is modular and so can easily be augmented with\\r\\n                                         tions for such scenarios also have the potential to translate         new capabilities based on the scenarios to be explored (e.g. to\\r\\n                                         to other applications and settings such as construction or            manipulate items). To monitor experiments, the arena is fitted\\r\\n                                         inspection.                                                           with overhead cameras and a motion capture system, allowing\\r\\n                                            Swarm robotics offers a solution to this unmet need. Large         for precise telemetry and replay capabilities. The testbed is\\r\\n                                         numbers of robots, following distributed rules that react to          accessible remotely through custom-built cloud infrastructure,\\r\\n                                         local interactions with other robots or local perception of           allowing for experiments to be run from anywhere in the world\\r\\n                                         their environment, can give rise to efficient, flexible, and          and enabling future fast integration between the digital and\\r\\n                                         coordinated behaviours. A swarm engineering approach has              physical word through digital twinning. In addition we present\\r\\n                                         the potential to be flexible to the number of robots involved,        an integrated development environment useable on Windows,\\r\\n                                                                                                               Linux and OSX lowering the barriers to entry for users of the\\r\\n                                           S. Jones, E. Milner, and S. Hauert are at the University of         system.\\r\\n                                         Bristol, UK. simon2.jones@bristol.ac.uk, emma.milner@bristol.ac.uk,      Beyond characterising the testbed, making use of the de-\\r\\n                                         sabine.hauert@bristol.ac.uk\\r\\n                                           M. Sooriyabandara is at Toshiba Research, Bristol, UK,              velopment pipeline illustrated in Figure 2, we demonstrate the\\r\\n                                         Mahesh.Sooriyabandara@toshiba-bril.com                                steps and processes required to take a conceptually simple\\r\\n\\x0c\\n\\n                                                                                                                                      2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                    Develop              Simulate             Validate              Deploy                Analyse\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2: Using the DOTS IDE, controllers are developed and tested locally in simulation. When ready, controllers are validated\\r\\nusing the online portal and deployed to the physical testbed for an experimental run. Experimental data captured is available\\r\\nfor download from the portal and subsequent analysis.\\r\\n\\r\\n\\r\\nswarm logistics algorithm and successfully run it on real-world       IML Loadrunner [9] swarm is technically sophisticated and\\r\\nrobots to reliably find, collect, and deposit five payload carriers   physically considerably larger than our design but does not\\r\\nin an entirely decentralised way.                                     appear to be open.\\r\\n   This paper is structured in the following way; in Section II\\r\\nwe provide background and discuss related work. In Section\\r\\nIII we detail the physical, electronic, and software systems of                     III. M ATERIALS AND M ETHODS\\r\\nthe testbed, with characterisation of the performance of various\\r\\n                                                                        In the following sections, we detail the robots, the arena,\\r\\nsubsystems. In Section IV we build a complete demonstration\\r\\n                                                                      the online portal for remote experimentation, and the cross-\\r\\nof a distributed logistics task. Section V concludes the article.\\r\\n                                                                      platform integrated development environment.\\r\\n           II. BACKGROUND AND RELATED WORK\\r\\n   Two recent reviews of multi-robot technologies for ware-           A. DOTS robots\\r\\nhouse automation show that amongst over 100 papers in the\\r\\narea, only very few considered decentralised, distributed, or            The robots were designed from the start to be low-cost,\\r\\nswarm approaches [1], [2]. Of those that did, solutions were          around \\xc2\\xa31000 per robot, simple to construct, to allow relatively\\r\\nonly partially decentralised [3]. This suggests swarm solutions       large numbers to be built, and high capability, by using\\r\\nfor logistics have largely remained unexplored, although the          commodity parts in innovative ways. Each robot is 250 mm in\\r\\ninterest in distributed multi-robot systems for intralogistics is     diameter with a holonomic drive capable of rapid omnidirec-\\r\\ngrowing as shown in a recent review by [4]. At the same time,         tional movement, up to 2 ms\\xe2\\x88\\x921 and typically 6 hours of battery\\r\\nswarm robotics [5] has 30 years of history, mostly focussing          life. They are equipped with multiple sensors; 360\\xc2\\xb0 vision with\\r\\non conceptual problems in the laboratory with translation to          four cameras and a further camera looking vertically upwards,\\r\\napplications increasing in the last couple of years [6].              laser time-of-flight sensors for accurate distance sensing of\\r\\n   We now need fresh strategies to design and deploy swarm            surrounding obstacles, and multiple environment sensors. Each\\r\\nsolutions that eschew the mantra of emergence from interac-           robot has a lifting platform, allowing the transport of payloads.\\r\\ntion of simple agents and embrace the use of high specification       Onboard computation is provided with a high-specification\\r\\nrobots at the agent level to enhance the performance of a             Single Board Computer (SBC), with six ARM CPUs and a\\r\\nrobot swarm without sacrificing its inherent scalability and          GPU. And there are multiple forms of communication avail-\\r\\nadaptability in real-world applications. This is made possible        able - WiFi, two Bluetooth Low Energy (BLE5) programmable\\r\\nnow due to the convergence of technologies including high             radios, an ultrawideband (UWB) distance ranging radio, and\\r\\nindividual robot specifications (fast motion, high-computation,       the ability to add a 5G modem.\\r\\nand high-precision sensing), 5G networking to power commu-               1) Cost minimisation: A central driving factor in the design\\r\\nnication between humans and interconnected robots, access to          process was cost minimisation while still achieving good\\r\\nhigh onboard computational power that allows for sophisti-            performance. The rapid progress in mobile phone capabilities\\r\\ncated local perception of the world and new algorithms for            means that high performance sensors such as cameras are\\r\\nswarm control. Swarm testbeds for research do exist. The              now available at very low cost. Communications advances\\r\\nRobotarium [7] is a complete system of small robots, with             means that fully programmable Bluetooth Low Energy (BLE5)\\r\\nassociated simulator, online access, tracking, and automated          modules cost only a few pounds. Single board computers based\\r\\ncharging and management. The individual robots are not                on mobile phone SoCs are widely available, high definition\\r\\nautonomous though, with controller code executes on a central         cameras cost \\xc2\\xa34. And the market for consumer drones has\\r\\nserver, with the robots acting as peripherals. Duckietown [8]         made very high power-to-weight ratio motors available at\\r\\nis an open source platform of small cheap autonomous robots           much lower cost than specialised servo motors. We leverage\\r\\ndesigned for teaching autonomous self-driving, but the em-            these advances to reduce the cost per robot to \\xc2\\xa31000, so the\\r\\nphasis is on users building their own testbed. The Fraunhofer         complete testbed can have many robots.\\r\\n\\x0c\\n\\n                                                                                                                                       3\\r\\n\\r\\n\\r\\n\\r\\n                                                                           2) Robot chassis: The mechanical chassis, shown in Figure\\r\\n              TABLE I: Features of the DOTS robots                      3, is designed to be easy to fabricate in volumes of a few\\r\\n                                                                        10s. Custom parts are made using 3D printing or simple\\r\\n  Specification             Value        Notes                          milling operations. It is constructed around two disks of 2 mm\\r\\n  Diameter                  250 mm                                      thick aluminium, 250 mm in diameter, for the base and top\\r\\n  Height                    145 mm       Lifter lowered\\r\\n                            197mm        Lifter raised                  surfaces. Between the base and top plates and mechanically\\r\\n  Weight                    3 kg                                        joining them 100 mm apart are three omniwheel assemblies,\\r\\n  Lifter max load           2 kg                                        positioned at 120\\xc2\\xb0 separation. Each wheel assembly consists of\\r\\n  Max velocity              2 ms\\xe2\\x88\\x921\\r\\n  Max acceleration          4 ms\\xe2\\x88\\x922                                      an omniwheel, a drone motor with rotational position encoder,\\r\\n  Wheel torque              0.35 Nm                                     and a 5:1 timing belt reduction drive between motor and wheel.\\r\\n  Battery capacity          100 Wh       2x Toshiba SCiB 23 Ah          Mounted on the base plate are the single board computer\\r\\n  Endurance                 6 hours      All cameras enabled,\\r\\n                                         moderate vision processing     (SBC) and the power supply PCBs, in both cases thermally\\r\\n                                         and movement                   bonded to the base plate to provide passive cooling. The\\r\\n                            14 hours     No cameras, low level          battery pack sits in the central area of the base plate. On\\r\\n                                         processing, occasional\\r\\n                                         movement                       55 mm standoffs above the base plate is the mainboard PCB,\\r\\n  Carriers                  330x330 mm   175 mm clearance               which contains all the sensors, four cameras, motor drives, and\\r\\n  Processor                 RockPi4      6x ARM CPUs, ARM Mali          associated support electronics. The top plate holds the payload\\r\\n                                         T860MP4 GPU\\r\\n  Cameras                   OV5647       5x 120\\xc2\\xb0FOV video up to         lifting mechanism and an upward-facing camera, connecting\\r\\n                                         1080p30                        via USB and power leads to the mainboard. Alternate top\\r\\n  Proximity                 VL53L1CX     16x IR laser time-of-flight,   plates, with different actuators, for example a small robot arm,\\r\\n                                         3 m range, 10 mm precision\\r\\n  IMU                       LSM6DSM      6DoF accelerometer and         can easily be fitted.\\r\\n                                         gyroscope                         3) System architecture: Many subsystems within the robot\\r\\n  Magnetometer              LIS2MDL      3DoF                           need to be integrated and connected to the single board\\r\\n  Temperature,              Si7021-A20   Temperature accuracy\\r\\n  humidity                               \\xc2\\xb10.4 \\xc2\\xb0C, relative humidity     computer. The system architecture is shown in Figure 4.\\r\\n                                         \\xc2\\xb13%                            Communications between sensors, actuators, and the single\\r\\n  Absolute pressure         LPS22HB      260 hPa- 1260 hPa, accuracy    board computer takes place on three bus types; USB for\\r\\n                                         \\xc2\\xb10.1 hPa\\r\\n  Microphones               IM69D130     MEMS sensor, FPGA-based        high bandwidth, mostly MJPEG compressed video from the\\r\\n                                         CIC filter                     cameras, I2 C for low bandwidth, mostly various sensors, and\\r\\n                                                                        SPI for medium bandwidth deterministic latency, used for\\r\\n                                                                        controlling the motor movements. An FPGA is used for glue\\r\\n                                                                        logic, for example to interface to the MEMS microphones\\r\\n                                                                        and the programmable LEDs. The BLE5 and UWB radios\\r\\n                                                                        are reprogrammable from the SBC via their single wire debug\\r\\n                Camera                                                  (SWD) interfaces [10].\\r\\n                                                                           4) Vision: Vision is the primary modality by which the\\r\\n          Top plate                                                     robot can understand its environment. Miniature cameras have\\r\\n              Lifter                                                    been revolutionised by progress in mobile devices. It is now\\r\\n     BLE5 radios                                                        possible to buy a 5 megapixel camera also capable of multiple\\r\\n                                                                        video resolutions up to 1080p for \\xc2\\xa34. A central design issue\\r\\nRaspberry Pi Zero                                                       is how to best use this cheap data - image processing is\\r\\n     Camera x4                                                          computationally heavy. We designed the system to have all\\r\\n   RGB LED x16                                                          round vision for the robot, achieved by using four cameras\\r\\n                                                                        with a 120\\xc2\\xb0 field of view (FOV) around the perimeter of the\\r\\nIRToF sensor x16\\r\\n                                                                        robot. The two front-facing cameras have overlapping fields\\r\\n     Mainboard\\r\\n     UWB radio\\r\\n                                                                        of view to allow for the possibility of stereo depth extraction.\\r\\n                                                                        A fifth camera is fitted in the centre of the lifting platform\\r\\n        Stand-off\\r\\n                                                                        looking upwards so that visual navigation under load carriers\\r\\n     Battery pack\\r\\n                                                                        can be achieved with suitable targeting patterns.\\r\\n    RockPi 4 SBC                                                           The amount of data in an uncompressed video stream\\r\\n        Drone motor\\r\\n                                                                        is high, 640x480 at 30 fps takes around 18 MBytes/s of\\r\\n                                                                        bandwidth. For five cameras, this is close to 100 MBytes/s,\\r\\n               Base plate\\r\\n                                                                        which would require a high performance interconnect such\\r\\n                       Omniwheel                                        a gigabit ethernet, impractical at low cost and indicating a\\r\\n                                   PSU\\r\\n                                                                        need for image compression. Rather than using fixed function\\r\\nFig. 3: Exploded view of the robot chassis showing major                webcam hardware to perform this compression, we looked\\r\\ncomponents.                                                             for flexibility. The approach we took was to use a local\\r\\n                                                                        Raspberry Pi Zero computer for each camera to perform image\\r\\n                                                                        compression and then send the data over USB. This gives a\\r\\n\\x0c\\n\\n                                                                                                                                                                             4\\r\\n\\r\\n\\r\\n\\r\\n                            RPI            Cam 0\\r\\n                                                                     Locations of cameras              TABLE II: Vision system latency, from photon arrival to\\r\\n                           Zero 0                                        and motors\\r\\n                                                                                                       change in memory buffer (glass-to-algorithm) under different\\r\\n                            RPI            Cam 3                              0       3\\r\\n                           Zero 3                                                                      camera refresh rates and utilisation of the USB fabric. Results\\r\\n                            RPI                                                                        are averaged over 1000 frames.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                       0\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                              2\\r\\n                           Zero 4          Cam 4\\r\\n                                                                                  4\\r\\n                            RPI\\r\\n                           RPI Zero 3      Cam 2                                                           Cameras     Latency x\\xcc\\x84 (ms)   Latency \\xcf\\x83 (ms)   Latency (frames)\\r\\n                           Zero 2\\r\\n                                                                                                           30 Hz refresh rate\\r\\n                            RPI\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                        1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                             2\\r\\n                           Zero 1          Cam 1                                                              1              78.2             9.23             2.35\\r\\n                                                                                  1\\r\\n            USB     USB     BLE                                                                               2              72.8             10.7             2.18\\r\\n                    hub    radio 0                                                                            3              74.6             11.7             2.24\\r\\n RockPi 4                   BLE                SWI                                                            4              73.4             11.5             2.20\\r\\n  SBC                      radio 1                                                                            5              78.5             11.5             2.36\\r\\n                                                                    Motor                                  60 Hz refresh rate\\r\\n                                                                    drive 0\\r\\n                                                                                                              1              53.2             15.8             3.19\\r\\n                                                                              Motor                           2              45.2             9.55             2.71\\r\\n                                                                              drive 1\\r\\n                                                                                                              3              45.3             9.82             2.72\\r\\n                                                                                            Motor\\r\\n                                                                                            drive 2           4              58.3             12.9             3.50\\r\\n                                                                                                              5              58.4             14.6             3.50\\r\\n                           FT2232\\r\\n                                                             Step/Dir Step/Dir Step/Dir\\r\\n\\r\\n            Serial/GPIO\\r\\n            SPI                                               FPGA\\r\\n            I2C                                                                                        from each.\\r\\n                                                                                                             a) Vision latency: A basic performance metric when\\r\\n                                Serial\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                          I2C            SWD      I2C                Custom           I2C\\r\\n                                                           8x                                          using vision for motion control is the time delay between\\r\\n  12V       5V     3.3V                                   ADC\\r\\n   4A       4A      2A                    UWB            temp         IM69D130              VL53L1CX   photons arriving at the camera, and when data corresponding\\r\\n                                          radio         humidity         mic                   x16\\r\\n                                                                                                       to those photons is available in a memory buffer on the single\\r\\n          PSU                                           barometer     IM69D130\\r\\n                                                                         mic                  RGB\\r\\n                                                                                             LEDx16\\r\\n                                                                                                       board computer, known as Glass-to-Algorithm [13]. Measuring\\r\\n   2x SCiB cell battery                            magnetometer                                        the vision latency is non-trivial, see [14]. We used a set of eight\\r\\n      pack 100Wh\\r\\n     protection and                                       6DoF                                         LEDs in approximately the centre of the field of view of one\\r\\n       monitoring                                          IMU\\r\\n                                                                                                       camera, and performed image processing on the destination\\r\\nFig. 4: Architecture of the robot. Green for computation and                                           buffer to extract the binary data displayed on the LEDs. By\\r\\ncommunication, red for power, orange for sensors, and blue                                             setting a code on the LEDs then timing how long before the\\r\\nfor interconnect fabric.                                                                               code was visible in the buffer data we can measure the latency.\\r\\n                                                                                                       After each code is detected, the next in a Gray-code sequence\\r\\n                                                                                                       is displayed to accumulate multiple timings.\\r\\ncost per camera of around \\xc2\\xa315.                                                                            Data from each camera is transmitted to the single board\\r\\n   The Raspberry Pi Zero is a low cost small form factor                                               computer via a USB hub on the mainboard. We load the system\\r\\nSingle Board Computer with a camera interface. Although the                                            by running a varying number of cameras simultaneously.\\r\\nCPU has quite a low performance (ARM1176 700MHz), it is                                                Measurements are taken with different camera refresh rates.\\r\\nnot generally appreciated how powerful the associated VPU                                              The results are shown in Table II for different refresh rates\\r\\n(Video Processing Unit) and GPU (Graphics Processing Unit)                                             with varying numbers of other cameras running and streaming\\r\\nis. This allows for the possibility of utilising this power to                                         data at the same time. Times averaged over 1000 samples.\\r\\nperform local image processing, e.g. to perform fiducial recog-                                           Because the LEDs are positioned at approximately the\\r\\nnition locally, rather than loading the main central processor.                                        centre vertically of the camera field of view, and the camera\\r\\nIn this paper, we use standard applications to stream MJPEG                                            performs a raster scan to read the sensed image, there is\\r\\ncompressed frames with low latency from the photon arrival                                             a minimum of half a frame before data is available to the\\r\\nat the camera, to the presence of data in a buffer at the central                                      Raspberry Pi for processing and compression. Since the LEDs\\r\\nprocessor.                                                                                             are changed uncorrelated to the frame rate, there may be\\r\\n   Although cameras are cheap, processing image streams to                                             between zero and one additional frame of delay uniformly\\r\\ngive useful information is not, and although the single board                                          distributed, giving a mean of one frame delay before the start\\r\\ncomputer is quite capable, it is not in the same class as a                                            of processing, compression, transmission, and then decompres-\\r\\ndesktop PC. This means we are not free to easily run more                                              sion into the destination image buffer.\\r\\ncomplex algorithms without considerable optimisation efforts                                              The worst-case figures of 79 ms at 30 Hz and 58 ms at 60 Hz\\r\\nto, for example, utilise GPU processing. As an example, most                                           for the most heavily loaded cases of five cameras streaming\\r\\nvisual SLAM algorithms are evaluated on PC-class systems.                                              image data compare well with state-of-the-art systems, which\\r\\nORB-SLAM2 was evaluated in a system with an Intel Core                                                 is reported as 50-80 ms by [13].\\r\\ni7-4790 and this is just fast enough to run at real-time with a                                              b) Camera calibration: In order that the perimeter cam-\\r\\nsingle camera at 640x480 30 Hz [11]. With this in mind, we                                             eras can be used to extract pose information from fiducial\\r\\ncan ease the task of understanding the environment by using                                            markers, it is necessary that they be calibrated so that their\\r\\nubiquitous fiducial markers - future systems will be able to                                           intrinsic parameters are known. We initially calibrated several\\r\\nuse greater processing power. The ArUco library [12] has a far                                         cameras using the ArUco calibration target and software\\r\\nlower computational load than visual SLAM, and this allows                                             tools. The process involves taking multiple images of the\\r\\nus to process all five camera streams and extract marker poses                                         calibration target then using the software tool to find intrinsics\\r\\n\\x0c\\n\\n                                                                                                                                           5\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            Sensor\\r\\n                                                            Comms fabric\\r\\n                                                            ROS node\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nRPI Zero    RPI Zero    RPI Zero     RPI Zero\\r\\nMJPEG       MJPEG       MJPEG        MJPEG           IMU         Odometry\\r\\n\\r\\n\\r\\n  USB        USB          USB            USB          I2C\\r\\n\\r\\nDecomp      Decomp      Decomp        Decomp\\r\\n\\r\\n Aruco       Aruco       Aruco           Aruco\\r\\n detect      detect      detect          detect\\r\\n\\r\\n              Extended Kalman Filter (robot_localization)                   Fig. 6: Ground truth X and Y positions (left axis), and visual\\r\\n                                                                            tracking position error (right axis), of robot while moving\\r\\n                                                                            along a square path.\\r\\n                                  Pose\\r\\n\\r\\n                                                                            IMU and wheel odometry information. The output of the EKF\\r\\nFig. 5: Visual tracking system. Each camera video stream is                 is a stream of poses in the map frame.\\r\\nsend as compressed MJPEG to the single board computer, then                    The robot was commanded to move twice around a square\\r\\ndecompressed and processed for fiducial marker poses. The                   of sides 1.6m, with velocities of 0.3ms\\xe2\\x88\\x921 and 0.5ms\\xe2\\x88\\x921 using\\r\\nposes, together with IMU and wheel odometry information                     ground truth as the position feedback. Ground truth from the\\r\\nare fed to an EKF to output robot pose.                                     motion capture system and estimated position were recorded.\\r\\n                                                                            Figure 6 shows the actual positions and the absolute error in\\r\\n                                                                            x and y axes. Maximum positional error in either axis never\\r\\nthat minimise the reprojection error. This showed that each                 exceeds 62mm, with \\xcf\\x83(x) = 22.3mm and \\xcf\\x83(y) = 16.1mm.\\r\\ncamera had significant differences in their intrinsic parameters,              5) Sensors: In addition to the vision system, the are a wide\\r\\nperhaps not surprising given their very low cost, but meaning               variety of sensors to apprehend the state of the environment\\r\\nwe had to calibrate each camera individually, rather than using             and the robot itself.\\r\\na standard calibration. We needed an automated approach - 20                     a) Proximity sensors: Surrounding the perimeter of the\\r\\nrobots, each with four cameras to calibrate would require many              robot are 16 equally-spaced ST Microelectronics VL53L1CX\\r\\nhundreds of images captured from different angles.                          infra-red laser time-of-flight distance sensors (IRToF). These\\r\\n   By attaching a calibration target to the arena wall at a fixed           are capable of measuring distance to several metres with\\r\\nand known location, we could execute a calibration process                  a precision of around 10 mm at an update rate of 50 Hz,\\r\\nautomatically taking multiple pictures per camera in about 5                giving a pointcloud of the environment around the robot. Each\\r\\nminutes per robot. Because the only angle that can be varied                detector has a field of view of approximately 22\\xc2\\xb0, which\\r\\nbetween the camera and the target is the rotation about robot Z,            can be partitioned into smaller regions, allowing for a higher\\r\\nthe calibration is less complete, but sufficient given the robot            resolution pointcloud at the cost of lower update rate.\\r\\nwill always be horizontal on the arena floor.                                  As well as returning distance measurements, the\\r\\n      c) Vision processing for localisation: To demonstrate the             VL53L1CX devices are capable of being operated in a\\r\\nability to usefully process vision from multiple cameras we                 mode that returns raw photon counts per temporal bin.\\r\\nbuilt a simple localisation system, shown in Figure 5, and                  This opens up intriguing possibilities - it is possible to\\r\\nbased on ArUco markermaps. This is a feature of the ArUco                   classify materials by their temporal response to incident\\r\\nlibrary that allows the specification of a map of the locations             illumination, and this is demonstrated in [16] with a custom\\r\\nof an arbitrary number of arbitrarily placed fiducial markers,              built time-of-flight camera. [17] show that this also possible\\r\\nwith library functions to return the 3D pose of a calibrated                with the VL53L1CX device, despite the much lower cost\\r\\ncamera which can see at least one of the markers in the map.                and limitations on output power and temporal resolution.\\r\\nWe fixed twelve fiducial markers around the walls of the arena,             They demonstrate successful identification of five different\\r\\nand encoded the locations in a markermap.                                   materials. There is no reason in principle that this robot could\\r\\n   Each camera was calibrated automatically, as described                   not function as a mobile material identification platform, for\\r\\nabove. The video stream from each of the four perimeter                     example in inspection applications.\\r\\ncameras is analysed and if there are any visible fiducials in                  There is little published data on the performance of the\\r\\nthe correct ID range, the pose of the robot in the global map               VL53L1CX sensors, outside the manufacturers specifications,\\r\\nframe is generated. This stream of poses is fed to an Extended              so we wanted to characterise this. The sensors were set\\r\\nKalman Filter filter (EKF, robot localization [15]), along with             up according to the manufacturers recommendations using\\r\\n\\x0c\\n\\n                                                                                                                                                                       6\\r\\n\\r\\n                                                                      Time-of-flight measurement accuracy\\r\\n                                          3.5                                                                                            350\\r\\n                                                      IRToF measurement (left scale)\\r\\n                                                      Ideal sensor (slope 1)\\r\\n                                          3.0         Multipath (slope 2)                                                                300\\r\\n                                                      Multipath (slope 3)\\r\\n\\r\\n                                          2.5                                                                                            250\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                              Measurement error (mm)\\r\\n                  Measured distance (m)\\r\\n\\r\\n                                          2.0                                                                                            200\\r\\n\\r\\n                                          1.5                                                                                            150\\r\\n\\r\\n                                          1.0                                                                                            100\\r\\n\\r\\n                                          0.5                                                                                            50\\r\\n\\r\\n                                          0.0                                                                                            0\\r\\n                                                                                                               Error: x, (right scale)\\r\\n                                          0.5                                                                                                50\\r\\n                                                0.0           0.5         1.0         1.5           2.0       2.5          3.0\\r\\n                                                                                  Actual distance (m)\\r\\nFig. 7: Plot of 2455 points of IRToF measured distance vs ground truth (left scale) and measurement error in 0.125 m bins\\r\\n(right scale). Bars are \\xc2\\xb1\\xcf\\x83. The inset shows the data in the range 0.05 m to 0.4 m. There are signs of multipath effects at close\\r\\nrange (less than 0.25 m) indicated by the green and red lines of slope 2 and 3.\\r\\n\\r\\n\\r\\nthe VL53L1X Ultra Lite driver1 for \\xe2\\x80\\x98short\\xe2\\x80\\x99 distance mode                                     other device setup possibilities to see if it possible to mitigate\\r\\nand update rate of 50 Hz. This notionally gives a maximum                                    this behaviour.\\r\\ndistance measurement of 1.3 m. We commanded a robot to\\r\\n                                                                                                  b) Additional sensors: In addition to the cameras and\\r\\nmove repeatedly between locations 0 m and 3.5 m from a\\r\\n                                                                                             proximity sensors, the robot is equipped with other sensors to\\r\\nvertical wall made of polypropylene blocks at a speed no\\r\\n                                                                                             understand the robot state or the state of the environment - 9\\r\\ngreater than 0.5 ms\\xe2\\x88\\x921 along a path at right angles to the wall.\\r\\n                                                                                             degree-of-freedom (DoF) Inertial Measurement Unit (IMU),\\r\\nThe robot pose was such that one sensor was directly pointing\\r\\n                                                                                             temperature, pressure, humidity, two MEMS microphones,\\r\\nat the wall. The ground truth of the robot position and the\\r\\n                                                                                             robot health, and auxiliary ADCs for future use.\\r\\ndistance readings from the IRToF sensor orthogonal to the wall\\r\\nwere captured. A total of 2455 measurement samples were                                        The IMU consists of a 6 DoF LSM6DSM accelerometer\\r\\ncollected. Figure 7 shows the results. Each measured distance                                and gyroscope and a 3 DoF LIS2MDL magnetometer. The\\r\\nis plotted against the ground truth distance of the sensor from                              IMU, together with information about wheel position, is the\\r\\nthe wall. We divided the measurements into bins 0.125 m wide                                 main way the robot performs odometry to estimate its local\\r\\nand calculated the mean and standard deviation of the error in                               movement. We currently acquire samples at 100Hz and filter\\r\\neach bin.                                                                                    with an EKF to match the motion control update rate.\\r\\n   The performance of the sensor is remarkably good over                                       The temperature, pressure and humidity sensors allow us\\r\\nmost of its range. Once above 0.25 m, up to 2.7 m, the mean                                  to monitor the state of the robots environment - useful in\\r\\nmeasurement error does not exceed 12 mm and is mostly less                                   a warehouse scenario, for example, noting locations with\\r\\nthan 5 mm. Even out to 3.2 m, way beyond the notional range                                  excessive humidity or temperatures outside desired limits.\\r\\nfor its mode of operation, the mean error never exceeds 30 mm.\\r\\n                                                                                                The two IM69D130 MEMS microphones are processed by\\r\\n   Of some interest is the behaviour of the sensor at close\\r\\n                                                                                             the on-board FPGA to a I2 S stream which is sent to the\\r\\ndistances, less than 0.25 m. Here, the mean error is much\\r\\n                                                                                             SBC. The audio is available using the standard Linux audio\\r\\nhigher, up to 75 mm, and examining the measured points in\\r\\n                                                                                             framework. We intend to use these to experiment with audio-\\r\\nmore detail (see inset in Figure 7) we can see that many of\\r\\n                                                                                             based localisation - sending ultrasonic signals at known times\\r\\nthe points do not fall on a line of slope 1, which would be\\r\\n                                                                                             and measuring time-of-arrival to estimate distances.\\r\\nexpected from a perfect sensor, but on lines of slope 2, and\\r\\npossibly slope 3. This would be characteristic of multipath                                    Robot health sensors monitor the state of various subsys-\\r\\nreflections e.g. reflecting off the wall, then the robot body, then                          tems on board. We measure the voltages and currents of each\\r\\nthe wall again before being sensed. We intend to investigate                                 power supply, the state of each cell of the battery pack, and the\\r\\n                                                                                             temperatures of the battery charging system, power supplies,\\r\\n  1 https://www.st.com/en/embedded-software/stsw-img009.html                                 motor drives, and single board computer. All this information\\r\\n\\x0c\\n\\n                                                                                                                                                                7\\r\\n\\r\\n\\r\\n\\r\\nis available through the Linux hwmon sensor framework2 .                 TABLE III: Motor drive cost comparison. Maxxon parts\\r\\n   6) Communication: Local communication is important for                chosen to give broadly similar performance\\r\\nswarm robotics, and an area where there is much potential                 Part             Maxxon                   Cost    Commodity                    Cost\\r\\nfor novel approaches. To facilitate experimentation, we in-               Motor            EC45 flat 120W            129    Sunnysky X2814                22\\r\\n                                                                                           P/N 608148                       900Kv\\r\\nclude multiple different radios. As well as the WiFi built in             Encoder          MILE 2048 P/N             97     Sensor PCB and                 10\\r\\nto the single board computer, each robot is equipped with                                  673027                           magnet\\r\\ntwo nRF52840-based BLE5 (Bluetooth Low Energy) USB-                       Controller       ESCON P/N                127     Custom drive                   25\\r\\n                                                                                           438725                           electronics\\r\\nconnected radio modules, and a DWM1001 UWB (Ultra wide                                                              \\xc2\\xa3353                                  \\xc2\\xa357\\r\\nband) module. A private 5G modem can be added. All of\\r\\nthese modules can be reprogrammed with custom firmware\\r\\nunder the control of the single board computer, allowing for                                    Encoder PCB\\r\\n                                                                                              support, 3D printed\\r\\non-the-fly installation of novel communication protocols.\\r\\n   We currently have firmware for the BLE radios that contin-                                                                                       Encoder PCB\\r\\nually advertises the robots unique name and a small amount of\\r\\ndata, and scans for other BLE radios. Each scan result contains\\r\\nthe name of any nearby robot sensed, along with its received\\r\\nsignal strength (RSSI), which can be used as a proxy measure\\r\\nfor distance.                                                                 12T drive pully                                                Diametrically\\r\\n   The DWM1001 radio is designed to perform two-way                                                                                        magnetised magnet\\r\\n\\r\\nranging between devices, measuring the distance between a\\r\\npair of radios to a claimed accuracy of around 0.1m. It is                 GT2 toothed belt                                              SunnySky X2814\\r\\n                                                                                                                                        900Kv drone motor\\r\\ninterfaced to the SBC using an SPI bus.\\r\\n   7) Mobility: For the collective transport of large loads, it\\r\\nis necessary for multiple robots to be able to move together\\r\\nin a fixed geometric relationship with each other. The only\\r\\nway to achieve this with arbitrary trajectories is for the robots                                                           Aluminium motor support\\r\\n\\r\\nto have holonomic motion. We use a three omniwheel system\\r\\nwith the wheels equally spaced 120\\xc2\\xb0 apart. The kinematics for\\r\\nthis type of drive are well known [18] and are shown below:                                                                60T pulley, 3D printed\\r\\n            \\xef\\xa3\\xab \\xef\\xa3\\xb6 \\xef\\xa3\\xab \\xe2\\x88\\x9a3                     \\xef\\xa3\\xb6\\xef\\xa3\\xab \\xef\\xa3\\xb6\\r\\n               v1        \\xe2\\x88\\x92 2      0.5 1        vx\\r\\n                                                                                                                      Omniwheel\\r\\n            \\xef\\xa3\\xad v2 \\xef\\xa3\\xb8 = \\xef\\xa3\\xad 0\\r\\n                           \\xe2\\x88\\x9a\\r\\n                                  \\xe2\\x88\\x921    1\\xef\\xa3\\xb8 \\xef\\xa3\\xad vy \\xef\\xa3\\xb8\\r\\n               v3            3                R\\xcf\\x89\\r\\n                              2  \\xe2\\x88\\x920.5 1\\r\\nwhere v1 , v2 , v3 are the tangential wheel velocities, vx , vy , \\xcf\\x89\\r\\nthe robot body linear and angular velocities, and R the radius           Fig. 8: Single drive assembly, showing commodity drone\\r\\nfrom centre of robot to wheels.                                          motor with magnetic angle encoder PCB driving omniwheel\\r\\n   Another important requirement for the robots is that they             via 5:1 toothed belt reduction.\\r\\ncan move fast and accurately. A limitation of other lab-\\r\\nbased swarm systems is that the locomotion is often based\\r\\non stepper motors or small geared DC motors. These are                   and the motors often use lower voltages and higher currents\\r\\nrelatively cheap and accurate but are slow and heavy. Much               than typical servos, but these deficiencies can be compensated\\r\\nhigher performance is possible with Brushless DC (BLDC)                  for with clever software, e.g. [23]. We designed drive circuitry\\r\\nservo motors. These servo motors are paired with a position              suitable for running the Odrive Robotics3 open source BLDC\\r\\nencoder and drive electronics that modulates the coil current            servo controller and tested various cheap drone motors, select-\\r\\nto achieve accurate torque, velocity and position control with           ing for the motor with the least cogging that met the required\\r\\nmuch higher performance to weight and size ratios than is                form factor and price. We replaced the costly traditional optical\\r\\ntypically possible with stepper motors. This comes at a cost,            position encoder with a high resolution magnetic field angle\\r\\na typical motor, encoder, and driver with the performance we             encoder IC. By glueing a small diametrically magnetised disc\\r\\nrequire costs around \\xc2\\xa3350.                                               magnet to the end of a motor shaft and positioning the encoder\\r\\n   There has been recent interest in using commodity drone               IC correctly in line with the motor axis, we can sense absolute\\r\\nmotors in place of dedicated servo motors [19]\\xe2\\x80\\x93[22]. The                 motor shaft angle with high precision and low cost.\\r\\nhigh power-to-weight ratios and low costs due to the large                  These innovations reduced the cost per robot of the motor\\r\\nmarket size make this an interesting alternative. There are              drives from over \\xc2\\xa31000 to less than \\xc2\\xa3200 with comparable\\r\\ndisadvantages - the motors are not designed for use as servos            performance. Table III illustrates this, and Figure 8 shows a\\r\\nand certain parameters which are more important in servo                 single complete wheel assembly.\\r\\nmotors are not controlled, e.g. the amount of torque cogging,               Many systems aimed at swarm robotics research e.g. epucks,\\r\\n  2 https://www.kernel.org/doc/html/latest/hwmon/hwmon-kernel-api.html     3 https://odriverobotics.com/\\r\\n\\x0c\\n\\n                                                                                                                                                              8\\r\\n\\r\\n\\r\\n\\r\\n                                                      \\xe1\\xba\\x8dimu\\r\\n                                                                                         TABLE IV: Subsystem power consumption. The table indi-\\r\\n                                                      \\xc3\\xbfimu\\r\\n                                                                IMU                      cates the power consumption of some subsystems and battery\\r\\n                                                      \\xcf\\x89imu\\r\\n                                                                              Motor      life under various conditions.\\r\\n                                              EKF                           encoder\\r\\n                                                      \\xe1\\xba\\x8benc                  velocities    Subsystem       Power (W)     Life1 (h)    Notes\\r\\n                                                      \\xe1\\xba\\x8fenc\\r\\n                                                      \\xcf\\x89enc     motorT\\r\\n                                                                     base                 Mainboard          2.5                     Includes all sensors except\\r\\n                                                                                                                                     vision\\r\\n                                                                                          RockPi4 SBC          2.5                   Basic telemetry and WiFi\\r\\n                                                                                                                                     networking\\r\\n                                       pos                                      M0                             9.5                   All 6 CPUs and GPU busy\\r\\n         req                                        \\xe1\\xba\\x8bbase                                 Vision system        7.0                   5x cameras streaming MJPEG\\r\\n                Path                                \\xe1\\xba\\x8f\\r\\n         fb    action      Traj gen goal     PID x3 \\xcf\\x89base  baseT\\r\\n                                                                motor           M1                                                   640x480 @30fps\\r\\n         res                                          base\\r\\n               server                                                                     Motor system          0                    Off\\r\\n                                     Position                                                                   2                    Random walk 0.5 ms\\xe2\\x88\\x921 (avg)\\r\\n        Path       Waypoint            goal         Velocity         Motor M2                                  15                    Full torque on all wheels\\r\\n p={wp1,..,wpn}   wpi=(x,y,\\xce\\xb8,\\xe1\\xba\\x8b,\\xe1\\xba\\x8f,\\xcf\\x89)T g=(x,y,\\xce\\xb8)                      velocity Motor\\r\\n                                                                                          Total                 6           14       Basic sensing and comms,\\r\\n                                                                             drives\\r\\n                                                                                                                                     occasional movement\\r\\n                                                 100Hz                                                         14            6       Sensing, comm, and cameras,\\r\\n                                                                                                                                     moderate vision processing\\r\\nFig. 9: Motion control system. A ROS action server accepts                                                                           and movement\\r\\nmulti-waypoint paths which get played out by the constrained                                                   34           2.5      All systems at maximum\\r\\ntrajectory generator at 100 Hz. The PID control loop operates                                                                        power consumption\\r\\n                                                                                                   1 Assuming 100 Wh battery capacity @85% efficiency\\r\\nin the robot local frame, with robot velocities transformed into\\r\\nthe appropriate motor velocities.\\r\\n                                                                                         monitored with INA219 power sensors.\\r\\npheno, kilobots etc (cite) are physically quite small and move                              A battery charger circuit appropriate to the chemistry of\\r\\nat low speeds. Collisions are not of high enough energy to                               the SCiB cells is integrated into the PSU module, enabling\\r\\ncause damage and the mass of the robots is low enough that                               the robot battery to be recharged with a commodity AC-DC\\r\\nlittle attention has to be paid to concerns that are important                           adaptor capable of providing 12 V@5 A.\\r\\nin larger robots such as trajectory generation respecting ac-                               Table IV shows the power consumption of various sub-\\r\\nceleration limits. The DOTS robots, however, have a mass of                              systems. The baseline idle consumption gives access to all\\r\\n3 kg, with a potential payload of 2 kg, so a proper trajectory                           sensors except for the vision system, and includes WiFi and\\r\\ngeneration and motion control loop is important. We want                                 Bluetooth communication. At this level, the endurance of a\\r\\nto be able to generate new trajectory points in real time                                robot with a full charge is around 16 hours. We can see that the\\r\\nwith low computational cost, and also to be able to revise                               consumption increases considerably when using the camera\\r\\nthe goals on-the-fly. This capability is important for agile                             vision system. The motor system figure is for all wheels\\r\\nquadrotor drones, so there has been recent work in this area.                            delivering maximum torque so would not be a continuous\\r\\nWe use the Ruckig trajectory generator described in [24],                                state. Average motor power when performing a random walk\\r\\nwhich is available as C++ library4 . The on-board motion                                 with a maximum speed of 0.5 ms\\xe2\\x88\\x921 was around 2 W. With full\\r\\ncontrol system is shown in Figure 9. The action server can                               camera usage and moderate movement and processing we get\\r\\naccept a path consisting of multiple waypoints with required                             an endurance of around 6 hours.\\r\\npositions and velocities, this is played out by the trajectory                              Battery life is limited to the point the output voltage falls\\r\\ngenerator producing intermediate goal positions at a rate of                             below the undervoltage protection limit. The largest dynamic\\r\\n100 Hz. Three PID controllers generate velocity commands                                 load component is the motor drives, so we compromise on the\\r\\nin the robot frame to satisfy the current goal on the path.                              maximum allowed acceleration in order to limit voltage drop\\r\\nPosition feedback uses the Inertial Measurement Unit (IMU)                               and extend effective endurance. The available torque implies\\r\\nand wheel odometry fed through an EKF filter. At any point,                              a maximum loaded acceleration of around 4 ms\\xe2\\x88\\x922 but we limit\\r\\nthe action server can accept a new trajectory or the cancellation                        this to 2 ms\\xe2\\x88\\x922 .\\r\\nof the existing trajectory and goals are updated on-the-fly                                 9) Manipulation: In order to demonstrate novel algorithms\\r\\nwhile respecting acceleration and jerk limits. Cancellation of                           for logistics and collective object transport, the robots need a\\r\\na trajectory results in maximum acceleration braking to zero                             means of carrying payloads. For this purpose, each robot is\\r\\nvelocity. Supplementary material ?? shows various examples                               equipped with a lifter - a platform on top of the robot that can\\r\\nof agile trajectory following.                                                           be raised and lowered by the use of a miniature scissor lift\\r\\n    8) Power: The power system consists of two Toshiba SCiB                              mechanism actuated by a high-powered model servo. In the\\r\\n23Ah Lithium Titanate cells, with a total energy capacity of                             centre of the lifter platform there is an upwards-facing camera\\r\\n100 Wh. These are combined into a battery pack, with a                                   that can be used to visually navigate.\\r\\nbattery management system to provide under- and overvoltage                                 To hold payloads, there are carriers, square trays with four\\r\\nprotection and monitoring. This is converted by the PSU                                  legs that the robot can manoeuvre underneath before raising\\r\\nmodule to the required voltages 3.3 V@2 A, 5 V@4A, and                                   the lifter. In order that a robot can position itself correctly to\\r\\n12 V@4 A. All voltages, currents, and the battery state are                              lift the carrier, we place a visual target on the underside.\\r\\n                                                                                            Each carrier weighs about 200 g, so with the lifting capacity\\r\\n  4 https://github.com/pantor/ruckig                                                     of 2 kg we can take a payload of around 1 8kg. In order to\\r\\n\\x0c\\n\\n                                                                                                                                    9\\r\\n\\r\\n\\r\\n\\r\\ndemonstrate collective transport, the carriers can be joined         more suitable protocol, this is the approach we used; Zenoh6\\r\\ntogether along their edges to form arbitrary shapes, to be           is an open-source project supported Eclipse Foundation7 . The\\r\\nmoved with an equal number of robots, one under each carrier.        Zenoh approach is agnostic of the particular DDS middleware\\r\\n                                                                     used, and is intended to be part of the next release of ROS2.\\r\\nB. Industrial Swarm Testbed                                          The Zenoh protocol is designed to overcome the deficiencies\\r\\n   The industrial swarm testbed is the collection of DOTS            of DDS regarding discovery scalability [32]. By using a\\r\\nrobots, the physical arena - a 5 m x 5 m main room overlooked        Zenoh-DDS bridge on each agent (robots and other partici-\\r\\nby an adjacent control room, the communications infrastruc-          pants) and disallowing DDS traffic off robot, we can achieve\\r\\nture comprising WiFi and private 5G network, motion tracking         transparent ROS2 connectivity with far lower discovery traffic\\r\\nsystem, video cameras, and the system software architecture.         levels, granular control over the topics that can seen outside\\r\\n   It also has an associated high fidelity simulation system,        the robot, true decentralisation, and no use of problematic\\r\\nbased on the widely used Gazebo simulator. Controllers can           multicast.\\r\\nbe safely developed in simulation and transferred without               A number of ROS nodes are always running on each robot,\\r\\nmodification to run on the real robots.                              interfacing to the hardware and providing a set of ROS topics\\r\\n   This section details the individual components making up          that constitute the robot API. This API is available on both\\r\\nthe testbed, before tying them together with a description of        the simulated and real robot, and is intended to be the only\\r\\nthe complete system architecture.                                    way a controller has access to the hardware.\\r\\n   1) Robot Operating System (ROS): We use ROS2 Galactic                2) Coordinate frames: We adhere to the ROS standard for\\r\\n[25] for the testbed. ROS2 is the next generation of ROS             coordinate frames REP-105 [33], see the arena diagram Figure\\r\\n[26], and, at first glance, would appear to have several             13, with the extension that each frame name that is related to\\r\\nadvantages over ROS1 for the type of decentralised swarm             an individual robot is prefixed with the unique robot name. The\\r\\ntestbed we describe here. Firstly, there is no ROS master,           global frame map is fixed to the centre of the arena, odom\\r\\nROS2 is completely decentralised. With multiple, potentially         is fixed to the starting position of the robot, and base link\\r\\ncommunicating, robots, we don\\xe2\\x80\\x99t need to choose one to be             is attached to the robot body, with +x pointing forward,\\r\\nthe master, nor do we need to use multimaster techniques             between the overlapping cameras. The base link frames are\\r\\n(e.g, [27]) as were necessary with ROS1. This is facilitated         updated relative to the odom frames using each robots inertial\\r\\nby the second major difference - the communications fabric of        measurements and wheel velocities. There is no transform\\r\\nROS2 uses middleware conforming to the DDS standard [28]             between map and any odom unless some form of localisation\\r\\nof which there are several suppliers. DDS supports automatic         is used on a robot, which is not necessary for many swarm\\r\\ndiscovery of the ROS graph and supports a rich set of Quality        applications. The ROS transform library TF [34] is used to\\r\\nof Service (QoS) policies that can be applied to different ROS       manage transforms. Each robot has its own transform tree,\\r\\ntopics, or communication channels. This maps onto a swarm            and a transform relay node is used to broadcast a subset of\\r\\nperspective quite naturally; communications within the ROS           the local transforms to global topics for use in visualisation\\r\\ngraph of a single robot may be marked as reliable meaning            and data gathering.\\r\\ndata will always be delivered, even if retries are necessary,           3) Containerisation: Controllers for the robots are de-\\r\\nwhereas inter-robot communications could be marked as best           ployed as Docker [35] containers. Docker provides a\\r\\neffort meaning data may be lost and dropped with no errors           lightweight virtualised environment making it easy to package\\r\\nor retries if the physical channel is unreliable.                    an application and associated dependencies into a container,\\r\\n   However, this enticing vision has yet to be borne out in          which can then be deployed to computation resources using\\r\\nreality. The DDS discovery process generates network traffic         an orchestration framework such as Kubernetes, we use the\\r\\nthat increases approximately O(n) where n is the number of           lightweight implementation K3S8 .\\r\\nparticipants when using UDP multicast, or O(n2 ) when using             Containers only have controlled access to the resources of\\r\\nunicast [29] [30]. A participant is typically a ROS2 process of      the compute node, facilitating security and abstraction - as far\\r\\nwhich there might be many per robot. Complete information            as the container is concerned, it has no knowledge of whether\\r\\nis maintained for all participants at every location, even if they   it has been deployed onto a real robot, or a compute node\\r\\nwill never communicate. Additionally, although the standard          communicating with a simulated environment.\\r\\ndiscovery protocol uses UDP multicast, this is very unreliable          4) Real world: The arena consists of a 5 m x 5 m main\\r\\non WiFi networks [31] forcing the use of unicast discovery.          room, overlooked by an adjacent control room. The main room\\r\\nBuilding robot systems with many mobile agents linked using          is equipped with an OptiTrack [36] motion capture system with\\r\\nwireless communication is becoming more common, and the              8x Flex 13 120Hz cameras for analysis of experiments. There\\r\\nlimitations of ROS2 in this regard have resulted in several          are separate high resolution video cameras. Communications\\r\\npossible solutions. One approach, the discovery server, uses         with the robots are provided with a dedicated 5 GHz WiFi\\r\\na central server to hold information about all participants,         access point and a private 5G base station. A Linux server\\r\\ngreatly reducing traffic. This so far is limited to a single DDS     is used for data collection, Kubernetes orchestration, video\\r\\nvendor, eProsima5 , and is the opposite of the decentralised\\r\\n                                                                       6 https://zenoh.io/\\r\\nvision. Another approach uses bridges between DDS and a\\r\\n                                                                       7 https://www.eclipse.org/\\r\\n  5 https://www.eprosima.com/                                          8 https://k3s.io/\\r\\n\\x0c\\n\\n                                                                                                                                                                  10\\r\\n\\r\\n\\r\\n\\r\\nstreaming, and general experiment management. The arena                                            Server\\r\\nserver and 5G base station are connected to the UMBRELLA                                                              Umbrella\\r\\n\\r\\nnetwork [37] with a fibre link.                                                                                      Arena ROS management\\r\\n   5) Simulation: The standard robot simulator Gazebo is                                              Container\\r\\n                                                                                                                        DDS middleware\\r\\nused. The simulation environment consists of a configurable                                          orchestration\\r\\n\\r\\nmodel of the real arena with various props such as carriers the                                                      Zenoh-DDS Optitrack-\\r\\n                                                                                                                       bridge  ROS bridge\\r\\nrobots can pick up, and obstacles, a Unified Robot Description\\r\\nFormat (URDF) model of the robot and some of its senses, and\\r\\n                                                                     Comms fabric\\r\\na ROS node that provides the same topic API that is present          (WiFi, private 5G, ethernet)\\r\\nin the real robots.\\r\\n   The robot is modelled taking into account the trade-off be-\\r\\ntween speed and fidelity. Rather than modelling the physically\\r\\ncomplex holonomic drive omniwheels we instead modelled               Robot                                           Robot                        Optitrack PC\\r\\n                                                                         Zenoh                                           Zenoh\\r\\nthe motion of the robot as a disc with friction sliding over the         bridge                                          bridge\\r\\n\\r\\nfloor, with a custom plugin using a PID loop to apply force               DDS\\r\\n                                                                       middleware\\r\\n                                                                                                                          DDS\\r\\n                                                                                                                       middleware\\r\\nand torque to the robot body in order meet velocity goals. This            ROS\\r\\n                                                                                     ROS user\\r\\n                                                                                                                          ROS\\r\\n                                                                                                                                    ROS user\\r\\n                                                                                     application                                    application\\r\\nhas the advantage over directly applying velocity of avoiding             nodes\\r\\n                                                                                      container\\r\\n                                                                                                                         nodes\\r\\n                                                                                                                                     container\\r\\n                                                                        Hardware                                        Hardware\\r\\nunphysical effects such as infinite acceleration, and results in        interface                                       interface\\r\\nrealistic behaviour at low computational cost.                           Sensors                                        Sensors\\r\\n                                                                         actuators                                      actuators                     Optitrack\\r\\n   The cameras, time-of-flight proximity sensors, and IMU (In-                                                                                        cameras\\r\\nertial Measurement Unit) are modelled using standard Gazebo\\r\\nROS plugins and the ROS node emulates hardware sensors                                                                Reality\\r\\nsuch as the battery state and controls actuation of the simulated\\r\\nlifting platform, presenting the same interface as the real          Fig. 10: System architecture. Each robot has a stack of low-\\r\\nhardware.                                                            level software interfacing to the sensors and actuators, with\\r\\n   Simulation is not reality - there are always differences. The     some ROS nodes running natively and a Zenoh-DDS bridge\\r\\nreality-gap [38] means that keeping in mind the limitations of       to handle DDS traffic over the arena communications fabric.\\r\\nsimulation when designing robot behaviours that will transfer        Docker containers with ROS user applications are deployed\\r\\nwell to real robots is important. For example, collisions are        by container orchestration from the server, which also handles\\r\\nhard to simulate, and bad for robots, so behaviours relying on       Optitrack, cameras, and Umbrella integration.\\r\\nthem in simulation should be avoided.\\r\\n   6) Online portal for remote experimentation: The physical\\r\\narena and the simulation environment are designed to be              capture system and video from the arena cameras to be\\r\\nuseable remotely. The UMBRELLA Platform, detailed in [37]            streamed to the UMBRELLA cloud system and made available\\r\\nprovides cloud infrastructure to facilitate this. An online portal   to through the online portal.\\r\\nis used for managing experiments through which users may                7) System Architecture: Figure 10 shows the testbed system\\r\\nupload controller containers and simulator and experiment            architecture. Robots in the arena all run a stack of hardware\\r\\nconfiguration files - controlling for example the number and         interface code, housekeeping and low-level ROS nodes that\\r\\nstarting positions or the robots, and download data generated        provide the robot API, and communication bridges. User\\r\\nduring an experimental run.                                          applications or controllers are deployed to the robots as Docker\\r\\n   The user can schedule or queue experiments to be run in           containers using Kubernetes orchestration. A dedicated PC\\r\\nsimulation or on the real robots. Because the real robots could      runs an Optitrack motion capture system. A Linux server\\r\\nbe damaged by collisions, the controller containers used for         PC is responsible for running container orchestration, arena\\r\\nexperiments on them need to be verified in a simulation run          system control, camera, ROS, and Optitrack data capture, and\\r\\nthat checks robot trajectories for dangerous approach velocities     interface to the UMBRELLA Platform.\\r\\nand other potential indicators of hazard. This is an open area\\r\\nof research [39]\\xe2\\x80\\x93[41].                                               C. DOTS integrated development environment\\r\\n   Controller containers for simulation are run on cloud ma-            In order to facilitate wider use of the DOTS system, we cre-\\r\\nchines of the same processor architecture as the robots (ARM         ated a Docker-based development and simulation environment,\\r\\n64 bit), communicating over ROS topics on a virtual network          capable of running on Windows, Linux, and OSX operating\\r\\nwith a server running a Gazebo simulator instance. When the          systems. There is a large barrier to entry in learning to\\r\\ncontroller container has been verified and is to be run on           program ROS-based robots - versions of ROS are closely tied\\r\\nthe real robots, it is passed to the testbed Linux server and        to particular releases of Ubuntu, dependencies are not always\\r\\nqueued to be run by the testbed technician, who ensures the          straightforward to install, custom plugins for the simulator\\r\\nphysical arena is correctly set up and the right number of           need to be compiled, and there are few public examples of\\r\\nrobots positioned.                                                   complete systems.\\r\\n   As well as data collected by the experiment containers,              Because Docker containers can run on multiple operating\\r\\nthe testbed server collects ground truth data from the motion        systems, we can package up the particular version of Ubuntu,\\r\\n\\x0c\\n\\n                                                                                                                                       11\\r\\n\\r\\n\\r\\n\\r\\nROS, and all the difficult to install dependencies and provide                                 Arena width 3.7m\\r\\neasy access to a complete ROS2 DOTS code development\\r\\n                                                                                Search zone 1.85m                     Drop zone 0.6m\\r\\nand simulation environment. All access is provided via web\\r\\ninterfaces using open-source technologies developed for cloud\\r\\napplications. The local file system is mounted within the\\r\\nDocker container.\\r\\n   1) Code-server: Code-server9 takes the popular open-\\r\\nsource Microsoft editor VSCode10 and makes it available via a\\r\\n                                                                                                        y\\r\\nbrowser window. We install this within the Docker container.\\r\\nBy connecting to a particular port on the localhost, a standard\\r\\nVSCode interface is available in the browser. This not only                                                       x\\r\\n\\r\\nallows browsing and editing of files, but also provides a\\r\\nterminal for command line access to the environment.\\r\\n   2) Gazebo: The Gazebo simulator is architecturally split\\r\\ninto gzserver which runs the simulation, and gzclient which\\r\\npresents the standard user interface on a Linux desktop. We\\r\\nmake available GZWeb, which is another client for gzserver\\r\\nthat presents a browser-based interface using WebGL [42].\\r\\nThis allows browser access and is more performant than using                        carriers                            robots\\r\\nthe standard interface via VNC.\\r\\n   3) VNC: VNC is a technology for allowing remote desk-             Fig. 11: Intralogistics task, robots starting on the right must\\r\\ntop access. We run a virtual framebuffer within the Docker           search for and pick up carriers in the search zone on the left.\\r\\ncontainer together with a simple window manager and a                Once picked up, the carriers must be moved to the drop zone\\r\\nstandard VNC server, and use the noVNC11 client to make              and deposited.\\r\\nthis available through a browser window. This gives access to\\r\\na standard Linux desktop and allows the use of other graphical\\r\\napplications such as the ROS data visualiser rviz.\\r\\n   These three applications packaged in a Docker container\\r\\n                                                                     described in Section III-A4c but could use, for example, the\\r\\nenable rapid access to a complete DOTS development and\\r\\n                                                                     magnetometer and Bluetooth beacons.\\r\\nsimulation environment in a platform agnostic way. Once a\\r\\ncontroller application has been prototyped, it can be converted         Although conceptually simple, for a robot to detect, ma-\\r\\ninto a Docker container for upload to the UMBRELLA online            noeuvre underneath a carrier and position itself correctly in the\\r\\nportal to be verified and for deployment to the real robots.         centre, then lift it, is non-trivial. Many swarm foraging tasks\\r\\n                                                                     demonstrated with real robots have used virtual objects [44],\\r\\n                              IV. R ESULTS                           objects that can be moved just by collision and pushing [45],\\r\\nA. Intralogistics use case                                           or have an actuator that mates with a particular structure [46].\\r\\n   In order to demonstrate the whole system, we implemented          These approaches abstract the difficulty of an arbitrary object\\r\\na conceptually simple swarm intralogistics task. Imagine a           collection task to various extents. We argue the system we\\r\\ncloakroom, similar to the scenario described in [43], where          demonstrate here is closer to real-world practical use because\\r\\nusers can deposit and collect bags and jackets, and a swarm          the location and lifting of the carriers require the sorts of\\r\\nof robots will move these to and from a storage area. It is          sensory processing and navigation typical of many non-trivial\\r\\npossible to perform this search and retrieval task in an entirely    robotics tasks.\\r\\ndistributed, decentralised manner. Here we implement one                We develop using the pipeline shown in Figure 2. The\\r\\naspect of the task, item retrieval, in this decentralised fashion.   DOTS integrated development environment is used to write\\r\\n   The arena, shown in Figure 17, has two regions, the search        controller code, in a mixture of Python and C++, which can be\\r\\nzone on the left-hand side, everywhere with x < 0, and the           quickly iterated to run on the local Gazebo simulator. Simple\\r\\ndrop zone on the rightmost 0.6m, everywhere with x > 1.25.           sub-behaviours are built up into the complete controller until\\r\\nRobots start randomly placed in the drop zone, and carriers          multiple robots are running successfully in simulation. Once\\r\\nin the search zone. The task is for the swarm to find and            satisfied with simulation the controllers are packaged into\\r\\nmove the carriers into the drop zone. Apart from the earlier         Docker containers for validation on the remote UMBRELLA\\r\\ndescribed sensors, we make available two synthetic senses: a         cloud service portal. This allows both larger simulations, with\\r\\ncompass that gives the absolute orientation of a robot, and          resources limited only by cloud availability, and state-of-the-\\r\\na zone sense, indicating if the robot is in either of the two        art validation of the simulated controller for safety. Once\\r\\nzones. Currently these are derived from the visual localisation      validated, the controller may be deployed onto the real robots\\r\\n  9 https://github.com/cdr/code-server                               in the testbed for an experiment run. During the run, multiple\\r\\n  10 https://code.visualstudio.com/                                  data sources are captured and these are then made available\\r\\n  11 https://novnc.com/info.html                                     for download from the portal.\\r\\n\\x0c\\n\\n                                                                                                                                                12\\r\\n\\r\\n\\r\\n\\r\\n                                                                                collision is detected, a new direction is chosen and the robot\\r\\n                                                                                moves in that direction at 0.5ms\\xe2\\x88\\x921 . The direction chosen is\\r\\n                                                                                randomly picked from a Gaussian distribution:\\r\\n                                                                                                          (\\r\\n                                                                                                            \\xcf\\x83 = 3.0 if x \\xe2\\x88\\x88 searchzone\\r\\n                                                                                   \\xce\\xb8 \\xe2\\x88\\xbc N (\\xcf\\x80, \\xcf\\x83 2 ) where                                   (1)\\r\\n                                                                                                            \\xcf\\x83 = 1.0 otherwise\\r\\n                                                        Take to                 This has probability p \\xe2\\x89\\x88 0.9 of moving in \\xe2\\x88\\x92x direction when\\r\\n              Explore       Pick up\\r\\n                                                       drop zone                outside the search zone, and p \\xe2\\x89\\x88 0.6 when in the search zone,\\r\\n                                                                                so robots will tend to move towards the search zone then\\r\\nFig. 12: Top level of Behaviour Tree controller. Robots explore                 randomly within it. The robot continues in the same direction\\r\\nuntil they find a carrier, the pick up the carrier, then take to                until another potential collision is detected. If, at any point, a\\r\\ndrop zone the carrier.                                                          carrier ID fiducial (on the four sides), or a carrier markermap\\r\\n                                                                                (on the underneath) are detected, the robot motion is stopped\\r\\n                                                                                and the explore behaviour returns success, moving to the pick\\r\\n                                    cfiducial\\r\\n                                                        carrier                 up behaviour, otherwise it returns running.\\r\\n                                                                                     b) Pick up: This behaviour is started if the robot has seen\\r\\n                                                                                an ID or markermap. In order of priority, the robot will try to\\r\\n                                                dock\\r\\n                                                                    z           move to the centre position under the carrier if a markermap\\r\\n              base_link\\r\\n                                                                        y\\r\\n                                                                                has been seen, or move underneath the carrier to the dock\\r\\n                          predock                                 map\\r\\n                                                                                (see frame diagram, Figure 13) position if an ID has been\\r\\n                                                                            x\\r\\n       odom                                                                     seen and robot is close to predock position or move to the\\r\\n                                                                                predock position if an ID has been seen. If the centre is reached\\r\\n                                                                                successfully, the lifting platform is raised and the behaviour\\r\\nFig. 13: Frames in use within the testbed. map is fixed to                      returns success and starts the take to drop zone behaviour.\\r\\nthe arena, odom to the robot starting position. base link is                    If the vision system loses track of the fiducial ID or the\\r\\nthe robot frame, with +x being forward. Each carrier has four                   markermap, the behaviour returns failure, otherwise it returns\\r\\nfiducials around the sides, with frame cfiducial, and the carrier               running. The consequence of failure is reversion to the explore\\r\\nas a whole has the frame carrier, centred in the markermap on                   behaviour.\\r\\nits underside. predock and dock are frames relative to cfiducial                     c) Take to drop zone: This behaviour is started if the\\r\\nthat the robot uses when navigating under a carrier.                            pick up behaviour has succeeded. It is similar to the explore\\r\\n                                                                                behaviour but biassed to move towards the drop zone. Move-\\r\\n                                                                                ment is slower at 0.3ms\\xe2\\x88\\x921 and collision detection suited to\\r\\nB. Robot behaviour                                                              the situation where the legs of the carrier can obscure some\\r\\n   We use a Behaviour Tree controller [47]\\xe2\\x80\\x93[51]. The top                        of the IRToF sensors. This behaviour will return running until\\r\\nlevel is shown in Figure 12. There are a sequence of actions                    the robot reaches the drop zone with no collision detection,\\r\\nthat are performed, once each action has been performed                         at which point it will lower the lifting platform and return\\r\\nsuccessfully, the next is started. Firstly, the robot explores the              success and revert to explore behaviour.\\r\\nenvironment, searching for a cargo carrier. If one is found, the                   2) Cargo carrier detection: Carriers have an ArUco fiducial\\r\\npick up behaviour is started. This involves manoeuvring under                   marker on each side, labelled cfiducial in Figure 13. These\\r\\nthe carrier, positioning itself centrally, then raising the lifting             markers has an ID unique to the carrier, and can be detected\\r\\nplatform. Finally, the robot does take to drop zone, moving in                  by the perimeter cameras of the robot from about 1m away. On\\r\\nthe direction of the nest region and once there, lowering the                   the underside of the carrier is a markermap, shown in Figure\\r\\nlifting platform. If there are any failures, or once a robot has                14, so the robot can locate the centre of the carrier using its\\r\\nsucceeded in all tasks, the behaviour reverts to explore.                       upwards-facing camera. The size and density of the fiducials\\r\\n   1) Behaviour trees: Behaviour Trees are hierarchical struc-                  making up the map are chosen such that there are always at\\r\\ntures that combine leaf nodes that interact with the environ-                   least one complete marker visible in the camera field-of-view\\r\\nment into subtrees of progressively more complex behaviours                     for all physically possible positions of the robot under the\\r\\nusing various composition nodes such as sequence and selec-                     carrier.\\r\\ntor, see [52] for more information. The whole tree is ticked at a                  When a carrier fiducial has been seen by any camera, the\\r\\nregular rate, in this case 10Hz, corresponding to the controller                transform from the robot to that fiducial is made available on\\r\\nupdate rate, with nodes returning success, failure, or running.                 the blackboard. The detection system then focusses on that\\r\\nA blackboard is used to interface between the tree and the                      ID for a short time (3s) and ignores all other IDs, this is to\\r\\nrobot, holding conditioned and abstracted senses and means of                   prevent flipping of attention when multiple IDs are visible. The\\r\\nactuation. The three top-level behaviours are described below                   fiducial transform can be used by the controller to navigate to\\r\\nin more detail.                                                                 the predock and dock locations relative to the carrier.\\r\\n      a) Explore: The explore behaviour sends the robot on a                       When a carrier markermap is visible, the transform to the\\r\\nballistic random walk. At the start, or whenever a potential                    centre of the carrier is made available on the blackboard. This\\r\\n\\x0c\\n\\n                                                                                                                                      13\\r\\n\\r\\n\\r\\n\\r\\n                                                                             TABLE V: Performance in intralogistics task\\r\\n                                                                                     Run    Carriers retrieved   Time (s)\\r\\n                                                                                      1             5              136\\r\\n                                                                                      2             5               99\\r\\n                                                                                      3             5              213\\r\\n                                                                                      4             5              116\\r\\n                                                                                      5             5              327\\r\\n                                                                                                    x\\xcc\\x84             178\\r\\n\\r\\n\\r\\n\\r\\n                                                                        Static objects result in corresponding cells in the map\\r\\n                                                                     reaching the upper limit of 1.0, transient or false returns tend\\r\\n                                                                     towards zero.\\r\\n                                                                        Collisions are predicted by projecting the current velocity\\r\\n                                                                     vector forward by a lookahead time, then taking the average\\r\\n                                                                     value of the cell contents over a circular area related to the\\r\\n                                                                     robot size. The scalar value is a collision likelihood which is\\r\\nFig. 14: Underside of a carrier, showing the markermap and           made available on the blackboard.\\r\\nthe frame. Legs in blue, and the grey areas show regions where          4) Navigation: The various behaviours need to navigate\\r\\nthe centreline of the upwards-facing camera cannot physically        and choose a path to follow. All navigation is performed in the\\r\\nreach.                                                               local frame of the robot - there is no global map. For example,\\r\\n                                                                     when exploring, a direction is chosen as described above, then\\r\\n                                                                     a lower-level navigation behaviour is started that creates a\\r\\n                                                                     trajectory, then continually monitors the collision map of the\\r\\n                                                                     locality to abort the trajectory playout if necessary. If there are\\r\\n                                                                     no paths available that don\\xe2\\x80\\x99t trigger a collision warning, for\\r\\n                                                                     example, if a robot is under a carrier, then a fallback behaviour\\r\\n                                                                     of picking a least-worst (lowest sum of nearby collision cells)\\r\\n                                                                     direction and moving slowly for short distances is used to\\r\\n                                                                     safely emerge into more open space.\\r\\n\\r\\n\\r\\nFig. 15: Example collision map as visualised by Rviz, with a         C. Performance\\r\\nframe indicator at the location of the robot, and the grey area         We ran the task five times, in each case the five robots were\\r\\nthe robot-relative map, showing two of the arena walls.              positioned with random orientation in the drop zone, and the\\r\\n                                                                     carriers in the search zone. Robots were allowed to run until\\r\\n                                                                     the task was completed and all robots had escaped the drop\\r\\ncan be used to navigate the robot to correct central location        zone, or more than 10 minutes elapsed. The minimum time\\r\\nunder the carrier in order to safely lift it.                        the task could take, assuming each robot goes directly to the\\r\\n   3) Collision: Information from the IRToF sensors is used          nearest carrier, the distance from robot to carrier is 3 m, search\\r\\nto build a dynamic robot-relative map of obstacles. The map is       speed is 0.5 ms\\xe2\\x88\\x921 , carry speed 0.3 ms\\xe2\\x88\\x921 , pickup and drop times\\r\\nan array of cells, centred on the robot, with values in the range    5 s, is 26 s Obviously, this assumes each robot moves directly\\r\\n[0, 1]. The value of a cell continually reduces by exponential       underneath a separate carrier, there are no collisions, and they\\r\\ndecay rate \\xce\\xbb, and is increased when a cell is affected by            move directly to the drop zone.\\r\\na sensor return. Sensor returns increase a cell location by             Table V shows the results. In every case, the swarm was\\r\\na Gaussian based on distance between each return location            able to successfully retrieve all the carriers within the allowed\\r\\n(x0i , yi0 ) and cell location (x, y):                               10 minutes. The average time to complete the task was 178\\r\\n                             q                                       seconds. All the runs are shown in the Supplementary video\\r\\n                        ri = (x0i \\xe2\\x88\\x92 x)2 + (yi0 \\xe2\\x88\\x92 y)2           (2)   material logistics task runs.\\r\\n                          n      r2\\r\\n                          X  \\xe2\\x88\\x92 2 i\\r\\n              S(x, y) =     e 2\\xcf\\x83sensor                        (3)                           V. C ONCLUSION\\r\\n                          i=1\\r\\n                                                                       In this work, we have introduced DOTS, our new open\\r\\nwhere S(x, y) is the sensor return function, showing the total       access industrial swarm testbed. With a purpose-build arena,\\r\\neffect all n sensor returns have at this location.                   20 high specification robots that move fast, have long en-\\r\\n   Each map location M (x, y) changes over time as:                  durance and high computational power, a platform-agnostic\\r\\n                                                                     development environment, and a cloud portal, this system\\r\\n Mt+\\xe2\\x88\\x86t (x, y) = min(Mt (x, y)(1 \\xe2\\x88\\x92 \\xce\\xbb\\xe2\\x88\\x86t) + S(x, y), 1.0) (4)           breaks new ground in enabling experimentation.\\r\\n                                                                       The intralogistics scenario demonstrates the abilities of\\r\\nwith the decay rate \\xce\\xbb = 1s, and \\xcf\\x83sensor = 25mm.                      the robot swarm to successfully complete a non-trivial task.\\r\\n\\x0c\\n\\n                                                                                                                                                  14\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                       A                                        B                                  C\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                       D                                        E                                   F\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    Fig. 16: Sequence showing a robot picking up a carrier. (A) Overall View. (B) While exploring, a robot detects a carrier fiducial\\r\\n    (blue arrow) and starts the pick up behaviour. It moves to the pre-dock position (orange arrow). (C) The robot is now facing\\r\\n    the carrier and is in the optimal position for accurate pose estimation of the carrier fiducial so (D) moves under the carrier and\\r\\n    (E) centres itself using upwards-facing camera and raises the lifting platform. Finally (F) the robot moves towards the drop\\r\\n    zone.\\r\\n\\r\\n\\r\\n                              Robot paths during logistics task                 and deposit multiple cargo carriers, is considerable. More\\r\\n                                                                      Robot 1   importantly, we demonstrate a process. The DOTS Testbed\\r\\n                 1.5                                                  Robot 2   pipeline reduces barriers to entry by making it easy to get\\r\\n                                                                      Robot 3\\r\\n                                                                      Robot 4   started, using the cross-platform IDE to experiment with ideas\\r\\n                                                                      Robot 5   in simulation, then making available a real physical robot\\r\\n                 1.0\\r\\n                                                                                swarm to see these ideas translated into reality.\\r\\n                                                                                   There are many directions this research can go in. Currently,\\r\\n                 0.5                                                            there is necessarily some manual management of the robots,\\r\\n                                                                                they need to be manually charged, set up for experimental runs,\\r\\nY position (m)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                and watched over. We are currently designing automatic charg-\\r\\n                 0.0\\r\\n                                                                                ing stations to aid this. Validation of controllers in simulation\\r\\n                                                                                is difficult, although the simulator is relatively high-fidelity, it\\r\\n                 0.5                                                            still suffers from differences to reality, and it is not possible\\r\\n                                                                                to guarantee the safety of a general-purpose controller; even\\r\\n                                                                                defining safety for a swarm is an open question. At the\\r\\n                 1.0                                                            moment, we are not utilising the full processing power of the\\r\\n                                                                                robotic platform - we intend to accelerate image processing,\\r\\n                 1.5                                                            moving it onto the GPUs, both on the single board computer\\r\\n                                                                                and on the peripheral Raspberry Pi Zeros. We look forward\\r\\n                       1.5   1.0     0.5        0.0       0.5   1.0    1.5      to exploring the possibilities for localisation and short-range\\r\\n                                           X position (m)                       communication of the BLE5 and UWE radios, and the two\\r\\n    Fig. 17: Tracks of the robots in one run of the logistics task,             microphones. The possibilities for decentralised stochastic\\r\\n    with thicker tracks when a robot is carrying a load. loads are              swarm algorithms to reliably perform logistics tasks is touched\\r\\n    carried from left to right. In this example, robot 3 never picks            upon here, and is open to further work. Designing toolkits of\\r\\n    up a load.                                                                  useful, working, and validated sub-behaviours from which to\\r\\n                                                                                more easily construct full applications is another step towards\\r\\n                                                                                wider real-world use of swarm techniques.\\r\\n                                                                                   Building swarm robotic applications for the real world is\\r\\n    Although conceptually simple, the underlying processing nec-                hard, and one of the biggest obstacles is the lack of actually\\r\\n    essary to handle the five concurrent video streams, time-of-                existing robot swarms with the types of capabilities necessary\\r\\n    flight pointcloud, and other senses, and entirely autonomously              to experiment with ideas. By making available such a swarm,\\r\\n    search for, find, manoeuvre under with precision, lift, carry               together with an accessible development pipeline, we hope to\\r\\n\\x0c\\n\\n                                                                                                                                                               15\\r\\n\\r\\n\\r\\n\\r\\nreduce the barriers to entry and stimulate further research.                      [16] S. Su, F. Heide, R. Swanson, J. Klein, C. Callenberg, M. Hullin, and\\r\\n                                                                                       W. Heidrich, \\xe2\\x80\\x9cMaterial classification using raw time-of-flight measure-\\r\\n                                                                                       ments,\\xe2\\x80\\x9d in Proceedings of the IEEE Conference on Computer Vision and\\r\\n                         ACKNOWLEDGMENT                                                Pattern Recognition, 2016, pp. 3503\\xe2\\x80\\x933511.\\r\\n                                                                                  [17] C. Callenberg, Z. Shi, F. Heide, and M. B. Hullin, \\xe2\\x80\\x9cLow-cost spad\\r\\n  S.J. was supported by EPSRC DTP Doctoral Prize Award                                 sensing for non-line-of-sight tracking, material classification and depth\\r\\n                                                                                       imaging,\\xe2\\x80\\x9d ACM Transactions on Graphics (TOG), vol. 40, no. 4, pp.\\r\\nEP/T517872/1. S.J. and S.H. were supported by EPSRC IAA                                1\\xe2\\x80\\x9312, 2021.\\r\\nAward EP/R511663/1. E.M. was supported by the EPSRC                               [18] R. Rojas and A. G. Fo\\xcc\\x88rster, \\xe2\\x80\\x9cHolonomic control of a robot with an\\r\\nCentre for Doctoral Training in Future Autonomous and                                  omnidirectional drive,\\xe2\\x80\\x9d KI-Ku\\xcc\\x88nstliche Intelligenz, vol. 20, no. 2, pp. 12\\xe2\\x80\\x93\\r\\n                                                                                       17, 2006.\\r\\nRobotic Systems (FARSCOPE) and an EPSRC ICASE Award.                              [19] B. Katz, J. Di Carlo, and S. Kim, \\xe2\\x80\\x9cMini cheetah: A platform for\\r\\nToshiba Bristol Research and Innovation Laboratory (BRIL)                              pushing the limits of dynamic quadruped control,\\xe2\\x80\\x9d in 2019 international\\r\\nprovided additional support.                                                           conference on robotics and automation (ICRA). IEEE, 2019, pp. 6295\\xe2\\x80\\x93\\r\\n                                                                                       6301.\\r\\n                                                                                  [20] U. H. Lee, C.-W. Pan, and E. J. Rouse, \\xe2\\x80\\x9cEmpirical characterization of\\r\\n                                                                                       a high-performance exterior-rotor type brushless dc motor and drive,\\xe2\\x80\\x9d\\r\\n                              R EFERENCES                                              in 2019 IEEE/RSJ International Conference on Intelligent Robots and\\r\\n                                                                                       Systems (IROS). IEEE, 2019, pp. 8018\\xe2\\x80\\x938025.\\r\\n [1] L. Custodio and R. Machado, \\xe2\\x80\\x9cFlexible automated warehouse: a liter-          [21] F. Grimminger, A. Meduri, M. Khadiv, J. Viereck, M. Wu\\xcc\\x88thrich,\\r\\n     ature review and an innovative framework,\\xe2\\x80\\x9d The International Journal              M. Naveau, V. Berenz, S. Heim, F. Widmaier, T. Flayols et al., \\xe2\\x80\\x9cAn\\r\\n     of Advanced Manufacturing Technology, vol. 106, no. 1, pp. 533\\xe2\\x80\\x93558,               open torque-controlled modular robot architecture for legged locomotion\\r\\n     2020.                                                                             research,\\xe2\\x80\\x9d IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.\\r\\n [2] Y. Jaghbeer, R. Hanson, and M. I. Johansson, \\xe2\\x80\\x9cAutomated order picking             3650\\xe2\\x80\\x933657, 2020.\\r\\n     systems and the links between design and performance: a systematic           [22] N. Kau, A. Schultz, N. Ferrante, and P. Slade, \\xe2\\x80\\x9cStanford doggo: An open-\\r\\n     literature review,\\xe2\\x80\\x9d International Journal of Production Research, vol. 58,        source, quasi-direct-drive quadruped,\\xe2\\x80\\x9d in 2019 International conference\\r\\n     no. 15, pp. 4489\\xe2\\x80\\x934505, 2020.                                                      on robotics and automation (ICRA). IEEE, 2019, pp. 6309\\xe2\\x80\\x936315.\\r\\n [3] I. Draganjac, D. Miklic\\xcc\\x81, Z. Kovac\\xcc\\x8cic\\xcc\\x81, G. Vasiljevic\\xcc\\x81, and S. Bogdan,       [23] P. Dini and S. Saponara, \\xe2\\x80\\x9cCogging torque reduction in brushless motors\\r\\n     \\xe2\\x80\\x9cDecentralized control of multi-agv systems in autonomous warehousing             by a nonlinear control technique,\\xe2\\x80\\x9d Energies, vol. 12, no. 11, p. 2224,\\r\\n     applications,\\xe2\\x80\\x9d IEEE Transactions on Automation Science and Engineer-              2019.\\r\\n     ing, vol. 13, no. 4, pp. 1433\\xe2\\x80\\x931447, 2016.                                    [24] L. Berscheid and T. Kro\\xcc\\x88ger, \\xe2\\x80\\x9cJerk-limited real-time trajectory generation\\r\\n [4] G. Fragapane, R. de Koster, F. Sgarbossa, and J. O. Strandhagen,                  with arbitrary target states,\\xe2\\x80\\x9d arXiv preprint arXiv:2105.04830, 2021.\\r\\n     \\xe2\\x80\\x9cPlanning and control of autonomous mobile robots for intralogistics:        [25] D. Thomas, W. Woodall, and E. Fernandez, \\xe2\\x80\\x9cNext-generation\\r\\n     Literature review and research agenda,\\xe2\\x80\\x9d European Journal of Opera-                ROS: Building on DDS,\\xe2\\x80\\x9d in ROSCon Chicago 2014. Mountain\\r\\n     tional Research, 2021.                                                            View, CA: Open Robotics, sep 2014. [Online]. Available: https:\\r\\n [5] E. S\\xcc\\xa7ahin, \\xe2\\x80\\x9cSwarm robotics: From sources of inspiration to domains of             //vimeo.com/106992622\\r\\n     application,\\xe2\\x80\\x9d in International Workshop on Swarm robotics (SR 2004),         [26] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,\\r\\n     S. W. S\\xcc\\xa7ahin E., Ed. Santa Monica, CA, USA: Springer, 2005, pp.                   R. Wheeler, and A. Y. Ng, \\xe2\\x80\\x9cROS: an open-source Robot Operating\\r\\n     10\\xe2\\x80\\x9320.                                                                            System,\\xe2\\x80\\x9d in IEEE International Conference on Robotics and Automation\\r\\n [6] M. Schranz, G. A. Di Caro, T. Schmickl, W. Elmenreich, F. Arvin,                  (ICRA 2009) Workshop on Open Source Robotics, vol. 3, no. 3.2. Kobe,\\r\\n     A. S\\xcc\\xa7ekerciog\\xcc\\x86lu, and M. Sende, \\xe2\\x80\\x9cSwarm intelligence and cyber-physical            Japan: IEEE, 2009, p. 5.\\r\\n     systems: concepts, challenges and future trends,\\xe2\\x80\\x9d Swarm and Evolution-       [27] S. H. Juan and F. H. Cotarelo, \\xe2\\x80\\x9cMulti-master ros systems,\\xe2\\x80\\x9d Institut de\\r\\n     ary Computation, vol. 60, p. 100762, 2021.                                        Robotics and Industrial Informatics, pp. 1\\xe2\\x80\\x9318, 2015.\\r\\n [7] D. Pickem, P. Glotfelter, L. Wang, M. Mote, A. Ames, E. Feron, and           [28] G. Pardo-Castellote, \\xe2\\x80\\x9cOmg data-distribution service: Architectural\\r\\n     M. Egerstedt, \\xe2\\x80\\x9cThe robotarium: A remotely accessible swarm robotics               overview,\\xe2\\x80\\x9d in 23rd International Conference on Distributed Computing\\r\\n     research testbed,\\xe2\\x80\\x9d in 2017 IEEE International Conference on Robotics              Systems Workshops, 2003. Proceedings. IEEE, 2003, pp. 200\\xe2\\x80\\x93206.\\r\\n     and Automation (ICRA). IEEE, 2017, pp. 1699\\xe2\\x80\\x931706.                            [29] K. An, A. Gokhale, D. Schmidt, S. Tambe, P. Pazandak, and G. Pardo-\\r\\n [8] L. Paull, J. Tani, H. Ahn, J. Alonso-Mora, L. Carlone, M. Cap, Y. F.              Castellote, \\xe2\\x80\\x9cContent-based filtering discovery protocol (cfdp) scalable\\r\\n     Chen, C. Choi, J. Dusek, Y. Fang et al., \\xe2\\x80\\x9cDuckietown: an open,                    and efficient omg dds discovery protocol,\\xe2\\x80\\x9d in Proceedings of the 8th\\r\\n     inexpensive and flexible platform for autonomy education and research,\\xe2\\x80\\x9d           ACM International Conference on Distributed Event-Based Systems,\\r\\n     in 2017 IEEE International Conference on Robotics and Automation                  2014, pp. 130\\xe2\\x80\\x93141.\\r\\n     (ICRA). IEEE, 2017, pp. 1497\\xe2\\x80\\x931504.                                           [30] J. Sanchez-Monedero, J. Povedano-Molina, J. M. Lopez-Vega, and\\r\\n [9] M. ten Hompel, H. Bayhan, J. Behling, L. Benkenstein, J. Emmerich,                J. M. Lopez-Soler, \\xe2\\x80\\x9cBloom filter-based discovery protocol for dds\\r\\n     G. Follert, M. Grzenia, C. Hammermeister, H. Hasse, D. Hoening                    middleware,\\xe2\\x80\\x9d Journal of Parallel and Distributed Computing, vol. 71,\\r\\n     et al., \\xe2\\x80\\x9cTechnical report: Loadrunner\\xc2\\xae, a new platform approach                   no. 10, pp. 1305\\xe2\\x80\\x931317, 2011.\\r\\n     on collaborative logistics services,\\xe2\\x80\\x9d Logistics Journal: nicht referierte    [31] W. Kumari, C. E. Perkins, M. McBride, D. Stanley, and J. C. Zu\\xcc\\x81n\\xcc\\x83iga,\\r\\n     Vero\\xcc\\x88ffentlichungen, vol. 2020, no. 10, 2020.                                     \\xe2\\x80\\x9cRfc 9119-multicast considerations over ieee 802 wireless media,\\xe2\\x80\\x9d 2021.\\r\\n[10] M. Williams, \\xe2\\x80\\x9cLow pin-count debug interfaces for multi-device sys-           [32] Eclipse Foundation. (2021) Minimizing discovery overhead in ROS2.\\r\\n     tems,\\xe2\\x80\\x9d 2009.                                                                      [Online]. Available: https://zenoh.io/blog/2021-03-23-discovery/\\r\\n[11] R. Mur-Artal and J. D. Tardo\\xcc\\x81s, \\xe2\\x80\\x9cOrb-slam2: An open-source slam              [33] W. Meeussen. (2010, October) Coordinate frames for mobile platforms.\\r\\n     system for monocular, stereo, and rgb-d cameras,\\xe2\\x80\\x9d IEEE transactions               [Online]. Available: https://ros.org/reps/rep-0105.html\\r\\n     on robotics, vol. 33, no. 5, pp. 1255\\xe2\\x80\\x931262, 2017.                            [34] T. Foote, \\xe2\\x80\\x9ctf: The transform library,\\xe2\\x80\\x9d in 2013 IEEE Conference on\\r\\n[12] F. J. Romero-Ramirez, R. Mun\\xcc\\x83oz-Salinas, and R. Medina-Carnicer,                  Technologies for Practical Robot Applications (TePRA). IEEE, 2013,\\r\\n     \\xe2\\x80\\x9cSpeeded up detection of squared fiducial markers,\\xe2\\x80\\x9d Image and vision              pp. 1\\xe2\\x80\\x936.\\r\\n     Computing, vol. 76, pp. 38\\xe2\\x80\\x9347, 2018.                                         [35] D. Merkel, \\xe2\\x80\\x9cDocker: lightweight linux containers for consistent devel-\\r\\n[13] C. Bachhuber, E. Steinbach, M. Freundl, and M. Reisslein, \\xe2\\x80\\x9cOn the                 opment and deployment,\\xe2\\x80\\x9d Linux journal, vol. 2014, no. 239, p. 2, 2014.\\r\\n     minimization of glass-to-glass and glass-to-algorithm delay in video         [36] NaturalPoint, Inc. DBA OptiTrack. (2022) Motion capture systems.\\r\\n     communication,\\xe2\\x80\\x9d IEEE Transactions on Multimedia, vol. 20, no. 1, pp.              [Online]. Available: https://optitrack.com/\\r\\n     238\\xe2\\x80\\x93252, 2017.                                                               [37] T. Farnham, S. Jones, A. Aijaz, Y. Jin, I. Mavromatis, U. Raza,\\r\\n[14] C. Bachhuber and E. Steinbach, \\xe2\\x80\\x9cAre today\\xe2\\x80\\x99s video communication                   A. Portelli, A. Stanoev, and M. Sooriyabandara, \\xe2\\x80\\x9cUmbrella collaborative\\r\\n     solutions ready for the tactile internet?\\xe2\\x80\\x9d in 2017 IEEE Wireless Com-             robotics testbed and iot platform,\\xe2\\x80\\x9d in 2021 IEEE 18th Annual Consumer\\r\\n     munications and Networking Conference Workshops (WCNCW). IEEE,                    Communications & Networking Conference (CCNC). IEEE, 2021, pp.\\r\\n     2017, pp. 1\\xe2\\x80\\x936.                                                                    1\\xe2\\x80\\x937.\\r\\n[15] T. Moore and D. Stouch, \\xe2\\x80\\x9cA generalized extended kalman filter imple-         [38] J.-B. Mouret and K. Chatzilygeroudis, \\xe2\\x80\\x9c20 years of reality gap: a\\r\\n     mentation for the robot operating system,\\xe2\\x80\\x9d in Proceedings of the 13th             few thoughts about simulators in evolutionary robotics,\\xe2\\x80\\x9d in Workshop\\r\\n     International Conference on Intelligent Autonomous Systems (IAS-13).              \\xe2\\x80\\x9dSimulation in Evolutionary Robotics\\xe2\\x80\\x9d, Genetic and Evolutionary Com-\\r\\n     Springer, July 2014.                                                              putation Conference (GECCO 2017). Berlin, Germany: ACM, 2017.\\r\\n\\x0c\\n\\n                                                                                 16\\r\\n\\r\\n\\r\\n\\r\\n[39] D. Pickem, L. Wang, P. Glotfelter, Y. Diaz-Mercado, M. Mote, A. Ames,\\r\\n     E. Feron, and M. Egerstedt, \\xe2\\x80\\x9cSafe, remote-access swarm robotics re-\\r\\n     search on the robotarium,\\xe2\\x80\\x9d arXiv preprint arXiv:1604.00640, 2016.\\r\\n[40] G. Beltrame, E. Merlo, J. Panerati, and C. Pinciroli, \\xe2\\x80\\x9cEngineering safety\\r\\n     in swarm robotics,\\xe2\\x80\\x9d in Proceedings of the 1st International Workshop\\r\\n     on Robotics Software Engineering, 2018, pp. 36\\xe2\\x80\\x9339.\\r\\n[41] K. I. Eder, W.-l. Huang, and J. Peleska, \\xe2\\x80\\x9cComplete agent-driven\\r\\n     model-based system testing for autonomous systems,\\xe2\\x80\\x9d arXiv preprint\\r\\n     arXiv:2110.12586, 2021.\\r\\n[42] Khronos Group, WebGL 1.0 Specification, Khronos Group, 2011.\\r\\n[43] S. Jones, E. Milner, M. Sooriyabandara, and S. Hauert, \\xe2\\x80\\x9cDistributed\\r\\n     situational awareness in robot swarms,\\xe2\\x80\\x9d Advanced Intelligent Systems,\\r\\n     vol. 2, no. 11, p. 2000110, 2020.\\r\\n[44] L. Pitonakova, R. Crowder, and S. Bullock, \\xe2\\x80\\x9cInformation exchange\\r\\n     design patterns for robot swarm foraging and their application in robot\\r\\n     control algorithms,\\xe2\\x80\\x9d Frontiers in Robotics and AI, vol. 5, p. 47, 2018.\\r\\n[45] S. Jones, A. F. Winfield, S. Hauert, and M. Studley, \\xe2\\x80\\x9cOnboard evolution\\r\\n     of understandable swarm behaviors,\\xe2\\x80\\x9d Advanced Intelligent Systems,\\r\\n     vol. 1, 2019.\\r\\n[46] M. Dorigo, D. Floreano, L. M. Gambardella, F. Mondada, S. Nolfi,\\r\\n     T. Baaboura, M. Birattari, M. Bonani, M. Brambilla, A. Brutschy et al.,\\r\\n     \\xe2\\x80\\x9cSwarmanoid: a novel concept for the study of heterogeneous robotic\\r\\n     swarms,\\xe2\\x80\\x9d IEEE Robotics & Automation Magazine, vol. 20, no. 4, pp.\\r\\n     60\\xe2\\x80\\x9371, 2013.\\r\\n[47] A. Champandard, \\xe2\\x80\\x9cBehavior trees for next-gen game AI,\\xe2\\x80\\x9d in Game\\r\\n     developers conference, audio lecture, 2007.\\r\\n[48] P. Ogren, \\xe2\\x80\\x9cIncreasing modularity of UAV control systems using com-\\r\\n     puter game behavior trees,\\xe2\\x80\\x9d in AIAA Guidance, Navigation and Control\\r\\n     Conference. Minneapolis, MN, USA: AIAA, 2012.\\r\\n[49] A. Klo\\xcc\\x88ckner, \\xe2\\x80\\x9cBehavior trees for uav mission management.\\xe2\\x80\\x9d in IN-\\r\\n     FORMATIK 2013 Informatik angepasst an Mensch, Organisation und\\r\\n     Umwelt. Koblenz, Germany: Springer, 2013, pp. 57\\xe2\\x80\\x9368.\\r\\n[50] S. Jones, M. Studley, S. Hauert, and A. F. Winfield, \\xe2\\x80\\x9cEvolving be-\\r\\n     haviour trees for swarm robotics,\\xe2\\x80\\x9d in 13th International Symposium\\r\\n     on Distributed Autonomous Robotic Systems (DARS 2016), R. Gro\\xc3\\x9f,\\r\\n     A. Kolling, S. Berman, E. Frazzoli, A. Martinoli, F. Matsuno, and\\r\\n     M. Gauci, Eds. London, UK: Springer, 2016.\\r\\n[51] M. Colledanchise, \\xe2\\x80\\x9cBehavior trees in robotics,\\xe2\\x80\\x9d 2017.\\r\\n[52] A. Marzinotto, M. Colledanchise, C. Smith, and P. Ogren, \\xe2\\x80\\x9cTowards a\\r\\n     unified behavior trees framework for robot control,\\xe2\\x80\\x9d in IEEE Interna-\\r\\n     tional Conference on Robotics and Automation (ICRA 2014). Hong\\r\\n     Kong, China: IEEE, 2014, pp. 5420\\xe2\\x80\\x935427.\\r\\n\\x0c']\n"
     ]
    }
   ],
   "source": [
    "#Encoding all the pdf text in UTF-8\n",
    "\n",
    "elements=[]\n",
    "for i in array_pdf_text:\n",
    "    elements.append(i.encode(\"utf-8\"))\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a4129a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Alldieck_Xu_Sminchisescu',\n",
       "  'Imghum_Implicit',\n",
       "  'Allen_Curless_Popovic',\n",
       "  'Hertzmann',\n",
       "  'Learning',\n",
       "  'Anguelov_Srinivasan_Koller',\n",
       "  'Thrun_Rodgers_Davis',\n",
       "  'Bagautdinov_Wu_Simon',\n",
       "  'Shiratori_Wei',\n",
       "  'Xu_Sheikh_Saragih',\n",
       "  'Khosla_Popovic',\n",
       "  'Register',\n",
       "  'Chen_Zheng_Black',\n",
       "  'Hilliges_Geiger',\n",
       "  'Chen_Zhang',\n",
       "  'Learn',\n",
       "  'Deng_Lewis',\n",
       "  'Norouzi_Tagliasacchi',\n",
       "  'Xe_Springer',\n",
       "  'Atzmon_Lipman',\n",
       "  'Habermann_Liu',\n",
       "  'Xu_Zollhoefer_Pons_Moll_Theobalt',\n",
       "  'Datta_Nowrouzezahrai',\n",
       "  'Proceedings',\n",
       "  'Kanazawa_Zhang',\n",
       "  'Felsen_Malik',\n",
       "  'Kim_Pons_Moll_Pujades',\n",
       "  'Kim_Black',\n",
       "  'Lee',\n",
       "  'Lea_Vidal_Reiter_Hager_Temporal',\n",
       "  'Bai',\n",
       "  'Li_Yang_Ross',\n",
       "  'Ling',\n",
       "  'Zinno_Cheng_Van_De_Panne',\n",
       "  'Liu_Habermann_Rudnev',\n",
       "  'Sarkar_Gu_Theobalt',\n",
       "  'Neural',\n",
       "  'Cline_Marching',\n",
       "  'Graphic_Xe',\n",
       "  'Ma_Saito_Yang',\n",
       "  'Tang_Black',\n",
       "  'Jun',\n",
       "  'Ma_Yang',\n",
       "  'Ghorbani_Troje',\n",
       "  'Martinez_Black',\n",
       "  'Mihajlovic_Zhang_Black',\n",
       "  'Mildenhall_Srinivasan',\n",
       "  'Tancik_Barron',\n",
       "  'Ramamoorthi_Ng',\n",
       "  'Oechsle_Geiger',\n",
       "  'Osman',\n",
       "  'Bolkart_Black_Star',\n",
       "  'Ott_Grebogi_Yorke',\n",
       "  'Palafox_Boz',\n",
       "  'Florence_Straub_Newcombe',\n",
       "  'Peng_Zhang',\n",
       "  'Wang_Shuai_Bao_Zhou',\n",
       "  'Fortunato_Sanchez',\n",
       "  'Gonzalez_Battaglia',\n",
       "  'Prokudin_Lassner_Romero',\n",
       "  'Efficient',\n",
       "  'Qi',\n",
       "  'Su_Mo_Guibas',\n",
       "  'Ronneberger_Fischer_Brox',\n",
       "  'Saito_Yang',\n",
       "  'Sanchez_Gonzalez',\n",
       "  'Godwin_Pfaff',\n",
       "  'Wiley_Online_Library',\n",
       "  'Sifakis_Barbic',\n",
       "  'Xe_X',\n",
       "  'Wang_Rojas_Kla',\n",
       "  'Tiwari_Sarafianos_Tung_Pons_Moll',\n",
       "  'Tsuchida_Fukayama_Hamasaki_Goto',\n",
       "  'Wang_Xe_Xbrien',\n",
       "  'Wang_Mihajlovic',\n",
       "  'Ma_Geiger_Tang',\n",
       "  'Metaavatar_Learning',\n",
       "  'Xiang_Prada',\n",
       "  'Bagautdinov_Xu',\n",
       "  'Dong_Wen_Hodgins_Wu',\n",
       "  'Xie_Takikawa',\n",
       "  'Saito_Litany',\n",
       "  'Xu_Bazavan',\n",
       "  'Zanfir_Freeman',\n",
       "  'Sukthankar_Sminchisescu',\n",
       "  'Yang_Liang_Lin_Learning',\n",
       "  'Zakharkin_Mazur_Grigorev_Lempitsky',\n",
       "  'Zheng_Zhou_Ceylan_Barbic',\n",
       "  'Pose_Accuracy_Nwe',\n",
       "  'Tab',\n",
       "  'Xe_Xcmean',\n",
       "  'Fig',\n",
       "  'Input_Pose_Accuracy_Subject'],\n",
       " ['Bernardo_Avila',\n",
       "  'Daniel_Guo_Moham',\n",
       "  'Jean_Baptiste_Alayrac_Adria_Recasens',\n",
       "  'Rosalia_Schneider',\n",
       "  'Relja_Arandjelovic_Jason_Ramapuram',\n",
       "  'Jeffrey_De_Fauw_Lu',\n",
       "  'Dieleman_Andrew_Zisserman',\n",
       "  'Alp_Gu',\n",
       "  'Natalia_Neverova',\n",
       "  'Kokkinos',\n",
       "  'Densepose_Dense',\n",
       "  'Leonid_Pishchulin',\n",
       "  'Bernt_Schiele',\n",
       "  'Andrew_Zisserman',\n",
       "  'Mahmoud_Assran',\n",
       "  'Nicolas_Ballas',\n",
       "  'Albert_Haque',\n",
       "  'Peng_Zelun_Luo_Alexandre',\n",
       "  'Michael_Rabbat',\n",
       "  'Ena_Yeung_Li_Fei_Fei',\n",
       "  'Hidalgo_Martinez',\n",
       "  'Wei_Devamanyu',\n",
       "  'Hazarika_Roger_Zimmermann',\n",
       "  'Soujanya_Sheikh',\n",
       "  'Ting_Chen',\n",
       "  'Simon_Kornblith_Mohammad_Norouzi_Ge',\n",
       "  'Wu_Saining_Xie_Ross',\n",
       "  'Con_Xinlei',\n",
       "  'Chen_Haoqi',\n",
       "  'Ross_Girshick_Kaiming',\n",
       "  'Antonio_Herna',\n",
       "  'Vela_Nadezhda',\n",
       "  'Alexander_Xianjie_Chen',\n",
       "  'Raquel_Urtasun',\n",
       "  'Alan_Yuille',\n",
       "  'Graph',\n",
       "  'Hu',\n",
       "  'Fangzhou_Hong',\n",
       "  'Cai_Ziwei',\n",
       "  'Liu_Yuxin',\n",
       "  'Chen_Ziqi_Zhang',\n",
       "  'Chunfeng_Yuan_Bing_Li_Ying_Garmentd_Garment',\n",
       "  'Deng_Weiming_Hu',\n",
       "  'Ji_Hou_Saining',\n",
       "  'Benjamin_Graham',\n",
       "  'Matthias_Nie',\n",
       "  'Box_Benchmark',\n",
       "  'Catalin_Ionescu',\n",
       "  'Angela_Dai',\n",
       "  'Angel_Chang_Manolis_Savva_Maciej_Hal_Sminchisescu',\n",
       "  'Thomas_Funkhouser',\n",
       "  'Ke_Gong',\n",
       "  'Yicheng_Li',\n",
       "  'Chen_Wang',\n",
       "  'Aaron',\n",
       "  'Sarna_Yang',\n",
       "  'Liang_Lin',\n",
       "  'Yonglong_Tian_Phillip_Isola',\n",
       "  'Ce_Liu',\n",
       "  'Dilip_Krishnan',\n",
       "  'Alex_Krizhevsky_Ilya',\n",
       "  'Geoffrey_Hinton',\n",
       "  'Xiaodan_Liang_Dongyu',\n",
       "  'Zhang_Xiaohui',\n",
       "  'Jianshu_Li',\n",
       "  'Jian_Zhao_Yunchao',\n",
       "  'Li_Terence_Sim_Shuicheng',\n",
       "  'Yan_Jiashi_Feng',\n",
       "  'Jean_Bastien_Grill',\n",
       "  'Corentin',\n",
       "  'Tallec_Pierre_Richemond_Elena_Buchatskaya_Carl_Do',\n",
       "  'Tsung_Yi_Lin',\n",
       "  'Michael_Maire_Serge_Belongie',\n",
       "  'Pamela_Mishkin_Jack_Clark',\n",
       "  'Learn_Pietro',\n",
       "  'Lawrence',\n",
       "  'Zitnick',\n",
       "  'Leonid_Pishchulin_Jayan_Springer',\n",
       "  'Jun_Liu_Amir_Shahroudy_Mauricio_Perez_Gang_Wang',\n",
       "  'Ling_Yu',\n",
       "  'Alex_Kot',\n",
       "  'Intelligence_Xe',\n",
       "  'Andrew_Rouditchenko_Angie',\n",
       "  'David_Harwath_Songtao',\n",
       "  'Jian_Sun',\n",
       "  'Self_Brian',\n",
       "  'Samuel_Thomas_Kartik_Au',\n",
       "  'Hilde_Kuehne_Rameswar',\n",
       "  'Rogerio_Feris',\n",
       "  'Avlnet_Learning',\n",
       "  'Yunze_Liu',\n",
       "  'Shanghang_Zhang',\n",
       "  'Hao_Dong',\n",
       "  'Li_Yi',\n",
       "  'Amir_Shahroudy_Jun',\n",
       "  'Li_Yi_Shanghang',\n",
       "  'Funkhouser_Hao_Dong',\n",
       "  'Rgb_Scene',\n",
       "  'Ke_Sun_Bin_Xiao_Dong_Liu_Jingdong_Wang',\n",
       "  'Julieta_Martinez',\n",
       "  'Feitong_Tan',\n",
       "  'Danhang_Tang_Mingsong_Dou',\n",
       "  'Kaiwen_Guo_Kaichiro',\n",
       "  'Jun_Miura',\n",
       "  'Rohit_Pandey',\n",
       "  'Keskin_Ruofei',\n",
       "  'Sun_Sofien',\n",
       "  'Humangps_Geodesic',\n",
       "  'Aaron_Van_Den_Oord_Yazhe_Li_Oriol_Vinyals',\n",
       "  'Repre',\n",
       "  'Yonglong_Tian',\n",
       "  'Dilip_Krishnan_Phillip_Isola',\n",
       "  'Con_Mandela_Patrick_Yuki_Asano',\n",
       "  'Fong_Joao_Henriques_Geoffrey_Zweig',\n",
       "  'Andrea',\n",
       "  'Proceedings_Part',\n",
       "  'Mandela_Patrick_Yuki_Asano',\n",
       "  'Wei_Lingxi',\n",
       "  'Jianlong_Chang_Fong_Joa',\n",
       "  'Henriques_Geoffrey_Zweig',\n",
       "  'Andrea_Xiaopeng_Zhang',\n",
       "  'Zhou_Houqiang_Li',\n",
       "  'Tian_Vedaldi',\n",
       "  'Xe_Zhirong',\n",
       "  'Wu_Yuanjun',\n",
       "  'Stella_Yu_Dahua_Lin',\n",
       "  'Ben_Poole_Sherjil_Ozair',\n",
       "  'Alex_Alemi',\n",
       "  'George_Tucker',\n",
       "  'Xe_Mation',\n",
       "  'Bin_Xiao_Haiping',\n",
       "  'Wu_Yichen_Wei',\n",
       "  'Charles_Qi_Hao_Su_Kaichun_Mo_Leonidas_Guibas',\n",
       "  'Xe_Fu',\n",
       "  'Boshen_Zhang_Yang_Xiao_Zhiguo_Cao_Taidong',\n",
       "  'Yu_Joey',\n",
       "  'Zhou_Junsong_Yuan',\n",
       "  'Aj_Anchor',\n",
       "  'Charles_Ruizhongtai_Qi_Li',\n",
       "  'Yi_Hao_Su_Leonidas',\n",
       "  'Alec_Radford',\n",
       "  'Jong_Wook_Kim',\n",
       "  'Chris_Hallacy',\n",
       "  'Sijie_Yan',\n",
       "  'Xiong_Dahua_Lin',\n",
       "  'Long_Zhao_Xi_Peng_Yu',\n",
       "  'Tian',\n",
       "  'Mubbasir_Kapadia_Dim',\n",
       "  'Metaxas',\n",
       "  'Depth_Human_Parsing',\n",
       "  'Hobbitlong_Pycontrast_Segmentation',\n",
       "  'Tab',\n",
       "  'Xdepth_Xe_Source',\n",
       "  'Xe',\n",
       "  'Scratch_Xe',\n",
       "  'Xno_Contrastive',\n",
       "  'Xe_Xcmc',\n",
       "  'Xe_Xb',\n",
       "  'Rgb_Hu',\n",
       "  'Nof_Depth',\n",
       "  'Xe_Xno',\n",
       "  'Xdepth_Xe',\n",
       "  'Xe_Xe',\n",
       "  'Fig',\n",
       "  'Fig_Fig',\n",
       "  'Xe_Xa',\n",
       "  'Xe_Xsample',\n",
       "  'Xe_Soft_Dense',\n",
       "  'Ours_Nturgbd_Mpii',\n",
       "  'Ours',\n",
       "  'Contrastive_Depth',\n",
       "  'Ours_Depth',\n",
       "  'Ours_Only_Depth'],\n",
       " ['Yasui_Ptep',\n",
       "  'Guo',\n",
       "  'Hanhart_Mei',\n",
       "  'Zhao_Zou_Rev',\n",
       "  'Nefediev_Shen_Thomas',\n",
       "  'Al_Ptep',\n",
       "  'Belle_Choi',\n",
       "  'Lett',\n",
       "  'Ballot_Richard_Phys',\n",
       "  'Silvestre_Brac',\n",
       "  'Gignoux_Richard',\n",
       "  'Bignamini',\n",
       "  'Piccinini_Polosa',\n",
       "  'Cho_Et_Al',\n",
       "  'Furumoto',\n",
       "  'Phys_Morita',\n",
       "  'Rev_Hatsuda',\n",
       "  'Haidenbauer_Nucl'],\n",
       " ['Zisserman_Nella',\n",
       "  'Ba',\n",
       "  'Kiros',\n",
       "  'Hinton_Layer',\n",
       "  'Brock_Donahue',\n",
       "  'Cao_Leng_Lischinski_Cohen_Or',\n",
       "  'Tu_Li',\n",
       "  'Shapeconv_Shape',\n",
       "  'Chandran_Zoss',\n",
       "  'Gotardo_Gross_Bradley',\n",
       "  'Xe_June',\n",
       "  'Chen_Koltun',\n",
       "  'Cook',\n",
       "  'Ramos_Rehfeld',\n",
       "  'Benenson_Franke_Roth_Schiele',\n",
       "  'Dosovitskiy_Beyer',\n",
       "  'Zhai_Unterthiner_Dehghani',\n",
       "  'Uszkoreit_Houlsby',\n",
       "  'Dumoulin_Perez_Schucher_Strub_Vries',\n",
       "  'Courville_Bengio',\n",
       "  'Distill',\n",
       "  'Liii',\n",
       "  'Gatys',\n",
       "  'Ecker',\n",
       "  'Godard_Mac_Aodha_Brostow_Unsupervised',\n",
       "  'Abadie_Mirza',\n",
       "  'Xu_Warde',\n",
       "  'Hendrycks_Gimpel',\n",
       "  'Heusel_Ramsauer_Unterthiner_Nessler_Hochreiter',\n",
       "  'Huang_Qin',\n",
       "  'Zhou_Zhu_Liu_Shao',\n",
       "  'Image_Generation',\n",
       "  'Huang_Belongie',\n",
       "  'Isola_Zhu',\n",
       "  'Zhou_Efros_Image',\n",
       "  'Jing_Liu_Ding_Wang_Ding_Song_Wen',\n",
       "  'Johnson_Alahi_Fei_Fei',\n",
       "  'Sebe_Welling',\n",
       "  'Kato_Beker',\n",
       "  'Morariu_Ando',\n",
       "  'Kehl_Gaidon',\n",
       "  'Kingma',\n",
       "  'Laine_Hellsten_Karras',\n",
       "  'Seol_Lehtinen_Aila',\n",
       "  'Li',\n",
       "  'Aittala_Durand_Lehtinen',\n",
       "  'Lin_Wang_Liu',\n",
       "  'Liu_Li_Chen_Li',\n",
       "  'Soft',\n",
       "  'Ma_Jia_Sun',\n",
       "  'Mirza_Osindero',\n",
       "  'Nathan_Silberman_Derek_Hoiem',\n",
       "  'David_Speierer_Ruiz_Jakob',\n",
       "  'David_Vicini_Zeltner_Jakob',\n",
       "  'Park_Liu',\n",
       "  'Zhu_Semantic',\n",
       "  'Ghorbani_Bolkart_Osman',\n",
       "  'Xe_Xb',\n",
       "  'Pani_Paudel',\n",
       "  'Perez_Strub_De_Vries_Dumoulin_Courville',\n",
       "  'Popovic_Paudel',\n",
       "  'Compositetask',\n",
       "  'Qi_Chen_Jia_Koltun',\n",
       "  'Gordon_Lo',\n",
       "  'Johnson_Gkioxari',\n",
       "  'Reddy',\n",
       "  'Ronneberger_Fischer_Brox',\n",
       "  'Rott_Shaham_Gharbi',\n",
       "  'Shechtman_Michaeli',\n",
       "  'Sitzmann_Zollho',\n",
       "  'Xfer_Wetzstein',\n",
       "  'Continuous',\n",
       "  'Sun_Probst_Paudel',\n",
       "  'Popovic_Kanakis',\n",
       "  'Tewari_Fried_Thies',\n",
       "  'Martin_Brualla_Simon_Saragih_Nie',\n",
       "  'Zhu',\n",
       "  'Theobalt_Agrawala_Shechtman_Goldman',\n",
       "  'Tewari_Zollo',\n",
       "  'Bernard_Garrido',\n",
       "  'Kim_Perez_Theobalt',\n",
       "  'Kim_Garrido',\n",
       "  'Bernard_Perez_Christian',\n",
       "  'Vaswani_Shazeer',\n",
       "  'Parmar_Uszkoreit_Jones_Gomez',\n",
       "  'Wang_Wang_Zhang',\n",
       "  'Owens',\n",
       "  'Efros_Cnn',\n",
       "  'Liu',\n",
       "  'Liu_Tao_Kautz_Catanzaro',\n",
       "  'Tao_Kautz_Catanzaro',\n",
       "  'Wang_Qinami_Karakozi',\n",
       "  'Xia_Yang_Xue_Wu',\n",
       "  'Yu_Koltun_Funkhouser',\n",
       "  'Yu_Smith',\n",
       "  'Inverserendernet_Learning',\n",
       "  'Yuan_Chen_Wang_Yu_Shi_Tay',\n",
       "  'Feng_Yan',\n",
       "  'Zamir',\n",
       "  'Sax_Shen',\n",
       "  'Malik_Savarese',\n",
       "  'Zhang_Ma',\n",
       "  'Li_Ding_Tang_Zhou_Yang',\n",
       "  'Zhu_Abdal_Qin_Wonka',\n",
       "  'Sean_Image'],\n",
       " ['Wu',\n",
       "  'Hanson_Johansson',\n",
       "  'Xe_Xcstanford',\n",
       "  'Draganjac',\n",
       "  'Saponara_Xe_Xccogging',\n",
       "  'Kro',\n",
       "  'Xe_Xcswarm',\n",
       "  'Ed',\n",
       "  'Wheeler_Ng',\n",
       "  'Robot_Operating',\n",
       "  'Xe_Xcmulti',\n",
       "  'Glotfelter',\n",
       "  'Pardo_Castellote',\n",
       "  'Egerstedt_Xe',\n",
       "  'Gokhale',\n",
       "  'Schmidt',\n",
       "  'Alonso_Mora',\n",
       "  'Cap_Castellote',\n",
       "  'Chen',\n",
       "  'Sanchez_Monedero',\n",
       "  'Molina_Lopez_Vega',\n",
       "  'Xe_Xcbloom',\n",
       "  'Follert',\n",
       "  'Xae',\n",
       "  'Kumari_Perkins',\n",
       "  'Xe_Xcrfc',\n",
       "  'Williams',\n",
       "  'Html',\n",
       "  'Xoz_Salinas',\n",
       "  'Xe_Computing',\n",
       "  'Merkel_Xe_Xcdocker',\n",
       "  'Xe_Xcare',\n",
       "  'Sooriyabandara_Xe_Xcumbrella',\n",
       "  'Com_Springer',\n",
       "  'Diaz_Mercado',\n",
       "  'Egerstedt_Xe_Xcsafe',\n",
       "  'Xe_Eder',\n",
       "  'Huang',\n",
       "  'Jones',\n",
       "  'Hauert_Xe_Xcdistribute',\n",
       "  'Jones_Winfield',\n",
       "  'Floreano_Gambardella',\n",
       "  'Ogren_Xe',\n",
       "  'Klo',\n",
       "  'Hauert_Winfield',\n",
       "  'Gro_Xc_Xf']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All function in one\n",
    "\n",
    "entities=[]\n",
    "def main():\n",
    "    for text in elements:\n",
    "        temp=after_references(str(text))\n",
    "        temp=preprocess_text(temp)\n",
    "        temp=extract(temp)\n",
    "        entities.append(temp)\n",
    "    return entities\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a35d1365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897bff1",
   "metadata": {},
   "source": [
    "## Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "164621c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready import *\n",
    "\n",
    "onto_path.append(\"owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8470daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Creating new ontology onto <http://test.org/onto.owl>.\n"
     ]
    }
   ],
   "source": [
    "onto = Ontology(\"http://test.org/onto.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5aad8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class References(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class quoted_by(Property):\n",
    "    ontolgy = onto\n",
    "    domain = [References]\n",
    "    range = [Author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd4f26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.sax.saxutils import escape\n",
    "\n",
    "def invalid_xml_remove(char):\n",
    "    \"\"\"Tracks illegal unicode characters\"\"\"\n",
    "    #http://stackoverflow.com/questions/1707890\n",
    "    # /fast-way-to-filter-illegal-xml-unicode-chars-in-python\n",
    "    illegal_unichrs = [ (0x00, 0x08), (0x0B, 0x1F), (0x7F, 0x84), (0x86, 0x9F),\n",
    "                    (0xD800, 0xDFFF), (0xFDD0, 0xFDDF), (0xFFFE, 0xFFFF),\n",
    "                    (0x1FFFE, 0x1FFFF), (0x2FFFE, 0x2FFFF), (0x3FFFE, 0x3FFFF),\n",
    "                    (0x4FFFE, 0x4FFFF), (0x5FFFE, 0x5FFFF), (0x6FFFE, 0x6FFFF),\n",
    "                    (0x7FFFE, 0x7FFFF), (0x8FFFE, 0x8FFFF), (0x9FFFE, 0x9FFFF),\n",
    "                    (0xAFFFE, 0xAFFFF), (0xBFFFE, 0xBFFFF), (0xCFFFE, 0xCFFFF),\n",
    "                    (0xDFFFE, 0xDFFFF), (0xEFFFE, 0xEFFFF), (0xFFFFE, 0xFFFFF),\n",
    "                    (0x10FFFE, 0x10FFFF) ]\n",
    "\n",
    "    illegal_ranges = [f\"{chr(low)}-{chr(high)}\"\n",
    "                  for (low, high) in illegal_unichrs\n",
    "                  if low < sys.maxunicode]\n",
    "\n",
    "    illegal_xml_re = re.compile(f'[{\"\".join(illegal_ranges)}]')\n",
    "    if illegal_xml_re.search(char) is not None:\n",
    "        #Replace with space\n",
    "        return ''\n",
    "    else:\n",
    "        return char\n",
    "\n",
    "def clean_char(char):\n",
    "    \"\"\"\n",
    "    Function for remove invalid XML characters from\n",
    "    incoming data.\n",
    "    \"\"\"\n",
    "    #Get rid of the ctrl characters first.\n",
    "    #http://stackoverflow.com/questions/1833873/python-regex-escape-characters\n",
    "    char = re.sub('\\x1b[^m]*m', '', char)\n",
    "    #Clean up invalid xml\n",
    "    char = invalid_xml_remove(char)\n",
    "    replacements = [\n",
    "        ('\\u201c', '\\\"'),\n",
    "        ('\\u0141', '\\\"'),\n",
    "        ('\\u201d', '\\\"'),\n",
    "        (\"\\u001B\", ''), #http://www.fileformat.info/info/unicode/char/1b/index.htm\n",
    "        (\"\\u0019\", ''), #http://www.fileformat.info/info/unicode/char/19/index.htm\n",
    "        (\"\\u0016\", ''), #http://www.fileformat.info/info/unicode/char/16/index.htm\n",
    "        (\"\\u001C\", ''), #http://www.fileformat.info/info/unicode/char/1c/index.htm\n",
    "        (\"\\u0003\", ''), #http://www.utf8-chartable.de/unicode-utf8-table.pl?utf8=0x\n",
    "        (\"\\u000C\", ''),\n",
    "        (\"\\u03b1\", ''),\n",
    "        (\"u\\u039C\", ''),\n",
    "        (\"\\u03C3\", ''),\n",
    "        (\"\\u0141\", ''),\n",
    "        (\"\\u0308\", ''),\n",
    "        (\"\\u2032\", ''),\n",
    "        (\"\\u03b8\", '')\n",
    "\n",
    "    \n",
    "    ]\n",
    "    for rep, new_char in replacements:\n",
    "        if char == rep:\n",
    "            return new_char\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4794f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_value(text: str):\n",
    "        \"\"\"Escape the illegal characters for an ontology property\"\"\"\n",
    "        if text is None:\n",
    "            return None\n",
    "        # function to escape XML character data\n",
    "        text = escape(text)\n",
    "        text = text.replace('\\n', '')\n",
    "        text = text.replace('\\r', '')\n",
    "        text = text.replace('\\f', '')\n",
    "        text = text.replace('\\b', '')\n",
    "        text = text.replace('\"', '')\n",
    "        text = text.replace('[', '')\n",
    "        text = text.replace(']', '')\n",
    "        text = text.replace('{', '')\n",
    "        text = text.replace('}', '')\n",
    "        text = text.replace('#', '')\n",
    "        text = text.replace('|', '')\n",
    "        text = text.replace(' ', '_')\n",
    "#         text = text.replace('σ', '')\n",
    "#         text = text.replace('ℓ', '')\n",
    "#         text = text.replace('Σ', '')\n",
    "#         text = text.replace('ı', '')\n",
    "#         text = text.replace('ē', '')\n",
    "#         text = text.replace('μ', '')\n",
    "#         text = text.replace('Μ', '')\n",
    "#         text = text.replace('Ł', '')\n",
    "#         text = text.replace('ł', '')\n",
    "#         text = text.replace('θ', '')\n",
    "#         text = text.replace('φ', '')\n",
    "#         text = text.replace('Φ', '')\n",
    "#         text = text.replace('̈', '')\n",
    "#         text = text.replace('́', '')\n",
    "#         text = text.replace('′', '')\n",
    "#         text = text.replace('∈', '')\n",
    "#         text = text.replace('ć', '')\n",
    "#         text = text.replace('ź', '')\n",
    "        text = clean_char(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2126e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Ziqian.Bai]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Timur.Bagautdinov]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Javier.Romero]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Michael.Zollh.fer]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Ping.Tan]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Shunsuke.Saito]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Fangzhou.Hong]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Liang.Pan]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Zhongang.Cai]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Ziwei.Liu]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Yuki.Kamiya]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Tetsuo.Hyodo]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Akira.Ohnishi]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Ritika.Chakraborty]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Nikola.Popovic]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Thomas.Probst]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Simon.Jones]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Emma.Milner]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Mahesh.Sooriyabandara]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n",
      "[onto.Sabine.Hauert]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(array_authors)):\n",
    "    for j in range(len(array_authors[i])):\n",
    "        aut_num = Author(escape_value(array_authors[i][j]))\n",
    "        for z in range(len(entities[i])):\n",
    "            ref_num = References(escape_value(entities[i][z]))\n",
    "            ref_num.quoted_by.append(aut_num)\n",
    "            print(ref_num.quoted_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e27e8710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Saving ontology onto to owl\\onto.owl...\n"
     ]
    }
   ],
   "source": [
    "onto.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472064be",
   "metadata": {},
   "source": [
    "### To conlude, we will change the xml encoding to UTF-8 otherwise, Protégé may not read the file ont.owl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
