{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126a7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfminer.six\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install nameparser\n",
    "# !pip install pdfminer\n",
    "# !pip install refextract\n",
    "# !pip install pdfx\n",
    "# !pip install -U textblob\n",
    "# !pip install owlready\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2ac646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "* Owlready * Creating new ontology owl <http://www.w3.org/2002/07/owl>.\n",
      "* Owlready * Creating new ontology 22-rdf-syntax-ns <http://www.w3.org/1999/02/22-rdf-syntax-ns>.\n",
      "* Owlready * Creating new ontology rdf-schema <http://www.w3.org/2000/01/rdf-schema>.\n",
      "* Owlready * Creating new ontology XMLSchema <http://www.w3.org/2001/XMLSchema>.\n",
      "* Owlready * Creating new ontology anonymous <http://anonymous>.\n",
      "* Owlready * Creating new ontology owlready_ontology <http://www.lesfleursdunormal.fr/static/_downloads/owlready_ontology.owl>.\n",
      "* Owlready *     ...loading ontology owlready_ontology from C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\owlready\\owlready_ontology.owl...\n"
     ]
    }
   ],
   "source": [
    "#Import of librairies\n",
    "\n",
    "#ARVIX\n",
    "import arxiv\n",
    "\n",
    "#PDFTEXT\n",
    "import urllib.request\n",
    "# import pdftotext\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "\n",
    "# Prevent future/deprecation warnings from showing in output\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#Local\n",
    "import os\n",
    "import requests as r\n",
    "import sys\n",
    "\n",
    "#AI\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk.tag.stanford as st\n",
    "from nltk.tag.stanford import StanfordNERTagger \n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "import boto3\n",
    "\n",
    "from owlready import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf6f98",
   "metadata": {},
   "source": [
    "## Download pdf file from Arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63be17ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2204.00616v1\n",
      "2204.00615v1\n",
      "2204.00613v1\n",
      "2204.00607v1\n",
      "2204.00604v1\n"
     ]
    }
   ],
   "source": [
    "#You can choose here how many pdf you want. Here 5 because it is enough to see the result\n",
    "max_results=5\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = \"Computer Science & AI\",\n",
    "    max_results = max_results,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "for result in search.results():\n",
    "    print(result.pdf_url[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b3ba48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link:  ['http://arxiv.org/pdf/2204.00616v1', 'http://arxiv.org/pdf/2204.00615v1', 'http://arxiv.org/pdf/2204.00613v1', 'http://arxiv.org/pdf/2204.00607v1', 'http://arxiv.org/pdf/2204.00604v1']\n",
      "Authors:  [['Samuel.Lavoie', 'Christos.Tsirigotis', 'Max.Schwarzer', 'Kenji.Kawaguchi', 'Ankit.Vani', 'Aaron.Courville'], ['Pakawut.Jiradilok'], ['Xiao.Wang', 'Haoqi.Fan', 'Yuandong.Tian', 'Daisuke.Kihara', 'Xinlei.Chen'], ['Bernhard.Sch.lkopf', 'Julius.von.K.gelgen'], ['Ye.Zhu', 'Kyle.Olszewski', 'Yu.Wu', 'Panos.Achlioptas', 'Menglei.Chai', 'Yan.Yan', 'Sergey.Tulyakov']]\n",
      "Title:  ['Simplicial Embeddings in Self-Supervised Learning and Downstream Classification', 'Large-scale Rook Placements', 'On the Importance of Asymmetry for Siamese Representation Learning', 'From Statistical to Causal Learning', 'Quantized GAN for Complex Music Generation from Dance Videos']\n"
     ]
    }
   ],
   "source": [
    "#Store in array all informations on pdf (title, authors and links)\n",
    "\n",
    "array_link=[]\n",
    "array_authors=[]\n",
    "array_title=[]\n",
    "\n",
    "for result in search.results():\n",
    "    array_link.append(result.pdf_url)\n",
    "    array_title.append(result.title)\n",
    "    temp=result.authors\n",
    "    array_authors.append([re.sub(\"[^A-Za-z0-9]\",\".\",str(i)) for i in temp])\n",
    "    \n",
    "    #Download pdf as link.pdf\n",
    "    result.download_pdf(dirpath=\".\\Download\", filename=f'{result.pdf_url[21:]}.pdf')\n",
    "    \n",
    "print(\"Link: \",array_link)\n",
    "print(\"Authors: \",array_authors)\n",
    "print(\"Title: \",array_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a5ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(value):\n",
    "    with open(f'Download/{value}.pdf', \"rb\") as f:\n",
    "        text = extract_text(f)\n",
    "        return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c06a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n6\\n1\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nSimplicial Embeddings in Self-Supervised Learning\\nand Downstream Classiﬁcation\\n\\nSamuel Lavoie(cid:5)†, Christos Tsirigotis(cid:5)†, Max Schwarzer(cid:5)†, Kenji Kawaguchi‡, Ankit Vani(cid:5)†,\\nAaron Courville(cid:5)†♣\\n(cid:5) Mila, † Université de Montréal, ‡ National University of Singapore, ♣ CIFAR Fellow\\n{samuel.lavoie.m,aaron.courville}@gmail.com\\n{christos.tsirigotis,max.schwarzer,ankit.vani}@umontreal.ca\\nkenji@comp.nus.edu.sg\\n\\nAbstract\\n\\nWe introduce Simplicial Embeddings (SEMs) as a way to constrain the encoded\\nrepresentations of a self-supervised model to L simplices of V dimensions each\\nusing a Softmax operation. This procedure imposes a structure on the represen-\\ntations that reduce their expressivity for training downstream classiﬁers, which\\nhelps them generalize better. Speciﬁcally, we show that the temperature τ of the\\nSoftmax operation controls for the SEM representation’s expressivity, allowing us\\nto derive a tighter downstream classiﬁer generalization bound than that for classi-\\nﬁers using unnormalized representations. We empirically demonstrate that SEMs\\nconsiderably improve generalization on natural image datasets such as CIFAR-100\\nand ImageNet. Finally, we also present evidence of the emergence of semantically\\nrelevant features in SEMs, a pattern that is absent from baseline self-supervised\\nmodels.\\n\\n1\\n\\nIntroduction\\n\\nSelf-supervised learning (SSL) is an emerging family of methods that aims to learn an embedding of\\nthe data without manual supervision, such as class labels. Those methods embed the data in some\\nrepresentations that render themselves amenable to ﬁtting a linear classiﬁer, as demonstrated in Hjelm\\net al. [2019]; Grill et al. [2020]; Saeed et al. [2020]; You et al. [2020]. This observation demonstrates\\nthat the representation learned by those SSL methods encodes the semantic content necessary to learn\\na classiﬁer as a linear combination of the features.\\n\\nIn this work, we propose to embed the latent representation of the data into L simplices of V\\ndimensions each by using a Softmax operation. We refer to the normalized embeddings as Simplicial\\nEmbeddings (SEMs) due to the geometrical structure of the representation induced by the Softmax.\\nThe SEMs have an effect both while training the representation and on the training of the downstream\\nclassiﬁer. For the former, the SEM is an inductive bias to ﬁt the data in a more constrained space\\nthat may lead to a simpler representation. For the latter, the Softmax allows us to control for the\\nexpressivity of the representation. This control gives us a better generalization bound for training\\ndownstream classiﬁers.\\n\\nWe demonstrate that the proposed SEMs improve the generalization of downstream classiﬁers trained\\nwith BYOL [Grill et al., 2020] and MoCo [He et al., 2020] on CIFAR-100 and ImageNet. We also\\nshow an improvement in transfer learning and robustness to out-of-distribution datasets. Finally,\\nwe present evidence that individual features of the SEMs encode semantical content related to\\nour intuitive notion of the semantics in CIFAR-100. In contrast, we argue that the baseline SSL\\nmethods may learn the semantics related to the classes as a linear combination of the features in the\\nrepresentation but not at the individual features’ level.\\n\\nPreprint. Under review.\\n\\n\\x0cConcretely, this work makes the following contributions:\\n\\n1. Propose the Simplicial Embeddings.\\n\\n2. Derive a generalization bound for downstream classiﬁers trained on the Simplicial Embed-\\n\\n3. Empirically studies the Simplicial Embeddings and its effect on the generalization of\\n\\ndings.\\n\\ndownstream classiﬁers.\\n\\n1.1 Related works\\n\\nThe use of Softmax as an inductive bias has been studied in other contexts, notably as an architectural\\ncomponent for models to attend to context-dependent queries via, for example, attention mecha-\\nnisms [Bahdanau et al., 2016; Vaswani et al., 2017] or memory augmented networks [Graves et al.,\\n2014]. Different from these, our method places the Softmax at the output of an encoder to constrain\\nthe representation and to allow control of the expressivity of the representation for downstream\\nclassiﬁers.\\n\\nOur work builds on top of the literature on self-supervised learning. Notably, we demonstrate the effect\\nof the SEM on contrastive approaches using the noise contrastive estimation (NCE) objective [Hjelm\\net al., 2019; Chen et al., 2020b] with memory banks [He et al., 2020] and on the bootstrapping\\napproaches [Grill et al., 2020; Chen and He, 2020]. Related, some works explicitly induce clustering\\nof the representation [Caron et al., 2019; Ym et al., 2019; Caron et al., 2020]. Contrary to these\\nworks, we do not explicitly induce clustering on the representation.\\n\\nIn the realm of improving the generalization of SSL methods, Wang et al. [2021] propose a method\\nto iteratively select a partition of the data and use this partition to minimize an IRM regularizer\\n[Arjovsky et al., 2020] with an SSL objective. Lee et al. [2021] present an objective to minimize\\nthe conditional entropy bottleneck. Contrary to these works, our methods do not require additional\\nobjectives as it is merely an inductive bias in the SSL models.\\n\\n2 Background on self-supervised learning\\n\\nModels trained with a contrastive objective learn to embed samples x ∈ X into representations\\nz ∈ Z, where Z is a bounded metric space. The aim is to both minimize the distance between\\nthe representation of a sample zi = fθ(xi) : x ∈ X and the representation of a positive sample\\nzj = fθ(xj), and to maximize the distance between zi and the representation of negative samples\\nfθ(x(cid:48)) : x(cid:48) ∈ X \\\\ xi. While the positive samples are typically augmented samples of xi, other\\nstrategies can be decided, such as choosing samples from the same labelled category [Khosla et al.,\\n2020]. A common contrastive objective is Noise Contrastive Estimation (NCE) [Hjelm et al., 2019;\\nChen et al., 2020b], which is deﬁned as\\n\\nLnce := − log\\n\\nexp(d(zi, zj)/t)\\n¯x∈X \\\\x exp(d(zi, ¯z)/t)\\n\\n,\\n\\n(cid:80)\\n\\n(1)\\n\\nwhere d is often taken to be the cosine similarity: d(x, y) := x(cid:62)y/(cid:107)x(cid:107)2(cid:107)y(cid:107)2 and t > 0 is a\\nhyper-parameter that denotes a temperature.\\n\\nUnlike most contrastive methods, BYOL [Grill et al., 2020] does not require negative samples.\\nInstead, it introduces a target network in which the parameters ξ are taken as an exponential moving\\naverage of the embedding function parameters, θ. More precisely, ξ ← αξ + (1 − α)θ, with\\nα ∈ [0, 1]. The authors deﬁne the anchor and positive samples as zθ = fθ(t(x)) and zξ = fξ(t(cid:48)((x))\\nrespectively, where t, t(cid:48) ∼ T are augmentations sampled from a set of possible augmentations deﬁned\\nby the practitioner. To prevent degenerate solutions, they re-normalize the representation using batch\\nnormalization [Ioffe and Szegedy, 2015], and utilize a stop-gradient operation on zξ that prevents the\\ngradient from back-propagating through the target network. They also introduce a prediction head\\nthat maps the representation to a prediction: zθ (cid:55)→ qθ. The BYOL objective is deﬁned as\\n\\nLbyol := 2 − 2 · d(qθ, zξ),\\n\\n(2)\\n\\nwhere d is chosen to be the cosine similarity.\\n\\n2\\n\\n\\x0c(a)\\n\\n(b)\\n\\nFigure 1: (a) Illustration of the proposed Simplicial Embeddings (SEM). στ represents the Softmax\\noperation with τ . We assume that z decomposes into L vectors in RV . (b) Histogram of the entropies\\nH(¯z(x)\\n) at the end of the pre-training phase, for a given temperature τ , of each simplex for each\\ni\\ntraining sample in CIFAR-100. (c) Integration of the SEM with BYOL [Grill et al., 2020].\\n\\n3 Simplcial Embeddings\\n\\nWe illustrate the proposed Simplicial Embeddings (SEMs) in Figure 1a. An encoder embeds a sample\\nx into a L × V representation z. A temperature parameter τ then scales the logits z ∈ RL×V before\\nre-normalizing each row via L independent Softmax operations. Then, the normalized vectors are\\nconcatenated to produce ¯z ∈ RLV . Concretely, the logits are re-normalized as follows:\\n\\n¯zi := [στ (zi1),\\n\\n. . . , στ (ziV )], στ (zij) =\\n\\n, ¯z := Concat(¯z1, . . . , ¯zL),\\n\\n(3)\\n\\nezij /τ\\nk=1 ezik/τ\\n\\n(cid:80)V\\n\\nfor all i ∈ [L] and j ∈ [V ].\\n\\nThe SEMs can be integrated easily into a NCE model [Hjelm et al., 2019; Chen et al., 2020b] or\\nBYOL [Grill et al., 2020]. We insert it after the encoder and before the projector in our experiments.\\nFigure 1c depicts how we use the SEMs in BYOL. The embedding ¯z is passed into the projector\\nmodule, which we deﬁne as a linear layer or a small MLP. Beyond this small modiﬁcation, the SSL\\nmethod considered remains unchanged.\\n\\n3.1\\n\\nInductive bias of the SEMs\\n\\ni\\n\\nj=1 p(¯z(x)\\n\\n) where (cid:80)V\\n\\nij ) = 1 and p(¯z(x)\\n\\nij ) ≥ 0 ∀j. Here, ¯z(x)\\n\\nWe now describe at a high level the inductive bias of the SEMs during the self-supervised learning\\nphase. We note that each simplex can be interpreted as representing a probability mass function\\np(¯z(x)\\nrepresents the simplex i for a\\ni\\nsample x. The simplex puts a constrain on how the its elements may organize: they may interpolate\\nbetween being a sparse vector and being a constant vector. The state of a simplex can be quantiﬁed\\n) that we denote as follows: H(¯zi) := − (cid:80)V\\nusing the entropy of p(¯z(x)\\nij ). That\\nis, if H(¯z(x)\\nWhile we may argue that the temperature parameter τ , which merely induces a scaling of the logit,\\nmay be subsumed during training, we demonstrate in Figure 1b that this temperature is an important\\ninitial condition for determining the state to which the simplex will converge. Here, we plot the\\nhistogram of the entropies H(¯z(x)\\n), for a given τ , of each simplex for each sample x in the training\\nset of CIFAR-100. The temperature parameter dictates in which state the representation will converge:\\na small τ will induce a sparse representation, and a large τ will induce a constant representation.\\n\\nj=1 p(¯z(x)\\n) = log(V ) then the vector is constant.\\n\\n) = 0 then the vector is sparse and if H(¯z(x)\\n\\nij ) log p(¯z(x)\\n\\ni\\n\\ni\\n\\ni\\n\\ni\\n\\n(c)\\n\\n3\\n\\nEncoderConcat(0.3, 0.3, 0.4)(0.1, 0.7, 0.2)(0.1, 0.1, 0.8)SEM......0.00.51.01.52.02.5Entropy H(z(x)i)0123456: 0.01: 0.1: 0.5: 1.0: 10.0EncoderEncoderSEMSEMPredictionProjectionSSL  lossstop gradProjection\\x0cInterestingly, for an intermediate temperature, the distribution of entropies is more spread out, rather\\nthan having the same variance smoothly translated toward the center of the histogram.\\n\\nThe above observation gives an intuition about how the induce a bias of the SEMs on the learned\\nrepresentation during SSL. Besides the qualitative properties of the vectors that the SEM may induce,\\nthis embedding has a particular structure that we may leverage for learning a classiﬁer with a better\\ngeneralization bound. Next, we theoretically present how the SEMs allow for a better generalization\\nof a downstream classiﬁer and derive a generalization bound for the classiﬁers trained on such\\nrepresentation.\\n\\n3.2 Theoretical bound on the downstream classiﬁer\\n\\nIn this subsection, we mathematically analyze the SEM to understand its beneﬁt and the effect of\\nthe hyper-parameter τ . We show that: (1) there is a trade-off between the training loss and the\\ngeneralization gap, which is controlled by the value of τ , and (2) the SEM can improve the base\\nmodel performance when we attain good balance in this trade-off.\\n\\nLet g represent the layer(s) after the normalization. With this notation, we can deﬁne a baseline\\nmodel without normalization as fbase(z) = g(z) and the corresponding model with normalization\\nas fSEM(τ )(z) = (g ◦ στ )(z). We consider a training dataset S = (zi, yi)n\\ni=1 of n samples that is\\nused for supervised training of a classiﬁer using the representations z, which are extracted from the\\nself-supervised encoder. To understand the quality of the ﬁnal model after supervised training of\\nthe classiﬁer, we analyze the generalization gap Ez,y[l(f (z), y)] − 1\\ni=1 l(f (z(i)), y(i)) for each\\nn\\nbase}, where l : R × Y → R≥0 is the per-sample loss.\\nf ∈ {f S\\n\\nSEM(τ ), f S\\n\\n(cid:80)n\\n\\nTo simplify the notation, we consider the normalization to [−1, +1]; i.e., z ∈ Z = [−1, +1]L×V .\\nWe assume that there exists ∆ > 0 such that for any i ∈ [L], if k = arg maxj∈[V ] zij, then\\nzik ≥ zij + ∆ for any j (cid:54)= k. Since ∆ can be arbitrarily small (e.g., much smaller than machine\\nprecision), this assumption typically holds in practice. Next, we deﬁne B to be the upper bound\\non the per-sample loss such that l(f (z), y) ≤ B for all f ∈ H and for all (z, y) ∈ Z × Y,\\nwhere H is the union of the hypothesis spaces of fSEM(τ ) and fbase. For example, B = 1\\nfor the 0-1 loss. We also use Qi = {q ∈ [−1, +1]V : i = arg maxj∈[V ] qj}, with the fol-\\nlowing two deﬁnitions: ϕ(f S\\n2, and ϕ(f S\\nSEM(τ )) =\\nsupi∈[V ] supq,q(cid:48)∈Qi\\nt=1 eqt/τ for j = 1, . . . , V . Next,\\nwe deﬁne GS to be the set of g returned by the training algorithm using dataset S, and R to be the\\nLipschitz constant of ly ◦ g for all y ∈ Y and g ∈ GS; i.e., |(ly ◦ g)(z) − (ly ◦ g)(z(cid:48))| ≤ R(cid:107)z − z(cid:48)(cid:107)F ,\\nwhere ly(q) = l(q, y). Finally, let c > 0 be a universal constant in (n, f, H, δ, H, τ, S).\\n\\n2, where στ (q)j = eqj /τ\\n\\nbase) = supi∈[V ] supq,q(cid:48)∈Qi\\n\\nt=1 (cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n\\nt=1 (cid:107)q − q(cid:48)(cid:107)2\\n\\n(cid:80)n\\n\\n(cid:80)n\\n\\n(cid:80)V\\n\\nUsing the established notation, Theorem 1 illuminates the advantage of the SEM and the effect of the\\nhyper-parameter τ on the performance of the downstream classiﬁer:\\nTheorem 1. Let V ≥ 2. For any δ > 0, with probability at least 1 − δ, the following holds for any\\nfS ∈ {f S\\n\\nSEM(τ ), f S\\n\\nbase}:\\n\\nEz,y[l(fS(z), y)] ≤\\n\\nl(fS(z(i)), y(i)) + R\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:114)\\n\\n(cid:114)\\n\\nLϕ(fS)\\nn\\n\\n+ c\\n\\nln(2/δ)\\nn\\n\\n.\\n\\nMoreover,\\n\\nϕ(f S\\n\\nSEM(τ )) → 0 as τ → 0\\n\\nand ϕ(f S\\n\\nSEM(τ )) − ϕ(f S\\n\\nbase) ≤\\n\\n(1 − V ) < 0 ∀τ > 0.\\n\\n3n\\n4\\n\\nThe ﬁrst statement of Theorem 1 shows that the expected loss is bounded by the three terms: training\\nloss 1\\nn\\n\\ni=1 l(fS(z(i)), y(i)), the second term R\\n\\n(cid:113) Lϕ(fS )\\nn\\n\\n(cid:113) ln(2/δ)\\nn\\n\\n. Since c is\\n\\n(cid:80)n\\n\\na universal constant in (n, f, H, δ, H, τ, S), the third term c\\ngoes to zero as n → ∞ and is\\nthe same for both models with and without soft-discretization. Thus, for the purpose of comparing the\\nmodels with and without soft-discretization, we can focus on the second term, where the difference\\narises.\\n\\n, and the third term c\\n(cid:113) ln(2/δ)\\nn\\n\\nTheorem 1 shows that the second term R\\nSEM(τ )) →\\n0 as τ → 0. Also, for any τ > 0, the second term with soft-discretization is strictly smaller than\\n\\ngoes to zero with the SEM; i.e., ϕ(f S\\n\\n(cid:113) Lϕ(fS )\\nn\\n\\n4\\n\\n\\x0cthat without soft-discretization as ϕ(f S\\nimprovement due to soft-discretization is expected to be higher as V increases.\\n\\nSEM(τ )) − ϕ(f S\\n\\nbase) ≤ 3n\\n\\n4 (1 − V ) < 0. This shows that the\\n\\nOverall, Theorem 1 shows the beneﬁt of the SEM as well as the trade-off with τ . When τ → 0, the\\nsecond term goes to zero, but the training loss (the ﬁrst term) can increase due to the reduction in\\nexpressivity and increased difﬁculty in optimization. Thus, we assert that the best τ is the one that\\nbalances this trade-off.\\n\\n4 Experiments\\n\\nWe study the effect of the Simplicial Embeddings on the generalization of self-supervised learn-\\ning methods1. We demonstrate that the Simplicial Embeddings improve the test set accuracy on\\nCIFAR-100 and ImageNet. On CIFAR-100, we also study the different properties of the SEM, and\\nwe demonstrate the emergence of semantic features relevant to the classes in the representation\\nfeature. On ImageNet, we show that the Simplicial Embeddings improve the test accuracy on several\\nrobustness test sets and the accuracy on transfer learning datasets.\\n\\n4.1 The effect of the SEM on downstream classiﬁcation\\n\\nMethod\\nSimCLR†\\nMOCO†\\nSWAV†\\nDINO†\\nBYOL\\nBYOL + SEM\\n\\nAccuracy\\n65.78\\n69.89\\n64.88\\n66.76\\n70.46\\n74.36\\n\\nMethod\\nSimCLR‡\\nBYOL\\nBYOL*\\nBYOL + SEM\\n\\nAccuracy\\n68.73\\n74.28\\n73.33\\n77.05\\n\\nMethod\\nSIMCLR‡\\nMOCO‡\\nMOCO + SEM\\nSIMSIAM(cid:5)\\nBYOL(cid:5)\\nBYOL + SEM\\n\\nAccuracy\\n63.1\\n67.3\\n69.0\\n70.0\\n70.6\\n72.8\\n\\n(a) CIFAR-100 on ResNet18\\n\\n(b) CIFAR-100 on ResNet50\\n\\n(c) ImageNet on ResNet50\\n\\nFigure 2: Accuracy on (a) CIFAR-100 with ResNet18 for 1000 epochs. (b) CIFAR-100 with ResNet50\\nfor 1000 epochs. (c) ImageNet with ResNet50 for 200 epochs. *Denotes the accuracy obtained when\\ntraining BYOL with a representation the same size as SEM. † Results taken from da Costa et al.\\n[2021]. ‡ Results taken from Wang et al. [2021]. (cid:5) Results taken from [Chen et al., 2020b]. Boldface\\nindicates highest accuracy. Green rows indicate a SSL method + SEM.\\n\\nComparison study. We ﬁrst compare the effect of using the SEM in a BYOL model with related\\nSSL approaches in the literature. We take a standard BYOL model, as implemented in the Solo-Learn\\nlibrary [da Costa et al., 2021], and implement the Simplicial Embeddings after the encoder. We test\\nour approach with a ResNet18 and ResNet50 on CIFAR-100 and with a ResNet50 for ImageNet [He\\net al., 2015]. Our models are trained with Stochastic Gradient Descent [Bottou et al., 2018] with a\\ncosine decay scheduler on the learning rate, as done in previous works [Grill et al., 2020; Chen et al.,\\n2020b]. We use a batch size of 256 for all of our models and train on a single A100 GPU. We selected\\nthe parameters of the SEM by performing a grid search over several values using a validation set and\\nre-trained our model using all the training data to evaluate the test set. We did not modify the default\\nhyper-parameters of the method, demonstrating that the gain in accuracy is a product of the SEM. We\\npresent the hyper-parameters used in the Appendix. We evaluate all of our models by training a linear\\nclassiﬁer, using the training data on top of the learned representations as it is typically done.\\n\\nWe compare our approach on CIFAR-100 and ImageNet in Table 2b and Table 2c respectively.\\nCompared with prior models, our approach improves the baseline methods by a considerable margin.\\nOn CIFAR-100, we compare with several baselines, such as DINO and SwaV. We also trained BYOL\\nwith the same representation size as what we used in the SEM, without the embedding, and observed a\\nmarginal performance decrease. As demonstrated in Zbontar et al. [2021], BYOL does not seemingly\\nbeneﬁt from large representations.\\n\\nThe SEM also presents a noticeable improvement compared to the baselines on ImageNet when\\ntrained for 200 epochs. Here, we trained our model on both BYOL and MOCO [He et al., 2020] to\\ndemonstrate that the effect of the SEM is not limited to BYOL.\\n\\n1We provide the code to reproduce the experiments: https://github.com/lavoiems/simplicial-embeddings\\n\\n5\\n\\n\\x0cFigure 3: Study of the effect of the parameters of the SEM. We plot the accuracy obtained for several\\ndownstream classiﬁcation SEM’s temperature (fSEM(τ )) and without SEM (fbase) described in the\\nlegend. We performed the training with a ResNet18 on CIFAR-100. Interpolation of Left: τ during\\nthe training of the SSL model. Middle: V . Right: L\\n\\nStudy of the SEM parameters. We study the effect of each of the parameters of the SEM and\\nevaluate their effect on the validation accuracy in Figure 3. We trained each model with a ResNet18\\non CIFAR-100 using the BYOL training procedure. We keep the other parameters constant to their\\ndefault value for each parameter that we study. The default value of τ is 1, V is 13 and L is 5000. For\\neach pre-trained SSL model, we trained 5 downstream classiﬁers, one on the unnormalized features\\ndenoted fbase and one on the normalized features for τ ∈ {0.01, 0.1, 1, 10}.\\n\\nWe observe that the temperature used to normalized the embedding before training the downstream\\nclassiﬁer, fSEM(τ ), is important for the downstream classiﬁcation and is generally better than training\\na classiﬁer on the unnormalized features (fbase) as predicted in Theorem 1. We observe the trade-off,\\nas presented in Section 3.2, between having a small and a large τ .\\n\\nWe also observe a trade-off between having a large and a small temperature when training the SSL\\nmodel. As demonstrated in Figure 1b, the temperature parameter has an impact on whether the\\nsimplicies will represent a sparse or a constant vector. We demonstrated that a small temperature\\nyields a set of sparse vectors while a large temperature yields a constant vector. Here, we observe\\nthat the temperature yielding the better validation accuracy offers a trade-off between a sparse and a\\nconstant vector. We hypothesize that a sparse vector leads to harder training but a smaller expressivity.\\nThus, the better temperature during the training of the SSL model is the one that offers a trade-off\\nbetween a sparse but trainable representation.\\n\\nIn Theorem 1, we demonstrated theoretically that the second term was more sensitive to the tem-\\nperature as V increased. This prediction is empirically veriﬁed in Figure 3 where we evaluate the\\nvalidation accuracy for several V . As V increases, the validation accuracy drops for larger τ s. For\\nexample, the validation accuracy drops when interpolating between V = 13 and V = 34 for τ = 10,\\nstays constant for τ = 1 and increases for the smaller temperatures.\\n\\nFinally, we interpolate the L parameter and demonstrate that larger L yields increased normalized\\nfeatures’ validation accuracy. As expected, the effect of ϕ(fS) grows with larger L, and thus we\\nwould expect a bigger difference between fbase and fSEM(τ ). This demonstrates empirically and\\ntheoretically that the SEM may scale the representation of SSL methods to a larger representation\\nand thus potentially increasing the scaling capability of these methods.\\n\\n4.2 Emergence of semantically relevant features\\n\\nIn this subsection, we investigate the semantic content held by the most predictive features of an\\nembedding. To make this study, we consider an encoder pretrained on CIFAR-100, using BYOL with\\nand without SEM, and a downstream linear classiﬁer trained on the embedding of the CIFAR-100\\nsamples. Consider the trained linear classiﬁer with a weight matrix W ∈ RN ×C, where N denotes\\nthe number of features, and C denotes the number of classes. This classiﬁer is trained by minimizing\\nthe cross-entropy loss between the predicted class and the given label.\\n\\nHere, we study the semantic relevance of the top K features for each class. Consider the weight\\nmatrix W . By preserving the top K parameters of this weight matrix for each class and pruning\\nthe features predictive for only one class, we create a bipartite graph between two set of nodes: the\\n\\n6\\n\\nfbasefSEM(=0.01)fSEM(=0.1)fSEM(=1)fSEM(=10)0.010.10.5110SSL 506070Valid accuracy23581334V62.565.067.570.072.5Valid accuracy5010050010005000L62.565.067.570.072.5Valid accuracy\\x0c(a)\\n\\n(b)\\n\\nFigure 4: Semantic relevance of the features. (a) Subset of WK, the bipartite graph of the most\\nimportant features shared between at least two classes of a classiﬁer trained on BYOL + SEMs\\nfeatures. The connected components emerge without additional interventions. (b) Relevance of the\\ntop K features to the semantics of the super-class of the categories of CIFAR-100. It is taken as the\\nnumber of pairwise categories in the same super-class for which a feature is among its top K most\\npredictive features over the total number of pairwise categories.\\n\\ncategories and the features. We denote this graph WK. We plot a subset W5, obtained when taking\\nthe top 5 features for each class, on the SEM representations in Figure 4a and the full bipartite graph\\non the SEM and the one obtained when applying the procedure on the representation obtained with\\nan unnormalized BYOL in the Appendix. In the graph obtained with the SEM, we observe that a\\nset of connected components emerge, and the connected components of the graph are semantically\\nrelated. For example, the ﬁrst set of connected components are fruits and vegetables, and the second\\nset of connected components are aquatic mammals. The same observation does not occur when this\\nexperiment is performed on the baseline BYOL and BYOL, with a large representation model. In\\nparticular, we do not see a small number of semantically related connected components. Instead, we\\nsee a large fully connected graphs. This observation suggests that the features learned by the baseline\\nmodel do not hold the same amount of semantic information. Instead, the semantic information could\\nbe encoded as a linear combination of several features, for example.\\n\\nWe also study more quantitatively the semantic relevance of the features in CIFAR-100. Two\\ncategories share a predictive feature on WK if they are 2-neighbour, that is they share a common\\npredictive feature. Let N (ci) returns all pairs (ci, cj) for all j 2-neighbour of ci. Moreover, deﬁne\\nthe operation is_super(ci, cj) which returns 1 if ci and cj are from the same CIFAR-100 superclass\\nand 0 otherwise. We reproduce the superclass of CIFAR-100 in Table 5. We deﬁne the semantic\\nrelevance as follows:\\n\\nRelevance(WK) :=\\n\\n(cid:80)\\n\\nC\\n(cid:88)\\n\\ni=1\\n\\n(ci,cj )∈N (ci) is_super(ci, cj)\\n|N (ci)|\\n\\n,\\n\\n(4)\\n\\nwhere C = 100 for CIFAR-100 and | · | is the cardinality of a set.\\n\\nWe compare the semantic relevance of BYOL+SEM with the control experiments BYOL and BYOL\\nwith a representation of the same size as BYOL+SEM but without the normalization. We observe\\nthat using the SEM yields more semantically relevant features than the baseline. This observation is\\nconsistent with the qualitative experiments presented earlier and indicates that the semantics encoded\\n\\nBYOL\\n\\nIN\\n68.3\\nBYOL + SEM 70.6\\n66.7\\nMOCO + SEM 68.0\\n\\nMOCO\\n\\nIN-V2\\n55.3\\n57.9\\n53.4\\n55.0\\n\\n100%\\nIN-R\\n16.5\\n18.1\\n14.0\\n15.21\\n\\nIN-A IN-C\\n35.4\\n0.68\\n38.9\\n0.77\\n0.69\\n31.1\\n33.8\\n0.61\\n\\nIN\\n46.8\\n47.9\\n43.5\\n44.1\\n\\nIN-V2\\n37.5\\n38.5\\n34.2\\n35.9\\n\\n1%\\nIN-R IN-A IN-C\\n0.71\\n12.2\\n25.0\\n25.3\\n12.2\\n0.65\\n0.51\\n20.1\\n8.7\\n21.4\\n0.51\\n9.1\\n\\nTable 1: Test accuracies of a linear probe trained with 100% and 1% of the IMAGENET samples on\\na pre-trained representation trained for 100 epochs. Boldface indicates the maximal value for each\\nevaluation set and each base model type (BYOL or MoCo).\\n\\n7\\n\\norangeappleS190sweet_pepperS181S196pearS29S173S13sharkS355S146S383turtleS185raywhaledolphinS363S338orchidpoppyroseS294tulipS311S343S13601020304050Top K features0.10.20.30.40.50.60.70.8Semantic relevanceBYOLBYOL + large repr.BYOL+SEM\\x0cFOOD CIFAR10 CIFAR-100 SUN DTD PETS FLOWERS CALTECH CARS\\n45.7\\n57.6 71.5 85.4\\nBYOL\\n71.3\\n57.3\\n60.5 72.5 87.1\\nBYOL + SEM 74.1\\n57.6 70.9 82.3\\n39.8\\nMOCO\\n70.6\\nMOCO + SEM 71.0\\n45.2\\n58.6 70.9 83.8\\nTable 2: Transfer learning accuracy by training a linear probe on a pre-trained representation with\\nIMAGENET for 100 epochss. Boldface indicates the maximal value for each transfer dataset and each\\nbase model type (BYOL or MoCo).\\n\\n77.8\\n82.4\\n74.3\\n77.5\\n\\n89.5\\n92.0\\n88.6\\n89.6\\n\\n84.6\\n88.6\\n81.5\\n84.5\\n\\n71.4\\n76.3\\n69.5\\n72.8\\n\\nin the baseline representation may follow a more complicated syntactic structure than those encoded\\nwith the SEM features.\\n\\n4.3 Out-of-distribution evaluation on ImageNet\\n\\nRobustness to out-of-distribution test sets on ImageNet. We perform a comparative study using\\nseveral robustness evaluation sets. Speciﬁcally, we use the validation set provided in IMAGENET;\\nIMAGENET-C, which exhibits a set of common image corruptions [Hendrycks and Dietterich, 2018];\\nIMAGENET-A [Chen et al., 2020a], which contains a set of natural adversarial examples that are\\nmisclassiﬁed by a Resnet-50 classiﬁer; IMAGENET-R [Hendrycks et al., 2021], which consists of\\ndifferent renderings for several ImageNet classes; and IMAGENET-V2 [Recht et al., 2019], a distinct\\ntest set for ImageNet collected using the same process. We use the methodology proposed in Djolonga\\net al. [2020, 2021] along with their software to perform our experiments.\\n\\nTable 1 shows the performance on these test sets using a linear probe trained with 100% of ImageNet’s\\ndata and 1% of ImageNet’s data. Using the SEM generally leads to an improvement in the in-\\ndistribution and out-of-distribution generalization. Notably, we observe a 2% improvement on BYOL\\ndue to the SEM on in-distribution IMAGENET. On average, there is an improvement of 2% and 0.5%\\nin the 100% and 1% data regimes respectively for BYOL. For MOCO, the average improvement due\\nto the SEM is 1.5% and 0.8% for the 100% and 1% data regimes respectively.\\n\\nTransfer learning on ImageNet. We probe the effect of inducing the SEM in BYOL and MoCo\\non the transfer accuracy to other classiﬁcation tasks from representations trained on IMAGENET.\\nWe follow the linear evaluation methodology described in previous works [Grill et al., 2020; Lee\\net al., 2021], which entails training a linear classiﬁer on the embeddings of the samples for each\\ndataset. We perform our transfer learning experiments on the following datasets: Food [Bossard et al.,\\n2014], CIFAR-10 [Krizhevsky, 2009], CIFAR-100 [Krizhevsky, 2009], SUN [Xiao et al., 2010],\\nDTD [Cimpoi et al., 2014], Pets [Parkhi et al., 2012], Flowers [Nilsback and Zisserman, 2008],\\nCalTech [Fei-Fei et al., 2004] and Cars [Krause et al., 2013].\\n\\nThis task evaluates the generality of the encoder as it has to encode samples from various out-of-\\ndistribution domains with categories that it may not have seen during training. We present our results\\nin Table 2 and observe that the SEM improves the transfer accuracy over the baseline for every\\ndataset.\\n\\n5 Conclusion\\n\\nThis work introduces the Simplicial Embeddings (SEM) as a simple and effective drop-in module\\nfor self-supervised learning that leads to representation with better generalization. Our theoretical\\ninsights demonstrate that the temperature parameter of the SEM allows for control over the trade-off\\nbetween the training loss and expressivity on downstream classiﬁers; we also observe that controlling\\nthe expressivity via the temperature parameter. We validate our theoretical prediction with a set\\nof controlled experiments. Moreover, we empirically demonstrate that the SEM improves the in-\\ndistribution test accuracy and out-of-distribution accuracy on several robustness test sets and transfer\\nlearning datasets.\\n\\nWe have also demonstrated that the SEM leads to more semantically relevant features for predicting\\nthe categories of a dataset compared to the baseline method. Thus, the SEM embedding may be\\nsimpler than the un-normalized embedding, leading to more interpretable representations. We want\\n\\n8\\n\\n\\x0cto study this in more depth in future works. Related, we would also like to investigate further why the\\nSEM leads to such representations.\\n\\nAcknowledgments and Disclosure of Funding\\n\\nThe authors are grateful for the insightful discussions with Xavier Bouthillier, Michael Noukhovitch,\\nHattie Zhou, Sébastien Lachapelle, Yuchen Lu, Eeshan Dhekane, and Devon Hjelm. We acknowledge\\nfunding support from Samsung and Hitachi, as well as support from Aaron Courville’s CIFAR CCAI\\nchair. We also wish to acknowledge Mila and Compute Canada for providing the computing infras-\\ntructure that enabled this project. Finally, this project would not have been possible without the con-\\ntribution of the following open source projects: Pytorch [Paszke et al., 2019], Orion [Bouthillier et al.,\\n2022], Solo-Learn [da Costa et al., 2021], Scikit-Learn [Pedregosa et al., 2011], and Numpy [Harris\\net al., 2020].\\n\\nReferences\\n\\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization.\\narXiv:1907.02893 [cs, stat], March 2020. URL http://arxiv.org/abs/1907.02893. arXiv:\\n1907.02893.\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly\\nLearning to Align and Translate. arXiv:1409.0473 [cs, stat], May 2016. URL http://arxiv.\\norg/abs/1409.0473. arXiv: 1409.0473.\\n\\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative com-\\nponents with random forests. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,\\neditors, Computer Vision – ECCV 2014, pages 446–461, Cham, 2014. Springer International\\nPublishing. ISBN 978-3-319-10599-4.\\n\\nLéon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization Methods for Large-Scale Machine\\nLearning. arXiv:1606.04838 [cs, math, stat], February 2018. URL http://arxiv.org/abs/\\n1606.04838. arXiv: 1606.04838.\\n\\nXavier Bouthillier, Christos Tsirigotis, François Corneau-Tremblay, Thomas Schweizer, Lin Dong,\\nPierre Delaunay, Fabrice Normandin, Mirko Bronzi, Dendi Suhubdy, Reyhane Askari, Michael\\nNoukhovitch, Chao Xue, Satya Ortiz-Gagné, Olivier Breuleux, Arnaud Bergeron, Olexa Bilaniuk,\\nSteven Bocco, Hadrien Bertrand, Guillaume Alain, Dmitriy Serdyuk, Peter Henderson, Pascal\\nLamblin, and Christopher Beckham. Epistimio/orion: Asynchronous Distributed Hyperparameter\\nOptimization, March 2022. URL https://doi.org/10.5281/zenodo.3478592.\\n\\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep Clustering for\\nUnsupervised Learning of Visual Features. arXiv:1807.05520 [cs], March 2019. URL http:\\n//arxiv.org/abs/1807.05520. arXiv: 1807.05520.\\n\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\\n\\nUnsupervised learning of visual features by contrasting cluster assignments. 2020.\\n\\nTianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial\\nRobustness: From Self-Supervised Pre-Training to Fine-Tuning. pages 699–708, 2020a. URL\\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Adversarial_\\nRobustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_\\npaper.html.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\\ncontrastive learning of visual representations. In Hal Daumé III and Aarti Singh, editors, Proceed-\\nings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of\\nMachine Learning Research, pages 1597–1607. PMLR, 13–18 Jul 2020b.\\n\\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint\\n\\narXiv:2011.10566, 2020.\\n\\n9\\n\\n\\x0cMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-\\nscribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 3606–3613, 2014.\\n\\nVictor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. Solo-learn: A library\\nof self-supervised methods for visual representation learning, 2021. URL https://github.com/\\nvturrisi/solo-learn.\\n\\nJosip Djolonga, Frances Hubis, Matthias Minderer, Zachary Nado, Jeremy Nixon, Rob Romijn-\\nders, Dustin Tran, and Mario Lucic. Robustness Metrics, 2020. URL https://github.com/\\ngoogle-research/robustness_metrics.\\n\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Syl-\\nvain Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On Robustness and Transferabil-\\nity of Convolutional Neural Networks. arXiv:2007.08558 [cs], March 2021. URL http:\\n//arxiv.org/abs/2007.08558. arXiv: 2007.08558.\\n\\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training\\nexamples: An incremental bayesian approach tested on 101 object categories. In 2004 conference\\non computer vision and pattern recognition workshop, pages 178–178. IEEE, 2004.\\n\\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv:1410.5401 [cs],\\n\\nDecember 2014. URL http://arxiv.org/abs/1410.5401. arXiv: 1410.5401.\\n\\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\\nBilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent -\\na new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\\nBalcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\\npages 21271–21284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\\npaper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\\n\\nCharles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\\nCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti\\nPicus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández\\ndel Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy,\\nWarren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming\\nwith NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2.\\nURL https://doi.org/10.1038/s41586-020-2649-2.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\\nRecognition. arXiv:1512.03385 [cs], December 2015. URL http://arxiv.org/abs/1512.\\n03385. arXiv: 1512.03385.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for\\nUnsupervised Visual Representation Learning. arXiv:1911.05722 [cs], March 2020. URL http:\\n//arxiv.org/abs/1911.05722. arXiv: 1911.05722.\\n\\nDan Hendrycks and Thomas Dietterich. Benchmarking Neural Network Robustness to Common\\nCorruptions and Perturbations. September 2018. URL https://openreview.net/forum?id=\\nHJz6tiCqYm.\\n\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\\nThe Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization.\\narXiv:2006.16241 [cs, stat], July 2021. URL http://arxiv.org/abs/2006.16241. arXiv:\\n2006.16241.\\n\\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\\nand maximization. In International Conference on Learning Representations, 2019. URL https:\\n//openreview.net/forum?id=Bklr3j0cKX.\\n\\n10\\n\\n\\x0cSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd\\nInternational Conference on Machine Learning, volume 37 of Proceedings of Machine Learning\\nResearch, pages 448–456, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.\\nmlr.press/v37/ioffe15.html.\\n\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,\\nAaron Maschinot, Ce Liu, and Dilip Krishnan.\\nIn\\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Ad-\\nvances in Neural Information Processing Systems, volume 33, pages 18661–18673. Cur-\\nran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\\nd89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf.\\n\\nSupervised contrastive learning.\\n\\nAlexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representa-\\n\\ntion learning. CoRR, abs/1901.09005, 2019. URL http://arxiv.org/abs/1901.09005.\\n\\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained\\ncategorization. In Proceedings of the IEEE international conference on computer vision workshops,\\npages 554–561, 2013.\\n\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\n\\nKuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive\\nVisual Representations. arXiv:2109.12909 [cs, math], September 2021. URL http://arxiv.\\norg/abs/2109.12909. arXiv: 2109.12909.\\n\\nMaria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number\\nof classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing,\\npages 722–729, 2008. doi: 10.1109/ICVGIP.2008.47.\\n\\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In 2012\\nIEEE Conference on Computer Vision and Pattern Recognition, pages 3498–3505, 2012. doi:\\n10.1109/CVPR.2012.6248092.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\\nhigh-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\'Alché-\\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,\\npages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\\npdf.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\\nE. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,\\n12:2825–2830, 2011.\\n\\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classiﬁers\\nIn Proceedings of the 36th International Conference on Machine\\nGeneralize to ImageNet?\\nLearning, pages 5389–5400. PMLR, May 2019. URL https://proceedings.mlr.press/\\nv97/recht19a.html. ISSN: 2640-3498.\\n\\nAaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive Learning of General-Purpose Audio\\nRepresentations. arXiv:2010.10915 [cs, eess], October 2020. URL http://arxiv.org/abs/\\n2010.10915. arXiv: 2010.10915.\\n\\nAad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. Springer\\nNew York, 1996. doi: 10.1007/978-1-4757-2545-2. URL https://doi.org/10.1007%\\n2F978-1-4757-2545-2.\\n\\n11\\n\\n\\x0cAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762 [cs], December 2017.\\nURL http://arxiv.org/abs/1706.03762. arXiv: 1706.03762 version: 5.\\n\\nTan Wang, Zhongqi Yue, Jianqiang Huang, Qianru Sun, and Hanwang Zhang. Self-Supervised\\nLearning Disentangled Group Representation as Feature. May 2021. URL https://openreview.\\nnet/forum?id=RQfcckT1M_4.\\n\\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\\nLarge-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on\\ncomputer vision and pattern recognition, pages 3485–3492. IEEE, 2010.\\n\\nAsano Ym, Rupprecht C, and Vedaldi A. Self-labelling via simultaneous clustering and representation\\n\\nlearning. September 2019. URL https://openreview.net/forum?id=Hyx-jyBFPr.\\n\\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\\ncontrastive learning with augmentations. CoRR, abs/2010.13902, 2020. URL https://arxiv.\\norg/abs/2010.13902.\\n\\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\\n\\nlearning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.\\n\\n12\\n\\n\\x0cA Proof of Theorem 1\\n\\nLet us introduce additional notations used in the proofs. Deﬁne r = (z, y) ∈ R, (cid:96)(f, r) = l(f (z), y),\\n\\n˜Cy,k1,...,kL = {(z, ˆy) ∈ Z × Y : ˆy = y, kj = arg max\\n\\nzj,t ∀j ∈ [L]},\\n\\nt∈[V ]\\n\\nand\\n\\n˜Zk1,...,kL = {z ∈ Z : kj = arg max\\n\\nzj,t ∀j ∈ [L]}.\\n\\nt∈[V ]\\n\\nto be\\n\\nthen deﬁne Ck\\n\\n=\\nWe\\nthe ﬂatten version of\\n{ ˜Cy,k1,...,kL,y}y∈Y,k1,...,kL∈[V ] with C1 = ˜C1,1,...,1, C2 = ˜C2,1,...,1, C|Y| = ˜C|Y|,1,...,1, C|Y|+1 =\\n˜C1,2,1,...,1, C2|Y| = ˜C|Y|,2,1,...,1, and so on. Similarly, deﬁne Zk to be the ﬂatten version of ˜Zk1,...,kL.\\nWe also use Qi = {q ∈ [−1, +1]V : i = arg maxj∈[V ] qj}, Ik := I S\\nk := {i ∈ [n] : ri ∈ Ck}, and\\n(cid:80)n\\nαk(h) := Er[(cid:96)(h, r)|r ∈ Ck]. Moreover, we deﬁne ϕ(f S\\nt=1 (cid:107)q − q(cid:48)(cid:107)2\\n2,\\neqj /τ\\nand ϕ(f S\\nt=1 eqt/τ for\\nj = 1, . . . , V .\\n\\nSEM(τ )) = supi∈[V ] supq,q(cid:48)∈Qi\\n\\nbase) = supi∈[V ] supq,q(cid:48)∈Qi\\n\\nt=1 (cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n\\n2 where στ (q)j =\\n\\n(cid:80)n\\n\\ni.e.,\\n\\nk=1\\n\\n(cid:80)V\\n\\n{Ck}K\\n\\n˜Cy,k1,...,kL;\\n\\nWe ﬁrst decompose the generalization gap into two terms using the following lemma:\\nLemma 1. For any δ > 0, with probability at least 1 − δ,the following holds for all h ∈ H:\\n\\nEr[(cid:96)(h, r)] −\\n\\n(cid:96)(h, ri) ≤\\n\\n|Ik|\\n\\n\\uf8edαk(h) −\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8 + c\\n\\n\\uf8eb\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8f6\\n\\n(cid:114)\\n\\nln(2/δ)\\nn\\n\\n.\\n\\nProof. We ﬁrst write the expected error as the sum of the conditional expected error:\\n\\nEr[(cid:96)(h, r)] =\\n\\nEr[(cid:96)(h, r)|r ∈ Ck] Pr(r ∈ Ck) =\\n\\nErk [(cid:96)(h, rk)] Pr(r ∈ Ck),\\n\\nwhere rk is the random variable for the conditional with r ∈ Ck. Using this, we decompose the\\ngeneralization error into two terms:\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nEr[(cid:96)(h, r)] −\\n\\n(cid:96)(h, ri)\\n\\n=\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nErk [(cid:96)(h, rk)]\\n\\nPr(r ∈ Ck) −\\n\\nErk [(cid:96)(h, rk)]\\n\\n(cid:19)\\n\\n|Ik|\\nn\\n\\n\\uf8eb\\n\\n+\\n\\n\\uf8ed\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n|Ik|\\nn\\n\\n−\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n\\uf8f6\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8 .\\n\\nThe second term in the right-hand side of (5) is further simpliﬁed by using\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:18)\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:96)(h, ri) =\\n\\n(cid:96)(h, ri),\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\n(cid:88)\\n\\nk=1\\n\\ni∈Ik\\n\\nas\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nErk [(cid:96)(h, rk)]\\n\\n|Ik|\\nn\\n\\n−\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:96)(h, ri) =\\n\\n|Ik|\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8eb\\n\\uf8edErk [(cid:96)(h, rk)] −\\n\\n\\uf8f6\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\nSubstituting these into equation (5) yields\\nn\\n(cid:88)\\n\\nEr[(cid:96)(h, r)] −\\n\\n(cid:96)(h, ri)\\n\\n1\\nn\\n\\ni=1\\n\\n(cid:18)\\n\\n=\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nErk [(cid:96)(h, rk)]\\n\\nPr(r ∈ Ck) −\\n\\n(cid:19)\\n\\n+\\n\\n|Ik|\\nn\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8eb\\n\\uf8edErk [(cid:96)(h, rk)] −\\n\\n|Ik|\\n\\n≤ B\\n\\nPr(r ∈ Ck) −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n|Ik|\\nn\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8eb\\n\\uf8edErk [(cid:96)(h, rk)] −\\n\\n|Ik|\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\uf8f6\\n\\n(5)\\n\\n(6)\\n\\n\\uf8f6\\n\\n13\\n\\n\\x0cBy using the Bretagnolle-Huber-Carol inequality [van der Vaart and Wellner, 1996, A6.6 Proposition],\\nwe have that for any δ > 0, with probability at least 1 − δ,\\n(cid:114)\\nK\\n(cid:88)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nk=1\\n\\nPr(r ∈ Ck) −\\n\\n|Ik|\\nn\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n≤\\n\\n2K ln(2/δ)\\nn\\n\\n.\\n\\n(cid:12)\\nHere, notice that the term of (cid:80)K\\n(cid:12)\\n(cid:12) does not depend on h ∈ H. Moreover,\\nnote that for any (f, h, M ) such that M > 0 and B ≥ 0 for all X, we have that P(f (X) ≥ M ) ≥\\nP(f (X) > M ) ≥ P(Bf (X) + h(X) > BM + h(X)), where the probability is with respect to the\\nrandomness of X. Thus, by combining (6) and (7), we have that for any h ∈ H, for any δ > 0, with\\nprobability at least 1 − δ, the following holds for all h ∈ H,\\n\\n(cid:12)\\n(cid:12)Pr(r ∈ Ck) − |Ik|\\n(cid:12)\\n\\nk=1\\n\\nn\\n\\n(7)\\n\\nEr[(cid:96)(h, r)] −\\n\\n(cid:96)(h, ri) ≤\\n\\n|Ik|\\n\\n\\uf8edαk(h) −\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8 + c\\n\\n\\uf8eb\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n\\uf8f6\\n\\n(cid:114)\\n\\nln(2/δ)\\nn\\n\\n.\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\nIn particular, the ﬁrst term from the previous lemma will be bounded with the following lemma:\\nLemma 2. For any f ∈ {f S\\n\\nSEM(τ ), f S\\n\\uf8eb\\n\\nbase},\\n\\n|Ik|\\n\\n\\uf8edαk(f ) −\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8f6\\n\\n(cid:114)\\n\\n(cid:96)(f, ri)\\n\\n\\uf8f8 ≤ R\\n\\nLϕ(f )\\nn\\n\\n.\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\nProof. By using the triangle inequality,\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n≤\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n\\uf8eb\\n\\uf8edEr[(cid:96)(f, r)|r ∈ Ck] −\\n\\n|Ik|\\n\\n\\uf8f6\\n\\n(cid:96)(f, ri)\\n\\n\\uf8f8\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n|Ik|\\n\\nEr[(cid:96)(f, r)|r ∈ Ck] −\\n\\n(cid:96)(f, ri)\\n\\n.\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nFurthermore, by using the triangle inequality,\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:96)(f, ri)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nEr[(cid:96)(f, r)|r ∈ Ck] −\\n\\nEr[(cid:96)(f, r)|r ∈ Ck] −\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:96)(f, ri)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n|Ik|\\n\\n1\\n|Ik|\\n\\n=\\n\\n≤\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:88)\\n\\n≤ sup\\n\\nr,r(cid:48)∈Ck\\n\\n(cid:12)Er[(cid:96)(f, r)|r ∈ Ck] − (cid:96)(f, ri)(cid:12)\\n(cid:12)\\n(cid:12)\\n\\ni∈Ik\\n(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\\n(cid:12)\\n(cid:12) .\\n\\nSEM(τ ) ◦στ , since gS\\n\\nSEM(τ ) ∈ GS, by using the Lipschitz continuity, boundedness,\\n\\nIf f = f S\\nand non-negativity,\\n\\nSEM(τ ) = gS\\n\\n(cid:12)\\n(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\\n\\nsup\\nr,r(cid:48)∈Ck\\n\\n|(ly ◦ gS\\n\\nSEM(τ ))(στ (z)) − (ly ◦ gS\\n\\nSEM(τ ))(στ (z(cid:48)))|\\n\\nsup\\nz,z(cid:48)∈Zk\\n\\n(cid:12) = sup\\ny∈Y\\n≤ R sup\\n\\nz,z(cid:48)∈Zk\\n\\n(cid:107)στ (z) − στ (z(cid:48))(cid:107)F\\n\\n(cid:118)\\n(cid:117)\\n(cid:117)\\n(cid:116)\\n\\nL\\n(cid:88)\\n\\nV\\n(cid:88)\\n\\nt=1\\n\\nj=1\\n\\n(στ (zt,j) − στ (z(cid:48)\\n\\nt,j))2\\n2\\n\\nsup\\ni∈[V ]\\n\\nsup\\nq,q(cid:48)∈Qi\\n\\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n2\\n\\n= R sup\\n\\nz,z(cid:48)∈Zk\\n\\nL\\n(cid:88)\\n\\nt=1\\n\\n(cid:118)\\n(cid:117)\\n(cid:117)\\n(cid:116)\\n\\n(cid:115)\\n\\n≤ R\\n\\n= R\\n\\nLϕ(f S\\n\\nSEM(τ ))\\nn\\n\\n14\\n\\n\\x0cbase = gS\\n\\nSimilarly, if f = f S\\nand non-negativity,\\n(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\\n(cid:12)\\n\\nbase, since gS\\n\\nsup\\nr,r(cid:48)∈Ck\\n\\nbase ∈ GS, by using the Lipschitz continuity, boundedness,\\n\\n|(ly ◦ gS\\n\\nbase)(z) − (ly ◦ gS\\n\\nbase)(z(cid:48))|\\n\\nsup\\nz,z(cid:48)∈Zk\\n\\n(cid:12) = sup\\ny∈Y\\n≤ R sup\\n\\n(cid:107)z − z(cid:48)(cid:107)F\\n\\nz,z(cid:48)∈Zk\\n(cid:114)\\n\\nLϕ(f S\\nbase)\\nn\\n\\n.\\n\\n≤ R\\n\\nTherefore, for any f ∈ {f S\\n\\nSEM(τ ), f S\\n\\nbase},\\n\\n\\uf8eb\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n|Ik|\\n\\n\\uf8edαk(f ) −\\n\\n(cid:96)(f, ri)\\n\\n\\uf8f8 ≤\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n\\uf8f6\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n|Ik|R(cid:112)Lϕ(f ) = R\\n\\n(cid:114)\\n\\nLϕ(f )\\nn\\n\\n.\\n\\nCombining Lemma 1 and Lemma 2, we obtain the following upper bound on the gap:\\nLemma 3. For any δ > 0, with probability at least 1 − δ,the following holds for any f ∈\\n{f S\\n\\nSEM(τ ), f S\\n\\nbase}:\\n\\nEr[(cid:96)(f, r)] −\\n\\n(cid:96)(f, ri) ≤ R\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:114)\\n\\n(cid:114)\\n\\nLϕ(f )\\nn\\n\\n+ c\\n\\nln(2/δ)\\nn\\n\\n.\\n\\nProof. This follows directly from combining Lemma 1 and Lemma 2.\\n\\nWe now provide an upper bound on ϕ(f S\\n\\nSEM(τ )) in the following lemma:\\n\\nLemma 4. For any τ > 0,\\n\\nϕ(f S\\n\\nSEM(τ ))\\nn\\n\\n≤\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+ (V − 1)\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\nProof. Recall the deﬁnition:\\n\\nϕ(f S\\n\\nSEM(τ ))\\nn\\n\\n= sup\\ni∈[V ]\\n\\nsup\\nq,q(cid:48)∈Qi\\n\\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n2.\\n\\nστ (q)j =\\n\\neqj /τ\\nt=1 eqt/τ\\n\\n,\\n\\n(cid:80)V\\n\\nwhere\\n\\nand\\n\\nfor j = 1, . . . , V . By the symmetry and independence over i ∈ [V ] inside of the ﬁrst supremum, we\\nhave\\n\\nϕ(f S\\n\\nSEM(τ ))\\nn\\n\\n= sup\\n\\nq,q(cid:48)∈Q1\\n\\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n2.\\n\\nFor any q, q(cid:48) ∈ Q1 and i ∈ {2, . . . , V } (with q = (q1, . . . , qV ) and q(cid:48) = (q(cid:48)\\nδi, δ(cid:48)\\n\\ni > 0 such that\\n\\n1, . . . , q(cid:48)\\n\\nV )), there exists\\n\\nHere, since zik − ∆ ≥ zij from the assumption, we have that for all i ∈ {2, . . . , V },\\n\\nqi = q1 − δi\\n\\ni = q(cid:48)\\nq(cid:48)\\n\\n1 − δ(cid:48)\\ni.\\n\\nδi, δ(cid:48)\\n\\ni ≥ ∆ > 0.\\n\\n15\\n\\n\\x0cThus, we can rewrite\\n\\nSimilarly,\\n\\nUsing these,\\n\\nV\\n(cid:88)\\n\\nt=1\\n\\nV\\n(cid:88)\\n\\nt=1\\n\\neqt/τ = eq1/τ +\\n\\ne(q1−δi)/τ\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\n= eq1/τ + eq1/τ\\n\\ne−δi/τ\\n\\n= eq1/τ\\n\\n\\uf8ed1 +\\n\\ne−δi/τ\\n\\n\\uf8eb\\n\\n\\uf8eb\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\n\\uf8f6\\n\\n\\uf8f8\\n\\n\\uf8f6\\n\\neq(cid:48)\\n\\nt/τ = eq(cid:48)\\n\\n1/τ\\n\\n\\uf8ed1 +\\n\\ne−δ(cid:48)\\n\\ni/τ\\n\\n\\uf8f8 .\\n\\nστ (q)1 =\\n\\neq1/τ\\nt=1 eqt/τ\\n\\n(cid:80)V\\n\\n=\\n\\neq1/τ\\n1 + (cid:80)V\\n\\n(cid:16)\\n\\neq1/τ\\n\\ni=2 e−δi/τ\\n\\n(cid:17) =\\n\\n1\\n1 + (cid:80)V\\ni=2 e−δi/τ\\n\\nand for all j ∈ {2, . . . , V },\\n\\nστ (q)j =\\n\\neqj /τ\\nt=1 eqt/τ\\n\\n(cid:80)V\\n\\n=\\n\\n=\\n\\n=\\n\\ne(q1−δj )/τ\\n1 + (cid:80)V\\n\\n(cid:16)\\n\\n(cid:17)\\n\\ni=2 e−δi/τ\\n\\neq1/τ\\n\\ne−δj /τ\\n\\n1 + (cid:80)V\\n\\ni=2 e−δi/τ\\n1\\n1 + eδj /τ + (cid:80)V\\n\\ne(δj −δi)/τ\\n\\ni∈Ij\\n\\nστ (q(cid:48))1 =\\n\\n1\\n1 + (cid:80)V\\ni=2 e−δ(cid:48)\\n\\ni/τ\\n\\n,\\n\\nστ (q(cid:48))j =\\n\\n1\\nj /τ + (cid:80)V\\n\\n1 + eδ(cid:48)\\n\\ne(δ(cid:48)\\n\\nj −δ(cid:48)\\n\\ni)/τ\\n\\ni∈Ij\\n\\n.\\n\\nwhere Ij := {2, . . . , V } \\\\ {j}. Similarly,\\n\\nand for all j ∈ {2, . . . , V },\\n\\nUsing these, for any q, q(cid:48) ∈ Q1,\\n\\n|στ (q)1 − στ (q(cid:48))1| =\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n≤\\n\\n=\\n\\n1\\n1 + (cid:80)V\\ni=2 e−δi/τ\\n1\\n1 + (cid:80)V\\ni=2 e−2/τ\\n\\n−\\n\\n−\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\ni/τ\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1 + (cid:80)V\\ni=2 e−δ(cid:48)\\n1\\n1 + (cid:80)V\\ni=2 e−∆/τ\\n1\\n1 + (V − 1)e−∆/τ\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n,\\n\\n16\\n\\n\\x0c(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n≤\\n\\n=\\n\\n=\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nand for all j ∈ {2, . . . , V },\\n\\n|στ (q)j − στ (q(cid:48))j| =\\n\\n1\\n1 + eδj /τ + (cid:80)V\\n\\ne(δj −δi)/τ\\n\\ni∈Ij\\n\\n−\\n\\n1\\nj /τ + (cid:80)V\\n\\n1 + eδ(cid:48)\\n\\ne(δ(cid:48)\\n\\nj −δ(cid:48)\\n\\ni)/τ\\n\\ni∈Ij\\n\\n1\\n1 + e∆/τ + (cid:80)V\\n\\ne(∆−2)/τ\\n\\ni∈Ij\\n\\n−\\n\\n1\\n1 + e2/τ + (cid:80)V\\n\\ne(2−∆)/τ\\n\\ni∈Ij\\n\\n1\\n1 + e∆/τ + (V − 2)e(∆−2)/τ\\n\\n1\\n1 + e2/τ + (V − 2)e(2−∆)/τ\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n.\\n\\n−\\n\\n−\\n\\nBy combining these,\\n\\nsup\\nq,q(cid:48)∈Q1\\n\\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n2\\n\\n= sup\\n\\nq,q(cid:48)∈Q1\\n\\nV\\n(cid:88)\\n\\nj=1\\n\\n|στ (q)j − στ (q(cid:48))j|2\\n\\n≤\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\n+ (V − 1)\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\nUsing the previous lemma, we will conclude the asymptotic behavior of ϕ(f S\\nlemma:\\nLemma 5. It holds that\\n\\nSEM(τ )) in the following\\n\\nϕ(f S\\n\\nSEM(τ )) → 0 as τ → 0.\\n\\nProof. Using Lemma 4,\\n\\nlim\\nτ →0\\n\\nϕ(f S\\n\\nSEM(τ )) ≤ lim\\nτ →0\\n\\nn\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+ n(V − 1) lim\\nτ →0\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\nlim\\nτ →0\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n=\\n\\n−\\n\\n= 0,\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1\\n\\nlim\\nτ →0\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n= |0 − 0|2 = 0.\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nMoreover,\\n\\nand\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nTherefore,\\n\\nSince ϕ(f S\\n\\nSEM(τ )) ≥ 0, this implies the statement of this lemma.\\n\\nlim\\nτ →0\\n\\nϕ(f S\\n\\nSEM(τ )) ≤ 0.\\n\\n17\\n\\n\\x0cAs we have analyzed ϕ(f S\\nSEM(τ )) and ϕ(f S\\nϕ(f S\\nLemma 6. For any τ > 0,\\n\\nSEM(τ )) in the previous two lemmas, we are now ready to compare\\n\\nbase), which is done in the following lemma:\\n\\nϕ(f S\\n\\nSEM(τ )) − ϕ(f S\\n\\nbase) ≤\\n\\n(1 − V ) < 0.\\n\\n3n\\n4\\n\\nProof. From Lemma 4, for any τ > 0,\\n\\nϕ(f S\\n\\nSEM(τ )) ≤ n\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n1\\n(cid:12)\\n(cid:12)\\n1\\n(cid:12)\\n(cid:18) 1\\n1\\n\\n+ n(V − 1)\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n≤ n\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+ n(V − 1)\\n\\n1\\n1 + (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2))\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n= n\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n+ n(V − 1)\\n\\n1\\n2 + (V − 2)e−2/τ\\n\\n−\\n\\n1\\n1 + e2/τ (V − 1)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n≤ n\\n\\n−\\n\\n+ n(V − 1)\\n\\n= n\\n\\n−\\n\\n+ n(V − 1)\\n\\n1\\nV\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:19)2\\n\\n1\\nV\\n\\n1\\nV\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n2\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n− 0\\n(cid:12)\\n(cid:12)\\n\\n1\\n4\\n\\n.\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nRecall the deﬁnition of\\n\\nϕ(f S\\n\\nbase) = sup\\ni∈[V ]\\n\\nsup\\nq,q(cid:48)∈Qi\\n\\nn(cid:107)q − q(cid:48)(cid:107)2\\n2.\\n\\nBy choosing an element in the set over which the supremum is taken, for any δ ≥ ∆ > 0,\\n\\nϕ(f S\\n\\nbase) ≥ sup\\n\\nn(cid:107)q − q(cid:48)(cid:107)2\\n\\n2 ≥ n(cid:107)ˆq − ˆq(cid:48)(cid:107)2\\n\\n2 = n\\n\\n(ˆqj − ˆq(cid:48)\\n\\nj)2\\n\\n2 = n(2 − δ)2V,\\n\\nq,q(cid:48)∈Q1\\n\\nV\\n(cid:88)\\n\\nj=1\\n\\nwhere ˆq1 = 1, ˆqj = 1 − δ for j ∈ {2, . . . , V }, ˆq(cid:48)\\nBy combining those, for for any τ > 0 and δ ≥ ∆ > 0,\\n\\n1 = δ − 1, and ˆq(cid:48)\\n\\nj = −1 for j ∈ {2, . . . , V }.\\n\\nϕ(f S\\n\\nSEM(τ )) − ϕ(f S\\nn\\n\\nbase)\\n\\n≤\\n\\n(cid:18) 1\\n1\\n\\n(cid:19)2\\n\\n1\\nV\\n\\n+ (V − 1)\\n\\n− (2 − δ)2V\\n\\n1\\n4\\n\\n≤ 1 +\\n\\nV −\\n\\n− (2 − δ)2V\\n\\n1\\n4\\n\\n=\\n\\n=\\n\\n+\\n\\n− V\\n\\nV − (2 − δ)2V\\n(cid:18)\\n\\n(2 − δ)2 −\\n\\n(cid:19)\\n\\n1\\n4\\n\\n(cid:18)\\n\\n≤\\n\\n− V\\n\\n1 −\\n\\n(cid:19)\\n\\n1\\n4\\n\\n=\\n\\n(1 − V )\\n\\n3\\n4\\n3\\n4\\n3\\n4\\n3\\n4\\n\\n−\\n\\n1\\n4\\n1\\n4\\n\\n18\\n\\n\\x0cWe combine the lemmas above to prove Theorem 1, which is restated below with its proof:\\n\\nTheorem 1. Let V ≥ 2. For any δ > 0, with probability at least 1 − δ, the following holds for any\\nfS ∈ {f S\\n\\nSEM(τ ), f S\\n\\nbase}:\\n\\nEz,y[l(fS(z), y)] ≤\\n\\nl(fS(z(i)), y(i)) + R\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:114)\\n\\n(cid:114)\\n\\nLϕ(fS)\\nn\\n\\n+ c\\n\\nln(2/δ)\\nn\\n\\n.\\n\\nMoreover,\\n\\nϕ(f S\\n\\nSEM(τ )) → 0 as τ → 0\\n\\nand ϕ(f S\\n\\nSEM(τ )) − ϕ(f S\\n\\nbase) ≤\\n\\n(1 − V ) < 0 ∀τ > 0.\\n\\n3n\\n4\\n\\nProof. The ﬁrst statement directly follows from Lemma 3. The second statement is proven by\\nLemma 5 and Lemma 6.\\n\\nB Hyperparameters\\n\\nWe present the hype-parameters used to train for BYOL+SEM on CIFAR100. The same parameters\\nwere used for ResNet18 and ResNet50.\\n\\nLearning rate\\nWeight-decay\\nOptimizer\\nBYOL EMA\\nVocabulary size (V)\\nMessage length (L)\\nτ online network\\nτ target network\\n\\n0.5\\n1e-4\\nAdamW\\n0.99\\n13\\n5000\\n0.5\\n0.5\\n\\n(a) BYOL+SEM hyper-parameters.\\n\\nC Experiment details for ImageNet\\n\\nC.1\\n\\nImage augmentation\\n\\nWe follow the same procedure as [Grill et al., 2020] for the image augmentation procedure. The\\naugmentation applied in order during training are:\\n\\n• Random Resize crop to a 224 × 224 image. A random patch of the image is selected and\\n\\nresized to a 224 × 224 image.\\n\\n• Random color jitter. Modifying the brightness, the contrast, the saturation and the hue.\\n\\n• Random gray scale. Randomly applying a gray scale ﬁlter to the image\\n\\n• Random gaussian blur. Randomly applying a gaussian blue ﬁlter.\\n\\n• Random solarization. Randomly applying a solarization ﬁlter.\\n\\nAt validation and test time, we resize the images to 256 × 256 and then center crop a patch of\\n224 × 224.\\n\\nFor both training and evaluation, we re-normalize the image using the statistic of the training set.\\n\\nC.2 Hyper-parameters\\n\\nWe summarize the hyper-parameters for BYOL with SEM and MoCo with SEM in table 4.\\n\\n19\\n\\n\\x0cLearning rate\\nBatch size\\nWeight-decay\\nOptimizer\\nEpochs\\nBase momentum\\nVocabulary size (V)\\nMessage length (L)\\nτ online network\\nτ target network\\n\\n0.9\\n256\\n1e-6\\nSGD with lars\\n100\\n0.99\\n29\\n465\\n2.397\\n2.386\\n\\n(a) BYOL+SEM hyper-parameters.\\n\\nLearning rate\\nBatch size\\nWeight-decay\\nOptimizer\\nEpochs\\nMoCo’s EMA\\nVocabulary size (V)\\nMessage Length (L)\\nτ online network\\nτ target network\\n\\n0.6\\n256\\n3e-5\\nSGD with lars\\n100\\n0.1\\n12\\n512\\n1.35\\n1.2\\n\\n(b) MoCo+SEM hyper-parameters.\\n\\nTable 4: ImageNet’s experiment hyper-parameters.\\n\\nC.3 Linear evaluation\\n\\nWe follow the evaluation protocol from [Chen et al., 2020b]. The linear evaluation is done by training\\na linear classiﬁer on the frozen representation of the ImageNet training samples. We train a linear\\nclassiﬁer with a cross-entropy objective for 100 epochs using SGD with nesterov and a batch size of\\n512. During training, we apply random resized crop and random horizontal ﬂip.\\n\\nC.4 Semi-supervised learning\\n\\nWe perform semi-supervised experiments by training a linear classiﬁer on top of a frozen repre-\\nsentation. The procedure is the same as the linear evaluation procedure with the exception that we\\ntrain with 1% of the training sample. That training samples are taken according to the split deﬁned\\nin [Chen et al., 2020b].\\n\\nC.5 Robustness experiments\\n\\nWe follow the evaluation procedure from [Lee et al., 2021]. We treated the robustness datasets as\\nadditional \"test sets\" in that we simply evaluated them using the evaluation procedure described\\nabove. The images were resized to a 256 × 256 before being center cropped to a 224 × 224 image.\\nThe evaluation procedure was performed using the public robustness benchmark evaluation code\\nof [Djolonga et al., 2020]2.\\n\\nC.6 Transfer learning experiments\\n\\nWe follow the linear evaluation protocol of [Kolesnikov et al., 2019; Chen et al., 2020b] We train a\\nlinear classiﬁer using a regularized multinomial logistic regression from the scikit-learn package [Pe-\\ndregosa et al., 2011]. The representation is frozen, so that we do not train the encoder backbone nor\\n\\n2https://github.com/google-research/robustness_metrics\\n\\n20\\n\\n\\x0cSuperclass\\naquatic mammals\\nﬁsh\\nﬂowers\\nfood containers\\nfruit and vegetables\\nhousehold electrical devices\\nhousehold furniture\\ninsects\\nlarge carnivores\\nlarge man-made outdoor things\\nlarge natural outdoor scenes\\nlarge omnivores and herbivores\\nmedium-sized mammals\\nnon-insect invertebrates\\npeople\\nreptiles\\nsmall mammals\\ntrees\\nvehicles 1\\nvehicles 2\\n\\nClasses\\nbeaver, dolphin, otter, seal, whale\\naquarium ﬁsh, ﬂatﬁsh, ray, shark, trout\\norchids, poppies, roses, sunﬂowers, tulips\\nbottles, bowls, cans, cups, plates\\napples, mushrooms, oranges, pears, sweet peppers\\nclock, computer keyboard, lamp, telephone, television\\nbed, chair, couch, table, wardrobe\\nbee, beetle, butterﬂy, caterpillar, cockroach\\nbear, leopard, lion, tiger, wolf\\nbridge, castle, house, road, skyscraper\\ncloud, forest, mountain, plain, sea\\ncamel, cattle, chimpanzee, elephant, kangaroo\\nfox, porcupine, possum, raccoon, skunk\\ncrab, lobster, snail, spider, worm\\nbaby, boy, girl, man, woman\\ncrocodile, dinosaur, lizard, snake, turtle\\nhamster, mouse, rabbit, shrew, squirrel\\nmaple, oak, palm, pine, willow\\nbicycle, bus, motorcycle, pickup truck, train\\nlawn-mower, rocket, streetcar, tank, tractor\\n\\nTable 5: Set of classes for each superclass on CIFAR-100.\\n\\nthe batch-normalization statistics. We do not perform any augmentations and the images are resized\\nto 224 pixels using bicubic resampling and the normalized using the statistics on ImageNet’s training\\nset. We tune the regularizer term from a range of 11 logarithmically-spaced values between 10−6 and\\n105 using a small validation set and re-train using the full training set.\\n\\nD CIFAR100 superclass\\n\\nThe 100 classes of CIFAR-100 [Krizhevsky, 2009] are grouped into 20 superclasses. The list of\\nsuperclass for each class in Table 5\\n\\nE Additional CIFAR-100 relevance graphs\\n\\n21\\n\\n\\x0c(a) BYOL baseline\\n\\n(b) BYOL baseline with a large rep-\\nresentation\\n\\n(c) BYOL + SEM\\n\\nFigure 5: Comparison of the full relevance graph W5 between BYOL and BYOL + SEM.\\n\\n22\\n\\nS363otterkangaroobottleS382lampS225beartankS144streetcarroadS249S167trainS94lobsterS273boygirlhousewomanS29S106cockroachS6appledolphinsealwhaleaquarium_fishflatfishraysharkorchidpoppyrosesunflowertulipbowlcancupplatemushroomorangepearsweet_pepperclockcomputer_keyboardtelephonetelevisionbedchaircouchtablewardrobebeebeetlebutterflycaterpillarleopardtigerwolfbridgecastleskyscrapercloudforestmountainplainseacamelcattlechimpanzeefoxpossumraccoonskunkcrabsnailwormbabymanlizardsnaketurtlehamstermouserabbitshrewsquirrelmaple_treeoak_treepalm_treewillow_treebicyclebusmotorcyclepickup_trucklawn_mowerrockettractorS55S63S47S279S162S69S134S242S34S241S0S98S158S361S91S66S12S95S388S53S374S190S22S59S183S261S1S375S27S35S130S110S127S43S111S258S380S214S30S85S292S193S238S336S128S132S284S14S99S93S332S236S298S260S281S68S204S161S182S177S381S64S227S320S318S266S230S196S126S181S184S114S187flatfishtableS16troutcaterpillarS194cattleS276orchidcomputer_keyboardS229palm_treeS184cloudrocketskunkchimpanzeeS256S187pickup_trucksnailcameltigerspiderS307dolphinsweet_pepperS85lionS434S415lobstersunflowermushroomS66orangesquirrelS189S319manS190snakeS195babywardrobewolfsealwillow_treecanS323maple_treeS252S404turtleleopardhamsterrabbitcockroachS353clockforestS87S61pine_treeS369couchS397tankotterwomanbicycleS365S148bedbeavertulipbeelawn_mowercrocodilelamptelephoneS435tractorS422oak_treeS233motorcycleshrewS284S33S146S13plainS78whalebusbeeS248spiderbeetlecockroachS178S240bearkangarootigerS393leopardhousecastleS40roadS107plainS403motorcyclelawn_mowertankS350tractorsealS61beaverS284porcupinebedS168couchS360chairlionfoxS216wolfS272skyscraperS227mountainS78seasnakeS98S49S219S2lizardwormorangeS185S30sweet_pepperpearS198S193applebusS18S133streetcarS370S106pickup_trucktrainroseS306poppyS289S329tulipS334S135orchidS210cancupS31S259S321S291bowlplatebottlewillow_treepine_treemaple_treeoak_treeforestS71palm_treeS138S327S405dolphinrayS151S347S17S379turtlesharkS176S189S355whaleS381S221S200S22boyS157girlwomanbabyS204flatfishmanmouseraccoonS336S271S300possumsquirrelS239S128shrewhamsterrabbit\\x0c',\n",
       " '2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\n\\nO\\nC\\n.\\nh\\nt\\na\\nm\\n\\n[\\n \\n \\n1\\nv\\n5\\n1\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nLARGE-SCALE ROOK PLACEMENTS\\n\\nPAKAWUT JIRADILOK\\n\\nAbstract. For each certain “nice” piecewise linear function f : [0, 1] → [0, 1], we con-\\nsider a family of growing Young diagrams {λ(f, N )}∞\\nN=1 by enlarging the region under\\nthe graph of f . We compute asymptotic formulas for the number of rook placements\\nof the shape λ(f, N ). We prove that the normalized cumulative X-ray of a uniformly\\nrandom permutation, as the size of the permutation grows, exhibits a limit shape phe-\\nnomenon.\\n\\n1. Introduction\\n\\nWhile rook placements are classical objects in combinatorics (cf. e.g. [Rio02, Chapter 7]\\nand [Sta12, Chapter 2]), there are many recent works in the literature studying them (e.g.\\n[BR06, BLRS14, BLRS16, Bar21]). Enumerative combinatorics of rook placements deals\\nwith problems of counting the number of ways to place a certain number of non-attacking\\nrooks on a certain subset of a chessboard. In many cases, one obtains nice formulas. For\\ninstance, it is a well-known elementary exercise that if the subset of the chessboard takes\\nthe shape of a Young diagram of a partition, then the number of rook placements has a nice\\nproduct formula.\\n\\nIn this paper, we study large-scale rook placements. We are interested in the family of\\nrook placements when the board on which non-attacking rooks are placed grows in size in\\nthe following manner. We deﬁne a class\\nof “nice” piecewise linear functions from [0, 1] to\\nbelongs to this class.\\n[0, 1] (see the exact deﬁnition in Subsection 3.1). Suppose that f\\nWe obtain a family of Young diagrams by dilating the region under the graph of f from the\\n[0, N ] for each positive integer N . More precisely, we let\\nunit square [0, 1]\\nλ(f, N ) be the Young diagram with N rows whose ith row has (λ(f, N ))i :=\\nf (i/N )\\n⌉\\nboxes (see Subsection 3.2 for details). Let RP(λ(f, N )) denote the set of rook placements of\\nthe shape λ(f, N ). Our ﬁrst main result of this paper, Theorem 3.7, says that the cardinality\\nof RP(λ(f, N )) behaves well asymptotically:\\n\\n[0, 1] to [0, N ]\\n\\n∈ P\\n\\nN\\n\\n×\\n\\n×\\n\\nP\\n\\n⌈\\n\\n·\\n\\nlog (# RP(λ(f, N ))) = N log N + Bf ·\\n\\nN +\\n\\nlog N + Of (1),\\n\\n1\\n2\\n\\nfor positive integers N which are multiples of a certain integer depending on f (see the\\nstatement of Theorem 3.7 for details), and we establish the following integral formula for\\nthe coeﬃcient Bf :\\n\\n1\\n\\nBf =\\n\\nlog(f (x) + x\\n\\n1) dx.\\n\\n−\\n\\n0\\nZ\\nOur next stop is a special subclass\\n\\nof the function class\\n\\nas\\nthe “combinatorial” class, since it contains, rather naturally, many familiar objects from\\nis a countable\\nalgebraic combinatorics such as Dyck paths and Motzkin paths. The class\\nPk (see Section 4 for the precise deﬁnition).\\nunion of ﬁnite families of functions:\\nBijective combinatorics in\\nis noteworthy, and so we spend Subsection 4.1 discussing it.\\ne\\nThis subsection contains a bijective-combinatorics ﬂavor which seems less analytic than its\\n\\n. We might refer to\\n\\ne\\n=\\n\\n∞k=1\\n\\nS\\n\\nP\\n\\nP\\n\\nP\\n\\nP\\n\\nP\\n\\nP\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\nDate: April 4, 2022.\\n2020 Mathematics Subject Classiﬁcation. 05A16 (Primary) 05A05, 05A19, 05A20, 60F05 (Secondary).\\nKey words and phrases. large-scale rook placement, Young diagram, partition, dilation, asymptotic for-\\nmula, integral formula, Dyck path, Motzkin path, Schr¨oder number, ground bump, waterfall, combinatorial\\ninequality, permutation, X-ray, cumulative X-ray, limit shape, random permutation.\\n\\n1\\n\\n\\x0cneighboring parts. For example, we provide bijective arguments resulting in Corollary 4.2\\nwhich states\\n\\n=\\n\\nPk|\\n\\n|\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n−\\n1\\n−\\n\\n.\\n\\n(cid:19)\\nThe functions in\\nPk are in one-to-one correspondence with combinatorial objects which we\\ncall “waterfalls” (see Subsection 4.1 for details). Studying waterfalls yields the following\\ncurious combinatorial formula, given in Proposition 4.9:\\ne\\n\\n(cid:18)\\n\\ne\\n\\nwt(D) =\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n−\\n1\\n−\\n\\n,\\n(cid:19)\\n\\n(cid:18)\\n\\nDyck(k)\\n\\nXD\\n∈\\n\\nwhere Dyck(k) denotes the family of Dyck paths of length 2k — lattice paths from (0, k)\\nto (k, 0) with k unit steps to the right and k unit steps down which never go below the line\\nX + Y = k — and the weight wt(D) is deﬁned as\\n\\nwt(D) :=\\n\\n#\\n\\nj\\n\\n{\\n\\n∈\\n\\nZ\\n\\n|\\n\\ni + j > k and (i, j)\\n\\nD\\n\\n.\\n\\n}\\n\\n∈\\n\\nk\\n\\n1\\n\\n−\\n\\ni=1\\nY\\n\\nHaving visited waterfalls, we proceed to Subsection 4.2. Functions in the combinatorial\\nclass allow for even more precise asymptotics for the number of rook placements. Our second\\nmain result, Theorem 4.10, provides the following asymptotic formula for f\\nPk as follows.\\nWe have\\ne\\nlog N + Df + Of (1/N ),\\n\\nlog (# RP(λ(f, N ))) = N log N + Bf ·\\n\\nN +\\n\\n1\\n2\\n\\n∈\\n\\nfor positive integers N\\nthe following integral formula for the coeﬃcient Df :\\n\\n∈\\n\\nkZ, where the coeﬃcient Bf is the same as before, and we give\\n\\nDf :=\\n\\nlog(2π) +\\n\\n1\\n2\\n\\n1\\n\\n1\\n2\\n\\n0\\nZ\\n\\nxf ′(x)\\nx(f (x) + x\\n\\nf (x) + 1\\n1)\\n\\n−\\n\\ndx.\\n\\n−\\n\\n∈\\n\\nNow that for each function f\\n\\nPk, there are two coeﬃcients Bf and Df associated to\\nit, one might wonder about the possible ranges of these numbers. Proposition 4.16 states\\nthat\\ne\\n\\n1\\nk ≤\\nBoth upper bound and lower bound are tight. Each of them is attained by exactly one\\nfunction in\\n\\nBf ≤ −\\n\\nlog k\\n\\n1.\\n\\n−\\n\\n−\\n\\nSimilarly, we have tight bounds for Df . Proposition 4.17 states that\\n\\nPk.\\ne\\n\\n1\\n2\\n\\nlog\\n\\n2π\\nk\\n\\nDf ≤\\n\\n≤\\n\\n1\\n2\\n\\nlog\\n\\n2kπ\\nk\\n\\n.\\n\\n(cid:18)\\nThe upper bound is attained by exactly one function in\\nPk. The equality cases for the lower\\nbound is rather interesting: the lower bound is attained by a Catalan-numerous family of\\ne\\nfunctions inside\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n(cid:19)\\n\\nEach rook placement — or, more generally, each permutation — comes with a certain\\nsequence of non-negative integers called the X-ray. An object which appears in the ﬁeld of\\ndiscrete tomography (cf. e.g. [HK99]), the X-ray of permutation has been investigated from\\nalgebraic and combinatorial points of view (cf. e.g. [BF14, BMPS05]). It is related to other\\nobjects in combinatorics such as Skolem sets (cf. e.g. [Nor08]) and permutohedra (cf. e.g.\\n[Pos09]). For each permutation π\\nn permutation matrix,\\nthe cumulative X-ray of π is the function ξπ : [0, 2n]\\n\\nSn, which we consider as an n\\n\\n[0, n] given by\\n\\n×\\n\\n∈\\n\\nPk.\\ne\\n\\nξπ(t) :=\\n\\n→\\nπij ,\\n\\n[n]\\nXi,j\\n∈\\ni+j\\nt\\n≤\\n2\\n\\n\\x0c∈\\n\\nSn is the function\\n\\n[0, 1] given by\\nξπ(nt). Thus, the graph of the normalized cumulative X-ray is simply the graph\\n\\nand the normalized cumulative X-ray of π\\nξπ(t) := 1\\nn ·\\nof the cumulative X-ray rescaled from the rectangle [0, 2n]\\nWe have arrived at our last stop, where we consider the X-ray of a random large rook\\ne\\nplacement. For each a partition λ, we let N\\nλ be the partition obtained from magnifying\\n⊙\\nλ by a factor of N (see Section 2 for the precise deﬁnition). Conjecture 5.9 predicts that the\\nξπ exhibits a limit shape phenomenon: for a ﬁxed real ε > 0,\\nnormalized cumulative X-ray\\nif π is a uniformly random rook placement of the shape N\\n\\n[0, n] to [0, 2]\\n\\nξπ : [0, 2]\\n\\nλ, then\\n\\n[0, 1].\\n\\n→\\n\\n×\\n\\n×\\n\\ne\\n\\n⊙\\n\\nmλ(t)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nsup\\n[0,2]\\n\\nξπ(t)\\n\\n−\\n\\n< ε\\n\\n1,\\n\\n! →\\n\\nas N\\n→\\nEquation (29) in Subsection 5.2 provides a formula for this function.\\n\\n→ ∞\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)e\\n[0, 1] is a certain function depending on the shape λ.\\n\\n \\nt\\n∈\\n, where mλ : [0, 2]\\n\\nOur third main result of this paper, Theorem 5.10, proves this conjecture in the special\\ncase when λ = (cid:3) is a partition with one box. In other words, it says that the normalized\\ncumulative X-ray of a uniformly random permutation, as the size of the permutation grows,\\nexhibits a limit shape phenomenon in the above sense. We note that it is easy to compute\\nthe limit shape for the permutation case explicitly:\\n\\ne\\n\\nP\\n\\nm(cid:3)(t) :=\\n\\nt2\\n2\\n\\n(\\n\\nt2\\n2 + 2t\\n\\n1\\n\\nif 0 < t\\nif 1 < t\\n\\n1, and\\n2.\\n\\n≤\\n≤\\n\\n−\\nWhile Theorem 5.10 proves Conjecture 5.9 for only one very special case, we hope that one\\nproof technique is applicable, perhaps with some more work, for other shapes λ as well.\\n\\n−\\n\\nWe remark that since rook placements can be considered as permutations, our work in\\nthis paper is closely related to an active and exciting ﬁeld of research on large permutations\\nand “permutons” (cf. e.g. [HKM+13, AM14, GGKK15, GHK+17, KKRW20]). For example,\\nour construction of the normalized cumulative X-ray is reminiscent of that of permutons. It\\nwould be interesting, in the author’s opinion, to see how tools from the permuton literature\\ncan be applied to better understand large rook placements.\\n\\nOutline. In Section 2, we give some deﬁnitions and present some elementary facts about\\nof “nice” piecewise linear functions. It\\nrook placements. Section 3 focuses on the class\\ncontains Theorem 3.7, our ﬁrst main result. Section 4 focuses on the “combinatorial” class\\n. We discuss some bijective combinatorics in Subsection 4.1. We establish Theorem 4.10,\\nP\\nour second main result, in Subsection 4.2. We give some properties of the coeﬃcient Df in\\nSubsection 4.3. We prove inequalities on the coeﬃcients Bf and Df in Subsection 4.4. In\\ne\\nSection 5, we discuss probabilities and X-rays. It contains Conjecture 5.9. We deduce this\\nconjecture in the special case of random permutations from Theorem 5.10, our third main\\nresult, in Subsection 5.3.\\n\\nP\\n\\nFor each positive integer n, let Sn denote the set of permutations of [n] :=\\n{\\nn matrix (“permutation matrix”)\\n\\nWe think of a permutation π\\n\\nSn as an n\\n\\n1, 2, . . . , n\\n\\n.\\n\\n}\\n\\n2. Rook Placements\\n\\n×\\nπ11\\nπ12\\nπ21\\nπ22\\n...\\n...\\nπn1 πn2\\n\\n∈\\n\\nπ = \\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n\\n· · ·\\n· · ·\\n. . .\\n\\n· · ·\\n\\nπ1n\\nπ2n\\n...\\nπnn\\n\\n\\uf8f9\\n\\n,\\n\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\nwhere each entry πij is either 0 or 1, each row has exactly one 1, and each column has\\nexactly one 1. A partition is a ﬁnite sequence of weakly decreasing positive integers. A\\npartition λ is said to have (exactly) n parts if the length of λ, as a ﬁnite sequence, is n.\\n3\\n\\n\\x0cLet us denote by Par the set of all partitions. By convention, we also include the empty\\npartition [ ] in Par. Consider the set\\n\\nIn other words,\\na partition λ\\n\\n{\\n\\n[λ1, λ2, . . . , λn]\\n\\nBn :=\\nBn is the set of partitions λ with exactly n parts such that λ1 = n. Given\\n\\nλ2 ≥ · · · ≥\\n\\nn = λ1 ≥\\n\\nλn > 0\\n\\nPar\\n\\n∈\\n\\n}\\n\\n|\\n\\n.\\n\\n∈ Bn, a rook placement of the shape λ is a permutation π\\n[n], if j > λn+1\\n\\ni, then πij = 0.\\n\\nfor any i, j\\n\\n∈\\n\\nSn such that\\n\\n∈\\n\\n−\\n\\nWe use the notation RP(λ) to denote the set of rook placements of the shape λ. The\\ncardinality of RP(λ) has a well-known and easy-to-prove formula: for any λ\\n\\n∈ Bn,\\n\\n(1)\\n\\nn\\n\\n# RP(λ) =\\n\\n(λi −\\n\\n(n\\n\\n−\\n\\ni)).\\n\\ni=1\\nY\\nOne particular point to notice about the product formula above that is particularly beautiful,\\nin the author’s opinion, is that the formula holds even when there are no rook placements\\n∈ Bn such that RP(λ) = ∅, the right-hand\\nof the shape λ. In other words, for partitions λ\\nside of the formula becomes zero (not some negative integer).\\nIt is well-known that for λ\\n∈ Bn,\\nIt\\ni.\\nn + 1\\n[n], we have λi ≥\\n−\\nis the\\n|Dn|\\n\\nRP(λ) is not empty.\\n.\\n}\\nDn if and only if for all i\\nis the central binomial coeﬃcient\\n\\nthe partition λ belongs to\\nis also well-known that\\n|Bn|\\nnth-Catalan number Cn := 1\\nn+1\\n\\nDn :=\\n\\nWe deﬁne\\n\\nand that\\n\\n∈ Bn |\\n\\n2\\n−\\n1\\n−\\n\\n2n\\nn\\n\\n2n\\nn\\n\\n∈\\n\\nλ\\n\\n{\\n\\n.\\n\\n(cid:0)\\n\\nPar(n) and let m be a positive integer. We deﬁne the partition m\\n\\nFor each non-negative integer n, let Par(n) denote the set of all partitions λ such that\\nthe sum of all parts of λ is n. Now we describe how we dilate partitions. Suppose that\\nPar(m2n) as\\nλ\\nfollows. Imagine starting with the Young diagram of λ, and then replacing each of the n\\nboxes of λ with an m\\nm array of boxes. The resulting diagram is the Young diagram of\\nm\\n\\nλ.\\n\\n×\\n\\n⊙\\n\\n∈\\n\\n∈\\n\\nλ\\n\\n(cid:1)\\n\\n(cid:0)\\n\\n(cid:1)\\n\\nλ) is an immediate consequence (and also\\n\\n⊙\\nThe following formula for the size of RP(m\\n\\na mild generalization) of Equation (1).\\n\\n⊙\\n\\nProposition 2.1. Let m and n be positive integers. For any partition λ = [λ1, λ2, . . . , λn]\\nBn, we have m\\n\\n∈ Bmn and\\n\\n⊙\\n\\nλ\\n\\n∈\\n\\n# RP(m\\n\\nλ) = m!n\\n\\n⊙\\n\\nn\\n\\n·\\n\\ni=1 (cid:18)\\nY\\n\\n(n\\nm(λi −\\nm\\n\\n−\\n\\ni))\\n\\n.\\n(cid:19)\\n\\nHere, the binomial coeﬃcient is deﬁned for a\\n\\nZ and b\\n\\nZ\\n\\n1 as\\n\\na\\nb\\n\\n:= a(a\\n\\n−\\n\\n1)\\n\\n(a\\n\\nb+1)\\n\\n···\\nb!\\n\\n−\\n\\n.\\n\\nIt is easy to see that for λ\\npositive integer m, the partition m\\n\\n∈\\n∈ Bn, the partition λ belongs to\\n\\n∈\\n\\n≥\\n\\nλ belongs to\\n\\nDmn.\\n\\n(cid:1)\\n\\n(cid:0)\\nDn if and only if for any\\n\\n3. The class\\n\\nof piecewise linear functions\\n\\n3.1. The functions and their lofts. Consider the class\\nwith the following properties:\\n\\nP\\n\\nof functions f : [0, 1]\\n\\n[0, 1]\\n\\n→\\n\\nf is weakly decreasing,\\nf is piecewise linear with a ﬁnite number of non-diﬀerentiable points,\\nall the non-diﬀerentiable points of f are rational numbers in [0, 1],\\nthere exists ε > 0 such that for any 0\\nf (1) > 0, and\\nfor any a\\n\\nx < ε, we have f (x) = 1,\\n\\n(0, 1), we have limx\\n\\na f (x) > 1\\n\\na.\\n\\n≤\\n\\n∈\\n\\nց\\n\\n−\\n\\n•\\n•\\n•\\n•\\n•\\n•\\n\\n4\\n\\n⊙\\n\\nP\\n\\n\\x0cExample 3.1. An example of a function in\\ngiven by\\n\\nP\\n\\nis the following function f : [0, 1]\\n\\n[0, 1]\\n\\n→\\n\\nf (x) := \\uf8f1\\n\\uf8f4\\uf8f2\\n\\n1\\n1\\n√2\\n1\\n√2 −\\n\\nx\\n√7\\n\\nif x < 1\\n2 ,\\nif x = 1\\n2 ,\\nif x > 1\\n2 .\\n\\n\\uf8f4\\uf8f3\\n\\nare satisﬁed.\\nIt is straightforward to check that all the conditions for functions to be in\\nNote that while we require the non-diﬀerentiable points to be rational numbers in [0, 1], it\\nis ﬁne for the values of the function at the non-diﬀerentiable points to be irrational. In our\\nexample here, the value of the function at the non-diﬀerentiable point 1/2 is 1/√2, which\\nis irrational. Moreover, it is also ﬁne for the slope of some piece of the function to be\\n1/√7, which\\nirrational. In our example here, the slope of the function when x\\nis irrational.\\n\\n(1/2, 1] is\\n\\n−\\n\\nP\\n\\n∈\\n\\nExample 3.2. Here we present a non-example. A function that does not belong to\\nfunction g : [0, 1]\\n\\n[0, 1] given by\\n\\nP\\n\\nis the\\n\\n→\\n\\ng(x) :=\\n\\n1\\n1\\n2\\n\\n(\\n\\n1\\n2 ,\\nif x\\n≤\\nif x > 1\\n2 .\\n\\n.\\n\\nP\\n\\nP\\n\\nNote that even though g(x) > 1\\nviolates the last condition for functions to belong to\\n\\nx for all x\\n\\n−\\n\\n∈\\n\\n(0, 1], the limit limx\\n\\n(1/2) g(x) = 1/2. This\\n\\nց\\n\\nThe following proposition gives some basic properties of functions in\\n\\n. These properties\\n\\ncan be proved immediately from the deﬁnition of\\n\\n, so we omit the proof.\\n\\nProposition 3.3. Let f\\n\\n. Then,\\n\\n(a) for every x\\n(b) for every a\\n(c) for every ε\\n\\n[0, 1], we have 0 < f (x)\\n≤\\na f (x) > 1\\n(0, 1], we have f (a) > 1\\n−\\n(0, 1], there exists δ > 0 such that for every x\\n\\n1.\\na and limx\\n\\nր\\n\\na.\\n[ε, 1], we have the\\n\\ninequality f (x) + x\\n\\n1 > δ.\\n\\nEach function f\\n\\ncomes with a useful quantity we call the loft of f deﬁned as follows.\\n\\n∈ P\\n\\n−\\n\\n∈\\n∈\\n∈\\n\\n∈ P\\n\\nDeﬁnition 3.4. For each function f\\n\\n, deﬁne the loft of f as\\n\\n∈ P\\n\\nloft(f ) := sup\\n\\nε\\n\\n[0, 1] :\\n\\n[0, ε], f (x) = 1, and\\n\\n(ε, 1], f (x) > 1\\n\\nx + ε\\n\\n.\\n\\n{\\n\\n∈\\n\\nx\\n\\n∀\\n\\n∈\\n\\nx\\n\\n∀\\n\\n∈\\n\\nThe following proposition gives some basic properties of the loft of a function in\\n\\n−\\n\\n}\\n\\n.\\n\\nP\\n\\nP\\n\\n−\\n∈\\n\\nProposition 3.5. Let f\\n\\n. Then,\\n\\n∈ P\\n\\n(a) its loft is strictly positive: 0 < loft(f )\\n≤\\nx\\n(b) for every real number x such that 0\\n(c) for every real number x such that loft(f )\\n(d) for every x\\n\\n[0, 1], we have\\n\\n≤\\n\\n1.\\n\\n≤\\n≤\\n\\n∈\\n\\nloft(f ), we have f (x) = 1.\\n1, we have f (x) + x\\nx\\n\\n≤\\n\\n1\\n\\n−\\n\\n≥\\n\\nloft(f ).\\n\\n≥\\nProof. Consider any function f\\n\\nx\\n\\nf (x) + x\\n\\n1\\n\\n−\\n\\n. Let\\n\\nmin\\n\\nx, loft(f )\\n}\\n≥\\ndenote the set from Deﬁnition 3.4:\\n\\n{\\n\\n.\\n\\n:=\\n\\nε\\n\\n{\\n\\n∈\\n\\nX\\n\\n[0, 1] :\\n\\nx\\n\\n∀\\n\\n∈\\n\\nx\\n\\n∀\\n\\n∈\\n\\n(ε, 1], f (x) > 1\\n\\nx + ε\\n\\n.\\n\\n−\\n\\n}\\n\\n∈ P\\n[0, ε], f (x) = 1, and\\n\\nX\\n\\n(a) It suﬃces to show that\\n\\n(0, 1]\\nthat f (a) = 1. By Proposition 3.3(c), there exists b > 0 such that for every x\\na, b\\nhave f (x) + x\\n\\n, there exists some a > 0 such\\n[a, 1], we\\n\\n(0, 1]. We claim that c\\n\\nX ∩\\n\\n∈ P\\n\\n∈\\n\\n= ∅. Since f\\n\\n−\\n\\n1 > b. Take c := min\\n{\\n[0, c], we have 1\\n≥\\na, then f (x) = 1 > 1\\n\\n} ∈\\nf (x)\\n\\n∈\\n(c, 1]. If c < x\\n\\n≥\\n\\nf (c)\\n\\n.\\nf (a) = 1 and so f (x) = 1. Second,\\nc\\n\\nx+c. If x > a, then f (x)+x\\n\\n1 > b\\n\\n∈ X\\n\\n≥\\n\\nFirst, for any x\\n\\nsuppose x\\nand thus f (x) > 1\\n\\n∈\\n\\n≤\\n\\n−\\n\\nx + c. This shows that c\\n5\\n\\n∈ X\\n\\n−\\n.\\n\\n−\\n\\n≥\\n\\n6\\n\\x0c(b) It suﬃces to show that f (loft(f )) = 1. If loft(f )\\n\\nthen for any positive integer n, there exists an ∈ X\\nloft(f ) > an, we have that\\n\\n∈ X\\nwith loft(f )\\n\\n, we are done. If loft(f ) /\\n\\n,\\n∈ X\\n1\\nn < an < loft(f ). Since\\n\\n−\\n\\nf (loft(f )) > 1\\n\\nloft(f ) + an > 1\\n\\n1\\nn\\n\\n.\\n\\n−\\n\\n−\\nSince n is arbitrary, we have f (loft(f )) = 1.\\n(c) This is similar to part (b). If loft(f )\\n\\n, we have f (x) > 1\\n\\nX\\nf (loft(f )) + loft(f )\\n\\n−\\n1 = loft(f ).\\n\\nx + loft(f ). If loft(f )\\n\\n∈ X\\n\\n∈ X\\n\\n−\\n\\nand x > loft(f ), then by the deﬁnition of\\n1 =\\n\\nand x = loft(f ), then f (x) + x\\n\\n−\\n\\nOn the other hand, if loft(f ) /\\n\\n∈ X\\n1\\nn < an < loft(f ). For every real number x\\n\\n, then for any positive integer n, there exists an ∈ X\\nloft(f ), we then have x > an,\\n\\nwith loft(f )\\nand thus\\n\\n−\\n\\nSince n is arbitrary, we have f (x)\\n\\n1\\n\\nx + loft(f ).\\n\\n(d) This part follows from parts (b) and (c).\\n\\n−\\n\\n≥\\n\\n−\\n\\nf (x) > 1\\n\\nx + an > 1\\n\\nx + loft(f )\\n\\n−\\n\\n≥\\n\\n1\\nn\\n\\n.\\n\\n−\\n\\n(cid:3)\\n\\nProposition 3.5(d) is an analytically useful property of the loft of a function in\\n\\n. It\\n[0, 1] is far enough from 0, the point (x, f (x)) on the graph of\\n\\nP\\n\\nsays roughly that once x\\n∈\\nthe function is far enough from the line X + Y = 1.\\n\\n∈ P\\n\\n.\\n3.2. An asymptotic formula for the number of rook placements for functions in\\nP\\nSuppose that a function f\\nis given. Let ρ1, ρ2, . . . , ρm be the non-diﬀerentiable points\\nof f inside the open interval (0, 1), listed in increasing order. (Here m is a non-negative\\ninteger. We use the convention that m = 0 if and only if f is diﬀerentiable on (0, 1), which\\nx < 1.) For convenience, we deﬁne ρ0 := 0 and ρm+1 := 1.\\nis when f (x) = 1 for all 0\\n≤\\n1, ρi). Deﬁne\\nNote that for each i\\n[m + 1], the function f is linear on the open interval (ρi\\nR to be the unique linear extension of f\\nthe function fi : [ρi\\n1, ρi)\\n1, ρi]. There exist a non-positive real number µi and a real number βi such that\\nto [ρi\\nfi(x) = µix + βi for x\\n\\n|(ρi−1,ρi) from (ρi\\n\\n∈\\n1, ρi]\\n\\n1, ρi].\\n\\n[ρi\\n\\n→\\n\\n−\\n\\n−\\n\\n−\\n\\n−\\n\\nNote that since we deﬁne fi on the closed interval [ρi\\n\\nhave diﬀerent values at ρi\\ninterior of the interval. Note also that f1(x)\\n\\n1, ρi], the functions fi and f might\\n1 and ρi. On the other hand, the two functions agree in the\\n1 (i.e., µ1 = 0 and β1 = 1).\\n\\nTake any positive integer N such that N ρi ∈\\n\\n≡\\nPar to be the partition with exactly N parts whose ith part is given by\\n\\nZ for every i. We deﬁne the partition\\n\\nλ(f, N )\\n\\n−\\n\\n−\\n\\n∈\\n\\n−\\n\\n(λ(f, N ))i :=\\n\\nN\\n\\n⌈\\n\\nf (i/N )\\n⌉\\n\\n.\\n\\n·\\n\\nOur deﬁnition of\\nwords, RP(λ(f, N )) is always non-empty.\\n\\n∈ DN ; in other\\nOur goal of this subsection is to compute an asymptotic formula for # RP(λ(f, N )) of\\n\\nguarantees that, as one may readily verify, λ(f, N )\\n\\nP\\n\\n∈\\n\\nthe form\\n\\nlog (# RP(λ(f, N ))) = Af ·\\nfor positive integers N such that N ρi ∈\\nthe implicit constant depends only on the function f .\\nBy Equation (1), we can write\\n\\nN log N + Bf ·\\nZ for every i. Here, the notation Of means that\\n\\nlog N + Of (1),\\n\\nN + Cf ·\\n\\n(2)\\n\\nlog (# RP(λ(f, N ))) =\\n\\nlog (N f (n/N ) + n\\n\\nN )\\n\\n+ R(f, N ),\\n\\n\\uf8eb\\n\\n\\uf8ed\\n\\nN\\nX0<n\\n≤\\n\\nwhere R(f, N ) is the discrepancy from rounding:\\n\\n(3)\\n\\nR(f, N ) :=\\n\\nN\\nX0<n\\n≤\\nProposition 3.6. We have R(f, N )\\n\\n⌈\\n\\nlog\\n\\nN f (n/N )\\n+ n\\n⌉\\nN f (n/N ) + n\\n\\n−\\n−\\n0 and R(f, N ) = Of (1).\\n\\n(cid:18)\\n\\nN\\nN\\n\\n≥\\n\\n6\\n\\n−\\n\\n\\uf8f6\\n\\n\\uf8f8\\n\\n.\\n\\n(cid:19)\\n\\n\\x0cProof. The ﬁrst item R(f, N )\\n0 is clear from Equation (3). We proceed to show that\\nR(f, N ) = Of (1). Notice that for 0 < n < ρ1N , we have f (n/N ) = 1. Therefore, we can\\nwrite\\n\\n≥\\n\\n(4)\\n\\nR(f, N ) = log\\n\\n⌈\\n\\nN f (ρ1)\\n+ ρ1N\\n⌉\\nN f (ρ1) + ρ1N\\n\\nN\\nN\\n\\n−\\n−\\n\\n+\\n\\nlog\\n\\n⌈\\n\\nN f (n/N )\\n+ n\\n⌉\\nN f (n/N ) + n\\n\\nN\\nN\\n\\n−\\n−\\n\\n.\\n\\n(cid:18)\\n\\n(cid:19)\\n(0, 1], by Proposition 3.3(c), there exists a > 0 such that for every x\\nlog(x) < 1/x,\\n\\n1 > a. Using this with the inequality log(\\n⌈\\n\\nXρ1N <n\\n≤\\n\\nx\\n⌉\\n\\n−\\n\\n−\\n\\n(cid:18)\\n\\nN\\n\\n)\\n\\n∈\\n∀\\n\\n(cid:19)\\n[ρ1, 1] we\\nx > 0, we\\n\\nSince ρ1 ∈\\nhave f (x) + x\\nobtain\\n\\n1\\n(f (n/N ) + (n/N )\\n\\nN\\n\\n1)\\n\\n−\\n\\nR(f, N ) <\\n\\n+\\n\\n+\\n\\n1\\nN (f (ρ1) + ρ1 −\\n1\\nN (f (ρ1) + ρ1 −\\n1\\nf (ρ1) + ρ1 −\\n\\n1)\\n\\n1)\\n\\n1\\nN\\n\\n≤\\n\\n=\\n\\nXρ1N <n\\n≤\\n\\nN\\n\\nN\\n\\nXρ1N <n\\n≤\\nρ1\\n1\\n−\\na\\n\\n.\\n\\n+\\n\\n·\\n1\\n\\n·\\n\\nN\\n\\na\\n\\n1 ·\\nSince ρ1 and a depend only on f (and not N ), the quantity above is Of (1).\\n\\n(cid:3)\\n\\nRecall that the function f is linear on each open interval (ρi\\n\\nthere might be a “jump.” For instance, in Example 3.1 above, the three values limx\\nf (ρ1), and limx\\njumps do not have a huge eﬀect on the summation in Equation (2):\\n\\n1, ρi), while, at each ρi,\\nρ1 f (x),\\nρ1 f (x) are all diﬀerent. It is not hard to see, however, that these possible\\n\\nր\\n\\nց\\n\\n−\\n\\n(5)\\n\\n(6)\\n\\n(9)\\n\\n(10)\\n\\n(11)\\n\\n(12)\\n\\nlog (N f (n/N ) + n\\n\\nN )\\n\\n−\\n\\nN\\n\\nX0<n\\n≤\\nm+1\\n\\n=\\n\\n\\uf8ee\\nXρi−1N <n\\n≤\\n\\uf8f0\\n\\uf8fb\\nCombining this with Equation (2) and Proposition 3.6, we obtain\\n\\ni=1\\nX\\n\\nρiN\\n\\n\\uf8f9\\n\\nlog((µi + 1)n + (βi −\\n\\n1)N )\\n\\n+ Of (1).\\n\\nm+1\\n\\n(7)\\n\\nlog (# RP (λ(f, N ))) =\\n\\n\\uf8ee\\nXρi−1N <n\\n≤\\n\\uf8f0\\nWe break the outer summation on the right-hand side above into when i = 1 and when\\ni\\n\\ni=1\\nX\\n\\nρiN\\n\\n\\uf8fb\\n\\n\\uf8f9\\n\\nlog((µi + 1)n + (βi −\\n\\n1)N )\\n\\n+ Of (1).\\n\\n≥\\n(8)\\n\\n2. When i = 1, we have, by Stirling’s formula,\\nlog((µi + 1)n + (βi −\\n\\nρiN\\n\\nXρi−1N <n\\n≤\\nN log N + (ρ1 log ρ1 −\\n= ρ1 ·\\n·\\nm + 1, observe that the function\\n\\nρ1)\\n\\n1)N ) = log((ρ1N )!)\\n\\nN +\\n\\nlog N + Of (1).\\n\\n1\\n2\\n\\nWhen 2\\n\\ni\\n\\n≤\\n\\n≤\\n\\nis well-deﬁned on the whole closed interval [ρi\\nmation formula (cf. [MV07, Appendix B]) with this function, we write\\n\\n1N, ρiN ]. Using the Euler-Maclaurin sum-\\n\\n−\\n\\nx\\n\\nlog((µi + 1)x + (βi −\\n\\n7→\\n\\n1)N )\\n\\nlog((µi + 1)n+ (βi −\\n\\n1)N ) =\\n\\nlog((µi + 1)x+ (βi −\\n\\n1)N )dx+ Of (1).\\n\\nρiN\\n\\nρi−1N\\n\\nZ\\n\\nXρi−1N <n\\n≤\\n\\nρiN\\n\\nBy the change of variables x\\n\\nN\\n\\nx′, we have\\n\\n·\\n\\n7→\\nlog((µi + 1)x + (βi −\\n\\n1)N )dx\\n\\nρiN\\n\\nρi−1N\\n\\nZ\\n\\n= (ρi −\\n\\nρi\\n\\n1)\\n\\n−\\n\\n·\\n\\nN log N +\\n\\nlog(f (x) + x\\n\\n1) dx\\n\\nN.\\n\\n−\\n\\n) ·\\n\\nρi\\n\\n(Z\\n\\nρi−1\\n\\n7\\n\\n\\x0cThe following is the main theorem of this section.\\n\\nTheorem 3.7. Let f\\n\\n. Let ρ0, ρ1, . . . , ρm+1 be as deﬁned above. We have\\n\\n∈ P\\n\\nlog (# RP(λ(f, N ))) = N log N + Bf ·\\n\\nlog N + Of (1),\\n\\n1\\n2\\nZ for every i, where\\n\\nN +\\n\\nfor positive integers N such that N ρi ∈\\n\\nNote that the integral is improper at x = 0. We interpret it as\\n\\n1\\n\\n0\\nZ\\n\\n1\\n\\nBf =\\n\\nlog(f (x) + x\\n\\n1) dx.\\n\\n−\\n\\n−\\n\\nlog(f (x) + x\\n\\n1) dx.\\n\\nlim\\n0\\nε\\nց\\n\\nε\\nZ\\n\\nProof of Theorem 3.7. This result is immediate from combining Equations (8)-(12) and ob-\\n(cid:3)\\n1) dx = limε\\nserving that limε\\n\\nlog(f (x) + x\\n\\nρ1.\\n\\n0\\n\\n0\\n\\nρ1\\nε\\n\\nց\\n\\nlog(x) dx = ρ1 log ρ1 −\\n\\nρ1\\nε\\n\\nց\\n\\n−\\n\\nR\\n\\n3.3. Properties of Bf . Let p\\nnorm of functions in\\nby (\\nfunctions f, g\\nfunctions.\\n\\n∈ P\\n\\nP\\n\\nP\\n\\nk\\n\\nf\\n\\n−\\n\\n≥\\n, we make the class\\n\\n, Lp). A technical remark is that in the construction of (\\ng\\n\\nfor which\\n\\nR\\n\\n1 be any positive real number. By considering the Lp-\\na metric space. Let us denote this metric space\\n, Lp), we identify any two\\n, Lp) is an equivalence class of\\n\\nkp = 0. An element in (\\nP\\n\\nP\\n\\nP\\n\\nNevertheless, it is easy to see that the map f\\n, Lp)\\nLet (R, Euclid) denote the set of real numbers equipped with the usual Euclidean metric.\\nWe have the following topological property of B, when considered as a function from (\\n, Lp)\\nto (R, Euclid).\\nProposition 3.8. The map B : (\\n\\nBf induces a well-deﬁned map\\nR.\\n\\n(R, Euclid) is discontinuous everywhere on\\n\\nB : (\\n\\n, Lp)\\n\\n7→\\n\\n→\\n\\nP\\n\\nP\\n\\n.\\n\\nP\\n\\n→\\n\\nbe an arbitrary function. For each positive integer n such that n−\\n\\nP\\n1 +\\n\\nProof. Let f\\n2−\\n\\nn2\\n\\n< loft(f ), deﬁne\\n\\n∈ P\\n\\nObserve that gn converges to f in Lp, as n\\nhave\\n\\n→ ∞\\n\\n. However, by the triangle inequality, we\\n\\ngn(x) :=\\n\\nf (x)\\n1\\n\\n−\\n\\n(\\n\\nx + 2−\\n\\nn2\\n\\nif x\\n1\\n≤\\nif x > 1\\n\\n1\\nn ,\\n1\\nn .\\n\\n−\\n−\\n\\nBgn |\\n\\n|\\n\\n=\\n\\nlog(gn(x) + x\\n\\nI1| − |\\n\\nI2| − |\\n\\nI3|\\n\\n,\\n\\n≥ |\\n\\n1\\n\\n0\\nZ\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nI1 :=\\n\\n−\\n\\n1) dx\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\nlog(gn(x) + x\\n\\n1) dx,\\n\\n−\\n\\nI2 :=\\n\\nlog(f (x) + x\\n\\n1) dx,\\n\\n1\\n\\n1\\nZ\\n\\n−\\n\\n1\\nn\\n1\\n\\n0\\n\\nZ\\n\\n1\\n\\n−\\n\\n−\\n\\nwhere\\n\\nand\\n\\nI3 :=\\n\\nlog(f (x) + x\\n\\n1) dx.\\n\\n1\\nZ\\nNote that\\nBf |\\n= (log 2)\\nof functions converging to f in Lp, but\\n\\nI2|\\n\\nI1|\\n\\nn,\\n\\n=\\n\\n|\\n\\n|\\n\\n·\\n\\n|\\n\\n1\\nn\\n−\\n, and\\n\\nI3| →\\n|\\nBgn | → ∞\\n\\n|\\n\\n0, as n\\n, as n\\n\\n→ ∞\\n.\\n→ ∞\\n\\n. Hence,\\n\\ngn}\\n\\n{\\n\\nis a sequence\\n(cid:3)\\n\\nThe following proposition is clear from the integral formula of Bf .\\n, we have Bf ≤ −\\n\\n(a) For every function f\\n\\nis tight. The equality is attained if and only if f (x) = 1 for every x\\n\\nProposition 3.9.\\n\\n∈ P\\n\\n(b) For any functions f, g\\n\\nsuch that\\n\\n[0, 1], f (x)\\n\\n1. The upper bound\\n[0, 1).\\ng(x), we have Bf ≥\\n\\nBg.\\n\\n∈\\n\\n≥\\n\\n∈ P\\n\\nx\\n\\n∀\\n\\n∈\\n\\n8\\n\\n\\x0c4. The class\\n\\nP\\n\\nof piecewise linear functions\\nPk to be the class of functions f\\ne\\nf (i/k)\\n\\nZ,\\n\\nFor each positive integer k, deﬁne\\n\\ne\\nthe following additional properties:\\n[k], we have k\\n\\n•\\n•\\n•\\n\\n∈\\n\\nfor each i\\nf is upper-semicontinuous, and\\nfor each i\\nslope.\\n\\n[k], the restriction f\\n\\n∈\\n\\n·\\n\\n∈\\n\\n|((i\\n−\\n\\n1)/k,i/k) is linear with a non-positive integer\\n\\nwhich satisfy\\n\\n∈ P\\n\\na\\n\\nPk. One important property about function f\\nWe let\\n:=\\n(0, 1], we have limx\\nր\\ne\\n\\na f (x) = f (a).\\n\\n∞k=1\\n\\nP\\n\\n∈\\n\\nP\\n\\ne\\n\\n∈\\n\\ne\\n\\nS\\n4.1. Bijective Combinatorics in\\nsize has a nice product formula.\\n\\nPk. It is easy to see that\\ne\\n\\nPk is a ﬁnite set. In fact, its\\ne\\n\\n2, the sizes of the following sets are equal:\\n\\nProposition 4.1. For each positive integer k\\n\\nis that for every\\n\\n3)-tuples (y1, y2, . . . , y2k\\n\\n3) of non-negative integers such that for\\n\\n•\\n•\\n\\nthe class\\nthe set\\neach m\\n\\nPk,\\nTk of (2k\\n[2k\\ne\\n−\\n∈\\n\\n−\\n3], we have\\n\\n≥\\n\\n−\\n\\n•\\n\\nthe set\\nhalf-plane\\n\\ny1 + y2 +\\n\\n+ ym ≤\\nGk of lattice paths from (0, 0) to (2k\\n.\\n2y\\n}\\n\\n(x, y) : x\\n\\n· · ·\\n\\n≥\\n\\n{\\n\\nl\\n−\\n\\n,\\n\\nm\\n2\\nm\\n1, k\\n\\n−\\n\\nProof. We construct the following bijections.\\n\\nthat 1\\n\\nFirst,\\n\\nPk → Tk. Given f\\n1, let\\nh\\n≤\\ne\\n\\n−\\n\\nk\\n\\n≤\\n\\nand for each ℓ such that 1\\n\\n∈\\n\\nPk, we deﬁne (y1, y2, . . . , y2k\\ne\\n1 := k\\n\\nf (x)\\n\\nf\\n\\n−\\n\\nh + 1\\nk\\n\\n,\\n\\n(cid:19)(cid:19)\\n\\n−\\n\\n(cid:18)\\n\\ny2h\\n\\n−\\n\\nℓ\\n\\nk\\n\\n≤\\n\\n≤\\n\\n−\\n\\n·\\n\\nh/k\\n\\nlim\\nx\\n(cid:18)\\nց\\n2, let\\n\\ny2ℓ := k\\n\\nℓ + 1\\nk\\n\\nf\\n(cid:18)\\n\\nSecond,\\n\\n−\\nTk → Gk. Send the tuple (y1, . . . , y2k\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n·\\n\\n−\\n\\nx\\n\\nց\\n3)\\n\\nE2N y1EN y2EN y3\\n\\nEN y2k−3EN k\\n\\nlim\\n(ℓ+1)/k\\n\\nf (x)\\n\\n.\\n\\n(cid:19)\\n\\n∈ Tk to the path\\ny2−···−\\ny1−\\n\\n−\\n\\n−\\n\\n1\\n\\ny2k−3,\\n\\n· · ·\\n\\nwhere E denotes the step (1, 0) and N denotes the step (0, 1).\\n\\nCorollary 4.2. For every positive integer k, we have\\n\\n|\\nNote that this sequence appears as A006013 on the OEIS [OEI].\\n\\n(cid:19)\\n\\n(cid:18)\\n\\ne\\n\\n=\\n\\nPk|\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n−\\n1\\n−\\n\\n.\\n\\n1) which are contained in the\\n\\n3) as follows. For each h such\\n\\n(cid:3)\\n\\nProof of Corollary 4.2. With Proposition 4.1, it suﬃces to show that\\nis well-known: see, for example, the sequence\\nSection 3], or references in A006013 on the OEIS [OEI].\\n\\n. This\\n|Gk|\\nin the work of Gessel and Xin [GX06,\\n(cid:3)\\n\\nbn}\\n\\n{\\n\\n(cid:0)\\n\\n(cid:1)\\n\\n= 1\\nk\\n\\n3k\\nk\\n\\n2\\n−\\n1\\n−\\n\\nIn the following discussion, by a lattice path, we mean the image of an injective continuous\\nR2 (under the usual Euclidean topology for both spaces) that is also a ﬁnite\\n\\nfunction [0, 1]\\nunion of segments s1, . . . , sn such that both end points of each si are lattice points.\\n\\n→\\n\\nFunctions in\\n\\nvertical segments. Formally, suppose a function f\\nof f into the square [0, k]\\n\\nPk can be seen as lattice paths, by dilating their graphs and then adding\\nPk is given. We ﬁrst dilate the graph\\ne\\ne\\n\\n[0, k] by\\n\\n×\\n\\n∈\\n\\nΓf :=\\n\\n(x, y)\\n\\nR\\n\\nR :\\n\\n∈\\n\\n×\\n\\nn\\n\\n= f\\n\\ny\\nk\\n\\n9\\n\\nx\\nk\\n\\n(cid:16)\\n\\n(cid:17)o\\n\\n[0, k]\\n\\n[0, k].\\n\\n⊆\\n\\n×\\n\\n\\x0cThen, we take the closure Γf of Γf with respect to the usual Euclidean topology on R2.\\nNote that the closure simply adds a ﬁnite number of points into the set Γf . Then, our path\\nPath(f ) is given by\\n\\nR2\\n\\n|\\n\\ny\\n\\n∈\\n\\n≤\\n\\nPath(f ) :=\\n\\nFor any f\\ndeﬁne\\n\\nthere exist y1, y2 such that y1 ≤\\n\\n.\\n(x, y)\\nPk, the path Path(f ) is a lattice path with endpoints (0, k) and (k, 0). Let us\\n(cid:9)\\n(cid:8)\\n∈\\nPk).\\nLk := Path(\\ne\\nThe map Path :\\ne\\nconsider the map Func :\\n\\nPk → Lk is a bijection. To go back from lattice paths to functions,\\ne\\n\\ny2 and (x, y1), (x, y2)\\n\\nPk given by\\nLk →\\n(Func(γ)) (x) := sup\\n∈\\ne\\n∈ Lk. This map simply shrinks the path back and then removes\\nLk as going from (0, k) to (k, 0), then they are exactly the lattice\\n\\ny : (kx, ky)\\n{\\n\\nfor any x\\nvertical segments. It is straightforward to see that Path and Func are inverses.\\n\\nIf we think of paths in\\n\\n[0, 1], for any γ\\n\\nΓf\\n\\n∈\\n\\n∈\\n\\nγ\\n\\n}\\n\\n,\\n\\npaths γ with the following properties:\\n\\nthe path γ starts at (0, k) and ends at (k, 0),\\neach step is either (1,\\n0 or (0,\\nthe path γ intersects with the diagonal X + Y = k exactly at its two endpoints.\\n\\nℓ) for ℓ\\n\\n1),\\n\\n−\\n\\n−\\n\\nZ\\n\\n∈\\n\\n≥\\n\\nThis class\\n\\nLk of lattice paths contains many familiar paths in algebraic combinatorics\\nsuch as Dyck paths and Motzkin paths. It is also closely related to plane S-trees, paren-\\nthesizations, and dissections of a convex polygon. See Stanley’s text [Sta99, Chapter 6] for\\ndetails.\\n\\nFor the following discussion, a Dyck path from (0, k) to (k, 0) is a lattice path starting\\n1), and ending at (k, 0) that never crosses (but might\\nfrom (0, k), using steps (1, 0) and (0,\\ntouch) the line X + Y = k. We let Dyck(k) denote the set of Dyck paths from (0, k) to\\n(k, 0).\\n\\n−\\n\\nSince paths in\\n\\n(0, k) and (k, 0), the Dyck paths in\\n(k, 1). Note that Dyck paths in\\nfunctions in\\n.\\ninto\\n\\nLk can intersect with the line X + Y = k only at the two endpoints\\nLk are in bijection with the Dyck paths from (1, k) to\\nPk) to piecewise constant\\nLk →\\nPk. We have thus obtained one trivial embedding of a Catalan-numerous family\\ne\\ne\\n\\nLk correspond (under Func :\\n\\nProposition 4.3. For each positive integer k, the number of piecewise constant functions\\nin\\n\\nP\\n\\ne\\n\\nPk is exactly the (k\\ne\\n\\n−\\n\\n1)st Catalan number\\n1\\nk\\n\\n1 =\\n\\nCk\\n\\n−\\n\\n2k\\nk\\n\\n2\\n−\\n1\\n−\\n\\n.\\n(cid:19)\\n\\n(cid:18)\\n\\nSimilarly, we have a Motzkin-numerous class of functions in\\n\\nas follows.\\n\\nP\\n\\nProposition 4.4. For each positive integer k\\nclass of all functions f\\n\\n2, deﬁne the subset\\ne\\n≥\\nPk which satisfy the following conditions:\\ne\\n\\n1,\\n(0, 1) of f , we have\\n\\neach linear piece of f either is constant or has slope\\nfor each non-diﬀerentiable point a\\n\\n−\\n\\n∈\\n\\nMOk ⊆\\n\\nPk to be the\\ne\\n\\n∈\\nf (x)\\n\\nk\\n\\nf (a)\\n\\nk\\n\\n·\\n\\n≡\\n\\n≡\\n·\\nf (1) is an even integer.\\n\\nlim\\na\\nx\\nց\\n\\nk\\n\\n(1\\n\\na)\\n\\n(mod 2),\\n\\n·\\n\\n−\\n\\nthe number k\\n\\n·\\n\\nThen, the size of\\nMotzkin numbers, we recommend Stanley’s text [Sta99, Exercises 6.37 and 6.38].)\\n\\nMOk is the (k\\n\\n2. (For more details about the\\n\\n2)nd Motzkin number Mk\\n\\n−\\n\\n−\\n\\nIf we drop the last two conditions about parity, we obtain Schr¨oder numbers.\\n\\nProposition 4.5. For each positive integer k, deﬁne the subset\\nof all functions f\\nThen, the size of\\nSchr¨oder numbers, we recommend Stanley’s text [Sta99, Section 6.2 and Exercises 6.39].)\\n10\\n\\nPk such that each linear piece of f either is constant or has slope\\ne\\n\\nPk to be the class\\n1.\\n−\\ne\\n1. (For more details about the\\n\\n1)st Schr¨oder number rk\\n\\n∈\\nSCk is the (k\\n\\nSCk ⊆\\n\\n−\\n\\n−\\n\\n•\\n•\\n•\\n\\n•\\n•\\n\\n•\\n\\n\\x0cThere is another embedding of a Catalan-numerous family in\\n\\n. The following proposi-\\n\\ntion is observed and proved by Alex Postnikov.\\n\\nProposition 4.6. Let k be a positive integer. The number of continuous functions in\\nis exactly the kth Catalan number\\n\\nP\\n\\ne\\n\\nPk\\n\\ne\\n\\nCk =\\n\\n1\\nk + 1\\n\\n2k\\nk\\n\\n.\\n(cid:19)\\n\\n(cid:18)\\n\\nProof. We construct an explicit bijection from the set of continuous functions in\\nPk to\\nDyck(k), the set of Dyck paths from (0, k) to (k, 0). Note that for each continuous function\\ne\\nf\\n[0, k], is a continuous lattice path from (0, k)\\nPk, the graph of f , after dilating to [0, k]\\nk\\ni=1 γi,\\nto (k, 0) without a vertical step. We can write this path as the union of k segments\\ne\\nwhere\\n\\n×\\n\\n∈\\n\\nγi :=\\n\\ni\\n(cid:20)(cid:18)\\n\\n−\\n\\n1, k\\n\\nf\\n\\n·\\n\\ni\\n\\n1\\n\\n−\\nk\\n\\n,\\n\\ni, k\\n\\nf\\n\\n·\\n\\ni\\nk\\n\\n(cid:18)\\nReplace each segment γi with an L-shaped broken segment with the same endpoints:\\ni\\nk\\n\\n(cid:19)(cid:19)(cid:21)\\n\\n1, k\\n\\n1, k\\n\\n(cid:19)(cid:19)\\n\\ni, k\\n\\n−\\nk\\n\\ni\\nk\\n\\ni\\nk\\n\\n(cid:18)\\n\\n(cid:18)\\n\\n1\\n\\nf\\n\\nf\\n\\nf\\n\\nf\\n\\ni\\n\\n,\\n\\n,\\n\\n−\\n\\n−\\n\\n∪\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n(cid:19)(cid:19)\\n\\n(cid:18)\\nk\\ni=1 γ′i is the desired Dyck path in Dyck(k).\\n\\n(cid:19)(cid:19)(cid:21)\\n\\n(cid:18)\\n\\n(cid:18)\\n\\n(cid:19)(cid:19)\\n\\n(cid:18)\\n\\n1, k\\n\\nγ′i :=\\n\\ni\\n(cid:20)(cid:18)\\nThen, the union\\n\\n−\\n\\ni\\n(cid:20)(cid:18)\\n\\ni\\n(cid:18)\\n\\n.\\n\\n(cid:18)\\n\\n(cid:19)(cid:19)(cid:21)\\n(cid:3)\\n\\nS\\n\\nRemark 4.7. From Proposition 4.6, we quickly obtain yet another Catalan-numerous fam-\\n1)st\\nily. The number of continuous functions f\\nCatalan number Ck\\n1. This is because under the bijection in the proof of Proposition 4.6,\\nthese functions become Dyck paths in Dyck(k) that visit (k\\n\\nPk for which f (1) = 1/k is the (k\\ne\\n\\n1, 1).\\n\\n−\\n\\n∈\\n\\n−\\n\\nS\\n\\n−\\n\\nThe bijection in the proof of Proposition 4.6 that sends continuous functions to Dyck\\npaths might be extended to the whole\\nPk. The image of the extended map can be understood\\nas Dyck paths with certain marks on vertical segments. These are combinatorial objects\\nwhich we call waterfalls.\\ne\\n\\nDeﬁnition 4.8. A waterfall of size k is a Dyck path γ\\nDyck(k) together with a choice of\\ncoloring of every unit segment in γ so that each segment is colored one of either green or\\nblue with the following rules:\\n\\n∈\\n\\n•\\n•\\n•\\n•\\n\\nevery horizontal segment is colored blue,\\nevery vertical segment on the line x = k is colored green,\\nevery vertical segment with an endpoint on the line x + y = k is colored green, and\\nif s1 and s2 are vertical segments such that s1 is immediately above s2 and s2 is\\ncolored blue, then s1 must also be colored blue.\\n\\nLet WT(k) denote the set of waterfalls of size k. From our discussion above, we have\\n\\nWe obtain the following curious combinatorial formula.\\n\\nWT(k)\\n|\\n\\n|\\n\\n=\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n−\\n1\\n−\\n\\n.\\n(cid:19)\\n\\n(cid:18)\\n\\nProposition 4.9. Let k be a positive integer. For each Dyck path D\\ndeﬁne the weight of D to be\\n\\n∈\\n\\nDyck(k), let us\\n\\nThen,\\n\\nwt(D) :=\\n\\n#\\n\\nj\\n\\n{\\n\\n∈\\n\\nZ\\n\\n|\\n\\ni + j > k and (i, j)\\n\\nD\\n\\n.\\n\\n}\\n\\n∈\\n\\nk\\n\\n1\\n\\n−\\n\\ni=1\\nY\\n\\nwt(D) =\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n−\\n1\\n−\\n\\n.\\n(cid:19)\\n\\n(cid:18)\\n\\nDyck(k)\\n\\nXD\\n∈\\n\\nProof. Note that the weight wt(D) is the number of waterfalls whose underlying Dyck paths\\nDyck(k) wt(D) counts the total number of waterfalls in WT(k). (cid:3)\\nare D. Therefore,\\n\\nD\\n\\n∈\\n\\nP\\n\\n11\\n\\n\\x0c(14)\\n\\n(15)\\n\\n(16)\\n\\n(17)\\n\\n4.2. A precise asymptotic formula for the number of rook placements for func-\\ntions in\\nPk. The goal of this subsection is to compute a precise asymptotic formula for\\n# RP(λ(f, N )), for each f\\ne\\n\\nlog (# RP(λ(f, N ))) = Af ·\\n{\\n\\nfor positive integers N\\nAf , Bf , and Cf are the same as before.\\n\\nN log N + Bf ·\\nk, 2k, 3k, . . .\\n}\\n\\n∈\\n\\n∈\\n\\nPk, of the form\\ne\\nkZ :=\\n\\nN + Cf ·\\n\\n. Since\\n\\nlog N + Df + Of (1/N ),\\n\\n, the quantities\\n\\nIn this subsection, we redeﬁne our notations µi and βi. These notations now have slightly\\n[k],\\nPk and for any i\\n∈\\n1)/k, i/k]. Let µi and βi\\ne\\n\\ndiﬀerent meanings from what they meant in Subsection 3.2. For f\\nwe know that f is a linear function on the half-open interval ((i\\nbe such that for x\\nBecause f\\n\\nFurthermore, the eﬀect from jumps (as in Equation (6)) is also zero. Therefore, for f\\nand for any positive integer N\\n\\nPk, the discrepancy from rounding, R(f, N ) (as in Proposition 3.6), is zero.\\nPk\\ne\\n\\n1)/k, i/k], we have f (x) = µix + βi.\\n\\nkZ, we have\\n\\n((i\\n\\n−\\n\\n−\\n\\n∈\\n\\n∈\\n\\n∈\\n\\n∈\\n\\nPk is a subclass of\\ne\\n\\nP\\n\\n∈\\n\\nk\\n\\ne\\n\\n(13)\\n\\nlog (# RP(λ(f, N ))) =\\n\\nlog((µi + 1)n + (βi −\\n\\n1)N ).\\n\\ni−1\\nk N <n\\nX\\nOnce again, we break the outer summation on the right-hand side above into when i = 1\\nand when i\\n\\n2. When i = 1, we have, by Stirling’s formula,\\n\\ni=1\\nX\\n\\ni\\nk N\\n\\n≤\\n\\n≥\\n\\nN\\nk\\n\\nX0<n\\n≤\\n1\\nk ·\\n\\n=\\n\\nlog((µi + 1)n + (βi −\\n\\n1)N ) =\\n\\nlog(n) = log((N/k)!)\\n\\nX0<n\\n≤\\n\\nN\\nk\\n\\nN log N\\n\\nN +\\n\\nlog N +\\n\\nlog\\n\\n+ O(k/N ),\\n\\n(log k) + 1\\nk\\n\\n·\\n\\n1\\n2\\n\\n1\\n2\\n\\n2π\\nk\\n\\n(cid:18)\\n\\n(cid:19)\\n\\nWhen 2\\n\\ni\\n\\nk, we use the Euler-Maclaurin summation formula (cf.\\n\\n[MV07, Appen-\\n\\nfor positive integers N\\n\\n≤\\ndix B]) to obtain\\n\\n≤\\n\\n−\\nkZ.\\n\\n∈\\n\\n=\\n\\nN log N +\\n\\nlog(f (x) + x\\n\\n1) dx\\n\\nN\\n\\nlog((µi + 1)n + (βi −\\n\\n1)N )\\n\\ni−1\\nk N <n\\nX\\n\\n≤\\n\\ni\\nk N\\n\\n1\\nk ·\\n\\ni/k\\n\\n1)/k\\n\\ni\\n\\n(Z\\n(i\\n−\\n(µi + 1)\\n(µi + 1)\\n\\n−\\n\\n) ·\\n\\n+ Of (1/N ).\\n\\n(18)\\n\\n1)\\nk + (βi −\\n1) !\\nk + (βi −\\nCombining these terms, we obtain the following theorem.\\n\\n1\\n2 ·\\n\\n·\\ni\\n−\\n\\nlog\\n\\n \\n\\n+\\n\\n1\\n\\n·\\n\\nTheorem 4.10. Let k be a positive integer. Let f\\n\\nlog (# RP(λ(f, N ))) = N log N + Bf ·\\n\\nfor positive integers N\\n\\nkZ, where Bf is as given in Theorem 3.7, and\\n\\n∈\\nN +\\n\\nPk. We have\\n1\\ne\\n2\\n\\nlog N + Df + Of (1/N ),\\n\\n∈\\n\\nDf :=\\n\\nlog(2π) +\\n\\n1\\n2\\n\\n1\\n\\n1\\n2\\n\\n0\\nZ\\n\\nxf ′(x)\\nx(f (x) + x\\n\\nf (x) + 1\\n1)\\n\\n−\\n\\ndx.\\n\\n−\\n\\nNote that one has to be careful about the integral in the formula of Df . Since in general\\nf has a number of non-diﬀerentiable points, the derivative f ′ might be undeﬁned for some\\nvalues of x. By the integral as expressed, we mean\\n\\n1\\n\\n0\\nZ\\n\\nxf ′(x)\\nx(f (x) + x\\n\\nf (x) + 1\\n1)\\n\\n−\\n\\ndx =\\n\\n−\\n\\nk\\n\\ni/k\\n\\ni=1 Z\\nX\\n\\n(i\\n\\n1)/k\\n\\n−\\n\\nxf ′(x)\\nx(f (x) + x\\n\\nf (x) + 1\\n1)\\n\\n−\\n\\ndx.\\n\\n−\\n\\nSince f is linear in ((i\\n\\n−\\n\\n1)/k, i/k), the sum of integrals on the right-hand side is well-deﬁned.\\nIn the following examples, we\\n\\nAn illustration of Theorem 4.10 is given in Figure 1.\\ncompute explicit asymptotic formulas for certain functions.\\n\\n12\\n\\n\\x0cExample 4.11. Suppose that k is a positive integer. Let f : [0, 1]\\n\\n[0, 1] be given as\\n\\n→\\n\\n→\\n\\n(cid:0)\\n\\nN\\n\\n(cid:17)\\n\\nfor all x\\n∈\\nTherefore,\\n\\n[0, 1]. Note that f\\n\\n1\\n\\nk and Df = 1\\n\\n2 log(2π/k).\\n\\n1\\nk −\\n\\nf (x) := min\\n\\n1 +\\n\\nx, 1\\n\\n,\\n\\n(cid:26)\\nPk. We have Bf =\\ne\\n\\n∈\\n\\n(cid:27)\\n\\nlog k\\n\\n−\\n\\n−\\n\\n2π\\nk ·\\n\\n∼ r\\n\\nN N + 1\\n\\n2\\n\\n(k−\\n\\n1e−\\n\\n1/k)N ,\\n\\n·\\n\\n# RP(λ(f, N ))\\n\\nas N\\n\\n, N\\n\\n→ ∞\\n\\n∈\\n\\nkZ.\\n\\nExample 4.12. Suppose that k is a positive integer. Let f : [0, 1]\\n\\n[0, 1] be given as\\n\\nf (x) := min\\n\\n1 +\\n\\n(cid:26)\\n\\n2\\n\\n− ⌈\\nk\\n\\nkx\\n⌉\\n\\n, 1\\n\\n,\\n\\n(cid:27)\\n\\n−\\n\\nfor all x\\n1\\nDk =\\n\\n[0, 1]. Note that f\\n2 log 2 + 1\\n\\n2 log k + k\\n\\n∈\\n2 log π. Therefore,\\n\\n∈\\n\\nPk. We have Bf =\\ne\\n\\n−\\n\\nlog k +\\n\\n2\\n\\nlog 2\\n\\n1 and\\n\\n−\\n\\n2\\nk\\n\\n−\\n\\n(cid:1)\\n\\n# RP(λ(f, N ))\\n\\nN N + 1\\n\\n2\\n\\n22\\n\\n−\\n\\n2/k k−\\n\\n1 e−\\n\\n1\\n\\n,\\n\\n2kπ\\nk ·\\n\\n∼ r\\n\\n·\\n\\n(cid:16)\\n\\nas N\\n\\n, N\\n\\n→ ∞\\n\\n∈\\n\\nkZ.\\n\\nAs another application of our result, we can detect the number of ground bumps of Dyck\\nDyck(k) is a Dyck path from (0, k) to (k, 0), then a ground bump\\npaths analytically. If D\\nof γ is an intersection between γ and the open line segment from (0, k) to (k, 0). Recall\\nthat a partition λ\\nIn the\\nfollowing proposition, a ground bump of λ is deﬁned as a ground bump of the Dyck path\\ncorresponding to λ.\\n\\n∈ Dk can be thought of as a Dyck path from (0, k) to (k, 0).\\n\\n∈\\n\\nProposition 4.13. Let λ be any nonempty partition such that RP(λ)\\nexist positive real numbers α1, α2, α3, α4 > 0 such that\\n\\n= ∅. Then, there\\n\\n⊙\\n. Furthermore, α1 = λ1 and\\n\\n∼\\n\\n# RP(N\\n\\nλ)\\n\\nN α1N +α2\\n\\nαN\\n\\n3 ·\\n\\n·\\n\\nα4,\\n\\nas N\\n\\n→ ∞\\n\\nα2 =\\n\\n(#ground bumps of λ) + 1\\n2\\n\\n.\\n\\nProof. Since RP(λ)\\nas a concatenation λ = λ(1)\\nground bumps. Observe that\\n\\n∗\\n\\n= ∅, we have that λ\\n\\nλ(2)\\n\\n∗ · · · ∗\\n\\n∈ Dλ1 . As a Dyck path, λ can be uniquely written\\nλ(p) such that each λ(i) is a Dyck path without\\n\\n(19)\\n\\n(20)\\n\\nand that\\n\\n# RP(N\\n\\nλ) =\\n\\n# RP(N\\n\\n⊙\\n\\nλ(i)),\\n\\n⊙\\n\\np\\n\\ni=1\\nY\\n\\np\\n\\ni=1\\nX\\n\\nλ1 =\\n\\nλ(i)\\n1 ,\\n\\nwhere λ(i)\\n\\n1 denotes the ﬁrst part of the partition λ(i).\\n\\nFor each i\\n\\n[p], there is a unique corresponding function f (i)\\n\\n. We have that for\\n\\nany positive integer N ,\\n\\n∈\\n\\nPλ(i)\\n\\n1\\n\\n∈\\n\\ne\\n\\n# RP(N\\n\\n⊙\\n\\nλ(i)) = # RP(λ(f (i), N λ(i)\\n\\n1 )).\\n\\n13\\n\\n6\\n6\\n\\x0c√2π\\n\\nN N +1/2\\n\\n(e−\\n\\n1)N\\n\\n∼\\n\\n·\\n\\n·\\n\\n4\\n3 π\\n\\n2\\n3 π\\n\\n8\\n3 π\\n\\n4\\n3 π\\n\\n4\\n3 π\\n\\n2\\n3 π\\n\\n∼\\n\\nq\\n\\n∼\\n\\nq\\n\\n∼\\n\\nq\\n\\n∼\\n\\nq\\n\\n∼\\n\\nq\\n\\n∼\\n\\nq\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\nN\\n\\nN\\n\\nN\\n\\n(cid:1)\\n\\n(cid:1)\\n\\n(cid:1)\\n\\nN\\n\\nN\\n\\n(cid:1)\\n\\n(cid:1)\\n\\nN N +1/2\\n\\n21 3−\\n\\n1 e−\\n\\n2/3\\n\\nN N +1/2\\n\\n24/3 3−\\n\\n1 e−\\n\\n1\\n\\nN N +1/2\\n\\n24/3 3−\\n\\n1 e−\\n\\n1\\n\\nN N +1/2\\n\\n22/3 3−\\n\\n1 e−\\n\\n2/3\\n\\nN N +1/2\\n\\n22/3 3−\\n\\n1 e−\\n\\n2/3\\n\\nN N +1/2\\n\\n3−\\n\\n1 e−\\n\\n1/3\\n\\nN\\n\\n(cid:1)\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n·\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(22)\\n\\n(24)\\n\\nFigure 1. The seven functions in the class\\nasymptotic formulas for # RP(λ(f, N )), as N\\n\\nP3 and their corresponding\\n3Z.\\n→ ∞\\ne\\n\\n, N\\n\\n∈\\n\\n(Note that the notation λ on the right-hand side of the equation above is an operator, not\\na partition.) Now, Theorem 4.10 gives\\n\\n(21)\\n\\nlog(# RP(N\\n\\n⊙\\n\\n1 N log N + (λ(i)\\nλi)) = λ(i)\\n1\\n2\\n\\nlog λ(i)\\n\\n+\\n\\n1 + Df (i) + Oλ(1/N ).\\n\\n1 log λ(i)\\n\\n1 )N + Bf (i) λ(i)\\n\\n1 N +\\n\\nlog N\\n\\n1\\n2\\n\\nCombining Equations (19), (20), and (21), we obtain\\n\\n(23)\\n\\nlog(# RP(N\\n\\nλ)) = λ1N log N +\\n\\n⊙\\n\\np\\n\\ni=1 (cid:16)\\nX\\np\\n1\\n2\\n\\ni=1 (cid:18)\\nX\\n\\nλ(i)\\n1 log λ(i)\\n\\n1 + Bf (i) λ(i)\\n\\ni\\n\\nN\\n\\n·\\n\\n(cid:17)\\n\\nlog λ(i)\\n\\n1 + Df (i)\\n\\n+ Oλ(1/N ),\\n\\n(cid:19)\\n\\n+\\n\\nlog N +\\n\\np\\n2\\n\\nfor positive integers N . Since p\\nthe proof.\\n\\n1 is the number of ground bumps of λ, we have ﬁnished\\n(cid:3)\\n\\n−\\n\\n4.3. Properties of Df . Theorem 4.10 gives an integral formula for Df . In applications,\\nit is also useful to have the following formula, which is immediate from Equations (15) and\\n(17).\\n\\nProposition 4.14. For any function f\\n1\\n2\\n\\nlog(2πf (1)) +\\n\\nDf =\\n\\n1\\n2\\n\\n, we have\\n\\nlog\\n\\nlimx\\n\\nց\\n\\n(cid:18)\\n\\nf (a) + a\\n\\n1\\na f (x) + x\\n\\n−\\n\\n.\\n\\n1\\n\\n(cid:19)\\n\\n−\\n\\nNote that the sum on the right-hand side is ﬁnite, since there are only ﬁnitely many discon-\\ntinuous points for f .\\n\\n∈\\n\\nP\\n\\ne\\n(0,1)\\nXa\\n∈\\n\\n14\\n\\n\\x0c, Lp) be the metric space obtained from\\n\\nDf induces a well-deﬁned map\\n\\nLet p\\nendowing\\n\\n≥\\nP\\n\\n1 be any positive real number. Let (\\nwith the Lp norm. The map f\\nD : (\\n\\n7→\\n, Lp)\\n\\nP\\n\\ne\\n→\\n\\ne\\n\\nR.\\nIn Proposition 3.8, we have seen that the map B : (\\nP\\ne\\neverywhere. The following proposition says that D, considered as a function from (\\nto (R, Euclid), exhibits a similar topological property.\\n\\n(R, Euclid) is discontinuous\\n, Lp)\\n\\n, Lp)\\n\\n→\\n\\nP\\n\\nP\\n\\nProposition 4.15. The map D : (\\n\\n, Lp)\\n\\ne\\n(R, Euclid) is discontinuous everywhere on\\n\\n.\\n\\nbe arbitrary. Let k be a positive integer for which f\\n\\n→\\n\\nP\\n\\ne\\n\\nProof. Let f\\nP\\npositive integer n\\ne\\n\\n∈\\n\\n≥\\n\\nk + 1, deﬁne\\n\\n∈\\n\\nP\\nPk. For each\\ne\\ne\\n\\ngn(x) =\\n\\nf (x)\\nn−\\n\\n(\\n\\n1 + n−\\n\\n2\\n\\nif x\\n1\\n≤\\nif x > 1\\n\\n1\\nn ,\\n1\\nn .\\n\\n−\\n−\\n\\nNote that\\nhand, we obtain from Proposition 4.14 that\\n\\nis a sequence of functions in\\n\\ngn}\\n\\n{\\n\\nP\\n\\nthat converges in Lp to f . On the other\\n\\nDgn =\\n\\ne\\n1\\nlog(n + 1) + Df +\\n2\\n\\n1\\n2\\n\\nf (1\\n\\n1\\nn )\\n−\\nf (1)\\n\\n1\\nn\\n\\n−\\n\\n.\\n\\n(cid:19)\\n\\nlog\\n\\n(cid:18)\\n\\nր\\n\\n, as n\\n\\n, as n\\n\\n.\\n→ ∞\\n\\n1 f (x) = f (1), the third term on the right-hand side converges to 0 as n\\n\\nSince limx\\nThe second term does not depend on n. The ﬁrst term goes to +\\nDgn → ∞\\n4.4. Bounds for Bf and Df . In this subsection, we determine the extremal values for\\nPk.\\nboth Bf and Df among all functions f\\ne\\n1\\nk ≤\\nBoth bounds are tight. The lower bound is attained if and only if f is the function in\\nExample 4.11. The upper bound is attained if and only if f (x) = 1 for every x\\n\\nProposition 4.16. Let k be a positive integer. Let f\\n\\nPk. Then,\\n1.\\ne\\n\\n.\\n→ ∞\\n. Therefore,\\n(cid:3)\\n\\n∈\\nBf ≤ −\\n\\n[0, 1].\\n\\n→ ∞\\n\\nlog k\\n\\n∞\\n\\n−\\n\\n−\\n\\n∈\\n\\n∈\\n\\nProof. The upper bound follows from Proposition 3.9(a). For the equality case of the upper\\nbound, note that since limx\\n[0, 1), then f (1) must\\nalso be 1.\\n\\n1 f (x) = f (1), if f (x) = 1 for all x\\n\\n∈\\n\\nր\\n\\nFor the lower bound, note that for any function f\\n\\n1/k. Therefore,\\n\\nby Proposition 3.5, we have\\n\\n1\\n\\n∈\\n\\nPk, we have loft(f )\\ne\\n\\n1\\n\\n≥\\n\\n1/k\\n\\nBf =\\n\\nlog(f (x) + x\\n\\n1) dx\\n\\nlog(x) dx +\\n\\nlog(1/k) dx\\n\\n0\\nZ\\n\\n0\\nZ\\nComputing the last expression yields the desired lower bound. The equality case happens\\n1 = 1/k on (1/k, 1]. This is exactly if and only if\\nwhen f (x) = 1 on [0, 1/k] and f (x) + x\\n(cid:3)\\nf is the function in Example 4.11.\\n\\n1/k\\n\\n−\\n\\nZ\\n\\n−\\n\\n≥\\n\\nProposition 4.17. Let k be a positive integer. Let f\\n\\n∈\\n\\nPk. Then,\\n2kπ\\n.\\ne\\nk\\n\\n1\\n2\\n\\n1\\n2\\n\\n2π\\nk\\n\\nlog\\n\\nDf ≤\\nBoth bounds are tight. The lower bound is attained if and only if f is continuous and\\nf (1) = 1/k (i.e., if and only if f is in the Catalan-numerous family discussed in Remark 4.7).\\nThe upper bound is attained if and only if f is the function in Example 4.12.\\n\\nlog\\n\\n≤\\n\\n(cid:19)\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n(cid:18)\\n\\nProof. For the lower bound, note that by the formula in Proposition 4.14, it is immediate\\nthat\\n\\nDf ≥\\n\\n1\\n2\\n\\n15\\n\\nlog(2πf (1)).\\n\\n\\x0cSince f\\n1/k. Combining the two inequalities yields the desired lower\\nbound. The equality is attained if and only if f (1) = 1/k and there are no “jumps.” In\\nother words, f is continuous and f (1) = 1/k.\\n\\nPk, we have f (1)\\ne\\n\\n≥\\n\\n∈\\n\\nFor the upper bound, we use the following strategy. We start with an arbitrary function\\nf\\nPk, and then we keep transforming the function (if possible) in a number of steps so that\\nin each step Df becomes larger. We claim that we can always end at the unique extremal\\nfunction in Example 4.12.\\ne\\n\\n∈\\n\\n∈\\n\\n−\\n\\n−\\n\\nFirst, start with any function f\\n\\nPk. Consider whether f has a linear piece with a\\n1)/k, i/k], f has a negative slope – modify the\\nstrictly negative slope. If so – say over ((i\\nfunction f so that over ((i\\n1)/k f (x)\\n1)/k, i/k], it becomes constant with the value limx\\ne\\nPk, and the value Df strictly increases.\\ninstead. The new function remains in\\ne\\n\\nSecond, now assume that the function f is already piecewise constant. Consider the\\n1/k, 1]. If it is strictly greater than 2/k, change the value to 2/k. This\\nvalue of f over (1\\nchange strictly increases Df . Then, consider the value of f over (1\\n1/k]. If it is\\nstrictly greater than 3/k, change the value to 3/k. Keep going in this manner from the right\\nto the left. The resulting function is the unique function in Example 4.12. This proves the\\nupper bound.\\n\\n2/k, 1\\n\\n−\\n\\n−\\n\\n−\\n\\nց\\n\\n−\\n\\n(i\\n\\nNote that in each step, if a change is made, the value of Df increases strictly. This shows\\nthat the equality case for the upper bound happens if and only if f is the unique function\\n(cid:3)\\nin Example 4.12.\\n\\n5. Cumulative X-rays of rook placements\\n\\n∈\\nLet n be a positive integer. Let λ\\n\\n5.1. Marginal Probabilities. In the following, for a ﬁnite nonempty set S, we denote\\nby Unif(S) the uniform distribution on S. The notation X\\nUnif(S) means that X is a\\n1.\\nuniform random variable so that\\n\\nS, P(X = s) =\\n∈ Dn be a partition. In what follows, let us consider\\nour partitions in the French notation so that the boxes of λ are bottom- and left-aligned\\nand there are λ1 boxes on the bottom row, λ2 boxes on the second row from the bottom,\\nand so on.\\n\\ns\\n∀\\n\\n∼\\n\\nS\\n\\n−\\n\\n|\\n\\n|\\n\\nProposition 5.1. Let π\\n\\nUnif(RP(λ)). Let i, j\\n\\n[n].\\n\\n∈\\n\\n(a) If λn+1\\n(b) If λn+1\\n\\ni < j, then E(πij ) = 0.\\n−\\nj, and suppose i′\\ni ≥\\n\\n∈\\n\\n−\\n\\n∼\\n\\n[n] is the smallest index such that λn+1\\n\\nj, then\\n\\ni′\\n\\n−\\n\\n≥\\n\\nE(πij ) =\\n\\nλn+1\\n\\nt −\\n\\nt\\nt + 1 \\uf8f6\\n\\n1\\n\\n.\\n\\n−\\nt −\\nProof. (a) It follows immediately from the deﬁnition of RP(λ) (cf. the beginning of Sec-\\ntion 2) that πij = 0.\\n\\nYi′\\n\\nλn+1\\n\\nλn+1\\n\\ni + 1\\n\\ni −\\n\\n\\uf8ed\\n\\n\\uf8f8\\n\\n\\uf8eb\\n\\nt<i\\n\\n−\\n\\n−\\n\\n≤\\n\\n·\\n\\n(b) Suppose that µ is the partition obtained by removing the ith row from the top (the\\ni) and the jth column from the left from the Young diagram of\\n\\nrow corresponding to λn+1\\nλ. Observe that the probability that πij = 1 is # RP(µ)/# RP(λ).\\n\\n−\\n\\nThe formula in Equation (1) gives\\n\\n# RP(µ) =\\n\\n(λn+1\\n\\nt −\\n\\n−\\n\\n(t\\n\\n1))\\n\\n−\\n\\n(λn+1\\n\\nt −\\n\\n−\\n\\nt)\\n\\n.\\n\\n \\n\\nYt<i′ or t>i\\n\\n! \\uf8eb\\n\\uf8ed\\n\\nt<i\\n\\nYi′\\n\\n≤\\n\\n\\uf8f6\\n\\n\\uf8f8\\n\\n1\\n\\nE(πij ) = P(πij = 1) =\\n\\n# RP(µ)\\n# RP(λ)\\n\\n=\\n\\nλn+1\\n\\nλn+1\\n\\n−\\nt −\\n\\n−\\n\\nt −\\n\\nt\\nt + 1 \\uf8f6\\n\\n·\\n\\nλn+1\\n\\ni −\\n\\n−\\n\\n,\\n\\ni + 1\\n\\n\\uf8f8\\n\\n(cid:3)\\n\\n\\uf8eb\\n\\n\\uf8ed\\n\\nt<i\\n\\nYi′\\n\\n≤\\n\\n16\\n\\nTherefore,\\n\\nas desired.\\n\\n\\x0cThe following corollary is immediate from Proposition 5.1.\\n\\nCorollary 5.2. If π\\n\\nUnif(RP(λ)) and i, j\\n\\n[n], then\\n\\n∼\\n\\n∈\\n\\n1\\n\\nE(πij )\\n\\n≤\\n\\nλn+1\\n\\ni −\\n\\n−\\n\\n.\\n\\ni + 1\\n\\nWe also have the following result. Let λ′\\n\\nCorollary 5.3. Let π\\nand λn+1\\ni2 ≥\\n\\n−\\n\\nUnif(RP(λ)). Suppose that i1, i2, j1, j2 ∈\\n\\nj1. If λi1 = λi2 and λ′j1 = λ′j2 , then E(πi1j1 ) = E(πi2j2 ).\\n\\n∼\\n\\n∈ Dn denote the conjugate partition of λ.\\ni1 ≥\\n\\n[n] satisfy λn+1\\n\\n−\\n\\nj1\\n\\nProof. We proceed in a similar manner to how we proved Proposition 5.1(b). Namely, let\\nµ(1) (and µ(2)) denote the resulting partition from removing the (i1, j1) (and resp. (i2, j2))\\nbox (together with the row and the column) from λ. It is not hard to see that µ(1) = µ(2).\\n(cid:3)\\nThis ﬁnishes the proof.\\n\\nLike before, we may think of λ as a Dyck path from (0, n) to (n, 0), which we can write\\n\\nas the following concatenation\\n\\nλ = Rr1Dd1Rr2Dd2\\n\\n,\\n\\n· · ·\\n\\nwhere R denotes a unit step to the right, and D denotes a unit step down. The equation\\nabove means that λ starts by going r1 steps to the right, and then d1 steps down, and so\\nas the minimum run of λ, denoted\\non. Let us refer to the quantity min\\nr1, d1, r2, d2, . . .\\n{\\nmr(λ). For instance, since λ\\nn, where the equality is attained if and\\n∈ Dn, we have mr(λ)\\nonly if λ = RnDn.\\n\\n≤\\n\\n}\\n\\nProposition 5.4. Let π\\nany t diﬀerent boxes b1 = (i1, j1), b2 = (i2, j2), . . ., bt = (it, jt)\\nis 1 in all these t boxes is\\n\\nUnif(RP(λ)). Let t\\n\\n≤\\n\\n∼\\n\\n∈\\n\\nmr(λ) be a positive integer. Then, for\\n[n]2, the probability that π\\n\\nE(πb1 πb2 · · ·\\n\\nπbt )\\n\\n≤\\n\\nmr(λ)\\n\\n(mr(λ)\\n\\n·\\n\\n(mr(λ)\\n\\nt + 1)\\n\\n−\\n\\n· · ·\\n\\n−\\n\\n.\\n\\n1\\n1)\\n\\nProof. If any of the t boxes is “outside” the Young diagram of λ, we are done. Suppose that\\nj1 and so on). Since removing\\nall these boxes are inside the Young diagram (i.e., λn+1\\na box (together with its row and its column) reduces the minimum run by at most 1, it\\nsuﬃces to show that if we remove one box b (together with its row and its column) from λ\\nand obtain a new partition µ, then\\n\\ni1 ≥\\n\\n−\\n\\n# RP(µ)\\n# RP(λ) ≤\\n\\n1\\nmr(λ)\\n\\n.\\n\\n(25)\\n\\n(26)\\n\\nConsider a box b = (i, j) such that λn+1\\nλn+1\\ndeﬁnition of the minimum run, we have\\n\\ni = λn+1\\n\\n−\\n\\n−\\n\\ni ≥\\n−\\ni′ . Let i′′ denote the largest index for which λn+1\\n\\nj. Let i′ denote the smallest index for which\\ni′′ . By the\\n\\ni = λn+1\\n\\n−\\n\\n−\\n\\nBy Corollaries 5.2 and 5.3, we have\\n\\ni′′\\n\\ni′ + 1\\n\\nmr(λ).\\n\\n−\\n\\n≥\\n\\n(27)\\n\\nE(πij) = E(πi′j)\\n\\nwhich implies (25).\\n\\n1\\n\\n≤\\n\\nλn+1\\n\\ni′\\n\\n−\\n\\n−\\n\\n=\\n\\ni′ + 1\\n\\n(λn+1\\n\\n1\\ni′′) + i′′ −\\n\\ni′′\\n\\n−\\n\\n−\\n\\n(26)\\n\\n1\\nmr(λ)\\n\\n,\\n\\ni′ + 1\\n\\n≤\\n\\n(cid:3)\\n\\nSince the minimum run also grows as we dilate partitions, we immediately have the\\n\\nfollowing corollary.\\n\\nCorollary 5.5. Let t and N be positive integers such that t\\nλ)). Then, for any t diﬀerent boxes b1, b2, . . . , bt ∈\\nUnif(RP(N\\n\\n⊙\\n\\nN . Suppose that π\\n\\n≤\\n[nN ]2, we have\\n\\n∼\\n\\nE(πb1 πb2 · · ·\\n\\nπbt )\\n\\n1\\n\\n≤\\n\\nN (N\\n17\\n\\n1)\\n\\n−\\n\\n· · ·\\n\\n(N\\n\\nt + 1)\\n\\n−\\n\\n.\\n\\n\\x0cThe proposition below shows that these marginal probabilities P(πi,j = 1) behave nicely\\n\\nin the following sense, when we dilate partitions.\\n\\nProposition 5.6. If π\\n[nN ], we have\\n\\n∼\\n\\nUnif(RP(N\\n\\nλ)) and π↓\\n\\nUnif(RP(λ)). Then, for any i, j\\n\\n⊙\\n\\n∼\\n\\nProof. This follows from a direct computation using Proposition 5.1(b).\\n\\nE(πi,j) =\\n\\n1\\nN ·\\n\\nE\\n\\nπ↓\\n⌈\\n\\n(cid:16)\\n\\ni/N\\n\\n,\\n\\nj/N\\n\\n⌉\\n\\n⌈\\n\\n⌉\\n\\n(cid:17)\\n\\n∈\\n\\n(cid:3)\\n\\n5.2. Cumulative X-rays. Let n be a positive integer. Suppose that a permutation π\\nis given. The cumulative X-ray of π is the piecewise constant function ξπ : [0, 2n]\\nby\\n\\n→\\n\\nSn\\n∈\\nR given\\n\\nξπ(t) :=\\n\\nπij .\\n\\n[n]\\nXi,j\\n∈\\nt\\ni+j\\n≤\\n\\nξπ(t) :=\\n\\nξπ(nt).\\ne\\n\\n1\\nn ·\\n\\n→\\n\\nWe also deﬁne the normalized version of cumulative X-rays. The normalized cumulative\\nX-ray of π\\n\\nSn is the piecewise constant function\\n\\n[0, 1] given by\\n\\nξπ : [0, 2]\\n\\n∈\\n\\nThe following is a counting lemma which is easy to prove.\\n\\nLemma 5.7. For each real number φ\\n\\nR and each positive integer N\\n\\nZ\\n\\n∈\\n\\n≥\\n\\n1, let\\n\\nS(φ; N ) := #\\n\\n[N ]\\n\\n[N ]\\n\\nx + y\\n\\n∈\\n\\n×\\n\\n|\\n\\nφN\\n\\n.\\n\\n}\\n\\n≤\\n\\ne\\n\\n∈\\n(x, y)\\n{\\n\\nThen, we have the following.\\n\\n(a) If φ\\n(b) If 0\\n\\n0, then S(φ; N ) = 0.\\nφ\\n\\n1, then\\n\\n≤\\n≤\\n\\n≤\\n\\nS(φ; N ) =\\n\\n=\\n\\nN 2 + O(φN ).\\n\\nφN\\n2\\n\\n⌋\\n\\n⌊\\n(cid:18)\\n\\n(cid:19)\\n\\nφ2\\n2\\n\\n(c) If 1\\n\\nφ\\n\\n2, then\\n\\n≤\\n\\n≤\\nS(φ; N ) = N 2\\n\\n2N\\n\\n− ⌊\\n\\nφN\\n2\\n\\n⌋\\n\\n+ 1\\n\\nφ2\\n2\\n\\n=\\n\\n(cid:19)\\n\\n−\\n\\n(cid:18)\\n\\n−\\n\\n(cid:18)\\n\\n+ 2φ\\n\\n1\\n\\nN 2 + O(φN ).\\n\\n−\\n\\n(cid:19)\\n\\n(d) If φ\\n\\n2, then S(φ; N ) = N 2.\\nIn (b) and (c), the implicit constants are absolute.\\n\\n≥\\n\\nFor convenience, let us reserve the symbol c. In the following, we let c denote the function\\n\\nc : R\\n\\nR given by\\n\\n→\\n\\n0\\nt2\\n2\\n\\nif t\\n0,\\n≤\\nif 0 < t\\nif 1 < t\\n2.\\nif t\\n\\n1\\n\\n−\\n1\\n\\nt2\\n2 + 2t\\n\\nc(t) := \\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\nS(φ; N ) = c(φ)N 2 + O(N ),\\n\\n≤\\n≤\\n\\n≥\\n\\n−\\n\\n1,\\n2, and\\n\\n∈ Dn, let us deﬁne a function mλ : [0, 2]\\ndx dy,\\n\\nmλ(t) :=\\n\\nE\\n\\n→\\n\\nπ\\n⌈\\n\\nx\\n\\ny\\n\\n,\\n⌉\\n\\n⌈\\n\\n⌉\\n\\n1\\nn\\n\\n[0, 1] by\\n\\nZ Z\\nx,y\\n∈\\nx+y\\n\\n(0,n]\\nnt\\n\\n≤\\n\\n(cid:0)\\n\\n(cid:1)\\n\\n18\\n\\nThus, Lemma 5.7 says that\\n\\nfor positive integers N .\\nFor each partition λ\\n\\n(28)\\n\\n(29)\\n\\nwhere π\\n\\nUnif(RP(λ)).\\n\\n∼\\n\\n\\x0cProposition 5.8. Let λ\\nfor any real number t\\n\\n∈ Dn and N\\n[0, 2], we have\\n\\n∈\\n\\nZ\\n\\n∈\\n\\n≥\\n\\n1. Suppose that π\\n\\nUnif(RP(N\\n\\nλ)). Then,\\n\\n∼\\n\\n⊙\\n\\nProof. From Proposition 5.6, we have\\n\\ne\\n\\n(cid:16)\\n\\n(cid:17)\\n\\nE\\n\\nξπ(t)\\n\\n= mλ(t) + Oλ\\n\\n1\\nN\\n\\n.\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n(30)\\n\\n(31)\\n\\n(32)\\n\\n(33)\\n\\n(34)\\n\\n(35)\\n\\n(36)\\n\\n1\\nnN 2\\n\\nE\\n\\nξπ(t)\\n\\n=\\n\\n(cid:16)\\n\\ne\\n\\n(cid:17)\\n\\n=\\n\\n1\\nnN 2\\n\\nE\\n\\nπ↓\\n⌈\\n\\n(cid:16)\\n\\ni/N\\n\\n,\\n\\nj/N\\n\\n⌉\\n\\n⌈\\n\\n⌉\\n\\n(cid:17)\\n\\n[nN ]\\nXi,j\\n∈\\nnN t\\ni+j\\n≤\\nn\\nn\\n\\na=1\\nX\\n\\nXb=1\\n\\n[nN ]\\nnN t\\n\\nXi,j\\n∈\\ni+j\\n≤\\nj/N\\n,\\n\\n⌉\\n\\n(\\n\\ni/N\\n\\n⌈\\n\\n⌉\\n\\n⌈\\n\\n)=(a,b)\\n\\nE\\n\\nπ↓a,b\\n\\n.\\n\\n(cid:16)\\n\\n(cid:17)\\n\\nNote that the innermost summation in the last expression above is over pairs (i, j) of\\naN , and\\nbN . By translation, the number of such pairs is exactly the number\\nb + 2)N . By our discussion above, the number\\n\\npositive integers such that (i) i, j\\n(iv) bN\\nof (x, y)\\nis exactly S(nt\\n\\nN + 1\\n≤\\n[N ]2 such that x + y\\nb + 2; N ).\\n\\nnN , (ii) i + j\\n\\nnN t, (iii) aN\\n\\nN + 1\\n\\n−\\n∈\\n\\n(nt\\n\\n≤\\n\\n≤\\n\\n−\\n\\n−\\n\\n≤\\n\\n≤\\n\\n≤\\n\\n≤\\n\\n−\\n\\na\\n\\nj\\n\\ni\\n\\na\\nTherefore, Equation (28) gives\\n\\n−\\n\\n−\\n\\nE\\n\\nξπ(t)\\n\\n=\\n\\n(cid:16)\\n\\ne\\n\\n(cid:17)\\n\\n=\\n\\n1\\nn\\n\\n1\\nn\\n\\nn\\n\\nn\\n\\nE\\n\\nE\\n\\na=1\\nX\\nn\\n\\nXb=1\\nn\\n\\n(cid:16)\\n\\n(cid:17) (cid:18)\\n\\nπ↓a,b\\n\\nc(nt\\n\\na\\n\\nb + 2) + O\\n\\n−\\n\\n−\\n\\nπ↓a,b\\n\\nc(nt\\n\\na\\n\\n−\\n\\n−\\n\\nb + 2) + Oλ\\n\\n.\\n\\n(cid:18)\\n\\n(cid:19)\\n\\na=1\\nX\\n\\nXb=1\\n\\n(cid:16)\\n\\n(cid:17)\\n\\n(cid:18)\\n\\n(cid:19)(cid:19)\\n\\n1\\nN\\n\\n1\\nN\\n\\nOn the other hand, by the deﬁnition of mλ, we have\\nn\\n\\nn\\n\\nb\\n\\na\\n\\nmλ(t) =\\n\\n1\\nn\\n\\n1\\nn\\n\\n1\\nn\\n\\n=\\n\\n=\\n\\nπ↓a,b\\n(cid:16)\\n\\nπ↓a,b\\n(cid:16)\\n\\nE\\n\\nE\\n\\nE\\n\\nπ↓a,b\\n(cid:16)\\n\\n(cid:17)\\n\\na=1\\nX\\nn\\n\\nXb=1\\nn\\n\\na=1\\nX\\nn\\n\\nXb=1\\nn\\n\\na=1\\nX\\n\\nXb=1\\n\\nb\\n(cid:17) Z\\n−\\n1\\n\\n1 Z\\na\\n1\\n\\n1\\n\\n−\\n\\n0 Z\\n\\n0\\n\\n(cid:17) Z\\n\\nc(nt\\n\\na\\n\\nb + 2),\\n\\n−\\n\\n−\\n\\n1(x + y\\n\\nnt) dx dy\\n\\n≤\\n\\n1(x + y\\n\\nnt\\n\\na\\n\\n−\\n\\n−\\n\\n≤\\n\\nb + 2) dx dy\\n\\nwhere 1 denotes the indicator function, and Equation (35) follows from simple changes of\\n1. Combining Equations (31) and (36) ﬁnishes the\\nvariables x\\n(cid:3)\\nproof.\\n\\n1 and y\\n\\nx + a\\n\\ny + b\\n\\n7→\\n\\n7→\\n\\n−\\n\\n−\\n\\nE\\n\\nIn particular, Proposition 5.8 implies a convergence of expectations. If for each N , we\\n[0, 2], the sequence\\n\\nhave a random variable π(N )\\n\\nλ)), then for any ﬁxed t\\n\\nUnif(RP(N\\n\\n∞\\n\\n∼\\nconverges to mλ(t).\\n\\n⊙\\n\\nn\\n\\nN =1\\n\\nξπ(N ) (t)\\nThe author of the present paper gives the following conjecture about this function mλ.\\n(cid:16)\\ne\\n∈ Dn be a ﬁxed partition. Fix a positive real number ε > 0. Suppose\\nλ)). Then,\\n\\nConjecture 5.9. Let λ\\nthat π\\n\\nUnif(RP(N\\n\\n(cid:17)o\\n\\n∈\\n\\n∼\\n\\n⊙\\n\\nas N\\n\\n.\\n→ ∞\\n\\nrandom permutations.\\n\\nP\\n\\nsup\\n[0,2]\\n\\n \\nt\\n∈\\n\\nξπ(t)\\n\\n−\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)e\\n\\nmλ(t)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n< ε\\n\\n1,\\n\\n! →\\n\\n19\\n\\nIn the next subsection, we give a proof of Conjecture 5.9 in the special case of uniformly\\n\\n\\x0c5.3. Limit shape for normalized cumulative X-rays of random permutations. Let\\nN be a positive integer, and let π\\nUnif(SN ) be a uniformly random permutation. For\\n[N + 1], we let\\neach k\\n\\n∼\\n\\n∈\\n\\nLet’s also deﬁne, for each k = 2, 3, . . . , 2N , the kth X-ray component\\n\\nXk := ξπ(k).\\n\\nxk :=\\n\\nπij .\\n\\n[N ]\\nXi,j\\n∈\\ni+j=k\\n\\nIndeed, there is a simple relation between these notations: Xk = x2 + x3 +\\n\\n+ xk.\\n\\nThe goal of this subsection is to prove the following result.\\n\\n· · ·\\n\\nTheorem 5.10. Let ε > 0 be any ﬁxed positive real number. Then,\\n\\nP\\n\\nk\\n\\n[N + 1],\\n\\n∀\\n\\n(cid:18)\\n\\n∈\\n\\nk(k\\n\\n1)\\n\\n−\\n2N\\n\\n< εN\\n\\n= 1\\n\\nOε\\n\\n(cid:19)\\n\\n−\\n\\n(cid:18)\\n\\nlog N\\nN log log N\\n\\n,\\n\\n(cid:19)\\n\\nXk −\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nas N\\n\\n.\\n→ ∞\\n\\nAs a corollary of Theorem 5.10, we obtain a proof of Conjecture 5.9 in the very special\\ncase when λ = (cid:3) is a partition with one box. In this case, the function mλ coincides with\\n|[0,2]. From the deﬁnition of c, we see that the graph of this function is a concatenation of\\nc\\ntwo parabolas.\\nHere is our rough strategy for proving the theorem. First, we show that with high\\nprobability the largest X-ray component maxk xk is small. Second, we give an upper bound\\non the size of the variance Var(Xk). Third, we argue that since the X-ray components are\\nsmall with high probability, it suﬃces to establish the bound\\n\\nwith high probability for all k simultaneously in a certain subset of [N + 1], instead of the\\nwhole [N + 1]. Fourth, we use Chebyshev’s tail bound to show that we have the bound\\n\\nk(k\\n\\n1)\\n\\nXk −\\n\\n−\\n2N\\n\\n< εN\\n\\nk(k\\n\\n1)\\n\\nXk −\\n\\n−\\n2N\\n\\n< εN\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nfor all k in the mentioned subset of [N + 1] simultaneously with high probability. This\\nﬁnishes the proof.\\n\\nNow we begin the ﬁrst step of our strategy.\\n\\nProposition 5.11. Let t\\n\\nN be a positive integer. We have\\n\\n≤\\n\\nP(xk ≥\\n\\nt)\\n\\n≤\\n\\nN + 1\\n(t + 1)!\\n\\n.\\n\\nN +1\\n\\nXk=2\\n\\nProof. The event xk ≥\\nof indices in [k\\n\\n1] such that\\n\\n−\\n\\nt is equivalent to the event that there exists a t-subset\\n\\ni1, i2, . . . , it}\\n\\n{\\n\\nπi1,k\\n\\ni1 = πi2,k\\n\\ni2 =\\n\\n= πit,k\\n\\nit = 1.\\n\\n−\\n\\n· · ·\\n\\n−\\n\\n−\\n\\nSince\\n\\n(37)\\n\\n(38)\\n\\nwe have\\n\\n(39)\\n\\nP(πi1,k\\n\\n−\\n\\ni1 = πi2,k\\n\\ni2 =\\n\\n−\\n\\n· · ·\\n\\n= πit,k\\n\\nit = 1) = E(πi1,k\\n\\n−\\n\\n−\\n\\ni1 πi2,k\\n1\\n\\ni2 · · ·\\n\\n−\\n\\nπit,k\\n\\nit )\\n\\n−\\n\\n,\\n\\nN (N\\n\\n1)\\n\\n(N\\n\\nt + 1)\\n\\n−\\n\\n· · ·\\n\\n−\\n\\n=\\n\\nP(xk ≥\\n\\nt)\\n\\n≤\\n\\n1\\nt! ·\\n\\n(k\\n−\\nN (N\\n20\\n\\n1)(k\\n1)\\n\\n−\\n\\n−\\n· · ·\\n\\n2)\\n(N\\n\\n· · ·\\n\\n(k\\n\\nt)\\n−\\nt + 1)\\n\\n.\\n\\n−\\n\\n\\x0cTherefore, by telescoping, we obtain\\n\\nN +1\\n\\nXk=2\\n\\nP(xk ≥\\n\\nt)\\n\\n≤\\n\\n(k\\n−\\nN (N\\n\\n1)(k\\n1)\\n\\n−\\n\\n−\\n· · ·\\n\\n2)\\n(N\\n\\n· · ·\\n\\n(k\\n\\nt)\\n−\\nt + 1)\\n\\nN +1\\n\\nXk=2\\n1\\nt!\\n\\nN +1\\n\\nk(k\\n\\n=\\n\\n=\\n\\nXk=2\\nN + 1\\n(t + 1)!\\n\\n,\\n\\n(40)\\n\\n(41)\\n\\n(42)\\n\\nas desired.\\n\\nBy using the bound\\n\\n−\\n\\n1)\\n\\n(k\\n· · ·\\n(t + 1)\\n\\nt)\\n−\\nN (N\\n\\n(k\\n\\n1)(k\\n\\n−\\n1)\\n\\n−\\n(N\\n\\n−\\n\\n· · ·\\n\\n(k\\n· · ·\\nt + 1)\\n\\n2)\\n\\n−\\n\\nt\\n\\n1)\\n\\n−\\n\\n−\\n\\n−\\n\\n−\\n·\\n\\n(43)\\n\\nP(xk ≥\\ntogether with Stirling’s formula, we obtain the following corollary.\\n\\nx2, x3, . . . , xN +1} ≥\\n\\nP(max\\n{\\n\\nXk=2\\n\\n≤\\n\\nt)\\n\\nt) ,\\n\\nN +1\\n\\nCorollary 5.12. For all suﬃciently large positive integers N , we have\\n\\nNext is the second step of the strategy. For each 2\\n\\nk\\n\\nN + 1, it is easy to see that\\n\\nmax\\n\\nx2, x3, . . . , xN +1} ≥\\n\\n{\\n\\n3 log N\\nlog log N\\n\\n1\\nN\\n\\n.\\n\\n≤\\n\\nP\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n≤\\n\\n≤\\n\\nE(Xk) = k(k\\n\\n1)\\n2N .\\n−\\nProposition 5.13. Let N\\n\\nProof. Observe that\\n\\n(44)\\n\\n2 be a positive integer. For 2\\n\\nk\\n\\nN + 1, we have\\n\\n≥\\nX 2\\nk\\n\\n=\\n\\nE\\n\\n(cid:0)\\n\\n(cid:1)\\n\\nk(k\\n\\n1)\\n\\nk(k\\n\\n−\\n2N\\n\\n+\\n\\n−\\n\\n1)(k\\n−\\n12N (N\\n\\n≤\\n\\n5)\\n\\n.\\n\\n−\\n\\n≤\\n2)(3k\\n1)\\n\\n−\\n\\nk\\n\\nE\\n\\nX 2\\nk\\n\\n=\\n\\n(cid:0)\\n\\n(cid:1)\\n\\nXℓ=2 Xi,j\\n[N ]\\n∈\\n\\ni+j=ℓ Xi\\n\\n,j\\n∈\\n′\\n′\\n+j\\ni\\n\\n[N ]\\nk\\n\\n′\\n\\n′\\n\\n≤\\n\\nE(πij πi′j′ ) .\\n\\nFor the innermost summation on the right-hand side above, there are three cases. First,\\n= (i, j) but (i′, j′) is on either\\n2 such\\n\\nif (i′, j′) = (i, j), then E(πij πi′j′ ) = 1/N . Second, if (i′, j′)\\nthe same row or the same column as (i, j), then E(πij πi′j′ ) = 0. There are 2k\\nordered pairs. For the rest, the expectation E(πijπi′j′ ) is\\n1) . Therefore,\\n\\n1\\nN (N\\n\\n−\\n\\n−\\n\\nℓ\\n\\n(45)\\n\\nk\\n\\nE\\n\\nX 2\\nk\\n\\n=\\n\\n(cid:0)\\n\\n(cid:1)\\n\\nXℓ=2 Xi,j\\n[N ]\\n∈\\ni+j=ℓ\\n\\n(cid:26)\\n\\nSimplify to ﬁnish.\\n\\n1\\nN\\n\\n+\\n\\nk(k\\n\\n1)\\n\\n−\\n2\\n\\n−\\n\\n(cid:18)\\n\\n2k + ℓ + 1\\n\\n−\\n\\n·\\n\\n(cid:19)\\n\\n1\\nN (N\\n\\n.\\n\\n1)\\n\\n(cid:27)\\n\\n−\\n\\nProposition 5.14. Let N\\n\\n2 be a positive integer. For 2\\n\\nk\\n\\nN + 1, we have\\n\\nVar(Xk) <\\n\\nk2\\n2N\\n\\n.\\n\\n≤\\n\\n≤\\n\\nProof. From Proposition 5.13 and some algebraic manipulation, we obtain\\n\\n(46)\\n\\nVar(Xk) = E\\n\\nX 2\\nk\\n\\n(EXk)2 =\\n\\nIt is not hard to see that\\n\\n(cid:0)\\n\\n(cid:1)\\n\\nk(k\\n\\n1)\\n2N −\\n\\n−\\n\\nk(k\\n\\n1)(4k\\n\\n5)\\n\\n−\\n6N (N\\n\\n−\\n1)\\n\\n+\\n\\nk2(k\\n−\\n4N 2(N\\n\\n1)2\\n1)\\n\\n.\\n\\n−\\n\\n−\\n\\nk(k\\n\\n1)(4k\\n\\n5)\\n\\n−\\n6N (N\\n\\n−\\n1)\\n\\n≥\\n\\nk2(k\\n−\\n4N 2(N\\n\\n1)2\\n1)\\n\\n,\\n\\n−\\n\\n−\\n\\nwhence Var(Xk)\\n\\nk(k\\n\\n1)\\n\\n2N < k2\\n2N .\\n\\n−\\n\\n≤\\n\\n21\\n\\n≥\\n\\n−\\n\\n(cid:3)\\n\\n(cid:3)\\n\\n(cid:3)\\n\\n6\\n\\x0c(47)\\n\\n(48)\\n\\nthen\\n\\n(cid:12)\\n(cid:12)\\nk′\\n(cid:12)\\n|\\n\\n−\\n\\n|\\n\\n(49)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(50)\\n\\n(51)\\n\\nWe now turn to the third step of the described strategy. For each real number ε > 0, we\\n\\nlet Eε denote the event\\n\\nk\\n\\n∀\\n\\n∈ {\\n\\n2, 3, . . . , N + 1\\n\\n,\\n\\nk(k\\n\\n1)\\n\\n−\\n2N\\n\\n< εN.\\n\\nFor each positive integer g\\n\\nN + 1, we let Eg,ε denote the event\\n\\n≤\\n\\ngZ\\n\\nk\\n\\n∀\\n\\n∈\\n\\n∩\\n\\n[N + 1],\\n\\nk(k\\n\\n1)\\n\\n−\\n2N\\n\\n< εN.\\n\\n}\\n\\nXk −\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nXk −\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nHere, gZ\\n\\n∩\\n\\n[N + 1] refers to the set\\n\\ng, 2g, . . . ,\\n\\nN +1\\ng\\n\\ng\\n\\n.\\n\\nj\\n\\nk\\n\\no\\n\\nn\\n\\nProposition 5.15. Let ε > 0 be a positive real number. For all suﬃciently large positive\\nintegers N , if\\n\\ng <\\n\\nε\\n8 ·\\n\\nN log log N\\nlog N\\n\\n,\\n\\nP(Eε)\\n\\nP\\n\\nEg,ε/2\\n\\n≥\\n\\n1\\nN\\n\\n.\\n\\n−\\n\\n3 log N\\nProof. Let A denote the event that max\\nlog log N . By Corollary 5.12,\\nit suﬃces to show that for all suﬃciently large positive integers N , we have the inclusion\\nEg,ε ⊆\\n\\nEε ∪\\n\\nA.\\n\\n{\\n\\n′\\n\\n′\\n\\n(cid:0)\\n(cid:1)\\nx2, x3, . . . , xN +1} ≥\\n\\nConsider any event in Eg,ε/2 \\\\\\n\\n(k\\n−\\n2N\\nlog log N , for any ℓ. We claim that for any k\\n\\nA. In this case,\\n\\n−\\n\\nk\\n\\n1)\\n\\n< εN\\n\\n2 for any k′ divisible\\n, we have\\n\\n2, 3, . . . , N + 1\\n\\n(cid:12)\\n(cid:12)\\n∈ {\\n(cid:12)\\n\\n}\\n\\nXk′\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nby g, and xℓ < 3 log N\\n1)\\nXk −\\n\\n< εN .\\n\\n−\\n2N\\n\\nk(k\\n\\nNote that we can ﬁnd an integer k′\\n\\n2, 3, . . . , N + 1\\n\\nwhich is a multiple of g such that\\n\\nk\\n\\n< g. By the triangle inequality, we have\\n\\n∈ {\\n\\n}\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nk(k\\n\\n1)\\n\\nXk −\\n\\n−\\n2N\\n\\nXk −\\n\\n≤ |\\n\\nXk′\\n\\n+\\n\\nXk′\\n\\n|\\n\\n−\\n\\n−\\n2N\\n\\n+\\n\\n−\\n2N\\n\\n−\\n\\n−\\n2N\\n\\nk′(k′\\n\\n1)\\n\\nk′(k′\\n\\n1)\\n\\nk(k\\n\\n1)\\n\\nThe ﬁrst term on the right-hand side is\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nXk −\\n\\n|\\n\\nXk′\\n\\n| ≤\\n\\ng\\n\\n·\\n\\n3 log N\\nlog log N\\n\\n<\\n\\nεN.\\n\\nThe second term is less than εN\\n\\n2 . The third term is\\n\\nk′(k′\\n\\n1)\\n\\nk(k\\n\\n1)\\n\\n−\\n2N\\n\\n−\\n\\n−\\n2N\\n\\n=\\n\\n(k′\\n\\n−\\n\\nk)(k′ + k\\n2N\\n\\n1)\\n\\n−\\n\\ng\\n\\n≤\\n\\n≤\\n\\n1\\n8\\n\\nεN,\\n\\nfor all suﬃciently large N . Therefore,\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nk(k\\n\\n1)\\n\\nXk −\\n(cid:12)\\n(cid:12)\\n(cid:12)\\nfor all suﬃciently large positive integers N . This shows that Eg,ε/2 \\\\\\n(cid:12)\\nproof.\\n\\n< εN,\\n\\n−\\n2N\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nA\\n\\nEε, ﬁnishing the\\n(cid:3)\\n\\n⊆\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n3\\n8\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nWe have arrived at the ﬁnal step of our strategy.\\n\\n22\\n\\n\\x0cProof of Theorem 5.10. Let Ec\\nshev’s tail bound, we have, for all suﬃciently large positive integers N ,\\n\\ng,ε/2 denote the complement of the event Eg,ε/2. By Cheby-\\n\\n(52)\\n\\nP\\n\\nEc\\n\\ng,ε/2\\n\\n(cid:16)\\n\\n≤\\n\\n(cid:17)\\n\\ngZ\\nXk\\n∩\\n∈\\n\\n[N +1]\\n\\nP\\n\\nXk −\\n\\n(cid:18)(cid:12)\\n(cid:12)\\n(cid:12)\\nVar(Xk)\\n(cid:12)\\n(εN/2)2\\n\\nk(k\\n\\n1)\\n\\n−\\n2N\\n\\nεN\\n2\\n\\n≥\\n\\n(cid:19)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(53)\\n\\n(54)\\n\\n(55)\\n\\n≤\\n\\n≤\\n\\n<\\n\\n[N +1]\\n\\ngZ\\nXk\\n∩\\n∈\\n2\\nε2N 3\\n\\n[N +1]\\n\\ngZ\\nXk\\n∩\\n∈\\nlog N\\nN log log N\\n\\n.\\n\\n3\\nε3 ·\\n\\nk2\\n\\n(by Proposition 5.14)\\n\\nCombining this with Proposition 5.15, we ﬁnish the proof.\\n\\n(cid:3)\\n\\nAcknowledgments\\n\\nI would like to thank Morris Ang, Alexei Borodin, Matthew Nicoletti, Alex Postnikov,\\nI would like to thank\\nSahana Vasudevan, and Wijit Yangjit for insightful discussions.\\nAlex Postnikov speciﬁcally for telling me about Proposition 4.6 and showing his proof to\\nme, which led to the discussion of waterfalls in this paper. I am grateful for Alex Post-\\nnikov and Alexei Borodin speciﬁcally for their encouragement. I would also like to thank\\nRichard Kenyon for sharing with me a copy of the slides from his “permutons” talk. I would\\nlike to thank Sorawee Porncharoenwase for algorithmic insights and technical help. I used\\nPolymake, R, Racket, and Wolfram Alpha to help with computations.\\n\\nReferences\\n\\n[AM14]\\n\\n[Bar21]\\n\\n[BF14]\\n\\nMahshid Atapour and Neal Madras. Large deviations and ratio limit theorems for pattern-\\navoiding permutations. Combin. Probab. Comput., 23(2):161–200, 2014.\\nKenneth Barrese. A graph theory of rook placements. Electron. J. Combin., 28(4):Paper No.\\n4.13, 26, 2021.\\nRichard A. Brualdi and Eliseu Fritscher. Hankel and Toeplitz X-rays of permutations. Linear\\nAlgebra Appl., 449:350–380, 2014.\\n\\n[BLRS14] Kenneth Barrese, Nicholas Loehr, Jeﬀrey Remmel, and Bruce E. Sagan. m-level rook placements.\\n\\nJ. Combin. Theory Ser. A, 124:130–165, 2014.\\n\\n[BLRS16] Kenneth Barrese, Nicholas Loehr, Jeﬀrey Remmel, and Bruce E. Sagan. Bijections on m-level\\n\\nrook placements. European J. Combin., 57:13–35, 2016.\\n\\n[BMPS05] Cecilia Bebeacua, Touﬁk Mansour, Alex Postnikov, and Simone Severini. On the X-rays of\\npermutations. In Proceedings of the Workshop on Discrete Tomography and its Applications,\\nvolume 20 of Electron. Notes Discrete Math., pages 193–203. Elsevier Sci. B. V., Amsterdam,\\n2005.\\nKaren S. Briggs and Jeﬀrey B. Remmel. m-rook numbers and a generalization of a formula of\\nFrobenius to Cm ≀ Sn. J. Combin. Theory Ser. A, 113(6):1138–1171, 2006.\\n\\n[BR06]\\n\\n[GGKK15] Roman Glebov, Andrzej Grzesik, Tereza Klimoˇsov´a, and Daniel Kr´al’. Finitely forcible graphons\\n\\nand permutons. J. Combin. Theory Ser. B, 110:112–135, 2015.\\n\\n[GX06]\\n\\n[GHK+17] Roman Glebov, Carlos Hoppen, Tereza Klimoˇsov´a, Yoshiharu Kohayakawa, Daniel Kr´al’, and\\nHong Liu. Densities in large permutations and parameter testing. European J. Combin., 60:89–\\n99, 2017.\\nIra M. Gessel and Guoce Xin. The generating function of ternary trees and continued fractions.\\nElectron. J. Combin., 13(1):Research Paper 53, 48, 2006.\\nGabor T. Herman and Attila Kuba, editors. Discrete tomography. Applied and Numerical Har-\\nmonic Analysis. Birkh¨auser Boston, Inc., Boston, MA, 1999. Foundations, algorithms, and ap-\\nplications.\\n\\n[HK99]\\n\\n[HKM+13] Carlos Hoppen, Yoshiharu Kohayakawa, Carlos Gustavo Moreira, Bal´azs R´ath, and Rudini\\nMenezes Sampaio. Limits of permutation sequences. J. Combin. Theory Ser. B, 103(1):93–113,\\n2013.\\n\\n[KKRW20] Richard Kenyon, Daniel Kr´aˇl, Charles Radin, and Peter Winkler. Permutations with ﬁxed pat-\\n\\ntern densities. Random Structures Algorithms, 56(1):220–250, 2020.\\n\\n23\\n\\n\\x0c[MV07]\\n\\n[Nor08]\\n[OEI]\\n\\n[Pos09]\\n\\n[Rio02]\\n\\n[Sta99]\\n\\n[Sta12]\\n\\n(2022), The On-Line Encyclopedia\\n\\nHugh L. Montgomery and Robert C. Vaughan. Multiplicative number theory. I. Classical the-\\nory, volume 97 of Cambridge Studies in Advanced Mathematics. Cambridge University Press,\\nCambridge, 2007.\\nGustav Nordh. Perfect Skolem sets. Discrete Math., 308(9):1653–1664, 2008.\\nOEIS Foundation Inc.\\nhttps://oeis.org/.\\nAlexander Postnikov. Permutohedra, associahedra, and beyond. Int. Math. Res. Not. IMRN,\\n(6):1026–1106, 2009.\\nJohn Riordan. An introduction to combinatorial analysis. Dover Publications, Inc., Mineola,\\nNY, 2002.\\nRichard P. Stanley. Enumerative combinatorics. Vol. 2, volume 62 of Cambridge Studies in\\nAdvanced Mathematics. Cambridge University Press, Cambridge, 1999. With a foreword by\\nGian-Carlo Rota and appendix 1 by Sergey Fomin.\\nRichard P. Stanley. Enumerative combinatorics. Volume 1, volume 49 of Cambridge Studies in\\nAdvanced Mathematics. Cambridge University Press, Cambridge, second edition, 2012.\\n\\nInteger Sequences.\\n\\nof\\n\\nDepartment of Mathematics, Massachusetts Institute of Technology, Cambridge, MA 02139\\nEmail address, P. Jiradilok: pakawut@mit.edu\\n\\n24\\n\\n\\x0c',\n",
       " 'On the Importance of Asymmetry for Siamese Representation Learning\\n\\nXiao Wang∗,† Haoqi Fan1,† Yuandong Tian1 Daisuke Kihara2 Xinlei Chen1\\n\\n1Facebook AI Research (FAIR)\\n2Purdue University\\nCode: https://github.com/facebookresearch/asym-siam\\n\\n2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n3\\n1\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nMany recent self-supervised frameworks for visual rep-\\nresentation learning are based on certain forms of Siamese\\nnetworks. Such networks are conceptually symmetric with\\ntwo parallel encoders, but often practically asymmetric as\\nnumerous mechanisms are devised to break the symmetry.\\nIn this work, we conduct a formal study on the importance\\nof asymmetry by explicitly distinguishing the two encoders\\nwithin the network – one produces source encodings and the\\nother targets. Our key insight is keeping a relatively lower\\nvariance in target than source generally beneﬁts learning.\\nThis is empirically justiﬁed by our results from ﬁve case\\nstudies covering different variance-oriented designs, and\\nis aligned with our preliminary theoretical analysis on the\\nbaseline. Moreover, we ﬁnd the improvements from asym-\\nmetric designs generalize well to longer training schedules,\\nmultiple other frameworks and newer backbones. Finally,\\nthe combined effect of several asymmetric designs achieves\\na state-of-the-art accuracy on ImageNet linear probing and\\ncompetitive results on downstream transfer. We hope our\\nexploration will inspire more research in exploiting asym-\\nmetry for Siamese representation learning.\\n\\n1. Introduction\\n\\nDespite different motivations and formulations, many re-\\ncent un-/self-supervised methods for visual representation\\nlearning [1, 6–8, 18, 19, 44] are based on certain forms of\\nSiamese networks [4]. Siamese networks are inherently\\nsymmetric, as the two encoders within such networks share\\nmany aspects in design. For example, their model architec-\\ntures (e.g., ResNet [20]) are usually the same; their network\\nweights are often copied over; their input distributions –\\ntypically compositions of multiple data augmentations [8] –\\nare by default identical; and their outputs are encouraged to\\nbe similar for the same image. Such a symmetric structure\\nnot only enables straightforward adaptation from off-the-\\nshelf, supervised learning architectures to self-supervised\\nlearning, but also introduces a minimal inductive bias to\\n\\nFigure 1. Asymmetry for Siamese representation learning. For\\nthe two encoders in a Siamese network, we treat one as a source\\nencoder, and the other as a target encoder. We ﬁnd it generally\\nbeneﬁcial to have relatively lower variance in target than source.\\n\\nlearn representations invariant w.r.t. various transformations\\nin computer vision [10].\\n\\nHowever, symmetry is not the only theme in these frame-\\nworks.\\nIn fact, numerous mechanisms were proposed to\\nbreak the conceptual symmetry. For example, BYOL [18]\\nand SimSiam [10] place a special predictor head on one\\nof the encoders, so architecture-wise they are no longer\\nsymmetric; MoCo [19] introduces momentum encoder, in\\nwhich the weights are computed with moving-averages in-\\nstead of directly copied; SwAV [6] and DINO [7] addition-\\nally adopt a multi-crop [27] strategy to enhance the augmen-\\ntation on one side, shifting the data distribution asymmetric\\nbetween encoders; even the InfoNCE loss [28] treats out-\\nputs from two encoders differently – one is positive-only\\nand the other also involves negatives. Among them, some\\nspeciﬁc asymmetric designs are crucial and well-studied\\n(e.g., stop-gradient to prevent collapse [10]), but the gen-\\neral role of asymmetry for Siamese representation learning\\nis yet to be better understood.\\n\\nIn this paper, we conduct a more formal study on the\\nimportance of asymmetry for Siamese learning. Deviat-\\ning from the original meaning of ‘Siamese’, we explic-\\nitly mark the two encoders within the network function-\\nally different: a source encoder and a target encoder.1 The\\n\\n1Depending on the context, source has also been referred as query/on-\\n\\n∗: work done during internship at FAIR. †: equal contribution.\\n\\nline/student; and target as key/teacher in the literature [18, 19, 32].\\n\\n1\\n\\nlowervariancehighervariancelosssourcetargetx\\x0csource encoder generates source encodings, and updates its\\nweights via normal gradient-based optimization like in su-\\npervised learning. The target encoder updates its weights\\nonly with their source counterparts, and outputs target en-\\ncodings which in turn judge the quality of sources. This\\nasymmetric encoder formulation also covers symmetric en-\\ncoders (e.g., in SimCLR [8]), where the target weights can\\nbe simply viewed as source duplicates.\\n\\nWith this distinction, our key insight is that keeping a rel-\\natively lower variance in target encodings than source can\\nhelp representation learning (illustrated in Fig. 1). We sys-\\ntematically study this phenomenon with our MoCo v2 [9]\\nvariant beyond existing – but scattered – evidence in the\\nliterature [5, 6, 19, 24, 37]. Speciﬁcally, given a variance-\\noriented design, we ﬁrst quantify its encoding variance with\\nour baseline model, and then apply it to source or target (or\\nboth) encoders and examine the inﬂuence on learned repre-\\nsentations. In total, we have conducted ﬁve case studies to\\nexplore various design spaces, ranging from encoder inputs,\\nto intermediate layers and all the way to network outputs.\\nThe results are well-aligned with our insight: designs that\\nincrease encoding variance generally help when applied to\\nsource encoders, whereas ones that decrease variance favor\\ntarget. We additionally provide a preliminary theoretical\\nanalysis taking MoCo pre-training objective as an example,\\naimed at revealing the underlying cause.\\n\\nOur observation generalizes well. First, we show the\\nimprovements from asymmetry – lower variance in target\\nthan source – can hold with longer pre-training schedules,\\nsuggesting they are not simply an outcome of faster con-\\nvergence. Second, directly applying proper asymmetric\\ndesigns from MoCo v2 to a variety of other frameworks\\n(e.g., BYOL [18], Barlow Twins [44]) also works well,\\ndespite notable changes in objective function (contrastive\\nor non-contrastive), model optimization (large-batch train-\\ning [43] or not), etc. Third, using MoCo v3 [11], we\\nalso experimented a more recent backbone – Vision Trans-\\nformer (ViT) [14] – and ﬁnd the generalization still holds\\nwell. Finally, several asymmetric designs are fairly com-\\npositional: their combined effect enables single-node pre-\\ntrained MoCo v2 to reach a top-1 linear probing accuracy\\nof 75.6% on ImageNet, a state-of-the-art with ResNet-50\\nbackbone. This model also demonstrates good transferring\\nability to other downstream classiﬁcation tasks [8, 15, 18].\\n\\nIn summary, our study reveals an intriguing correlation\\nbetween the relative source-target variance and the learned\\nrepresentation quality. We have to note that such correla-\\ntion has limitations, especially as self-supervised learning\\nfollows a staged evaluation paradigm and the ﬁnal result is\\ninevitably inﬂuenced by many other factors. Nonetheless,\\nwe hope our exploration will raise the awareness of the im-\\nportant role played by asymmetry for Siamese representa-\\ntion learning, and inspire more research in this direction.\\n\\n2. Related Work\\n\\nSiamese networks are weight-sharing networks [4] that\\nprocess multiple inputs and produce multiple outputs in par-\\nallel. It has been widely used in computer vision [3,4,31,38]\\nand has recently caught attention in self-supervised learn-\\ning [8, 10]. This can be explained by the design of Siamese\\nnetworks, which can conveniently learn invariance in a\\ndata-driven fashion – a widely acknowledged property for\\nuseful visual representations [10]. While a na¨ıve applica-\\ntion of Siamese network can incur collapse, various formu-\\nlations and mechanisms (e.g., contrastive learning [8, 19],\\nonline balanced clustering [6, 7], extra predictor [10, 18],\\nvariance reduction loss [1, 44]) – many of them asymmetric\\n– have been proposed to maintain healthy learning dynam-\\nics. Our focus is not on collapse prevention. Instead, we\\nstudy generic designs that change encoding variance, ana-\\nlyze their effect on the output representations, and show that\\nan asymmetry between source and target helps learning.\\n\\nSymmetry for Siamese learning. While the theme of the\\npaper is asymmetry, symmetry is also a powerful concept\\nin Siamese learning. One advantage of symmetry is in re-\\nducing the computation cost when source and target en-\\ncoders share the same backbone weights. In such frame-\\nworks [8,10], source features can be reused for targets, sav-\\ning the extra need to compute with a second encoder. Re-\\ncently, symmetric designs alone are also shown to yield the\\nsame level of performance as asymmetric methods [1, 44].\\nInterestingly, there is often an attempt to symmetrize the\\nloss by forwarding image views once as source and once\\nas target [11, 18], even when the encoder weights are not\\nshared (e.g., in case of a momentum encoder [19]). Com-\\npared to using a single asymmetric loss but training for 2×\\nas long, this practice has the same number of forward/back-\\nward passes and we empirically verify it generates similar\\nresults across frameworks (see Sec. 6.2) [10]. Therefore, we\\nbelieve loss symmetrization is not essential beyond plausi-\\nble better performance at the ‘same’ training epochs.\\n\\nAsymmetric source-target variance. Asymmetry in vari-\\nance is already serving self-supervised learning in implicit\\nways. MoCo [19] itself is a successful example: by smooth-\\ning its target encoder, the memory bank stores consistent\\nkeys with smaller variance across training iterations. Mo-\\nmentum update has been extended to normalization statis-\\ntics to further reduce variance [5, 24], again applied on tar-\\ngets. State-of-the-art on ImageNet [37, 41, 47] is held by\\nusing high-variance, strong augmentations on source views.\\nSiamese networks are also popular in semi-supervised\\nlearning, where some examples are unlabeled. To create\\nmore reliable pseudo labels, the common practice is to aver-\\nage predicted labels over augmented views [2,30,36], which\\neffectively reduces variance on target. Such evidences are\\nscattered in the literature, and we analyze it systematically.\\n\\n2\\n\\n\\x0c(a) MultiCrop (Sec. 4.1)\\n\\n(b) ScaleMix (Sec. 4.2)\\n\\n(c) AsymAug (Sec. 4.3)\\n\\n(d) SyncBN (Sec. 4.4)\\n\\n(e) MeanEnc (Sec. 4.5)\\n\\nFigure 2. We present ﬁve case studies exploring different variance-oriented designs for source and target encoders. For each column, we\\nshow the speciﬁc design on the top, and its inﬂuence on the encoding variance (both the cumulative distribution function and the mean on\\nthe validation set as our empirical reference) at the bottom. Each design is then applied to either the source, the target, or both encoders.\\nThe resulting representation is evaluated by linear probing on ImageNet. Best viewed on a screen and zoomed in. See Sec. 4 for details.\\n\\n3. Methodology Overview\\n\\nIn this section we give an overview for our methodology\\nto systematically study variance-oriented encoder designs.\\nFirst, we specify our variance of interest. While exactly\\nquantifying such variance during training is hard, we pro-\\nvide an approximate reference for such variance using our\\nbaseline model. Now, for each design we can then compute\\nits variance reference and quantify the relative change in\\ncomparison to a vanilla encoder. Regardless of the change\\n(higher or lower), we plug-in the design to either the source,\\nthe target, or both encoders and see its inﬂuence on result-\\ning representations after pre-training. The inﬂuence is mea-\\nsured by linear probing on ImageNet [13]. For a particular\\ndesign, if applying it to both (or neither) encoders is bet-\\nter, then it implies maintaining symmetry is important; if it\\nprefers either source or target, then it means asymmetry is\\nbeneﬁcial. In such cases, we also check whether the change\\nin variance is correlated with the encoder preference.\\n\\nIn total, we have conducted ﬁve case studies exploring\\nvarious design spaces, ranging from encoder inputs (i.e.,\\ndata augmentations), to intermediate layers (i.e., different\\nbatch sizes for Batch Normalization [21]) all the way to net-\\nwork outputs (i.e., averaging multiple encodings to reduce\\nvariance). Fig. 2 shows these designs and their variance\\nplots in conjunction with our baseline. We detail our base-\\nline and each case study in Sec. 4, and ﬁrst motivate our\\nvariance of interest and its reference in the following.\\n\\nVariance of interest. As each encoding is the encoder out-\\nput of an augmented view from an image, the total variance\\ni) changes\\nin encodings mainly comes from three types:\\nto the encoder, ii) changes across images, and iii) changes\\nwithin a single image. For type i), MoCo [19] with its mo-\\nmentum encoder is already a major, well-studied asymmet-\\n\\nric design that intuitively reduces the target variance across\\ntraining iterations. For type ii), as Siamese representation\\nlearning encourages uniformity [10, 35], the cross-image\\nvariance quickly converges to a constant dependent only on\\nencoding dimensions (evidenced in Appendix A).2 There-\\nfore, we focus on type iii), i.e., intra-image variance as the\\nmain subject of our study. Note that it does not restrict us\\nto design input augmentations as the only means to adjust\\nvariance, as will be discussed in Secs. 4.4 and 4.5.\\n\\nVariance reference. Exactly quantifying intra-image vari-\\nance requires sampling all possible augmentations of all im-\\nages and forward all of them to obtain encodings for all\\ntraining steps. Even if possible, this process is highly ex-\\npensive and also probably unnecessary. Therefore, we re-\\nsort to an approximation with the goal of keeping a refer-\\nence to characterize the encoding variance when changed.\\n\\nTo this end, we simply augment each image in the val-\\nidation set r times and feed them to a pre-trained baseline\\nencoder. The output encodings are then used to compute\\nthe per-image, intra-sample variance, which jointly form a\\ndistribution. All variances across the entire set are then av-\\neraged to a single value v, the reference variance used to\\nmeasure different designs. More details are listed in Sec. 7.\\n\\n4. Case Studies for Source-Target Variance\\n\\nIn this section, we introduce our baseline and perform\\nﬁve empirical case studies exploring the impact of differ-\\nent designs. For each one of them, we record its corre-\\nsponding variance reference v, and linear-probing accura-\\ncies when placed on encoders with different conﬁgurations\\n\\n2If encodings are uniformly distributed on the unit hypersphere (due to\\n(cid:96)2 normalization), their variance is 1/d where d is the encoding dimension.\\n\\n3\\n\\n\\x0cwithout preset bias. Since our goal is to analyze the behav-\\nior, all models in this section are pre-trained for 100 epochs,\\nwith the generalization toward longer schedules deferred\\nto Sec. 6.1 after we draw the connection between variance\\nchange and encoder preference in Sec. 4.6.\\n\\nBaseline. Our baseline is an improved variant of MoCo\\nv2 [9], which itself is an improved baseline over origi-\\nnal MoCo [19].\\nIt consists of a gradient-updated source\\nencoder fs, a momentum-updated target encoder ft, and\\nan encoding-updated memory bank [40].\\nInspired by\\nSimCLR [8], each MoCo v2 encoder further uses a pro-\\njection head (projector), which is a 2-layer MLP without\\nBatch Normalization (BN) [21] in-between. Our baseline\\nadds an additional fully connected layer (2048-d, with BN)\\nbefore the 2-layer MLP. Inherited from MoCo v1, all BNs\\nin fs are performed per GPU device, and all BNs in ft are\\nshufﬂed [19]. All the output encodings z are (cid:96)2 normalized\\nto unit-length vectors before InfoNCE loss [28]. We do not\\nemploy any loss symmetrization [6,18] in this baseline, thus\\none source/target pair only contributes to the loss once.\\n\\nCompared to vanilla MoCo v2 [9], our baseline is gen-\\nerally better in linear probing on ImageNet [13] (detailed\\nin Sec. 7). The table below summarizes the top-1 accuracy\\n(%) using ResNet-50 [20] and the same evaluation protocol:\\n\\nMoCo v2 [9]\\nMoCo v2, ours\\n\\n100 ep\\n64.7\\n65.8\\n\\n200 ep\\n67.9\\n69.0\\n\\n400 ep\\n69.6\\n70.5\\n\\n800 ep\\n70.7\\n71.9\\n\\nThe improvement (∼1 percent) is consistent across different\\nnumber of training epochs. We also notice no degradation\\nin object detection transfer on VOC [16] – e.g., achieving\\n57.4 mAP at 800 pre-training epochs, same as original [9].\\nThe variance reference for our baseline v0 is 8.5 (×10−4).\\n\\n4.1. Study 1: MultiCrop Augmentation\\n\\nWe begin our study with an existing design in the litera-\\nture – multi-crop augmentation (or ‘MultiCrop’) [6, 7, 27].\\nBesides the two basic views needed for Siamese learning,\\nMultiCrop takes additional views from each image per iter-\\nation. To alleviate the added computation cost, a common\\nstrategy is to have m low-resolution crops (e.g., 96×96 [6])\\ninstead of standard-resolution crops (224×224) as added\\nviews (illustrated in Fig. 2a top for m=4). As a side effect,\\ninputting small crops can potentially increase the variance\\nfor an encoder due to the size and crop-distribution changes.\\nThis is conﬁrmed in Fig. 2a bottom, where we compare\\nthe variance distribution of MultiCrop to our baseline on\\nthe ImageNet val set. We show the cumulative distribution\\nfunction in solid lines with increasing per-image variances\\nfrom left to right, and the mean variances v and v0 in dotted\\nvertical lines. MultiCrop has signiﬁcantly higher variance\\nthan our baseline: v=38.0 vs. 8.5 (×10−4).\\n\\nWe plug-in MultiCrop to either the source, the target, or\\nboth encoders (detailed in Appendix D). The table below\\n\\nsummarizes the corresponding top-1 accuracy and change\\n(∆) to the baseline in linear probing:\\n\\n+MultiCrop ( ↑ )\\n\\nneither\\n\\nsource\\n\\naccuracy (%)\\n∆ (%)\\n\\n65.8\\n/\\n\\n69.9\\n+4.1\\n\\ntarget\\n\\n57.1\\n-8.7\\n\\nboth\\n\\n61.7\\n-4.1\\n\\nAs a design that increases variance (indicated by ‘ ↑ ’\\nin table), MultiCrop improves the accuracy substantially\\n(+4.1%) when applied to the source encoder, and hurts\\nwhen applied to the target. When applied to both, the per-\\nformance also degenerates signiﬁcantly (-4.1%), even with\\nmore crops processed per training iteration than to source\\nalone. These results indicate that the source encoder is the\\npreferred place of applying MultiCrop (column shaded in\\ngray ) – which also matches the common protocols in the\\nliterature when multi-crop augmentation is used [6, 7, 27].\\n\\n4.2. Study 2: ScaleMix Augmentation\\n\\nNext, we introduce and study a different type of augmen-\\ntation called ‘ScaleMix’, illustrated in Fig. 2b top (more\\ndetails are found in Appendix B). As the name suggests,\\nit generates new views of an image by mixing two views\\nof potentially different scales together via binary masking.\\nThe masking strategy follows CutMix [29], where an entire\\nregion – denoted by a box with randomly sampled coordi-\\nnates – is cropped and pasted. Unlike CutMix, ScaleMix\\nonly operates on views from the same image, and the out-\\nput is a single view of standard size (224×224). This single\\nview can be regarded as an efﬁcient approximation of mul-\\ntiple crops in MultiCrop, without the need to process small\\ncrops separately. Like MultiCrop, ScaleMix also introduces\\nextra variance to the encoding space (as shown in Fig. 2b\\nbottom), with a mean variance of v=29.5 (×10−4).\\n\\nAgain, we apply ScaleMix augmentation to the source,\\nthe target, or both encoders without preset preference. The\\nresults for linear probing are summarized in the table below:\\n\\n+ScaleMix ( ↑ )\\n\\nneither\\n\\nsource\\n\\naccuracy (%)\\n∆ (%)\\n\\n65.8\\n/\\n\\n67.3\\n+1.5\\n\\ntarget\\n\\n52.8\\n-13.0\\n\\nboth\\n\\n64.8\\n-1.0\\n\\nWe observe a similar trend as the MultiCrop case: ScaleMix\\nbeneﬁts source encoders, harms target encoders, and the ef-\\nfect neutralizes when applied to both. This suggests source\\nencoder is again the preferred choice for ScaleMix.\\n\\n4.3. Study 3: General Asymmetric Augmentations\\n\\nThe original v2 recipe is symmetric:\\n\\nMultiCrop and ScaleMix are mostly on geometric trans-\\nformations of images. Next, we study the behavior by vary-\\ning other ingredients in the MoCo v2 augmentation recipe.\\nthe same set of\\naugmentations (e.g., random resized cropping, color jitter-\\ning [40], blurring [8]) is used for both source and target.\\nIn this case study, we add or remove augmentations (be-\\nyond geometric ones), and present two more recipes: one\\n\\n4\\n\\n\\x0cdeemed stronger (‘StrongerAug’), and the other weaker\\n(‘WeakerAug’) compared to the original one (detailed in\\nAppendix D). Together, they can form general asymmet-\\nric augmentation recipes for source and target. Comply-\\ning with the intuition, we ﬁnd StrongerAug has higher vari-\\nance 19.7 (×10−4), and WeakerAug has lower variance 6.9\\n(×10−4) w.r.t. to the baseline v0 (shown in Fig. 2c bottom).\\nThe results are split into three tables for clarity. The in-\\n\\nﬂuence of WeakerAug is summarized ﬁrst:\\n\\n+WeakerAug ( ↓ )\\n\\nneither\\n\\nsource\\n\\naccuracy (%)\\n∆ (%)\\n\\n65.8\\n/\\n\\n51.0\\n-14.8\\n\\ntarget\\n\\n67.2\\n+1.4\\n\\nboth\\n\\n46.8\\n-19.0\\n\\nInterestingly, the effect of WeakerAug on source/target en-\\ncoder is opposite compared to the previous studies:\\nit\\nhurts source but helps target (referred as ‘AsymAug’). A\\nsymmetric WeakerAug on both does not work, suggesting\\nthe heavy reliance of Siamese learning on augmentation\\nrecipes [8, 18]. On the StrongerAug side:\\n\\n+StrongerAug ( ↑ )\\n\\nneither\\n\\nsource\\n\\naccuracy (%)\\n∆ (%)\\n\\n65.8\\n/\\n\\n66.7\\n+0.9\\n\\ntarget\\n\\n62.2\\n-3.6\\n\\nboth\\n\\n66.2\\n+0.4\\n\\nIt helps most when used only on source, but harms accu-\\nracy when used only on target. For completeness, we also\\nexperimented changing augmentation strength in opposite\\ndirections for source and target:\\n\\nStronger & Weaker\\n\\nsource ↑ target ↓\\n\\nsource ↓ target ↑\\n\\naccuracy (%)\\n∆ (%)\\n\\n67.2\\n+1.4\\n\\n44.3\\n-21.5\\n\\nCompared to having WeakerAug on target alone (67.2%),\\nfurther adding StrongerAug on source does not bring ex-\\ntra gains. In contrast, stronger augmentations on target and\\nweaker augmentations on source results in the worst perfor-\\nmance in all the cases we have studied.\\n\\n4.4. Study 4: Sync BatchNorm\\n\\nAlthough input data augmentation is a major source of\\nintra-image variance, it is not the only cause of such vari-\\nance within output encodings. One notable source lies in\\nintermediate BN layers [21], a popular normalization tech-\\nnique in modern vision architectures [20]. During training,\\nthe statistics for BN are computed per-batch, which means\\nif other images within the batch are replaced, the output will\\nlikely change even if the current image stays the same. As\\na result, the magnitude of this variance is largely controlled\\nby the batch size: a sufﬁciently large size can provide nearly\\nstable statistics, whereas for small batches (e.g., below 16)\\nthe estimation is generally less accurate [39]. For MoCo\\nv2, its effective batch size is 32, because the default BN\\nperforms normalization only on the same device (256 im-\\nages/8 GPUs).3 A natural alternative is to employ SyncBN\\n\\n3MoCo v2 inherits MoCo v1 and uses ‘shufﬂed BN’ in ft. It shufﬂes\\nthe input to avoid cheating but the normalization still happens per-device.\\n\\nthat normalizes over all devices, so the batch size is 256 (il-\\nlustrated in Fig. 2d top for 4 devices). From the zoomed-in\\nvariance plot (Fig. 2d bottom), SyncBN leads to a slight de-\\ncrease in variance from 8.5 to 8.3 (×10−4) in this case –\\nsuggesting 32 is already sufﬁciently stable in our baseline.\\nFor efﬁciency and generalizability, we replace the single\\nBN in our 3-layer projector with SyncBN.4 As before, we\\ntried different combinations on encoders and the results are:\\n+SyncBN ( ↓ )\\n\\nneither\\n\\nsource\\n\\ntarget\\n\\nboth\\n\\naccuracy (%)\\n∆ (%)\\n\\n65.8\\n/\\n\\n64.7\\n-0.9\\n\\n66.5\\n+0.7\\n\\n66.0\\n+0.2\\n\\nDespite the seemly minor modiﬁcation, SyncBN still leads\\nto a notable improvement when applied to target (referred\\nas ‘AsymBN’) and degeneration to source. SyncBN on both\\nencoders is at-par with the baseline per-device BNs.\\n\\n4.5. Study 5: Mean Encoding\\n\\nIn this last study we focus on the encoder output. Ac-\\ncording to basic statistics, a direct approach to reduce the\\nvariance of a random variable is to perform i.i.d. sam-\\npling multiple times and take the mean as the new variable.\\nSpeciﬁcally for v, we can reduce it by a factor of ∼n if the\\noutput encoding z is averaged from n separate encodings\\n{z1, . . . , zn} (illustrated in Fig. 2e top for n=2).5 These\\nencodings can be simply generated by running the same en-\\ncoder on n augmented views of the same image (detailed\\nin Appendix D). For example, we show v is 4.2 (×10−4),\\nabout half of v0 when two encodings are averaged in Fig. 2e\\nbottom. We name this design ‘MeanEnc’ for an encoder.\\n\\nAs discussed in our Sec. 2 (also shown in [10]), increas-\\ning the number of views per training iteration can lead to\\nbetter performance by itself. To minimize this effect, we\\nconduct our main analysis of MeanEnc by ﬁxing the total\\nnumber of views to 4 per training iteration. The 4 views are\\nsplit between source (ns) and target (nt) encoders, shown\\nin the ﬁrst 3 result columns below:\\n\\n+MeanEnc ( ↓ )\\n\\naccuracy (%)\\n∆ (%)\\n\\nns =1\\nnt =3\\n67.9\\n+2.1\\n\\nns =2\\nnt =2\\n67.1\\n+1.3\\n\\nns =3\\nnt =1\\n59.9\\n-5.9\\n\\nns =1\\nnt =2\\n67.5\\n+1.7\\n\\nWith more views in the target encoder (and simultane-\\nously fewer views in source), we observe a trend for better\\naccuracy. Having 2 views in both encoders still keeps sym-\\nmetry, so its improvement over baseline (65.8%) is an out-\\ncome of more views. For simplicity, we also experimented\\nMeanEnc with 2 views in the target encoder alone (last col-\\numn). The result strikes a better balance between speed and\\naccuracy, so we pick this setting as default for MeanEnc.\\n\\n4Replacing all BNs including ones in ResNet also exhibits the same\\npattern. Replacing BNs in projector only is noticeably faster, and general-\\nizes to other BN-free backbones such as ViT [14].\\n\\n5Here the reduction is approximate because we jointly forward multiple\\nviews which doubles or triples the batch size in BN; and encodings are\\nfurther (cid:96)2 normalized before calculating v.\\n\\n5\\n\\n\\x0cvariance change\\nencoder preference\\n\\nMultiCrop\\n(Sec. 4.1)\\n↑\\nsource\\n\\nScaleMix\\n(Sec. 4.2)\\n↑\\nsource\\n\\nWeakerAug\\n(Sec. 4.3)\\n↓\\ntarget\\n\\nStrongerAug\\n(Sec. 4.3)\\n↑\\nsource\\n\\nSyncBN\\n(Sec. 4.4)\\n↓\\ntarget\\n\\nMeanEnc\\n(Sec. 4.5)\\n↓\\ntarget\\n\\nTable 1. Summary of the 6 designs covered in our case studies. For each design, we list its qualitative change in intra-image variance v,\\nand its preferred encoder. We see a consistent pattern that higher-variance designs prefer source, whilst lower-variance ones prefer target.\\n\\n4.6. Summary of Studies\\n\\nwrite the gradient ﬂow of W as:\\n\\nIn total, we covered 6 variance-oriented designs in the\\n5 case studies described above. Interestingly, none of them\\nachieves best result when designs are symmetrically applied\\nto both (or neither) encoders. Instead, all of them have a\\nsingle preferred encoder in the Siamese network. This phe-\\nnomenon directly supports the importance of asymmetry for\\nSiamese representation learning.\\n\\nMoreover, we observe a consistent pattern: designs that\\nintroduce higher encoding variance generally help when\\nplaced on source encoders, whereas designs that decrease\\nvariance favor target encoders. We summarize the relation\\nbetween: i) change of variance and ii) encoder preference\\nin Tab. 1. This is well-aligned with our insight: the speciﬁc\\nasymmetry of a relatively lower variance in target encod-\\nings than source can beneﬁt Siamese representation learn-\\ning, and not the other way around.\\n\\nFrom the results, we do have to note that such a pattern\\nholds within a reasonable range of v, and more extreme\\nasymmetry does not always lead to better performance (e.g.,\\nwhen further increasing source augmentation strength while\\nhaving WeakerAug in target). Moreover, asymmetry is usu-\\nally not the only factor in play for self-supervised frame-\\nworks; other factors (e.g. the number of views in MeanEnc)\\ncan also inﬂuence the ﬁnal outcome of our pipelines.\\n\\n5. Theoretical Analysis for Variance\\n\\nHere we aim to provide a preliminary theoretical analysis\\nfor MoCo following [33, 34] (More details in Appendix C).\\nConsider the following simpliﬁed InfoNCE objective:6\\n\\nL = −\\n\\n1\\nN\\n\\nN\\n(cid:88)\\n\\ni=1\\n\\nlog\\n\\n(cid:80)\\n\\nexp(Sii(cid:48)/τ )\\nj(cid:54)=i exp(Sij(cid:48)/τ )\\n\\n,\\n\\n(1)\\n\\ni z(cid:48)\\n\\nwhere N is batch size, τ is temperature, Sii(cid:48)=z(cid:62)\\ni and\\nSij(cid:48)=z(cid:62)\\nj are pairwise similarities between source encod-\\nings zi and targets z(cid:48)\\ni (target weights and encodings all\\ncome with prime (cid:48)). For MoCo, gradients are only back-\\npropagated through the source zi, but not z(cid:48)\\n\\ni z(cid:48)\\n\\ni or z(cid:48)\\nj.\\n\\nNow, let’s take the last linear layer immediately before z\\nas an example for analysis. Let f be the input features of this\\nlayer, W be its weight matrix (so z=W f ), and denotes co-\\nefﬁcients αij(cid:48)= exp(Sij(cid:48)/τ )/ (cid:80)\\nk(cid:54)=i exp(Sik(cid:48)/τ ), we can\\n\\n6We make two simpliﬁcations to InfoNCE [28] by ignoring (cid:96)2 normal-\\n\\nization and the positive term exp(Sii(cid:48) /τ ) in the denominator [42].\\n\\ndL\\ndW\\n\\n= W (cid:48) 1\\nτ N\\n\\nN\\n(cid:88)\\n\\n(cid:88)\\n\\ni=1\\n\\nj(cid:54)=i\\n\\nαij(cid:48)(f (cid:48)\\n\\nj − f (cid:48)\\n\\ni )f (cid:62)\\ni .\\n\\n(2)\\n\\nTo study the behavior of gradients especially w.r.t. our\\nvariance of interest, we can model intra-image variance as\\nan additive noise in f (and f (cid:48)) that affects training. Specif-\\nically, let ˜f be the feature corresponding to the original im-\\nage, we can assume:\\n\\ni]=Σ(cid:48).\\n\\ni =˜fi+e(cid:48)\\n\\ni]=¯e(cid:48) and V[e(cid:48)\\n\\n• Source features fi=˜fi+ei, with E[ei]=¯e and V[ei]=Σ;\\ni, with E[e(cid:48)\\n• Target side f (cid:48)\\nE[·] computes expectation and V[·] outputs variance.\\nNote that ˜fi and ˜fj are from different images, while ei, e(cid:48)\\ni\\nand e(cid:48)\\nj model intra-sample variance that comes from mul-\\ntiple sources, e.g., input augmentations, BNs with different\\nbatch sizes (Sec. 4.4), etc. Due to the independent augmen-\\ntation process, these noises are modeled as independent of\\neach other.\\n\\nUnder such setting, we can arrive at the following result\\n(detailed derivations in Appendix C) to better understand\\nour observation from a theoretical perspective:\\n\\nHigher variance on the target side is not necessary and\\ncan be less stable. With higher variance on the target side\\n(i.e., Σ(cid:48) has larger eigenvalues), the variance of the gradi-\\nent w.r.t. W , V[dL/dW ], will become larger without affect-\\ning its expectation E[dL/dW ]. Intuitively, this asymmetry\\ncomes from an asymmetric structure in Eq. (2): there is a\\nsubtraction term (f (cid:48)\\ni ) on the target side, but not on the\\nsource side (fi). To make the training dynamics more sta-\\nble, maintaining a relative lower variance on the target side\\nthan source is preferred.\\n\\nj−f (cid:48)\\n\\n6. Generalization Studies and Results\\n\\nThe keyword of this section is generalization, for which\\nwe study our insight for Siamese learning under various\\nconditions. Speciﬁcally for MoCo v2, we study the behav-\\nior of asymmetric designs by training with longer sched-\\nules, and by composing multiple designs together. As a by-\\nproduct, our ﬁnal model achieves state-of-the-art on Ima-\\ngeNet, and performs well beyond when transferred to other\\ndatasets. Besides MoCo v2, we seek generalizations across\\nmore frameworks and backbones and ﬁnd it also holds well.\\nUnless otherwise speciﬁed, all the evaluations are top-1 lin-\\near probing accuracy on ImageNet [13].\\n\\n6\\n\\n\\x0cFigure 3. Generalization to longer pre-training. Here y-axis is accuracy (%) and x-axis is number of epochs (log-scale). Asymmetric\\ndesigns consistently outperform the baseline. MultiCrop as the single strongest one reaches 73.7% at 800-ep without loss symmetrization.\\n\\n(%)\\n\\nMoCo v3 [11]\\nasym., 2× / ∆\\nSimCLR [8]\\nasym., 2× / ∆\\nBYOL [18]\\nasym., 2× / ∆\\nSimSiam [10]\\nasym., 2× / ∆\\nBarlow Twins [44]\\nasym. / ∆\\n\\nbaseline\\n69.9\\n69.7\\n65.0\\n65.0\\n69.5\\n69.0\\n67.8\\n67.4\\n66.8\\n66.4\\n\\nScaleMix AsymBN MeanEnc\\n70.1\\n+0.4\\n65.8\\n+0.8\\n69.9\\n+0.9\\n68.0\\n+0.6\\n66.6\\n+0.2\\n\\n70.6\\n+0.9\\n66.4\\n+1.4\\n69.7\\n+0.7\\n68.0\\n+0.6\\n67.1\\n+0.7\\n\\n70.7\\n+1.0\\n66.3\\n+1.3\\n70.4\\n+1.4\\n68.7\\n+1.3\\n67.3\\n+0.9\\n\\nTable 2. Generalization to more frameworks. We cover 5 of them\\nand convert each to and asymmetric one ﬁrst. In the second col-\\numn, we show similar results using our asymmetric versions com-\\npared to the original ones at 100-ep (in gray), optionally with 2×\\ntraining schedules.7 On top of these, we ﬁnd asymmetric designs\\nhelp learning across the board: third to ﬁfth columns list accura-\\ncies and improvements over the asymmetric baseline.\\n\\n6.1. Longer Training\\n\\nThe ﬁrst generalization is to longer training schedules.\\nMost Siamese learning frameworks [6, 8, 18], including\\nour baseline MoCo v2, produce substantially better results\\nin linear probing with more training epochs. Meanwhile,\\nlower variance in target – in the extreme a ﬁxed target per\\nimage, could result in faster convergence closer to super-\\nvised learning where longer training is not as helpful [20].\\nWe run our baseline with the ﬁve asymmetric setups studied\\nin Sec. 4 for 200, 400 and 800 epochs to check the behav-\\niors, and put the trends in Fig. 3. Overall, all the asymmetric\\nmodels outperform the baseline across different epoch num-\\nbers. The maintained gap suggests the gain from asymmetry\\ncannot be simply explained away by faster convergence.\\n\\n6.2. More Frameworks\\n\\nNext we examine the generalization to other frameworks.\\nRoughly ranked by its similarity to our baseline MoCo v2\\nfrom closest to furthest, they are: i) MoCo v3 [11], where\\nthe memory bank is replaced by large batch sizes [43];\\nii) SimCLR [8], where no momentum encoder is needed;\\niii) BYOL [18], where the contrastive formulation is chal-\\nlenged by learning only on comparing positive pairs; iv)\\nSimSiam [10], where neither momentum encoder nor nega-\\ntive pairs are required; and v) Barlow Twins [44], where a\\nfully symmetric pipeline for Siamese learning is discovered.\\nNote that we only outlined major differences above and\\nmore subtleties (including detailed setup for each frame-\\nwork in this paper) are found in Appendix D.\\n\\n(%)\\nMoCo v3, ViT [11]\\nasym., 2× / ∆\\n\\nbaseline\\n69.1\\n68.7\\n\\nScaleMix AsymBN MeanEnc\\n69.4\\n+0.7\\n\\n69.4\\n+0.7\\n\\n69.1\\n+0.4\\n\\nTable 3. Generalization to ViT [14], a new architecture gaining\\npopularity in vision and is recently studied in MoCo v3 [11]. The\\nprocedure and table format follow Tab. 2.\\n\\nFor ease of applying asymmetric designs to these frame-\\nworks, we ﬁrst convert their symmetrized components to an\\nasymmetric form following our source-target formulation.\\nA popular one is loss symmetrization, used by all except\\nBarlow Twins. We remove it by only forwarding a pair of\\nviews through the network once (instead of twice) per it-\\neration. Intuitively, training 2× as long can roughly com-\\npensate for the symmetrized loss with fair amount of com-\\npute, as discussed in Sec. 2 and analyzed in [10]. More-\\nover, methods without momentum encoders [8,10,44] reuse\\nsource encoders for targets.\\nIn such cases, we explic-\\nitly maintain a target encoder by using an online clone of\\nthe source one, and stopping gradients from ﬂowing into\\nthe branch – a choice deviated from SimCLR and Barlow\\nTwins [8, 44]. We show in Tab. 2 (second column) that our\\nasymmetric versions work similarly in accuracy compared\\nto the original ones, despite the above modiﬁcations.7\\n\\nWe pick ScaleMix, AsymBN and MeanEnc as three rep-\\nresentative designs which range from encoder inputs to\\noutputs. MultiCrop is relatively well studied in the liter-\\nature [6, 7] and we ﬁnd it non-trivial to train MultiCrop\\nwith large batch sizes [8, 11, 18, 44]. More recent frame-\\nworks [11,18,44] already employ stronger asymmetric aug-\\nmentation recipes [18] like AsymAug. Thus we did not in-\\nclude them in our comparisons listed in Tab. 2 (last three\\ncolumns). Our asymmetric source-target designs generalize\\nwell beyond MoCo v2, showing consistent improvements\\nacross the board with same number of pre-training epochs.\\n\\n6.3. ViT Backbone\\n\\nWith MoCo v3, we also benchmarked a newly proposed\\nbackbone: ViT [14]. We follow the same procedure by ﬁrst\\nbuilding an asymmetric baseline and then applying different\\ndesigns (detailed in Appendix D). Again, we ﬁnd asymme-\\ntry works well (Tab. 3). The only notable difference is the\\nreduced gap for ScaleMix, which is likely related to patches\\nfed for ViT not aligned with ScaleMix masks [22].\\n\\n7We keep all the optimization hyper-parameters the same when running\\nthe asymmetric version. The results can be further improved when e.g.\\nlearning rate is adjusted following the batch size change [17].\\n\\n7\\n\\n100200400800657065.869.969.071.870.572.871.973.7MultiCropbaseline100200400800657065.867.369.070.070.572.071.973.2ScaleMixbaseline100200400800657065.867.269.069.770.571.371.972.5AsymAugbaseline100200400800657065.866.469.069.370.571.271.972.3AsymBNbaseline100200400800657065.867.569.070.270.571.871.972.6MeanEncbaseline\\x0cSupervised\\nSimCLR [8]\\nBYOL [18]\\nNNCLR [15]\\nOurs, 1600-ep\\n\\nFood-101 CIFAR-10 CIFAR-100 Birdsnap SUN-397\\n78.3\\n71.6\\n78.4\\n79.0\\n77.8\\n\\n93.6\\n90.6\\n91.3\\n93.7\\n92.8\\n\\n53.7\\n37.4\\n57.2\\n61.4\\n58.5\\n\\n61.9\\n58.8\\n62.2\\n62.5\\n67.8\\n\\n72.3\\n68.4\\n75.3\\n76.7\\n79.4\\n\\nCars\\n66.7\\n50.3\\n67.8\\n67.1\\n69.7\\n\\nAircraft VOC-07 DTD\\n74.9\\n87.5\\n74.5\\n85.5\\n75.5\\n82.5\\n75.5\\n83.0\\n80.2\\n93.8\\n\\n61.0\\n50.3\\n60.6\\n64.1\\n59.3\\n\\nPets\\n91.5\\n83.6\\n90.4\\n91.8\\n87.2\\n\\nCaltech-101 Flowers\\n\\n94.5\\n90.3\\n94.2\\n91.3\\n93.1\\n\\n94.7\\n91.2\\n96.1\\n95.1\\n92.5\\n\\nTable 4. Generalization by transferring our model to 12 different downstream datasets with linear probing. We follow the protocol\\nof [15, 18] and report results on the test set. For VOC-07, we cite the improved numbers from [44] for fair comparisons. Our 1600-ep\\nmodel achieves best results on 5 out of 12, while being less competitive on tasks with iconic images (such as CIFAR [23] and Aircraft [26]).\\n\\n6.4. Design Compositions\\n\\nAs another aspect for generalization, we compose mul-\\ntiple asymmetric designs together and check their joint ef-\\nfect on representation quality. To this end, we fall back to\\nour MoCo v2 baseline (100-ep) and start from our strongest\\nsingle asymmetric design, MultiCrop. When pairing it with\\nother two input designs (ScaleMix an AsymAug), we ﬁnd\\ntheir added value has mostly diminished so we did not in-\\nclude them. On the target side, we ﬁrst enabled SyncBN,\\nand then enabled MeanEnc (nt =2) to reduce variance, and\\nboth designs further improved performance:\\n+MultiCrop +MultiCrop\\n+AsymBN\\n\\ncompositions\\n\\nnone\\n\\n65.8\\n-\\n\\n70.4\\n+4.6\\n\\naccuracy (%)\\n69.9\\n∆ (%)\\n+4.1\\nWhile our exploration on this front is preliminary and im-\\nprovement is not guaranteed (as discussed in Sec. 4.6), it in-\\ndicates different asymmetric designs can be compositional.\\nFinally, we pre-train our best composition (shaded col-\\numn above) for 1600 epochs to check its limit. We arrive at\\n75.6% on ImageNet linear probing (more details in Sec. 7).\\nThis puts us in the state-of-the-art cohort [37, 41, 47] with\\nsingle-node training and no other bells or whistles.\\n\\n+MultiCrop\\n+AsymBN\\n+MeanEnc\\n71.3\\n+5.5\\n\\n6.5. Transfer Learning\\n\\nIn Tab. 4, we show transfer learning results of our ﬁnal\\nImageNet 1600-ep model to 12 standard downstream classi-\\nﬁcation tasks for linear probing [8,15,18]. For each dataset,\\nwe search the learning rate on the validation set and report\\nresults on the test set, following the protocol of [15,18] (see\\nAppendix D). Our model performs competitively against\\nthe most recent NNCLR [15]), achieving best on 5 tasks but\\nlags behind on ones with iconic images. We hypothesis it’s\\ndue to MultiCrop which used local small crops. We further\\ntransferred to Places-205 [46], which focuses on scene-level\\nunderstanding. We ﬁnd our model indeed achieves state-of-\\nthe-art (56.8%), slightly better than SwAV [6] which also\\nused MultiCrop. These results verify our learned represen-\\ntation is effective beyond ImageNet.\\n\\n7. Implementation Details\\n\\nWe list the most important implementation details for our\\n\\npaper below. Other subtleties are found in Appendix D.\\n\\nVariance reference. We use ImageNet val set (50k images\\nin total), r=32 views, and the 800-ep pre-trained baseline\\nsource encoder for variance calculation.8 Encodings are\\n(cid:96)2 normalized. To fully mimic the pre-training setting, we\\nuse online per-batch statistics for BN, not recorded moving-\\naverage ones from the training set.\\n\\nPre-training. By default, we adopt the same MoCo v2\\nsetup (e.g., augmentation recipe, SGD optimizer etc.) for\\nexperiments on our baseline. A half-cycle cosine learning\\nrate decay schedule [25] is used given the number of pre-\\ntraining epochs. Mixed-precision is enabled for efﬁciency.\\n\\nLinear probing. Linear probing freezes backbone after\\npre-training, and only trains a linear classiﬁer on top of\\nthe global image features to test the representation qual-\\nity. By default on ImageNet, we use LARS [43] opti-\\nmizer with batch size 4096, initial learning rate lr=1.6 (lin-\\nearly scaled [17]), weight decay 0 and train the classiﬁer\\nfor 90 epochs with a half-cycle cosine schedule following\\nSimSiam [10]. We choose LARS over SGD as the former\\nshows better adaptation for explorations, without the need\\nto search hyper-parameters (e.g. lr) extensively for good\\nperformance. For our ﬁnal model, we switched back to\\nSGD optimizer following MoCo [20], with an initial learn-\\ning rate of 120 and batch size of 256.\\n\\n8. Conclusion\\n\\nThrough systematic studies, we have revealed an inter-\\nesting correlation between the asymmetry of source-target\\nvariance and the representation quality for Siamese learn-\\ning methods. While such a correlation is conditioned on\\nother factors and certainly not universal, we ﬁnd as guide-\\nline it is generally applicable to various training sched-\\nules, frameworks and backbones. Composing asymmet-\\nric designs helps us achieve state-of-the-art with MoCo v2,\\nand the learned representation transfers well to other down-\\nstream classiﬁcation tasks. We hope our work will inspire\\nmore research exploiting the importance of asymmetry for\\nSiamese learning, e.g. for object detection transfer [19] or\\nspeeding up model convergence for carbon neutral training.\\n\\n8A potential concern is the variance reference being biased by out-of-\\ndistribution views, since the baseline model has not seen certain data (e.g.,\\nsmall crops) during training. To address this, we also experimented with a\\nmodel pre-trained with all the asymmetric designs. The trends still hold.\\n\\n8\\n\\n\\x0cAcknowledgements. XC would like to thank Kaiming He\\non helpful discussions through this project. XW would like\\nto thank Yutong Bai on helpful discussions through this\\nproject.\\n\\nA. Cross-Image Variance\\n\\nIn this section, we show evidence with our MoCo v2\\nbaseline that cross-image variance quickly converges to a\\nconstant that only depends on the encoding dimension d.\\nThis is through a monitor installed on the output encodings\\nduring training. Speciﬁcally, for each iteration, we compute\\nthe variance of the output (cid:96)2-normalized vectors from the\\nsource encoder along the batch axis and average them over\\nthe channel axis. Since each training batch contains differ-\\nent images rather than different views of the same image,\\nthe resulting value reﬂects the cross-image variance. Three\\nencoding dimensions, d∈{64, 128, 256} are experimented,\\nand their variances during the 100-epoch training process\\nare separately recorded in Fig. 4.\\n\\nFrom the plot, we ﬁnd that all the variances quickly and\\nseparately converge to 1/d. For example, when the encod-\\ning dimension d is 128 (default), the variance converges to\\n1/128; when d is 64, it converges to 1/64. The same obser-\\nvations are made regardless of other designs for the encoder\\n(e.g., MultiCrop or SyncBN). We believe it is a natural out-\\ncome of Siamese representation learning which generally\\nencourages uniformity [10, 35] – encodings of different im-\\nages distribute uniformly on the unit hypersphere. There-\\nfore, cross-image variance is deemed not an ideal reference\\nto distinguish designs. Instead, we use intra-image variance\\nwhich has a much smaller magnitude (×10−4), but carries\\nuseful signals to tell different designs apart (see Fig. 2).\\n\\nB. ScaleMix\\n\\nThe goal of ScaleMix is to generate a new view vs by\\ncombining two random sampled views of the same size\\n(height H and width W ): v1 and v2. The generated new\\nview is treated as a normal view of the input image x and\\nused for Siamese learning. Speciﬁcally, following the pro-\\ntocol of [29], we deﬁne the combining operation as:\\n\\nvs = M · v1 + (1 − M ) · v2,\\n\\nwhere M ∈{0, 1}H×W denotes a binary mask indicating\\nwhere to use pixels from which view, and · is an element-\\nwise multiplication. Note that different from other mixing\\noperations [29,45], we do not mix outputs as both views are\\nfrom the same image.\\n\\nThe binary values in M are determined by bounding box\\ncoordinates B= (x, y, w, h), where (x, y) is the box center,\\nand (w, h) is the box size. Given B, its corresponding re-\\ngion in M is set to all 0 and otherwise all 1. Intuitively,\\n\\nFigure 4. Cross-image variance tracked during the 100-epoch\\ntraining process for our MoCo v2 baseline, with three encoding\\ndimension options: d∈{64, 128, 256}. All of them quickly con-\\nverge to 1/d (dotted lines).\\n\\nthis means the region B in v1 is removed and ﬁlled with the\\npatch cropped from B of v2.\\n\\nThe box coordinates B are randomly sampled. We keep\\nthe aspect ratio of B ﬁxed and the same as the input views,\\nand only vary the size of the box according to a random vari-\\nable λ uniformly drawn from (0, 1): w=W\\nλ.\\nBox centers (x, y) are again uniformly sampled.\\n\\nλ, h=H\\n\\n√\\n\\n√\\n\\nC. Detailed Theoretical Analysis\\n\\nGiven the outputs: z from the source encoder and z(cid:48) from\\nthe target encoder (prime (cid:48) indicates target-related), the In-\\nfoNCE [28] loss used by MoCo is deﬁned as:\\n\\nL := −\\n\\nlog\\n\\n1\\nN\\n\\nN\\n(cid:88)\\n\\ni=1\\n\\nexp(Sii(cid:48)/τ )\\n\\n(cid:15) exp(Sii(cid:48)/τ ) + (cid:80)\\n\\nj(cid:54)=i exp(Sij(cid:48)/τ )\\n\\n,\\n\\ni z(cid:48)\\n\\n(3)\\nwhere N is batch size, τ is temperature, Sii(cid:48)=z(cid:62)\\ni and\\nSij(cid:48)=z(cid:62)\\nj are pairwise similarities between source and tar-\\nget encodings. We additionally introduce the parameter (cid:15)\\nthat controls the weight for the positive term in the denom-\\ninator, where for standard loss (cid:15)=1.\\n\\ni z(cid:48)\\n\\nFor MoCo, only the source encoder receives gradient,\\n\\nand we take derivatives only for zi:\\n\\n∂L\\n∂zi\\n\\n=\\n\\n1\\nτ\\n\\n(cid:88)\\n\\nj(cid:54)=i\\n\\nαii(cid:48)j(cid:48)(z(cid:48)\\n\\nj − z(cid:48)\\n\\ni),\\n\\nwhere\\n\\nαii(cid:48)j(cid:48) =\\n\\nexp(Sij(cid:48)/τ − Sii(cid:48)/τ )\\nk(cid:54)=i exp(Sik(cid:48)/τ − Sii(cid:48)/τ )\\n\\n.\\n\\n(cid:15) + (cid:80)\\n\\nFor the simpliﬁed case where (cid:15)=0 [42], we can have:\\n\\nαii(cid:48)j(cid:48) = αij(cid:48) =\\n\\nexp(Sij(cid:48)/τ )\\nk(cid:54)=i exp(Sik(cid:48)/τ )\\n\\n,\\n\\n(cid:80)\\n\\nwhich is independent of target encoding z(cid:48)\\ni.\\n\\nNow, let’s consider the last linear layer immediately be-\\nfore z as an example for analysis. Let f be the input features\\n\\n(4)\\n\\n(5)\\n\\n(6)\\n\\n9\\n\\n020406080100Epoch12561128164Varianced=64d=128d=256\\x0cof this layer, W be its weight matrix (so z=W f and we do\\nnot consider (cid:96)2 normalization applied to z). In this case, we\\ncan write down the dynamics of the source weight W based\\non the gradient descent rule:\\n\\n˙W := −\\n\\n= −\\n\\n1\\nN\\n\\nN\\n(cid:88)\\n\\ni=1\\n\\n∂L\\n∂zi\\n\\nf (cid:62)\\ni\\n\\n∂L\\n∂W\\n\\n1\\nτ N\\n\\nN\\n(cid:88)\\n\\n(cid:88)\\n\\ni=1\\n\\nj(cid:54)=i\\n\\n= −\\n\\nαij(cid:48)(z(cid:48)\\n\\nj − z(cid:48)\\n\\ni)f (cid:62)\\ni ,\\n\\n(7)\\n\\n(8)\\n\\nwhere ˙W is a simpliﬁed notion of the change to w.r.t. W fol-\\nlowing gradient decent. Since both z(cid:48)\\ni come from the\\ni=W (cid:48)f (cid:48)\\nj and z(cid:48)\\ntarget encoder weight W (cid:48), we have z(cid:48)\\ni\\nand thus:\\n\\nj and z(cid:48)\\nj=W (cid:48)f (cid:48)\\n\\n˙W = −W (cid:48) 1\\nτ N\\n\\nN\\n(cid:88)\\n\\n(cid:88)\\n\\ni=1\\n\\nj(cid:54)=i\\n\\nαij(cid:48)(f (cid:48)\\n\\nj − f (cid:48)\\n\\ni )f (cid:62)\\ni\\n\\n(9)\\n\\nWe deﬁne ¯f :=E[f ] to be the mean of the input feature and\\nΣf :=V[f ] to be the co-variance matrix of the input feature f ,\\nwhere E[·] computes expectation and V[·] outputs variance.\\nThese two quantities will be used later.\\n\\nNow let’s consider how intra-image variance in both tar-\\nget and source sides affect training. To reach a clear con-\\nclusion, we now make two assumptions.\\n\\nAssumption 1: additive noise. We can model the intra-\\nimage variance as additive noise. Speciﬁcally, let ˜f be the\\nfeature corresponding to the original image, we can assume:\\n• fi=˜fi+ei. That is, the input feature of the last layer fi\\nreceives source noise ei with E[ei]=¯e and V[ei]=Σ;\\nj=˜fj+e(cid:48)\\nj receives target\\nj]=¯e(cid:48) and V[e(cid:48)\\nj]=Σ(cid:48). Note that for\\nnoise e(cid:48)\\nthe feature of a different image f (cid:48)\\ni , it also undergoes\\nthe same process on the target side and thus we have\\ni =˜fi+e(cid:48)\\nf (cid:48)\\ni.\\n\\nj. That is, the input feature f (cid:48)\\nj with E[e(cid:48)\\n\\n• f (cid:48)\\n\\nNote that the noise is not necessarily zero mean-ed. Since\\nthe augmentations of fi and f (cid:48)\\ni are\\nindependent of each other: P(ei, e(cid:48)\\ni). Same for\\nei and ej where i(cid:54)=j.\\n\\ni are independent, ei and e(cid:48)\\ni)=P(ei)P(e(cid:48)\\n\\nAssumption 2: all αij(cid:48) are constant and independent\\nof f . Alternatively, if we consider the quadratic loss (i.e.,\\nLq= (cid:80)\\nj(cid:54)=i (Sij(cid:48)−Sii(cid:48))), then all αij(cid:48) are constant and this\\nassumption holds true. For InfoNCE this may not hold, and\\nwe leverage this assumption for simplicity of derivations.\\n\\nUnder these two assumptions, we now compute Ef [ ˙W ],\\nthe expectation of the weight gradient over input feature f\\nof the last layer. This gets rid of inter-image variance, and\\nfocuses on intra-image variance only:\\n\\nEf [ ˙W ] =\\n\\nW (cid:48)(Σf − R).\\n\\n(10)\\n\\n1\\nτ\\n\\nHere the residual term R is as follows:\\n\\nR := −\\n\\nN\\n(cid:88)\\n\\n1\\nN\\n\\ni(¯f + ei)(cid:62),\\nˆe(cid:48)\\n\\ni=1\\nj−e(cid:48)\\nwhere ˆe(cid:48)\\ni\\nj and e(cid:48)\\nwhich is a weighted sum of e(cid:48)\\ni.\\n\\nj(cid:54)=i αij(cid:48)e(cid:48)\\n\\ni:= (cid:80)\\n\\nis also a random variable\\n\\nFrom the deﬁnition (Eq. (5)), we have (cid:80)\\n\\nj(cid:54)=i αij(cid:48)=1. e(cid:48)\\nj\\ni are independent. Therefore we can compute the\\n\\nand e(cid:48)\\nmean and variance of ˆe(cid:48)\\n\\ni as:\\n\\nE[ˆe(cid:48)\\ni := V[ˆe(cid:48)\\n\\ni] = 0,\\ni] = (1 +\\n\\nˆΣ(cid:48)\\n\\nij(cid:48))Σ(cid:48).\\nα2\\n\\n(cid:88)\\n\\nj(cid:54)=i\\n\\n(11)\\n\\n(12)\\n\\n(13)\\n\\nNow for the residual term R, we also have Ee[R]=0.\\n\\nTherefore, the full expectation for ˙W can be written as:\\n\\nE[ ˙W ] := Ee[Ef [ ˙W ]] =\\n\\nW (cid:48)Σf .\\n\\n(14)\\n\\n1\\nτ\\n\\nThis means the source weight will grow along the direc-\\ntion that maximizes the distance between different images.\\nMore precisely, it grows along the eigenvector that corre-\\nsponds to the maximal eigenvalue of Σf .\\n\\nNow we can check the inﬂuence of intra-image variance\\nfrom source and target encoders. The inﬂuence can be char-\\nacterized by the term Ve[Ef [ ˙W ]]. For simplicity, we can\\ncompute Ve[Ef [tr(R)]] – i.e. the variance on the trace of R,\\nsince Σf remains constant for intra-image variance.\\n\\nLeveraging the independence of {ˆe(cid:48)\\n\\ni, ei} among different\\n\\nimages, we can arrive at:\\n\\nVe[Ef [tr(R)]] = tr\\n\\n(cid:104) ˆΣ(cid:48)(¯f¯f (cid:62) + ¯e¯e(cid:62) + Σ)\\n\\n(cid:105)\\n\\n,\\n\\n(15)\\n\\nwhere ˆΣ(cid:48):= 1\\nN\\n\\n(cid:80)N\\n\\ni=1\\n\\nˆΣ(cid:48)\\n\\ni is the mean of all variances of ˆe(cid:48)\\ni.\\n\\nFrom Eq. (15) we can notice that: i) if there is large mag-\\nnitude of source feature mean ¯f and/or source noise mean ¯e,\\nthen the variance will be large; ii) this effect will be magni-\\nﬁed with more target-side variance (i.e., larger eigenvalues\\nof Σ(cid:48) and thus ˆΣ(cid:48)), without affecting the average gradient;\\niii) large magnitude of feature mean and/or noise mean on\\nthe target side does not inﬂuence the variance. This asym-\\nmetry between source and target suggests that the training\\nprocedure an be negatively affected if the target variance is\\ntoo large, coupled by ¯f¯f (cid:62) and ¯e¯e(cid:62) in Eq. (15).\\n\\nj − f (cid:48)\\n\\nThe intuition why there is such an asymmetry is the\\nfollowing: in Eq. (9), while the target side has a subtrac-\\ntion f (cid:48)\\ni which cancels out the mean, the source side fi\\ndoesn’t. This leads to the mean values being kept on the\\nsource side which couples with the target variance, whereas\\nno mean values from the target side are kept.\\n\\nTherefore, we can infer that higher variance on the target\\nside is less necessary compared to the source side – it will\\nincur more instability during training without affecting the\\nmean of gradients.\\n\\n10\\n\\n\\x0cD. More Implementation Details\\n\\nMultiCrop. Our MultiCrop recipe largely follows the work\\nof SwAV [6]. Speciﬁcally, 224-sized crops are sampled\\nwith a scale range of (0.14, 1), and 96-sized small crops are\\nsampled from (0.05, 0.14). We use m=6 small crops by\\ndefault, and each is forwarded separately with the encoder.\\nWhen applied to one encoder, all (1+6)=7 encodings are\\ncompared against the single encoding from the other side;\\nwhen applied jointly, (7×2)=14 encodings are paired by\\ncrop size to compute loss terms. Unlike the practice in\\nSwAV, no loss symmetrization is employed and the 6 losses\\nfrom small crops are averaged before adding to the stan-\\ndard loss. When target encoder is involved in MultiCrop,\\nwe also create a separate memory bank [19] dedicated to\\nsmall crops, updated with 1 out of the 6 crops.\\n\\nAsymAug. For StrongerAug, we use additional augmenta-\\ntions from RandAug [12], same as [37]. For WeakerAug,\\nwe simply remove all the color- and blur-related augmenta-\\ntions and only keep geometric ones in the MoCo v2 recipe.\\nThis leaves us with random resized cropping and ﬂipping.\\n\\nMeanEnc. Deviating from MultiCrop, augmentations used\\nfor computing the mean are forwarded jointly through the\\nencoder thanks to the uniform size of 224×224. Joint for-\\nwarding enlarges the batch size in BN, which further re-\\nduces the variance. The output encodings are averaged be-\\nfore (cid:96)2 normalization.\\nOther frameworks. Different from MoCo v2 which uses\\nshufﬂe BN [19] across 8 GPUs, all the frameworks stud-\\nied in Sec. 6.2 use SyncBN by default. Therefore, when\\napplying AsymBN to them, we keep the target encoder un-\\ntouched and change the BNs in the source encoder instead.\\nTo minimize the impact from the number of GPU devices\\n(e.g., MoCo v3 uses 16 GPUs to ﬁt a batch size of 4096\\nfor ResNet; whereas for ViT it uses 32 GPUs), we always\\ndivide the full batch into 8 groups and the normalization is\\nperformed within each group – this mimics the per-device\\nBN operation in MoCo v2 while being more general.\\n\\nMoreover, for MoCo v2 we only convert the single BN in\\nthe target projector to SyncBN. This has minimal inﬂuence\\non efﬁciency as SyncBN can be expensive and converting\\nall of them (including ones in the encoder) can signiﬁcantly\\nslow down training. Now since we are converting SyncBN\\nback, we choose to convert all BNs in the source encoder\\nwhenever possible to reduce inter-device communications\\nfor efﬁciency purposes.\\n\\nMore recent frameworks [11, 44] adopt the asymmetric\\naugmentation recipe in BYOL [18], in such cases, we use\\none composition for source and the other for target half the\\ntime during pre-training, and swap them in the other half.\\n\\nTo have a fair comparison with frameworks pre-trained\\nfor 100 epochs, we optionally train 2× as long when the\\ndefault loss is symmetrized and ours is asymmetric.\\n\\nUnless otherwise speciﬁed, we follow the same design\\nchoices in MoCo v2 when applying ScaleMix and MeanEnc\\nto other frameworks. In addition, there are subtleties asso-\\nciated with each individual framework listed below:\\n\\n• MoCo v3 [11]. Since MoCo v3 also employs an addi-\\ntional predictor on the source side, we involve both the\\npredictor and the backbone when applying AsymBN.\\n\\n• SimCLR [8]. The original SimCLR uses 2×N −2\\nnegative examples for contrastive learning [8], which\\nincludes all the other images in the same batch, mul-\\ntiplied by 2 for the two augmentations per image.\\nAfter converting to the asymmetric version, we only\\nuse N −1 negative samples – same as in MoCo v3 –\\nand it causes a gap. We ﬁnd a simple change of In-\\nfoNCE [28] temperature from 0.1 to 0.2 can roughly\\ncompensate for this gap. For AsymBN, we convert all\\nthe BNs in the source encoder, not just the ones in the\\nprojector. For ScaleMix, we apply this augmentation\\nhalf the time – we empirically ﬁnd applying ScaleMix\\nall the time will cause a considerable drop in perfor-\\nmance compared to the asymmetric baseline, for rea-\\nsons yet to be understood.\\n\\n• BYOL [18]. BYOL initiated the additional predictor\\nwhich also has BNs. We convert all the BNs in the\\nsource encoder when AsymBN is used, not just ones\\nin the projector.\\n\\n• SimSiam [10]. Additional predictor is again used in\\nSimSiam and plays an important role in collapse pre-\\nvention. We convert all the BNs in the source encoder\\nafter the conversion to an asymmetric version.\\n\\n• Barlow Twins [44]. This is a fully symmetric frame-\\nwork and no loss symmetrization is used by default.\\nTherefore, we also pre-train the asymmetric version\\nfor 100 epochs, not 2× as long. Same as SimCLR,\\nScaleMix is applied with half the frequency. All the\\nencoder BNs are converted when AsymBN is used.\\n\\nViT backbone. MoCo v3 [11] with its default hyper-\\nparameters for ViT backbone is used. ViT as a backbone\\ndoes not have BN. Therefore we convert BNs in the projec-\\ntor and predictor when using AsymBN.\\n\\nTransfer learning. We follow the linear probing protocol\\nto evaluate our model on transfer learning tasks. Different\\nfrom ImageNet, we use SGD optimizer with momentum 0.9\\nand weight decay 0 for training. The learning rate is ad-\\njusted via grid search on the validation set, and the ﬁnal\\nresults are reported on the test set. All models are trained\\nfor 100 epochs, with a half-cycle cosine decaying schedule\\nfor learning rate.\\n\\n11\\n\\n\\x0cReferences\\n\\n[1] Adrien Bardes,\\n\\nJean Ponce, and Yann LeCun.\\n\\nVi-\\ncreg: Variance-invariance-covariance regularization for self-\\nsupervised learning. arXiv preprint arXiv:2105.04906, 2021.\\n1, 2\\n\\n[2] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas\\nPapernot, Avital Oliver, and Colin A Raffel. Mixmatch: A\\nholistic approach to semi-supervised learning. In NeurIPS,\\n2019. 2\\n\\n[3] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea\\nVedaldi, and Philip HS Torr. Fully-convolutional siamese\\nnetworks for object tracking. In ECCV, 2016. 2\\n\\n[4] Jane Bromley,\\n\\nIsabelle Guyon, Yann LeCun, Eduard\\nS¨ackinger, and Roopak Shah. Signature veriﬁcation using\\na “Siamese” time delay neural network. In NeurIPS, 1994.\\n1, 2\\n\\n[5] Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Char-\\nless Fowlkes, Zhuowen Tu, and Stefano Soatto. Exponential\\nmoving average normalization for self-supervised and semi-\\nsupervised learning. In CVPR, 2021. 2\\n\\n[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\\notr Bojanowski, and Armand Joulin. Unsupervised learning\\nof visual features by contrasting cluster assignments. arXiv\\npreprint arXiv:2006.09882, 2020. 1, 2, 4, 7, 8, 11\\n\\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\\ning properties in self-supervised vision transformers. arXiv\\npreprint arXiv:2104.14294, 2021. 1, 2, 4, 7\\n\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. arXiv preprint arXiv:2002.05709,\\n2020. 1, 2, 4, 5, 7, 8, 11\\n\\n[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\\nImproved baselines with momentum contrastive learning.\\narXiv preprint arXiv:2003.04297, 2020. 2, 4\\n\\n[10] Xinlei Chen and Kaiming He. Exploring simple siamese rep-\\n\\nresentation learning. In CVPR, 2021. 1, 2, 3, 5, 7, 8, 9, 11\\n\\n[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical\\nstudy of training self-supervised vision transformers. arXiv\\npreprint arXiv:2104.02057, 2021. 2, 7, 11\\n\\n[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\\nLe. Randaugment: Practical automated data augmentation\\nwith a reduced search space. In CVPRW, 2020. 11\\n\\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\ndatabase. In CVPR, 2009. 3, 4, 6\\n\\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. In ICLR, 2021. 2, 5, 7\\n\\n[15] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre\\nSermanet, and Andrew Zisserman. With a little help from\\nmy friends: Nearest-neighbor contrastive learning of visual\\nrepresentations. arXiv preprint arXiv:2104.14548, 2021. 2,\\n8\\n\\n[16] Mark Everingham, Luc Van Gool, Christopher KI Williams,\\nJohn Winn, and Andrew Zisserman. The pascal visual object\\nclasses (voc) challenge. IJCV, 2010. 4\\n\\n[17] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noord-\\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\\nYangqing Jia, and Kaiming He. Accurate, large minibatch\\nSGD: Training ImageNet in 1 hour. arXiv:1706.02677, 2017.\\n7, 8\\n\\n[18] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\\nnew approach to self-supervised learning. arXiv preprint\\narXiv:2006.07733, 2020. 1, 2, 4, 5, 7, 8, 11\\n\\n[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\nGirshick. Momentum contrast for unsupervised visual rep-\\nresentation learning. In CVPR, 2020. 1, 2, 3, 4, 8, 11\\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nIn CVPR,\\n\\nDeep residual learning for image recognition.\\n2016. 1, 4, 5, 7, 8\\n\\n[21] Sergey Ioffe and Christian Szegedy. Batch normalization:\\nAccelerating deep network training by reducing internal co-\\nvariate shift. In ICML, 2015. 3, 4, 5\\n\\n[22] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi,\\nXiaojie Jin, Anran Wang, and Jiashi Feng. All tokens matter:\\nToken labeling for training better vision transformers. arXiv\\npreprint arXiv:2104.10858, 2021. 7\\n\\n[23] Alex Krizhevsky. Learning multiple layers of features from\\n\\ntiny images. Tech Report, 2009. 8\\n\\n[24] Zeming Li, Songtao Liu, and Jian Sun. Momentum2\\nteacher: Momentum teacher with momentum statistics for\\nself-supervised learning. arXiv preprint arXiv:2101.07525,\\n2021. 2\\n\\nSgdr:\\n\\nStochas-\\narXiv preprint\\n\\n[25] Ilya Loshchilov and Frank Hutter.\\n\\ntic gradient descent with warm restarts.\\narXiv:1608.03983, 2016. 8\\n[26] Subhransu Maji, Esa Rahtu,\\n\\nJuho Kannala, Matthew\\nBlaschko, and Andrea Vedaldi. Fine-grained visual classi-\\nﬁcation of aircraft. arXiv preprint arXiv:1306.5151, 2013.\\n8\\n\\n[27] Ishan Misra and Laurens van der Maaten. Self-supervised\\nlearning of pretext-invariant representations. In CVPR, 2020.\\n1, 4\\n\\n[28] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\\nsentation learning with contrastive predictive coding. arXiv\\npreprint arXiv:1807.03748, 2018. 1, 4, 6, 9, 11\\n\\n[29] Yun Sangdoo, Han Dongyoon, Oh Seong, Joon, Chun\\nSanghyuk, Choe Junsuk, and Yoo Youngjoon. Cutmix: Reg-\\nularization strategy to train strong classiﬁers with localizable\\nfeatures. In ICCV, 2019. 4, 9\\n\\n[30] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao\\nZhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han\\nZhang, and Colin Raffel. Fixmatch: Simplifying semi-\\nsupervised learning with consistency and conﬁdence. arXiv\\npreprint arXiv:2001.07685, 2020. 2\\n\\n[31] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior\\nWolf. Deepface: Closing the gap to human-level perfor-\\nmance in face veriﬁcation. In CVPR, 2014. 2\\n\\n12\\n\\n\\x0c[32] Antti Tarvainen and Harri Valpola. Mean teachers are better\\nrole models: Weight-averaged consistency targets improve\\nsemi-supervised deep learning results. In NeurIPS, 2017. 1\\n[33] Yuandong Tian, Xinlei Chen, and Surya Ganguli. Un-\\nderstanding self-supervised learning dynamics without con-\\ntrastive pairs. arXiv preprint arXiv:2102.06810, 2021. 6\\n[34] Yuandong Tian, Lantao Yu, Xinlei Chen, and Surya Gan-\\nguli. Understanding self-supervised learning with dual deep\\nnetworks. arXiv preprint arXiv:2010.00578, 2020. 6\\n[35] Tongzhou Wang and Phillip Isola. Understanding contrastive\\nrepresentation learning through alignment and uniformity on\\nthe hypersphere. In ICML, 2020. 3, 9\\n\\n[36] Xiao Wang, Daisuke Kihara, Jiebo Luo, and Guo-Jun\\nEnaet: Self-trained ensemble autoencoding trans-\\narXiv preprint\\n\\nQi.\\nformations for semi-supervised learning.\\narXiv:1911.09265, 2019. 2\\n\\n[37] Xiao Wang and Guo-Jun Qi. Contrastive learning with\\nstronger augmentations. arXiv preprint arXiv:2104.07713,\\n2021. 2, 8, 11\\n\\n[38] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey,\\nHongyuan Zha, Le Song, and Shu-Tao Xia. Iterative learning\\nwith open-set noisy labels. In CVPR, 2018. 2\\n\\n[39] Yuxin Wu and Kaiming He. Group normalization. In ECCV,\\n\\n2018. 5\\n\\n[40] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\nUnsupervised feature learning via non-parametric instance\\ndiscrimination. In CVPR, 2018. 4\\n\\n[41] Haohang Xu, Xiaopeng Zhang, Hao Li, Lingxi Xie, Hongkai\\nXiong, and Qi Tian. Seed the views: Hierarchical seman-\\ntic alignment for contrastive representation learning. arXiv\\npreprint arXiv:2012.02733, 2020. 2, 8\\n\\n[42] Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh\\nLiu, Yubei Chen, and Yann LeCun. Decoupled contrastive\\nlearning. arXiv preprint arXiv:2110.06848, 2021. 6, 9\\n[43] Yang You, Igor Gitman, and Boris Ginsburg. Large batch\\ntraining of convolutional networks. arXiv:1708.03888, 2017.\\n2, 7, 8\\n\\n[44] Jure Zbontar, Li Jing,\\n\\nIshan Misra, Yann LeCun, and\\nSt´ephane Deny. Barlow twins: Self-supervised learning via\\narXiv preprint arXiv:2103.03230,\\nredundancy reduction.\\n2021. 1, 2, 7, 8, 11\\n\\n[45] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\\ntion. arXiv preprint arXiv:1710.09412, 2017. 9\\n\\n[46] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-\\nralba, and Aude Oliva. Learning deep features for scene\\nrecognition using places database. In NeurIPS, 2014. 8\\n[47] Pan Zhou, Caiming Xiong, Xiao-Tong Yuan, and Steven\\nA theory-driven self-labeling reﬁnement method\\narXiv preprint\\n\\nHoi.\\nfor contrastive representation learning.\\narXiv:2106.14749, 2021. 2, 8\\n\\n13\\n\\n\\x0c',\n",
       " '2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\nI\\n\\nA\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n7\\n0\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nFROM STATISTICAL TO CAUSAL LEARNING\\n\\nBernhard Sch¨olkopf\\nMax Planck Institute for Intelligent Systems, T¨ubingen, Germany\\nbs@tuebingen.mpg.de\\n\\nJulius von K ¨ugelgen\\nMax Planck Institute for Intelligent Systems, T¨ubingen, Germany\\nUniversity of Cambridge, United Kingdom\\njvk@tuebingen.mpg.de\\n\\nApril 4, 2022\\n\\nABSTRACT\\n\\nWe describe basic ideas underlying research to build and understand artiﬁcially intelligent systems:\\nfrom symbolic approaches via statistical learning to interventional models relying on concepts of\\ncausality. Some of the hard open problems of machine learning and AI are intrinsically related\\nto causality, and progress may require advances in our understanding of how to model and infer\\ncausality from data.*\\n\\nMathematics Subject Classiﬁcation 2020\\n\\nPrimary 68T05; Secondary 68Q32, 68T01, 68T10, 68T30, 68T37\\n\\nCausal inference, machine learning, causal representation learning\\n\\nKeywords\\n\\n1\\n\\nIntroduction\\n\\nIn 1958, the New York Times reported on a new machine called the perceptron. Frank Rosenblatt, its inventor,\\ndemonstrated that the perceptron was able to learn from experience. He predicted that later perceptrons would be able\\nto recognize people, or instantly translate spoken language. Now a reality, this must have sounded like distant science\\nﬁction at the time. In hindsight, we may consider it the birth of machine learning, the ﬁeld fueling most of the current\\nadvances in artiﬁcial intelligence (AI).\\n\\nAround the same time, another equally revolutionary development took place: scientists understood that computers\\ncould do more than compute numbers:\\nthey can process symbols. Although this insight was also motivated by\\nartiﬁcial intelligence, in hindsight it was the birth of the ﬁeld of computer science. There was great optimism that the\\nmanipulation of symbols, in programs written by humans, implementing rules designed by humans, should be enough\\nto generate intelligence. Below, we shall refer to this as the symbol-rule hypothesis.1\\n\\nThere was initially encouraging progress on seemingly hard problems such as automatic theorem proving and\\ncomputer chess. One of the fathers of the ﬁeld, Herb Simon, predicted in 1956 that “machines will be capable,\\nwithin twenty years, of doing any work a man can do.” However, problems that appeared simple, such as most things\\nanimals could do, turned out to be hard. This came to be known as Moravec‘s paradox. When IBM’s Deep Blue\\n\\n*To appear in the Proceedings of the International Congress of Mathematicians 2022, EMS Press. Both authors contributed\\n\\nequally to this work; names listed in alphabetical order.\\n\\n1The term should be taken with a grain of salt, since it suggests a separation between representations and computations which\\n\\nis hard to uphold in practice.\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nchess computer beat Garry Kasparov in 1997, Kasparov was physically facing a human during the match: while\\nDeep Blue was capable of analyzing the game’s search tree in unprecedented detail, it was unable to recognize and\\nphysically move chess pieces, so this task had to be relegated to a human, in an inversion of the famous mechanical\\nturk.2 In the years to follow, the ﬁeld of AI entered what came to be known as the AI winter. The community got\\ndisillusioned with the lack of progress and prospects, and interest greatly declined. However, largely independently of\\nthe ﬁeld of classic AI, machine learning eventually started to boom. Like Rosenblatt’s early work, it was built on the\\nobservation that all existing examples of truly intelligent systems—i.e., animals, including humans—were not built\\non the symbol-rule hypothesis: both the representations and the rules implemented by natural intelligent systems are\\nacquired from experience, through processes of evolution and learning.\\n\\nRather than exploring the well-known dichotomy between rule-based and learning-based approaches, we will explore\\nthe less known questions of causality and interventions. While the ﬁeld of causality in computer science was initially\\nstrongly linked to classic AI, recent years have witnessed great interest in connecting it to machine learning [112].\\nBelow, we explore some of these connections, drawing from [126, 131]. We will argue that the causal view is rel-\\nevant when it comes to addressing crucial open problems of machine learning, related to notions of robustness and\\ngeneralization beyond the training distribution.\\n\\nOverview In statistical learning, our starting point is a joint distribution p(X) generating the observable data. Here,\\nX is a random vector, and we are usually given a dataset x1, . . . , xm sampled i.i.d. from p. We are often interested in\\nestimating properties of conditionals of some components of X given others, e.g., a classiﬁer (which may be obtained\\nby thresholding a conditional at 0.5). This is a nontrivial inverse problem, giving rise to statistical learning theory (§ 2).\\n\\nCausal learning is motivated by shortcomings of statistical learning (§ 3). Its starting point is a structural causal model\\n(SCM) [105] (§ 4). In an SCM, the components X1, . . . , Xn of X are identiﬁed with vertices of a directed graph whose\\narrows represent direct causal inﬂuences, and there is a random variable Ui for each vertex, along with a function fi\\nwhich computes Xi from its graph parents PAi and Ui, i.e.,\\n\\nXi := fi(PAi, Ui).\\n\\n(1.1)\\n\\nGiven a distribution over the Ui, which are assumed independent, this also gives rise to a probabilistic model p(X).\\nHowever, the model in (1.1) is more structured: the graph connectivity and the functions fi create particular depen-\\ndences between the observables. Moreover, it describes how the system behaves under intervention: by replacing\\nfunctions by constants, we can compute the effect of setting some variables to speciﬁc values.\\n\\nCausal learning builds on assumptions different from standard machine learning (§ 5), and addresses a different level\\nin the modeling hierarchy (§ 6). It also comes with new problems, such as causal discovery, where we seek to infer\\nproperties of graph and functions from data (§ 7). In some cases, conditional independences among the Xi contain\\ninformation about the graph [145]; but novel assumptions let us handle some cases that were previously unsolvable\\n[64]. Those assumptions have nontrivial implications for machine learning tasks such as semi-supervised learning,\\ncovariate shift adaptation and transfer learning [130] (§ 8). Once provided with a causal model, causal reasoning (§ 9)\\nallows us to identify and estimate certain causal queries of interest from observational data. We conclude with a list of\\nsome current and open problems (§ 10), with a particular emphasis on the topic of causal representation learning.\\n\\nThe presentation and notation will be somewhat informal in several respects. We generally assume that all distribu-\\ntions possess densities (w.r.t. a suitable reference measure). We sometimes write p(x) for the distribution (or density)\\nof a random variable X. Accordingly, the same p can denote another distribution p(y), distinguished by the argument\\nof p(·). We also sometimes use summation for marginalization which supposes discrete variables; the corresponding\\nexpressions for continuous quantities would use integrals.\\n\\n2 Statistical Learning Theory\\n\\nSuppose we have measured two statistically dependent observables and found the points to lie approximately on\\na straight line. An empirical scientist might be willing to hypothesize a corresponding law of nature (see Fig. 1).\\nHowever, already Leibniz pointed out that if we scatter spots of ink randomly on a piece of paper by shaking a quill\\npen, we can also ﬁnd a mathematical equation satisﬁed by these points [81]. He argued that we would not call this a\\nlaw of nature, because no matter how the points are distributed, there always exists such an equation; we would only\\ncall it a law of nature only if the equation is simple. This raises the question of what makes an equation simple. The\\nphysicist Rutherford took the pragmatic view that if there is a law, it should be directly evident from the data: “if your\\nexperiment needs statistics, you ought to have done a better experiment.”3 This view may have been a healthy one\\n\\n2https://en.wikipedia.org/wiki/Mechanical_Turk\\n3Cited after http://www.warwick.ac.uk/statsdept/staff/JEHS/data/jehsquot.pdf.\\n\\n2\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 1: Given a small number of observations, how do\\nwe ﬁnd a law underlying them? Leibniz argued that even\\nif we generate a random set of points, we can always ﬁnd a\\nmathematical equation satisﬁed by these points.\\n\\nwhen faced with low-dimensional inference problems where regularities are immediately obvious; however, modern\\nAI is facing inference problems that are harder: they are often high-dimensional and nonlinear, yet we may have little\\nprior knowledge about the underlying regularity (e.g., for medical data, we usually do not have a mechanistic model).\\n\\nStatistical learning theory studies the problem of how to still perform valid inference, provided that we have sufﬁ-\\nciently large datasets and the computational means to process them. Let us look at some theoretical results for the\\nsimplest learning scenario, drawing from [133]; for details, see [153]. Suppose we are given empirical observations,\\n\\n(x1, y1), . . . , (xm, ym) ∈ X × Y,\\nwhere X is some nonempty set from which the inputs come, and Y = {±1} is the output set, in our case consisting\\nof just two classes. This situation is called pattern recognition, and our goal is to use the training data (2.1) to infer\\na function f : X → {±1} (from some function class chosen a priori) which will produce the correct output for a\\nnew input x which we may not have seen before. To formalize what we mean by “correct”, we make the assumption\\nthat all observations (xi, yi) have been generated independently by performing a random experiment described by an\\nunknown probability distribution p(x, y)—a setting referred to as i.i.d. (independent and identically distributed) data.\\nOur goal will be to minimize the expected error (or risk)\\n\\n(2.1)\\n\\nR[f ] =\\n\\nc(y, f (x)) dp(x, y),\\n\\n(2.2)\\n\\n(cid:90)\\n\\nX×Y\\n\\nwhere c is a so-called loss function, e.g., the misclassiﬁcation error c(y, f (x)) = 1\\nwhenever f (x) = y and 1 otherwise.\\n\\n2 |f (x) − y| taking the value 0\\n\\nThe difﬁculty of the task stems from the fact that we are trying to minimize a quantity that we cannot evaluate: since\\nwe do not know p, we cannot compute (2.2). We do know, however, the training data (2.1) sampled from p. We can\\nthus try to infer a function f from the training sample whose risk is close to the minimum of (2.2). To this end, we\\nneed what is called an induction principle.\\n\\nOne way to proceed is to use the training sample to approximate (2.2) by a ﬁnite sum, referred to as the empirical risk\\n\\nRemp[f ] =\\n\\nc(xi, yi, f (xi)).\\n\\n(2.3)\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nThe empirical risk minimization (ERM) induction principle recommends that we choose (or “learn”) an f that mini-\\nmizes (2.3). We can then ask whether the ERM principle is statistically consistent: in the limit of inﬁnitely many data\\npoints, will ERM lead to a solution which will do as well as possible on future data generated by p?\\n\\nIt turns out that if the function class over which we minimize (2.3) is too large, then ERM is not consistent. Hence,\\nwe need to suitably restrict the class of possible functions. For instance, ERM is consistent for all probability\\ndistributions, provided that the VC dimension of the function class is ﬁnite. The VC dimension is an example of a\\ncapacity measure. It is deﬁned as the maximal number of points that can be separated (classiﬁed) in all possible ways\\nusing functions from the class. E.g., using linear classiﬁers (separating classes by straight lines) on R2, we can realize\\nall possible classiﬁcations for 3 suitably chosen points, but we can no longer do this once we have 4 points, no matter\\nhow they are placed (see Fig. 2). This means that the VC dimension of this function class is 3. More generally, for\\nlinear separations in Rd, the VC dimension is d + 1.\\n\\nWhenever the VC dimension is ﬁnite, our class of functions (or explanations) becomes falsiﬁable in the sense that start-\\ning from a certain number of observations, no longer all possible labelings of the points can be explained (cf. Fig. 2).\\nIf we can nevertheless explain a sufﬁciently large set of observed data, we thus have reason to believe that this is a\\nmeaningful ﬁnding.\\n\\n3\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 2: Using straight lines, we can separate three points in all possible ways; we cannot do this for four points,\\nno matter how they are placed. The class of linear separations is not “falsiﬁable” using three points, but it becomes\\nfalsiﬁable once we have four or more points.\\n\\nMuch of machine learning research is concerned with restrictions on classes of functions to make inference possible,\\nbe it by imposing prior distributions on function classes, through other constraints, or by designing self-regularizing\\nlearning procedures, e.g., gradient descent methods for neural networks [79]. While there is a solid theoretical under-\\nstanding of supervised machine learning as described above (i.e., function learning from input-output examples), there\\nare still details under investigation, such as the recently observed phenomenon of “double descent” [7].\\n\\nA popular constraint, implemented in the Support Vector Machine (SVM) [153, 133], is to consider linear separations\\nwith large margin: it turns out that for large margin separations in high-dimensional (or inﬁnite-dimensional) spaces,\\nthe capacity can be much smaller than the dimensionality, making learning possible in situations where it would\\notherwise fail.\\n\\nFor some learning algorithms, including SVMs and nearest neighbor classiﬁers, there are strong universal consistency\\nresults, guaranteeing convergence of the algorithm to the lowest achievable risk, for any problem to be learned [28,\\n153, 133, 147]. Note, however, that this convergence can be arbitrarily slow.\\n\\nFor a given sample size, it will depend on the problem being learned whether we achieve low expected error. In\\naddition to asymptotic consistency statements, learning theory makes ﬁnite sample size statements: one can prove that\\nwith probability at least 1 − δ (for δ > 0), for all functions f in a class of functions with VC dimension h,\\n\\nR[f ] ≤ Remp[f ] +\\n\\nh (log(2m/h) + 1) + log\\n\\n(2.4)\\n\\n(cid:115)\\n\\n(cid:18)\\n\\n1\\nm\\n\\n(cid:19)\\n\\n.\\n\\n4\\nδ\\n\\nThis is an example of a class of results that relate the training error Remp[f ] and the test error R[f ] using a conﬁdence\\ninterval (the square root term) depending on a capacity measure of a function class (here, its VC dimension h). It\\nsays that with high probability, the expected error R[f ] on future observations generated by the unknown probability\\ndistribution is small, provided the two terms on the right hand side are small: the training error Remp[f ] (i.e., the error\\non the examples we have already seen), and the square root term, which will be small whenever the capacity h is small\\ncompared to the number of training observations m. If, on the other hand, we try to learn something that may not\\nmake sense, such as the mapping from the name of people to their telephone number, we would ﬁnd that to explain\\nall the training data (i.e., to obtain a small Remp[f ]), we need a model whose capacity h is large, and the second term\\nbecomes large. In any case, it is crucial for both consistency results and ﬁnite sample error bounds such as (2.4) that\\nwe have i.i.d. data.\\n\\nKernel Methods A symmetric function k : X2 → R, where X is a nonempty set, is called a positive deﬁnite (pd)\\nkernel if for arbitrary points x1, . . . , xm ∈ X and coefﬁcients a1, . . . , am ∈ R:\\n\\nThe kernel is called strictly positive deﬁnite if for pairwise distinct points, the implication (cid:80)\\n∀i : ai = 0 is valid. Any positive deﬁnite kernel induces a mapping\\n\\ni,j aiajk(xi, xj) = 0 =⇒\\n\\ninto a reproducing kernel Hilbert space (RKHS) H satisfying\\n\\n(2.5)\\n\\n(2.6)\\n\\n(cid:88)\\n\\ni,j\\n\\naiajk(xi, xj) ≥ 0.\\n\\nΦ : x (cid:55)→ k(x, .)\\n\\n(cid:104)k(x, .), k(x(cid:48), .)(cid:105) = k(x, x(cid:48))\\n\\n4\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nfor all x, x(cid:48) ∈ X. Although H may be inﬁnite-dimensional, we can construct practical classiﬁcation algorithms in H\\nprovided that all computational steps are carried out in terms of scalar products, since those can be reduced to kernel\\nevaluations (2.6).\\n\\nIn the SVM algorithm, the capacity of the function class is restricted by enforcing a large margin of class separation\\nin H via a suitable RKHS regularization term. The solution can be shown to take the form\\n\\nf (x) = sgn\\n\\nαik(xi, x) + b\\n\\n,\\n\\n(cid:19)\\n\\n(cid:18) (cid:88)\\n\\ni\\n\\n(2.7)\\n\\n(2.8)\\n\\nwhere the learned parameters αi and b are the solution of a convex quadratic optimization problem. A similar ex-\\npansion of the solution in terms of kernel functions evaluated at training points holds true for a larger class of kernel\\nalgorithms beyond SVMs, regularized by an RKHS norm [127].\\n\\nIn kernel methods, the kernel plays three roles which are crucial for machine learning: it acts as a similarity measure\\nfor data points, induces a representation in a linear space4 via (2.5), and parametrizes the function class within which\\nthe solution is sought, cf. (2.7).\\n\\nKernel Mean Embeddings Consider two sets of points X := {x1, . . . , xm} ⊂ X and Y := {y1, . . . , yn} ⊂ X. We\\ndeﬁne the mean map µ as [133]\\n\\nµ(X) =\\n\\nk(xi, ·).\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nFor polynomial kernels k(x, x(cid:48)) = ((cid:104)x, x(cid:48)(cid:105) + 1)d, we have µ(X) = µ(Y ) if all empirical moments up to order d\\ncoincide. For strictly pd kernels, the means coincide only if X = Y , rendering µ injective [134]. The mean map\\nhas some other interesting properties [144], e.g., µ(X) represents the operation of taking a mean of a function on the\\nsample X:\\n\\n(cid:104)µ(X), f (cid:105) =\\n\\nk(xi, ·), f\\n\\n=\\n\\nf (xi)\\n\\n(cid:28) 1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\n(cid:29)\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nMoreover, we have\\n\\n(cid:107)µ(X) − µ(Y )(cid:107) = sup\\n(cid:107)f (cid:107)≤1\\n\\n|(cid:104)µ(X) − µ(Y ), f (cid:105)| = sup\\n(cid:107)f (cid:107)≤1\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nf (xi) −\\n\\nf (yi)\\n\\n.\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nIf Ex,x(cid:48)∼p[k(x, x(cid:48))], Ex,x(cid:48)∼q[k(x, x(cid:48))] < ∞, then the above statements, including the injectivity of µ, generalize to\\nBorel measures p, q, if we deﬁne the mean map as\\n\\nµ : p (cid:55)→ Ex∼p[k(x, ·)],\\n\\nand replace the notion of strictly pd kernels by that of characteristic kernels [33]. This means that we do not lose\\ninformation when representing a probability distribution in the RKHS. This enables us to work with distributions using\\nHilbert space methods, and construct practical algorithms analyzing distributions using scalar product evaluations.\\n\\nNote that the mean map µ can be viewed as a generalization of the moment generating function Mp of a random\\nvariable x with distribution p,\\n\\nMp(.) = Ex∼p\\n\\n(cid:104)\\n\\ne(cid:104)x, · (cid:105)(cid:105)\\n\\n.\\n\\nThe map µ has applications in a number of tasks including computing functions of random variables [132], testing\\nof homogeneity [41], and of independence [43]. The latter will be of particular interest to causal inference: we\\ncan develop a kernel-based independence test by computing the distance between sample-based embeddings of a\\njoint distribution p(X, Y ) and the product of its marginals p(X), p(Y ) [42, 44, 43, 165, 115], and generalize it to\\nconditional independence testing [33, 101], as required for certain causal discovery methods (see § 7).\\n\\n3 From Statistical to Causal Models\\n\\nMethods Relying on i.i.d. Data In current successes of machine learning [79], we generally (i) have large amounts\\nof data, often from simulations or large-scale human labeling, (ii) use high capacity machine learning models (e.g.,\\nneural networks with many adjustable parameters), and (iii) employ high performance computing. Statistical learning\\n\\n4Note that the data domain X need not have any structure other than being a nonempty set.\\n\\n5\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 3: Measurements of two genes (x-axis), gene A (left) and gene B (right), show the same strong positive\\ncorrelation with a phenotype (y-axis). However, this statistical information alone is insufﬁcient to predict the outcome\\nof a knock-out experiment where the activity of a gene is set to zero (vertical lines at x = 0). Answering such\\ninterventional questions requires additional causal knowledge (inset causal graphs): knocking out gene A, which is a\\ndirect cause, would lead to a reduction in phenotype, whereas knocking out gene B, which shares a common cause, or\\nconfounder, with the phenotype but has no causal effect on it, would leave the phenotype unaffected. This shows that\\ncorrelation alone is not enough to predict the outcome of perturbations to a system (toy data, ﬁgure from [112]).\\n\\ntheory offers a partial explanation for recent successes of learning: huge datasets enable training complex models and\\nthus solving increasingly difﬁcult tasks.\\n\\nHowever, a crucial aspect that is often ignored is that we (iv) assume that the data are i.i.d. This assumption is crucial\\nfor good performance in practice, and it underlies theoretical statements such as (2.4). When faced with problems\\nthat violate the i.i.d. assumption, all bets are off. Vision systems can be grossly misled if an object that is normally\\nrecognized with high accuracy is placed in a context that in the training set may be negatively correlated with the\\npresence of the object. For instance, such a system may fail to recognize a cow standing on the beach. In order to\\nsuccessfully generalize in such settings, we would need to construct systems which do not merely rely on statistical\\ndependences, but instead model mechanisms that are robust across certain violations of the i.i.d. assumption. As we\\nwill argue, causality provides a natural framework for capturing such stable mechanisms and reasoning about different\\ntypes of distribution shifts.\\n\\nCorrelation vs Causation It is a commonplace that correlation does not imply causation. Two popular and\\nillustrative examples are the positive correlation between chocolate consumption and nobel prizes per capita [91],\\nand that between the number of stork breeding pairs and human birth rates [89], neither of which admit a sensible\\ninterpretation in terms of direct causation. These examples naturally lead to the following questions: What exactly\\ndo we mean by “causation”? What is its relationship to correlation? And, if correlation alone is not enough, what is\\nneeded to infer causation?\\n\\nHere, we adopt a notion of causality based on manipulability [159] and intervention [105] which has proven useful in\\nﬁelds such as agriculture [161], econometrics [46, 52] and epidemiology [119].\\n\\nDeﬁnition 3.1 (Causal effect). We say that a random variable X has a causal effect on a random variable Y if there\\nexist x (cid:54)= x(cid:48) s.t. the distribution of Y after intervening on X and setting it to x differs from the distribution of Y after\\nsetting X to x(cid:48).\\n\\nInherent to the notion of causation, there is a directionality and asymmetry which does not exist for correlation: if\\nX is correlated with Y , then Y is equally correlated with X; but, if X has a causal effect on Y , the converse (in the\\ngeneric case) does not hold.\\n\\nWe illustrate the intervention-based notion of causation and its difference from correlation (or, more generally, sta-\\ntistical dependence) in Fig. 3. Here, knocking out two genes XA and XB that are indistinguishable based on their\\ncorrelation with a phenotype Y would have very different effects. Only intervening on XA would change the dis-\\ntribution of Y , whereas XB does not have a causal effect on Y —instead, their correlation arises from a different\\n(confounded) causal structure. Such causal relationships are most commonly represented in the form of causal graphs\\nwhere directed arrows indicate a direct causal effect.\\n\\n6\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nX\\n\\nY\\n\\nX\\n\\nY\\n\\nX\\n\\nY\\n\\n(a)\\n\\n(b)\\n\\nZ\\n\\n(c)\\n\\nFigure 4: Reichenbach’s common cause principle [117] postulates that statistical dependence between two random\\nvariables X and Y has three elementary possible causal explanations shown as causal graphs in (a)–(c). It thus states\\nthat association is always induced by an underlying causal process. In (a) the common cause Z coincides with X, and\\nin (b) it coincides with Y . Grey nodes indicate observed and white nodes unobserved variables.\\n\\nThe example in Fig. 3 shows that the same correlation can be explained by multiple causal graphs which lead to\\ndifferent experimental outcomes, i.e., correlation does not imply causation. However, there is a connection between\\ncorrelation and causation, expressed by Reichenbach [117] as the Common Cause Principle, see Fig. 4.\\nPrinciple 3.2 (Common Cause). If two random variables X and Y are statistically dependent (X (cid:54)⊥⊥ Y ), then there\\nexists a random variable Z which causally inﬂuences both of them and which explains all their dependence in the\\nsense of rendering them conditionally independent (X ⊥⊥ Y | Z). As a special case, Z may coincide with X or Y .\\n\\nAccording to Principle 3.2, statistical dependence always results from underlying causal relationships by which vari-\\nables, including potentially unobserved ones, inﬂuence each other. Correlation is thus an epiphenomenon, the by-\\nproduct of a causal process.\\n\\nFor the example of chocolate consumption (X) and Nobel laureates (Y ), common sense suggests that neither of the two\\nvariables should have a causal effect on the other, i.e., neither chocolate consumption driving scientiﬁc success (X →\\nY ; Fig. 4a) nor Nobel laureates increasing chocolate consumption (Y → X; Fig. 4c) seem plausible. Principle 3.2\\nthen tells us that the observed correlation must be explained by a common cause Z as in Fig. 4c. A plausible candidate\\nfor such a confounder could, for example, be economic factors driving both consumer spending and investment in\\neducation and science.\\n\\nWithout such background knowledge or additional assumptions, however, we cannot distinguish the three cases\\nin Fig. 4 through passive observation, i.e., in a purely data-driven way: the class of observational distributions over X\\nand Y that can be realized by these models is the same in all three cases.\\n\\nTo be clear, this does not mean that correlation cannot be useful, nor that causal insight is always required. Both genes\\nin Fig. 3 remain useful features for making predictions in a passive, or observational, setting in which we measure the\\nactivities of certain genes and are asked to predict the phenotype. Similarly, chocolate consumption remains predictive\\nof winning Nobel prizes. However, if we want to answer interventional questions, such as the outcome of a gene-\\nknockout experiment or the effect of a policy enforcing higher chocolate consumption, we need more than correlation:\\na causal model.\\n\\n4 Causal Modeling Frameworks\\n\\nCausal inference has a long history in a variety of disciplines, including statistics, econometrics, epidemiology, and\\nAI. As a result, different frameworks for causal modeling have emerged over the years and coexist today. The ﬁrst\\nframework described below (CGM) starts from the distribution of the observables, combining it with a directed graph\\nto endow it with causal semantics. The second one (SCM) starts from a graph and a set of functional assignments,\\nand generates the observed distribution as the push-forward of an unobserved noise distribution. Finally, we cover a\\nnon-graphical approach (PO) popular in statistics.\\n\\nCausal Graphical Models (CGMs) The graphical models framework [78, 75] provides a compact way of represent-\\ning joint probability distributions by encoding the dependence structure between variables in graphical form. Directed\\ngraphical models are also known as Bayesian networks [102]. While they do not offer a causal interpretation per se—\\nindeed, different graphical models can be compatible with the same distribution (cf. Principle 3.2)—when edges are\\nendowed with the notion of direct causal effect (Defn. 3.1), we refer to them as causal graphical models (CGM) [145].\\nDeﬁnition 4.1 (CGM). A CGM M = (G, p) over n random variables X1, . . . , Xn consists of: (i) a directed acyclic\\ngraph (DAG) G in which directed edges (Xj → Xi) represent a direct causal effect of Xj on Xi; and (ii) a joint\\ndistribution p(X1, . . . , Xn) which is Markovian w.r.t. G:\\n\\np(X1, . . . , Xn) =\\n\\np(Xi | PAi)\\n\\n(4.1)\\n\\nn\\n(cid:89)\\n\\ni=1\\n\\n7\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nX1\\n\\nX2\\n\\nX3\\n\\n(a) G\\n\\nX1\\n\\nx2\\n\\nX3\\n\\n(b) G(cid:48)\\n\\nX1\\n\\nX2\\n\\nx3\\n\\n(c) G(cid:48)(cid:48)\\n\\nFigure 5: (a) A directed acyclic graph (DAG) G over three variables. A causal graphical model (G, p) with causal\\ngraph G and observational distribution p can be used to answer interventional queries using the concept of graph\\nsurgery: when a variable is intervened upon and set to a constant (white diamonds), this removes any inﬂuence from\\nother variables, captured graphically by removing all incoming edges. (b) and (c) show post-intervention graphs G(cid:48)\\nand G(cid:48)(cid:48) for do(X2 := x2) and do(X3 := x3), respectively. (An intervention on X1 would leave the graph unaffected.)\\n\\nwhere PAi = {Xj : (Xj → Xi) ∈ G} denotes the set of parents, or direct causes, of Xi in G.\\n\\nWe will refer to (4.1) as the causal (or disentangled) factorization. While many other entangled factorizations are\\npossible, e.g.,\\n\\np(X1, . . . , Xn) =\\n\\np(Xi | Xi+1, . . . , Xn),\\n\\n(4.2)\\n\\nn\\n(cid:89)\\n\\ni=1\\n\\nonly (4.1) decomposes the joint distribution into causal conditionals, or causal mechanisms, p(Xi | PAi), which can\\nhave a meaningful physical interpretation, rather than being mere mathematical objects such as the factors on the RHS\\nof (4.2).\\n\\nIt turns out that (4.1) is equivalent to the following condition.\\nDeﬁnition 4.2 (Causal Markov condition). A distribution p satisﬁes the causal Markov condition w.r.t. a DAG G if\\nevery variable is conditionally independent of its non-descendants in G given its parents in G.\\n\\nDefn. 4.2 can equivalently be expressed in terms of d-separation, a graphical criterion for directed graphs [105], by\\nsaying that d-separation in G implies (conditional) independence in p. The causal Markov condition thus provides a\\nlink between properties of p and G.\\n\\nWhat makes CGMs causal is the interpretation of edges as cause-effect relationships which enables reasoning about\\nthe outcome of interventions using the do-operator [105] and the concept of graph surgery [145]. The central idea is\\nthat intervening on a variable, say by externally forcing it to take on a particular value, renders it independent of its\\ncauses and breaks their causal inﬂuence on it, see Fig. 5 for an illustration. For example, if a gene is knocked out, it\\nis no longer inﬂuenced by other genes that were previously regulating it; instead, its activity is now solely determined\\nby the intervention. This is fundamentally different from conditioning since passively observing the activity of a gene\\nprovides information about its driving factors (i.e., its direct causes).\\n\\nTo emphasize this difference between passive observation and active intervention, Pearl [105] introduced the notation\\ndo(X := x) to denote an intervention by which variable X is set to value x. The term graph surgery refers to the idea\\nthat the effect of such an intervention can be captured in the form of a modiﬁcation to the original graph by removing\\nall incoming edges to the intervened variable. Interventional queries can then be answered by performing probabilistic\\ninference in the modiﬁed post-intervention graph which typically implies additional (conditional) independences due\\nto the removed edges.\\nExample 4.3. The interventional distribution p(X3 | do(X2 = x2)) for the CGM in Fig. 5 is obtained via probabilistic\\ninference w.r.t. the post-intervention graph G(cid:48) where X1 ⊥⊥ X2:\\n\\np(X3|do(X2 := x2)) =\\n\\np(x1)p(X3|x1, x2)\\n\\n(cid:54)=\\n\\np(x1 | x2)p(X3 | x1, x2) = p(X3 | x2)\\n\\n(4.3)\\n\\n(4.4)\\n\\nIt differs from the conditional p(X3 | x2) for which inference in done over G where X1 (cid:54)⊥⊥ X2. Note the marginal\\np(x1) in (4.3), in contrast to the conditional p(x1 | x2) in (4.4): this is precisely the link which is broken by the\\nintervention do(X2 := x2), see Fig. 5b. The RHS of (4.3) is an example of covariate adjustment: it controls for the\\nconfounder X1 of the causal effect of X2 on X3, see § 9 for more details on adjustment and computing interventions.\\n\\n(cid:88)\\n\\nx1∈X1\\n(cid:88)\\n\\nx1∈X1\\n\\n8\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nCGMs have been widely used in constraint- and score-based approaches to causal discovery [145, 47] which we\\nwill discuss in § 7. Due to their conceptual simplicity, they are a useful and intuitive model for reasoning about\\ninterventions. However, their capacity as a causal model is limited in that they do not support counterfactual reasoning,\\nwhich is better addressed by the two causal modeling frameworks which we will discuss next.\\n\\nStructural Causal Models (SCMs) Structural Causal Models, also referred to as functional causal models or\\nnon-parametric structural equation models, have ties to the graphical approach presented above, but rely on using\\ndirected functional parent-child relationships rather than causal conditionals. While conceptually simple in hindsight,\\nthis constituted a major step in the understanding of causality, as later expressed by [105] (p. 104):\\n\\n“We played around with the possibility of replacing the parents-child relationship p(Xi | PAi)\\nwith its functional counterpart Xi = fi(PAi, Ui) and, suddenly, everything began to fall into place:\\nWe ﬁnally had a mathematical object to which we could attribute familiar properties of physical\\nmechanisms instead of those slippery epistemic probabilities p(Xi | PAi) with which we had been\\nworking so long in the study of Bayesian networks.”\\n\\nDeﬁnition 4.4 (SCM). An SCM M = (F, pU) over a set X of n random variables X1, . . . , Xn consists of (i) a set F\\nof n assignments (the structural equations),\\n\\nF = {Xi := fi(PAi, Ui)}n\\nwhere fi are deterministic functions computing each variable Xi from its causal parents PAi ⊆ X \\\\ {Xi} and an\\nexogenous noise variable Ui; and (ii) a joint distribution pU(U1, . . . , Un) over the exogenous noise variables.\\n\\n(4.5)\\n\\ni=1\\n\\nThe paradigm of SCMs views the processes fi by which each observable Xi is generated from others as a physical\\nmechanism. All randomness comes from the unobserved (also referred to as unexplained) noise terms Ui which\\ncapture both possible stochasticity of the process, as well as uncertainty due to unmeasured parts of the system.\\n\\nNote also the assignment symbol “:=” which is used instead of an equality sign to indicate the asymmetry of the causal\\nrelationship: the LHS quantity is deﬁned to take on the RHS value. For example, we cannot simply rewrite a structural\\nequation X2 := f2(X1, U2) as X1 = g(X2, U2) for some g, as would be the case for a standard (invertible) equation.\\n\\nIn parametric, linear form (i.e., with linear fi), SCMs are also known as structural equation models and have a long\\nhistory in path analysis [161] and economics [46, 52].\\n\\nEach SCM induces a corresponding causal graph via the input variables to the structural equations which is useful as\\na representation and provides a link to CGMs.\\nDeﬁnition 4.5 (Induced causal graph). The causal graph G induced by an SCM M is the directed graph with vertex\\nset X and a directed edge from each vertex in PAi to Xi for all i.\\nExample 4.6. Consider an SCM over X = {X1, X2, X3} with some pU(U1, U2, U3) and\\n\\nX1 := f1(U1),\\n\\nX2 := f2(X1, U2),\\n\\nX3 := f3(X1, X2, U3).\\n\\nFollowing Defn. 4.5, the induced graph then corresponds to G in Fig. 5.\\n\\nDefn. 4.4 allows for a rich class of causal models, including ones with cyclic causal relations and ones which do not\\nobey the causal Markov condition (Defn. 4.2) due to complex covariance structures between the noise terms. While\\nwork exists on such cyclic or confounded SCMs [13], it is common to make the following two assumptions.\\nAssumption 4.7 (Acyclicity). The induced graph G is a DAG: it does not contain cycles.\\nAssumption 4.8 (Causal sufﬁciency/no hidden confounders). The Ui are jointly independent, i.e., their distribution\\nfactorises: pU(U1, . . . , Un) = pU1(U1) × · · · × pUn (Un).\\n\\nAssumption 4.7 implies5 the existence of a well-deﬁned, unique (observational) distribution over X from which we\\ncan draw via ancestral sampling:6 ﬁrst, we draw the noise variables from pU, and then we iteratively compute the\\ncorresponding Xi’s in topological order of the induced DAG (i.e., starting at the root node of the graph), substituting\\npreviously computed Xi into the structural equations where necessary.\\nFormally, the (observational) distribution\\np(X1, . . . , Xn) induced by an SCM under Assumption 4.7 is deﬁned as the push-forward of the noise distribution pU\\nthrough the structural equations F. Under Assumption 4.8, the causal conditionals are thus given by\\n\\np(Xi | PAi = pai) := pUi(f −1\\npai\\n\\n(Xi))\\n\\nfor\\n\\ni = 1, . . . , n,\\n\\n(4.6)\\n\\n5Acyclicity is a sufﬁcient, but not a necessary condition.\\n6Since neither F nor p are known a priori, ancestral sampling should be seen as a hypothetical sampling procedure; inference\\n\\nand learning are generally still necessary.\\n\\n9\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nwhere f −1\\npai\\n\\n(Xi) denotes the pre-image of Xi under fi for ﬁxed PAi = pai.\\n\\nAssumption 4.8 rules out the existence of hidden confounders because any unmeasured variables affecting more than\\none of the Xi simultaneously would constitute a dependence between some of the noise terms (which account for any\\nexternal, or exogenous, inﬂuences not explained by the observed Xi). In combination with Assumption 4.7, Assump-\\ntion 4.8 (also known as causal sufﬁciency), thus ensures that the distribution induced by an SCM factorises according\\nto its induced causal graph as in (4.1). In other words, it guarantees that the causal Markov condition is satisﬁed w.r.t.\\nthe induced causal graph [105]. Below, unless explicitly stated otherwise, we will assume causal sufﬁciency.\\n\\nDue to the conceptual similarity between interventions and the assignment character of structural equations, the com-\\nputation of interventional distributions ﬁts in naturally into the SCM framework. To model an intervention, we simply\\nreplace the corresponding structural equation and consider the resulting entailed distribution.\\nDeﬁnition 4.9 (Interventions in SCMs). An intervention do(Xi := xi) in an SCM M = (F, pU ) is modeled by\\nreplacing the ith structural equation in F by Xi := xi, yielding the intervened SCM Mdo(Xi:=xi) = (F(cid:48), pU ). The\\ninterventional distribution p(X−i | do(Xi = xi)), where X−i = X \\\\ {Xi}, and intervention graph G(cid:48) are those\\ninduced by Mdo(Xi=xi).\\n\\nThis way of handling interventions coincides with that for CGMs: e.g., after performing do(X2 := x2) in Ex. 4.6, X1\\nno longer appears in the structural equation for X2, and the edge X1 → X2 hence disappears in the intervened graph,\\nas is the case for G(cid:48) in Fig. 5.\\n\\nIn contrast to CGMs, SCMs also provide a framework for counterfactual reasoning. While (i) observations describe\\nwhat is passively seen or measured and (ii) interventions describe active external manipulation or experimentation, (iii)\\ncounterfactuals are statements about what would or could have been, given that something else was in fact observed.\\nThese three modes of reasoning are sometimes referred to as the three rungs of the “ladder of causation” [108]. As an\\nexample, consider the following counterfactual query:\\n\\nGiven that patient X received treatment A and their health got worse, what would have happened if\\nthey had been given treatment B instead, all else being equal?\\n\\nThe “all else being equal” part highlights the difference between interventions and counterfactuals: observing the\\nfactual outcome (i.e., what actually happened) provides information about the background state of the system (as\\ncaptured by the noise terms in SCMs) which can be used to reason about alternative, counterfactual, outcomes. This\\ndiffers from an intervention where such background information is not available. For example, observing that treatment\\nA did not work may tell us that the patient has a rare condition and that treatment B would have therefore worked.\\nHowever, given that treatment A has been prescribed, the patient’s condition may have changed, and B may no longer\\nwork in a future intervention.\\n\\nNote that counterfactuals cannot be observed empirically by their very deﬁnition and are therefore unfalsiﬁable. Some\\ntherefore consider them unscientiﬁc [116] or at least problematic [26]. On the other hand, humans seem to perform\\ncounterfactual reasoning in practice, developing this ability in early childhood [14].\\n\\nCounterfactuals are computed in SCMs through the following three-step procedure:\\n\\n1. Update the noise distribution to its posterior given the observed evidence (“abduction”).\\n2. Manipulate the structural equations to capture the hypothetical intervention (“action”).\\n3. Use the modiﬁed SCM to infer the quantity of interest (“prediction”).\\n\\nDeﬁnition 4.10 (Counterfactuals in SCMs). Given evidence X = x observed from an SCM M = (F, pU ), the\\ncounterfactual SCM MX=x is obtained by updating pU with its posterior: MX=x = (F, pU |X=x). Counterfactuals\\nare then computed by performing interventions in the counterfactual SCM MX=x, see Defn. 4.9.\\n\\nNote that while computing interventions only involved manipulating the structural equations, counterfactuals also\\ninvolve updating the noise distribution, highlighting the conceptual difference between the two. Updating pU requires\\nknowledge of the interaction between noise and observed variables, i.e., of the structural equations, which explains\\nwhy additional assumptions are necessary. Note that the updated noise variables no longer need to be independent,\\neven if the original system was causally sufﬁcient (Assumption 4.8).\\nExample 4.11 (Computing counterfactuals with SCMs). Consider an SCM M deﬁned by\\n\\nX := UX ,\\n\\nY := 3X + UY ,\\n\\nUX , UY ∼ N(0, 1).\\n\\n(4.7)\\n\\nSuppose we observe X = 2 and Y = 6.5 and want to answer the counterfactual “what would Y have been, had\\nX = 1?”, i.e., we are interested in p(YX=1 | X = 2, Y = 6.5). Updating the noise using the observed evidence\\n\\n10\\n\\n\\x0cTable 1: Causal inference as a missing data problem: for each individ-\\nual i (rows), only the PO Yi(Ti) corresponding to the assigned treat-\\nment Ti is observed; the other PO is a counterfactual. Hence, the unit-\\nlevel causal effect τi = Yi(1) − Yi(0) is non-identiﬁable.\\n\\nFROM STATISTICAL TO CAUSAL LEARNING\\n\\ni\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n\\nTi\\n1\\n0\\n1\\n1\\n0\\n0\\n\\nYi(1)\\n7\\n?\\n3\\n6\\n?\\n?\\n\\nYi(0)\\n?\\n8\\n?\\n?\\n4\\n1\\n\\nτi\\n?\\n?\\n?\\n?\\n?\\n?\\n\\nvia (4.7), we obtain the counterfactual SCM MX=2,Y =6.5,\\n\\nX := UX ,\\n\\nY := 3X + UY ,\\n\\nUX ∼ δ(2),\\n\\nUY ∼ δ(0.5),\\n\\n(4.8)\\n\\nwhere δ(·) denotes the Dirac delta measure. Performing the intervention do(X := 1) in (4.8) then gives the\\nresult p(YX=1 | X = 2, Y = 6.5) = δ(3.5), i.e., “Y would have been 3.5”. This differs from the interventional\\ndistribution p(Y | do(X = 1)) = N(3, 1), since the factual observation helped determine the background\\nstate (UX = 2, UY = 0.5).\\n\\nThe SCM viewpoint is intuitive and lends itself well to studying restrictions on function classes to enable induction\\n(§ 2). For this reason, we will mostly focus on SCMs in the subsequent sections.\\n\\nPotential Outcomes (PO) The potential outcomes framework was initially proposed by Neyman [98] for random-\\nized studies [31], and later popularized and extended to observational settings by Rubin [125] and others. It is popular\\nwithin statistics and epidemiology and perhaps best understood in the context of the latter. This is also reﬂected in\\nits terminology: in the most common setting, we consider a binary treatment variable T , with T = 1 and T = 0\\ncorresponding to treatment and control, respectively, whose causal effect on an outcome variable Y (often a measure\\nof health) is of interest.\\n\\nOne interpretation of the PO framework consistent with its roots in statistics is to view causal inference as a missing\\ndata problem. In the PO framework, for each individual (or unit) i and treatment value t there is a PO, or potential\\nresponse, denoted Yi(t) capturing what would happen if individual i received treatment t. The POs are considered\\ndeterministic quantities in the sense that for a given individual i, Yi(1) and Yi(0) are ﬁxed and all randomness in the\\nrealized outcome Yi stems from randomness in the treatment assignment:\\n\\nTo decide whether patient i should receive treatment, we need to reason about the individualized treatment effect (ITE)\\nτi as captured by the difference of the two POs.\\nDeﬁnition 4.12 (ITE). The ITE for individual i under a binary treatment is deﬁned as\\n\\nThe “fundamental problem of causal inference” [51] is that only one of the POs is ever observed for each i. The other,\\nunobserved PO becomes a counterfactual:\\n\\nConsequently, τi can never be measured or computed from data, i.e., it is not identiﬁable (without further assumptions),\\nas illustrated in Tab. 1.\\n\\nImplicit in the form of (4.9) and (4.11) are the following two assumptions.\\nAssumption 4.13 (Stable unit treatment value; SUTVA). The observation on one unit should be unaffected by the\\nparticular assignment of treatments to the other units [23].\\nAssumption 4.14 (Consistency). If individual i receives treatment t, then the observed outcome is Yi = Yi(t), i.e.,\\nthe potential outcome for t.\\n\\nAssumption 4.13 is usually understood as (i) units do not interfere, and (ii) there is only one treatment level per group\\n(treated or control) leading to well-deﬁned POs [61]. It can be violated, e.g., through (i) population dynamics such\\nas herd immunity from vaccination or (ii) technical errors or varying within-group dosage, respectively. However, for\\nmany situations such as controlled studies it can be a reasonable assumption, and we can then view different units as\\nindependent samples from a population.\\n\\n11\\n\\nYi = T Yi(1) + (1 − T )Yi(0).\\n\\nτi = Yi(1) − Yi(0).\\n\\nY CF\\ni = (1 − T )Yi(1) + T Yi(0).\\n\\n(4.9)\\n\\n(4.10)\\n\\n(4.11)\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nSo far, we have considered POs for a given unit as deterministic quantities. However, most times it is impossible to\\nfully characterize a unit, e.g., when dealing with complex subjects such as humans. Such lack of complete information\\nintroduces uncertainty, so that POs are often instead treated as random variables. This parallels the combination of\\ndeterministic structural equations with exogenous noise variables in SCMs.7 Indeed, there is a equivalence between\\nPOs and SCMs [105]:\\n\\nYi(t) = Y | do(T = t)\\n\\nin an SCM with\\n\\nU = ui,\\n\\nAn individual in the PO framework thus corresponds to a particular instantiation of the Uj in an SCM: the outcome is\\ndeterministic given U, but since we do not observe ui (nor can we characterize a given individual based on observed\\ncovariates), the counterfactual outcome is treated as a random variable. In practice, all we observe is a featurised\\ndescription xi of an individual i and have to reason about expected POs, E[Y (1), Y (0) | xi].\\nAnother common assumption is that of no hidden confounders which we have already encountered in form of the\\ncausal Markov condition (Defn. 4.2) for CGMs and causal sufﬁciency (Assumption 4.8) for SCMs. In the PO frame-\\nwork this becomes no hidden confounding between treatment and outcome and is referred to as (conditional) ignora-\\nbility.\\nAssumption 4.15 (Conditional ignorability). Given a treatment T ∈ {0, 1}, potential outcomes Y (0), Y (1), and\\nobserved covariates X, we have:\\n\\nY (0) ⊥⊥ T | X and Y (1) ⊥⊥ T | X.\\n\\n(4.12)\\n\\nThe PO framework is tailored toward studying the (confounded) effect of a typically binary treatment variable on an\\noutcome and is mostly used for causal reasoning, i.e., estimating individual and population level causal effects (§ 9).\\nIn this context, it is sometimes seen as an advantage that an explicit graphical representation is not needed. At the same\\ntime, the lack of a causal graph and the need for special treatment and outcome variables make POs rather unsuitable\\nfor causal discovery where other frameworks prevail.\\n\\n5\\n\\nIndependent Causal Mechanisms\\n\\nLet us consider the disentangled factorization (4.1) of the joint distribution p(X1, . . . , Xn). This factorization ac-\\ncording to the causal graph is possible whenever the Ui are independent. We now consider an additional notion of\\nindependence, concerning how the factors in (4.1) relate to one another.\\n\\nConsider a dataset that consists of altitude A and average annual temperature T of weather stations [112]. Supposed\\nwe ﬁnd that A and T are correlated, which we attribute due to the fact that the altitude has a causal effect on the\\ntemperature. Suppose we had two such datasets, one for Austria and one for Switzerland. The two joint distributions\\nmay be different, since the marginal distributions p(A) over altitudes will likely differ. The conditionals p(T | A),\\nhowever, may be rather similar, since they reﬂect physical mechanisms generating temperature from altitude. The\\ncausal factorization p(A)p(T | A) thus contains a component p(T | A) that generalizes across countries, while the\\nentangled factorization p(T )p(A | T ) does not. A similar reasoning applies when we consider interventions in a\\nsystem. For a model to correctly predict the effect of interventions, it needs to have components that are robust when\\nmoving from an observational distribution to certain interventional distributions.\\n\\nThe above insights can be stated as follows [130, 112]:\\nPrinciple 5.1 (Independent Causal Mechanisms (ICM)). The causal generative process of a system’s variables is\\ncomposed of autonomous modules that do not inform or inﬂuence each other. In the probabilistic case, this means\\nthat the conditional distribution of each variable given its causes (i.e., its mechanism) does not inform or inﬂuence the\\nother mechanisms.\\n\\nThis principle subsumes several notions important to causality, including separate intervenability of causal variables,\\nmodularity and autonomy of subsystems, and invariance [105, 112]. In the two-variable case, it reduces to an inde-\\npendence between the cause distribution and the mechanism producing the effect from the cause.\\n\\nApplied to the causal factorization (4.1), the principle tells us that the factors should be independent in two senses:\\n\\n(inﬂuence)\\n\\n(information)\\n\\nchanging (or intervening upon) one mechanism p(Xi | PAi) does not change the other mechanisms\\np(Xj | PAj) (i (cid:54)= j), and\\nknowing some mechanisms p(Xi | PAi) (i (cid:54)= j) does not give us information about a mechanism\\np(Xj | PAj).\\n\\n7When all noise variables in an SCM are ﬁxed, the other variables are uniquely determined; without complete background\\n\\nknowledge, on the other hand, they are random.\\n\\n12\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nIf p(x) and f are\\nFigure 6:\\nchosen independently,\\nthen\\npeaks of p(y) tend to occur\\nin regions where f has small\\nslope. As a result, p(y) con-\\ntains information about f −1\\n(ﬁgure from [112]).\\n\\ny\\n\\nf (x)\\n\\np\\n(\\ny\\n)\\n\\nx\\n\\np(x)\\n\\nWe view any real-world distribution as a product of causal mechanisms. A change in such a distribution (e.g., when\\nmoving from one setting/domain to a related one) will always be due to a change in at least one of those mechanisms.\\nConsistent with Principle 5.1, we hypothesize [131]:\\n\\nPrinciple 5.2 (Sparse Mechanism Shift (SMS)). Small distribution changes tend to manifest themselves in a sparse or\\nlocal way in the causal/disentangled factorization (4.1), i.e., they should usually not affect all factors simultaneously.\\n\\nIn contrast, in a non-causal factorization, e.g., (4.2), many terms will be affected simultaneously if we change one\\nof the physical mechanisms responsible for a system’s statistical dependences. Such a factorization may thus be\\ncalled entangled. The notion of disentanglement has recently gained popularity in machine learning [9, 50, 82, 148],\\nsometimes loosely identiﬁed with statistical independence. The notion of invariant, autonomous, and independent\\nmechanisms has appeared in various guises throughout the history of causality research, see [1, 105, 130, 112, 131].\\n\\nMeasures of Dependence of Mechanisms Note that the dependence of two mechanisms p(Xi | PAi) and p(Xj |\\nPAj) does not coincide with the statistical dependence of the random variables Xi and Xj. Indeed, in a causal graph,\\nmany of the random variables will be dependent even if all the mechanisms are independent.\\n\\nConsider two variables and structural assignments X := U and Y := f (X). I.e., the cause X is a noise variable (with\\ndensity p(x)), and the effect Y is a deterministic function of the cause. Let us moreover assume that the ranges of X\\nand Y are both [0, 1], and f is strictly monotonically increasing. The ICM principle then reduces to an independence\\nof p(x) and f . Let us consider p(x) and the derivative f (cid:48) as random variables on the probability space [0, 1] with\\nLebesgue measure, and use their correlation as a measure of dependence of mechanisms. It can be shown that for\\nf (cid:54)= id, independence of p(x) and f (cid:48) implies dependence between p(y) and (f −1)(cid:48) (see Fig. 6). Other measures are\\npossible and admit information-geometric interpretations. Intuitively, under the ICM assumption (Principle 5.1), the\\n“irregularity” of the effect distribution becomes a sum of (i) irregularity already present in the input distribution and (ii)\\nirregularity introduced by the mechanism f , i.e., the irregularities of the two mechanisms add up rather than (partly)\\ncompensating each other. This would not be the case in the opposite (“anticausal”) direction (for details, see [64]).\\nOther dependence measures have been proposed for high-dimensional linear settings and time series [63, 137, 12, 68].\\n\\nAlgorithmic Independence So far, we have discussed links between causal and statistical structures. The fundamen-\\ntal of the two is the causal structure, since it captures the physical mechanisms that generate statistical dependences in\\nthe ﬁrst place. The statistical structure is an epiphenomenon that follows if we make the unexplained variables random.\\nIt is awkward to talk about the (statistical) information contained in a mechanism, since deterministic functions in the\\ngeneric case neither generate nor destroy information. This motivated us to devise an algorithmic model of causal\\nstructures in terms of Kolmogorov complexity [66]. The Kolmogorov complexity (or algorithmic information) of a bit\\nstring is essentially the length of its shortest compression on a Turing machine, and thus a measure of its information\\ncontent. Independence of mechanisms can be deﬁned as vanishing mutual algorithmic information: two conditionals\\nare considered independent if knowing (the shortest compression of) one does not help achieve a shorter compression\\nof the other one.\\n\\nAlgorithmic information theory is an elegant framework for non-statistical graphical models. Just like statistical CGMs\\nare obtained from SCMs by making the unexplained variables Ui random, we obtain algorithmic CGMs by turning the\\nUi into bit strings (jointly independent across nodes), and viewing the node Xi as the output of a ﬁxed Turing machine\\nrunning the program Ui with input PAi. Similar to the statistical case, one can deﬁne a local causal Markov condition,\\na global one in terms of d-separation, and a decomposition of the joint Kolmogorov complexity in analogy to (4.1),\\nand prove that they are implied by the SCM [66]. This approach shows that concepts of causality are not intrinsically\\ntied to statistics: causality is about mechanisms governing ﬂow of information which may or may not be statistical.\\n\\n13\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nTable 2: Simple model taxonomy. The most detailed model (top) is a mechanistic or physical one, often in terms\\nof differential equations. At the other end of the spectrum (bottom), we have a purely statistical model; this can be\\nlearned from data and is useful for predictions but often provides little insight beyond modeling associations between\\nepiphenomena. Causal models can be seen as intermediate descriptions, abstracting away from physical realism while\\nretaining the power to answer certain interventional or counterfactual questions.\\n\\nModel\\n\\nPredict in i.i.d. Predict under distr. Answer counter-\\nfactual questions\\nshift/intervention\\n\\nsetting\\n\\nObtain\\nphysical insight\\n\\nLearn from\\ndata\\n\\nMechanistic/physical\\nStructural causal\\nCausal graphical\\nStatistical\\n\\nyes\\nyes\\nyes\\nyes\\n\\nyes\\nyes\\nyes\\nno\\n\\nyes\\nyes\\nno\\nno\\n\\nyes\\n?\\n?\\nno\\n\\n?\\n?\\n?\\nyes\\n\\nThe assumption of algorithmically independent mechanisms has interesting implications for physics:\\nit implies\\nthe second law of thermodynamics (i.e., the arrow of time). Consider a process where an incoming ordered beam\\nof photons (the cause) is scattered by an object (the mechanism). Then the outgoing beam (the effect) contains\\ninformation about the object. Microscopically, the time evolution is reversible; however, the photons contain\\ninformation about the object only after the scattering. What underlies Loschmidt’s paradox [86]?\\n\\nThe asymmetry can be explained by applying the ICM Principle 5.1 to initial state and system dynamics, postulating\\nthat the two be algorithmically independent, i.e., knowing one does not allow a shorter description of the other one.\\nThe Kolmogorov complexity of the system’s state can then be shown to be non-decreasing under time evolution [62].\\nIf we view Kolmogorov complexity as a measure of entropy, this means that the entropy of the state can only stay\\nconstant or increase, amounting to the second law of thermodynamics.\\n\\nNote that the resulting state after time evolution is clearly not independent of the system dynamic: it is precisely the\\nstate that, when fed to the inverse dynamics, would return us to the original (ordered) state.\\n\\n6 Levels of Causal Modeling\\n\\nCoupled differential equations are the canonical way of modeling physical phenomena. They allow us to predict the\\nfuture behavior of a system, to reason about the effect of interventions, and—by suitable averaging procedures—to\\npredict statistical dependences that are generated by a coupled time evolution. They also allow us to gain insight into\\na system, explain its functioning, and, in particular, read off its causal structure.\\n\\nConsider a coupled set of ordinary differential equations\\n\\ndx\\ndt\\n\\n= f (x), x ∈ Rd,\\n\\n(6.1)\\n\\n(6.2)\\n\\nwith initial value x(t0) = x0. We assume that they correctly describe the physical mechanisms of a system.8 The\\nPicard–Lindel¨of theorem states that, at least locally, if f is Lipschitz, there exists a unique solution x(t). This implies,\\nin particular, that the immediate future of x is implied by its past values.\\n\\nIn terms of inﬁnitesimal differentials dt and dx = x(t + dt) − x(t), (6.1) reads:\\n\\nx(t + dt) = x(t) + dt · f (x(t)).\\n\\nFrom this, we can ascertain which entries of the vector x(t) cause the future of others x(t + dt), i.e., the causal\\nstructure.\\n\\nCompared to a differential equation, a statistical model derived from the joint distribution of a set of (time-independent)\\nrandom variables is a rather superﬁcial description of a system.\\nIt exploits that some of the variables allow the\\nprediction of others as long as the experimental conditions do not change. If we drive a differential equation system\\nwith certain types of noise, or if we average over time, statistical dependences between components of x may emerge,\\nwhich can be exploited by machine learning. In contrast to the differential equation model, such a model does not\\nallow us to predict the effect of interventions; however, its strength is that it can often be learned from data.\\n\\n8I.e., they do not merely phenomenologically describe its time evolution without capturing the underlying mechanisms (e.g.,\\n\\ndue to unobserved confounding, or a form of coarse-graining that does not preserve the causal structure [124, 131]).\\n\\n14\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nIt aims to provide understanding and predict the effect of\\nCausal modeling lies in between these two extremes.\\ninterventions. Causal discovery and learning tries to arrive at such models in a data-driven way, using only weak\\nassumptions (see Table 2, from [112, 131]).\\n\\nWhile we may naively think that causality is always about time, most existing causal models do not (and need not)\\nconsider time. For instance, returning to our example of altitude and temperature, there is an underlying dynamical\\nphysical process that results in higher places tending to be colder. On the level of microscopic equations of motion\\nfor the involved particles, there is a temporal causal structure. However, when we talk about dependence or causality\\nbetween altitude and temperature, we need not worry about the details of this temporal structure; we are given a\\ndataset where time does not appear, and we can reason about how that dataset would look if we were to intervene on\\ntemperature or altitude.\\n\\nSome work exists trying to build bridges between these different levels of description. One can derive SCMs that\\ndescribe the interventional behavior of a coupled system that is in an equilibrium state and perturbed in an adiabatic\\nway [96], with generalizations to oscillatory systems [123]. In this work, an SCM arises as a high-level abstraction of\\nan underlying system of differential equations. It can only be derived if suitable high-level variables can be deﬁned\\n[124], which in practice may well be the exception rather than the rule.\\n\\n7 Causal Discovery\\n\\nSometimes, domain knowledge or the temporal ordering of events can help constrain the causal relationships between\\nvariables: e.g., we may know that certain attributes like age or sex are not caused by others; treatments inﬂuence health\\noutcomes; and events do not causally inﬂuence their past. When such domain knowledge is unavailable or incomplete,\\nwe need to perform causal discovery: infer which variables causally inﬂuence which others, i.e., learn the causal\\nstructure (e.g., a DAG) from data. Since experiments are often difﬁcult and expensive to perform while observational\\n(i.e., passively collected) data is abundant, causal discovery from observational data is of particular interest.\\n\\nAs discussed in § 3 in the context of the Common Cause Principle 3.2, the case where we have two variables is already\\ndifﬁcult since the same dependence can be explained by multiple different causal structures. One might thus wonder\\nif the case of more observables is completely hopeless. Surprisingly, this is not the case: the problem becomes easier\\n(in a certain sense) because there are nontrivial conditional independence properties [146, 25, 35] implied by a causal\\nstructure. We ﬁrst review two classical approaches to the multi-variate setting before returning to the two-variable case.\\n\\nConstraint-Based Methods Constraint-based approaches to causal discovery test which (conditional) indepen-\\ndences can be inferred from the data and then try to ﬁnd a graph which implies them. They are therefore also known\\nas independence-based methods. Such a procedure requires a way of linking properties of the data distribution p to\\nproperties of the underlying causal graph G. This link is known as the faithfulness assumption.\\n\\nAssumption 7.1 (Faithfulness). The only (conditional) independences satisﬁed by p are those implied by the causal\\nMarkov condition (Defn. 4.2).\\n\\nFaithfulness can be seen as the converse of the causal Markov condition. Together, they constitute a one-to-one\\ncorrespondence between graphical separation in G and conditional independence in p. While the causal Markov\\ncondition is satisﬁed by construction, faithfulness is an assumption which may be violated. A classical example for a\\nviolation of faithfulness is when causal effects along different paths cancel.\\n\\nExample 7.2 (Violation of faithfulness). Consider the SCM from Ex. 4.6 and let\\n\\nX1 := U1,\\n\\nX2 := αX1 + U2,\\n\\nX3 := βX1 + γX2 + U3\\n\\ni.i.d.∼ N(0, 1). By substitution, we obtain X3 = (β + αγ)X1 + γU2 + U3. Hence X3 ⊥⊥ X1 whenever\\nwith U1, U2, U3\\nβ + αγ = 0, even though this independence is not implied by the causal Markov condition over the induced causal\\ngraph G, see Fig. 5. Here, faithfulness is violated if the direct effect of X1 on X3 (β) and the indirect effect via X2\\n(αγ) cancel.\\n\\nApart from relying on faithfulness, a fundamental limitation to constraint-based methods is the fact that many different\\nDAGs may encode the same d-separation / independence relations. This is referred to as Markov equivalence and\\nillustrated in Fig. 7.\\n\\nDeﬁnition 7.3 (Markov equivalence). Two DAGs are said to be Markov equivalent if they encode the same d-\\nseparation statements. The set of all DAGs encoding the same d-separations is called a Markov equivalence class.\\n\\n15\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nX\\n\\nX\\n\\nY\\n\\nY\\n\\nZ\\n\\nZ\\n\\nY\\n\\nX\\n\\nZ\\n\\n(a) Chains\\n\\n(b) Fork\\n\\nX\\n\\nZ\\n\\nY\\n\\n(c) Collider\\n\\nFigure 7: Illustration of Markov equivalence using common graph motifs. The chains in (a) and the fork in (b) all\\nimply the relation X ⊥⊥ Z | Y (and no others). They thus form a Markov equivalence class, meaning they cannot be\\ndistinguished using conditional independence testing alone. The collider, or v-structure, in (c) implies X ⊥⊥ Z (but\\nX (cid:54)⊥⊥ Z | Y ) and forms its own Markov equivalence class, so it can be uniquely identiﬁed from observational data.\\nFor this reason, v-structures are helpful for causal discovery. It can be shown that two graphs are Markov equivalent\\niff. they share the same skeleton and v-structures.\\n\\nConstraint-based algorithms typically ﬁrst construct an undirected graph, or skeleton, which captures the (conditional)\\nindependences found by testing, and then direct as many edges as possible using Meek’s orientation rules [90]. The\\nﬁrst step carries most of the computational weight and various algorithms have been devised to solve it efﬁciently.\\n\\nThe simplest procedure is implemented in the IC [110] and SGS [145] algorithms. For each pair of variables (X, Y ),\\nthese search through all subsets W of the remaining variables to check whether X ⊥⊥ Y | W. If no such set W is\\nfound, then X and Y are connected with an edge. Since this can be slow due to the large number of subsets, the PC\\nalgorithm [145] uses a much more efﬁcient search procedure. It starts from a complete graph and then sequentially test\\nonly subsets of the neighbors of X or Y of increasing size, removing an edge when a separating subset is found. This\\nneighbor search is no longer guaranteed to give the right result for causally insufﬁcient systems, i.e., in the presence\\nof hidden confounders. The FCI (for Fast Causal Inference) algorithm [145] addresses this setting, and produces a\\npartially directed causal graph as output.\\n\\nApart from being limited to recovering a Markov equivalence class, constraint-based methods can suffer from statis-\\ntical issues. In practice, datasets are ﬁnite, and conditional independence testing is a notoriously difﬁcult problem,\\nespecially if conditioning sets are continuous and multi-dimensional. So while in principle, the conditional indepen-\\ndences implied by the causal Markov condition hold true irrespective of the complexity of the functions appearing\\nin an SCM, for ﬁnite datasets, conditional independence testing is hard without additional assumptions [136]. Re-\\ncent progress in (conditional) independence testing heavily relies on kernel function classes to represent probability\\ndistributions in reproducing kernel Hilbert spaces, see § 2.\\n\\nScore-Based Methods Score-based approaches to causal discovery assign a score to each graph G from a set of\\ncandidate graphs (usually the set of all DAGs). The score S is supposed to reﬂect how well G explains the observed\\ndata D = {x1, . . . , xm}, and we choose the graph ˆG maximizing this score,\\n\\nˆG = argmax\\n\\nS(G | D).\\n\\nG\\n\\nVarious score functions have been proposed, but most methods assume a parametric model which factorises according\\nto G, parametrised by θ ∈ Θ. Two common choices are multinomial models for discrete data [22] and linear Gaussian\\nmodels for continuous data [34]. E.g., a penalized maximum likelihood approach using the BIC [135] as a score yields\\n\\nSBIC(G | D) = log p(D | G, ˆθMLE) −\\n\\nlog m\\n\\n(7.1)\\n\\nk\\n2\\n\\nwhere k is the number of parameters and ˆθMLE is the maximum likelihood estimate for θ to D in G. Note that k\\ngenerally increases with the number of edges in G so that the second term in (7.1) penalizes complex graphs which do\\nnot lead to substantial improvements.\\n\\nAnother choice of score function is the marginal likelihood, or evidence, in a Bayesian approach to causal discovery,\\nwhich requires specifying prior distributions over graphs and parameters, p(G, θ) = p(G)p(θ | G). The score for G is\\nthen given by\\n\\nSBAYES(G | D) = p(D | G) =\\n\\np(D | G, θ)p(θ | G)d θ.\\n\\n(7.2)\\n\\nThis integral is intractable in general, but can be computed exactly for some models such as a Dirichlet-multinomial\\nunder some mild additional assumptions [47, 48].\\n\\n(cid:90)\\n\\nΘ\\n\\n16\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nA major drawback of score-based approaches is the combinatorial size of the search space. The number of DAGs\\nover n random variables grows super-exponentially and can be computed recursively (to account for acyclicity con-\\nstraints) [120]. E.g., the number of DAGs for n = 5 and n = 10 nodes is 29281 and 4175098976430598143,\\nrespectively. Finding the best scoring DAG is NP-hard [20]. To overcome this problem, greedy search techniques can\\nbe applied, e.g., greedy equivalence search (GES) [21] which optimizes for the BIC.\\n\\nIn recent years, another class of methods has emerged that is based on assuming particular functional forms for the\\nSCM assignments. Those arose from studying the cause-effect inference problem, as discussed below.\\n\\nCause-Effect Inference.\\nIn the case of only two variables, the ternary concept of conditional independences col-\\nlapses and the causal Markov condition (Defn. 4.2) thus has no nontrivial implications. However, we have seen in § 5\\nthat assuming an independence of mechanisms (Principle 5.1) lets us ﬁnd asymmetries between cause and effect, and\\nthus address the cause-effect inference problem previously considered unsolvable [64]. It turns out that this problem\\ncan be also addressed by making additional assumptions on function classes, as not only the graph topology leaves\\na footprint in the observational distribution, but so do the functions fi in an SCM. Such assumptions are typical for\\nmachine learning, where it is well-known that ﬁnite-sample generalization without assumptions on function classes is\\nimpossible, and where much attention is devoted to properties of function classes (e.g., priors or capacity measures),\\nas discussed in § 2.\\n\\nLet us provide an intuition as to why assumptions on the functions in an SCM should help learn about them from data.\\nConsider a toy SCM with only two observables X → Y . In this case, the structural equations (4.5) turn into\\n\\nX := U,\\n\\nY := f (X, V )\\n\\n(7.3)\\n\\n(7.4)\\n\\nwith noises U ⊥⊥ V . Now think of V acting as a random selector variable choosing from among a set of functions\\nF = {fv(x) ≡ f (x, v) | v ∈ supp(V )}. If f (x, v) depends on v in a non-smooth way, it should be hard to glean\\ninformation about the SCM from a ﬁnite dataset, given that V is not observed and it randomly switches between\\narbitrarily different fv.9 This motivates restricting the complexity with which f depends on V . A natural restriction is\\nto assume an additive noise model\\n\\nX := U,\\n\\nY := f (X) + V.\\n\\nIf f in (7.3) depends smoothly on V , and if V is relatively well concentrated, this can be motivated by a local Taylor\\nexpansion argument. Such assumptions drastically reduce the effective size of the function class—without them, the\\nlatter could depend exponentially on the cardinality of the support of V .\\n\\nRestrictions of function classes can break the symmetry between cause and effect in the two-variable case: one can\\nshow that given a distribution over X, Y generated by an additive noise model, one cannot ﬁt an additive noise model\\nin the opposite direction (i.e., with the roles of X and Y interchanged) [53, 95, 114, 76, 6]. This is subject to certain\\ngenericity assumptions, and notable exceptions include the case where U, V are Gaussian and f is linear. It general-\\nizes results of [140] for linear functions, and it can be generalized to include nonlinear rescaling [164], cycles [94],\\nconfounders [65], and multi-variable causal discovery [113]. There is now a range of methods that can detect causal\\ndirection better than chance [97].\\n\\nWe have thus gathered some evidence that ideas from machine learning can help tackle causality problems that were\\npreviously considered hard. Equally intriguing, however, is the opposite direction: can causality help us improve\\nmachine learning?\\n\\nNonstationarity-Based Methods The last family of causal discovery approaches we mention is based on ideas of\\nnonstationarity and invariance [130]. These approaches do not apply to purely observational data collected in an\\ni.i.d. setting. In contrast, they aim to leverage heterogeneity of data collected from different environments. The main\\nidea is the following: since causal systems are modular in the sense of the ICM Principle 5.1, changing one of the\\nindependent mechanisms should leave the other components, or causal conditionals, unaffected (SMS Principle 5.2).\\nA correct factorization of the joint distribution according to the underlying causal structure should thus be able to\\nexplain heterogeneity by localized changes in one (or few) of the mechanisms while the others remain invariant.\\n\\nOne of the ﬁrst works to use this idea [151] analyzed which causal structures can be distinguished given data resulting\\nfrom a set of mechanism changes. Recent work [54] additionally aims to learn a low-dimensional representation of\\nthe mechanism changes. Other works [111, 121] have proposed methods for ﬁnding the direct causes of a given target\\nvariable. Using a recent result on identiﬁability of non-linear ICA [59] which also relies on non-stationarity, a method\\n\\n9Suppose X and Y are binary, and U, V are uniform Bernoulli variables, the latter selecting from F = {id, not} (i.e., identity\\nand negation). In this case, the entailed distribution for Y is uniform, independent of X, even though we have X → Y . We would\\nbe unable to discern X → Y from data. (This would also constitute a violation of faithfulness (Assumption 7.1)).\\n\\n17\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nfor learning general non-linear SCMs was proposed [93]. Here the idea is to train a classiﬁer to discriminate between\\nthe true value of some nonstationarity variable (such as a time-stamp or environment indicator) and a shufﬂed version\\nthereof.\\n\\n8\\n\\nImplications for Machine Learning\\n\\nSemi-Supervised Learning Suppose our underlying causal graph is X → Y , and we wish to learn a mapping\\nX → Y . The causal factorization (4.1) in this case is\\n\\np(X, Y ) = p(X) p(Y | X).\\n\\n(8.1)\\n\\nThe ICM Principle 5.1 posits that the modules in a joint distribution’s causal factorization do not inform or inﬂuence\\neach other. This means that, in particular, p(X) should contain no information about p(Y | X), which implies that\\nsemi-supervised learning [17] should be futile, as it is trying to use additional information about p(X) (from unlabeled\\ndata) to improve our estimate of p(Y | X = x). How about the opposite direction? Is there hope that semi-supervised\\nlearning should be possible in that case? It turns out the answer is yes, due to work on cause-effect inference using\\nthe ICM Principle 5.1 [24]. It introduced a measure of dependence between the input and the conditional of output\\ngiven input, and showed that if this dependence is zero in the causal direction, then it is strictly positive in the opposite\\ndirection. Independence of cause and mechanism in the causal direction thus implies that in the backward direction\\n(i.e., for anticausal learning), the distribution of the input variable should contain information about the conditional\\nof output given input, i.e., the quantity that machine learning is usually concerned with. This is exactly the kind of\\ninformation that semi-supervised learning requires when trying to improve the estimate of output given input by using\\nunlabeled inputs. This suggests that semi-supervised learning should be impossible for causal learning problems, but\\nfeasible otherwise, in particular for anticausal ones. A meta-analysis of published semi-supervised learning benchmark\\nstudies corroborated this prediction [130], and similar results apply for natural language processing [69]. These\\nﬁndings are intriguing since they provide insight into physical properties of learning problems, thus going beyond the\\nmethods and applications that machine learning studies usually provide.\\n\\nSubsequent developments include further theoretical analyses [67, 112] and a form of conditional semi-supervised\\nlearning [158]. The view of semi-supervised learning as exploiting dependences between a marginal p(x) and a\\nnon-causal conditional p(y | x) is consistent with the common assumptions employed to justify semi-supervised\\nlearning [17, 126].\\n\\nInvariance and Robustness We have discussed the shortcomings of the i.i.d. assumption, which rarely holds true\\nexactly in practice, and the fact that real-world intelligent agents need to be able to generalize not just within a single\\ni.i.d. setting, but across related problems. This notion has been termed out-of-distribution (o.o.d.) generalization,\\nattracting signiﬁcant attention in recent years [131]. While most work so far has been empirical, statistical bounds\\nwould be desirable that generalize (2.4), including additional quantities measuring the distance between training and\\ntest distribution, incorporating meaningful assumptions [138]. Such assumptions are necessary [8], and could be\\ncausal, or related to invariance properties.\\n\\nThe recent phenomenon of “adversarial vulnerability” [149] shows that minuscule targeted violations of the i.i.d.\\nassumption, generated by adding suitably chosen noise to images (imperceptible to humans), can lead to dangerous\\nerrors such as confusion of trafﬁc signs. These examples are compelling as they showcase non-robustnesses of\\nartiﬁcial systems which are not shared by human perception. Our own perception thus exhibits invariance or\\nrobustness properties that are not easily learned from a single training set.\\n\\nEarly causal work related to domain shift [130] looked at the problem of learning from multiple cause-effect datasets\\nthat share a functional mechanism but differ in noise distributions. More generally, given (data from) multiple distri-\\nbutions, one can try to identify components which are robust, and ﬁnd means to transfer them across problems [166, 4,\\n163, 36, 55]. According to the ICM Principle 5.1, invariance of conditionals or functions (also referred to as covariate\\nshift in simple settings) should only hold in the causal direction, a reversal of the impossibility described for SSL.\\n\\nBuilding on the work of [130, 111], the idea of invariance for prediction has also been used for supervised learning\\n[121, 3, 87]. In particular, “invariant risk minimization” (IRM) was proposed as an alternative to ERM, cf. (2.3).\\n\\n9 Causal Reasoning\\n\\nIn contrast to causal discovery (§ 7), which aims to uncover the causal structure underlying a set of variables, causal\\nreasoning starts from a known (or postulated) causal graph and answers causal queries of interest. While causal\\ndiscovery often looks for qualitative relationships, causal reasoning usually aims to quantify them. This requires\\n\\n18\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 8: Left: Covid-19 case fatality rates (CFRs) in Italy and China by age and in aggregate (“Total”), including\\nall conﬁrmed cases and fatalities up to the time of reporting in early 2020 (see legend): for all age groups, CFRs in\\nItaly are lower than in China, but the total CFR in Italy is higher, an example of Simpson’s paradox. Right: The case\\ndemographic differs between countries: in Italy, most cases occurred in the older population (ﬁgure from [154]).\\n\\ntwo steps: (i) identifying the query, i.e., deriving an estimand for it that only involves observed quantitites; and (ii)\\nestimating this using data. Often, the quantities of interest can be described as treatment effects, i.e., contrasts between\\ntwo interventions.\\nDeﬁnition 9.1 (Treatment effects). The conditional average treatment effect (CATE), conditioned on (a subset of)\\nfeatures x, is deﬁned as\\n\\nτ (x) := E[Y | x, do(T = 1)] − E[Y | x, do(T = 0)] = E[Y (1) − Y (0) | x].\\n\\nThe average treatment effect (ATE) is deﬁned as the population average of the CATE,\\n\\nτ := E[τ (X)] = E[Y | do(T = 1)] − E[Y | do(T = 0)] = E[Y (1) − Y (0)].\\n\\n(9.1)\\n\\n(9.2)\\n\\nWhile ITE (Defn. 4.12) and CATE (9.1) are sometimes used interchangeably, there is a conceptual difference: ITE\\nrefers to the difference of two POs and is thus bound to an individual, while CATE applies to subpopulations, e.g.,\\nthe CATE for females in their 40s. Since the ITE is fundamentally impossible to observe, it is often estimated by the\\nCATE conditional on an individual’s features xi using suitable additional assumptions.\\n\\nAs is clear from Defn. 9.1, the treatment effects we want to estimate involve interventional expressions. However, we\\nusually only have access to observational data. Causal reasoning can thus be cast as answering interventional queries\\nusing observational data and a causal model. This involves dealing with confounders, both observed and unobserved.\\n\\nBefore discussing how to identify and estimate causal effects, we illustrate why causal assumptions are necessary\\nusing a well-known statistical phenomenon.\\n\\nSimpson’s Paradox and Covid-19 Simpson’s paradox refers to the observation that aggregating data across sub-\\npopulations may yield opposite trends (and thus lead to reversed conclusions) from considering subpopulations sep-\\narately [143]. We observed a textbook example of this during the Covid-19 pandemic by comparing case fatality\\nrates (CFRs), i.e., the proportion of conﬁrmed Covid-19 cases which end in fatality, across different countries and age\\ngroups as illustrated in Fig. 8 [154]: for all age groups, CFRs in Italy are lower than in China, but the total CFR in\\nItaly is higher.\\n\\nHow can such a pattern be explained? The case demographic (see Fig. 8, right) is rather different across the two coun-\\ntries, i.e., there is a statistical association between country and age. In particular, Italy recorded a much larger pro-\\nportion of cases in older patients who are generally at higher risk of dying from Covid-19 (see Fig. 8, left). While\\nthis provides a consistent explanation in a statistical sense, the phenomenon may still seem puzzling as it deﬁes our\\ncausal intuition. Humans appear to naturally extrapolate conditional probabilities to read them as causal effects, which\\ncan lead to inconsistent conclusions and may leave one wondering: how can the disease in Italy be less fatal for the\\nyoung, less fatal for the old, but more fatal for the people overall? It is for this reason that the reversal of (conditional)\\nprobabilities in Fig. 8 is perceived as and referred to as a “paradox” [106, 49].\\n\\nIf we consider the country as treatment whose causal effect on fatality is of interest, then causal assumptions (e.g., in\\nthe form of a causal graph) are needed to decide how to handle covariates such as age that are statistically associated\\n\\n19\\n\\n0-910-1920-2930-3940-4950-5960-6970-7980+TotalAge02468101214%Case fatality rates (CFRs) by age groupChina, 17 FebruaryItaly, 9 March0-910-1920-2930-3940-4950-5960-6970-7980+Age05101520%Proportion of confirmed cases by age groupChina, 17 FebruaryItaly, 9 March\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 9: Treatment effect estimation with three observed covariates\\nX1, X2, X3: here, the valid adjustment sets for T → Y (see Prop. 9.3)\\nare {X1}, {X2}, and {X1, X2}. Including X3 opens the non-directed\\npath T → X3 ← X2 → Y and lies on the directed path T → X3 → Y ,\\nboth of which can introduce bias.\\n\\nX1\\n\\nX2\\n\\nT\\n\\nY\\n\\nX3\\n\\nwith the treatment, e.g., whether to stratify by (i.e., adjust for) age or not. This also explains why randomized controlled\\ntrials (RCTs) [31] are the gold standard for causal reasoning: randomizing the assignment breaks any potential links\\nbetween the treatment variable and other covariates, thus eliminating potential problems of bias. However, RCTs are\\ncostly and sometimes unethical to perform, so that causal reasoning often relies on observational data only.10\\n\\nWe ﬁrst consider the simplest setting without hidden confounders and with overlap. We start with identiﬁcation of\\ntreatment effects on the population level, and then discuss different techniques for estimating these from data.\\n\\nIdentiﬁcation In absence of unmeasured variables (i.e., without hidden confounding), and provided we know the\\ncausal graph, it is straight-forward to compute causal effects by adjusting for covariates. A principled approach to\\ndo so for any given graph was proposed by Robins [118] and is known as the g-computation formula (where the g\\nstands for general). It is also known as truncated factorisation [105] or manipulation theorem [145]. It relies on the\\nindependence of causal mechanisms (Principle 5.1), i.e., the fact that intervening on a variable leaves the other causal\\nconditionals in (4.1) unaffected:\\n\\np(X1, . . . , Xn | do(Xi = xi)) = δ(Xi = xi)\\n\\np(Xj | PAj)\\n\\n(9.3)\\n\\n(cid:89)\\n\\nj(cid:54)=i\\n\\nFrom (9.3) the interventional distribution of interest can then be obtained by marginalization. This is related to the idea\\nof graph surgery (see Fig. 5), and leads to a set of three inference rules for manipulating interventional distributions\\nknown as do-calculus [105] that have been shown to be complete for identifying causal effects [56, 141].\\n\\nNote that covariate adjustment may be needed even if there are no clear confounders directly inﬂuencing both treatment\\nand outcome, as shown by the example in Fig. 9.\\nExample 9.2. Applying the g-computation formula (9.3) to the setting of Fig. 9, we obtain\\n\\np(y | do(t)) =\\n\\np(x1)\\n\\np(x2 | x1)\\n\\np(x3 | t, x2)p(y | t, x2, x3)\\n\\n(cid:88)\\n\\nx2\\n(cid:88)\\n\\nx2\\n\\n(cid:88)\\n\\nx1\\n(cid:88)\\n\\n=\\n\\n(a)\\n=\\n\\nx1\\n(cid:88)\\n\\nx1,x2\\n\\n(cid:88)\\n\\nx3\\n\\n(cid:88)\\n\\nx2\\n\\n(cid:88)\\n\\n(b)\\n=\\n\\nx1\\n\\np(x1, x2)p(y | t, x1, x2)\\n\\np(x1)p(y | t, x1)\\n\\np(x1)\\n\\np(x2 | x1)p(y | t, x2) =\\n\\np(x2)p(y | t, x2)\\n\\nwhere the last line follows by using the following conditional independences implied by the graph: (a) Y ⊥⊥ X1 |\\n{T, X2}, and (b) X2 ⊥⊥ T | X1.\\n\\nNote that both the RHS in (9.5) and both sides in (9.6) take the form\\n\\np(y | do(t)) =\\n\\np(z)p(y|t, z).\\n\\n(cid:88)\\n\\nz\\nIn this case we call Z a valid adjustment set for the effect of T on Y . Here, {X1}, {X2}, and {X1, X2} are all\\nvalid adjustment sets, but it can be shown that, e.g., {X1, X3} is not (see Fig. 9). As computing the g-formula with\\nmany covariates can be cumbersome, graphical criteria for which subsets constitute valid adjustment sets are useful\\nin practice, even in the absence of unobserved confounders.\\nProposition 9.3 ([142]). Under causal sufﬁciency, a set Z is a valid adjustment set for the causal effect of a singleton\\ntreatment T on an outcome Y (in the sense of (9.7)) if and only if the following two conditions hold: (i) Z contains no\\ndescendant of any node on a directed path from T to Y (except for descendants of T which are not on a directed path\\nfrom T to Y ); and (ii) Z blocks all non-directed paths from T to Y .\\n\\n10For a treatment of more general types of data fusion and transportability of experimental ﬁndings across different populations\\n\\nwe refer to [107, 5].\\n\\n20\\n\\n(9.4)\\n\\n(9.5)\\n\\n(9.6)\\n\\n(9.7)\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nHere, a path is called directed if all directed edges on it point in the same direction, and non-directed otherwise. A\\npath is blocked (by a set of vertices Z) if it contains a triple of consecutive nodes connected in one of the following\\nthree ways: A → B → C with B ∈ Z, A ← B → C with B ∈ Z, or A → B ← C, where neither B nor any\\ndescendant of B is in Z.\\n\\nTwo well-known types of adjustment set implied by Prop. 9.3 are parent adjustment, where Z = PaT ; and the\\nbackdoor criterion, where Z is constrained to contain no descendants of T and to block all “back-door paths” from T\\nto Y (T ← ... Y ).\\n\\nNote that Prop. 9.3 only holds singleton treatments (i.e., interventions on a single variable). For treatments T involving\\nmultiple variables, a slightly more complicated version of Prop. 9.3 can be given in terms of proper causal paths, and\\nwe refer to [103, 109] for details.\\n\\nLet us brieﬂy return to our earlier example of Simpson’s paradox and Covid-19. Considering a plausible causal graph\\nfor this setting [154], we ﬁnd that age A acts as a mediator C → A → F of the causal effect of country C on fatality F\\n(there is likely also a direct effect C → F , potentially mediated by other, unobserved variables). If we are interested\\nin the (total) causal effect of C on F (i.e., the overall inﬂuence of country on fatality), A should not be included for\\nadjustment according to Prop. 9.3, and, subject to causal sufﬁciency, the total CFRs can be interpreted causally.11 For\\nanother classic example of Simpson’s paradox in the context of kidney stone treatment [18], on the other hand, the\\nsize of the stone acts as a confounder and thus needs to be adjusted for to obtain sound causal conclusions.\\n\\nValid covariate adjustment and the g-formula tell us how to compute interventions from the observational distribution\\nwhen there are no hidden confounders. To actually identify causal effects from data, however, we need to also be\\nable to estimate the involved quantities in (9.7). This is a problem if a subgroup of the population never (or always)\\nreceives a certain treatment. We thus need the additional assumption of a non-zero probability of receiving each\\npossible treatment, referred to as overlap, or common support.\\nAssumption 9.4 (Overlap/common treatment support). For any treatment t and any conﬁguration of features x, it\\nholds that: 0 < p(T = t | X = x) < 1.\\n\\nThe combination of overlap and ignorability (i.e., no hidden confounders—see Assumption 4.15) is also referred to\\nas strong ignorability and is a sufﬁcient condition for identifying ATE and CATE: the absence of hidden confounders\\nguarantees the existence of a valid adjustment set Z ⊆ X for which p(Y | do(T = t), Z) = p(Y | T = t, Z), and\\noverlap guarantees that we can actually estimate the latter term for any z occurring with non-zero probability.12\\n\\nRegression Adjustment Having identiﬁed a valid adjustment set (using Prop. 9.3), regression adjustment works by\\nﬁtting a regression function ˆf to E[Y | Z = z, T = t] = f (z, t) using an observational sample {(yi, ti, zi)}m\\ni=1. We\\ncan then use ˆf to impute counterfactual outcomes as ˆyCF\\ni = ˆf (zi, 1 − ti) in order to estimate the CATE. The ATE is\\nthen given by the population average and can be estimated as\\n\\nˆτregression-adj. =\\n\\n(cid:88)\\n\\n(cid:0)yi − ˆf (zi, 0)(cid:1) +\\n\\n(cid:88)\\n\\n(cid:0) ˆf (zi, 1) − yi\\n\\n(cid:1),\\n\\n(9.8)\\n\\n1\\nm1\\n\\ni : ti=1\\n\\n1\\nm0\\n\\ni : ti=0\\n\\nwhere m1 and m0 are the number of observations from the treatment and control groups, respectively. Note the\\ndifference to the RCT estimator where no adjustment is necessary,\\n1\\nm0\\n\\nˆτRCT =\\n\\n1\\nm1\\n\\nyi −\\n\\n(9.9)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\nyi.\\n\\ni : ti=1\\n\\ni : ti=0\\n\\nMatching and Weighting Approaches While regression adjustment indirectly estimates ATE via CATE, matching\\nand weighting approaches aim to estimate ATE directly. The general is idea to emulate the conditions of an RCT as\\nwell as possible.\\n\\nMatching approaches work by splitting the population into subgroups based on feature similarity. This can be done on\\nan individual level (so-called one-to-one or nearest neighbor matching) by matching each individual i with the most\\nsimilar one, j(i), from the opposite treatment group (i.e., ti (cid:54)= tj(i)). The difference of their outcomes, yi − yj(i), is\\nthen considered as a sample of the ATE, and their average taken as an estimate thereof,\\n1\\nm1\\n\\n(yi − yj(i)) +\\n\\nˆτNN-matching =\\n\\n(yj(i) − yi).\\n\\n1\\nm0\\n\\n(9.10)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\ni : ti=1\\n\\ni : ti=0\\n\\n11Mediation analysis [104] provides tools to tease apart and quantify the direct and indirect effects; the age-speciﬁc CFRs in Fig. 8\\n\\nthen correspond to controlled direct effects [154].\\n\\n12The overlap assumption can thus be relaxed to hold for at least one valid adjustment set.\\n\\n21\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nH\\n\\nM\\n\\nT\\n\\nY\\n\\nI\\n\\nT\\n\\nY\\n\\n(a) Front-door\\n\\n(b) IV\\n\\n(c) RDD\\n\\nH\\n\\nS\\n\\nT\\n\\nH\\n\\nY\\n\\nFigure 10: Overview of special settings which allow to estimate causal effects of treatment T on outcome Y when the\\nstrong ignorability assumption (no hidden confounding, and overlap) does not hold. In (a) the hidden confounder H is\\ndealt with by means of an observed mediator M , while (b) relies on an instrumental variable (IV) which is independent\\nof H. (c) In a regression discontinuity design (RDD), treatment assignment is a threshold function of some observed\\ndecision score S so that there is no overlap between treatment groups.\\n\\nAlternatively, the population can be split into larger subgroups with similar features (so-called strata). Each stratum is\\nthen treated as an independent RCT. If there are K strata containing m1, ..., mK observations each, the stratiﬁed ATE\\nestimator is\\n\\nˆτstratiﬁed =\\n\\n(cid:80)K\\n\\nk=1 mk ˆτ (k)\\n(cid:80)K\\nk=1 mk\\n\\nRCT\\n\\n(9.11)\\n\\n(9.12)\\n\\nRCT is the estimator from (9.9) applied to observation in the kth stratum.\\n\\nwhere ˆτ (k)\\nWeighting approaches, on the other hand, aim to counteract the confounding bias by reweighting each observation\\nto make the population more representative of an RCT. This means that underrepresented treatment groups are up-\\nweighted and overrepresented ones downweighted. An example is the inverse probability weighting (IPW) estimator,\\n\\nˆτIPW =\\n\\n1\\nm1\\n\\n(cid:88)\\n\\ni : ti=1\\n\\nyi\\np(T = 1 | Z = zi)\\n\\n−\\n\\n1\\nm0\\n\\n(cid:88)\\n\\ni : ti=0\\n\\nyi\\np(T = 0 | Z = zi)\\n\\n.\\n\\nThe treatment probability p(T = 1 | Z) is also known as propensity score. While from a theoretical point of view Z\\nshould be a valid adjustment set, practitioners sometimes use all covariates to construct a propensity score.\\n\\nPropensity Score-Methods To overcome the curse of dimensionality and gain statistical efﬁciency in high-\\ndimensional, low-data regimes, propensity scores can be a useful tool, because covariates and treatment are rendered\\nconditionally independent, T ⊥⊥ Z | s(z), by the propensity score s(z) := p(T = 1 | Z = z) [122]. Instead of\\nadjusting for large feature sets or performing matching in high-dimensional spaces, the scalar propensity score can be\\nused instead. Applying this idea to the above methods gives rise to propensity score adjustment and propensity score\\nmatching. For the latter, the difference in propensity scores is used as similarity between instances to ﬁnd nearest\\nneighbors or to deﬁne strata.\\n\\nWhile simplifying in one respect, the propensity score needs to be estimated from data which is an additional source\\nof error. The standard approach for this is to estimate s(z) by logistic regression, but more sophisticated methods\\nare also possible. However, propensity score methods still rely on having identiﬁed a valid adjustment set Z to give\\nunbiased results. Using all covariates to estimate s, without checking for validity as an adjustment set, can thus lead\\nto wrong results.\\n\\nNext, we consider the case of causal reasoning with unobserved confounders. While it is not possible to identify causal\\neffects in the general case, we will discuss two particular situations in which ATE can still be estimated. These are\\nshown in Fig. 10a and b.\\n\\nFront-Door Adjustment The ﬁrst situation in which identiﬁcation is possible even though a hidden variable H\\nconfounds the effect between treatment and outcome is known as front-door adjustment. The corresponding causal\\ngraph is shown in Fig. 10a. Front-door adjustment relies on the existence of an observed variable M which blocks\\nall directed paths from T to Y , so that T only causally inﬂuences Y through M . For this reason M is also called a\\nmediator. The other important assumption is that the hidden confounder does not inﬂuence the mediator other than\\nthrough the treatment T , i.e., M ⊥⊥ H | T . In this case, and provided p(t, m) > 0 for all t and m, the causal effect of\\nT on Y is identiﬁable and is given by the following.\\nProposition 9.5 (Front-door adjustment). For the causal graph in Fig. 10a it holds that:\\n\\np(y | do(t)) =\\n\\np(m | t)\\n\\np(t(cid:48))p(y | m, t(cid:48)).\\n\\n(cid:88)\\n\\nm\\n\\n(cid:88)\\n\\nt(cid:48)\\n\\n(9.13)\\n\\n22\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n(cid:88)\\n\\nm\\n\\n(cid:88)\\n\\nt(cid:48)\\n\\nWe give a sketch of the derivation, and refer to [105] for a proof using the rules of do-calculus. Since M mediates the\\ncausal effect of T on Y , we have that\\n\\np(y | do(t)) =\\n\\np(m | do(t))p(y | do(m)).\\n\\n(9.14)\\n\\nSince there are no backdoor paths from T to M we have p(m | do(t)) = p(m | t).\\n\\nMoreover, {T } is a valid adjustment set for the effect of M on Y by Prop. 9.3, so\\n\\np(y | do(m)) =\\n\\np(t(cid:48))p(y | m, t(cid:48)).\\n\\n(9.15)\\n\\nSubstituting into (9.14) then yields expression (9.13).\\n\\nWe point out that the setting presented here is only the simplest form of front-door adjustment which is sufﬁcient to\\nconvey the main idea. It can be amended to include observed covariates X as well, as long as the conditions on the\\nmediator remain satisﬁed.\\n\\nInstrumental Variables (IV) The second setting for causal reasoning with hidden confounders is based on the idea\\nof instrumental variables [2, 29, 160], see Fig. 10b. The IV approach relies on the existence of a special observed\\nvariable I called instrument.\\nDeﬁnition 9.6 (IV). A variable I is a valid instrument for estimating the effect of treatment T on outcome Y con-\\nfounded by a hidden variable H if all of the following three conditions hold: (i) I ⊥⊥ H; (ii) I (cid:54)⊥⊥ T ; and (iii)\\nI ⊥⊥ Y | T .\\n\\nCondition (i) states that the instrument is independent of any hidden confounders H. Since this assumption cannot be\\ntested, background knowledge is necessary to justify the use of a variable as IV in practice. Conditions (ii) and (iii)\\nstate that the instrument is correlated with treatment, and only affects the outcome through T , and are referred to as\\nrelevance and exclusion restriction, respectively.\\nGiven a valid IV, we apply a two-stage procedure: ﬁrst obtain an estimate ˆT of the treatment variable T that is\\nindependent of H by predicting T from I. Having thus created an unconfounded version of the treatment, a regression\\nof Y on ˆT then reveals the correct causal effect. We demonstrate this idea for a simple linear model with continuous\\ntreatment variable where the causal effect can be obtained by two-stage least squares (2SLS).\\nExample 9.7 (Linear IV with 2SLS). Consider the linear SCM deﬁned by\\n\\nT := aI + bH + UT ,\\n\\nY := cH + dT + UY .\\n\\nwith UT , UY independent noise terms. Then, since I ⊥⊥ H, linear regression of T on I recovers the coefﬁcient a via\\nˆT = aI. Substituting for T in the structural equation for Y gives\\n\\nY := daI + (c + bd)H + UY + dUT .\\nA second linear regression of Y on ˆT = aI recovers the causal effect d because (I ⊥⊥ H) =⇒ ( ˆT ⊥⊥ H), whereas a\\nnaive regression of Y on T would give a different result, as T (cid:54)⊥⊥ H.\\n\\nIVs have been studied extensively and more sophisticated versions than the simple example above exist, allowing for\\nnon-linear interactions and observed covariates.\\n\\nHaving discussed some special settings to deal with hidden confounding, we brieﬂy present a technique to deal with\\nviolations of the overlap assumption.\\n\\nRegression Discontinuity Design In a regression discontinuity design (RDD) the treatment assignment mechanism\\nbehaves like a threshold function, i.e., the propensity score is discontinuous [60]. In the simplest setting, the assign-\\nment of treatment or control is determined by whether an observed score S is above a threshold s0, T := I{S ≥ s0}.\\nThis score in turn depends on other covariates which may or may not be observed. For example, patients may be\\nassigned a risk score, and treatment is only prescribed if this score surpasses a given threshold. Since the score may\\nbe assigned by another institution, not all relevant covariates H are usually observed. However, it is assumed that\\nthe treatment decision only depends on the score, e.g., because doctors comply with the ofﬁcial rules. The causal\\ngraph for such a simple RDD setting is shown in Fig. 10c. While the score S constitutes a valid adjustment set in\\nprinciple, the problem with RDDs is the lack of overlap: patients with low scores are always assigned T = 0 and\\npatients with high scores are always assigned T = 1. Because of this, covariate adjustment, matching, or weighting\\n\\n23\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\napproaches do not apply. The general idea of an RDD is to overcome this challenge by comparing observations with\\nscore in a small neighborhood of the decision cut-off value s0, motivated by the consideration that patients with close\\nscores but on opposite sides of s0 differ only in whether they received the treatment or not. For example, if the treat-\\nment cut-off value is 0.5 for a score in [0,1], then patients with scores of 0.49 and 0.51 are comparable and can be\\ntreated as samples from an RCT. An RDD (in its simplest form) thus focuses on differences in the regression function\\nE[Y | S = s, T = t(s)] = f (s) for s ∈ [s0 − (cid:15), s0 + (cid:15)], where (cid:15) > 0 is small.\\n\\nHalf-Sibling Regression and Exoplanet Detection We conclude this section with a real-world application perform-\\ning causal reasoning in a confounded additive noise model. Launched in 2009, NASA’s Kepler space telescope initially\\nobserved 150000 stars over four years, in search of exoplanet transits. These are events where a planet partially oc-\\ncludes its host star, causing a slight decrease in brightness, often orders of magnitude smaller than the inﬂuence of\\ntelescope errors. When looking at stellar light curves, we noticed that the noise structure was often shared across\\nstars that were light years apart. Since that made direct interaction of the stars impossible, it was clear that the shared\\ninformation was due to the telescope acting as a confounder. We thus devised a method that (a) regresses a given star\\nof interest on a large set of other stars chosen such that their measurements contain no information about the star’s\\nastrophysical signal, and (b) removes that regression in order to cancel the telescope’s inﬂuence.13 The method is\\ncalled “half-sibling” regression since target and predictors share a parent, namely the telescope. The method recovers\\nthe random variable representing the astrophysical signal almost surely (up to a constant offset), for an additive noise\\nmodel (speciﬁcally, the observed light curve is a sum of the unknown astrophysical signal and an unknown function\\nof the telescope noise), subject to the assumption that the telescope’s effect on the star is in principle predictable from\\nthe other stars [128].\\n\\nIn 2013, the Kepler spacecraft suffered a technical failure, which left it with only two functioning reaction wheels,\\ninsufﬁcient for the precise spatial orientation required by the original Kepler mission. NASA decided to use the\\nremaining fuel to make further observations, however the systematic error was signiﬁcantly larger than before—a\\ngodsend for our method designed to remove exactly these errors. We augmented it with models of exoplanet tran-\\nsits and an efﬁcient way to search light curves, leading to the discovery of 36 planet candidates [32], of which 21\\nwere subsequently validated as bona ﬁde exoplanets [92]. Four years later, astronomers found traces of water in the\\natmosphere of the exoplanet K2-18b—the ﬁrst such discovery for an exoplanet in the habitable zone, i.e., allowing\\nfor liquid water [10, 152]. The planet turned out to be one that had been ﬁrst detected in our work [32] (exoplanet\\ncandidate EPIC 201912552).\\n\\n10 Current Research and Open Problems\\n\\nConservation of Information We have previously argued that the mechanization of information processing plays\\ncurrently plays a similar role to the mechanization of energy processing in earlier industrial revolutions [126]. Our\\npresent understanding of information is rather incomplete, as was the understanding of energy during the course of\\nthe ﬁrst two industrial revolutions. The profound modern understanding of energy came with Emmy Noether and the\\ninsight that energy conservation is due to a symmetry (or covariance) of the fundamental laws of physics: they look\\nthe same no matter how we shift time. One might argue that information, suitably conceptualized, should also be a\\nconserved quantity, and that this might also be a consequence of symmetries. The notions of invariance/independence\\ndiscussed above may be able to play a role in this respect.\\n\\nMass seemingly played two fundamentally different roles (inertia and gravitation) until Einstein furnished a deeper\\nconnection in general relativity. It is noteworthy that causality introduces a layer of complexity underlying the symmet-\\nric notion of statistical mutual information. Discussing source coding and channel coding, Shannon [139] remarked:\\nThis duality can be pursued further and is related to a duality between past and future and the notions of control and\\nknowledge. Thus we may have knowledge of the past but cannot control it; we may control the future but have no\\nknowledge of it.\\n\\nWhat is an Object? Following the i.i.d. pattern recognition paradigm, machine learning learns objects by extracting\\npatterns from many observations. An complementary view may consider objects as modules that can be separately\\nmanipulated or intervened upon [150]. The idea that objects are deﬁned by their behavior under transformation has\\nbeen inﬂuential in ﬁelds ranging from psychology to mathematics [74, 88].\\n\\nCausal Representation Learning.\\nIn hindsight, it appears somewhat naive that ﬁrst attempts to build AI tried\\nto realize intelligence by programs written by humans, since existing examples of intelligent systems appear much\\n\\n13For events that are localized in time (such as exoplanet transits), we further argued that the same applies for suitably chosen\\n\\npast and future values of the star itself, which can thus also be used as predictors.\\n\\n24\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nClassic AI:\\nsymbols provided a priori;\\nrules provided a priori.\\n\\nMachine Learning:\\nrepresentations (symbols) learned from data;\\nonly include statistical information.\\n\\nCausal Modeling:\\nstructural causal models assume the causal vari-\\nables (symbols) are given.\\n\\nCausal Representation Learning:\\ncapture interventions, reasoning, planning— “Thinking is act-\\ning in an imagined space” (Konrad Lorenz)\\n\\nFigure 11: Causal representation learning aims to automatically learn representations that contain not just statistical\\ninformation, but support interventions, reasoning, and planning. The long-term goal of this ﬁeld is to learn causal\\nworld models supporting AI, or causal digital twins of complex systems.\\n\\ntoo complex for that. However, there is a second problem, which is just as signiﬁcant: classic AI assumed that the\\nsymbols which were the basis of algorithms were provided a priori by humans. When building a chess program, it is\\nclear that the algorithms operate on chess board positions and chess pieces; however, if we want to solve a real-world\\nproblem in an unstructured environment (e.g., recognize spoken language), it is not clear what constitutes the basic\\nsymbols to be processed.\\n\\nTraditional causal discovery and reasoning assumed that the elementary units are random variables connected by a\\ncausal graph. Real-world observations, however, are usually not structured into such units to begin with. For instance,\\nobjects in images that permit causal reasoning ﬁrst need to be discovered [85, 157, 150, 84]. The emerging ﬁeld of\\ncausal representation learning strives to learn these variables from data, much like machine learning went beyond\\nsymbolic AI in not requiring that the symbols that algorithms manipulate be given a priori (see Fig. 11).\\n\\nDeﬁning objects or variables, and structural models connecting them, can sometimes be achieved by coarse-graining of\\nmicroscopic models, including microscopic SCMs [124], ordinary differential equations [123], and temporally aggre-\\ngated time series [37]. While most causal models in economics, medicine, or psychology use variables that are abstrac-\\ntions of more elementary concepts, it is challenging to state general conditions under which coarse-grained variables\\nadmit causal models with well-deﬁned interventions [15, 124, 16]. The task of identifying suitable units that admit\\ncausal models aligns with the general goal of modern machine learning to learn meaningful representations for data,\\nwhere meaningful can mean robust, transferable, interpretable, explainable, or fair [77, 72, 162, 71, 70, 155]. To com-\\nbine structural causal modeling (Defn. 4.4) and representation learning, we may try to devise machine learning models\\nwhose inputs may be high-dimensional and unstructured, but whose inner workings are (partly) governed by an SCM.\\n\\nSuppose that our high-dimensional, low-level observations X = (X1, ..., Xd) are explained by a small number of\\nunobserved, or latent, variables S = (S1, ..., Sn) where n (cid:28) d, in that X is generated by applying an injective map\\ng : Rn → Rd to S (see Fig. 12c):\\n\\nX = g(S).\\n(10.1)\\nA common assumption regarding (10.1) is that the latent Si are jointly independent, e.g., for independent component\\nanalysis (ICA) [57] (where g is referred to as a mixing) or disentangled representation learning [9] (where g is called\\na decoder). Presently, however, we instead want think of the latent Si as causal variables that support interventions\\nand reasoning.\\n\\nThe Si may thus well be dependent, and possess a causal factorization (4.1),\\n\\np(S1, . . . , Sn) =\\n\\np(Si | PAi),\\n\\n(10.2)\\n\\ninduced by an underlying (acyclic) SCM M = (F, pU) with jointly independent Ui and\\n\\nF = {Si := fi(PAi, Ui)}n\\nOur goal is to learn a latent causal model consisting of (i) the causal representation S = g−1(X), along with (ii) the\\ncorresponding causal graph and (iii) the mechanisms p(Si | PAi) or fi. This is a challenging task, since none of\\nthem are directly observed or known a priori; instead we typically only have access to observations of X. In fact,\\nthere is no hope in an i.i.d. setting since already the simpler case with independent Si (and n = d) is generally not\\nidentiﬁable (i.e., for arbitrary nonlinear g in (10.1)): even independence does not sufﬁciently constrain the problem to\\nuniquely recover, or identify, the true Si’s up to any simple class of ambiguities such as permutations and element-wise\\ninvertible transformations of the Si [58].\\n\\n(10.3)\\n\\ni=1.\\n\\nn\\n(cid:89)\\n\\ni=1\\n\\n25\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\ng−1 ?\\n\\nS1\\n\\n?\\n\\nX2\\n\\n?\\n\\nX3\\n\\nX1\\n\\n?\\n\\n(a)\\n\\nX1\\n\\nX3\\n\\nx2\\n\\n(b)\\n\\nE[X3 | do(x2)]?\\n\\nS2\\n\\nS3\\n\\ng\\n\\n(c)\\n\\nFigure 12: Overview of different causal learning tasks: (a) causal discovery (§ 7) aims to learn the causal graph (or\\nSCM) connecting a set of observed variables; (b) causal reasoning (§ 9) aims to answer interventional or counterfactual\\nqueries based on a (partial) causal model over observed variables Xi; (c) causal representation learning (§ 10) aims\\nto infer a causal model consisting of a small number of high-level, abstract causal variables Si and their relations from\\npotentially high-dimensional, low-level observations X = g(S).\\n\\nTo link causal representation learning to the well-studied ICA setting with independent latents in (10.1), we can\\nconsider the so-called reduced form of an (acyclic) SCM: by recursive substitution of the structural assignments (10.3)\\nin topological order of the causal graph, we can write the latent causal variables S as function of the noise variables\\nonly\\n\\nS = fRF(U).\\n(10.4)\\nDue to acyclicity, this mapping fRF : Rn → Rn has a lower triangular Jacobian (possibly after re-ordering the Si\\nw.l.o.g.). However, (10.4) is strictly less informative than (10.3): while they entail the same distribution (10.2), the\\nformer no longer naturally supports interventions on the Si but only changes to the noise distribution pU (an example\\nof a so-called soft intervention [30]). At the same time, the reduced form (10.4) allows us to rewrite (10.1) as\\n\\nX = g ◦ fRF(U)\\n\\n(10.5)\\n\\nThrough this lens, the task of learning the reduced form (10.4) could be seen as structured form of nonlinear ICA\\n(i.e., (10.1) with independent latents) where we additionally want to learn an intermediate representation through fRF.\\nHowever, as discussed, we cannot even solve the problem with independent latents (i.e., identify g ◦ fRF in (10.5)) [58],\\nlet alone separate the SCM and mixing functions to recover the intermediate causal representation.\\n\\nIt is not surprising that is is not possible to solve the strictly harder causal representation learning problem in an i.i.d.\\nsetting and that additional causal learning signals are needed. This gives rise to the following questions: How can we\\ndevise causal training algorithms to learn the Si? And, what types of additional data, assumptions, and constraints\\nwould these algorithms require beyond the i.i.d. setting? Two general ideas are to (i) build on the ICM Principle 5.1\\nand enforce some form of (algorithmic) independence between the learned causal mechanisms p(Si | PAi) or fi, and\\n(ii) use heterogeneous (non-i.i.d.) data, e.g., from multiple views or different environments, arising from interventions\\nin the underlying latent SCM (10.3). We brieﬂy discuss some more concrete ideas based on recent work.\\n\\nGenerative Approach: Causal Auto-Encoders. One approach is to try to learn the generative causal model (10.1)\\nand (10.3), or its reduced form (10.4), using an auto-encoder approach [73]. An auto-encoder consists of an encoder\\nfunction q : Rd → Rn which maps X to a latent “bottleneck” representation (e.g., comprising the unexplained noise\\nvariables U), and a decoder function ˆg : Rn → Rd mapping back to the observations. For example, the decoder\\nmay directly implement the composition ˆg = g ◦ fRF from (10.4). Alternatively, it could consist of multiple modules,\\nimplementing (10.1) and (10.3) separately. A standard procedure to train such an auto-encoder architecture is to\\nminimise the reconstruction error, i.e., to satisfy ˆg ◦ q ≈ id on a training set of observations of X. As discussed, this\\nalone is insufﬁcient, so to make it causal we can impose additional constraints on the structure of the decoder [80]\\nand try to make the causal mechanisms independent by ensuring that they are invariant across problems and can be\\nindependently intervened upon. For example, if we intervene on the causal variables Si or noise distribution pU\\nin our model of (10.3) or (10.4), respectively, this should still produce “valid” observations, as assessed, e.g., by\\nthe discriminator of a generative adversarial network [38]. While we ideally want to manipulate the causal variables,\\nanother way to intervene is to replace noise variables with the corresponding values computed from other input images,\\na procedure that has been referred to as hybridization [11]. Alternatively, if we have access to multiple environments,\\ni.e., datasets collected under different conditions, we could rely on the Sparse Mechanism Shift Principle 5.2 by\\nrequiring that changes can be explained by shifts in only a few of the p(Si | PAi).\\n\\nDiscriminative Approach: Self-Supervised Causal Representation Learning. A different machine learning approach\\nfor unsupervised representation learning, that is not based on generative modeling but is discriminative in nature, is\\nself-supervised learning with data augmentation. Here, the main idea is to apply some hand-crafted transformations\\n\\n26\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nto the observation to generate augmented views that are thought to share the main semantic characteristics with the\\noriginal observation (e.g., random crops or blurs for images). One then directly learns a representation by maximizing\\nthe similarity across encodings of views related to each other by augmentations, while enforcing diversity across those\\nof unrelated views. In recent work [156], we set out to better understand this approach theoretically, as well as to\\ninvestigate its potential for learning causal representations. Starting from (10.1), we postulate a latent causal model of\\nthe form Sc → Ss, where Sc is a (potentially multivariate) content variable, deﬁned as the high-level semantic part\\nof the representation S = (Sc, Ss) that is assumed invariant across views; and Ss is a (potentially multivariate) style\\nvariable, deﬁned as the remaining part of the representation that may change. Within this setting, data augmentations\\nhave a natural interpretation as counterfactuals under a hypothetical intervention on the style variables, given the\\noriginal view. It can be shown that in this case, subject to some technical assumptions, common contrastive self-\\nsupervised learning algorithms [19, 99, 45] as well as appropriately constrained generative models isolate, or recover,\\nthe true content variables Sc up to an invertible transformation. By extending this approach to use multiple augmented\\nviews of the same observation, and linking these to different counterfactuals in the underlying latent SCM, it may be\\npossible to recover a more-ﬁne grained causal representation.\\n\\nIndependent Mechanism Analysis. We also explored [40] to what extent the ICM Principle 5.1 may be useful for un-\\nsupervised representation learning tasks such as (10.1), particularly for imposing additional constraints on the mixing\\nfunction g. It turns out that independence between p(S) and the mixing g—measured, e.g., as discussed in § 5 in\\nthe context of Fig. 6 and [64]—does not impose nontrivial constraints when S is not observed, even when the Si are\\nassumed independent as in ICA. However, by thinking of each Si as independently inﬂuencing the observed distri-\\nbution, we postulate another type of independence between the partial derivatives ∂g\\nof the mixing g which has a\\n∂Si\\ngeometric interpretation as an orthogonality condition on the columns of the Jacobian of g. The resulting indepen-\\ndent mechanism analysis (IMA) approach rules out some of the common examples of non-identiﬁability of nonlinear\\nICA [58, 82] mentioned above. Since IMA does not require independent sources, it may also be a useful constraint for\\ncausal representation learning algorithms.\\n\\nLearning Transferable Mechanisms and Multi-Task Learning Machine learning excels in i.i.d. settings, and\\nthrough the use of high capacity learning algorithms we can achieve outstanding performance on many problems,\\nprovided we have i.i.d. data for each individual problem (§ 2). However, natural intelligence excels at generalizing\\nacross tasks and settings. Suppose we want to build a system that can solve multiple tasks in multiple environments.\\nIf we view learning as data compression, it would make sense for that system to utilize components that apply across\\ntasks and environments, and thus need to be stored only once [126].\\n\\nIndeed, an artiﬁcial or natural agent in a complex world is faced with limited resources. This concerns training\\ndata, i.e., we only have limited data for each individual task/domain, and thus need to ﬁnd ways of pooling/re-using\\ndata, in stark contrast to the current industry practice of large-scale labelling work done by humans. It also concerns\\ncomputational resources: animals have constraints on the resources (e.g., space, energy) used by their brains, and\\nevolutionary neuroscience knows examples where brain regions get re-purposed. Similar constraints apply as machine\\nlearning systems get embedded in physical devices that may be small and battery-powered. Versatile AI models that\\nrobustly solve a range of problems in the real world will thus likely need to re-use components, which requires that\\nthe components are robust across tasks and environments [129, 131]. This calls for a structure whose modules are\\nmaximally reusable. An elegant way to do this would be to employ a modular structure that mirrors modularity that\\nIn other words, if the are mechanisms at play in the world play similar roles across a range\\nexists in the world.\\nof environments, tasks, and settings, then it would be prudent for a model to employ corresponding computational\\nmodules [39]. For instance, if variations of natural lighting (the position of the sun, clouds, etc.)\\nimply that the\\nvisual environment can appear in brightness conditions spanning several orders of magnitude, then visual processing\\nalgorithms in our nervous system should employ methods that can factor out these variations, rather than building\\nseparate sets of object recognizers for every lighting condition. If our brain were to model the lighting changes by a\\ngain control mechanism, say, then this mechanism in itself need not have anything to do with the physical mechanisms\\nbringing about brightness differences. It would, however, play a role in a modular structure that corresponds to the\\nrole the physical mechanisms play in the world’s modular structure—in other words, it would represent the physical\\nmechanism. Searching for the most versatile yet compact models would then automatically produce a bias towards\\nmodels that exhibit certain forms of structural isomorphy to a world that we cannot directly recognize.\\n\\nA sensible inductive bias to learn such models is to look for independent causal mechanisms [83], and competitive\\ntraining can play a role in this: for a pattern recognition task, learning causal models that contain independent mecha-\\nnisms helps in transferring modules across substantially different domains [100].\\n\\nInterventional World Models, Surrogate Models, Digital Twins, and Reasoning Modern representation learning\\nIt does so, however,\\nexcels at learning representations of data that preserve relevant statistical properties [9, 79].\\n\\n27\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nwithout taking into account causal properties of the variables, i.e., it does not care about the interventional properties\\nof the variables it analyzes or reconstructs. Going forward, causality will play a major role in taking representation\\nlearning to the next level, moving beyond the representation of statistical dependence structures towards models that\\nsupport intervention, planning, and reasoning. This would realize Konrad Lorenz’ notion of thinking as acting in an\\nimagined space. It would also provide a means to learn causal digital twins that go beyond reproducing statistical\\ndependences captured by surrogate models trained using machine learning.\\n\\nThe idea of surrogate modeling is that we may have a complex phenomenon for which we have access to computa-\\ntionally expensive simulation data. If the mappings involved (e.g., from parameter settings to target quantities) can\\nbe ﬁtted from data, we can employ machine learning, which will often speed them up by orders of magnitude. Such\\na speed-up can qualitatively change the usability of a model: for instance, we have recently built a system to map\\ngravitational wave measurements to a probability distribution of physical parameters of a black hole merger event, in-\\ncluding sky position [27]. The fact that this model only requires seconds to evaluate makes it possible to immediately\\nstart electromagnetic follow-up observations using telescopes as soon as a gravitational wave event has been detected,\\nenabling analysis of transient events.\\n\\nGoing forward, we anticipate that surrogate modeling will beneﬁt from respecting the causal factorization (4.1) de-\\ncomposing the overall dependence structure into mechanisms (i.e., causal conditionals). We can then build an overall\\nmodel of a system by modeling the mechanisms independently, each of them using the optimal method. Some of the\\nconditionals we may know analytically, some we may be able to transfer from related problems, if they are invariant.\\nFor some, we may have access to real data to estimate them, and for others, we may need to resort to simulations,\\npossibly ﬁtted using surrogate models.\\n\\nIf the model is required to fully capture the effects of all possible interventions, then all components should be ﬁtted\\nas described in the causal directions (i.e., we ﬁt the causal mechanisms). Such a model then allows to employ all\\nthe causal reasoning machinery described in § 4 and § 9 (e.g., computing interventional and, in the case of SCMs,\\ncounterfactual distributions). If, on the other hand, a model only needs to capture some of the possible interventions,\\nand is used in a purely predictive/observational mode for other variables, then we can get away with also using and\\nﬁtting some non-causal modules, i.e., using a decomposition which lies in between (4.1) and (4.2).\\n\\nWe believe that this overall framework will be a principled and powerful approach to build such (causal) digital twins\\nor causal surrogate models by combining a range of methods and bringing them to bear according to their strengths.\\n\\nConcluding Remarks. Most of the discussed ﬁelds are still in their infancy, and the above account is biased by\\npersonal taste and knowledge. With the current hype around machine learning, there is much to say in favor of some\\nhumility towards what machine learning can do, and thus towards the current state of AI—the hard problems have not\\nbeen solved yet, making basic research in this ﬁeld all the more exciting.\\n\\nAckowledgements Many thanks to all past and present members of the T¨ubingen causality team, and to Cian East-\\nwood and Elias Bareinboim for feedback on the manuscript.\\n\\nReferences\\n\\n[1] J. Aldrich, Autonomy. Oxford Economic Papers 41 (1989), 15–34\\n\\n[2] J. D. Angrist, G. W. Imbens, and D. B. Rubin, Identiﬁcation of causal effects using instrumental variables. Journal of the\\n\\nAmerican statistical Association 91 (1996), no. 434, 444–455\\n\\n[3] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz, Invariant risk minimization. arXiv preprint 1907.02893 (2019)\\n\\n[4] E. Bareinboim and J. Pearl, Transportability from multiple environments with limited experiments: Completeness results. In\\n\\nAdvances in Neural Information Processing Systems 27, pp. 280–288, 2014\\n\\n[5] E. Bareinboim and J. Pearl, Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences\\n\\n113 (2016), no. 27, 7345–7352\\n\\n[6] S. Bauer, B. Sch¨olkopf, and J. Peters, The arrow of time in multivariate time series. In Proceedings of the 33nd international\\n\\nconference on machine learning, pp. 2043–2051, 48, 2016\\n\\n[7] M. Belkin, D. Hsu, S. Ma, and S. Mandal, Reconciling modern machine learning practice and the bias-variance trade-off.\\n\\n2018, arXiv:1812.11118\\n\\n[8] S. Ben-David, T. Lu, T. Luu, and D. P´al, Impossibility theorems for domain adaptation. In Proceedings of the international\\n\\nconference on artiﬁcial intelligence and statistics 13 (AISTATS), pp. 129–136, 2010\\n\\n[9] Y. Bengio, A. Courville, and P. Vincent, Representation learning: A review and new perspectives. IEEE Transactions on\\n\\nPattern Analysis and Machine Intelligence 35 (2013), no. 8, 1798–1828\\n\\n28\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[10] B. Benneke, I. Wong, C. Piaulet, H. A. Knutson, I. J. M. Crossﬁeld, J. Lothringer, C. V. Morley, P. Gao, T. P. Greene,\\nC. Dressing, D. Dragomir, A. W. Howard, P. R. McCullough, E. M. R. K. J. J. Fortney, and J. Fraine, Water vapor on the\\nhabitable-zone exoplanet K2-18b. arXiv preprint 1909.04642 (2019)\\n\\n[11] M. Besserve, A. Mehrjou, R. Sun, and B. Sch¨olkopf, Counterfactuals uncover the modular structure of deep generative\\n\\nmodels. In International conference on learning representations, 2020\\n\\n[12] M. Besserve, N. Shajarisales, B. Sch¨olkopf, and D. Janzing, Group invariance principles for causal generative models. In\\nProceedings of the 21st international conference on artiﬁcial intelligence and statistics (aistats), pp. 557–565, 2018\\n\\n[13] S. Bongers, P. Forr´e, J. Peters, and J. M. Mooij, Foundations of structural causal models with cycles and latent variables. The\\n\\nAnnals of Statistics 49 (2021), no. 5, 2885–2915\\n\\n[14] D. Buchsbaum, S. Bridgers, D. Skolnick Weisberg, and A. Gopnik, The power of possibility: Causal learning, counterfactual\\nreasoning, and pretend play. Philosophical Transactions of the Royal Society B: Biological Sciences 367 (2012), no. 1599,\\n2202–2212\\n\\n[15] K. Chalupka, F. Eberhardt, and P. Perona, Multi-level cause-effect systems. In Artiﬁcial intelligence and statistics, pp. 361–\\n\\n369, PMLR, 2016\\n\\n[16] K. Chalupka, F. Eberhardt, and P. Perona, Causal feature learning: an overview. Behaviormetrika 44 (2017), no. 1, 137–164\\n\\n[17] O. Chapelle, B. Sch¨olkopf, and A. Zien (eds.), Semi-supervised learning. MIT Press, Cambridge, MA, USA, 2006\\n\\n[18] C. R. Charig, D. R. Webb, S. R. Payne, and J. E. Wickham, Comparison of treatment of renal calculi by open surgery,\\npercutaneous nephrolithotomy, and extracorporeal shockwave lithotripsy. Br Med J (Clin Res Ed) 292 (1986), no. 6524,\\n879–882\\n\\n[19] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, A simple framework for contrastive learning of visual representations.\\n\\narXiv preprint 2002.05709 (2020)\\n\\n[20] D. M. Chickering, Learning bayesian networks is np-complete. In Learning from data, pp. 121–130, Springer, 1996\\n\\n[21] D. M. Chickering, Optimal structure identiﬁcation with greedy search. Journal of machine learning research 3 (2002), no.\\n\\n[22] G. F. Cooper and E. Herskovits, A bayesian method for the induction of probabilistic networks from data. Machine learning\\n\\nNov, 507–554\\n\\n9 (1992), no. 4, 309–347\\n\\n[23] D. R. Cox, Planning of experiments (1958)\\n\\n[24] P. Daniuˇsis, D. Janzing, J. M. Mooij, J. Zscheischler, B. Steudel, K. Zhang, and B. Sch¨olkopf, Inferring deterministic causal\\n\\nrelations. In Proceedings of the 26th annual conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 143–150, 2010\\n\\n[25] A. P. Dawid, Conditional independence in statistical theory. Journal of the Royal Statistical Society B 41 (1979), no. 1, 1–31\\n\\n[26] A. P. Dawid, Causal inference without counterfactuals. Journal of the American Statistical Association 95 (2000), no. 450,\\n\\n407–424\\n\\n[27] M. Dax, S. R. Green, J. Gair, J. H. Macke, A. Buonanno, and B. Sch¨olkopf, Real-time gravitational-wave science with neural\\n\\nposterior estimation. Physical Review Letters (2021)\\n\\n[28] L. Devroye, L. Gy¨orﬁ, and G. Lugosi, A probabilistic theory of pattern recognition. Applications of Mathematics 31,\\n\\n[29] V. Didelez, S. Meng, and N. A. Sheehan, Assumptions of IV methods for observational epidemiology. Statistical Science 25\\n\\nSpringer, New York, NY, 1996\\n\\n(2010), 22–40\\n\\n[30] F. Eberhardt and R. Scheines, Interventions and causal inference. Philosophy of Science 74 (2007), no. 5, 981–995\\n\\n[31] R. A. Fisher, The design of experiments. Oliver & Boyd, Edinburgh & London. (1937), no. 2\\n\\n[32] D. Foreman-Mackey, B. T. Montet, D. W. Hogg, T. D. Morton, D. Wang, and B. Sch¨olkopf, A systematic search for transiting\\n\\nplanets in the K2 data. The Astrophysical Journal 806 (2015), no. 2\\n\\n[33] K. Fukumizu, A. Gretton, X. Sun, and B. Sch¨olkopf, Kernel measures of conditional dependence. In Advances in neural\\n\\n[34] D. Geiger and D. Heckerman, Learning gaussian networks. In Proceedings of the tenth international conference on uncer-\\n\\ninformation processing systems, pp. 489–496, 2008\\n\\ntainty in artiﬁcial intelligence, pp. 235–243, 1994\\n\\n[35] D. Geiger and J. Pearl, Logical and algorithmic properties of independence and their application to Bayesian networks.\\n\\nAnnals of Mathematics and Artiﬁcial Intelligence 2 (1990), 165–178\\n\\n[36] M. Gong, K. Zhang, T. Liu, D. Tao, C. Glymour, and B. Sch¨olkopf, Domain adaptation with conditional transferable com-\\n\\nponents. In Proceedings of the 33nd international conference on machine learning, pp. 2839–2848, 2016\\n\\n[37] M. Gong, K. Zhang, B. Sch¨olkopf, C. Glymour, and D. Tao, Causal discovery from temporally aggregated time series. In\\n\\nProceedings of the thirty-third conference on uncertainty in artiﬁcial intelligence, 2017\\n\\n[38] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative\\n\\nadversarial nets. In Advances in neural information processing systems 27, pp. 2672–2680, 2014\\n\\n29\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[39] A. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, Y. Bengio, and B. Sch¨olkopf, Recurrent independent mechanisms.\\n\\nIn International conference on learning representations, 2020\\n\\n[40] L. Gresele, J. von K¨ugelgen, V. Stimper, B. Sch¨olkopf, and M. Besserve, Independent mechanism analysis, a new concept?\\n\\nIn Advances in neural information processing systems 34, 2021\\n\\n[41] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch¨olkopf, and A. J. Smola, A kernel method for the two-sample-problem. In\\n\\nAdvances in neural information processing systems 19, pp. 513–520, 2007\\n\\n[42] A. Gretton, O. Bousquet, A. Smola, and B. Sch¨olkopf, Measuring statistical dependence with Hilbert-Schmidt norms. In\\n\\nAlgorithmic learning theory, pp. 63–78, Springer-Verlag, 2005\\n\\n[43] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch¨olkopf, and A. J. Smola, A kernel statistical test of independence. In\\n\\nAdvances in neural information processing systems 20, pp. 585–592, 2008\\n\\n[44] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Sch¨olkopf, Kernel methods for measuring independence. Journal\\n\\nof Machine Learning Research 6 (2005), 2075–2129\\n\\n[45] U. M. Gutmann and A. Hyv¨arinen, Noise-contrastive estimation: A new estimation principle for unnormalized statistical\\n\\nmodels. In International conference on artiﬁcial intelligence and statistics, pp. 297–304, 2010\\n\\n[46] T. Haavelmo, The probability approach in econometrics. Econometrica: Journal of the Econometric Society (1944), iii–115\\n\\n[47] D. Heckerman, D. Geiger, and D. M. Chickering, Learning bayesian networks: The combination of knowledge and statistical\\n\\ndata. Machine learning 20 (1995), no. 3, 197–243\\n\\n[48] D. Heckerman, C. Meek, and G. Cooper, A bayesian approach to causal discovery. In Innovations in machine learning, pp.\\n\\n[49] M. A. Hern´an, D. Clayton, and N. Keiding, The Simpson’s paradox unraveled. International journal of epidemiology 40\\n\\n1–28, Springer, 2006\\n\\n(2011), no. 3, 780–785\\n\\n[50] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, Beta-VAE: Learning\\nbasic visual concepts with a constrained variational framework. In International conference on learning representations,\\n2016\\n\\n[51] P. W. Holland, Statistics and causal inference. Journal of the American statistical Association 81 (1986), no. 396, 945–960\\n\\n[52] K. D. Hoover, Causality in macroeconomics. Cambridge University Press, 2001\\n\\n[53] P. O. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Sch¨olkopf, Nonlinear causal discovery with additive noise models. In\\n\\nAdvances in Neural Information Processing Systems 21 (NIPS), pp. 689–696, 2009\\n\\n[54] B. Huang, K. Zhang, J. Zhang, J. D. Ramsey, R. Sanchez-Romero, C. Glymour, and B. Sch¨olkopf, Causal discovery from\\n\\nheterogeneous/nonstationary data. J. Mach. Learn. Res. 21 (2020), no. 89, 1–53\\n\\n[55] B. Huang, K. Zhang, J. Zhang, R. Sanchez-Romero, C. Glymour, and B. Sch¨olkopf, Behind distribution shift: Mining driving\\nforces of changes and causal arrows. In IEEE 17th international conference on data mining (icdm 2017), pp. 913–918, 2017\\n\\n[56] Y. Huang and M. Valtorta, Pearl’s calculus of intervention is complete. In Proceedings of the twenty-second conference on\\n\\nuncertainty in artiﬁcial intelligence, pp. 217–224, 2006\\n\\n[57] A. Hyv¨arinen and E. Oja, Independent component analysis: algorithms and applications. Neural networks 13 (2000), no.\\n\\n[58] A. Hyv¨arinen and P. Pajunen, Nonlinear independent component analysis: Existence and uniqueness results. Neural networks\\n\\n[59] A. Hyvarinen, H. Sasaki, and R. Turner, Nonlinear ica using auxiliary variables and generalized contrastive learning. In The\\n\\n22nd international conference on artiﬁcial intelligence and statistics, pp. 859–868, 2019\\n\\n[60] G. W. Imbens and T. Lemieux, Regression discontinuity designs: A guide to practice. Journal of econometrics 142 (2008),\\n\\n[61] G. W. Imbens and D. B. Rubin, Causal inference in statistics, social, and biomedical sciences. Cambridge University Press,\\n\\n4-5, 411–430\\n\\n12 (1999), no. 3, 429–439\\n\\nno. 2, 615–635\\n\\n2015\\n\\n[62] D. Janzing, R. Chaves, and B. Sch¨olkopf, Algorithmic independence of initial condition and dynamical law in thermody-\\n\\nnamics and causal inference. New Journal of Physics 18 (2016), no. 093052, 1–13\\n\\n[63] D. Janzing, P. Hoyer, and B. Sch¨olkopf, Telling cause from effect based on high-dimensional observations. In Proceedings\\nof the 27th international conference on machine learning, edited by J. F¨urnkranz and T. Joachims, pp. 479–486, 2010\\n\\n[64] D. Janzing, J. M. Mooij, K. Zhang, J. Lemeire, J. Zscheischler, P. Daniuˇsis, B. Steudel, and B. Sch¨olkopf, Information-\\n\\ngeometric approach to inferring causal directions. Artiﬁcial Intelligence 182–183 (2012), 1–31\\n\\n[65] D. Janzing, J. Peters, J. M. Mooij, and B. Sch¨olkopf, Identifying confounders using additive noise models. In Proceedings\\n\\nof the 25th annual conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 249–257, 2009\\n\\n[66] D. Janzing and B. Sch¨olkopf, Causal inference using the algorithmic Markov condition. IEEE Transactions on Information\\n\\nTheory 56 (2010), no. 10, 5168–5194\\n\\n30\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[67] D. Janzing and B. Sch¨olkopf, Semi-supervised interpolation in an anticausal learning scenario. Journal of Machine Learning\\n\\nResearch 16 (2015), 1923–1948\\n\\n[68] D. Janzing and B. Sch¨olkopf, Detecting non-causal artifacts in multivariate linear regression models. In Proceedings of the\\n\\n35th international conference on machine learning (ICML), pp. 2250–2258, 2018\\n\\n[69] Z. Jin, J. von K¨ugelgen, J. Ni, T. Vaidhya, A. Kaushal, M. Sachan, and B. Sch¨olkopf, Causal direction of data collection\\nmatters: Implications of causal and anticausal learning for nlp. In Proceedings of the 2021 conference on empirical methods\\nin natural language processing (emnlp), 2021\\n\\n[70] A.-H. Karimi, B. Sch¨olkopf, and I. Valera, Algorithmic recourse: from counterfactual explanations to interventions. In\\n\\nConference on fairness, accountability, and transparency, pp. 353–362, 2021\\n\\n[71] A.-H. Karimi, J. von K¨ugelgen, B. Sch¨olkopf, and I. Valera, Algorithmic recourse under imperfect causal knowledge: a\\n\\nprobabilistic approach. In Advances in neural information processing systems 33, 2020\\n\\n[72] N. Kilbertus, M. Rojas Carulla, G. Parascandolo, M. Hardt, D. Janzing, and B. Sch¨olkopf, Avoiding discrimination through\\n\\ncausal reasoning. In Advances in neural information processing systems 30, pp. 656–666, 2017\\n\\n[73] D. P. Kingma and M. Welling, Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114 (2013)\\n\\n[74] F. Klein, Vergleichende Betrachtungen ¨uber neuere geometrische Forschungen. Verlag von Andreas Deichert, Erlangen,\\n\\n1872\\n\\n[75] D. Koller and N. Friedman, Probabilistic graphical models: principles and techniques. MIT press, 2009\\n\\n[76] S. Kpotufe, E. Sgouritsa, D. Janzing, and B. Sch¨olkopf, Consistency of causal inference under the additive noise model. In\\n\\nProceedings of the 31th international conference on machine learning, pp. 478–486, 2014\\n\\n[77] M. J. Kusner, J. Loftus, C. Russell, and R. Silva, Counterfactual fairness. In Advances in neural information processing\\n\\nsystems 30, pp. 4066–4076, Curran Associates, Inc., 2017\\n\\n[78] S. L. Lauritzen, Graphical models. 17, Clarendon Press, 1996\\n\\n[79] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning. Nature 521 (2015), no. 7553, 436–444\\n\\n[80] F. Leeb, Y. Annadani, S. Bauer, and B. Sch¨olkopf, Structural autoencoders improve representations for generation and\\n\\ntransfer. arXiv preprint 2006.07796 (2020)\\n\\n[81] G. W. Leibniz, Discours de m´etaphysique. 1686, (cited after Chaitin, 2010)\\n\\n[82] F. Locatello, S. Bauer, M. Lucic, G. R¨atsch, S. Gelly, B. Sch¨olkopf, and O. Bachem, Challenging common assumptions\\nin the unsupervised learning of disentangled representations. Proceedings of the 36th International Conference on Machine\\nLearning (2019)\\n\\n[83] F. Locatello, D. Vincent, I. Tolstikhin, G. R¨atsch, S. Gelly, and B. Sch¨olkopf, Competitive training of mixtures of independent\\n\\ndeep generative models. arXiv preprint 1804.11130 (2018)\\n\\n[84] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf, Object-\\n\\ncentric learning with slot attention. In Advances in neural information processing systems, 2020\\n\\n[85] D. Lopez-Paz, R. Nishihara, S. Chintala, B. Sch¨olkopf, and L. Bottou, Discovering causal signals in images. In Ieee confer-\\n\\nence on computer vision and pattern recognition (cvpr), pp. 58–66, 2017\\n\\n[86] J. Loschmidt, ¨Uber den Zustand des W¨armegleichgewichtes eines Systems von K¨orpern mit R¨ucksicht auf die Schwerkraft.\\nAkademie der Wissenschaften, Wien. Mathematisch-Naturwissenschaftliche Klasse, Sitzungsberichte 73 (1876), 128–142\\n\\n[87] C. Lu, Y. Wu, J. M. Hern´andez-Lobato, and B. Sch¨olkopf, Nonlinear invariant risk minimization: A causal approach. 2021,\\n\\narXiv:2102.12353\\n\\n[88] S. MacLane, Categories for the working mathematician. Springer-Verlag, New York, 1971\\n\\n[89] R. Matthews, Storks deliver babies (p= 0.008). Teaching Statistics 22 (2000), no. 2, 36–38\\n\\n[90] C. Meek, Causal inference and causal explanation with background knowledge. In Proceedings of the eleventh conference\\n\\non uncertainty in artiﬁcial intelligence, pp. 403–410, Morgan Kaufmann Publishers Inc., 1995\\n\\n[91] F. H. Messerli, Chocolate consumption, cognitive function, and nobel laureates. The New England Journal of Medicine 367\\n\\n(2012), no. 16, 1562–1564\\n\\n[92] B. T. Montet, T. D. Morton, D. Foreman-Mackey, J. A. Johnson, D. W. Hogg, B. P. Bowler, D. W. Latham, A. Bieryla, and\\nA. W. Mann, Stellar and planetary properties of K2 campaign 1 candidates and validation of 17 planets, including a planet\\nreceiving earth-like insolation. The Astrophysical Journal 809 (2015), no. 1, 25\\n\\n[93] R. P. Monti, K. Zhang, and A. Hyv¨arinen, Causal discovery with general non-linear relationships using non-linear ica. In\\n\\nUncertainty in artiﬁcial intelligence, pp. 186–195, PMLR, 2020\\n\\n[94] J. M. Mooij, D. Janzing, T. Heskes, and B. Sch¨olkopf, On causal discovery with cyclic additive noise models. In Advances\\n\\nin Neural Information Processing Systems 24 (NIPS), 2011\\n\\n[95] J. M. Mooij, D. Janzing, J. Peters, and B. Sch¨olkopf, Regression by dependence minimization and its application to causal\\n\\ninference. In Proceedings of the 26th international conference on machine learning (ICML), pp. 745–752, 2009\\n\\n31\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[96] J. M. Mooij, D. Janzing, and B. Sch¨olkopf, From ordinary differential equations to structural causal models: The deter-\\nministic case. In Proceedings of the 29th annual conference on uncertainty in artiﬁcial intelligence (UAI), pp. 440–448,\\n2013\\n\\n[97] J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Sch¨olkopf, Distinguishing cause from effect using observational\\n\\ndata: methods and benchmarks. Journal of Machine Learning Research 17 (2016), no. 32, 1–102\\n\\n[98] J. S. Neyman, On the application of probability theory to agricultural experiments. essay on principles. section 9.(tlanslated\\nand edited by dm dabrowska and tp speed, statistical science (1990), 5, 465-480). Annals of Agricultural Sciences 10 (1923),\\n1–51\\n\\n[99] A. v. d. Oord, Y. Li, and O. Vinyals, Representation learning with contrastive predictive coding. arXiv preprint 1807.03748\\n\\n(2018)\\n\\n[100] G. Parascandolo, N. Kilbertus, M. Rojas-Carulla, and B. Sch¨olkopf, Learning independent causal mechanisms. In Proceed-\\n\\nings of the 35th international conference on machine learning, pmlr 80:4036-4044, 2018\\n\\n[101] J. Park and K. Muandet, A measure-theoretic approach to kernel conditional mean embeddings. In Advances in neural\\n\\ninformation processing systems 33 (neurips 2020), pp. 21247–21259, Curran Associates, Inc., 2020\\n\\n[102] J. Pearl, Bayesian networks: A model of self-activated memory for evidential reasoning. In Proceedings of the 7th conference\\n\\nof the cognitive science society, 1985, pp. 329–334, 1985\\n\\n[103] J. Pearl, Causal diagrams for empirical research. Biometrika 82 (1995), no. 4, 669–688\\n\\n[104] J. Pearl, Direct and indirect effects. In Proceedings of the seventeenth conference on uncertainty in artiﬁcial intelligence, pp.\\n\\n[105] J. Pearl, Causality: Models, reasoning, and inference. 2nd edn., Cambridge University Press, New York, NY, 2009\\n\\n[106] J. Pearl, Comment: understanding simpson’s paradox. The American Statistician 68 (2014), no. 1, 8–13\\n\\n[107] J. Pearl and E. Bareinboim, External validity: From do-calculus to transportability across populations. Statistical Science 29\\n\\n411–420, 2001\\n\\n(2014), no. 4, 579–595\\n\\n[108] J. Pearl and D. Mackenzie, The book of why: the new science of cause and effect. Basic Books, 2018\\n\\n[109] J. Pearl and A. Paz, Confounding equivalence in causal inference. Journal of Causal Inference 2 (2014), no. 1, 75–93\\n\\n[110] J. Pearl and T. Verma, A theory of inferred causation. In Principles of knowledge representation and reasoning: Proceedings\\n\\nof the second international conference, p. 441, 2, 1991\\n\\n[111] J. Peters, P. B¨uhlmann, and N. Meinshausen, Causal inference by using invariant prediction: identiﬁcation and conﬁdence\\n\\nintervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 78 (2016), no. 5, 947–1012\\n\\n[112] J. Peters, D. Janzing, and B. Sch¨olkopf, Elements of causal inference - foundations and learning algorithms. MIT Press,\\n\\nCambridge, MA, USA, 2017\\n\\n[113] J. Peters, J. M. Mooij, D. Janzing, and B. Sch¨olkopf, Identiﬁability of causal graphs using functional models. In Proceedings\\n\\nof the 27th annual conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 589–598, 2011\\n\\n[114] J. Peters, J. M. Mooij, D. Janzing, and B. Sch¨olkopf, Causal discovery with continuous additive noise models. Journal of\\n\\nMachine Learning Research 15 (2014), 2009–2053\\n\\n[115] N. Pﬁster, P. B¨uhlmann, B. Sch¨olkopf, and J. Peters, Kernel-based tests for joint independence. Journal of the Royal Statis-\\n\\ntical Society: Series B (Statistical Methodology) 80 (2018), no. 1, 5–31\\n\\n[116] K. Popper, The logic of scientiﬁc discovery (1959)\\n\\n[117] H. Reichenbach, The direction of time. University of California Press, Berkeley, CA, 1956\\n\\n[118] J. Robins, A new approach to causal inference in mortality studies with a sustained exposure period—application to control\\n\\nof the healthy worker survivor effect. Mathematical modelling 7 (1986), no. 9-12, 1393–1512\\n\\n[119] J. M. Robins, M. A. Hernan, and B. Brumback, Marginal structural models and causal inference in epidemiology. Epidemi-\\n\\nology 11 (2000), no. 5, 550–560\\n\\nmichigan, ann arbor, mich., 1971). 1973\\n\\nLearning Research 19 (2018), no. 36, 1–34\\n\\nBiometrika 70 (1983), no. 1, 41–55\\n\\n[120] R. W. Robinson, Counting labeled acyclic digraphs, new directions in the theory of graphs (proc. third ann arbor conf., univ.\\n\\n[121] M. Rojas-Carulla, B. Sch¨olkopf, R. Turner, and J. Peters, Invariant models for causal transfer learning. Journal of Machine\\n\\n[122] P. R. Rosenbaum and D. B. Rubin, The central role of the propensity score in observational studies for causal effects.\\n\\n[123] P. K. Rubenstein, S. Bongers, B. Sch¨olkopf, and J. M. Mooij, From deterministic ODEs to dynamic structural causal models.\\n\\nIn Proceedings of the 34th conference on uncertainty in artiﬁcial intelligence (uai), 2018\\n\\n[124] P. K. Rubenstein, S. Weichwald, S. Bongers, J. M. Mooij, D. Janzing, M. Grosse-Wentrup, and B. Sch¨olkopf, Causal con-\\nsistency of structural equation models. In Proceedings of the thirty-third conference on uncertainty in artiﬁcial intelligence,\\n2017\\n\\n32\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[125] D. B. Rubin, Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of educational\\n\\nPsychology 66 (1974), no. 5, 688\\n\\n[126] B. Sch¨olkopf, Causality for machine learning. arXiv preprint 1911.10500, to appear in: R. Dechter, J. Halpern, and H.\\n\\nGeffner. Probabilistic and Causal Inference: The Works of Judea Pearl. ACM books (2019)\\n\\n[127] B. Sch¨olkopf, R. Herbrich, and A. J. Smola, A generalized representer theorem. In Annual conference on computational\\nlearning theory, edited by D. Helmbold and R. Williamson, pp. 416–426, no. 2111 in Lecture Notes in Computer Science,\\nSpringer, Berlin, 2001\\n\\n[128] B. Sch¨olkopf, D. Hogg, D. Wang, D. Foreman-Mackey, D. Janzing, C.-J. Simon-Gabriel, and J. Peters, Modeling confound-\\ning by half-sibling regression. Proceedings of the National Academy of Science (PNAS) 113 (2016), no. 27, 7391–7398\\n\\n[129] B. Sch¨olkopf, D. Janzing, and D. Lopez-Paz, Causal and statistical learning. In Oberwolfach reports, pp. 1896–1899, 13(3),\\n\\n2016\\n\\n[130] B. Sch¨olkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. M. Mooij, On causal and anticausal learning. In Proceed-\\n\\nings of the 29th international conference on machine learning (ICML), pp. 1255–1262, 2012\\n\\n[131] B. Sch¨olkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio, Toward causal representation\\n\\nlearning. Proceedings of the IEEE 109 (2021), no. 5, 612–634\\n\\n[132] B. Sch¨olkopf, K. Muandet, K. Fukumizu, S. Harmeling, and J. Peters, Computing functions of random variables via repro-\\n\\nducing kernel Hilbert space representations. Statistics and Computing 25 (2015), no. 4, 755–766\\n\\n[133] B. Sch¨olkopf and A. J. Smola, Learning with kernels. MIT Press, Cambridge, MA, 2002\\n\\n[134] B. Sch¨olkopf, B. K. Sriperumbudur, A. Gretton, and K. Fukumizu, RKHS representation of measures applied to homogene-\\nity, independence, and Fourier optics. In Oberwolfach reports, edited by K. Jetter, S. Smale, and D.-X. Zhou, pp. 42–44, 30,\\n2008\\n\\n[135] G. Schwarz et al., Estimating the dimension of a model. The annals of statistics 6 (1978), no. 2, 461–464\\n\\n[136] R. D. Shah and J. Peters, The hardness of conditional independence testing and the generalised covariance measure. The\\n\\nAnnals of Statistics 48 (2020), no. 3, 1514–1538\\n\\n[137] N. Shajarisales, D. Janzing, B. Sch¨olkopf, and M. Besserve, Telling cause from effect in deterministic linear dynamical\\n\\nsystems. In Proceedings of the 32nd international conference on machine learning (ICML), pp. 285–294, 2015\\n\\n[138] U. Shalit, F. D. Johansson, and D. Sontag, Estimating individual treatment effect: generalization bounds and algorithms. In\\n\\nInternational conference on machine learning, pp. 3076–3085, 2017\\n\\n[139] C. E. Shannon, Coding theorems for a discrete source with a ﬁdelity criterion. In Ire international convention records, pp.\\n\\n142–163, 7, 1959\\n\\n[140] S. Shimizu, P. O. Hoyer, A. Hyv¨arinen, and A. J. Kerminen, A linear non-Gaussian acyclic model for causal discovery.\\n\\nJournal of Machine Learning Research 7 (2006), 2003–2030\\n\\n[141] I. Shpitser and J. Pearl, Identiﬁcation of joint interventional distributions in recursive semi-markovian causal models. In\\n\\nProceedings of the 21st national conference on artiﬁcial intelligence, pp. 1219–1226, 2006\\n\\n[142] I. Shpitser, T. VanderWeele, and J. M. Robins, On the validity of covariate adjustment for estimating causal effects. In\\nProceedings of the twenty-sixth conference on uncertainty in artiﬁcial intelligence, pp. 527–536, AUAI Press, 2010\\n\\n[143] E. H. Simpson, The interpretation of interaction in contingency tables. Journal of the Royal Statistical Society: Series B\\n\\n(Methodological) 13 (1951), no. 2, 238–241\\n\\n[144] A. J. Smola, A. Gretton, L. Song, and B. Sch¨olkopf, A Hilbert space embedding for distributions. In Algorithmic learning\\n\\ntheory: 18th international conference, pp. 13–31, 2007\\n\\n[145] P. Spirtes, C. Glymour, and R. Scheines, Causation, prediction, and search. 2nd edn., MIT Press, Cambridge, MA, 2000\\n\\n[146] W. Spohn, Grundlagen der Entscheidungstheorie. Scriptor-Verlag, 1978\\n\\n[147] I. Steinwart and A. Christmann, Support vector machines. Springer, New York, NY, 2008\\n\\n[148] R. Suter, D. Miladinovic, B. Sch¨olkopf, and S. Bauer, Robustly disentangled causal mechanisms: Validating deep represen-\\ntations for interventional robustness. In International conference on machine learning, pp. 6056–6065, PMLR, 2019\\n\\n[149] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, Intriguing properties of neural\\n\\nnetworks. arXiv preprint 1312.6199 (2013)\\n\\n[150] M. Tangemann, S. Schneider, J. von K¨ugelgen, F. Locatello, P. Gehler, T. Brox, M. K¨ummerer, M. Bethge, and B. Sch¨olkopf,\\n\\nUnsupervised object learning via common fate. arXiv preprint arXiv:2110.06562 (2021)\\n\\n[151] J. Tian and J. Pearl, Causal discovery from changes. In Proceedings of the seventeenth conference on uncertainty in artiﬁcial\\n\\nintelligence, pp. 512–521, Morgan Kaufmann Publishers Inc., 2001\\n\\n[152] A. Tsiaras, I. Waldmann, G. Tinetti, J. Tennyson, and S. Yurchenko, Water vapour in the atmosphere of the habitable-zone\\n\\neight-earth-mass planet K2-18b. Nature Astronomy (2019)\\n\\n[153] V. N. Vapnik, Statistical learning theory. Wiley, New York, NY, 1998\\n\\n33\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[154] J. von K¨ugelgen, L. Gresele, and B. Sch¨olkopf, Simpson’s paradox in Covid-19 case fatality rates: a mediation analysis of\\n\\nage-related causal effects. IEEE Transactions on Artiﬁcial Intelligence 2 (2021), no. 1, 18–27\\n\\n[155] J. von K¨ugelgen, A.-H. Karimi, U. Bhatt, I. Valera, A. Weller, and B. Sch¨olkopf, On the fairness of causal algorithmic\\n\\nrecourse. In 36th aaai conference on artiﬁcial intelligence, 2022\\n\\n[156] J. von K¨ugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Sch¨olkopf, M. Besserve, and F. Locatello, Self-supervised learning\\nwith data augmentations provably isolates content from style. In Advances in neural information processing systems 34,\\n2021\\n\\n[157] J. von K¨ugelgen, I. Ustyuzhaninov, P. Gehler, M. Bethge, and B. Sch¨olkopf, Towards causal generative scene models via\\n\\ncompetition of experts. In ICLR 2020 workshop on causal learning for decision making, 2020\\n\\n[158] J. von K¨ugelgen, A. Mey, M. Loog, and B. Sch¨olkopf, Semi-supervised learning, causality and the conditional cluster\\n\\nassumption. Conference on Uncertainty in Artiﬁcial Intelligence (2020)\\n\\n[159] J. Woodward, Causation and manipulability (2001)\\n\\n[160] P. G. Wright, Tariff on animal and vegetable oils. Macmillan Company, New York, 1928\\n\\n[161] S. Wright, Correlation and causation. Journal of Agricultural Research 20 (1921), 557–580\\n\\n[162] J. Zhang and E. Bareinboim, Fairness in decision-making - the causal explanation formula. In Proceedings of the thirty-\\n\\nsecond AAAI conference on artiﬁcial intelligence, pp. 2037–2045, 2018\\n\\n[163] K. Zhang, M. Gong, and B. Sch¨olkopf, Multi-source domain adaptation: A causal view. In Proceedings of the 29th aaai\\n\\nconference on artiﬁcial intelligence, pp. 3150–3157, 2015\\n\\n[164] K. Zhang and A. Hyv¨arinen, On the identiﬁability of the post-nonlinear causal model. In Proceedings of the 25th annual\\n\\nconference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 647–655, 2009\\n\\n[165] K. Zhang, J. Peters, D. Janzing, and B. Sch¨olkopf, Kernel-based conditional independence test and application in causal\\ndiscovery. In Proceedings of the 27th annual conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 804–813, 2011\\n\\n[166] K. Zhang, B. Sch¨olkopf, K. Muandet, and Z. Wang, Domain adaptation under target and conditional shift. In Proceedings of\\n\\nthe 30th international conference on machine learning, pp. 819–827, 2013\\n\\n34\\n\\n\\x0c',\n",
       " 'Quantized GAN for Complex Music Generation from Dance Videos\\n\\nYe Zhu *\\nIllinois Institute of Technology\\n\\nKyle Olszewski\\nSnap Inc.\\n\\nYu Wu\\nPrinceton University\\n\\nPanos Achlioptas\\nSnap Inc.\\n\\nMenglei Chai\\nSnap Inc.\\n\\nYan Yan\\nIllinois Institute of Technology\\n\\nSergey Tulyakov\\nSnap Inc.\\n\\n2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n4\\n0\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe present Dance2Music-GAN (D2M-GAN), a novel ad-\\nversarial multi-modal framework that generates complex\\nmusical samples conditioned on dance videos. Our pro-\\nposed framework takes dance video frames and human body\\nmotion as input, and learns to generate music samples that\\nplausibly accompany the corresponding input. Unlike most\\nexisting conditional music generation works that generate\\nspeciﬁc types of mono-instrumental sounds using symbolic\\naudio representations (e.g., MIDI), and that heavily rely\\non pre-deﬁned musical synthesizers, in this work we gen-\\nerate dance music in complex styles (e.g., pop, breakdanc-\\ning, etc.) by employing a Vector Quantized (VQ) audio rep-\\nresentation, and leverage both its generality and the high\\nabstraction capacity of its symbolic and continuous coun-\\nterparts. By performing an extensive set of experiments on\\nmultiple datasets, and following a comprehensive evalua-\\ntion protocol, we assess the generative quality of our ap-\\nproach against several alternatives. The quantitative re-\\nsults, which measure the music consistency, beats corre-\\nspondence, and music diversity, clearly demonstrate the\\neffectiveness of our proposed method. Last but not least,\\nwe curate a challenging dance-music dataset of in-the-wild\\nTikTok videos, which we use to further demonstrate the ef-\\nﬁcacy of our approach in real-world applications – and\\nwhich we hope to serve as a starting point for relevant fu-\\nture research. The code is available at https://github.com/L-\\nYeZhu/D2M-GAN.\\n\\n1. Introduction\\n\\n“When the music and dance create with accord, their\\nmagic captivates both the heart and the mind.” 1 As a natu-\\nral form of expressive art, dance and music have enriched\\nour daily lives with a harmonious interplay of melodies,\\n\\n*This work was mainly done while the author was an intern at Snap\\n\\nInc.\\n\\n1Jean-Georges Noverre.\\n\\nFigure 1. Task illustration. We introduce a Vector Quantiza-\\ntion framework for music generation from dance videos, which\\ntakes human body motion and visual frames as input, and gener-\\nates suitable corresponding music. Our proposed model can gen-\\nerate complex and rich dance music - in contrast to most existing\\nconditional music generation works, which typically output mono-\\ninstrumental sounds.\\n\\nrhythms, and movements across the millennia. The grow-\\ning popularity of social media platforms for sharing dance\\nvideos, such as TikTok, has also demonstrated their signif-\\nicance as a source of entertainment in modern society. At\\nthe same time, new research is ﬂourishing in the wake of\\nthis trend by exploring multi-modal generative tasks link-\\ning dance motion and music [1, 37–39].\\n\\nAlthough seemingly intuitive, music generation from\\ndance videos has been a challenging task compared to its\\ncounterpart in the inverse direction (i.e., dance generation\\nfrom music) for two primary reasons. First, typical au-\\ndio music signals are high-dimensional and require sophis-\\nticated temporal correlations for overall coherence [4, 28].\\nFor example, CD-quality audio has a typical sampling rate\\nof 44.1 kHz, resulting in over 2.5 million data points (“di-\\nmensions”) for a one-minute musical piece [9].\\nIn con-\\ntrast, most dance generation works output the relatively\\nlow-dimensional motion data in the form of 2D or 3D skele-\\nton keypoints (e.g., displacements for dozens of joints) con-\\nditioned on the music [37, 39, 53, 56], which are then ren-\\ndered into dance sequences and videos. To tackle the chal-\\n\\n1\\n\\nQuantizedD2M-GAN···12kK+19N······\\x0clenge of the high dimensionality of audio data, research\\nstudies on music generation from visual input [16, 25, 57]\\noften rely on low-dimensional intermediate symbolic au-\\ndio representations (e.g., a 1D piano-roll or 2D MIDI). The\\nsymbolic representations provide existing learning frame-\\nworks with a more explicit audio-visual correlation map-\\nping and more stable training, as well as widely-established\\nstandard music synthesizers for decoding the intermediate\\nrepresentations. However, such symbolic-based works suf-\\nfer from serious limitations on the ﬂexibility of the gener-\\nated music. This brings us to the second challenge of dance\\nvideo conditioned music generation: a separately trained\\nmodel is usually required for each instrument, and the gen-\\nerated music is composed with acoustic sounds from a sin-\\ngle predeﬁned instrument [12, 16, 46]. Consequently, the\\nresulting music is typically simple, and lacking in harmony\\nand richness consistent with the accompanying real-world\\ndance videos (e.g., see the person dancing in a hip-hop style\\nwith piano-based generated samples in our supplementary\\nvideo). These facts make existing conditional music gen-\\neration works difﬁcult to generalize into complex musical\\nstyles and real-world scenarios.\\n\\nTo ﬁll this gap, we propose a novel adversarial multi-\\nmodal framework that learns to generate complex musical\\nsamples from dance videos via Vector Quantized (VQ) au-\\ndio representations. Inspired by the recent success of VQ-\\nVAE [9,45,52] and VQ-GAN [14], we adopt quantized vec-\\ntors as our intermediate audio representation, and leverage\\nboth their increased abstraction ability compared to contin-\\nuous raw audio signals, as well as their ﬂexibility to bet-\\nter represent complex real-world music in comparison to\\nclassic symbolic representations. Speciﬁcally, our frame-\\nwork takes the visual frames and dance motion as input\\n(Figure 1), which are encoded and fused to generate the\\ncorresponding audio VQ representations. After retrieving\\nthe generated VQ representations from a learned codebook,\\nthese entries are decoded back to the raw audio domain us-\\ning a ﬁne-tuned JukeBox decoder [9]. Additionally, we de-\\nploy a convolution-based backbone and follow a hierarchi-\\ncal structure with two separate abstraction levels (i.e., dif-\\nferent hop-lengths) for the audio signals to demonstrate the\\nscalability of our framework. The higher-level model has a\\nlarger hop-length and fewer parameters, resulting in faster\\ninference. In contrast, the lower-level model has a lower ab-\\nstraction level with smaller hop-length, which enables the\\ngeneration of music with higher ﬁdelity and better quality.\\n\\nLast but not least, we also procure a real-world paired\\ndance-music dataset collected from TikTok video compila-\\ntions. Our dataset contains in total 445 dance videos with 85\\nsongs and an average per-video duration of approximately\\n12.5 seconds. Unlike existing datasets (e.g., AIST [39,60]),\\nours is more challenging and better reﬂects the conditions of\\nreal-world scenarios, thus providing a new asset for relevant\\n\\nfuture research.\\n\\nEmploying such datasets, we conduct extensive experi-\\nments to demonstrate the effectiveness and robustness of the\\nproposed framework. Speciﬁcally, we design and follow a\\nrich evaluation protocol to consider its generative quality\\nwith respect to the correspondence to the input dance mo-\\ntion in in terms of beats, genre and coherence. The general\\nquality of the generated music is also assessed. The attained\\nresults (both quantitative and qualitative) demonstrate that\\nour model can generate plausible dance music in terms of\\nvarious musical features, outperforming several competitive\\nconditional music generation methods.\\n\\nIn summary, our main contributions are:\\n\\n• We propose D2M-GAN, a novel adversarial multi-modal\\nframework that generates complex, free-form music from\\ndance videos via Vector Quantized (VQ) representations.\\n\\n• The proposed model, using a VQ generator and a multi-\\nscale discriminator, is able to effectively capture the tem-\\nporal correlations and rhythm for the musical sequence to\\ngenerate complex music.\\n\\n• To assess our model, we introduce a comprehensive eval-\\nuation protocol for music conditionally generated from\\nvideos, and demonstrate how the proposed D2M-GAN\\ngenerates more complex and plausibly corresponding mu-\\nsic compared to existing approaches.\\n\\n• Last but not least, we create a novel real-world dataset\\nwith dance videos captured in the wild – and use it to\\nestablish a new, more challenging setup for conditioned\\nmusic generation, which further demonstrates the superi-\\nority of our framework.\\n\\n2. Related Work\\n\\n2.1. Audio, Vision, and Motion\\n\\nCombining data from audio, vision, and motion has been\\na popular research topic in recent years within the ﬁeld of\\nmulti-modal learning. Research focusing on general audio-\\nvisual learning typically assumes that the two modalities are\\nintrinsically correlated based on the natural synchronization\\nof the audio and visual signals [2, 3, 34, 47, 48, 67]. Such\\njointly learned audio-visual representations thus can be ap-\\nplied in multiple downstream tasks, like sound source sep-\\naration [17–20, 66], audio-visual captioning [51, 62], audio-\\nvisual action recognition [21, 31], and audio-visual event\\nlocalization and parsing [59, 63, 64, 67].\\n\\nOn the other hand, another branch of studies closely re-\\nlated to our work has investigated the correlations between\\nmotion and sounds [15,16,37–39,53,56,68]. A large portion\\nof this research aims to generate human motion based on\\naudio signals, either in the form of 2D poses [37, 53, 56] or\\ndirect 3D motion [29, 39, 58]. For the inverse direction that\\n\\n2\\n\\n\\x0cseeks to generate audio from motion, Zhao et al. [66] in-\\ntroduces an end-to-end model to generate sounds from mo-\\ntion trajectories using a curriculum learning scheme. Gan\\net al. [16] propose a graph-based transformer framework to\\ngenerate music from performance videos using raw move-\\nment as input. Di et al. [10] propose to generate video\\nbackground music conditioned on the motion and special\\ntiming/rhythmic features of the input videos. In contrast to\\nthese previous works, our work combines three modalities,\\nwhich takes the vision and motion data as input and gener-\\nates music accordingly.\\n\\n2.2. Music Generation\\n\\nRaw music generation is a challenging task due to the\\nhigh dimensionality of the audio data and its sophisti-\\ncated temporal correlations. Therefore, the existing mu-\\nsic generation approaches usually adopt an intermediate\\naudio representation for learning generative models to re-\\nduce the computational demand and simplify the learning\\nprocess [9, 12, 25, 35, 44]. Classical audio representations\\nmainly employ the symbolic and continuous approaches.\\nMusegan [12] introduces a multi-track GAN-based model\\nfor instrumental music generation via 1D piano-roll sym-\\nbolic representations. Music Transformer [25] aims to im-\\nprove the long-term coherence of generated musical pieces\\nusing 2D event-based MIDI-like audio representations [46].\\nMelgan [35] is a generative model for music in form of the\\naudio mel-spectrogram features. Recently, JukeBox [9] in-\\ntroduces a generic music generation model based on the\\nnovel Vector Quantized (VQ) representations. Our pro-\\nposed framework adopts this VQ representation for music\\ngeneration.\\n\\n2.3. Vector Quantized Generative Models\\n\\nVQ-VAEs [45, 52] are ﬁrstly proposed as a variant of the\\nVariational Auto-Encoder (VAE) [32] with discrete codes\\nand learned priors. Following works have demonstrated\\nthe potential of VQ-based framework in multiple genera-\\ntive tasks such as image and audio synthesis [9, 14, 26].\\nSpeciﬁcally, the VQ-VAE [45] is initially tested for gen-\\nerating images, videos, and speech. An improved version\\nof VQ-VAE [52] is proposed with a multi-scale hierarchi-\\ncal organization. Esser et al. [14] apply the VQ represen-\\ntations in the GAN-based framework for generating high-\\nresolution images. Dhariwal et al. [9] introduce the Juke-\\nBox as a large-scale generative model for music synthe-\\nsis based on VQ-VAE. Compared to the classic symbolic\\nand continuous audio representations, the VQ representa-\\ntions leverage the beneﬁts of ﬂexibility (i.e., the ability to\\nrepresent complex music genres with a uniﬁed codebook in\\ncontrast to symbolic representations) and high compression\\nlevels (i.e., the learned codebooks largely reduce the data\\ndimensionality compared to raw waveform or spectrogram).\\n\\nOur proposed framework combines both the GAN [23] and\\nVAE [32], which uses the GAN-based learning to generate\\nVQ representations from the dance videos, and adopts the\\nVAE-based decoder for synthesizing music.\\n\\n3. Method\\n\\nAn overview of the architecture of the proposed D2M-\\nGAN is shown in Figure 2. Our approach employs a hier-\\narchical structure with two levels of generative models that\\nare independently trained with a similar pipeline for ﬂexible\\nscalability. In each level, the model consists of four compo-\\nnents: the motion module, the visual module, the VQ mod-\\nule consisting of a VQ generator with multi-scale discrim-\\ninators, and the music synthesizer. Our hierarchical struc-\\nture provides the ﬂexibility to balance the generated music\\nquality and computational costs given practical application\\nconsiderations.\\n\\n3.1. Data Representations\\n\\nDuring inference, the input to our proposed D2M-GAN\\ncomes from two domains: the visual frames of the dance\\nvideos and the inferred human body motion of the dancers.\\nThe ground-truth audio is also used as the supervision for\\nthe discriminators during the training stage. For the human\\nbody motion, several different data representations, such as\\nthe 3D Skinned Multi-Person Linear model (SMPL) [41]\\nor 2D body keypoints [5, 6] can be employed in our frame-\\nwork. We use SMPL and 2D body keypoints for different\\ndatasets in our experiments. To encode the visual frames,\\nwe extract I3D features [7] using a model pre-trained on\\nKinectics [30]. For the musical data, we adopt quantized\\nvectors as the intermediate audio representation.\\nIn or-\\nder to leverage the strong representation ability of code-\\nbooks trained on a large-scale musical dataset, we use the\\ncodebooks from a pre-trained JukeBox [9] model, which is\\ntrained on a dataset of 1.2 million songs.\\n\\n3.2. Generator\\n\\nThe generator G = {Gm, Gv, Gvq} includes the motion\\nmodule Gm, the visual module Gv, and the principal VQ\\ngenerator Gvq in the VQ module, which takes the fused\\nmotion-visual data as input and outputs the desired VQ au-\\ndio representations.\\n\\nfvq = Gvq(Gm(xm), Gv(xv)) = G(xm, xv),\\n\\n(1)\\n\\nwhere xm and xv represent the motion and visual input\\ndata, respectively. fvq is the output VQ representations. All\\nthese modules are implemented as convolution-based feed-\\nforward networks. For the principal VQ generator, we use\\nleaky rectiﬁed activation functions [65] for its hidden lay-\\ners and a tanh activation for its last layer before output to\\npromote the stability of GAN-based training [50].\\n\\n3\\n\\n\\x0cFigure 2. Overview of the proposed architecture of the D2M-GAN. Our model takes the motion and visual data from the dance videos as\\ninput and process them with the motion and visual modules, respectively. It then forwards the fused representation containing information\\nfrom both modalities to ground the generation of audio VQ-based representations with the VQ module. The resulting features are calibrated\\nby a multi-scale GAN-based discriminator and are used to perform a lookup in the pre-learned codebook. Last, the retrieved codebook\\nentries are decoded to raw musical samples via by a pre-trained and ﬁne-tuned decoder, responsible for synthesizing music.\\n\\nIt is also worth noting that we ﬁnd that using batch nor-\\nmalization and the aforementioned activation function de-\\nsigns [42, 50, 55] is crucial for a stable GAN training in our\\nframework. However, the application of the tanh activa-\\ntion will also restrict the output VQ representations within\\nthe data range between −1 and +1. We choose to scale ac-\\ntivation after the last tanh activation by multiplying by a\\nfactor σ. The hyper-parameter σ enlarges the data range\\nof VQ output and makes it possible to perform the lookup\\nof pre-learned large-scale codebooks LookUp(f ′\\nvq) with\\nf ′\\nvq = σfvq. Another signiﬁcant observation regarding the\\ngenerator’s design is using a wide receptive ﬁeld. Music has\\nlong temporal dependencies and correlations compared to\\nimages, therefore, the principal VQ generator with a larger\\nreceptive ﬁeld is beneﬁcial for generating music samples\\nwith better quality, which is consistent with the ﬁndings\\nfrom previous works [11, 35]. To this end, we design our\\ngenerator with relatively large kernel sizes in the convolu-\\ntional layers, and we also add residual blocks with dilations\\nafter the convolutional layers. All previously described sub-\\nmodules within our generator G are jointly optimized.\\n\\n3.3. Multi-Scale Discriminator\\n\\nSimilar\\n\\nthe discriminator\\n\\nto the generator,\\n\\nin the\\nD2M-GAN is also expected to capture the long-term\\ndependencies of musical sig-\\nnals encoded in the gener-\\nated sequence of VQ fea-\\ntures. However, unlike the\\ngenerator design, which fo-\\ncuses on increasing the re-\\nceptive ﬁelds of\\nthe neu-\\nral networks, we address\\nthis problem in the discrim-\\ninator design by using a\\nmulti-scale architecture. The\\nmulti-scale discriminator de-\\nsign has been studied in pre-\\nvious works within the ﬁeld\\nof audio synthesis and gener-\\nation [33, 35, 61].\\n\\nIllustration of the\\nFigure 3.\\nimportant\\nreshape operation\\nand the window-based dis-\\ncriminator for our D2M-GAN.\\n\\nThe discriminator D = {D1, D2, D3} in the VQ module\\nof our D2M-GAN is composed of 3 discriminators that op-\\nerate on the sequence of generated VQ representations and\\n\\n4\\n\\n···Motion InputVisual InputMotion EncoderI3D modelsfused motion-visualfeaturesLow-LevelVQ GeneratorHigh-LevelVQ GeneratorGeneratedVQD1D2D3Low-LevelMulti-scaleDiscriminatorGround TruthVQFine-tuned JukeBoxEncoderFine-tuned JukeBoxDecoderCodebook Lookup79k12kK+19N······GT AudioGenerated AudioOmitted due tosimilar pipeline with low level···1. Motion Module2. Visual Module3. VQ Module4. Synthesis ModulereshapetimeD1 with window-based objective\\x0cits downsampled features by a factor of 2 and 4, respec-\\ntively. Speciﬁcally, unlike the multi-scale discriminators\\nproposed in previous works that directly take the raw audio\\nas input, we reshape the VQ representations f ′\\nvq along the\\ntemporal dimension before feeding them into the discrim-\\ninators, which is also important for D2M-GAN to reach a\\nstable adversarial training, as music is a temporal audio se-\\nquence. Finally, we use the window-based objectives [35]\\n(Markovian window-based discriminator analog to image\\nInstead of learning to distinguish the\\npatches in [27]).\\ndistributions between two entire sequences, window-based\\nobjective learns to classify between distributions of small\\nchunks of VQ sequences to further enhance the overall co-\\nherence as illustrated in Figure 3.\\n\\n3.4. Lookup and Synthesis\\n\\nAfter generating the VQ representations, we perform a\\ncodebook lookup operation similar to other VQ-based gen-\\nerative models [9, 14, 45, 52] to retrieve the closest corre-\\nsponding entries.\\n\\nFinally, we ﬁne-tune the decoder from the JukeBox [9]\\nwithout modifying the codebook entries as the music syn-\\nthesizer for our learned VQ representations. Speciﬁcally,\\nwe also adopt the GAN-based technique for ﬁne-tuning the\\nmusic synthesizer, where the generator is replaced by the\\ndecoder of JukeBox and the discriminator follows the simi-\\nlar architecture as described in the previous subsection.\\n\\n3.5. Training Objectives\\n\\nGAN Loss. We use the hinge loss version of GAN objec-\\ntive [40, 43] adopted for our music generation task to train\\nthe proposed D2M-GAN.\\n\\nLadv.(D; G) = ∑\\nk\\n\\nLadv.(Dk; G)\\n\\n(Eφ(xa)[min(0, 1 − Dk(φ(xa)))]\\n\\n= ∑\\nk\\n\\n+ E(xm,xv)[min(0, 1 + Dk(G(xm, xv)))]),\\n−Dk(G(xm, xv))],\\n\\nLadv.(G; D) = Exm,xv [∑\\n\\n(2)\\n\\n(3)\\n\\nk\\n\\nwhere xa is the original music in a waveform, φ represents\\nthe ﬁne-tuned encoder from JukeBox [9]. k indicates the\\nnumber of multi-scale discriminators, which is empirically\\nchosen to be 3 in our case.\\nFeature Matching Loss. To encourage the construction\\nof subtle details in audio signals, we also include a feature\\nmatching loss [36] in the overall training objective. Similar\\nto the audio generation works [33,35], the feature matching\\nloss is deﬁned as the L1 distance between the discriminator\\nfeature maps of the real and generated VQ features.\\n\\nLF M (G; D) =\\n1\\nNi\\n\\nE(xm,xv)[\\n\\nT\\n∑\\ni=1\\n\\n∥Di\\n\\n(φ(xa)) − Di\\n\\n(G(xm, xv))∥1].\\n\\n(4)\\n\\nCodebook Commitment Loss. The codebook commit-\\nment loss [45, 52] is deﬁned as the L1 distance between\\nthe generated VQ features and the corresponding codebook\\nentries of the ground truth VQ features after the codebook\\nlookup process.\\n\\nLcode(G) = E(xm,xv)[∥LookU p(φ(xa) − G(xm, xv)∥1].(5)\\n\\nAudio Perceptual Losses. To further improve the percep-\\ntual auditory quality, we consider the perception losses of\\nthe raw audio signals from both time and frequency do-\\nmains. Speciﬁcally, the perceptual losses are calculated as\\nthe L1 distance between the original audio and the gener-\\nated audio samples:\\n\\nLwav(G) = E(xm,xv)[∥xa − G(xm, xv)∥1].\\nLM el(G) = E(xm,xv)[∥θ(xa) − θ(G(xm, xv))∥1].\\nwhere θ is the function to compute the mel-spectrogram fea-\\ntures for the audio signal waveforms.\\nFinal Loss. The ﬁnal training objective for the entire gen-\\nerator module is deﬁned as follows:\\n\\n(6)\\n\\n(7)\\n\\nLG = Ladv.(G; D) + λf mLF M (G; D)\\n+ λcLcode + λwLwav + λmLmel,\\n\\n(8)\\n\\nwhere the λf m, λc, λa, and λmel are set to be 3, 15, 40 and\\n15, respectively during our experiments for both levels.\\n\\n4. Experiments\\n\\n4.1. Experimental Setup\\n\\nDatasets. We validate the effectiveness of our method by\\nconducting experiments on two datasets with paired dance\\nvideo and music: the AIST++ [39] and our proposed Tik-\\nTok dance-music dataset. The AIST++ dataset [39] is a\\nsubset of AIST dataset [60] with 3D motion annotations.\\nWe adopt the ofﬁcial cross-modality data splits for training,\\nvalidation, and testing, where the videos are divided with-\\nout overlapping musical pieces between the training and the\\nvalidation/testing sets. The number of videos in each split is\\n980, 20, and 20, respectively. The videos from this dataset\\nare ﬁlmed in professional studios with clean backgrounds.\\nThere are in total 10 different dance genres and correspond-\\ning music styles, which include breakdancing, pop, lock,\\netc. The number of total songs is 60, with 6 songs for each\\ntype of music. We use this dataset for the main experiments\\nand evaluations.\\n\\nWe also collect and annotate a TikTok dance-music\\ndataset which contains 445 dance videos, with an aver-\\nage length of 12.5 seconds. This dataset contains 85 dif-\\nferent songs, with the majority of videos having a single\\ndance performer, and a maximum of ﬁve performers. The\\ntraining-testing splits contain 392 and 53 videos, respec-\\ntively, without overlapping songs. Figure 4 shows example\\n\\n5\\n\\n\\x0cthis method is also monotonic in terms of the musical instru-\\nment. Controllable Music Transformer (CMT) [10]: CMT\\nis a Transformer-based model proposed for video back-\\nground music generation using MIDI representation. In ad-\\ndition to the above cross-modality models that are closely\\nrelated to our work, we also consider Ground Truth: GT\\nsamples are the original music from dance videos. Juke-\\nBox [9]: music samples generated or reconstructed via the\\nJukeBox model.\\n\\n4.2. Music Evaluations\\n\\nWe design a comprehensive evaluation protocol that in-\\ncorporates objective (i.e., metrics that can be automati-\\ncally calculated) and subjective (i.e., scores given by hu-\\nman testers) metrics to evaluate the generated music from\\nvarious perspectives. Speciﬁcally, the evaluations are di-\\nvided into two categories: the ﬁrst category, which is also\\nthe focus of our work, measures correlations between the\\ngenerated music and the input dance videos, for which we\\ncompare our proposed model with other cross-modality mu-\\nsic generation works [1, 10, 16] and a random baseline from\\nJukeBox [9]. The second category focuses on the quality\\nof the music in general, for which we use the reconstructed\\nsamples using JukeBox [9] given the original audio as input\\nand GT samples for comparisons.\\nRhythm. Musical rhythm accounts for an important char-\\nacteristic of the generated music samples, especially given\\nthe dance video as input. To evaluate the correspondence\\nbetween the dance beats and generated musical rhythm, we\\nadopt two objective scores as evaluation metrics, which are\\nthe Beats Coverage Scores and the Beats Hit Scores sim-\\nilar to [8, 37]. Previous works [8, 37] have demonstrated\\nthe kinematic dance and musical beats (i.e., rhythm) are\\ngenerally aligned, we can therefore reasonably evaluate the\\nmusical rhythm by comparing the beats from the generated\\nmusic and those from the GT music samples as shown in\\nFig. 5. We detect the musical beats by the second-level on-\\nset strength [13], which can be considered as the start of an\\nacoustic event. We deﬁne the number of detected beats from\\nthe generated music samples as Bg, the total beats from the\\noriginal music as Bt, and the number of aligned beats from\\nthe generative samples as Ba. The Beats Coverage Scores\\nBg/Bt measure the ratio of overall generated beats to the\\ntotal musical beats. The Beats Hit Scores Ba/Bt measure\\nthe ratio of aligned beats to the total musical beats. The\\nquantitative results are presented in Table 1. We observe\\nthat both levels of our proposed D2M-GAN achieve better\\nscores compared to competing methods.\\nGenre and Diversity. Dance and music are both diverse\\nin terms of genres. The generated music samples are ex-\\npected to be diverse and harmonious with the given dance\\nstyle (e.g., breakdancing with strong beats paired with mu-\\nsic in fast rhythm). Therefore, we calculate the genre ac-\\n\\nFigure 4. Examples of dance videos from our TikTok dance-\\nmusic dataset. Unlike the AIST dataset [60] where dancing is\\nperformed by professional dancers in a studio environment, our\\ndataset consists of real-world videos collected “in the wild”.\\n\\nframes of the dance videos and makes apparent the key dif-\\nferences compared to the professional studio ﬁlmed dance\\nvideo from AIST [60]. Our videos have wildly different\\nbackgrounds, and often contain incomplete human body\\nskeleton data, which signiﬁcantly increases the challenge\\nof learning from this dataset. For the TikTok music dataset,\\nwe use 2D human skeleton data as the underlying motion\\nrepresentation.\\nImplementation Details. For the presented experiments,\\nwe adopt a sampling rate of 22.5 kHz for all audio sig-\\nnals. We use the video and audio segments in the length\\nof 2 seconds for training and standard testing in the main\\nexperiments. The generation of longer sequences is also in-\\nvestigated in Section 4.3. The hop lengths for the high and\\nlow level are 128 and 32, respectively. During the GAN\\ntraining, we adopt the Adam optimizer with a learning rate\\nof 1e-4 with β1 = 0.5 and β2 = 0.9 for the generators and\\ndiscriminators. We deﬁne the scaling factor σ = 100 for the\\nVQ generators. The number of discriminators k is 3 for the\\nmulti-scale structure. The batch size is set to be 16 for all\\nexperiments. During the ﬁne-tuning of the JukeBox synthe-\\nsizer, we use the Adam optimizer with a learning rate of 1e-\\n5 with β1 = 0.5 and β2 = 0.9 for the synthesizer and multi-\\nscale discriminators. We perform a denoising process [54]\\non the generated raw music data for better audio quality.\\nComparisons. We compare our proposed method with sev-\\neral baselines. Foley Music [16]: Foley Music model gener-\\nates MIDI musical representations based on keypoints mo-\\ntion data and then converts the MIDI back to a raw wave-\\nform using a pre-deﬁned MIDI synthesizer. Speciﬁcally,\\nthe MIDI audio representation is unique for each musical\\ninstrument, and therefore the Foley music model can only\\ngenerate musical samples with mono-instrumental sound.\\nDance2Music [1]: Similar to [16], the generated music with\\n\\n6\\n\\n\\x0cCategory\\n\\nFeatures\\n\\nType\\n\\nMetric\\n\\nDance-Music\\n\\nRhythm\\n\\nObj.\\n\\nBeats Coverage\\n&\\nBeats Hit\\n\\nDance-Music Genre&Diversity\\n\\nObj.\\n\\nGenre Accuracy\\n(Retrieval-based)\\n\\nDance-Music\\n\\nCoherence\\n\\nSubj. Mean Opinion Scores\\n\\nMusic\\n\\nOverall quality\\n\\nSubj. Mean Opinion Scores\\n\\nMethods\\nDance2Music [1]\\nFoley Music [16]\\nCMT [10]\\nOurs High-level\\nOurs Low-level\\n\\nDance2Music [1]\\nFoley Music [16]\\nCMT [10]\\nOurs High-level\\nOurs Low-level\\n\\nRandom JukeBox [9]\\nDance2Music [1]\\nFoley Music [16]\\nCMT [10]\\nOurs High-level\\nOurs Low-level\\nGT\\n\\nJukeBox [9]\\nOurs High-level\\nOurs Low-level\\nGT\\n\\nScores\\n83.5 & 82.4\\n74.1 & 69.4\\n85.5 & 83.5\\n88.2 & 84.7\\n92.3 & 91.7\\n\\n7.0\\n8.1\\n11.6\\n24.4\\n26.7\\n\\n2.0\\n2.8\\n2.8\\n3.0\\n3.5\\n3.3\\n4.6\\n\\n3.5\\n3.5\\n3.7\\n4.8\\n\\nTable 1. Evaluation protocol and the corresponding results for the experiments on the AIST++ dataset [39]. Obj. stands for Objective,\\nwhich means the scores are automatically calculated. Subj. stands for Subjective, which means the scores are given by human evaluators.\\n\\nthe retrieved musical sample has the same genre as the given\\ndance style, we consider the segment to be genre accurate.\\nThe genre accuracy is then calculated by Sc/St, where Sc\\ncounts the number of genre accurate segments and St is the\\ntotal number of segments from the testing split.\\n\\nWe observe in Table 1 that the genre accuracy scores\\nof our D2M-GAN are considerably higher compared to the\\ncompeting methods. This is due to the reason that the\\ncompeting methods rely on MIDI events as audio repre-\\nsentations, which require a speciﬁc synthesizer for each in-\\nstrument, and thus can only generate music samples with\\nmono-instrumental sound.\\nIn contrast, our generated VQ\\naudio representations can represent complex dance mu-\\nsic similar to the input music types, which helps to in-\\ncrease the diversity of the generated music samples. It also\\nmakes the generated samples to be more harmonious with\\nthe dance videos compared to acoustic instrumental sounds\\nfrom [1,10,16], as shown in the next evaluation protocol for\\nthe coherence test.\\nCoherence. Since we generate music samples conditioned\\non the dance videos, the dance video input and the output\\nare expected to be harmonious and coherent when com-\\nbined together. Speciﬁcally, a given dance sequence could\\nbe accompanied by multiple appropriate songs. However,\\nthe evaluation of the dance-music coherence is very subjec-\\ntive, therefore we conduct the Mean Opinion Scores (MOS)\\nhuman test for assessing the coherence feature. During the\\nevaluation process, the human testers are asked to give a\\n\\nFigure 5. Qualitative example of rhythm evaluations and beat\\ncorrespondence. The lower-abstraction level model (D2M-Low)\\nappears to align better than its high-counterpart (D2M-High) with\\nthe ground-truth (GT), which is consistent with the quantitative\\nscores from the Table 1.\\n\\ncuracy for evaluating whether the generated music samples\\nhave a consistent genre with the dance style. The calcu-\\nlation of this objective metric requires the annotations of\\ndance and music genres, therefore, we use the retrieved mu-\\nsical samples from the AIST++ [39] for this evaluation set-\\nting. Speciﬁcally, we retrieve the musical samples with the\\nhighest similarity scores from the segment-level database\\nformed by original audio samples with the same sequence\\nlength. The similarities scores are deﬁned as the euclidean\\ndistance between the audio features extracted via a VGG-\\nlike network [24] pre-trained on AudioSet [22]. In case that\\n\\n7\\n\\nTimeGTD2M LowD2M High\\x0cBeats Coverage Beats Hit\\n\\nBeats Coverage Beats Hit Genre Acc.\\n\\nModels\\nHigh w/o M\\nHigh w/o V\\nHigh (full)\\nLow w/o M\\nLow w/o V\\nLow (full)\\n\\n85.5\\n86.3\\n88.4\\n83.8\\n85.2\\n87.1\\n\\n72.4\\n81.7\\n82.3\\n74.6\\n81.7\\n83.9\\n\\nTable 2. Evaluations for the experiments on the TikTok dataset.\\n\\nscore between 1 and 5 to evaluate the coherence between\\nthe dance moves and the music given a video with audio\\nsounds. The higher scores indicate the fact the tester feels\\nthe given dance and music are more coherent. We prepare\\nthe videos with original visual frames and fused generated\\nmusic samples for testing.\\nIn addition to the previously\\ncross-modality generation methods [1, 10, 16], we also in-\\nclude the GT samples and the randomly generated music\\nfrom JukeBox [9] for comparison. Our D2M-GAN achieves\\nbetter scores compared to other baselines, which validates\\nthe fact that our proposed framework is able to catch the\\ncorrelations with the given dance video and generates rather\\ncomplex music that well matches the input.\\nOverall Quality. Although our main research focus is to\\nlearn the dance-music correlations in this work, we also\\nlook at the general sound quality of the generated samples.\\nWe conduct the subjective MOS tests similar to the coher-\\nence evaluation, where the human testers are asked to give\\na score between 1 to 5 for the general quality of the music\\nsamples. During this test, only audio signals are played to\\nthe testers. The JukeBox samples are obtained by directly\\nfeeding the GT samples as input. The MOS tests show that\\nour D2M-GAN is able to generate music sample with plau-\\nsible sound quality comparable to the JukeBox. JukeBox\\nhas multiple variants with different hop lengths, we com-\\npare with samples obtained from the model with same au-\\ndio hop length for fairness (i.e., the hop lengths for our high\\nand low levels are 128 and 32, respectively.). It is worth\\nnoting that synthesizing high quality audio itself has been\\na vary challenging and computational demanding research\\ntopic, for example, it takes 3 hrs to sample a 20-seconds\\nhigh-quality music sample with a hop length of 8 [9].\\nResults on the TikTok Dance-Music Dataset. Compared\\nto the AIST++ [39], our TikTok dance-music dataset is a\\nmore challenging dataset with “in the wild” video settings\\nthat contains various occlusions and noisy backgrounds. Ta-\\nble 2 shows the quantitative evaluation results for the ex-\\nperiments on the TikTok dataset, which demonstrates the\\noverall robustness of the proposed D2M-GAN.\\n\\n4.3. Ablation Studies\\n\\nSequence Length. In the main experiments, we use the 2-\\nsecond length sequence for experiments with reference to\\n\\nLength\\nHigh - 2s\\nHigh - 3s\\nHigh - 4s\\nLow - 2s\\nLow - 3s\\nLow - 4s\\n\\nModels\\nHigh w/o M\\nHigh w/o V\\nHigh (full)\\nLow w/o M\\nLow w/o V\\nLow (full)\\n\\n88.2\\n88.2\\n87.1\\n92.3\\n90.1\\n88.2\\n\\n83.5\\n87.1\\n88.2\\n89.4\\n90.6\\n92.3\\n\\n84.7\\n85.3\\n83.0\\n91.7\\n88.2\\n84.7\\n\\n82.9\\n88.2\\n84.7\\n87.6\\n90.0\\n91.7\\n\\n24.4\\n25.6\\n23.3\\n26.7\\n25.6\\n23.3\\n\\n15.1\\n16.3\\n24.4\\n15.1\\n17.4\\n26.7\\n\\nTable 3. Results for ablation studies in terms of sequence length.\\n\\nBeats Coverage Beats Hit Genre Acc.\\n\\nTable 4. Results for ablation studies in terms of input modalities\\non the AIST++ dataset. M means the motion data, and V means\\nthe visual data.\\n\\nBeats Coverage Beats Hit Genre Acc.\\n\\nModels\\nHigh 1-layer D.\\nHigh 2-layer D.\\nHigh w/o scaling\\nHigh w/o reshape\\nHigh w/o ﬁne-tune\\nHigh (full)\\nLow 1-layer D.\\nLow 2-layer D.\\nLow w/o scaling\\nLow w/o reshape\\nLow w/o ﬁne-tune\\nLow (full)\\n\\n75.3\\n85.3\\n72.9\\n73.5\\n87.0\\n88.2\\n73.5\\n87.0\\n72.4\\n73.5\\n92.3\\n92.3\\n\\n72.9\\n82.9\\n71.8\\n70.1\\n84.7\\n84.7\\n71.8\\n85.9\\n70.1\\n71.8\\n91.2\\n91.7\\n\\n9.3\\n21.0\\n14.0\\n11.6\\n24.4\\n24.4\\n8.1\\n22.1\\n12.8\\n12.8\\n26.7\\n26.7\\n\\nTable 5. Results for ablation studies in terms of model architec-\\ntures on the AIST++ dataset. D. means discriminators.\\n\\nother similar cross-modality generation tasks [39]. How-\\never, our model can also be effectively trained and tested\\nwith a longer sequence length as shown in Table 3 via a\\nrelatively larger network with more parameters.\\nData Modality. We perform ablation studies in terms of the\\ninput data modalities, by removing either the dance motion\\nor the visual frame from the input data. Table 4 lists the\\ncorresponding experimental results. We observe that both\\nmotion and visual data contribute to our conditioned mu-\\nsic generation task. Speciﬁcally, the motion data impose\\na larger impact on the musical rhythm, which is consistent\\nwith our expectations since the musical rhythm is closely\\ncorrelated with the dance motion.\\nModel Architecture. We also test various variants of our\\nD2M-GAN in terms of the model architecture and proposed\\nmodel design techniques. The corresponding results are\\nrepresented in Tabel 5. The experimental results show that\\nthe multi-scale layer for the discriminators, the scaling op-\\neration in the generator, as well as the reshape techniques\\nfor discriminators are crucial.\\n\\n8\\n\\n\\x0cBeats Coverage Beats Hit Genre Acc.\\n\\nReferences\\n\\nLosses\\nHigh w/o LF M\\nHigh w/o Lwav\\nHigh w/o Lmel\\nHigh (full)\\nLow w/o LF M\\nLow w/o Lwav\\nLow w/o Lmel\\nLow (full)\\n\\n85.3\\n85.9\\n77.6\\n88.2\\n91.7\\n89.4\\n78.8\\n92.3\\n\\n84.7\\n84.7\\n76.5\\n84.7\\n90.1\\n88.8\\n77.1\\n91.7\\n\\n23.3\\n23.3\\n18.6\\n24.4\\n24.4\\n23.3\\n17.4\\n26.7\\n\\nTable 6. Results for ablation studies in terms of losses on the\\nAIST++ dataset.\\n\\nLoss function. We analyze the impact of different losses\\nincluded in the overall training objective. The results from\\nTable 6 show the contributions of each loss term. Specif-\\nically, we observe the audio perceptual loss from the fre-\\nquency domain Lmel helps with the generation of musical\\nrhythm, it is reasonable due to the fact that mel-spectrogram\\nfeatures help to capture the high frequencies from the audio\\nsignals, which is closely related to the dance beats.\\n\\n5. Conclusion and Discussion\\n\\nWe propose D2M-GAN framework for complex music\\ngeneration from dance videos via the VQ audio representa-\\ntions. Extensive experiments on multiple datasets, and com-\\nprehensive evaluations in terms of various musical charac-\\nteristics prove the effectiveness of our method. We also in-\\ntroduce a novel TikTok dance-music dataset in this work.\\n\\nAs for limitations, the proposed D2M-GAN is an end-to-\\nend framework, an interesting future direction is to explore\\nhow one can use controllable conditioning information to\\npromote an interactively editing/generating system.\\n\\n[1] Gunjan Aggarwal and Devi Parikh. Dance2music: Au-\\narXiv preprint\\n\\ntomatic dance-driven music generation.\\narXiv:2107.06252, 2021. 1, 6, 7, 8\\n\\n[2] Relja Arandjelovic and Andrew Zisserman. Look, listen and\\n\\nlearn. In ICCV, 2017. 2\\n\\n[3] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Sound-\\nnet: Learning sound representations from unlabeled video.\\nNeurIPS, 2016. 2\\n\\n[4] Jean-Pierre Briot, Ga¨etan Hadjeres, and Franc¸ois-David Pa-\\nchet. Deep learning techniques for music generation, vol-\\nume 1. Springer, 2020. 1\\n\\n[5] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.\\nSheikh. Openpose: Realtime multi-person 2d pose estima-\\ntion using part afﬁnity ﬁelds. IEEE TPAMI, 2019. 3\\n\\n[6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\\nRealtime multi-person 2d pose estimation using part afﬁnity\\nﬁelds. In CVPR, 2017. 3\\n\\n[7] Joao Carreira and Andrew Zisserman. Quo vadis, action\\nrecognition? a new model and the kinetics dataset. In CVPR,\\n2017. 3\\n\\n[8] Abe Davis and Maneesh Agrawala. Visual rhythm and beat.\\n\\nIn ACM Transactions on Graphics (TOG), 2018. 6\\n\\n[9] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook\\nKim, Alec Radford, and Ilya Sutskever. Jukebox: A gen-\\nerative model for music. arXiv preprint arXiv:2005.00341,\\n2020. 1, 2, 3, 5, 6, 7, 8, 12\\n\\n[10] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan\\nZhu, Zexin He, Hongming Liu, and Shuicheng Yan. Video\\nbackground music generation with controllable music trans-\\nformer. In ACMMM, 2021. 3, 6, 7, 8\\n\\n[11] Chris Donahue, Julian McAuley, and Miller Puckette. Ad-\\n\\nversarial audio synthesis. In ICLR, 2019. 4\\n\\n[12] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan\\nYang. Musegan: Multi-track sequential generative adversar-\\nial networks for symbolic music generation and accompani-\\nment. In AAAI, 2018. 2, 3\\n\\n[13] Daniel PW Ellis. Beat tracking by dynamic programming.\\n\\nJournal of New Music Research, 2007. 6\\n\\n[14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\\ntransformers for high-resolution image synthesis. In CVPR,\\n2021. 2, 3, 5\\n\\n[15] Joao P Ferreira, Thiago M Coutinho, Thiago L Gomes,\\nJos´e F Neto, Rafael Azevedo, Renato Martins, and Erick-\\nson R Nascimento. Learning to dance: A graph convolu-\\ntional adversarial network to generate realistic dance mo-\\ntions from audio. Computers & Graphics, 2021. 2\\n\\n[16] Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenen-\\nbaum, and Antonio Torralba. Foley music: Learning to gen-\\nerate music from videos. In ECCV, 2020. 2, 3, 6, 7, 8\\n[17] Chuang Gan, Deng Huang, Hang Zhao, Joshua B Tenen-\\nbaum, and Antonio Torralba. Music gesture for visual sound\\nseparation. In CVPR, 2020. 2\\n\\n[18] Ruohan Gao, Rogerio Feris, and Kristen Grauman. Learning\\nto separate object sounds by watching unlabeled video. In\\nECCV, 2018. 2\\n\\n9\\n\\n\\x0c[19] Ruohan Gao and Kristen Grauman. 2.5 d visual sound. In\\n\\nCVPR, 2019. 2\\n\\n[20] Ruohan Gao and Kristen Grauman. Co-separating sounds of\\n\\nvisual objects. In ICCV, 2019. 2\\n\\n[21] Ruohan Gao, Tae-Hyun Oh, Kristen Grauman, and Lorenzo\\nTorresani. Listen to look: Action recognition by previewing\\naudio. In CVPR, 2020. 2\\n\\n[22] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren\\nJansen, Wade Lawrence, R Channing Moore, Manoj Plakal,\\nand Marvin Ritter. Audio set: An ontology and human-\\nIn ICASSP. IEEE, 2017.\\nlabeled dataset for audio events.\\n7\\n\\n[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\nYoshua Bengio. Generative adversarial nets. In NIPS, 2014.\\n3\\n\\n[24] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F\\nGemmeke, Aren Jansen, R Channing Moore, Manoj Plakal,\\nDevin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn ar-\\nchitectures for large-scale audio classiﬁcation. In ICASSP.\\nIEEE, 2017. 7\\n\\n[25] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit,\\nNoam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M\\nDai, Matthew D Hoffman, Monica Dinculescu, and Douglas\\nEck. Music transformer: Generating music with long-term\\nstructure. In ICLR, 2019. 2, 3\\n\\n[26] Vladimir Iashin and Esa Rahtu. Taming visually guided\\nIn British Machine Vision Conference\\n\\nsound generation.\\n(BMVC), 2021. 3\\n\\n[27] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\\nImage-to-image translation with conditional adver-\\n\\nEfros.\\nsarial networks. In CVPR, 2017. 5\\n\\n[28] Shulei Ji, Jing Luo, and Xinyu Yang. A comprehensive sur-\\nvey on deep music generation: Multi-level representations,\\nalgorithms, evaluations, and future directions. arXiv preprint\\narXiv:2011.06801, 2020. 1\\n\\n[29] Hsuan-Kai Kao and Li Su. Temporally guided music-to-\\n\\nbody-movement generation. In ACMMM, 2020. 2\\n\\n[30] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\\nman action video dataset. arXiv preprint arXiv:1705.06950,\\n2017. 3\\n\\n[31] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and\\nDima Damen. Epic-fusion: Audio-visual temporal binding\\nfor egocentric action recognition. In ICCV, 2019. 2\\n\\n[32] Diederik P Kingma and Max Welling. Auto-encoding varia-\\n\\ntional bayes. In ICLR, 2014. 3\\n\\n[33] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hiﬁ-gan:\\nGenerative adversarial networks for efﬁcient and high ﬁ-\\ndelity speech synthesis. In NIPS, 2020. 4, 5\\n\\n[34] Bruno Korbar, Du Tran, and Lorenzo Torresani. Coopera-\\ntive learning of audio and video models from self-supervised\\nsynchronization. In NeurIPS, 2018. 2\\n\\n[35] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lu-\\ncas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de\\n\\nBr´ebisson, Yoshua Bengio, and Aaron C Courville. Mel-\\ngan: Generative adversarial networks for conditional wave-\\nform synthesis. In NIPS, 2019. 3, 4, 5\\n\\n[36] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo\\nLarochelle, and Ole Winther. Autoencoding beyond pixels\\nusing a learned similarity metric. In International conference\\non machine learning. PMLR, 2016. 5\\n\\n[37] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun\\nWang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz.\\nDancing to music. In NIPS, 2019. 1, 2, 6, 13\\n\\n[38] Buyu Li, Yongchi Zhao, and Lu Sheng. Dancenet3d: Music\\nbased dance generation with parametric motion transformer.\\narXiv preprint arXiv:2103.10206, 2021. 1, 2\\n\\n[39] Ruilong Li, Shan Yang, David A. Ross, and Angjoo\\nKanazawa. Ai choreographer: Music conditioned 3d dance\\ngeneration with aist++. In ICCV, 2021. 1, 2, 5, 7, 8, 13\\n[40] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv\\n\\npreprint arXiv:1705.02894, 2017. 5\\n\\n[41] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\\nmulti-person linear model. ACM Trans. Graphics (Proc.\\nSIGGRAPH Asia), 34, 2015. 3\\n\\n[42] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain\\nGelly, and Olivier Bousquet. Are gans created equal? a\\nlarge-scale study. In NeurIPS, 2018. 4\\n\\n[43] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and\\nYuichi Yoshida. Spectral normalization for generative ad-\\nversarial networks. arXiv preprint:1802.05957, 2018. 5\\n[44] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen\\nSimonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,\\nAndrew Senior, and Koray Kavukcuoglu. Wavenet: A gen-\\nerative model for raw audio. In ICLR, 2016. 3\\n[45] Aaron van den Oord, Oriol Vinyals,\\n\\nand Koray\\nNeural discrete representation learning.\\n\\nKavukcuoglu.\\nIn NIPS, 2017. 2, 3, 5, 12\\n\\n[46] Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck,\\nand Karen Simonyan. This time with feeling: Learning ex-\\npressive musical performance. Neural Computing and Ap-\\nplications, pages 955–967, 2020. 2, 3\\n\\n[47] Andrew Owens and Alexei A Efros. Audio-visual scene\\nanalysis with self-supervised multisensory features.\\nIn\\nECCV, 2018. 2\\n\\n[48] Andrew Owens, Jiajun Wu, Josh H McDermott, William T\\nFreeman, and Antonio Torralba. Ambient sound provides\\nsupervision for visual learning. In ECCV, 2016. 2\\n\\n[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-\\nperative style, high-performance deep learning library. In H.\\nWallach, H. Larochelle, A. Beygelzimer, F. d ´Alch´e-Buc, E.\\nFox, and R. Garnett, editors, Advances in Neural Informa-\\ntion Processing Systems 32, pages 8024–8035. Curran Asso-\\nciates, Inc., 2019. 12\\n\\n10\\n\\n\\x0c[67] Ye Zhu, Yu Wu, Hugo Latapie, Yi Yang, and Yan Yan. Learn-\\ning audio-visual correlations from variational cross-modal\\ngeneration. In ICCASP, 2021. 2\\n\\n[68] Wenlin Zhuang, Congyi Wang, Siyu Xia, Jinxiang Chai, and\\nYangang Wang. Music2dance: Dancenet for music-driven\\ndance generation. arXiv preprint arXiv:2002.03761, 2020. 2\\n\\n[50] Alec Radford, Luke Metz, and Soumith Chintala. Un-\\nsupervised representation learning with deep convolu-\\narXiv preprint\\ntional generative adversarial networks.\\narXiv:1511.06434, 2015. 3, 4\\n\\n[51] Tanzila Rahman, Bicheng Xu, and Leonid Sigal. Watch,\\nlisten and tell: Multi-modal weakly supervised dense event\\ncaptioning. In ICCV, 2019. 2\\n\\n[52] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Gen-\\nerating diverse high-ﬁdelity images with vq-vae-2. In NIPS,\\n2019. 2, 3, 5\\n\\n[53] Xuanchi Ren, Haoran Li, Zijian Huang, and Qifeng Chen.\\nSelf-supervised dance video synthesis conditioned on music.\\nIn ACM MM, 2020. 1, 2\\n\\n[54] Tim Sainburg, Marvin Thielk, and Timothy Q Gentner. Find-\\ning, visualizing, and quantifying latent structure across di-\\nverse animal vocal repertoires. PLoS computational biology,\\n16(10):e1008228, 2020. 6\\n\\n[55] Tim Salimans and Durk P Kingma. Weight normalization:\\nA simple reparameterization to accelerate training of deep\\nneural networks. In NeurIPS, 2016. 4\\n\\n[56] Eli Shlizerman, Lucio Dery, Hayden Schoen, and Ira\\nKemelmacher-Shlizerman. Audio to body dynamics.\\nIn\\nCVPR, 2018. 1, 2\\n\\n[57] Kun Su, Xiulong Liu, and Eli Shlizerman. Audeo: Audio\\ngeneration for a silent performance video. In NeurIPS, 2020.\\n2\\n\\n[58] Taoran Tang, Jia Jia, and Hanyang Mao. Dance with melody:\\nAn lstm-autoencoder approach to music-oriented dance syn-\\nthesis. In ACMMM, 2018. 2\\n\\n[59] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chen-\\nliang Xu. Audio-visual event localization in unconstrained\\nvideos. In ECCV, 2018. 2\\n\\n[60] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki,\\nand Masataka Goto. Aist dance video database: Multi-genre,\\nmulti-dancer, and multi-camera database for dance informa-\\ntion processing. In Proceedings of the 20th International So-\\nciety for Music Information Retrieval Conference, (ISMIR),\\n2019. 2, 5, 6, 13\\n\\n[61] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,\\nJan Kautz, and Bryan Catanzaro. High-resolution image syn-\\nthesis and semantic manipulation with conditional gans. In\\nCVPR, 2018. 4\\n\\n[62] Xin Wang, Yuan-Fang Wang, and William Yang Wang.\\nWatch, listen, and describe: Globally and locally aligned\\nIn NAACL,\\ncross-modal attentions for video captioning.\\n2018. 2\\n\\n[63] Yu Wu and Yi Yang. Exploring heterogeneous clues for\\nIn CVPR,\\n\\nweakly-supervised audio-visual video parsing.\\n2021. 2\\n\\n[64] Yu Wu, Linchao Zhu, Yan Yan, and Yi Yang. Dual attention\\nmatching for audio-visual event localization. In ICCV, 2019.\\n2\\n\\n[65] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical\\nevaluation of rectiﬁed activations in convolutional network.\\narXiv preprint arXiv:1505.00853, 2015. 3\\n\\n[66] Hang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Tor-\\n\\nralba. The sound of motions. In ICCV, 2019. 2, 3\\n\\n11\\n\\n\\x0cA. Network Architecture\\n\\nA.1. Generator\\n\\nThe following Table 7, Table 8, Table 9 and Table 10\\nshow the detailed model architectures of the motion en-\\ncoder, high-level VQ generator, low-level VQ generator and\\nthe residual block, respectively.\\n\\n6 × 1, stride=1, Conv 256, LeakyReLU\\nResidual Stack 256\\n3 × 1, stride=1, Conv 512, LeakyReLU\\nResidual Stack 512\\n3 × 1, stride=1, Conv 1024, LeakyReLU\\nResidual Stack 1024\\n3 × 1 , stride=1, Conv 1024, LeakyReLU\\n4 × 1, stride=1, Conv 1\\n\\nTable 7. Architecture for the motion encoder.\\n\\n6 × 1, stride=2, Conv 32, LeaklyReLU\\nResidual Stack 32\\n41 × 1, stride=2, Conv 64, LeakyReLU\\nResidual Stack 64\\n41 × 1, stride=1, Conv 128, LeakyReLU\\nResidual Stack 128\\n41 × 1, stride=1, Conv 256, LeaklyReLU\\nResidual Stack 256\\n41 × 1, stride=1, Conv 512, LeakyReLU\\nResidual Stack 512\\n40 × 1, stride=1, Conv 64\\nTanh()\\n\\nTable 8. Architecture for the high-level VQ generator.\\n\\nA.2. Discriminator\\n\\nWe adopt the multi-scale discriminator design for the\\nproposed D2M-GAN, where is formed by a stack of 3\\ndiscriminator blocks that operates on the original VQ\\nsequence, and its downsampled features based on the\\nwindow-based objective functions as introduced in the main\\npaper. The architecture of each discriminator block is\\nshown below in Table 11.\\n\\nB. Experimental Details\\n\\nWe implement\\n\\nthe entire framework using the Py-\\nTorch [49] framework for automatic differentiation and\\nGPU-accelerated training and inference.\\nPre-learned Codebook. We adopt two independently pre-\\ntrained codebooks for two levels in our D2M-GAN. Speciﬁ-\\ncally, the original JukeBox [9] contains three levels of VQ-\\n\\n12\\n\\n6 × 1, stride=2, Conv 32, LeakyReLU\\nResidual Stack 32\\n4 × 1, stride=1, Conv 64, LeakyReLU\\nResidual Stack 64\\n40 × 1, stride=2, Conv 128, LeakyReLU\\nResidual Stack 128\\n40 × 1, stride=1, Conv 256, LeakyReLU\\nResidual Stack 256\\n40 × 1, stride=1, Conv 512, LeakyReLU\\nResidual Stack 512\\n40 × 1, stride=1, Conv 1024, LeakyReLU\\nResidual Stack 1024\\n40 × 1, stride=1, Conv 1024, LeakyReLU\\n40 × 1, stride=1, Conv 64, LeakyReLU\\nTanh()\\n\\nTable 9. Architecture for the low-level VQ generator.\\n\\nLeakyReLU, dilation=1, Conv\\nLeakyReLU, dilation=1. Conv\\nShortcut Path\\nLeakyReLU, dilation=3, Conv\\nLeakyReLU, dilation=1, Conv\\nShortcut Path\\nLeakyReLU, dilation=9, Conv\\nLeakyReLU, dilation=1, Conv\\nShortcut Path\\n\\nTable 10. Architecture for the residual stack.\\n\\n15 × 1, stride=1, Conv 16, LeakyReLU\\n41 × 1, stride=4, Groups=4, Conv 64, LeakyReLU\\n41 × 1, stride=4, Groups=16, Conv 256, LeakyReLU\\n41 × 1, stride=4, Groups=64, Conv 1024, LeakyReLU\\n41 × 1, stride=4, Groups=256, Conv 1024, LeakyReLU\\n5 × 1, stride=1, Conv 1024, LeakyReLU\\n3 × 1, stride=1, Conv 1\\n\\nTable 11. Architecture for the discriminator block.\\n\\nVAE [45] based models, which are deﬁned as top, middle\\nand bottom levels with hop lengths of 128, 32, and 8, re-\\nspectively. We adopt the top level codebook for the high-\\nlevel D2M-GAN, and the middle level codebook for the\\nlow-level D2M-GAN. Therefore, for a two-second audio se-\\nquence with a sampling rate of 22050 Hz, the generated VQ\\nsequences from the high-level and low-level VQ generators\\nare in dimension of 64 × 344 and 64 × 1378, respectively,\\nwhere 64 is the dimension of the codebook entry, 344 and\\n1378 are the sequence lengths.\\nTraining Losses. Since our proposed D2M-GAN includes\\n\\n\\x0cFigure 6. Training losses for the proposed D2M-GAN. Adv. stands for adversarial.\\n\\nmultiple loss terms in the overall training objective, we\\nshow the change of each loss term during the training pro-\\ncess in Figure 6. It is worth noting the model architectures\\nand techniques described in our main paper are crucial for\\nD2M-GAN to maintain a stable training. Notably, the code-\\nbook commitement loss, audio waveform loss and audio\\nmel-spectrogram loss can reach the comparable levels with\\nthe GT audio samples after convergence.\\n\\nC. TikTok Dance-Music Dataset\\n\\nThe current version of our TikTok dance-music dataset\\ncontains in total 445 videos, which we annotate from 15\\nTikTok dance video compilations. There are 85 different\\nsongs, with majority of videos having a single dance per-\\nformer, and a maximum of ﬁve performers. The average\\nlength of each video is approximately 12.5s. We split the\\ntraining and testing set based on the music IDs, and ensure\\nthat there are no overlapping songs for two splits.\\n\\nCompared to the existing music and dance datasets such\\nas AIST++ [39, 60], our dataset is closer to the real-world\\nscenario with various background, which is also our initial\\nmotivation to introduce this dataset. Additionally, majority\\nof the current datasets available are not initially proposed\\nfor the dance to music generation task, AIST [60] is de-\\nsigned for dance music processing, AIST++ [39] provides\\nthe extra annoations for the subset of AIST for generat-\\ning dance motion conditioned on music, some other sim-\\nilar datasets for motion generation have also been intro-\\nduced [37]. Therefore, we hope that our proposed TikTok\\ndance-music dataset can serve as a starting point for rele-\\nvant future researches.\\n\\nD. Subjective Evaluations\\n\\nWe conduct the Mean Opinion Scores (MOS) test for the\\nsubjective evaluations. In total, 26 subjects participated our\\nMOS tests, among which 9 of them are females, the rest are\\nmales.\\n\\nTwo of our music evaluation protocols are based on the\\nhuman subjective evaluations, which are the dance-music\\ncoherence test and the music overall quality test. For the\\ndance-music coherence test, each evaluator is asked to rate\\n15 dance videos that are post-processed by fusing the orig-\\ninal visual frames and generated music samples from dif-\\nferent models. Speciﬁcally, the evaluators are asked to rate\\nfrom the coherence aspect of the dance video (i.e., whether\\nthey feels the music is coherent with the dance moves)\\nwith reference to the GT videos and original music. For\\nthe overall quality test, 15 audio samples (without video\\nframes) are played during the test for each evaluator, af-\\nter which the evaluator is asked to rate the sound quality\\nfrom the score range of 1 to 5. It is worth noting that for\\nthe overall quality test, we do not compare with the music\\nsamples obtained from the symbolic MIDI representation\\nbased methods. This is due to the reason that the symbolic\\nrepresentations and pre-deﬁned music synthesizers in na-\\nture do not introduce audio noises to the generated signals,\\nwhich makes the music samples sound rather “clean and\\nhigh-quality”, while the continuous or VQ audio represen-\\ntations can hardly achieve the similar effects with a learned\\nmusic synthesizer (samples included in our demo video).\\nTherefore, we do not include the MIDI-based methods as\\nour baselines for fairness considerations.\\n\\n13\\n\\n(a) Adv. loss for generator.(b) Adv. loss for discriminator.(c) Feature matching loss.(d) Codebook commitment loss.(e) Audio waveform loss.(f) Audio mel-spectrogram loss.\\x0c']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the text into one string\n",
    "\n",
    "array_pdf_text=[]\n",
    "def pdf_to_text():\n",
    "    for i in array_link:\n",
    "        array_pdf_text.append((load_pdf(i[21:])))\n",
    "    return array_pdf_text\n",
    "pdf_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87320f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Samuel.Lavoie', 'Christos.Tsirigotis', 'Max.Schwarzer', 'Kenji.Kawaguchi', 'Ankit.Vani', 'Aaron.Courville']\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      " \n",
      "r\n",
      "p\n",
      "A\n",
      " \n",
      "1\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      "G\n",
      "L\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "1\n",
      "v\n",
      "6\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      ".\n",
      "4\n",
      "0\n",
      "2\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Simplicial Embeddings in Self-Supervised Learning\n",
      "and Downstream Classiﬁcation\n",
      "\n",
      "Samuel Lavoie(cid:5)†, Christos Tsirigotis(cid:5)†, Max Schwarzer(cid:5)†, Kenji Kawaguchi‡, Ankit Vani(cid:5)†,\n",
      "Aaron Courville(cid:5)†♣\n",
      "(cid:5) Mila, † Université de Montréal, ‡ National University of Singapore, ♣ CIFAR Fellow\n",
      "{samuel.lavoie.m,aaron.courville}@gmail.com\n",
      "{christos.tsirigotis,max.schwarzer,ankit.vani}@umontreal.ca\n",
      "kenji@comp.nus.edu.sg\n",
      "\n",
      "Abstract\n",
      "\n",
      "We introduce Simplicial Embeddings (SEMs) as a way to constrain the encoded\n",
      "representations of a self-supervised model to L simplices of V dimensions each\n",
      "using a Softmax operation. This procedure imposes a structure on the represen-\n",
      "tations that reduce their expressivity for training downstream classiﬁers, which\n",
      "helps them generalize better. Speciﬁcally, we show that the temperature τ of the\n",
      "Softmax operation controls for the SEM representation’s expressivity, allowing us\n",
      "to derive a tighter downstream classiﬁer generalization bound than that for classi-\n",
      "ﬁers using unnormalized representations. We empirically demonstrate that SEMs\n",
      "considerably improve generalization on natural image datasets such as CIFAR-100\n",
      "and ImageNet. Finally, we also present evidence of the emergence of semantically\n",
      "relevant features in SEMs, a pattern that is absent from baseline self-supervised\n",
      "models.\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Self-supervised learning (SSL) is an emerging family of methods that aims to learn an embedding of\n",
      "the data without manual supervision, such as class labels. Those methods embed the data in some\n",
      "representations that render themselves amenable to ﬁtting a linear classiﬁer, as demonstrated in Hjelm\n",
      "et al. [2019]; Grill et al. [2020]; Saeed et al. [2020]; You et al. [2020]. This observation demonstrates\n",
      "that the representation learned by those SSL methods encodes the semantic content necessary to learn\n",
      "a classiﬁer as a linear combination of the features.\n",
      "\n",
      "In this work, we propose to embed the latent representation of the data into L simplices of V\n",
      "dimensions each by using a Softmax operation. We refer to the normalized embeddings as Simplicial\n",
      "Embeddings (SEMs) due to the geometrical structure of the representation induced by the Softmax.\n",
      "The SEMs have an effect both while training the representation and on the training of the downstream\n",
      "classiﬁer. For the former, the SEM is an inductive bias to ﬁt the data in a more constrained space\n",
      "that may lead to a simpler representation. For the latter, the Softmax allows us to control for the\n",
      "expressivity of the representation. This control gives us a better generalization bound for training\n",
      "downstream classiﬁers.\n",
      "\n",
      "We demonstrate that the proposed SEMs improve the generalization of downstream classiﬁers trained\n",
      "with BYOL [Grill et al., 2020] and MoCo [He et al., 2020] on CIFAR-100 and ImageNet. We also\n",
      "show an improvement in transfer learning and robustness to out-of-distribution datasets. Finally,\n",
      "we present evidence that individual features of the SEMs encode semantical content related to\n",
      "our intuitive notion of the semantics in CIFAR-100. In contrast, we argue that the baseline SSL\n",
      "methods may learn the semantics related to the classes as a linear combination of the features in the\n",
      "representation but not at the individual features’ level.\n",
      "\n",
      "Preprint. Under review.\n",
      "\n",
      "\f",
      "Concretely, this work makes the following contributions:\n",
      "\n",
      "1. Propose the Simplicial Embeddings.\n",
      "\n",
      "2. Derive a generalization bound for downstream classiﬁers trained on the Simplicial Embed-\n",
      "\n",
      "3. Empirically studies the Simplicial Embeddings and its effect on the generalization of\n",
      "\n",
      "dings.\n",
      "\n",
      "downstream classiﬁers.\n",
      "\n",
      "1.1 Related works\n",
      "\n",
      "The use of Softmax as an inductive bias has been studied in other contexts, notably as an architectural\n",
      "component for models to attend to context-dependent queries via, for example, attention mecha-\n",
      "nisms [Bahdanau et al., 2016; Vaswani et al., 2017] or memory augmented networks [Graves et al.,\n",
      "2014]. Different from these, our method places the Softmax at the output of an encoder to constrain\n",
      "the representation and to allow control of the expressivity of the representation for downstream\n",
      "classiﬁers.\n",
      "\n",
      "Our work builds on top of the literature on self-supervised learning. Notably, we demonstrate the effect\n",
      "of the SEM on contrastive approaches using the noise contrastive estimation (NCE) objective [Hjelm\n",
      "et al., 2019; Chen et al., 2020b] with memory banks [He et al., 2020] and on the bootstrapping\n",
      "approaches [Grill et al., 2020; Chen and He, 2020]. Related, some works explicitly induce clustering\n",
      "of the representation [Caron et al., 2019; Ym et al., 2019; Caron et al., 2020]. Contrary to these\n",
      "works, we do not explicitly induce clustering on the representation.\n",
      "\n",
      "In the realm of improving the generalization of SSL methods, Wang et al. [2021] propose a method\n",
      "to iteratively select a partition of the data and use this partition to minimize an IRM regularizer\n",
      "[Arjovsky et al., 2020] with an SSL objective. Lee et al. [2021] present an objective to minimize\n",
      "the conditional entropy bottleneck. Contrary to these works, our methods do not require additional\n",
      "objectives as it is merely an inductive bias in the SSL models.\n",
      "\n",
      "2 Background on self-supervised learning\n",
      "\n",
      "Models trained with a contrastive objective learn to embed samples x ∈ X into representations\n",
      "z ∈ Z, where Z is a bounded metric space. The aim is to both minimize the distance between\n",
      "the representation of a sample zi = fθ(xi) : x ∈ X and the representation of a positive sample\n",
      "zj = fθ(xj), and to maximize the distance between zi and the representation of negative samples\n",
      "fθ(x(cid:48)) : x(cid:48) ∈ X \\ xi. While the positive samples are typically augmented samples of xi, other\n",
      "strategies can be decided, such as choosing samples from the same labelled category [Khosla et al.,\n",
      "2020]. A common contrastive objective is Noise Contrastive Estimation (NCE) [Hjelm et al., 2019;\n",
      "Chen et al., 2020b], which is deﬁned as\n",
      "\n",
      "Lnce := − log\n",
      "\n",
      "exp(d(zi, zj)/t)\n",
      "¯x∈X \\x exp(d(zi, ¯z)/t)\n",
      "\n",
      ",\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "(1)\n",
      "\n",
      "where d is often taken to be the cosine similarity: d(x, y) := x(cid:62)y/(cid:107)x(cid:107)2(cid:107)y(cid:107)2 and t > 0 is a\n",
      "hyper-parameter that denotes a temperature.\n",
      "\n",
      "Unlike most contrastive methods, BYOL [Grill et al., 2020] does not require negative samples.\n",
      "Instead, it introduces a target network in which the parameters ξ are taken as an exponential moving\n",
      "average of the embedding function parameters, θ. More precisely, ξ ← αξ + (1 − α)θ, with\n",
      "α ∈ [0, 1]. The authors deﬁne the anchor and positive samples as zθ = fθ(t(x)) and zξ = fξ(t(cid:48)((x))\n",
      "respectively, where t, t(cid:48) ∼ T are augmentations sampled from a set of possible augmentations deﬁned\n",
      "by the practitioner. To prevent degenerate solutions, they re-normalize the representation using batch\n",
      "normalization [Ioffe and Szegedy, 2015], and utilize a stop-gradient operation on zξ that prevents the\n",
      "gradient from back-propagating through the target network. They also introduce a prediction head\n",
      "that maps the representation to a prediction: zθ (cid:55)→ qθ. The BYOL objective is deﬁned as\n",
      "\n",
      "Lbyol := 2 − 2 · d(qθ, zξ),\n",
      "\n",
      "(2)\n",
      "\n",
      "where d is chosen to be the cosine similarity.\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1: (a) Illustration of the proposed Simplicial Embeddings (SEM). στ represents the Softmax\n",
      "operation with τ . We assume that z decomposes into L vectors in RV . (b) Histogram of the entropies\n",
      "H(¯z(x)\n",
      ") at the end of the pre-training phase, for a given temperature τ , of each simplex for each\n",
      "i\n",
      "training sample in CIFAR-100. (c) Integration of the SEM with BYOL [Grill et al., 2020].\n",
      "\n",
      "3 Simplcial Embeddings\n",
      "\n",
      "We illustrate the proposed Simplicial Embeddings (SEMs) in Figure 1a. An encoder embeds a sample\n",
      "x into a L × V representation z. A temperature parameter τ then scales the logits z ∈ RL×V before\n",
      "re-normalizing each row via L independent Softmax operations. Then, the normalized vectors are\n",
      "concatenated to produce ¯z ∈ RLV . Concretely, the logits are re-normalized as follows:\n",
      "\n",
      "¯zi := [στ (zi1),\n",
      "\n",
      ". . . , στ (ziV )], στ (zij) =\n",
      "\n",
      ", ¯z := Concat(¯z1, . . . , ¯zL),\n",
      "\n",
      "(3)\n",
      "\n",
      "ezij /τ\n",
      "k=1 ezik/τ\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "for all i ∈ [L] and j ∈ [V ].\n",
      "\n",
      "The SEMs can be integrated easily into a NCE model [Hjelm et al., 2019; Chen et al., 2020b] or\n",
      "BYOL [Grill et al., 2020]. We insert it after the encoder and before the projector in our experiments.\n",
      "Figure 1c depicts how we use the SEMs in BYOL. The embedding ¯z is passed into the projector\n",
      "module, which we deﬁne as a linear layer or a small MLP. Beyond this small modiﬁcation, the SSL\n",
      "method considered remains unchanged.\n",
      "\n",
      "3.1\n",
      "\n",
      "Inductive bias of the SEMs\n",
      "\n",
      "i\n",
      "\n",
      "j=1 p(¯z(x)\n",
      "\n",
      ") where (cid:80)V\n",
      "\n",
      "ij ) = 1 and p(¯z(x)\n",
      "\n",
      "ij ) ≥ 0 ∀j. Here, ¯z(x)\n",
      "\n",
      "We now describe at a high level the inductive bias of the SEMs during the self-supervised learning\n",
      "phase. We note that each simplex can be interpreted as representing a probability mass function\n",
      "p(¯z(x)\n",
      "represents the simplex i for a\n",
      "i\n",
      "sample x. The simplex puts a constrain on how the its elements may organize: they may interpolate\n",
      "between being a sparse vector and being a constant vector. The state of a simplex can be quantiﬁed\n",
      ") that we denote as follows: H(¯zi) := − (cid:80)V\n",
      "using the entropy of p(¯z(x)\n",
      "ij ). That\n",
      "is, if H(¯z(x)\n",
      "While we may argue that the temperature parameter τ , which merely induces a scaling of the logit,\n",
      "may be subsumed during training, we demonstrate in Figure 1b that this temperature is an important\n",
      "initial condition for determining the state to which the simplex will converge. Here, we plot the\n",
      "histogram of the entropies H(¯z(x)\n",
      "), for a given τ , of each simplex for each sample x in the training\n",
      "set of CIFAR-100. The temperature parameter dictates in which state the representation will converge:\n",
      "a small τ will induce a sparse representation, and a large τ will induce a constant representation.\n",
      "\n",
      "j=1 p(¯z(x)\n",
      ") = log(V ) then the vector is constant.\n",
      "\n",
      ") = 0 then the vector is sparse and if H(¯z(x)\n",
      "\n",
      "ij ) log p(¯z(x)\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "(c)\n",
      "\n",
      "3\n",
      "\n",
      "EncoderConcat(0.3, 0.3, 0.4)(0.1, 0.7, 0.2)(0.1, 0.1, 0.8)SEM......0.00.51.01.52.02.5Entropy H(z(x)i)0123456: 0.01: 0.1: 0.5: 1.0: 10.0EncoderEncoderSEMSEMPredictionProjectionSSL  lossstop gradProjection\f",
      "Interestingly, for an intermediate temperature, the distribution of entropies is more spread out, rather\n",
      "than having the same variance smoothly translated toward the center of the histogram.\n",
      "\n",
      "The above observation gives an intuition about how the induce a bias of the SEMs on the learned\n",
      "representation during SSL. Besides the qualitative properties of the vectors that the SEM may induce,\n",
      "this embedding has a particular structure that we may leverage for learning a classiﬁer with a better\n",
      "generalization bound. Next, we theoretically present how the SEMs allow for a better generalization\n",
      "of a downstream classiﬁer and derive a generalization bound for the classiﬁers trained on such\n",
      "representation.\n",
      "\n",
      "3.2 Theoretical bound on the downstream classiﬁer\n",
      "\n",
      "In this subsection, we mathematically analyze the SEM to understand its beneﬁt and the effect of\n",
      "the hyper-parameter τ . We show that: (1) there is a trade-off between the training loss and the\n",
      "generalization gap, which is controlled by the value of τ , and (2) the SEM can improve the base\n",
      "model performance when we attain good balance in this trade-off.\n",
      "\n",
      "Let g represent the layer(s) after the normalization. With this notation, we can deﬁne a baseline\n",
      "model without normalization as fbase(z) = g(z) and the corresponding model with normalization\n",
      "as fSEM(τ )(z) = (g ◦ στ )(z). We consider a training dataset S = (zi, yi)n\n",
      "i=1 of n samples that is\n",
      "used for supervised training of a classiﬁer using the representations z, which are extracted from the\n",
      "self-supervised encoder. To understand the quality of the ﬁnal model after supervised training of\n",
      "the classiﬁer, we analyze the generalization gap Ez,y[l(f (z), y)] − 1\n",
      "i=1 l(f (z(i)), y(i)) for each\n",
      "n\n",
      "base}, where l : R × Y → R≥0 is the per-sample loss.\n",
      "f ∈ {f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "(cid:80)n\n",
      "\n",
      "To simplify the notation, we consider the normalization to [−1, +1]; i.e., z ∈ Z = [−1, +1]L×V .\n",
      "We assume that there exists ∆ > 0 such that for any i ∈ [L], if k = arg maxj∈[V ] zij, then\n",
      "zik ≥ zij + ∆ for any j (cid:54)= k. Since ∆ can be arbitrarily small (e.g., much smaller than machine\n",
      "precision), this assumption typically holds in practice. Next, we deﬁne B to be the upper bound\n",
      "on the per-sample loss such that l(f (z), y) ≤ B for all f ∈ H and for all (z, y) ∈ Z × Y,\n",
      "where H is the union of the hypothesis spaces of fSEM(τ ) and fbase. For example, B = 1\n",
      "for the 0-1 loss. We also use Qi = {q ∈ [−1, +1]V : i = arg maxj∈[V ] qj}, with the fol-\n",
      "lowing two deﬁnitions: ϕ(f S\n",
      "2, and ϕ(f S\n",
      "SEM(τ )) =\n",
      "supi∈[V ] supq,q(cid:48)∈Qi\n",
      "t=1 eqt/τ for j = 1, . . . , V . Next,\n",
      "we deﬁne GS to be the set of g returned by the training algorithm using dataset S, and R to be the\n",
      "Lipschitz constant of ly ◦ g for all y ∈ Y and g ∈ GS; i.e., |(ly ◦ g)(z) − (ly ◦ g)(z(cid:48))| ≤ R(cid:107)z − z(cid:48)(cid:107)F ,\n",
      "where ly(q) = l(q, y). Finally, let c > 0 be a universal constant in (n, f, H, δ, H, τ, S).\n",
      "\n",
      "2, where στ (q)j = eqj /τ\n",
      "\n",
      "base) = supi∈[V ] supq,q(cid:48)∈Qi\n",
      "\n",
      "t=1 (cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "\n",
      "t=1 (cid:107)q − q(cid:48)(cid:107)2\n",
      "\n",
      "(cid:80)n\n",
      "\n",
      "(cid:80)n\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "Using the established notation, Theorem 1 illuminates the advantage of the SEM and the effect of the\n",
      "hyper-parameter τ on the performance of the downstream classiﬁer:\n",
      "Theorem 1. Let V ≥ 2. For any δ > 0, with probability at least 1 − δ, the following holds for any\n",
      "fS ∈ {f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "base}:\n",
      "\n",
      "Ez,y[l(fS(z), y)] ≤\n",
      "\n",
      "l(fS(z(i)), y(i)) + R\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "Lϕ(fS)\n",
      "n\n",
      "\n",
      "+ c\n",
      "\n",
      "ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "Moreover,\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) → 0 as τ → 0\n",
      "\n",
      "and ϕ(f S\n",
      "\n",
      "SEM(τ )) − ϕ(f S\n",
      "\n",
      "base) ≤\n",
      "\n",
      "(1 − V ) < 0 ∀τ > 0.\n",
      "\n",
      "3n\n",
      "4\n",
      "\n",
      "The ﬁrst statement of Theorem 1 shows that the expected loss is bounded by the three terms: training\n",
      "loss 1\n",
      "n\n",
      "\n",
      "i=1 l(fS(z(i)), y(i)), the second term R\n",
      "\n",
      "(cid:113) Lϕ(fS )\n",
      "n\n",
      "\n",
      "(cid:113) ln(2/δ)\n",
      "n\n",
      "\n",
      ". Since c is\n",
      "\n",
      "(cid:80)n\n",
      "\n",
      "a universal constant in (n, f, H, δ, H, τ, S), the third term c\n",
      "goes to zero as n → ∞ and is\n",
      "the same for both models with and without soft-discretization. Thus, for the purpose of comparing the\n",
      "models with and without soft-discretization, we can focus on the second term, where the difference\n",
      "arises.\n",
      "\n",
      ", and the third term c\n",
      "(cid:113) ln(2/δ)\n",
      "n\n",
      "\n",
      "Theorem 1 shows that the second term R\n",
      "SEM(τ )) →\n",
      "0 as τ → 0. Also, for any τ > 0, the second term with soft-discretization is strictly smaller than\n",
      "\n",
      "goes to zero with the SEM; i.e., ϕ(f S\n",
      "\n",
      "(cid:113) Lϕ(fS )\n",
      "n\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "that without soft-discretization as ϕ(f S\n",
      "improvement due to soft-discretization is expected to be higher as V increases.\n",
      "\n",
      "SEM(τ )) − ϕ(f S\n",
      "\n",
      "base) ≤ 3n\n",
      "\n",
      "4 (1 − V ) < 0. This shows that the\n",
      "\n",
      "Overall, Theorem 1 shows the beneﬁt of the SEM as well as the trade-off with τ . When τ → 0, the\n",
      "second term goes to zero, but the training loss (the ﬁrst term) can increase due to the reduction in\n",
      "expressivity and increased difﬁculty in optimization. Thus, we assert that the best τ is the one that\n",
      "balances this trade-off.\n",
      "\n",
      "4 Experiments\n",
      "\n",
      "We study the effect of the Simplicial Embeddings on the generalization of self-supervised learn-\n",
      "ing methods1. We demonstrate that the Simplicial Embeddings improve the test set accuracy on\n",
      "CIFAR-100 and ImageNet. On CIFAR-100, we also study the different properties of the SEM, and\n",
      "we demonstrate the emergence of semantic features relevant to the classes in the representation\n",
      "feature. On ImageNet, we show that the Simplicial Embeddings improve the test accuracy on several\n",
      "robustness test sets and the accuracy on transfer learning datasets.\n",
      "\n",
      "4.1 The effect of the SEM on downstream classiﬁcation\n",
      "\n",
      "Method\n",
      "SimCLR†\n",
      "MOCO†\n",
      "SWAV†\n",
      "DINO†\n",
      "BYOL\n",
      "BYOL + SEM\n",
      "\n",
      "Accuracy\n",
      "65.78\n",
      "69.89\n",
      "64.88\n",
      "66.76\n",
      "70.46\n",
      "74.36\n",
      "\n",
      "Method\n",
      "SimCLR‡\n",
      "BYOL\n",
      "BYOL*\n",
      "BYOL + SEM\n",
      "\n",
      "Accuracy\n",
      "68.73\n",
      "74.28\n",
      "73.33\n",
      "77.05\n",
      "\n",
      "Method\n",
      "SIMCLR‡\n",
      "MOCO‡\n",
      "MOCO + SEM\n",
      "SIMSIAM(cid:5)\n",
      "BYOL(cid:5)\n",
      "BYOL + SEM\n",
      "\n",
      "Accuracy\n",
      "63.1\n",
      "67.3\n",
      "69.0\n",
      "70.0\n",
      "70.6\n",
      "72.8\n",
      "\n",
      "(a) CIFAR-100 on ResNet18\n",
      "\n",
      "(b) CIFAR-100 on ResNet50\n",
      "\n",
      "(c) ImageNet on ResNet50\n",
      "\n",
      "Figure 2: Accuracy on (a) CIFAR-100 with ResNet18 for 1000 epochs. (b) CIFAR-100 with ResNet50\n",
      "for 1000 epochs. (c) ImageNet with ResNet50 for 200 epochs. *Denotes the accuracy obtained when\n",
      "training BYOL with a representation the same size as SEM. † Results taken from da Costa et al.\n",
      "[2021]. ‡ Results taken from Wang et al. [2021]. (cid:5) Results taken from [Chen et al., 2020b]. Boldface\n",
      "indicates highest accuracy. Green rows indicate a SSL method + SEM.\n",
      "\n",
      "Comparison study. We ﬁrst compare the effect of using the SEM in a BYOL model with related\n",
      "SSL approaches in the literature. We take a standard BYOL model, as implemented in the Solo-Learn\n",
      "library [da Costa et al., 2021], and implement the Simplicial Embeddings after the encoder. We test\n",
      "our approach with a ResNet18 and ResNet50 on CIFAR-100 and with a ResNet50 for ImageNet [He\n",
      "et al., 2015]. Our models are trained with Stochastic Gradient Descent [Bottou et al., 2018] with a\n",
      "cosine decay scheduler on the learning rate, as done in previous works [Grill et al., 2020; Chen et al.,\n",
      "2020b]. We use a batch size of 256 for all of our models and train on a single A100 GPU. We selected\n",
      "the parameters of the SEM by performing a grid search over several values using a validation set and\n",
      "re-trained our model using all the training data to evaluate the test set. We did not modify the default\n",
      "hyper-parameters of the method, demonstrating that the gain in accuracy is a product of the SEM. We\n",
      "present the hyper-parameters used in the Appendix. We evaluate all of our models by training a linear\n",
      "classiﬁer, using the training data on top of the learned representations as it is typically done.\n",
      "\n",
      "We compare our approach on CIFAR-100 and ImageNet in Table 2b and Table 2c respectively.\n",
      "Compared with prior models, our approach improves the baseline methods by a considerable margin.\n",
      "On CIFAR-100, we compare with several baselines, such as DINO and SwaV. We also trained BYOL\n",
      "with the same representation size as what we used in the SEM, without the embedding, and observed a\n",
      "marginal performance decrease. As demonstrated in Zbontar et al. [2021], BYOL does not seemingly\n",
      "beneﬁt from large representations.\n",
      "\n",
      "The SEM also presents a noticeable improvement compared to the baselines on ImageNet when\n",
      "trained for 200 epochs. Here, we trained our model on both BYOL and MOCO [He et al., 2020] to\n",
      "demonstrate that the effect of the SEM is not limited to BYOL.\n",
      "\n",
      "1We provide the code to reproduce the experiments: https://github.com/lavoiems/simplicial-embeddings\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Figure 3: Study of the effect of the parameters of the SEM. We plot the accuracy obtained for several\n",
      "downstream classiﬁcation SEM’s temperature (fSEM(τ )) and without SEM (fbase) described in the\n",
      "legend. We performed the training with a ResNet18 on CIFAR-100. Interpolation of Left: τ during\n",
      "the training of the SSL model. Middle: V . Right: L\n",
      "\n",
      "Study of the SEM parameters. We study the effect of each of the parameters of the SEM and\n",
      "evaluate their effect on the validation accuracy in Figure 3. We trained each model with a ResNet18\n",
      "on CIFAR-100 using the BYOL training procedure. We keep the other parameters constant to their\n",
      "default value for each parameter that we study. The default value of τ is 1, V is 13 and L is 5000. For\n",
      "each pre-trained SSL model, we trained 5 downstream classiﬁers, one on the unnormalized features\n",
      "denoted fbase and one on the normalized features for τ ∈ {0.01, 0.1, 1, 10}.\n",
      "\n",
      "We observe that the temperature used to normalized the embedding before training the downstream\n",
      "classiﬁer, fSEM(τ ), is important for the downstream classiﬁcation and is generally better than training\n",
      "a classiﬁer on the unnormalized features (fbase) as predicted in Theorem 1. We observe the trade-off,\n",
      "as presented in Section 3.2, between having a small and a large τ .\n",
      "\n",
      "We also observe a trade-off between having a large and a small temperature when training the SSL\n",
      "model. As demonstrated in Figure 1b, the temperature parameter has an impact on whether the\n",
      "simplicies will represent a sparse or a constant vector. We demonstrated that a small temperature\n",
      "yields a set of sparse vectors while a large temperature yields a constant vector. Here, we observe\n",
      "that the temperature yielding the better validation accuracy offers a trade-off between a sparse and a\n",
      "constant vector. We hypothesize that a sparse vector leads to harder training but a smaller expressivity.\n",
      "Thus, the better temperature during the training of the SSL model is the one that offers a trade-off\n",
      "between a sparse but trainable representation.\n",
      "\n",
      "In Theorem 1, we demonstrated theoretically that the second term was more sensitive to the tem-\n",
      "perature as V increased. This prediction is empirically veriﬁed in Figure 3 where we evaluate the\n",
      "validation accuracy for several V . As V increases, the validation accuracy drops for larger τ s. For\n",
      "example, the validation accuracy drops when interpolating between V = 13 and V = 34 for τ = 10,\n",
      "stays constant for τ = 1 and increases for the smaller temperatures.\n",
      "\n",
      "Finally, we interpolate the L parameter and demonstrate that larger L yields increased normalized\n",
      "features’ validation accuracy. As expected, the effect of ϕ(fS) grows with larger L, and thus we\n",
      "would expect a bigger difference between fbase and fSEM(τ ). This demonstrates empirically and\n",
      "theoretically that the SEM may scale the representation of SSL methods to a larger representation\n",
      "and thus potentially increasing the scaling capability of these methods.\n",
      "\n",
      "4.2 Emergence of semantically relevant features\n",
      "\n",
      "In this subsection, we investigate the semantic content held by the most predictive features of an\n",
      "embedding. To make this study, we consider an encoder pretrained on CIFAR-100, using BYOL with\n",
      "and without SEM, and a downstream linear classiﬁer trained on the embedding of the CIFAR-100\n",
      "samples. Consider the trained linear classiﬁer with a weight matrix W ∈ RN ×C, where N denotes\n",
      "the number of features, and C denotes the number of classes. This classiﬁer is trained by minimizing\n",
      "the cross-entropy loss between the predicted class and the given label.\n",
      "\n",
      "Here, we study the semantic relevance of the top K features for each class. Consider the weight\n",
      "matrix W . By preserving the top K parameters of this weight matrix for each class and pruning\n",
      "the features predictive for only one class, we create a bipartite graph between two set of nodes: the\n",
      "\n",
      "6\n",
      "\n",
      "fbasefSEM(=0.01)fSEM(=0.1)fSEM(=1)fSEM(=10)0.010.10.5110SSL 506070Valid accuracy23581334V62.565.067.570.072.5Valid accuracy5010050010005000L62.565.067.570.072.5Valid accuracy\f",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 4: Semantic relevance of the features. (a) Subset of WK, the bipartite graph of the most\n",
      "important features shared between at least two classes of a classiﬁer trained on BYOL + SEMs\n",
      "features. The connected components emerge without additional interventions. (b) Relevance of the\n",
      "top K features to the semantics of the super-class of the categories of CIFAR-100. It is taken as the\n",
      "number of pairwise categories in the same super-class for which a feature is among its top K most\n",
      "predictive features over the total number of pairwise categories.\n",
      "\n",
      "categories and the features. We denote this graph WK. We plot a subset W5, obtained when taking\n",
      "the top 5 features for each class, on the SEM representations in Figure 4a and the full bipartite graph\n",
      "on the SEM and the one obtained when applying the procedure on the representation obtained with\n",
      "an unnormalized BYOL in the Appendix. In the graph obtained with the SEM, we observe that a\n",
      "set of connected components emerge, and the connected components of the graph are semantically\n",
      "related. For example, the ﬁrst set of connected components are fruits and vegetables, and the second\n",
      "set of connected components are aquatic mammals. The same observation does not occur when this\n",
      "experiment is performed on the baseline BYOL and BYOL, with a large representation model. In\n",
      "particular, we do not see a small number of semantically related connected components. Instead, we\n",
      "see a large fully connected graphs. This observation suggests that the features learned by the baseline\n",
      "model do not hold the same amount of semantic information. Instead, the semantic information could\n",
      "be encoded as a linear combination of several features, for example.\n",
      "\n",
      "We also study more quantitatively the semantic relevance of the features in CIFAR-100. Two\n",
      "categories share a predictive feature on WK if they are 2-neighbour, that is they share a common\n",
      "predictive feature. Let N (ci) returns all pairs (ci, cj) for all j 2-neighbour of ci. Moreover, deﬁne\n",
      "the operation is_super(ci, cj) which returns 1 if ci and cj are from the same CIFAR-100 superclass\n",
      "and 0 otherwise. We reproduce the superclass of CIFAR-100 in Table 5. We deﬁne the semantic\n",
      "relevance as follows:\n",
      "\n",
      "Relevance(WK) :=\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "C\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(ci,cj )∈N (ci) is_super(ci, cj)\n",
      "|N (ci)|\n",
      "\n",
      ",\n",
      "\n",
      "(4)\n",
      "\n",
      "where C = 100 for CIFAR-100 and | · | is the cardinality of a set.\n",
      "\n",
      "We compare the semantic relevance of BYOL+SEM with the control experiments BYOL and BYOL\n",
      "with a representation of the same size as BYOL+SEM but without the normalization. We observe\n",
      "that using the SEM yields more semantically relevant features than the baseline. This observation is\n",
      "consistent with the qualitative experiments presented earlier and indicates that the semantics encoded\n",
      "\n",
      "BYOL\n",
      "\n",
      "IN\n",
      "68.3\n",
      "BYOL + SEM 70.6\n",
      "66.7\n",
      "MOCO + SEM 68.0\n",
      "\n",
      "MOCO\n",
      "\n",
      "IN-V2\n",
      "55.3\n",
      "57.9\n",
      "53.4\n",
      "55.0\n",
      "\n",
      "100%\n",
      "IN-R\n",
      "16.5\n",
      "18.1\n",
      "14.0\n",
      "15.21\n",
      "\n",
      "IN-A IN-C\n",
      "35.4\n",
      "0.68\n",
      "38.9\n",
      "0.77\n",
      "0.69\n",
      "31.1\n",
      "33.8\n",
      "0.61\n",
      "\n",
      "IN\n",
      "46.8\n",
      "47.9\n",
      "43.5\n",
      "44.1\n",
      "\n",
      "IN-V2\n",
      "37.5\n",
      "38.5\n",
      "34.2\n",
      "35.9\n",
      "\n",
      "1%\n",
      "IN-R IN-A IN-C\n",
      "0.71\n",
      "12.2\n",
      "25.0\n",
      "25.3\n",
      "12.2\n",
      "0.65\n",
      "0.51\n",
      "20.1\n",
      "8.7\n",
      "21.4\n",
      "0.51\n",
      "9.1\n",
      "\n",
      "Table 1: Test accuracies of a linear probe trained with 100% and 1% of the IMAGENET samples on\n",
      "a pre-trained representation trained for 100 epochs. Boldface indicates the maximal value for each\n",
      "evaluation set and each base model type (BYOL or MoCo).\n",
      "\n",
      "7\n",
      "\n",
      "orangeappleS190sweet_pepperS181S196pearS29S173S13sharkS355S146S383turtleS185raywhaledolphinS363S338orchidpoppyroseS294tulipS311S343S13601020304050Top K features0.10.20.30.40.50.60.70.8Semantic relevanceBYOLBYOL + large repr.BYOL+SEM\f",
      "FOOD CIFAR10 CIFAR-100 SUN DTD PETS FLOWERS CALTECH CARS\n",
      "45.7\n",
      "57.6 71.5 85.4\n",
      "BYOL\n",
      "71.3\n",
      "57.3\n",
      "60.5 72.5 87.1\n",
      "BYOL + SEM 74.1\n",
      "57.6 70.9 82.3\n",
      "39.8\n",
      "MOCO\n",
      "70.6\n",
      "MOCO + SEM 71.0\n",
      "45.2\n",
      "58.6 70.9 83.8\n",
      "Table 2: Transfer learning accuracy by training a linear probe on a pre-trained representation with\n",
      "IMAGENET for 100 epochss. Boldface indicates the maximal value for each transfer dataset and each\n",
      "base model type (BYOL or MoCo).\n",
      "\n",
      "77.8\n",
      "82.4\n",
      "74.3\n",
      "77.5\n",
      "\n",
      "89.5\n",
      "92.0\n",
      "88.6\n",
      "89.6\n",
      "\n",
      "84.6\n",
      "88.6\n",
      "81.5\n",
      "84.5\n",
      "\n",
      "71.4\n",
      "76.3\n",
      "69.5\n",
      "72.8\n",
      "\n",
      "in the baseline representation may follow a more complicated syntactic structure than those encoded\n",
      "with the SEM features.\n",
      "\n",
      "4.3 Out-of-distribution evaluation on ImageNet\n",
      "\n",
      "Robustness to out-of-distribution test sets on ImageNet. We perform a comparative study using\n",
      "several robustness evaluation sets. Speciﬁcally, we use the validation set provided in IMAGENET;\n",
      "IMAGENET-C, which exhibits a set of common image corruptions [Hendrycks and Dietterich, 2018];\n",
      "IMAGENET-A [Chen et al., 2020a], which contains a set of natural adversarial examples that are\n",
      "misclassiﬁed by a Resnet-50 classiﬁer; IMAGENET-R [Hendrycks et al., 2021], which consists of\n",
      "different renderings for several ImageNet classes; and IMAGENET-V2 [Recht et al., 2019], a distinct\n",
      "test set for ImageNet collected using the same process. We use the methodology proposed in Djolonga\n",
      "et al. [2020, 2021] along with their software to perform our experiments.\n",
      "\n",
      "Table 1 shows the performance on these test sets using a linear probe trained with 100% of ImageNet’s\n",
      "data and 1% of ImageNet’s data. Using the SEM generally leads to an improvement in the in-\n",
      "distribution and out-of-distribution generalization. Notably, we observe a 2% improvement on BYOL\n",
      "due to the SEM on in-distribution IMAGENET. On average, there is an improvement of 2% and 0.5%\n",
      "in the 100% and 1% data regimes respectively for BYOL. For MOCO, the average improvement due\n",
      "to the SEM is 1.5% and 0.8% for the 100% and 1% data regimes respectively.\n",
      "\n",
      "Transfer learning on ImageNet. We probe the effect of inducing the SEM in BYOL and MoCo\n",
      "on the transfer accuracy to other classiﬁcation tasks from representations trained on IMAGENET.\n",
      "We follow the linear evaluation methodology described in previous works [Grill et al., 2020; Lee\n",
      "et al., 2021], which entails training a linear classiﬁer on the embeddings of the samples for each\n",
      "dataset. We perform our transfer learning experiments on the following datasets: Food [Bossard et al.,\n",
      "2014], CIFAR-10 [Krizhevsky, 2009], CIFAR-100 [Krizhevsky, 2009], SUN [Xiao et al., 2010],\n",
      "DTD [Cimpoi et al., 2014], Pets [Parkhi et al., 2012], Flowers [Nilsback and Zisserman, 2008],\n",
      "CalTech [Fei-Fei et al., 2004] and Cars [Krause et al., 2013].\n",
      "\n",
      "This task evaluates the generality of the encoder as it has to encode samples from various out-of-\n",
      "distribution domains with categories that it may not have seen during training. We present our results\n",
      "in Table 2 and observe that the SEM improves the transfer accuracy over the baseline for every\n",
      "dataset.\n",
      "\n",
      "5 Conclusion\n",
      "\n",
      "This work introduces the Simplicial Embeddings (SEM) as a simple and effective drop-in module\n",
      "for self-supervised learning that leads to representation with better generalization. Our theoretical\n",
      "insights demonstrate that the temperature parameter of the SEM allows for control over the trade-off\n",
      "between the training loss and expressivity on downstream classiﬁers; we also observe that controlling\n",
      "the expressivity via the temperature parameter. We validate our theoretical prediction with a set\n",
      "of controlled experiments. Moreover, we empirically demonstrate that the SEM improves the in-\n",
      "distribution test accuracy and out-of-distribution accuracy on several robustness test sets and transfer\n",
      "learning datasets.\n",
      "\n",
      "We have also demonstrated that the SEM leads to more semantically relevant features for predicting\n",
      "the categories of a dataset compared to the baseline method. Thus, the SEM embedding may be\n",
      "simpler than the un-normalized embedding, leading to more interpretable representations. We want\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "to study this in more depth in future works. Related, we would also like to investigate further why the\n",
      "SEM leads to such representations.\n",
      "\n",
      "Acknowledgments and Disclosure of Funding\n",
      "\n",
      "The authors are grateful for the insightful discussions with Xavier Bouthillier, Michael Noukhovitch,\n",
      "Hattie Zhou, Sébastien Lachapelle, Yuchen Lu, Eeshan Dhekane, and Devon Hjelm. We acknowledge\n",
      "funding support from Samsung and Hitachi, as well as support from Aaron Courville’s CIFAR CCAI\n",
      "chair. We also wish to acknowledge Mila and Compute Canada for providing the computing infras-\n",
      "tructure that enabled this project. Finally, this project would not have been possible without the con-\n",
      "tribution of the following open source projects: Pytorch [Paszke et al., 2019], Orion [Bouthillier et al.,\n",
      "2022], Solo-Learn [da Costa et al., 2021], Scikit-Learn [Pedregosa et al., 2011], and Numpy [Harris\n",
      "et al., 2020].\n",
      "\n",
      "References\n",
      "\n",
      "Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization.\n",
      "arXiv:1907.02893 [cs, stat], March 2020. URL http://arxiv.org/abs/1907.02893. arXiv:\n",
      "1907.02893.\n",
      "\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly\n",
      "Learning to Align and Translate. arXiv:1409.0473 [cs, stat], May 2016. URL http://arxiv.\n",
      "org/abs/1409.0473. arXiv: 1409.0473.\n",
      "\n",
      "Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative com-\n",
      "ponents with random forests. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,\n",
      "editors, Computer Vision – ECCV 2014, pages 446–461, Cham, 2014. Springer International\n",
      "Publishing. ISBN 978-3-319-10599-4.\n",
      "\n",
      "Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization Methods for Large-Scale Machine\n",
      "Learning. arXiv:1606.04838 [cs, math, stat], February 2018. URL http://arxiv.org/abs/\n",
      "1606.04838. arXiv: 1606.04838.\n",
      "\n",
      "Xavier Bouthillier, Christos Tsirigotis, François Corneau-Tremblay, Thomas Schweizer, Lin Dong,\n",
      "Pierre Delaunay, Fabrice Normandin, Mirko Bronzi, Dendi Suhubdy, Reyhane Askari, Michael\n",
      "Noukhovitch, Chao Xue, Satya Ortiz-Gagné, Olivier Breuleux, Arnaud Bergeron, Olexa Bilaniuk,\n",
      "Steven Bocco, Hadrien Bertrand, Guillaume Alain, Dmitriy Serdyuk, Peter Henderson, Pascal\n",
      "Lamblin, and Christopher Beckham. Epistimio/orion: Asynchronous Distributed Hyperparameter\n",
      "Optimization, March 2022. URL https://doi.org/10.5281/zenodo.3478592.\n",
      "\n",
      "Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep Clustering for\n",
      "Unsupervised Learning of Visual Features. arXiv:1807.05520 [cs], March 2019. URL http:\n",
      "//arxiv.org/abs/1807.05520. arXiv: 1807.05520.\n",
      "\n",
      "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\n",
      "\n",
      "Unsupervised learning of visual features by contrasting cluster assignments. 2020.\n",
      "\n",
      "Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial\n",
      "Robustness: From Self-Supervised Pre-Training to Fine-Tuning. pages 699–708, 2020a. URL\n",
      "https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Adversarial_\n",
      "Robustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_\n",
      "paper.html.\n",
      "\n",
      "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n",
      "contrastive learning of visual representations. In Hal Daumé III and Aarti Singh, editors, Proceed-\n",
      "ings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of\n",
      "Machine Learning Research, pages 1597–1607. PMLR, 13–18 Jul 2020b.\n",
      "\n",
      "Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint\n",
      "\n",
      "arXiv:2011.10566, 2020.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-\n",
      "scribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and\n",
      "Pattern Recognition, pages 3606–3613, 2014.\n",
      "\n",
      "Victor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. Solo-learn: A library\n",
      "of self-supervised methods for visual representation learning, 2021. URL https://github.com/\n",
      "vturrisi/solo-learn.\n",
      "\n",
      "Josip Djolonga, Frances Hubis, Matthias Minderer, Zachary Nado, Jeremy Nixon, Rob Romijn-\n",
      "ders, Dustin Tran, and Mario Lucic. Robustness Metrics, 2020. URL https://github.com/\n",
      "google-research/robustness_metrics.\n",
      "\n",
      "Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\n",
      "Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Syl-\n",
      "vain Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On Robustness and Transferabil-\n",
      "ity of Convolutional Neural Networks. arXiv:2007.08558 [cs], March 2021. URL http:\n",
      "//arxiv.org/abs/2007.08558. arXiv: 2007.08558.\n",
      "\n",
      "Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training\n",
      "examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference\n",
      "on computer vision and pattern recognition workshop, pages 178–178. IEEE, 2004.\n",
      "\n",
      "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv:1410.5401 [cs],\n",
      "\n",
      "December 2014. URL http://arxiv.org/abs/1410.5401. arXiv: 1410.5401.\n",
      "\n",
      "Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\n",
      "Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\n",
      "Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent -\n",
      "a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\n",
      "Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\n",
      "pages 21271–21284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\n",
      "paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\n",
      "\n",
      "Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\n",
      "Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti\n",
      "Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández\n",
      "del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy,\n",
      "Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming\n",
      "with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2.\n",
      "URL https://doi.org/10.1038/s41586-020-2649-2.\n",
      "\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\n",
      "Recognition. arXiv:1512.03385 [cs], December 2015. URL http://arxiv.org/abs/1512.\n",
      "03385. arXiv: 1512.03385.\n",
      "\n",
      "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for\n",
      "Unsupervised Visual Representation Learning. arXiv:1911.05722 [cs], March 2020. URL http:\n",
      "//arxiv.org/abs/1911.05722. arXiv: 1911.05722.\n",
      "\n",
      "Dan Hendrycks and Thomas Dietterich. Benchmarking Neural Network Robustness to Common\n",
      "Corruptions and Perturbations. September 2018. URL https://openreview.net/forum?id=\n",
      "HJz6tiCqYm.\n",
      "\n",
      "Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\n",
      "Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\n",
      "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization.\n",
      "arXiv:2006.16241 [cs, stat], July 2021. URL http://arxiv.org/abs/2006.16241. arXiv:\n",
      "2006.16241.\n",
      "\n",
      "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\n",
      "Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\n",
      "and maximization. In International Conference on Learning Representations, 2019. URL https:\n",
      "//openreview.net/forum?id=Bklr3j0cKX.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\n",
      "reducing internal covariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd\n",
      "International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning\n",
      "Research, pages 448–456, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.\n",
      "mlr.press/v37/ioffe15.html.\n",
      "\n",
      "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,\n",
      "Aaron Maschinot, Ce Liu, and Dilip Krishnan.\n",
      "In\n",
      "H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Ad-\n",
      "vances in Neural Information Processing Systems, volume 33, pages 18661–18673. Cur-\n",
      "ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n",
      "d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf.\n",
      "\n",
      "Supervised contrastive learning.\n",
      "\n",
      "Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representa-\n",
      "\n",
      "tion learning. CoRR, abs/1901.09005, 2019. URL http://arxiv.org/abs/1901.09005.\n",
      "\n",
      "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained\n",
      "categorization. In Proceedings of the IEEE international conference on computer vision workshops,\n",
      "pages 554–561, 2013.\n",
      "\n",
      "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n",
      "\n",
      "Kuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive\n",
      "Visual Representations. arXiv:2109.12909 [cs, math], September 2021. URL http://arxiv.\n",
      "org/abs/2109.12909. arXiv: 2109.12909.\n",
      "\n",
      "Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number\n",
      "of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing,\n",
      "pages 722–729, 2008. doi: 10.1109/ICVGIP.2008.47.\n",
      "\n",
      "Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In 2012\n",
      "IEEE Conference on Computer Vision and Pattern Recognition, pages 3498–3505, 2012. doi:\n",
      "10.1109/CVPR.2012.6248092.\n",
      "\n",
      "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\n",
      "Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\n",
      "Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\n",
      "Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\n",
      "high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-\n",
      "Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,\n",
      "pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n",
      "9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\n",
      "pdf.\n",
      "\n",
      "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\n",
      "hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\n",
      "E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,\n",
      "12:2825–2830, 2011.\n",
      "\n",
      "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classiﬁers\n",
      "In Proceedings of the 36th International Conference on Machine\n",
      "Generalize to ImageNet?\n",
      "Learning, pages 5389–5400. PMLR, May 2019. URL https://proceedings.mlr.press/\n",
      "v97/recht19a.html. ISSN: 2640-3498.\n",
      "\n",
      "Aaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive Learning of General-Purpose Audio\n",
      "Representations. arXiv:2010.10915 [cs, eess], October 2020. URL http://arxiv.org/abs/\n",
      "2010.10915. arXiv: 2010.10915.\n",
      "\n",
      "Aad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. Springer\n",
      "New York, 1996. doi: 10.1007/978-1-4757-2545-2. URL https://doi.org/10.1007%\n",
      "2F978-1-4757-2545-2.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
      "Kaiser, and Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762 [cs], December 2017.\n",
      "URL http://arxiv.org/abs/1706.03762. arXiv: 1706.03762 version: 5.\n",
      "\n",
      "Tan Wang, Zhongqi Yue, Jianqiang Huang, Qianru Sun, and Hanwang Zhang. Self-Supervised\n",
      "Learning Disentangled Group Representation as Feature. May 2021. URL https://openreview.\n",
      "net/forum?id=RQfcckT1M_4.\n",
      "\n",
      "Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\n",
      "Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on\n",
      "computer vision and pattern recognition, pages 3485–3492. IEEE, 2010.\n",
      "\n",
      "Asano Ym, Rupprecht C, and Vedaldi A. Self-labelling via simultaneous clustering and representation\n",
      "\n",
      "learning. September 2019. URL https://openreview.net/forum?id=Hyx-jyBFPr.\n",
      "\n",
      "Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\n",
      "contrastive learning with augmentations. CoRR, abs/2010.13902, 2020. URL https://arxiv.\n",
      "org/abs/2010.13902.\n",
      "\n",
      "Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\n",
      "\n",
      "learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "A Proof of Theorem 1\n",
      "\n",
      "Let us introduce additional notations used in the proofs. Deﬁne r = (z, y) ∈ R, (cid:96)(f, r) = l(f (z), y),\n",
      "\n",
      "˜Cy,k1,...,kL = {(z, ˆy) ∈ Z × Y : ˆy = y, kj = arg max\n",
      "\n",
      "zj,t ∀j ∈ [L]},\n",
      "\n",
      "t∈[V ]\n",
      "\n",
      "and\n",
      "\n",
      "˜Zk1,...,kL = {z ∈ Z : kj = arg max\n",
      "\n",
      "zj,t ∀j ∈ [L]}.\n",
      "\n",
      "t∈[V ]\n",
      "\n",
      "to be\n",
      "\n",
      "then deﬁne Ck\n",
      "\n",
      "=\n",
      "We\n",
      "the ﬂatten version of\n",
      "{ ˜Cy,k1,...,kL,y}y∈Y,k1,...,kL∈[V ] with C1 = ˜C1,1,...,1, C2 = ˜C2,1,...,1, C|Y| = ˜C|Y|,1,...,1, C|Y|+1 =\n",
      "˜C1,2,1,...,1, C2|Y| = ˜C|Y|,2,1,...,1, and so on. Similarly, deﬁne Zk to be the ﬂatten version of ˜Zk1,...,kL.\n",
      "We also use Qi = {q ∈ [−1, +1]V : i = arg maxj∈[V ] qj}, Ik := I S\n",
      "k := {i ∈ [n] : ri ∈ Ck}, and\n",
      "(cid:80)n\n",
      "αk(h) := Er[(cid:96)(h, r)|r ∈ Ck]. Moreover, we deﬁne ϕ(f S\n",
      "t=1 (cid:107)q − q(cid:48)(cid:107)2\n",
      "2,\n",
      "eqj /τ\n",
      "and ϕ(f S\n",
      "t=1 eqt/τ for\n",
      "j = 1, . . . , V .\n",
      "\n",
      "SEM(τ )) = supi∈[V ] supq,q(cid:48)∈Qi\n",
      "\n",
      "base) = supi∈[V ] supq,q(cid:48)∈Qi\n",
      "\n",
      "t=1 (cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "\n",
      "2 where στ (q)j =\n",
      "\n",
      "(cid:80)n\n",
      "\n",
      "i.e.,\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "{Ck}K\n",
      "\n",
      "˜Cy,k1,...,kL;\n",
      "\n",
      "We ﬁrst decompose the generalization gap into two terms using the following lemma:\n",
      "Lemma 1. For any δ > 0, with probability at least 1 − δ,the following holds for all h ∈ H:\n",
      "\n",
      "Er[(cid:96)(h, r)] −\n",
      "\n",
      "(cid:96)(h, ri) ≤\n",
      "\n",
      "|Ik|\n",
      "\n",
      "αk(h) −\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      " + c\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "Proof. We ﬁrst write the expected error as the sum of the conditional expected error:\n",
      "\n",
      "Er[(cid:96)(h, r)] =\n",
      "\n",
      "Er[(cid:96)(h, r)|r ∈ Ck] Pr(r ∈ Ck) =\n",
      "\n",
      "Erk [(cid:96)(h, rk)] Pr(r ∈ Ck),\n",
      "\n",
      "where rk is the random variable for the conditional with r ∈ Ck. Using this, we decompose the\n",
      "generalization error into two terms:\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "Er[(cid:96)(h, r)] −\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "=\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "Erk [(cid:96)(h, rk)]\n",
      "\n",
      "Pr(r ∈ Ck) −\n",
      "\n",
      "Erk [(cid:96)(h, rk)]\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "\n",
      "\n",
      "+\n",
      "\n",
      "\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      " .\n",
      "\n",
      "The second term in the right-hand side of (5) is further simpliﬁed by using\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:96)(h, ri) =\n",
      "\n",
      "(cid:96)(h, ri),\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "as\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "Erk [(cid:96)(h, rk)]\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:96)(h, ri) =\n",
      "\n",
      "|Ik|\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "Erk [(cid:96)(h, rk)] −\n",
      "\n",
      "\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "Substituting these into equation (5) yields\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "Er[(cid:96)(h, r)] −\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "=\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "Erk [(cid:96)(h, rk)]\n",
      "\n",
      "Pr(r ∈ Ck) −\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "+\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "Erk [(cid:96)(h, rk)] −\n",
      "\n",
      "|Ik|\n",
      "\n",
      "≤ B\n",
      "\n",
      "Pr(r ∈ Ck) −\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "+\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "Erk [(cid:96)(h, rk)] −\n",
      "\n",
      "|Ik|\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "By using the Bretagnolle-Huber-Carol inequality [van der Vaart and Wellner, 1996, A6.6 Proposition],\n",
      "we have that for any δ > 0, with probability at least 1 − δ,\n",
      "(cid:114)\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "k=1\n",
      "\n",
      "Pr(r ∈ Ck) −\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "≤\n",
      "\n",
      "2K ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "(cid:12)\n",
      "Here, notice that the term of (cid:80)K\n",
      "(cid:12)\n",
      "(cid:12) does not depend on h ∈ H. Moreover,\n",
      "note that for any (f, h, M ) such that M > 0 and B ≥ 0 for all X, we have that P(f (X) ≥ M ) ≥\n",
      "P(f (X) > M ) ≥ P(Bf (X) + h(X) > BM + h(X)), where the probability is with respect to the\n",
      "randomness of X. Thus, by combining (6) and (7), we have that for any h ∈ H, for any δ > 0, with\n",
      "probability at least 1 − δ, the following holds for all h ∈ H,\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)Pr(r ∈ Ck) − |Ik|\n",
      "(cid:12)\n",
      "\n",
      "k=1\n",
      "\n",
      "n\n",
      "\n",
      "(7)\n",
      "\n",
      "Er[(cid:96)(h, r)] −\n",
      "\n",
      "(cid:96)(h, ri) ≤\n",
      "\n",
      "|Ik|\n",
      "\n",
      "αk(h) −\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      " + c\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "In particular, the ﬁrst term from the previous lemma will be bounded with the following lemma:\n",
      "Lemma 2. For any f ∈ {f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "\n",
      "base},\n",
      "\n",
      "|Ik|\n",
      "\n",
      "αk(f ) −\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "(cid:96)(f, ri)\n",
      "\n",
      " ≤ R\n",
      "\n",
      "Lϕ(f )\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "Proof. By using the triangle inequality,\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "≤\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "\n",
      "Er[(cid:96)(f, r)|r ∈ Ck] −\n",
      "\n",
      "|Ik|\n",
      "\n",
      "\n",
      "\n",
      "(cid:96)(f, ri)\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "|Ik|\n",
      "\n",
      "Er[(cid:96)(f, r)|r ∈ Ck] −\n",
      "\n",
      "(cid:96)(f, ri)\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Furthermore, by using the triangle inequality,\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:96)(f, ri)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Er[(cid:96)(f, r)|r ∈ Ck] −\n",
      "\n",
      "Er[(cid:96)(f, r)|r ∈ Ck] −\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:96)(f, ri)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "=\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "≤ sup\n",
      "\n",
      "r,r(cid:48)∈Ck\n",
      "\n",
      "(cid:12)Er[(cid:96)(f, r)|r ∈ Ck] − (cid:96)(f, ri)(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "i∈Ik\n",
      "(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\n",
      "(cid:12)\n",
      "(cid:12) .\n",
      "\n",
      "SEM(τ ) ◦στ , since gS\n",
      "\n",
      "SEM(τ ) ∈ GS, by using the Lipschitz continuity, boundedness,\n",
      "\n",
      "If f = f S\n",
      "and non-negativity,\n",
      "\n",
      "SEM(τ ) = gS\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\n",
      "\n",
      "sup\n",
      "r,r(cid:48)∈Ck\n",
      "\n",
      "|(ly ◦ gS\n",
      "\n",
      "SEM(τ ))(στ (z)) − (ly ◦ gS\n",
      "\n",
      "SEM(τ ))(στ (z(cid:48)))|\n",
      "\n",
      "sup\n",
      "z,z(cid:48)∈Zk\n",
      "\n",
      "(cid:12) = sup\n",
      "y∈Y\n",
      "≤ R sup\n",
      "\n",
      "z,z(cid:48)∈Zk\n",
      "\n",
      "(cid:107)στ (z) − στ (z(cid:48))(cid:107)F\n",
      "\n",
      "(cid:118)\n",
      "(cid:117)\n",
      "(cid:117)\n",
      "(cid:116)\n",
      "\n",
      "L\n",
      "(cid:88)\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "j=1\n",
      "\n",
      "(στ (zt,j) − στ (z(cid:48)\n",
      "\n",
      "t,j))2\n",
      "2\n",
      "\n",
      "sup\n",
      "i∈[V ]\n",
      "\n",
      "sup\n",
      "q,q(cid:48)∈Qi\n",
      "\n",
      "(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "2\n",
      "\n",
      "= R sup\n",
      "\n",
      "z,z(cid:48)∈Zk\n",
      "\n",
      "L\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:118)\n",
      "(cid:117)\n",
      "(cid:117)\n",
      "(cid:116)\n",
      "\n",
      "(cid:115)\n",
      "\n",
      "≤ R\n",
      "\n",
      "= R\n",
      "\n",
      "Lϕ(f S\n",
      "\n",
      "SEM(τ ))\n",
      "n\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "base = gS\n",
      "\n",
      "Similarly, if f = f S\n",
      "and non-negativity,\n",
      "(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "base, since gS\n",
      "\n",
      "sup\n",
      "r,r(cid:48)∈Ck\n",
      "\n",
      "base ∈ GS, by using the Lipschitz continuity, boundedness,\n",
      "\n",
      "|(ly ◦ gS\n",
      "\n",
      "base)(z) − (ly ◦ gS\n",
      "\n",
      "base)(z(cid:48))|\n",
      "\n",
      "sup\n",
      "z,z(cid:48)∈Zk\n",
      "\n",
      "(cid:12) = sup\n",
      "y∈Y\n",
      "≤ R sup\n",
      "\n",
      "(cid:107)z − z(cid:48)(cid:107)F\n",
      "\n",
      "z,z(cid:48)∈Zk\n",
      "(cid:114)\n",
      "\n",
      "Lϕ(f S\n",
      "base)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "≤ R\n",
      "\n",
      "Therefore, for any f ∈ {f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "base},\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "|Ik|\n",
      "\n",
      "αk(f ) −\n",
      "\n",
      "(cid:96)(f, ri)\n",
      "\n",
      " ≤\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "|Ik|R(cid:112)Lϕ(f ) = R\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "Lϕ(f )\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "Combining Lemma 1 and Lemma 2, we obtain the following upper bound on the gap:\n",
      "Lemma 3. For any δ > 0, with probability at least 1 − δ,the following holds for any f ∈\n",
      "{f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "base}:\n",
      "\n",
      "Er[(cid:96)(f, r)] −\n",
      "\n",
      "(cid:96)(f, ri) ≤ R\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "Lϕ(f )\n",
      "n\n",
      "\n",
      "+ c\n",
      "\n",
      "ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "Proof. This follows directly from combining Lemma 1 and Lemma 2.\n",
      "\n",
      "We now provide an upper bound on ϕ(f S\n",
      "\n",
      "SEM(τ )) in the following lemma:\n",
      "\n",
      "Lemma 4. For any τ > 0,\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ ))\n",
      "n\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "+ (V − 1)\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      ".\n",
      "\n",
      "Proof. Recall the deﬁnition:\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ ))\n",
      "n\n",
      "\n",
      "= sup\n",
      "i∈[V ]\n",
      "\n",
      "sup\n",
      "q,q(cid:48)∈Qi\n",
      "\n",
      "(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "2.\n",
      "\n",
      "στ (q)j =\n",
      "\n",
      "eqj /τ\n",
      "t=1 eqt/τ\n",
      "\n",
      ",\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "where\n",
      "\n",
      "and\n",
      "\n",
      "for j = 1, . . . , V . By the symmetry and independence over i ∈ [V ] inside of the ﬁrst supremum, we\n",
      "have\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ ))\n",
      "n\n",
      "\n",
      "= sup\n",
      "\n",
      "q,q(cid:48)∈Q1\n",
      "\n",
      "(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "2.\n",
      "\n",
      "For any q, q(cid:48) ∈ Q1 and i ∈ {2, . . . , V } (with q = (q1, . . . , qV ) and q(cid:48) = (q(cid:48)\n",
      "δi, δ(cid:48)\n",
      "\n",
      "i > 0 such that\n",
      "\n",
      "1, . . . , q(cid:48)\n",
      "\n",
      "V )), there exists\n",
      "\n",
      "Here, since zik − ∆ ≥ zij from the assumption, we have that for all i ∈ {2, . . . , V },\n",
      "\n",
      "qi = q1 − δi\n",
      "\n",
      "i = q(cid:48)\n",
      "q(cid:48)\n",
      "\n",
      "1 − δ(cid:48)\n",
      "i.\n",
      "\n",
      "δi, δ(cid:48)\n",
      "\n",
      "i ≥ ∆ > 0.\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Thus, we can rewrite\n",
      "\n",
      "Similarly,\n",
      "\n",
      "Using these,\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "eqt/τ = eq1/τ +\n",
      "\n",
      "e(q1−δi)/τ\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "i=2\n",
      "\n",
      "= eq1/τ + eq1/τ\n",
      "\n",
      "e−δi/τ\n",
      "\n",
      "= eq1/τ\n",
      "\n",
      "1 +\n",
      "\n",
      "e−δi/τ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "i=2\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "i=2\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "i=2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "eq(cid:48)\n",
      "\n",
      "t/τ = eq(cid:48)\n",
      "\n",
      "1/τ\n",
      "\n",
      "1 +\n",
      "\n",
      "e−δ(cid:48)\n",
      "\n",
      "i/τ\n",
      "\n",
      " .\n",
      "\n",
      "στ (q)1 =\n",
      "\n",
      "eq1/τ\n",
      "t=1 eqt/τ\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "=\n",
      "\n",
      "eq1/τ\n",
      "1 + (cid:80)V\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "eq1/τ\n",
      "\n",
      "i=2 e−δi/τ\n",
      "\n",
      "(cid:17) =\n",
      "\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−δi/τ\n",
      "\n",
      "and for all j ∈ {2, . . . , V },\n",
      "\n",
      "στ (q)j =\n",
      "\n",
      "eqj /τ\n",
      "t=1 eqt/τ\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "e(q1−δj )/τ\n",
      "1 + (cid:80)V\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "i=2 e−δi/τ\n",
      "\n",
      "eq1/τ\n",
      "\n",
      "e−δj /τ\n",
      "\n",
      "1 + (cid:80)V\n",
      "\n",
      "i=2 e−δi/τ\n",
      "1\n",
      "1 + eδj /τ + (cid:80)V\n",
      "\n",
      "e(δj −δi)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "στ (q(cid:48))1 =\n",
      "\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−δ(cid:48)\n",
      "\n",
      "i/τ\n",
      "\n",
      ",\n",
      "\n",
      "στ (q(cid:48))j =\n",
      "\n",
      "1\n",
      "j /τ + (cid:80)V\n",
      "\n",
      "1 + eδ(cid:48)\n",
      "\n",
      "e(δ(cid:48)\n",
      "\n",
      "j −δ(cid:48)\n",
      "\n",
      "i)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      ".\n",
      "\n",
      "where Ij := {2, . . . , V } \\ {j}. Similarly,\n",
      "\n",
      "and for all j ∈ {2, . . . , V },\n",
      "\n",
      "Using these, for any q, q(cid:48) ∈ Q1,\n",
      "\n",
      "|στ (q)1 − στ (q(cid:48))1| =\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "≤\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−δi/τ\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "i/τ\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−δ(cid:48)\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−∆/τ\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      ",\n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "≤\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "and for all j ∈ {2, . . . , V },\n",
      "\n",
      "|στ (q)j − στ (q(cid:48))j| =\n",
      "\n",
      "1\n",
      "1 + eδj /τ + (cid:80)V\n",
      "\n",
      "e(δj −δi)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "j /τ + (cid:80)V\n",
      "\n",
      "1 + eδ(cid:48)\n",
      "\n",
      "e(δ(cid:48)\n",
      "\n",
      "j −δ(cid:48)\n",
      "\n",
      "i)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "1\n",
      "1 + e∆/τ + (cid:80)V\n",
      "\n",
      "e(∆−2)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ + (cid:80)V\n",
      "\n",
      "e(2−∆)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "1\n",
      "1 + e∆/τ + (V − 2)e(∆−2)/τ\n",
      "\n",
      "1\n",
      "1 + e2/τ + (V − 2)e(2−∆)/τ\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "By combining these,\n",
      "\n",
      "sup\n",
      "q,q(cid:48)∈Q1\n",
      "\n",
      "(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "2\n",
      "\n",
      "= sup\n",
      "\n",
      "q,q(cid:48)∈Q1\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "|στ (q)j − στ (q(cid:48))j|2\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      ".\n",
      "\n",
      "+ (V − 1)\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "Using the previous lemma, we will conclude the asymptotic behavior of ϕ(f S\n",
      "lemma:\n",
      "Lemma 5. It holds that\n",
      "\n",
      "SEM(τ )) in the following\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) → 0 as τ → 0.\n",
      "\n",
      "Proof. Using Lemma 4,\n",
      "\n",
      "lim\n",
      "τ →0\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) ≤ lim\n",
      "τ →0\n",
      "\n",
      "n\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "+ n(V − 1) lim\n",
      "τ →0\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      ".\n",
      "\n",
      "lim\n",
      "τ →0\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "=\n",
      "\n",
      "−\n",
      "\n",
      "= 0,\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "lim\n",
      "τ →0\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "= |0 − 0|2 = 0.\n",
      "\n",
      "(cid:12)\n",
      "2\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Moreover,\n",
      "\n",
      "and\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Therefore,\n",
      "\n",
      "Since ϕ(f S\n",
      "\n",
      "SEM(τ )) ≥ 0, this implies the statement of this lemma.\n",
      "\n",
      "lim\n",
      "τ →0\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) ≤ 0.\n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "As we have analyzed ϕ(f S\n",
      "SEM(τ )) and ϕ(f S\n",
      "ϕ(f S\n",
      "Lemma 6. For any τ > 0,\n",
      "\n",
      "SEM(τ )) in the previous two lemmas, we are now ready to compare\n",
      "\n",
      "base), which is done in the following lemma:\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) − ϕ(f S\n",
      "\n",
      "base) ≤\n",
      "\n",
      "(1 − V ) < 0.\n",
      "\n",
      "3n\n",
      "4\n",
      "\n",
      "Proof. From Lemma 4, for any τ > 0,\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) ≤ n\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "1\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "1\n",
      "(cid:12)\n",
      "(cid:18) 1\n",
      "1\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "≤ n\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "1\n",
      "1 + (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2))\n",
      "\n",
      "(cid:12)\n",
      "2\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "= n\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "1\n",
      "2 + (V − 2)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (V − 1)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "≤ n\n",
      "\n",
      "−\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "= n\n",
      "\n",
      "−\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "1\n",
      "V\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:19)2\n",
      "\n",
      "1\n",
      "V\n",
      "\n",
      "1\n",
      "V\n",
      "\n",
      "(cid:12)\n",
      "2\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "− 0\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      ".\n",
      "\n",
      "(cid:12)\n",
      "2\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Recall the deﬁnition of\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "base) = sup\n",
      "i∈[V ]\n",
      "\n",
      "sup\n",
      "q,q(cid:48)∈Qi\n",
      "\n",
      "n(cid:107)q − q(cid:48)(cid:107)2\n",
      "2.\n",
      "\n",
      "By choosing an element in the set over which the supremum is taken, for any δ ≥ ∆ > 0,\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "base) ≥ sup\n",
      "\n",
      "n(cid:107)q − q(cid:48)(cid:107)2\n",
      "\n",
      "2 ≥ n(cid:107)ˆq − ˆq(cid:48)(cid:107)2\n",
      "\n",
      "2 = n\n",
      "\n",
      "(ˆqj − ˆq(cid:48)\n",
      "\n",
      "j)2\n",
      "\n",
      "2 = n(2 − δ)2V,\n",
      "\n",
      "q,q(cid:48)∈Q1\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "where ˆq1 = 1, ˆqj = 1 − δ for j ∈ {2, . . . , V }, ˆq(cid:48)\n",
      "By combining those, for for any τ > 0 and δ ≥ ∆ > 0,\n",
      "\n",
      "1 = δ − 1, and ˆq(cid:48)\n",
      "\n",
      "j = −1 for j ∈ {2, . . . , V }.\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) − ϕ(f S\n",
      "n\n",
      "\n",
      "base)\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:18) 1\n",
      "1\n",
      "\n",
      "(cid:19)2\n",
      "\n",
      "1\n",
      "V\n",
      "\n",
      "+ (V − 1)\n",
      "\n",
      "− (2 − δ)2V\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      "≤ 1 +\n",
      "\n",
      "V −\n",
      "\n",
      "− (2 − δ)2V\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "+\n",
      "\n",
      "− V\n",
      "\n",
      "V − (2 − δ)2V\n",
      "(cid:18)\n",
      "\n",
      "(2 − δ)2 −\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "≤\n",
      "\n",
      "− V\n",
      "\n",
      "1 −\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      "=\n",
      "\n",
      "(1 − V )\n",
      "\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "4\n",
      "1\n",
      "4\n",
      "\n",
      "18\n",
      "\n",
      "\f",
      "We combine the lemmas above to prove Theorem 1, which is restated below with its proof:\n",
      "\n",
      "Theorem 1. Let V ≥ 2. For any δ > 0, with probability at least 1 − δ, the following holds for any\n",
      "fS ∈ {f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "base}:\n",
      "\n",
      "Ez,y[l(fS(z), y)] ≤\n",
      "\n",
      "l(fS(z(i)), y(i)) + R\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "Lϕ(fS)\n",
      "n\n",
      "\n",
      "+ c\n",
      "\n",
      "ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "Moreover,\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) → 0 as τ → 0\n",
      "\n",
      "and ϕ(f S\n",
      "\n",
      "SEM(τ )) − ϕ(f S\n",
      "\n",
      "base) ≤\n",
      "\n",
      "(1 − V ) < 0 ∀τ > 0.\n",
      "\n",
      "3n\n",
      "4\n",
      "\n",
      "Proof. The ﬁrst statement directly follows from Lemma 3. The second statement is proven by\n",
      "Lemma 5 and Lemma 6.\n",
      "\n",
      "B Hyperparameters\n",
      "\n",
      "We present the hype-parameters used to train for BYOL+SEM on CIFAR100. The same parameters\n",
      "were used for ResNet18 and ResNet50.\n",
      "\n",
      "Learning rate\n",
      "Weight-decay\n",
      "Optimizer\n",
      "BYOL EMA\n",
      "Vocabulary size (V)\n",
      "Message length (L)\n",
      "τ online network\n",
      "τ target network\n",
      "\n",
      "0.5\n",
      "1e-4\n",
      "AdamW\n",
      "0.99\n",
      "13\n",
      "5000\n",
      "0.5\n",
      "0.5\n",
      "\n",
      "(a) BYOL+SEM hyper-parameters.\n",
      "\n",
      "C Experiment details for ImageNet\n",
      "\n",
      "C.1\n",
      "\n",
      "Image augmentation\n",
      "\n",
      "We follow the same procedure as [Grill et al., 2020] for the image augmentation procedure. The\n",
      "augmentation applied in order during training are:\n",
      "\n",
      "• Random Resize crop to a 224 × 224 image. A random patch of the image is selected and\n",
      "\n",
      "resized to a 224 × 224 image.\n",
      "\n",
      "• Random color jitter. Modifying the brightness, the contrast, the saturation and the hue.\n",
      "\n",
      "• Random gray scale. Randomly applying a gray scale ﬁlter to the image\n",
      "\n",
      "• Random gaussian blur. Randomly applying a gaussian blue ﬁlter.\n",
      "\n",
      "• Random solarization. Randomly applying a solarization ﬁlter.\n",
      "\n",
      "At validation and test time, we resize the images to 256 × 256 and then center crop a patch of\n",
      "224 × 224.\n",
      "\n",
      "For both training and evaluation, we re-normalize the image using the statistic of the training set.\n",
      "\n",
      "C.2 Hyper-parameters\n",
      "\n",
      "We summarize the hyper-parameters for BYOL with SEM and MoCo with SEM in table 4.\n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "Learning rate\n",
      "Batch size\n",
      "Weight-decay\n",
      "Optimizer\n",
      "Epochs\n",
      "Base momentum\n",
      "Vocabulary size (V)\n",
      "Message length (L)\n",
      "τ online network\n",
      "τ target network\n",
      "\n",
      "0.9\n",
      "256\n",
      "1e-6\n",
      "SGD with lars\n",
      "100\n",
      "0.99\n",
      "29\n",
      "465\n",
      "2.397\n",
      "2.386\n",
      "\n",
      "(a) BYOL+SEM hyper-parameters.\n",
      "\n",
      "Learning rate\n",
      "Batch size\n",
      "Weight-decay\n",
      "Optimizer\n",
      "Epochs\n",
      "MoCo’s EMA\n",
      "Vocabulary size (V)\n",
      "Message Length (L)\n",
      "τ online network\n",
      "τ target network\n",
      "\n",
      "0.6\n",
      "256\n",
      "3e-5\n",
      "SGD with lars\n",
      "100\n",
      "0.1\n",
      "12\n",
      "512\n",
      "1.35\n",
      "1.2\n",
      "\n",
      "(b) MoCo+SEM hyper-parameters.\n",
      "\n",
      "Table 4: ImageNet’s experiment hyper-parameters.\n",
      "\n",
      "C.3 Linear evaluation\n",
      "\n",
      "We follow the evaluation protocol from [Chen et al., 2020b]. The linear evaluation is done by training\n",
      "a linear classiﬁer on the frozen representation of the ImageNet training samples. We train a linear\n",
      "classiﬁer with a cross-entropy objective for 100 epochs using SGD with nesterov and a batch size of\n",
      "512. During training, we apply random resized crop and random horizontal ﬂip.\n",
      "\n",
      "C.4 Semi-supervised learning\n",
      "\n",
      "We perform semi-supervised experiments by training a linear classiﬁer on top of a frozen repre-\n",
      "sentation. The procedure is the same as the linear evaluation procedure with the exception that we\n",
      "train with 1% of the training sample. That training samples are taken according to the split deﬁned\n",
      "in [Chen et al., 2020b].\n",
      "\n",
      "C.5 Robustness experiments\n",
      "\n",
      "We follow the evaluation procedure from [Lee et al., 2021]. We treated the robustness datasets as\n",
      "additional \"test sets\" in that we simply evaluated them using the evaluation procedure described\n",
      "above. The images were resized to a 256 × 256 before being center cropped to a 224 × 224 image.\n",
      "The evaluation procedure was performed using the public robustness benchmark evaluation code\n",
      "of [Djolonga et al., 2020]2.\n",
      "\n",
      "C.6 Transfer learning experiments\n",
      "\n",
      "We follow the linear evaluation protocol of [Kolesnikov et al., 2019; Chen et al., 2020b] We train a\n",
      "linear classiﬁer using a regularized multinomial logistic regression from the scikit-learn package [Pe-\n",
      "dregosa et al., 2011]. The representation is frozen, so that we do not train the encoder backbone nor\n",
      "\n",
      "2https://github.com/google-research/robustness_metrics\n",
      "\n",
      "20\n",
      "\n",
      "\f",
      "Superclass\n",
      "aquatic mammals\n",
      "ﬁsh\n",
      "ﬂowers\n",
      "food containers\n",
      "fruit and vegetables\n",
      "household electrical devices\n",
      "household furniture\n",
      "insects\n",
      "large carnivores\n",
      "large man-made outdoor things\n",
      "large natural outdoor scenes\n",
      "large omnivores and herbivores\n",
      "medium-sized mammals\n",
      "non-insect invertebrates\n",
      "people\n",
      "reptiles\n",
      "small mammals\n",
      "trees\n",
      "vehicles 1\n",
      "vehicles 2\n",
      "\n",
      "Classes\n",
      "beaver, dolphin, otter, seal, whale\n",
      "aquarium ﬁsh, ﬂatﬁsh, ray, shark, trout\n",
      "orchids, poppies, roses, sunﬂowers, tulips\n",
      "bottles, bowls, cans, cups, plates\n",
      "apples, mushrooms, oranges, pears, sweet peppers\n",
      "clock, computer keyboard, lamp, telephone, television\n",
      "bed, chair, couch, table, wardrobe\n",
      "bee, beetle, butterﬂy, caterpillar, cockroach\n",
      "bear, leopard, lion, tiger, wolf\n",
      "bridge, castle, house, road, skyscraper\n",
      "cloud, forest, mountain, plain, sea\n",
      "camel, cattle, chimpanzee, elephant, kangaroo\n",
      "fox, porcupine, possum, raccoon, skunk\n",
      "crab, lobster, snail, spider, worm\n",
      "baby, boy, girl, man, woman\n",
      "crocodile, dinosaur, lizard, snake, turtle\n",
      "hamster, mouse, rabbit, shrew, squirrel\n",
      "maple, oak, palm, pine, willow\n",
      "bicycle, bus, motorcycle, pickup truck, train\n",
      "lawn-mower, rocket, streetcar, tank, tractor\n",
      "\n",
      "Table 5: Set of classes for each superclass on CIFAR-100.\n",
      "\n",
      "the batch-normalization statistics. We do not perform any augmentations and the images are resized\n",
      "to 224 pixels using bicubic resampling and the normalized using the statistics on ImageNet’s training\n",
      "set. We tune the regularizer term from a range of 11 logarithmically-spaced values between 10−6 and\n",
      "105 using a small validation set and re-train using the full training set.\n",
      "\n",
      "D CIFAR100 superclass\n",
      "\n",
      "The 100 classes of CIFAR-100 [Krizhevsky, 2009] are grouped into 20 superclasses. The list of\n",
      "superclass for each class in Table 5\n",
      "\n",
      "E Additional CIFAR-100 relevance graphs\n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "(a) BYOL baseline\n",
      "\n",
      "(b) BYOL baseline with a large rep-\n",
      "resentation\n",
      "\n",
      "(c) BYOL + SEM\n",
      "\n",
      "Figure 5: Comparison of the full relevance graph W5 between BYOL and BYOL + SEM.\n",
      "\n",
      "22\n",
      "\n",
      "S363otterkangaroobottleS382lampS225beartankS144streetcarroadS249S167trainS94lobsterS273boygirlhousewomanS29S106cockroachS6appledolphinsealwhaleaquarium_fishflatfishraysharkorchidpoppyrosesunflowertulipbowlcancupplatemushroomorangepearsweet_pepperclockcomputer_keyboardtelephonetelevisionbedchaircouchtablewardrobebeebeetlebutterflycaterpillarleopardtigerwolfbridgecastleskyscrapercloudforestmountainplainseacamelcattlechimpanzeefoxpossumraccoonskunkcrabsnailwormbabymanlizardsnaketurtlehamstermouserabbitshrewsquirrelmaple_treeoak_treepalm_treewillow_treebicyclebusmotorcyclepickup_trucklawn_mowerrockettractorS55S63S47S279S162S69S134S242S34S241S0S98S158S361S91S66S12S95S388S53S374S190S22S59S183S261S1S375S27S35S130S110S127S43S111S258S380S214S30S85S292S193S238S336S128S132S284S14S99S93S332S236S298S260S281S68S204S161S182S177S381S64S227S320S318S266S230S196S126S181S184S114S187flatfishtableS16troutcaterpillarS194cattleS276orchidcomputer_keyboardS229palm_treeS184cloudrocketskunkchimpanzeeS256S187pickup_trucksnailcameltigerspiderS307dolphinsweet_pepperS85lionS434S415lobstersunflowermushroomS66orangesquirrelS189S319manS190snakeS195babywardrobewolfsealwillow_treecanS323maple_treeS252S404turtleleopardhamsterrabbitcockroachS353clockforestS87S61pine_treeS369couchS397tankotterwomanbicycleS365S148bedbeavertulipbeelawn_mowercrocodilelamptelephoneS435tractorS422oak_treeS233motorcycleshrewS284S33S146S13plainS78whalebusbeeS248spiderbeetlecockroachS178S240bearkangarootigerS393leopardhousecastleS40roadS107plainS403motorcyclelawn_mowertankS350tractorsealS61beaverS284porcupinebedS168couchS360chairlionfoxS216wolfS272skyscraperS227mountainS78seasnakeS98S49S219S2lizardwormorangeS185S30sweet_pepperpearS198S193applebusS18S133streetcarS370S106pickup_trucktrainroseS306poppyS289S329tulipS334S135orchidS210cancupS31S259S321S291bowlplatebottlewillow_treepine_treemaple_treeoak_treeforestS71palm_treeS138S327S405dolphinrayS151S347S17S379turtlesharkS176S189S355whaleS381S221S200S22boyS157girlwomanbabyS204flatfishmanmouseraccoonS336S271S300possumsquirrelS239S128shrewhamsterrabbit\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "\n",
    "print(array_authors[0])\n",
    "print(array_pdf_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc82c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pakawut.Jiradilok']\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      " \n",
      "r\n",
      "p\n",
      "A\n",
      " \n",
      "1\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      "O\n",
      "C\n",
      ".\n",
      "h\n",
      "t\n",
      "a\n",
      "m\n",
      "\n",
      "[\n",
      " \n",
      " \n",
      "1\n",
      "v\n",
      "5\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      ".\n",
      "4\n",
      "0\n",
      "2\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "LARGE-SCALE ROOK PLACEMENTS\n",
      "\n",
      "PAKAWUT JIRADILOK\n",
      "\n",
      "Abstract. For each certain “nice” piecewise linear function f : [0, 1] → [0, 1], we con-\n",
      "sider a family of growing Young diagrams {λ(f, N )}∞\n",
      "N=1 by enlarging the region under\n",
      "the graph of f . We compute asymptotic formulas for the number of rook placements\n",
      "of the shape λ(f, N ). We prove that the normalized cumulative X-ray of a uniformly\n",
      "random permutation, as the size of the permutation grows, exhibits a limit shape phe-\n",
      "nomenon.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "While rook placements are classical objects in combinatorics (cf. e.g. [Rio02, Chapter 7]\n",
      "and [Sta12, Chapter 2]), there are many recent works in the literature studying them (e.g.\n",
      "[BR06, BLRS14, BLRS16, Bar21]). Enumerative combinatorics of rook placements deals\n",
      "with problems of counting the number of ways to place a certain number of non-attacking\n",
      "rooks on a certain subset of a chessboard. In many cases, one obtains nice formulas. For\n",
      "instance, it is a well-known elementary exercise that if the subset of the chessboard takes\n",
      "the shape of a Young diagram of a partition, then the number of rook placements has a nice\n",
      "product formula.\n",
      "\n",
      "In this paper, we study large-scale rook placements. We are interested in the family of\n",
      "rook placements when the board on which non-attacking rooks are placed grows in size in\n",
      "the following manner. We deﬁne a class\n",
      "of “nice” piecewise linear functions from [0, 1] to\n",
      "belongs to this class.\n",
      "[0, 1] (see the exact deﬁnition in Subsection 3.1). Suppose that f\n",
      "We obtain a family of Young diagrams by dilating the region under the graph of f from the\n",
      "[0, N ] for each positive integer N . More precisely, we let\n",
      "unit square [0, 1]\n",
      "λ(f, N ) be the Young diagram with N rows whose ith row has (λ(f, N ))i :=\n",
      "f (i/N )\n",
      "⌉\n",
      "boxes (see Subsection 3.2 for details). Let RP(λ(f, N )) denote the set of rook placements of\n",
      "the shape λ(f, N ). Our ﬁrst main result of this paper, Theorem 3.7, says that the cardinality\n",
      "of RP(λ(f, N )) behaves well asymptotically:\n",
      "\n",
      "[0, 1] to [0, N ]\n",
      "\n",
      "∈ P\n",
      "\n",
      "N\n",
      "\n",
      "×\n",
      "\n",
      "×\n",
      "\n",
      "P\n",
      "\n",
      "⌈\n",
      "\n",
      "·\n",
      "\n",
      "log (# RP(λ(f, N ))) = N log N + Bf ·\n",
      "\n",
      "N +\n",
      "\n",
      "log N + Of (1),\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "for positive integers N which are multiples of a certain integer depending on f (see the\n",
      "statement of Theorem 3.7 for details), and we establish the following integral formula for\n",
      "the coeﬃcient Bf :\n",
      "\n",
      "1\n",
      "\n",
      "Bf =\n",
      "\n",
      "log(f (x) + x\n",
      "\n",
      "1) dx.\n",
      "\n",
      "−\n",
      "\n",
      "0\n",
      "Z\n",
      "Our next stop is a special subclass\n",
      "\n",
      "of the function class\n",
      "\n",
      "as\n",
      "the “combinatorial” class, since it contains, rather naturally, many familiar objects from\n",
      "is a countable\n",
      "algebraic combinatorics such as Dyck paths and Motzkin paths. The class\n",
      "Pk (see Section 4 for the precise deﬁnition).\n",
      "union of ﬁnite families of functions:\n",
      "Bijective combinatorics in\n",
      "is noteworthy, and so we spend Subsection 4.1 discussing it.\n",
      "e\n",
      "This subsection contains a bijective-combinatorics ﬂavor which seems less analytic than its\n",
      "\n",
      ". We might refer to\n",
      "\n",
      "e\n",
      "=\n",
      "\n",
      "∞k=1\n",
      "\n",
      "S\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "e\n",
      "\n",
      "e\n",
      "\n",
      "e\n",
      "\n",
      "e\n",
      "\n",
      "Date: April 4, 2022.\n",
      "2020 Mathematics Subject Classiﬁcation. 05A16 (Primary) 05A05, 05A19, 05A20, 60F05 (Secondary).\n",
      "Key words and phrases. large-scale rook placement, Young diagram, partition, dilation, asymptotic for-\n",
      "mula, integral formula, Dyck path, Motzkin path, Schr¨oder number, ground bump, waterfall, combinatorial\n",
      "inequality, permutation, X-ray, cumulative X-ray, limit shape, random permutation.\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "neighboring parts. For example, we provide bijective arguments resulting in Corollary 4.2\n",
      "which states\n",
      "\n",
      "=\n",
      "\n",
      "Pk|\n",
      "\n",
      "|\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "3k\n",
      "k\n",
      "\n",
      "2\n",
      "−\n",
      "1\n",
      "−\n",
      "\n",
      ".\n",
      "\n",
      "(cid:19)\n",
      "The functions in\n",
      "Pk are in one-to-one correspondence with combinatorial objects which we\n",
      "call “waterfalls” (see Subsection 4.1 for details). Studying waterfalls yields the following\n",
      "curious combinatorial formula, given in Proposition 4.9:\n",
      "e\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "e\n",
      "\n",
      "wt(D) =\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "3k\n",
      "k\n",
      "\n",
      "2\n",
      "−\n",
      "1\n",
      "−\n",
      "\n",
      ",\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "Dyck(k)\n",
      "\n",
      "XD\n",
      "∈\n",
      "\n",
      "where Dyck(k) denotes the family of Dyck paths of length 2k — lattice paths from (0, k)\n",
      "to (k, 0) with k unit steps to the right and k unit steps down which never go below the line\n",
      "X + Y = k — and the weight wt(D) is deﬁned as\n",
      "\n",
      "wt(D) :=\n",
      "\n",
      "#\n",
      "\n",
      "j\n",
      "\n",
      "{\n",
      "\n",
      "∈\n",
      "\n",
      "Z\n",
      "\n",
      "|\n",
      "\n",
      "i + j > k and (i, j)\n",
      "\n",
      "D\n",
      "\n",
      ".\n",
      "\n",
      "}\n",
      "\n",
      "∈\n",
      "\n",
      "k\n",
      "\n",
      "1\n",
      "\n",
      "−\n",
      "\n",
      "i=1\n",
      "Y\n",
      "\n",
      "Having visited waterfalls, we proceed to Subsection 4.2. Functions in the combinatorial\n",
      "class allow for even more precise asymptotics for the number of rook placements. Our second\n",
      "main result, Theorem 4.10, provides the following asymptotic formula for f\n",
      "Pk as follows.\n",
      "We have\n",
      "e\n",
      "log N + Df + Of (1/N ),\n",
      "\n",
      "log (# RP(λ(f, N ))) = N log N + Bf ·\n",
      "\n",
      "N +\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "∈\n",
      "\n",
      "for positive integers N\n",
      "the following integral formula for the coeﬃcient Df :\n",
      "\n",
      "∈\n",
      "\n",
      "kZ, where the coeﬃcient Bf is the same as before, and we give\n",
      "\n",
      "Df :=\n",
      "\n",
      "log(2π) +\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "0\n",
      "Z\n",
      "\n",
      "xf ′(x)\n",
      "x(f (x) + x\n",
      "\n",
      "f (x) + 1\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "dx.\n",
      "\n",
      "−\n",
      "\n",
      "∈\n",
      "\n",
      "Now that for each function f\n",
      "\n",
      "Pk, there are two coeﬃcients Bf and Df associated to\n",
      "it, one might wonder about the possible ranges of these numbers. Proposition 4.16 states\n",
      "that\n",
      "e\n",
      "\n",
      "1\n",
      "k ≤\n",
      "Both upper bound and lower bound are tight. Each of them is attained by exactly one\n",
      "function in\n",
      "\n",
      "Bf ≤ −\n",
      "\n",
      "log k\n",
      "\n",
      "1.\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "Similarly, we have tight bounds for Df . Proposition 4.17 states that\n",
      "\n",
      "Pk.\n",
      "e\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "log\n",
      "\n",
      "2π\n",
      "k\n",
      "\n",
      "Df ≤\n",
      "\n",
      "≤\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "log\n",
      "\n",
      "2kπ\n",
      "k\n",
      "\n",
      ".\n",
      "\n",
      "(cid:18)\n",
      "The upper bound is attained by exactly one function in\n",
      "Pk. The equality cases for the lower\n",
      "bound is rather interesting: the lower bound is attained by a Catalan-numerous family of\n",
      "e\n",
      "functions inside\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "Each rook placement — or, more generally, each permutation — comes with a certain\n",
      "sequence of non-negative integers called the X-ray. An object which appears in the ﬁeld of\n",
      "discrete tomography (cf. e.g. [HK99]), the X-ray of permutation has been investigated from\n",
      "algebraic and combinatorial points of view (cf. e.g. [BF14, BMPS05]). It is related to other\n",
      "objects in combinatorics such as Skolem sets (cf. e.g. [Nor08]) and permutohedra (cf. e.g.\n",
      "[Pos09]). For each permutation π\n",
      "n permutation matrix,\n",
      "the cumulative X-ray of π is the function ξπ : [0, 2n]\n",
      "\n",
      "Sn, which we consider as an n\n",
      "\n",
      "[0, n] given by\n",
      "\n",
      "×\n",
      "\n",
      "∈\n",
      "\n",
      "Pk.\n",
      "e\n",
      "\n",
      "ξπ(t) :=\n",
      "\n",
      "→\n",
      "πij ,\n",
      "\n",
      "[n]\n",
      "Xi,j\n",
      "∈\n",
      "i+j\n",
      "t\n",
      "≤\n",
      "2\n",
      "\n",
      "\f",
      "∈\n",
      "\n",
      "Sn is the function\n",
      "\n",
      "[0, 1] given by\n",
      "ξπ(nt). Thus, the graph of the normalized cumulative X-ray is simply the graph\n",
      "\n",
      "and the normalized cumulative X-ray of π\n",
      "ξπ(t) := 1\n",
      "n ·\n",
      "of the cumulative X-ray rescaled from the rectangle [0, 2n]\n",
      "We have arrived at our last stop, where we consider the X-ray of a random large rook\n",
      "e\n",
      "placement. For each a partition λ, we let N\n",
      "λ be the partition obtained from magnifying\n",
      "⊙\n",
      "λ by a factor of N (see Section 2 for the precise deﬁnition). Conjecture 5.9 predicts that the\n",
      "ξπ exhibits a limit shape phenomenon: for a ﬁxed real ε > 0,\n",
      "normalized cumulative X-ray\n",
      "if π is a uniformly random rook placement of the shape N\n",
      "\n",
      "[0, n] to [0, 2]\n",
      "\n",
      "ξπ : [0, 2]\n",
      "\n",
      "λ, then\n",
      "\n",
      "[0, 1].\n",
      "\n",
      "→\n",
      "\n",
      "×\n",
      "\n",
      "×\n",
      "\n",
      "e\n",
      "\n",
      "⊙\n",
      "\n",
      "mλ(t)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "sup\n",
      "[0,2]\n",
      "\n",
      "ξπ(t)\n",
      "\n",
      "−\n",
      "\n",
      "< ε\n",
      "\n",
      "1,\n",
      "\n",
      "! →\n",
      "\n",
      "as N\n",
      "→\n",
      "Equation (29) in Subsection 5.2 provides a formula for this function.\n",
      "\n",
      "→ ∞\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)e\n",
      "[0, 1] is a certain function depending on the shape λ.\n",
      "\n",
      " \n",
      "t\n",
      "∈\n",
      ", where mλ : [0, 2]\n",
      "\n",
      "Our third main result of this paper, Theorem 5.10, proves this conjecture in the special\n",
      "case when λ = (cid:3) is a partition with one box. In other words, it says that the normalized\n",
      "cumulative X-ray of a uniformly random permutation, as the size of the permutation grows,\n",
      "exhibits a limit shape phenomenon in the above sense. We note that it is easy to compute\n",
      "the limit shape for the permutation case explicitly:\n",
      "\n",
      "e\n",
      "\n",
      "P\n",
      "\n",
      "m(cid:3)(t) :=\n",
      "\n",
      "t2\n",
      "2\n",
      "\n",
      "(\n",
      "\n",
      "t2\n",
      "2 + 2t\n",
      "\n",
      "1\n",
      "\n",
      "if 0 < t\n",
      "if 1 < t\n",
      "\n",
      "1, and\n",
      "2.\n",
      "\n",
      "≤\n",
      "≤\n",
      "\n",
      "−\n",
      "While Theorem 5.10 proves Conjecture 5.9 for only one very special case, we hope that one\n",
      "proof technique is applicable, perhaps with some more work, for other shapes λ as well.\n",
      "\n",
      "−\n",
      "\n",
      "We remark that since rook placements can be considered as permutations, our work in\n",
      "this paper is closely related to an active and exciting ﬁeld of research on large permutations\n",
      "and “permutons” (cf. e.g. [HKM+13, AM14, GGKK15, GHK+17, KKRW20]). For example,\n",
      "our construction of the normalized cumulative X-ray is reminiscent of that of permutons. It\n",
      "would be interesting, in the author’s opinion, to see how tools from the permuton literature\n",
      "can be applied to better understand large rook placements.\n",
      "\n",
      "Outline. In Section 2, we give some deﬁnitions and present some elementary facts about\n",
      "of “nice” piecewise linear functions. It\n",
      "rook placements. Section 3 focuses on the class\n",
      "contains Theorem 3.7, our ﬁrst main result. Section 4 focuses on the “combinatorial” class\n",
      ". We discuss some bijective combinatorics in Subsection 4.1. We establish Theorem 4.10,\n",
      "P\n",
      "our second main result, in Subsection 4.2. We give some properties of the coeﬃcient Df in\n",
      "Subsection 4.3. We prove inequalities on the coeﬃcients Bf and Df in Subsection 4.4. In\n",
      "e\n",
      "Section 5, we discuss probabilities and X-rays. It contains Conjecture 5.9. We deduce this\n",
      "conjecture in the special case of random permutations from Theorem 5.10, our third main\n",
      "result, in Subsection 5.3.\n",
      "\n",
      "P\n",
      "\n",
      "For each positive integer n, let Sn denote the set of permutations of [n] :=\n",
      "{\n",
      "n matrix (“permutation matrix”)\n",
      "\n",
      "We think of a permutation π\n",
      "\n",
      "Sn as an n\n",
      "\n",
      "1, 2, . . . , n\n",
      "\n",
      ".\n",
      "\n",
      "}\n",
      "\n",
      "2. Rook Placements\n",
      "\n",
      "×\n",
      "π11\n",
      "π12\n",
      "π21\n",
      "π22\n",
      "...\n",
      "...\n",
      "πn1 πn2\n",
      "\n",
      "∈\n",
      "\n",
      "π = \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "· · ·\n",
      "· · ·\n",
      ". . .\n",
      "\n",
      "· · ·\n",
      "\n",
      "π1n\n",
      "π2n\n",
      "...\n",
      "πnn\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "where each entry πij is either 0 or 1, each row has exactly one 1, and each column has\n",
      "exactly one 1. A partition is a ﬁnite sequence of weakly decreasing positive integers. A\n",
      "partition λ is said to have (exactly) n parts if the length of λ, as a ﬁnite sequence, is n.\n",
      "3\n",
      "\n",
      "\f",
      "Let us denote by Par the set of all partitions. By convention, we also include the empty\n",
      "partition [ ] in Par. Consider the set\n",
      "\n",
      "In other words,\n",
      "a partition λ\n",
      "\n",
      "{\n",
      "\n",
      "[λ1, λ2, . . . , λn]\n",
      "\n",
      "Bn :=\n",
      "Bn is the set of partitions λ with exactly n parts such that λ1 = n. Given\n",
      "\n",
      "λ2 ≥ · · · ≥\n",
      "\n",
      "n = λ1 ≥\n",
      "\n",
      "λn > 0\n",
      "\n",
      "Par\n",
      "\n",
      "∈\n",
      "\n",
      "}\n",
      "\n",
      "|\n",
      "\n",
      ".\n",
      "\n",
      "∈ Bn, a rook placement of the shape λ is a permutation π\n",
      "[n], if j > λn+1\n",
      "\n",
      "i, then πij = 0.\n",
      "\n",
      "for any i, j\n",
      "\n",
      "∈\n",
      "\n",
      "Sn such that\n",
      "\n",
      "∈\n",
      "\n",
      "−\n",
      "\n",
      "We use the notation RP(λ) to denote the set of rook placements of the shape λ. The\n",
      "cardinality of RP(λ) has a well-known and easy-to-prove formula: for any λ\n",
      "\n",
      "∈ Bn,\n",
      "\n",
      "(1)\n",
      "\n",
      "n\n",
      "\n",
      "# RP(λ) =\n",
      "\n",
      "(λi −\n",
      "\n",
      "(n\n",
      "\n",
      "−\n",
      "\n",
      "i)).\n",
      "\n",
      "i=1\n",
      "Y\n",
      "One particular point to notice about the product formula above that is particularly beautiful,\n",
      "in the author’s opinion, is that the formula holds even when there are no rook placements\n",
      "∈ Bn such that RP(λ) = ∅, the right-hand\n",
      "of the shape λ. In other words, for partitions λ\n",
      "side of the formula becomes zero (not some negative integer).\n",
      "It is well-known that for λ\n",
      "∈ Bn,\n",
      "It\n",
      "i.\n",
      "n + 1\n",
      "[n], we have λi ≥\n",
      "−\n",
      "is the\n",
      "|Dn|\n",
      "\n",
      "RP(λ) is not empty.\n",
      ".\n",
      "}\n",
      "Dn if and only if for all i\n",
      "is the central binomial coeﬃcient\n",
      "\n",
      "the partition λ belongs to\n",
      "is also well-known that\n",
      "|Bn|\n",
      "nth-Catalan number Cn := 1\n",
      "n+1\n",
      "\n",
      "Dn :=\n",
      "\n",
      "We deﬁne\n",
      "\n",
      "and that\n",
      "\n",
      "∈ Bn |\n",
      "\n",
      "2\n",
      "−\n",
      "1\n",
      "−\n",
      "\n",
      "2n\n",
      "n\n",
      "\n",
      "2n\n",
      "n\n",
      "\n",
      "∈\n",
      "\n",
      "λ\n",
      "\n",
      "{\n",
      "\n",
      ".\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "Par(n) and let m be a positive integer. We deﬁne the partition m\n",
      "\n",
      "For each non-negative integer n, let Par(n) denote the set of all partitions λ such that\n",
      "the sum of all parts of λ is n. Now we describe how we dilate partitions. Suppose that\n",
      "Par(m2n) as\n",
      "λ\n",
      "follows. Imagine starting with the Young diagram of λ, and then replacing each of the n\n",
      "boxes of λ with an m\n",
      "m array of boxes. The resulting diagram is the Young diagram of\n",
      "m\n",
      "\n",
      "λ.\n",
      "\n",
      "×\n",
      "\n",
      "⊙\n",
      "\n",
      "∈\n",
      "\n",
      "∈\n",
      "\n",
      "λ\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "λ) is an immediate consequence (and also\n",
      "\n",
      "⊙\n",
      "The following formula for the size of RP(m\n",
      "\n",
      "a mild generalization) of Equation (1).\n",
      "\n",
      "⊙\n",
      "\n",
      "Proposition 2.1. Let m and n be positive integers. For any partition λ = [λ1, λ2, . . . , λn]\n",
      "Bn, we have m\n",
      "\n",
      "∈ Bmn and\n",
      "\n",
      "⊙\n",
      "\n",
      "λ\n",
      "\n",
      "∈\n",
      "\n",
      "# RP(m\n",
      "\n",
      "λ) = m!n\n",
      "\n",
      "⊙\n",
      "\n",
      "n\n",
      "\n",
      "·\n",
      "\n",
      "i=1 (cid:18)\n",
      "Y\n",
      "\n",
      "(n\n",
      "m(λi −\n",
      "m\n",
      "\n",
      "−\n",
      "\n",
      "i))\n",
      "\n",
      ".\n",
      "(cid:19)\n",
      "\n",
      "Here, the binomial coeﬃcient is deﬁned for a\n",
      "\n",
      "Z and b\n",
      "\n",
      "Z\n",
      "\n",
      "1 as\n",
      "\n",
      "a\n",
      "b\n",
      "\n",
      ":= a(a\n",
      "\n",
      "−\n",
      "\n",
      "1)\n",
      "\n",
      "(a\n",
      "\n",
      "b+1)\n",
      "\n",
      "···\n",
      "b!\n",
      "\n",
      "−\n",
      "\n",
      ".\n",
      "\n",
      "It is easy to see that for λ\n",
      "positive integer m, the partition m\n",
      "\n",
      "∈\n",
      "∈ Bn, the partition λ belongs to\n",
      "\n",
      "∈\n",
      "\n",
      "≥\n",
      "\n",
      "λ belongs to\n",
      "\n",
      "Dmn.\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:0)\n",
      "Dn if and only if for any\n",
      "\n",
      "3. The class\n",
      "\n",
      "of piecewise linear functions\n",
      "\n",
      "3.1. The functions and their lofts. Consider the class\n",
      "with the following properties:\n",
      "\n",
      "P\n",
      "\n",
      "of functions f : [0, 1]\n",
      "\n",
      "[0, 1]\n",
      "\n",
      "→\n",
      "\n",
      "f is weakly decreasing,\n",
      "f is piecewise linear with a ﬁnite number of non-diﬀerentiable points,\n",
      "all the non-diﬀerentiable points of f are rational numbers in [0, 1],\n",
      "there exists ε > 0 such that for any 0\n",
      "f (1) > 0, and\n",
      "for any a\n",
      "\n",
      "x < ε, we have f (x) = 1,\n",
      "\n",
      "(0, 1), we have limx\n",
      "\n",
      "a f (x) > 1\n",
      "\n",
      "a.\n",
      "\n",
      "≤\n",
      "\n",
      "∈\n",
      "\n",
      "ց\n",
      "\n",
      "−\n",
      "\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "\n",
      "4\n",
      "\n",
      "⊙\n",
      "\n",
      "P\n",
      "\n",
      "\f",
      "Example 3.1. An example of a function in\n",
      "given by\n",
      "\n",
      "P\n",
      "\n",
      "is the following function f : [0, 1]\n",
      "\n",
      "[0, 1]\n",
      "\n",
      "→\n",
      "\n",
      "f (x) := \n",
      "\n",
      "\n",
      "1\n",
      "1\n",
      "√2\n",
      "1\n",
      "√2 −\n",
      "\n",
      "x\n",
      "√7\n",
      "\n",
      "if x < 1\n",
      "2 ,\n",
      "if x = 1\n",
      "2 ,\n",
      "if x > 1\n",
      "2 .\n",
      "\n",
      "\n",
      "\n",
      "are satisﬁed.\n",
      "It is straightforward to check that all the conditions for functions to be in\n",
      "Note that while we require the non-diﬀerentiable points to be rational numbers in [0, 1], it\n",
      "is ﬁne for the values of the function at the non-diﬀerentiable points to be irrational. In our\n",
      "example here, the value of the function at the non-diﬀerentiable point 1/2 is 1/√2, which\n",
      "is irrational. Moreover, it is also ﬁne for the slope of some piece of the function to be\n",
      "1/√7, which\n",
      "irrational. In our example here, the slope of the function when x\n",
      "is irrational.\n",
      "\n",
      "(1/2, 1] is\n",
      "\n",
      "−\n",
      "\n",
      "P\n",
      "\n",
      "∈\n",
      "\n",
      "Example 3.2. Here we present a non-example. A function that does not belong to\n",
      "function g : [0, 1]\n",
      "\n",
      "[0, 1] given by\n",
      "\n",
      "P\n",
      "\n",
      "is the\n",
      "\n",
      "→\n",
      "\n",
      "g(x) :=\n",
      "\n",
      "1\n",
      "1\n",
      "2\n",
      "\n",
      "(\n",
      "\n",
      "1\n",
      "2 ,\n",
      "if x\n",
      "≤\n",
      "if x > 1\n",
      "2 .\n",
      "\n",
      ".\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "Note that even though g(x) > 1\n",
      "violates the last condition for functions to belong to\n",
      "\n",
      "x for all x\n",
      "\n",
      "−\n",
      "\n",
      "∈\n",
      "\n",
      "(0, 1], the limit limx\n",
      "\n",
      "(1/2) g(x) = 1/2. This\n",
      "\n",
      "ց\n",
      "\n",
      "The following proposition gives some basic properties of functions in\n",
      "\n",
      ". These properties\n",
      "\n",
      "can be proved immediately from the deﬁnition of\n",
      "\n",
      ", so we omit the proof.\n",
      "\n",
      "Proposition 3.3. Let f\n",
      "\n",
      ". Then,\n",
      "\n",
      "(a) for every x\n",
      "(b) for every a\n",
      "(c) for every ε\n",
      "\n",
      "[0, 1], we have 0 < f (x)\n",
      "≤\n",
      "a f (x) > 1\n",
      "(0, 1], we have f (a) > 1\n",
      "−\n",
      "(0, 1], there exists δ > 0 such that for every x\n",
      "\n",
      "1.\n",
      "a and limx\n",
      "\n",
      "ր\n",
      "\n",
      "a.\n",
      "[ε, 1], we have the\n",
      "\n",
      "inequality f (x) + x\n",
      "\n",
      "1 > δ.\n",
      "\n",
      "Each function f\n",
      "\n",
      "comes with a useful quantity we call the loft of f deﬁned as follows.\n",
      "\n",
      "∈ P\n",
      "\n",
      "−\n",
      "\n",
      "∈\n",
      "∈\n",
      "∈\n",
      "\n",
      "∈ P\n",
      "\n",
      "Deﬁnition 3.4. For each function f\n",
      "\n",
      ", deﬁne the loft of f as\n",
      "\n",
      "∈ P\n",
      "\n",
      "loft(f ) := sup\n",
      "\n",
      "ε\n",
      "\n",
      "[0, 1] :\n",
      "\n",
      "[0, ε], f (x) = 1, and\n",
      "\n",
      "(ε, 1], f (x) > 1\n",
      "\n",
      "x + ε\n",
      "\n",
      ".\n",
      "\n",
      "{\n",
      "\n",
      "∈\n",
      "\n",
      "x\n",
      "\n",
      "∀\n",
      "\n",
      "∈\n",
      "\n",
      "x\n",
      "\n",
      "∀\n",
      "\n",
      "∈\n",
      "\n",
      "The following proposition gives some basic properties of the loft of a function in\n",
      "\n",
      "−\n",
      "\n",
      "}\n",
      "\n",
      ".\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "−\n",
      "∈\n",
      "\n",
      "Proposition 3.5. Let f\n",
      "\n",
      ". Then,\n",
      "\n",
      "∈ P\n",
      "\n",
      "(a) its loft is strictly positive: 0 < loft(f )\n",
      "≤\n",
      "x\n",
      "(b) for every real number x such that 0\n",
      "(c) for every real number x such that loft(f )\n",
      "(d) for every x\n",
      "\n",
      "[0, 1], we have\n",
      "\n",
      "≤\n",
      "\n",
      "1.\n",
      "\n",
      "≤\n",
      "≤\n",
      "\n",
      "∈\n",
      "\n",
      "loft(f ), we have f (x) = 1.\n",
      "1, we have f (x) + x\n",
      "x\n",
      "\n",
      "≤\n",
      "\n",
      "1\n",
      "\n",
      "−\n",
      "\n",
      "≥\n",
      "\n",
      "loft(f ).\n",
      "\n",
      "≥\n",
      "Proof. Consider any function f\n",
      "\n",
      "x\n",
      "\n",
      "f (x) + x\n",
      "\n",
      "1\n",
      "\n",
      "−\n",
      "\n",
      ". Let\n",
      "\n",
      "min\n",
      "\n",
      "x, loft(f )\n",
      "}\n",
      "≥\n",
      "denote the set from Deﬁnition 3.4:\n",
      "\n",
      "{\n",
      "\n",
      ".\n",
      "\n",
      ":=\n",
      "\n",
      "ε\n",
      "\n",
      "{\n",
      "\n",
      "∈\n",
      "\n",
      "X\n",
      "\n",
      "[0, 1] :\n",
      "\n",
      "x\n",
      "\n",
      "∀\n",
      "\n",
      "∈\n",
      "\n",
      "x\n",
      "\n",
      "∀\n",
      "\n",
      "∈\n",
      "\n",
      "(ε, 1], f (x) > 1\n",
      "\n",
      "x + ε\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "}\n",
      "\n",
      "∈ P\n",
      "[0, ε], f (x) = 1, and\n",
      "\n",
      "X\n",
      "\n",
      "(a) It suﬃces to show that\n",
      "\n",
      "(0, 1]\n",
      "that f (a) = 1. By Proposition 3.3(c), there exists b > 0 such that for every x\n",
      "a, b\n",
      "have f (x) + x\n",
      "\n",
      ", there exists some a > 0 such\n",
      "[a, 1], we\n",
      "\n",
      "(0, 1]. We claim that c\n",
      "\n",
      "X ∩\n",
      "\n",
      "∈ P\n",
      "\n",
      "∈\n",
      "\n",
      "= ∅. Since f\n",
      "\n",
      "−\n",
      "\n",
      "1 > b. Take c := min\n",
      "{\n",
      "[0, c], we have 1\n",
      "≥\n",
      "a, then f (x) = 1 > 1\n",
      "\n",
      "} ∈\n",
      "f (x)\n",
      "\n",
      "∈\n",
      "(c, 1]. If c < x\n",
      "\n",
      "≥\n",
      "\n",
      "f (c)\n",
      "\n",
      ".\n",
      "f (a) = 1 and so f (x) = 1. Second,\n",
      "c\n",
      "\n",
      "x+c. If x > a, then f (x)+x\n",
      "\n",
      "1 > b\n",
      "\n",
      "∈ X\n",
      "\n",
      "≥\n",
      "\n",
      "First, for any x\n",
      "\n",
      "suppose x\n",
      "and thus f (x) > 1\n",
      "\n",
      "∈\n",
      "\n",
      "≤\n",
      "\n",
      "−\n",
      "\n",
      "x + c. This shows that c\n",
      "5\n",
      "\n",
      "∈ X\n",
      "\n",
      "−\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "≥\n",
      "\n",
      "6\n",
      "\f",
      "(b) It suﬃces to show that f (loft(f )) = 1. If loft(f )\n",
      "\n",
      "then for any positive integer n, there exists an ∈ X\n",
      "loft(f ) > an, we have that\n",
      "\n",
      "∈ X\n",
      "with loft(f )\n",
      "\n",
      ", we are done. If loft(f ) /\n",
      "\n",
      ",\n",
      "∈ X\n",
      "1\n",
      "n < an < loft(f ). Since\n",
      "\n",
      "−\n",
      "\n",
      "f (loft(f )) > 1\n",
      "\n",
      "loft(f ) + an > 1\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "Since n is arbitrary, we have f (loft(f )) = 1.\n",
      "(c) This is similar to part (b). If loft(f )\n",
      "\n",
      ", we have f (x) > 1\n",
      "\n",
      "X\n",
      "f (loft(f )) + loft(f )\n",
      "\n",
      "−\n",
      "1 = loft(f ).\n",
      "\n",
      "x + loft(f ). If loft(f )\n",
      "\n",
      "∈ X\n",
      "\n",
      "∈ X\n",
      "\n",
      "−\n",
      "\n",
      "and x > loft(f ), then by the deﬁnition of\n",
      "1 =\n",
      "\n",
      "and x = loft(f ), then f (x) + x\n",
      "\n",
      "−\n",
      "\n",
      "On the other hand, if loft(f ) /\n",
      "\n",
      "∈ X\n",
      "1\n",
      "n < an < loft(f ). For every real number x\n",
      "\n",
      ", then for any positive integer n, there exists an ∈ X\n",
      "loft(f ), we then have x > an,\n",
      "\n",
      "with loft(f )\n",
      "and thus\n",
      "\n",
      "−\n",
      "\n",
      "Since n is arbitrary, we have f (x)\n",
      "\n",
      "1\n",
      "\n",
      "x + loft(f ).\n",
      "\n",
      "(d) This part follows from parts (b) and (c).\n",
      "\n",
      "−\n",
      "\n",
      "≥\n",
      "\n",
      "−\n",
      "\n",
      "f (x) > 1\n",
      "\n",
      "x + an > 1\n",
      "\n",
      "x + loft(f )\n",
      "\n",
      "−\n",
      "\n",
      "≥\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "Proposition 3.5(d) is an analytically useful property of the loft of a function in\n",
      "\n",
      ". It\n",
      "[0, 1] is far enough from 0, the point (x, f (x)) on the graph of\n",
      "\n",
      "P\n",
      "\n",
      "says roughly that once x\n",
      "∈\n",
      "the function is far enough from the line X + Y = 1.\n",
      "\n",
      "∈ P\n",
      "\n",
      ".\n",
      "3.2. An asymptotic formula for the number of rook placements for functions in\n",
      "P\n",
      "Suppose that a function f\n",
      "is given. Let ρ1, ρ2, . . . , ρm be the non-diﬀerentiable points\n",
      "of f inside the open interval (0, 1), listed in increasing order. (Here m is a non-negative\n",
      "integer. We use the convention that m = 0 if and only if f is diﬀerentiable on (0, 1), which\n",
      "x < 1.) For convenience, we deﬁne ρ0 := 0 and ρm+1 := 1.\n",
      "is when f (x) = 1 for all 0\n",
      "≤\n",
      "1, ρi). Deﬁne\n",
      "Note that for each i\n",
      "[m + 1], the function f is linear on the open interval (ρi\n",
      "R to be the unique linear extension of f\n",
      "the function fi : [ρi\n",
      "1, ρi)\n",
      "1, ρi]. There exist a non-positive real number µi and a real number βi such that\n",
      "to [ρi\n",
      "fi(x) = µix + βi for x\n",
      "\n",
      "|(ρi−1,ρi) from (ρi\n",
      "\n",
      "∈\n",
      "1, ρi]\n",
      "\n",
      "1, ρi].\n",
      "\n",
      "[ρi\n",
      "\n",
      "→\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "Note that since we deﬁne fi on the closed interval [ρi\n",
      "\n",
      "have diﬀerent values at ρi\n",
      "interior of the interval. Note also that f1(x)\n",
      "\n",
      "1, ρi], the functions fi and f might\n",
      "1 and ρi. On the other hand, the two functions agree in the\n",
      "1 (i.e., µ1 = 0 and β1 = 1).\n",
      "\n",
      "Take any positive integer N such that N ρi ∈\n",
      "\n",
      "≡\n",
      "Par to be the partition with exactly N parts whose ith part is given by\n",
      "\n",
      "Z for every i. We deﬁne the partition\n",
      "\n",
      "λ(f, N )\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "∈\n",
      "\n",
      "−\n",
      "\n",
      "(λ(f, N ))i :=\n",
      "\n",
      "N\n",
      "\n",
      "⌈\n",
      "\n",
      "f (i/N )\n",
      "⌉\n",
      "\n",
      ".\n",
      "\n",
      "·\n",
      "\n",
      "Our deﬁnition of\n",
      "words, RP(λ(f, N )) is always non-empty.\n",
      "\n",
      "∈ DN ; in other\n",
      "Our goal of this subsection is to compute an asymptotic formula for # RP(λ(f, N )) of\n",
      "\n",
      "guarantees that, as one may readily verify, λ(f, N )\n",
      "\n",
      "P\n",
      "\n",
      "∈\n",
      "\n",
      "the form\n",
      "\n",
      "log (# RP(λ(f, N ))) = Af ·\n",
      "for positive integers N such that N ρi ∈\n",
      "the implicit constant depends only on the function f .\n",
      "By Equation (1), we can write\n",
      "\n",
      "N log N + Bf ·\n",
      "Z for every i. Here, the notation Of means that\n",
      "\n",
      "log N + Of (1),\n",
      "\n",
      "N + Cf ·\n",
      "\n",
      "(2)\n",
      "\n",
      "log (# RP(λ(f, N ))) =\n",
      "\n",
      "log (N f (n/N ) + n\n",
      "\n",
      "N )\n",
      "\n",
      "+ R(f, N ),\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N\n",
      "X0<n\n",
      "≤\n",
      "\n",
      "where R(f, N ) is the discrepancy from rounding:\n",
      "\n",
      "(3)\n",
      "\n",
      "R(f, N ) :=\n",
      "\n",
      "N\n",
      "X0<n\n",
      "≤\n",
      "Proposition 3.6. We have R(f, N )\n",
      "\n",
      "⌈\n",
      "\n",
      "log\n",
      "\n",
      "N f (n/N )\n",
      "+ n\n",
      "⌉\n",
      "N f (n/N ) + n\n",
      "\n",
      "−\n",
      "−\n",
      "0 and R(f, N ) = Of (1).\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "N\n",
      "N\n",
      "\n",
      "≥\n",
      "\n",
      "6\n",
      "\n",
      "−\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "\f",
      "Proof. The ﬁrst item R(f, N )\n",
      "0 is clear from Equation (3). We proceed to show that\n",
      "R(f, N ) = Of (1). Notice that for 0 < n < ρ1N , we have f (n/N ) = 1. Therefore, we can\n",
      "write\n",
      "\n",
      "≥\n",
      "\n",
      "(4)\n",
      "\n",
      "R(f, N ) = log\n",
      "\n",
      "⌈\n",
      "\n",
      "N f (ρ1)\n",
      "+ ρ1N\n",
      "⌉\n",
      "N f (ρ1) + ρ1N\n",
      "\n",
      "N\n",
      "N\n",
      "\n",
      "−\n",
      "−\n",
      "\n",
      "+\n",
      "\n",
      "log\n",
      "\n",
      "⌈\n",
      "\n",
      "N f (n/N )\n",
      "+ n\n",
      "⌉\n",
      "N f (n/N ) + n\n",
      "\n",
      "N\n",
      "N\n",
      "\n",
      "−\n",
      "−\n",
      "\n",
      ".\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "(0, 1], by Proposition 3.3(c), there exists a > 0 such that for every x\n",
      "log(x) < 1/x,\n",
      "\n",
      "1 > a. Using this with the inequality log(\n",
      "⌈\n",
      "\n",
      "Xρ1N <n\n",
      "≤\n",
      "\n",
      "x\n",
      "⌉\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "N\n",
      "\n",
      ")\n",
      "\n",
      "∈\n",
      "∀\n",
      "\n",
      "(cid:19)\n",
      "[ρ1, 1] we\n",
      "x > 0, we\n",
      "\n",
      "Since ρ1 ∈\n",
      "have f (x) + x\n",
      "obtain\n",
      "\n",
      "1\n",
      "(f (n/N ) + (n/N )\n",
      "\n",
      "N\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "R(f, N ) <\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "1\n",
      "N (f (ρ1) + ρ1 −\n",
      "1\n",
      "N (f (ρ1) + ρ1 −\n",
      "1\n",
      "f (ρ1) + ρ1 −\n",
      "\n",
      "1)\n",
      "\n",
      "1)\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      "≤\n",
      "\n",
      "=\n",
      "\n",
      "Xρ1N <n\n",
      "≤\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "Xρ1N <n\n",
      "≤\n",
      "ρ1\n",
      "1\n",
      "−\n",
      "a\n",
      "\n",
      ".\n",
      "\n",
      "+\n",
      "\n",
      "·\n",
      "1\n",
      "\n",
      "·\n",
      "\n",
      "N\n",
      "\n",
      "a\n",
      "\n",
      "1 ·\n",
      "Since ρ1 and a depend only on f (and not N ), the quantity above is Of (1).\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "Recall that the function f is linear on each open interval (ρi\n",
      "\n",
      "there might be a “jump.” For instance, in Example 3.1 above, the three values limx\n",
      "f (ρ1), and limx\n",
      "jumps do not have a huge eﬀect on the summation in Equation (2):\n",
      "\n",
      "1, ρi), while, at each ρi,\n",
      "ρ1 f (x),\n",
      "ρ1 f (x) are all diﬀerent. It is not hard to see, however, that these possible\n",
      "\n",
      "ր\n",
      "\n",
      "ց\n",
      "\n",
      "−\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "(9)\n",
      "\n",
      "(10)\n",
      "\n",
      "(11)\n",
      "\n",
      "(12)\n",
      "\n",
      "log (N f (n/N ) + n\n",
      "\n",
      "N )\n",
      "\n",
      "−\n",
      "\n",
      "N\n",
      "\n",
      "X0<n\n",
      "≤\n",
      "m+1\n",
      "\n",
      "=\n",
      "\n",
      "\n",
      "Xρi−1N <n\n",
      "≤\n",
      "\n",
      "\n",
      "Combining this with Equation (2) and Proposition 3.6, we obtain\n",
      "\n",
      "i=1\n",
      "X\n",
      "\n",
      "ρiN\n",
      "\n",
      "\n",
      "\n",
      "log((µi + 1)n + (βi −\n",
      "\n",
      "1)N )\n",
      "\n",
      "+ Of (1).\n",
      "\n",
      "m+1\n",
      "\n",
      "(7)\n",
      "\n",
      "log (# RP (λ(f, N ))) =\n",
      "\n",
      "\n",
      "Xρi−1N <n\n",
      "≤\n",
      "\n",
      "We break the outer summation on the right-hand side above into when i = 1 and when\n",
      "i\n",
      "\n",
      "i=1\n",
      "X\n",
      "\n",
      "ρiN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "log((µi + 1)n + (βi −\n",
      "\n",
      "1)N )\n",
      "\n",
      "+ Of (1).\n",
      "\n",
      "≥\n",
      "(8)\n",
      "\n",
      "2. When i = 1, we have, by Stirling’s formula,\n",
      "log((µi + 1)n + (βi −\n",
      "\n",
      "ρiN\n",
      "\n",
      "Xρi−1N <n\n",
      "≤\n",
      "N log N + (ρ1 log ρ1 −\n",
      "= ρ1 ·\n",
      "·\n",
      "m + 1, observe that the function\n",
      "\n",
      "ρ1)\n",
      "\n",
      "1)N ) = log((ρ1N )!)\n",
      "\n",
      "N +\n",
      "\n",
      "log N + Of (1).\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "When 2\n",
      "\n",
      "i\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "is well-deﬁned on the whole closed interval [ρi\n",
      "mation formula (cf. [MV07, Appendix B]) with this function, we write\n",
      "\n",
      "1N, ρiN ]. Using the Euler-Maclaurin sum-\n",
      "\n",
      "−\n",
      "\n",
      "x\n",
      "\n",
      "log((µi + 1)x + (βi −\n",
      "\n",
      "7→\n",
      "\n",
      "1)N )\n",
      "\n",
      "log((µi + 1)n+ (βi −\n",
      "\n",
      "1)N ) =\n",
      "\n",
      "log((µi + 1)x+ (βi −\n",
      "\n",
      "1)N )dx+ Of (1).\n",
      "\n",
      "ρiN\n",
      "\n",
      "ρi−1N\n",
      "\n",
      "Z\n",
      "\n",
      "Xρi−1N <n\n",
      "≤\n",
      "\n",
      "ρiN\n",
      "\n",
      "By the change of variables x\n",
      "\n",
      "N\n",
      "\n",
      "x′, we have\n",
      "\n",
      "·\n",
      "\n",
      "7→\n",
      "log((µi + 1)x + (βi −\n",
      "\n",
      "1)N )dx\n",
      "\n",
      "ρiN\n",
      "\n",
      "ρi−1N\n",
      "\n",
      "Z\n",
      "\n",
      "= (ρi −\n",
      "\n",
      "ρi\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "·\n",
      "\n",
      "N log N +\n",
      "\n",
      "log(f (x) + x\n",
      "\n",
      "1) dx\n",
      "\n",
      "N.\n",
      "\n",
      "−\n",
      "\n",
      ") ·\n",
      "\n",
      "ρi\n",
      "\n",
      "(Z\n",
      "\n",
      "ρi−1\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "The following is the main theorem of this section.\n",
      "\n",
      "Theorem 3.7. Let f\n",
      "\n",
      ". Let ρ0, ρ1, . . . , ρm+1 be as deﬁned above. We have\n",
      "\n",
      "∈ P\n",
      "\n",
      "log (# RP(λ(f, N ))) = N log N + Bf ·\n",
      "\n",
      "log N + Of (1),\n",
      "\n",
      "1\n",
      "2\n",
      "Z for every i, where\n",
      "\n",
      "N +\n",
      "\n",
      "for positive integers N such that N ρi ∈\n",
      "\n",
      "Note that the integral is improper at x = 0. We interpret it as\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "Z\n",
      "\n",
      "1\n",
      "\n",
      "Bf =\n",
      "\n",
      "log(f (x) + x\n",
      "\n",
      "1) dx.\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "log(f (x) + x\n",
      "\n",
      "1) dx.\n",
      "\n",
      "lim\n",
      "0\n",
      "ε\n",
      "ց\n",
      "\n",
      "ε\n",
      "Z\n",
      "\n",
      "Proof of Theorem 3.7. This result is immediate from combining Equations (8)-(12) and ob-\n",
      "(cid:3)\n",
      "1) dx = limε\n",
      "serving that limε\n",
      "\n",
      "log(f (x) + x\n",
      "\n",
      "ρ1.\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "ρ1\n",
      "ε\n",
      "\n",
      "ց\n",
      "\n",
      "log(x) dx = ρ1 log ρ1 −\n",
      "\n",
      "ρ1\n",
      "ε\n",
      "\n",
      "ց\n",
      "\n",
      "−\n",
      "\n",
      "R\n",
      "\n",
      "3.3. Properties of Bf . Let p\n",
      "norm of functions in\n",
      "by (\n",
      "functions f, g\n",
      "functions.\n",
      "\n",
      "∈ P\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "k\n",
      "\n",
      "f\n",
      "\n",
      "−\n",
      "\n",
      "≥\n",
      ", we make the class\n",
      "\n",
      ", Lp). A technical remark is that in the construction of (\n",
      "g\n",
      "\n",
      "for which\n",
      "\n",
      "R\n",
      "\n",
      "1 be any positive real number. By considering the Lp-\n",
      "a metric space. Let us denote this metric space\n",
      ", Lp), we identify any two\n",
      ", Lp) is an equivalence class of\n",
      "\n",
      "kp = 0. An element in (\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "Nevertheless, it is easy to see that the map f\n",
      ", Lp)\n",
      "Let (R, Euclid) denote the set of real numbers equipped with the usual Euclidean metric.\n",
      "We have the following topological property of B, when considered as a function from (\n",
      ", Lp)\n",
      "to (R, Euclid).\n",
      "Proposition 3.8. The map B : (\n",
      "\n",
      "Bf induces a well-deﬁned map\n",
      "R.\n",
      "\n",
      "(R, Euclid) is discontinuous everywhere on\n",
      "\n",
      "B : (\n",
      "\n",
      ", Lp)\n",
      "\n",
      "7→\n",
      "\n",
      "→\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      ".\n",
      "\n",
      "P\n",
      "\n",
      "→\n",
      "\n",
      "be an arbitrary function. For each positive integer n such that n−\n",
      "\n",
      "P\n",
      "1 +\n",
      "\n",
      "Proof. Let f\n",
      "2−\n",
      "\n",
      "n2\n",
      "\n",
      "< loft(f ), deﬁne\n",
      "\n",
      "∈ P\n",
      "\n",
      "Observe that gn converges to f in Lp, as n\n",
      "have\n",
      "\n",
      "→ ∞\n",
      "\n",
      ". However, by the triangle inequality, we\n",
      "\n",
      "gn(x) :=\n",
      "\n",
      "f (x)\n",
      "1\n",
      "\n",
      "−\n",
      "\n",
      "(\n",
      "\n",
      "x + 2−\n",
      "\n",
      "n2\n",
      "\n",
      "if x\n",
      "1\n",
      "≤\n",
      "if x > 1\n",
      "\n",
      "1\n",
      "n ,\n",
      "1\n",
      "n .\n",
      "\n",
      "−\n",
      "−\n",
      "\n",
      "Bgn |\n",
      "\n",
      "|\n",
      "\n",
      "=\n",
      "\n",
      "log(gn(x) + x\n",
      "\n",
      "I1| − |\n",
      "\n",
      "I2| − |\n",
      "\n",
      "I3|\n",
      "\n",
      ",\n",
      "\n",
      "≥ |\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "Z\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "I1 :=\n",
      "\n",
      "−\n",
      "\n",
      "1) dx\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "log(gn(x) + x\n",
      "\n",
      "1) dx,\n",
      "\n",
      "−\n",
      "\n",
      "I2 :=\n",
      "\n",
      "log(f (x) + x\n",
      "\n",
      "1) dx,\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "Z\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "n\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "Z\n",
      "\n",
      "1\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "where\n",
      "\n",
      "and\n",
      "\n",
      "I3 :=\n",
      "\n",
      "log(f (x) + x\n",
      "\n",
      "1) dx.\n",
      "\n",
      "1\n",
      "Z\n",
      "Note that\n",
      "Bf |\n",
      "= (log 2)\n",
      "of functions converging to f in Lp, but\n",
      "\n",
      "I2|\n",
      "\n",
      "I1|\n",
      "\n",
      "n,\n",
      "\n",
      "=\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "·\n",
      "\n",
      "|\n",
      "\n",
      "1\n",
      "n\n",
      "−\n",
      ", and\n",
      "\n",
      "I3| →\n",
      "|\n",
      "Bgn | → ∞\n",
      "\n",
      "|\n",
      "\n",
      "0, as n\n",
      ", as n\n",
      "\n",
      "→ ∞\n",
      ".\n",
      "→ ∞\n",
      "\n",
      ". Hence,\n",
      "\n",
      "gn}\n",
      "\n",
      "{\n",
      "\n",
      "is a sequence\n",
      "(cid:3)\n",
      "\n",
      "The following proposition is clear from the integral formula of Bf .\n",
      ", we have Bf ≤ −\n",
      "\n",
      "(a) For every function f\n",
      "\n",
      "is tight. The equality is attained if and only if f (x) = 1 for every x\n",
      "\n",
      "Proposition 3.9.\n",
      "\n",
      "∈ P\n",
      "\n",
      "(b) For any functions f, g\n",
      "\n",
      "such that\n",
      "\n",
      "[0, 1], f (x)\n",
      "\n",
      "1. The upper bound\n",
      "[0, 1).\n",
      "g(x), we have Bf ≥\n",
      "\n",
      "Bg.\n",
      "\n",
      "∈\n",
      "\n",
      "≥\n",
      "\n",
      "∈ P\n",
      "\n",
      "x\n",
      "\n",
      "∀\n",
      "\n",
      "∈\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "4. The class\n",
      "\n",
      "P\n",
      "\n",
      "of piecewise linear functions\n",
      "Pk to be the class of functions f\n",
      "e\n",
      "f (i/k)\n",
      "\n",
      "Z,\n",
      "\n",
      "For each positive integer k, deﬁne\n",
      "\n",
      "e\n",
      "the following additional properties:\n",
      "[k], we have k\n",
      "\n",
      "•\n",
      "•\n",
      "•\n",
      "\n",
      "∈\n",
      "\n",
      "for each i\n",
      "f is upper-semicontinuous, and\n",
      "for each i\n",
      "slope.\n",
      "\n",
      "[k], the restriction f\n",
      "\n",
      "∈\n",
      "\n",
      "·\n",
      "\n",
      "∈\n",
      "\n",
      "|((i\n",
      "−\n",
      "\n",
      "1)/k,i/k) is linear with a non-positive integer\n",
      "\n",
      "which satisfy\n",
      "\n",
      "∈ P\n",
      "\n",
      "a\n",
      "\n",
      "Pk. One important property about function f\n",
      "We let\n",
      ":=\n",
      "(0, 1], we have limx\n",
      "ր\n",
      "e\n",
      "\n",
      "a f (x) = f (a).\n",
      "\n",
      "∞k=1\n",
      "\n",
      "P\n",
      "\n",
      "∈\n",
      "\n",
      "P\n",
      "\n",
      "e\n",
      "\n",
      "∈\n",
      "\n",
      "e\n",
      "\n",
      "S\n",
      "4.1. Bijective Combinatorics in\n",
      "size has a nice product formula.\n",
      "\n",
      "Pk. It is easy to see that\n",
      "e\n",
      "\n",
      "Pk is a ﬁnite set. In fact, its\n",
      "e\n",
      "\n",
      "2, the sizes of the following sets are equal:\n",
      "\n",
      "Proposition 4.1. For each positive integer k\n",
      "\n",
      "is that for every\n",
      "\n",
      "3)-tuples (y1, y2, . . . , y2k\n",
      "\n",
      "3) of non-negative integers such that for\n",
      "\n",
      "•\n",
      "•\n",
      "\n",
      "the class\n",
      "the set\n",
      "each m\n",
      "\n",
      "Pk,\n",
      "Tk of (2k\n",
      "[2k\n",
      "e\n",
      "−\n",
      "∈\n",
      "\n",
      "−\n",
      "3], we have\n",
      "\n",
      "≥\n",
      "\n",
      "−\n",
      "\n",
      "•\n",
      "\n",
      "the set\n",
      "half-plane\n",
      "\n",
      "y1 + y2 +\n",
      "\n",
      "+ ym ≤\n",
      "Gk of lattice paths from (0, 0) to (2k\n",
      ".\n",
      "2y\n",
      "}\n",
      "\n",
      "(x, y) : x\n",
      "\n",
      "· · ·\n",
      "\n",
      "≥\n",
      "\n",
      "{\n",
      "\n",
      "l\n",
      "−\n",
      "\n",
      ",\n",
      "\n",
      "m\n",
      "2\n",
      "m\n",
      "1, k\n",
      "\n",
      "−\n",
      "\n",
      "Proof. We construct the following bijections.\n",
      "\n",
      "that 1\n",
      "\n",
      "First,\n",
      "\n",
      "Pk → Tk. Given f\n",
      "1, let\n",
      "h\n",
      "≤\n",
      "e\n",
      "\n",
      "−\n",
      "\n",
      "k\n",
      "\n",
      "≤\n",
      "\n",
      "and for each ℓ such that 1\n",
      "\n",
      "∈\n",
      "\n",
      "Pk, we deﬁne (y1, y2, . . . , y2k\n",
      "e\n",
      "1 := k\n",
      "\n",
      "f (x)\n",
      "\n",
      "f\n",
      "\n",
      "−\n",
      "\n",
      "h + 1\n",
      "k\n",
      "\n",
      ",\n",
      "\n",
      "(cid:19)(cid:19)\n",
      "\n",
      "−\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "y2h\n",
      "\n",
      "−\n",
      "\n",
      "ℓ\n",
      "\n",
      "k\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "−\n",
      "\n",
      "·\n",
      "\n",
      "h/k\n",
      "\n",
      "lim\n",
      "x\n",
      "(cid:18)\n",
      "ց\n",
      "2, let\n",
      "\n",
      "y2ℓ := k\n",
      "\n",
      "ℓ + 1\n",
      "k\n",
      "\n",
      "f\n",
      "(cid:18)\n",
      "\n",
      "Second,\n",
      "\n",
      "−\n",
      "Tk → Gk. Send the tuple (y1, . . . , y2k\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "·\n",
      "\n",
      "−\n",
      "\n",
      "x\n",
      "\n",
      "ց\n",
      "3)\n",
      "\n",
      "E2N y1EN y2EN y3\n",
      "\n",
      "EN y2k−3EN k\n",
      "\n",
      "lim\n",
      "(ℓ+1)/k\n",
      "\n",
      "f (x)\n",
      "\n",
      ".\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "∈ Tk to the path\n",
      "y2−···−\n",
      "y1−\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "\n",
      "y2k−3,\n",
      "\n",
      "· · ·\n",
      "\n",
      "where E denotes the step (1, 0) and N denotes the step (0, 1).\n",
      "\n",
      "Corollary 4.2. For every positive integer k, we have\n",
      "\n",
      "|\n",
      "Note that this sequence appears as A006013 on the OEIS [OEI].\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "e\n",
      "\n",
      "=\n",
      "\n",
      "Pk|\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "3k\n",
      "k\n",
      "\n",
      "2\n",
      "−\n",
      "1\n",
      "−\n",
      "\n",
      ".\n",
      "\n",
      "1) which are contained in the\n",
      "\n",
      "3) as follows. For each h such\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "Proof of Corollary 4.2. With Proposition 4.1, it suﬃces to show that\n",
      "is well-known: see, for example, the sequence\n",
      "Section 3], or references in A006013 on the OEIS [OEI].\n",
      "\n",
      ". This\n",
      "|Gk|\n",
      "in the work of Gessel and Xin [GX06,\n",
      "(cid:3)\n",
      "\n",
      "bn}\n",
      "\n",
      "{\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "= 1\n",
      "k\n",
      "\n",
      "3k\n",
      "k\n",
      "\n",
      "2\n",
      "−\n",
      "1\n",
      "−\n",
      "\n",
      "In the following discussion, by a lattice path, we mean the image of an injective continuous\n",
      "R2 (under the usual Euclidean topology for both spaces) that is also a ﬁnite\n",
      "\n",
      "function [0, 1]\n",
      "union of segments s1, . . . , sn such that both end points of each si are lattice points.\n",
      "\n",
      "→\n",
      "\n",
      "Functions in\n",
      "\n",
      "vertical segments. Formally, suppose a function f\n",
      "of f into the square [0, k]\n",
      "\n",
      "Pk can be seen as lattice paths, by dilating their graphs and then adding\n",
      "Pk is given. We ﬁrst dilate the graph\n",
      "e\n",
      "e\n",
      "\n",
      "[0, k] by\n",
      "\n",
      "×\n",
      "\n",
      "∈\n",
      "\n",
      "Γf :=\n",
      "\n",
      "(x, y)\n",
      "\n",
      "R\n",
      "\n",
      "R :\n",
      "\n",
      "∈\n",
      "\n",
      "×\n",
      "\n",
      "n\n",
      "\n",
      "= f\n",
      "\n",
      "y\n",
      "k\n",
      "\n",
      "9\n",
      "\n",
      "x\n",
      "k\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)o\n",
      "\n",
      "[0, k]\n",
      "\n",
      "[0, k].\n",
      "\n",
      "⊆\n",
      "\n",
      "×\n",
      "\n",
      "\f",
      "Then, we take the closure Γf of Γf with respect to the usual Euclidean topology on R2.\n",
      "Note that the closure simply adds a ﬁnite number of points into the set Γf . Then, our path\n",
      "Path(f ) is given by\n",
      "\n",
      "R2\n",
      "\n",
      "|\n",
      "\n",
      "y\n",
      "\n",
      "∈\n",
      "\n",
      "≤\n",
      "\n",
      "Path(f ) :=\n",
      "\n",
      "For any f\n",
      "deﬁne\n",
      "\n",
      "there exist y1, y2 such that y1 ≤\n",
      "\n",
      ".\n",
      "(x, y)\n",
      "Pk, the path Path(f ) is a lattice path with endpoints (0, k) and (k, 0). Let us\n",
      "(cid:9)\n",
      "(cid:8)\n",
      "∈\n",
      "Pk).\n",
      "Lk := Path(\n",
      "e\n",
      "The map Path :\n",
      "e\n",
      "consider the map Func :\n",
      "\n",
      "Pk → Lk is a bijection. To go back from lattice paths to functions,\n",
      "e\n",
      "\n",
      "y2 and (x, y1), (x, y2)\n",
      "\n",
      "Pk given by\n",
      "Lk →\n",
      "(Func(γ)) (x) := sup\n",
      "∈\n",
      "e\n",
      "∈ Lk. This map simply shrinks the path back and then removes\n",
      "Lk as going from (0, k) to (k, 0), then they are exactly the lattice\n",
      "\n",
      "y : (kx, ky)\n",
      "{\n",
      "\n",
      "for any x\n",
      "vertical segments. It is straightforward to see that Path and Func are inverses.\n",
      "\n",
      "If we think of paths in\n",
      "\n",
      "[0, 1], for any γ\n",
      "\n",
      "Γf\n",
      "\n",
      "∈\n",
      "\n",
      "∈\n",
      "\n",
      "γ\n",
      "\n",
      "}\n",
      "\n",
      ",\n",
      "\n",
      "paths γ with the following properties:\n",
      "\n",
      "the path γ starts at (0, k) and ends at (k, 0),\n",
      "each step is either (1,\n",
      "0 or (0,\n",
      "the path γ intersects with the diagonal X + Y = k exactly at its two endpoints.\n",
      "\n",
      "ℓ) for ℓ\n",
      "\n",
      "1),\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "Z\n",
      "\n",
      "∈\n",
      "\n",
      "≥\n",
      "\n",
      "This class\n",
      "\n",
      "Lk of lattice paths contains many familiar paths in algebraic combinatorics\n",
      "such as Dyck paths and Motzkin paths. It is also closely related to plane S-trees, paren-\n",
      "thesizations, and dissections of a convex polygon. See Stanley’s text [Sta99, Chapter 6] for\n",
      "details.\n",
      "\n",
      "For the following discussion, a Dyck path from (0, k) to (k, 0) is a lattice path starting\n",
      "1), and ending at (k, 0) that never crosses (but might\n",
      "from (0, k), using steps (1, 0) and (0,\n",
      "touch) the line X + Y = k. We let Dyck(k) denote the set of Dyck paths from (0, k) to\n",
      "(k, 0).\n",
      "\n",
      "−\n",
      "\n",
      "Since paths in\n",
      "\n",
      "(0, k) and (k, 0), the Dyck paths in\n",
      "(k, 1). Note that Dyck paths in\n",
      "functions in\n",
      ".\n",
      "into\n",
      "\n",
      "Lk can intersect with the line X + Y = k only at the two endpoints\n",
      "Lk are in bijection with the Dyck paths from (1, k) to\n",
      "Pk) to piecewise constant\n",
      "Lk →\n",
      "Pk. We have thus obtained one trivial embedding of a Catalan-numerous family\n",
      "e\n",
      "e\n",
      "\n",
      "Lk correspond (under Func :\n",
      "\n",
      "Proposition 4.3. For each positive integer k, the number of piecewise constant functions\n",
      "in\n",
      "\n",
      "P\n",
      "\n",
      "e\n",
      "\n",
      "Pk is exactly the (k\n",
      "e\n",
      "\n",
      "−\n",
      "\n",
      "1)st Catalan number\n",
      "1\n",
      "k\n",
      "\n",
      "1 =\n",
      "\n",
      "Ck\n",
      "\n",
      "−\n",
      "\n",
      "2k\n",
      "k\n",
      "\n",
      "2\n",
      "−\n",
      "1\n",
      "−\n",
      "\n",
      ".\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "Similarly, we have a Motzkin-numerous class of functions in\n",
      "\n",
      "as follows.\n",
      "\n",
      "P\n",
      "\n",
      "Proposition 4.4. For each positive integer k\n",
      "class of all functions f\n",
      "\n",
      "2, deﬁne the subset\n",
      "e\n",
      "≥\n",
      "Pk which satisfy the following conditions:\n",
      "e\n",
      "\n",
      "1,\n",
      "(0, 1) of f , we have\n",
      "\n",
      "each linear piece of f either is constant or has slope\n",
      "for each non-diﬀerentiable point a\n",
      "\n",
      "−\n",
      "\n",
      "∈\n",
      "\n",
      "MOk ⊆\n",
      "\n",
      "Pk to be the\n",
      "e\n",
      "\n",
      "∈\n",
      "f (x)\n",
      "\n",
      "k\n",
      "\n",
      "f (a)\n",
      "\n",
      "k\n",
      "\n",
      "·\n",
      "\n",
      "≡\n",
      "\n",
      "≡\n",
      "·\n",
      "f (1) is an even integer.\n",
      "\n",
      "lim\n",
      "a\n",
      "x\n",
      "ց\n",
      "\n",
      "k\n",
      "\n",
      "(1\n",
      "\n",
      "a)\n",
      "\n",
      "(mod 2),\n",
      "\n",
      "·\n",
      "\n",
      "−\n",
      "\n",
      "the number k\n",
      "\n",
      "·\n",
      "\n",
      "Then, the size of\n",
      "Motzkin numbers, we recommend Stanley’s text [Sta99, Exercises 6.37 and 6.38].)\n",
      "\n",
      "MOk is the (k\n",
      "\n",
      "2. (For more details about the\n",
      "\n",
      "2)nd Motzkin number Mk\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "If we drop the last two conditions about parity, we obtain Schr¨oder numbers.\n",
      "\n",
      "Proposition 4.5. For each positive integer k, deﬁne the subset\n",
      "of all functions f\n",
      "Then, the size of\n",
      "Schr¨oder numbers, we recommend Stanley’s text [Sta99, Section 6.2 and Exercises 6.39].)\n",
      "10\n",
      "\n",
      "Pk such that each linear piece of f either is constant or has slope\n",
      "e\n",
      "\n",
      "Pk to be the class\n",
      "1.\n",
      "−\n",
      "e\n",
      "1. (For more details about the\n",
      "\n",
      "1)st Schr¨oder number rk\n",
      "\n",
      "∈\n",
      "SCk is the (k\n",
      "\n",
      "SCk ⊆\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "•\n",
      "•\n",
      "•\n",
      "\n",
      "•\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "\f",
      "There is another embedding of a Catalan-numerous family in\n",
      "\n",
      ". The following proposi-\n",
      "\n",
      "tion is observed and proved by Alex Postnikov.\n",
      "\n",
      "Proposition 4.6. Let k be a positive integer. The number of continuous functions in\n",
      "is exactly the kth Catalan number\n",
      "\n",
      "P\n",
      "\n",
      "e\n",
      "\n",
      "Pk\n",
      "\n",
      "e\n",
      "\n",
      "Ck =\n",
      "\n",
      "1\n",
      "k + 1\n",
      "\n",
      "2k\n",
      "k\n",
      "\n",
      ".\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "Proof. We construct an explicit bijection from the set of continuous functions in\n",
      "Pk to\n",
      "Dyck(k), the set of Dyck paths from (0, k) to (k, 0). Note that for each continuous function\n",
      "e\n",
      "f\n",
      "[0, k], is a continuous lattice path from (0, k)\n",
      "Pk, the graph of f , after dilating to [0, k]\n",
      "k\n",
      "i=1 γi,\n",
      "to (k, 0) without a vertical step. We can write this path as the union of k segments\n",
      "e\n",
      "where\n",
      "\n",
      "×\n",
      "\n",
      "∈\n",
      "\n",
      "γi :=\n",
      "\n",
      "i\n",
      "(cid:20)(cid:18)\n",
      "\n",
      "−\n",
      "\n",
      "1, k\n",
      "\n",
      "f\n",
      "\n",
      "·\n",
      "\n",
      "i\n",
      "\n",
      "1\n",
      "\n",
      "−\n",
      "k\n",
      "\n",
      ",\n",
      "\n",
      "i, k\n",
      "\n",
      "f\n",
      "\n",
      "·\n",
      "\n",
      "i\n",
      "k\n",
      "\n",
      "(cid:18)\n",
      "Replace each segment γi with an L-shaped broken segment with the same endpoints:\n",
      "i\n",
      "k\n",
      "\n",
      "(cid:19)(cid:19)(cid:21)\n",
      "\n",
      "1, k\n",
      "\n",
      "1, k\n",
      "\n",
      "(cid:19)(cid:19)\n",
      "\n",
      "i, k\n",
      "\n",
      "−\n",
      "k\n",
      "\n",
      "i\n",
      "k\n",
      "\n",
      "i\n",
      "k\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "1\n",
      "\n",
      "f\n",
      "\n",
      "f\n",
      "\n",
      "f\n",
      "\n",
      "f\n",
      "\n",
      "i\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "∪\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "(cid:19)(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "k\n",
      "i=1 γ′i is the desired Dyck path in Dyck(k).\n",
      "\n",
      "(cid:19)(cid:19)(cid:21)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "1, k\n",
      "\n",
      "γ′i :=\n",
      "\n",
      "i\n",
      "(cid:20)(cid:18)\n",
      "Then, the union\n",
      "\n",
      "−\n",
      "\n",
      "i\n",
      "(cid:20)(cid:18)\n",
      "\n",
      "i\n",
      "(cid:18)\n",
      "\n",
      ".\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)(cid:19)(cid:21)\n",
      "(cid:3)\n",
      "\n",
      "S\n",
      "\n",
      "Remark 4.7. From Proposition 4.6, we quickly obtain yet another Catalan-numerous fam-\n",
      "1)st\n",
      "ily. The number of continuous functions f\n",
      "Catalan number Ck\n",
      "1. This is because under the bijection in the proof of Proposition 4.6,\n",
      "these functions become Dyck paths in Dyck(k) that visit (k\n",
      "\n",
      "Pk for which f (1) = 1/k is the (k\n",
      "e\n",
      "\n",
      "1, 1).\n",
      "\n",
      "−\n",
      "\n",
      "∈\n",
      "\n",
      "−\n",
      "\n",
      "S\n",
      "\n",
      "−\n",
      "\n",
      "The bijection in the proof of Proposition 4.6 that sends continuous functions to Dyck\n",
      "paths might be extended to the whole\n",
      "Pk. The image of the extended map can be understood\n",
      "as Dyck paths with certain marks on vertical segments. These are combinatorial objects\n",
      "which we call waterfalls.\n",
      "e\n",
      "\n",
      "Deﬁnition 4.8. A waterfall of size k is a Dyck path γ\n",
      "Dyck(k) together with a choice of\n",
      "coloring of every unit segment in γ so that each segment is colored one of either green or\n",
      "blue with the following rules:\n",
      "\n",
      "∈\n",
      "\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "\n",
      "every horizontal segment is colored blue,\n",
      "every vertical segment on the line x = k is colored green,\n",
      "every vertical segment with an endpoint on the line x + y = k is colored green, and\n",
      "if s1 and s2 are vertical segments such that s1 is immediately above s2 and s2 is\n",
      "colored blue, then s1 must also be colored blue.\n",
      "\n",
      "Let WT(k) denote the set of waterfalls of size k. From our discussion above, we have\n",
      "\n",
      "We obtain the following curious combinatorial formula.\n",
      "\n",
      "WT(k)\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "3k\n",
      "k\n",
      "\n",
      "2\n",
      "−\n",
      "1\n",
      "−\n",
      "\n",
      ".\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "Proposition 4.9. Let k be a positive integer. For each Dyck path D\n",
      "deﬁne the weight of D to be\n",
      "\n",
      "∈\n",
      "\n",
      "Dyck(k), let us\n",
      "\n",
      "Then,\n",
      "\n",
      "wt(D) :=\n",
      "\n",
      "#\n",
      "\n",
      "j\n",
      "\n",
      "{\n",
      "\n",
      "∈\n",
      "\n",
      "Z\n",
      "\n",
      "|\n",
      "\n",
      "i + j > k and (i, j)\n",
      "\n",
      "D\n",
      "\n",
      ".\n",
      "\n",
      "}\n",
      "\n",
      "∈\n",
      "\n",
      "k\n",
      "\n",
      "1\n",
      "\n",
      "−\n",
      "\n",
      "i=1\n",
      "Y\n",
      "\n",
      "wt(D) =\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "3k\n",
      "k\n",
      "\n",
      "2\n",
      "−\n",
      "1\n",
      "−\n",
      "\n",
      ".\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "Dyck(k)\n",
      "\n",
      "XD\n",
      "∈\n",
      "\n",
      "Proof. Note that the weight wt(D) is the number of waterfalls whose underlying Dyck paths\n",
      "Dyck(k) wt(D) counts the total number of waterfalls in WT(k). (cid:3)\n",
      "are D. Therefore,\n",
      "\n",
      "D\n",
      "\n",
      "∈\n",
      "\n",
      "P\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "(14)\n",
      "\n",
      "(15)\n",
      "\n",
      "(16)\n",
      "\n",
      "(17)\n",
      "\n",
      "4.2. A precise asymptotic formula for the number of rook placements for func-\n",
      "tions in\n",
      "Pk. The goal of this subsection is to compute a precise asymptotic formula for\n",
      "# RP(λ(f, N )), for each f\n",
      "e\n",
      "\n",
      "log (# RP(λ(f, N ))) = Af ·\n",
      "{\n",
      "\n",
      "for positive integers N\n",
      "Af , Bf , and Cf are the same as before.\n",
      "\n",
      "N log N + Bf ·\n",
      "k, 2k, 3k, . . .\n",
      "}\n",
      "\n",
      "∈\n",
      "\n",
      "∈\n",
      "\n",
      "Pk, of the form\n",
      "e\n",
      "kZ :=\n",
      "\n",
      "N + Cf ·\n",
      "\n",
      ". Since\n",
      "\n",
      "log N + Df + Of (1/N ),\n",
      "\n",
      ", the quantities\n",
      "\n",
      "In this subsection, we redeﬁne our notations µi and βi. These notations now have slightly\n",
      "[k],\n",
      "Pk and for any i\n",
      "∈\n",
      "1)/k, i/k]. Let µi and βi\n",
      "e\n",
      "\n",
      "diﬀerent meanings from what they meant in Subsection 3.2. For f\n",
      "we know that f is a linear function on the half-open interval ((i\n",
      "be such that for x\n",
      "Because f\n",
      "\n",
      "Furthermore, the eﬀect from jumps (as in Equation (6)) is also zero. Therefore, for f\n",
      "and for any positive integer N\n",
      "\n",
      "Pk, the discrepancy from rounding, R(f, N ) (as in Proposition 3.6), is zero.\n",
      "Pk\n",
      "e\n",
      "\n",
      "1)/k, i/k], we have f (x) = µix + βi.\n",
      "\n",
      "kZ, we have\n",
      "\n",
      "((i\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "∈\n",
      "\n",
      "∈\n",
      "\n",
      "∈\n",
      "\n",
      "∈\n",
      "\n",
      "Pk is a subclass of\n",
      "e\n",
      "\n",
      "P\n",
      "\n",
      "∈\n",
      "\n",
      "k\n",
      "\n",
      "e\n",
      "\n",
      "(13)\n",
      "\n",
      "log (# RP(λ(f, N ))) =\n",
      "\n",
      "log((µi + 1)n + (βi −\n",
      "\n",
      "1)N ).\n",
      "\n",
      "i−1\n",
      "k N <n\n",
      "X\n",
      "Once again, we break the outer summation on the right-hand side above into when i = 1\n",
      "and when i\n",
      "\n",
      "2. When i = 1, we have, by Stirling’s formula,\n",
      "\n",
      "i=1\n",
      "X\n",
      "\n",
      "i\n",
      "k N\n",
      "\n",
      "≤\n",
      "\n",
      "≥\n",
      "\n",
      "N\n",
      "k\n",
      "\n",
      "X0<n\n",
      "≤\n",
      "1\n",
      "k ·\n",
      "\n",
      "=\n",
      "\n",
      "log((µi + 1)n + (βi −\n",
      "\n",
      "1)N ) =\n",
      "\n",
      "log(n) = log((N/k)!)\n",
      "\n",
      "X0<n\n",
      "≤\n",
      "\n",
      "N\n",
      "k\n",
      "\n",
      "N log N\n",
      "\n",
      "N +\n",
      "\n",
      "log N +\n",
      "\n",
      "log\n",
      "\n",
      "+ O(k/N ),\n",
      "\n",
      "(log k) + 1\n",
      "k\n",
      "\n",
      "·\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "2π\n",
      "k\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "When 2\n",
      "\n",
      "i\n",
      "\n",
      "k, we use the Euler-Maclaurin summation formula (cf.\n",
      "\n",
      "[MV07, Appen-\n",
      "\n",
      "for positive integers N\n",
      "\n",
      "≤\n",
      "dix B]) to obtain\n",
      "\n",
      "≤\n",
      "\n",
      "−\n",
      "kZ.\n",
      "\n",
      "∈\n",
      "\n",
      "=\n",
      "\n",
      "N log N +\n",
      "\n",
      "log(f (x) + x\n",
      "\n",
      "1) dx\n",
      "\n",
      "N\n",
      "\n",
      "log((µi + 1)n + (βi −\n",
      "\n",
      "1)N )\n",
      "\n",
      "i−1\n",
      "k N <n\n",
      "X\n",
      "\n",
      "≤\n",
      "\n",
      "i\n",
      "k N\n",
      "\n",
      "1\n",
      "k ·\n",
      "\n",
      "i/k\n",
      "\n",
      "1)/k\n",
      "\n",
      "i\n",
      "\n",
      "(Z\n",
      "(i\n",
      "−\n",
      "(µi + 1)\n",
      "(µi + 1)\n",
      "\n",
      "−\n",
      "\n",
      ") ·\n",
      "\n",
      "+ Of (1/N ).\n",
      "\n",
      "(18)\n",
      "\n",
      "1)\n",
      "k + (βi −\n",
      "1) !\n",
      "k + (βi −\n",
      "Combining these terms, we obtain the following theorem.\n",
      "\n",
      "1\n",
      "2 ·\n",
      "\n",
      "·\n",
      "i\n",
      "−\n",
      "\n",
      "log\n",
      "\n",
      " \n",
      "\n",
      "+\n",
      "\n",
      "1\n",
      "\n",
      "·\n",
      "\n",
      "Theorem 4.10. Let k be a positive integer. Let f\n",
      "\n",
      "log (# RP(λ(f, N ))) = N log N + Bf ·\n",
      "\n",
      "for positive integers N\n",
      "\n",
      "kZ, where Bf is as given in Theorem 3.7, and\n",
      "\n",
      "∈\n",
      "N +\n",
      "\n",
      "Pk. We have\n",
      "1\n",
      "e\n",
      "2\n",
      "\n",
      "log N + Df + Of (1/N ),\n",
      "\n",
      "∈\n",
      "\n",
      "Df :=\n",
      "\n",
      "log(2π) +\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "0\n",
      "Z\n",
      "\n",
      "xf ′(x)\n",
      "x(f (x) + x\n",
      "\n",
      "f (x) + 1\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "dx.\n",
      "\n",
      "−\n",
      "\n",
      "Note that one has to be careful about the integral in the formula of Df . Since in general\n",
      "f has a number of non-diﬀerentiable points, the derivative f ′ might be undeﬁned for some\n",
      "values of x. By the integral as expressed, we mean\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "Z\n",
      "\n",
      "xf ′(x)\n",
      "x(f (x) + x\n",
      "\n",
      "f (x) + 1\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "dx =\n",
      "\n",
      "−\n",
      "\n",
      "k\n",
      "\n",
      "i/k\n",
      "\n",
      "i=1 Z\n",
      "X\n",
      "\n",
      "(i\n",
      "\n",
      "1)/k\n",
      "\n",
      "−\n",
      "\n",
      "xf ′(x)\n",
      "x(f (x) + x\n",
      "\n",
      "f (x) + 1\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "dx.\n",
      "\n",
      "−\n",
      "\n",
      "Since f is linear in ((i\n",
      "\n",
      "−\n",
      "\n",
      "1)/k, i/k), the sum of integrals on the right-hand side is well-deﬁned.\n",
      "In the following examples, we\n",
      "\n",
      "An illustration of Theorem 4.10 is given in Figure 1.\n",
      "compute explicit asymptotic formulas for certain functions.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Example 4.11. Suppose that k is a positive integer. Let f : [0, 1]\n",
      "\n",
      "[0, 1] be given as\n",
      "\n",
      "→\n",
      "\n",
      "→\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "N\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "for all x\n",
      "∈\n",
      "Therefore,\n",
      "\n",
      "[0, 1]. Note that f\n",
      "\n",
      "1\n",
      "\n",
      "k and Df = 1\n",
      "\n",
      "2 log(2π/k).\n",
      "\n",
      "1\n",
      "k −\n",
      "\n",
      "f (x) := min\n",
      "\n",
      "1 +\n",
      "\n",
      "x, 1\n",
      "\n",
      ",\n",
      "\n",
      "(cid:26)\n",
      "Pk. We have Bf =\n",
      "e\n",
      "\n",
      "∈\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "log k\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "2π\n",
      "k ·\n",
      "\n",
      "∼ r\n",
      "\n",
      "N N + 1\n",
      "\n",
      "2\n",
      "\n",
      "(k−\n",
      "\n",
      "1e−\n",
      "\n",
      "1/k)N ,\n",
      "\n",
      "·\n",
      "\n",
      "# RP(λ(f, N ))\n",
      "\n",
      "as N\n",
      "\n",
      ", N\n",
      "\n",
      "→ ∞\n",
      "\n",
      "∈\n",
      "\n",
      "kZ.\n",
      "\n",
      "Example 4.12. Suppose that k is a positive integer. Let f : [0, 1]\n",
      "\n",
      "[0, 1] be given as\n",
      "\n",
      "f (x) := min\n",
      "\n",
      "1 +\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "2\n",
      "\n",
      "− ⌈\n",
      "k\n",
      "\n",
      "kx\n",
      "⌉\n",
      "\n",
      ", 1\n",
      "\n",
      ",\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "−\n",
      "\n",
      "for all x\n",
      "1\n",
      "Dk =\n",
      "\n",
      "[0, 1]. Note that f\n",
      "2 log 2 + 1\n",
      "\n",
      "2 log k + k\n",
      "\n",
      "∈\n",
      "2 log π. Therefore,\n",
      "\n",
      "∈\n",
      "\n",
      "Pk. We have Bf =\n",
      "e\n",
      "\n",
      "−\n",
      "\n",
      "log k +\n",
      "\n",
      "2\n",
      "\n",
      "log 2\n",
      "\n",
      "1 and\n",
      "\n",
      "−\n",
      "\n",
      "2\n",
      "k\n",
      "\n",
      "−\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "# RP(λ(f, N ))\n",
      "\n",
      "N N + 1\n",
      "\n",
      "2\n",
      "\n",
      "22\n",
      "\n",
      "−\n",
      "\n",
      "2/k k−\n",
      "\n",
      "1 e−\n",
      "\n",
      "1\n",
      "\n",
      ",\n",
      "\n",
      "2kπ\n",
      "k ·\n",
      "\n",
      "∼ r\n",
      "\n",
      "·\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "as N\n",
      "\n",
      ", N\n",
      "\n",
      "→ ∞\n",
      "\n",
      "∈\n",
      "\n",
      "kZ.\n",
      "\n",
      "As another application of our result, we can detect the number of ground bumps of Dyck\n",
      "Dyck(k) is a Dyck path from (0, k) to (k, 0), then a ground bump\n",
      "paths analytically. If D\n",
      "of γ is an intersection between γ and the open line segment from (0, k) to (k, 0). Recall\n",
      "that a partition λ\n",
      "In the\n",
      "following proposition, a ground bump of λ is deﬁned as a ground bump of the Dyck path\n",
      "corresponding to λ.\n",
      "\n",
      "∈ Dk can be thought of as a Dyck path from (0, k) to (k, 0).\n",
      "\n",
      "∈\n",
      "\n",
      "Proposition 4.13. Let λ be any nonempty partition such that RP(λ)\n",
      "exist positive real numbers α1, α2, α3, α4 > 0 such that\n",
      "\n",
      "= ∅. Then, there\n",
      "\n",
      "⊙\n",
      ". Furthermore, α1 = λ1 and\n",
      "\n",
      "∼\n",
      "\n",
      "# RP(N\n",
      "\n",
      "λ)\n",
      "\n",
      "N α1N +α2\n",
      "\n",
      "αN\n",
      "\n",
      "3 ·\n",
      "\n",
      "·\n",
      "\n",
      "α4,\n",
      "\n",
      "as N\n",
      "\n",
      "→ ∞\n",
      "\n",
      "α2 =\n",
      "\n",
      "(#ground bumps of λ) + 1\n",
      "2\n",
      "\n",
      ".\n",
      "\n",
      "Proof. Since RP(λ)\n",
      "as a concatenation λ = λ(1)\n",
      "ground bumps. Observe that\n",
      "\n",
      "∗\n",
      "\n",
      "= ∅, we have that λ\n",
      "\n",
      "λ(2)\n",
      "\n",
      "∗ · · · ∗\n",
      "\n",
      "∈ Dλ1 . As a Dyck path, λ can be uniquely written\n",
      "λ(p) such that each λ(i) is a Dyck path without\n",
      "\n",
      "(19)\n",
      "\n",
      "(20)\n",
      "\n",
      "and that\n",
      "\n",
      "# RP(N\n",
      "\n",
      "λ) =\n",
      "\n",
      "# RP(N\n",
      "\n",
      "⊙\n",
      "\n",
      "λ(i)),\n",
      "\n",
      "⊙\n",
      "\n",
      "p\n",
      "\n",
      "i=1\n",
      "Y\n",
      "\n",
      "p\n",
      "\n",
      "i=1\n",
      "X\n",
      "\n",
      "λ1 =\n",
      "\n",
      "λ(i)\n",
      "1 ,\n",
      "\n",
      "where λ(i)\n",
      "\n",
      "1 denotes the ﬁrst part of the partition λ(i).\n",
      "\n",
      "For each i\n",
      "\n",
      "[p], there is a unique corresponding function f (i)\n",
      "\n",
      ". We have that for\n",
      "\n",
      "any positive integer N ,\n",
      "\n",
      "∈\n",
      "\n",
      "Pλ(i)\n",
      "\n",
      "1\n",
      "\n",
      "∈\n",
      "\n",
      "e\n",
      "\n",
      "# RP(N\n",
      "\n",
      "⊙\n",
      "\n",
      "λ(i)) = # RP(λ(f (i), N λ(i)\n",
      "\n",
      "1 )).\n",
      "\n",
      "13\n",
      "\n",
      "6\n",
      "6\n",
      "\f",
      "√2π\n",
      "\n",
      "N N +1/2\n",
      "\n",
      "(e−\n",
      "\n",
      "1)N\n",
      "\n",
      "∼\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "4\n",
      "3 π\n",
      "\n",
      "2\n",
      "3 π\n",
      "\n",
      "8\n",
      "3 π\n",
      "\n",
      "4\n",
      "3 π\n",
      "\n",
      "4\n",
      "3 π\n",
      "\n",
      "2\n",
      "3 π\n",
      "\n",
      "∼\n",
      "\n",
      "q\n",
      "\n",
      "∼\n",
      "\n",
      "q\n",
      "\n",
      "∼\n",
      "\n",
      "q\n",
      "\n",
      "∼\n",
      "\n",
      "q\n",
      "\n",
      "∼\n",
      "\n",
      "q\n",
      "\n",
      "∼\n",
      "\n",
      "q\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "N N +1/2\n",
      "\n",
      "21 3−\n",
      "\n",
      "1 e−\n",
      "\n",
      "2/3\n",
      "\n",
      "N N +1/2\n",
      "\n",
      "24/3 3−\n",
      "\n",
      "1 e−\n",
      "\n",
      "1\n",
      "\n",
      "N N +1/2\n",
      "\n",
      "24/3 3−\n",
      "\n",
      "1 e−\n",
      "\n",
      "1\n",
      "\n",
      "N N +1/2\n",
      "\n",
      "22/3 3−\n",
      "\n",
      "1 e−\n",
      "\n",
      "2/3\n",
      "\n",
      "N N +1/2\n",
      "\n",
      "22/3 3−\n",
      "\n",
      "1 e−\n",
      "\n",
      "2/3\n",
      "\n",
      "N N +1/2\n",
      "\n",
      "3−\n",
      "\n",
      "1 e−\n",
      "\n",
      "1/3\n",
      "\n",
      "N\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(22)\n",
      "\n",
      "(24)\n",
      "\n",
      "Figure 1. The seven functions in the class\n",
      "asymptotic formulas for # RP(λ(f, N )), as N\n",
      "\n",
      "P3 and their corresponding\n",
      "3Z.\n",
      "→ ∞\n",
      "e\n",
      "\n",
      ", N\n",
      "\n",
      "∈\n",
      "\n",
      "(Note that the notation λ on the right-hand side of the equation above is an operator, not\n",
      "a partition.) Now, Theorem 4.10 gives\n",
      "\n",
      "(21)\n",
      "\n",
      "log(# RP(N\n",
      "\n",
      "⊙\n",
      "\n",
      "1 N log N + (λ(i)\n",
      "λi)) = λ(i)\n",
      "1\n",
      "2\n",
      "\n",
      "log λ(i)\n",
      "\n",
      "+\n",
      "\n",
      "1 + Df (i) + Oλ(1/N ).\n",
      "\n",
      "1 log λ(i)\n",
      "\n",
      "1 )N + Bf (i) λ(i)\n",
      "\n",
      "1 N +\n",
      "\n",
      "log N\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "Combining Equations (19), (20), and (21), we obtain\n",
      "\n",
      "(23)\n",
      "\n",
      "log(# RP(N\n",
      "\n",
      "λ)) = λ1N log N +\n",
      "\n",
      "⊙\n",
      "\n",
      "p\n",
      "\n",
      "i=1 (cid:16)\n",
      "X\n",
      "p\n",
      "1\n",
      "2\n",
      "\n",
      "i=1 (cid:18)\n",
      "X\n",
      "\n",
      "λ(i)\n",
      "1 log λ(i)\n",
      "\n",
      "1 + Bf (i) λ(i)\n",
      "\n",
      "i\n",
      "\n",
      "N\n",
      "\n",
      "·\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "log λ(i)\n",
      "\n",
      "1 + Df (i)\n",
      "\n",
      "+ Oλ(1/N ),\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "+\n",
      "\n",
      "log N +\n",
      "\n",
      "p\n",
      "2\n",
      "\n",
      "for positive integers N . Since p\n",
      "the proof.\n",
      "\n",
      "1 is the number of ground bumps of λ, we have ﬁnished\n",
      "(cid:3)\n",
      "\n",
      "−\n",
      "\n",
      "4.3. Properties of Df . Theorem 4.10 gives an integral formula for Df . In applications,\n",
      "it is also useful to have the following formula, which is immediate from Equations (15) and\n",
      "(17).\n",
      "\n",
      "Proposition 4.14. For any function f\n",
      "1\n",
      "2\n",
      "\n",
      "log(2πf (1)) +\n",
      "\n",
      "Df =\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      ", we have\n",
      "\n",
      "log\n",
      "\n",
      "limx\n",
      "\n",
      "ց\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "f (a) + a\n",
      "\n",
      "1\n",
      "a f (x) + x\n",
      "\n",
      "−\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "−\n",
      "\n",
      "Note that the sum on the right-hand side is ﬁnite, since there are only ﬁnitely many discon-\n",
      "tinuous points for f .\n",
      "\n",
      "∈\n",
      "\n",
      "P\n",
      "\n",
      "e\n",
      "(0,1)\n",
      "Xa\n",
      "∈\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      ", Lp) be the metric space obtained from\n",
      "\n",
      "Df induces a well-deﬁned map\n",
      "\n",
      "Let p\n",
      "endowing\n",
      "\n",
      "≥\n",
      "P\n",
      "\n",
      "1 be any positive real number. Let (\n",
      "with the Lp norm. The map f\n",
      "D : (\n",
      "\n",
      "7→\n",
      ", Lp)\n",
      "\n",
      "P\n",
      "\n",
      "e\n",
      "→\n",
      "\n",
      "e\n",
      "\n",
      "R.\n",
      "In Proposition 3.8, we have seen that the map B : (\n",
      "P\n",
      "e\n",
      "everywhere. The following proposition says that D, considered as a function from (\n",
      "to (R, Euclid), exhibits a similar topological property.\n",
      "\n",
      "(R, Euclid) is discontinuous\n",
      ", Lp)\n",
      "\n",
      ", Lp)\n",
      "\n",
      "→\n",
      "\n",
      "P\n",
      "\n",
      "P\n",
      "\n",
      "Proposition 4.15. The map D : (\n",
      "\n",
      ", Lp)\n",
      "\n",
      "e\n",
      "(R, Euclid) is discontinuous everywhere on\n",
      "\n",
      ".\n",
      "\n",
      "be arbitrary. Let k be a positive integer for which f\n",
      "\n",
      "→\n",
      "\n",
      "P\n",
      "\n",
      "e\n",
      "\n",
      "Proof. Let f\n",
      "P\n",
      "positive integer n\n",
      "e\n",
      "\n",
      "∈\n",
      "\n",
      "≥\n",
      "\n",
      "k + 1, deﬁne\n",
      "\n",
      "∈\n",
      "\n",
      "P\n",
      "Pk. For each\n",
      "e\n",
      "e\n",
      "\n",
      "gn(x) =\n",
      "\n",
      "f (x)\n",
      "n−\n",
      "\n",
      "(\n",
      "\n",
      "1 + n−\n",
      "\n",
      "2\n",
      "\n",
      "if x\n",
      "1\n",
      "≤\n",
      "if x > 1\n",
      "\n",
      "1\n",
      "n ,\n",
      "1\n",
      "n .\n",
      "\n",
      "−\n",
      "−\n",
      "\n",
      "Note that\n",
      "hand, we obtain from Proposition 4.14 that\n",
      "\n",
      "is a sequence of functions in\n",
      "\n",
      "gn}\n",
      "\n",
      "{\n",
      "\n",
      "P\n",
      "\n",
      "that converges in Lp to f . On the other\n",
      "\n",
      "Dgn =\n",
      "\n",
      "e\n",
      "1\n",
      "log(n + 1) + Df +\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "f (1\n",
      "\n",
      "1\n",
      "n )\n",
      "−\n",
      "f (1)\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "−\n",
      "\n",
      ".\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "log\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "ր\n",
      "\n",
      ", as n\n",
      "\n",
      ", as n\n",
      "\n",
      ".\n",
      "→ ∞\n",
      "\n",
      "1 f (x) = f (1), the third term on the right-hand side converges to 0 as n\n",
      "\n",
      "Since limx\n",
      "The second term does not depend on n. The ﬁrst term goes to +\n",
      "Dgn → ∞\n",
      "4.4. Bounds for Bf and Df . In this subsection, we determine the extremal values for\n",
      "Pk.\n",
      "both Bf and Df among all functions f\n",
      "e\n",
      "1\n",
      "k ≤\n",
      "Both bounds are tight. The lower bound is attained if and only if f is the function in\n",
      "Example 4.11. The upper bound is attained if and only if f (x) = 1 for every x\n",
      "\n",
      "Proposition 4.16. Let k be a positive integer. Let f\n",
      "\n",
      "Pk. Then,\n",
      "1.\n",
      "e\n",
      "\n",
      ".\n",
      "→ ∞\n",
      ". Therefore,\n",
      "(cid:3)\n",
      "\n",
      "∈\n",
      "Bf ≤ −\n",
      "\n",
      "[0, 1].\n",
      "\n",
      "→ ∞\n",
      "\n",
      "log k\n",
      "\n",
      "∞\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "∈\n",
      "\n",
      "∈\n",
      "\n",
      "Proof. The upper bound follows from Proposition 3.9(a). For the equality case of the upper\n",
      "bound, note that since limx\n",
      "[0, 1), then f (1) must\n",
      "also be 1.\n",
      "\n",
      "1 f (x) = f (1), if f (x) = 1 for all x\n",
      "\n",
      "∈\n",
      "\n",
      "ր\n",
      "\n",
      "For the lower bound, note that for any function f\n",
      "\n",
      "1/k. Therefore,\n",
      "\n",
      "by Proposition 3.5, we have\n",
      "\n",
      "1\n",
      "\n",
      "∈\n",
      "\n",
      "Pk, we have loft(f )\n",
      "e\n",
      "\n",
      "1\n",
      "\n",
      "≥\n",
      "\n",
      "1/k\n",
      "\n",
      "Bf =\n",
      "\n",
      "log(f (x) + x\n",
      "\n",
      "1) dx\n",
      "\n",
      "log(x) dx +\n",
      "\n",
      "log(1/k) dx\n",
      "\n",
      "0\n",
      "Z\n",
      "\n",
      "0\n",
      "Z\n",
      "Computing the last expression yields the desired lower bound. The equality case happens\n",
      "1 = 1/k on (1/k, 1]. This is exactly if and only if\n",
      "when f (x) = 1 on [0, 1/k] and f (x) + x\n",
      "(cid:3)\n",
      "f is the function in Example 4.11.\n",
      "\n",
      "1/k\n",
      "\n",
      "−\n",
      "\n",
      "Z\n",
      "\n",
      "−\n",
      "\n",
      "≥\n",
      "\n",
      "Proposition 4.17. Let k be a positive integer. Let f\n",
      "\n",
      "∈\n",
      "\n",
      "Pk. Then,\n",
      "2kπ\n",
      ".\n",
      "e\n",
      "k\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "2π\n",
      "k\n",
      "\n",
      "log\n",
      "\n",
      "Df ≤\n",
      "Both bounds are tight. The lower bound is attained if and only if f is continuous and\n",
      "f (1) = 1/k (i.e., if and only if f is in the Catalan-numerous family discussed in Remark 4.7).\n",
      "The upper bound is attained if and only if f is the function in Example 4.12.\n",
      "\n",
      "log\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "Proof. For the lower bound, note that by the formula in Proposition 4.14, it is immediate\n",
      "that\n",
      "\n",
      "Df ≥\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "15\n",
      "\n",
      "log(2πf (1)).\n",
      "\n",
      "\f",
      "Since f\n",
      "1/k. Combining the two inequalities yields the desired lower\n",
      "bound. The equality is attained if and only if f (1) = 1/k and there are no “jumps.” In\n",
      "other words, f is continuous and f (1) = 1/k.\n",
      "\n",
      "Pk, we have f (1)\n",
      "e\n",
      "\n",
      "≥\n",
      "\n",
      "∈\n",
      "\n",
      "For the upper bound, we use the following strategy. We start with an arbitrary function\n",
      "f\n",
      "Pk, and then we keep transforming the function (if possible) in a number of steps so that\n",
      "in each step Df becomes larger. We claim that we can always end at the unique extremal\n",
      "function in Example 4.12.\n",
      "e\n",
      "\n",
      "∈\n",
      "\n",
      "∈\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "First, start with any function f\n",
      "\n",
      "Pk. Consider whether f has a linear piece with a\n",
      "1)/k, i/k], f has a negative slope – modify the\n",
      "strictly negative slope. If so – say over ((i\n",
      "function f so that over ((i\n",
      "1)/k f (x)\n",
      "1)/k, i/k], it becomes constant with the value limx\n",
      "e\n",
      "Pk, and the value Df strictly increases.\n",
      "instead. The new function remains in\n",
      "e\n",
      "\n",
      "Second, now assume that the function f is already piecewise constant. Consider the\n",
      "1/k, 1]. If it is strictly greater than 2/k, change the value to 2/k. This\n",
      "value of f over (1\n",
      "change strictly increases Df . Then, consider the value of f over (1\n",
      "1/k]. If it is\n",
      "strictly greater than 3/k, change the value to 3/k. Keep going in this manner from the right\n",
      "to the left. The resulting function is the unique function in Example 4.12. This proves the\n",
      "upper bound.\n",
      "\n",
      "2/k, 1\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "ց\n",
      "\n",
      "−\n",
      "\n",
      "(i\n",
      "\n",
      "Note that in each step, if a change is made, the value of Df increases strictly. This shows\n",
      "that the equality case for the upper bound happens if and only if f is the unique function\n",
      "(cid:3)\n",
      "in Example 4.12.\n",
      "\n",
      "5. Cumulative X-rays of rook placements\n",
      "\n",
      "∈\n",
      "Let n be a positive integer. Let λ\n",
      "\n",
      "5.1. Marginal Probabilities. In the following, for a ﬁnite nonempty set S, we denote\n",
      "by Unif(S) the uniform distribution on S. The notation X\n",
      "Unif(S) means that X is a\n",
      "1.\n",
      "uniform random variable so that\n",
      "\n",
      "S, P(X = s) =\n",
      "∈ Dn be a partition. In what follows, let us consider\n",
      "our partitions in the French notation so that the boxes of λ are bottom- and left-aligned\n",
      "and there are λ1 boxes on the bottom row, λ2 boxes on the second row from the bottom,\n",
      "and so on.\n",
      "\n",
      "s\n",
      "∀\n",
      "\n",
      "∼\n",
      "\n",
      "S\n",
      "\n",
      "−\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "Proposition 5.1. Let π\n",
      "\n",
      "Unif(RP(λ)). Let i, j\n",
      "\n",
      "[n].\n",
      "\n",
      "∈\n",
      "\n",
      "(a) If λn+1\n",
      "(b) If λn+1\n",
      "\n",
      "i < j, then E(πij ) = 0.\n",
      "−\n",
      "j, and suppose i′\n",
      "i ≥\n",
      "\n",
      "∈\n",
      "\n",
      "−\n",
      "\n",
      "∼\n",
      "\n",
      "[n] is the smallest index such that λn+1\n",
      "\n",
      "j, then\n",
      "\n",
      "i′\n",
      "\n",
      "−\n",
      "\n",
      "≥\n",
      "\n",
      "E(πij ) =\n",
      "\n",
      "λn+1\n",
      "\n",
      "t −\n",
      "\n",
      "t\n",
      "t + 1 \n",
      "\n",
      "1\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "t −\n",
      "Proof. (a) It follows immediately from the deﬁnition of RP(λ) (cf. the beginning of Sec-\n",
      "tion 2) that πij = 0.\n",
      "\n",
      "Yi′\n",
      "\n",
      "λn+1\n",
      "\n",
      "λn+1\n",
      "\n",
      "i + 1\n",
      "\n",
      "i −\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t<i\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "≤\n",
      "\n",
      "·\n",
      "\n",
      "(b) Suppose that µ is the partition obtained by removing the ith row from the top (the\n",
      "i) and the jth column from the left from the Young diagram of\n",
      "\n",
      "row corresponding to λn+1\n",
      "λ. Observe that the probability that πij = 1 is # RP(µ)/# RP(λ).\n",
      "\n",
      "−\n",
      "\n",
      "The formula in Equation (1) gives\n",
      "\n",
      "# RP(µ) =\n",
      "\n",
      "(λn+1\n",
      "\n",
      "t −\n",
      "\n",
      "−\n",
      "\n",
      "(t\n",
      "\n",
      "1))\n",
      "\n",
      "−\n",
      "\n",
      "(λn+1\n",
      "\n",
      "t −\n",
      "\n",
      "−\n",
      "\n",
      "t)\n",
      "\n",
      ".\n",
      "\n",
      " \n",
      "\n",
      "Yt<i′ or t>i\n",
      "\n",
      "! \n",
      "\n",
      "\n",
      "t<i\n",
      "\n",
      "Yi′\n",
      "\n",
      "≤\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "E(πij ) = P(πij = 1) =\n",
      "\n",
      "# RP(µ)\n",
      "# RP(λ)\n",
      "\n",
      "=\n",
      "\n",
      "λn+1\n",
      "\n",
      "λn+1\n",
      "\n",
      "−\n",
      "t −\n",
      "\n",
      "−\n",
      "\n",
      "t −\n",
      "\n",
      "t\n",
      "t + 1 \n",
      "\n",
      "·\n",
      "\n",
      "λn+1\n",
      "\n",
      "i −\n",
      "\n",
      "−\n",
      "\n",
      ",\n",
      "\n",
      "i + 1\n",
      "\n",
      "\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t<i\n",
      "\n",
      "Yi′\n",
      "\n",
      "≤\n",
      "\n",
      "16\n",
      "\n",
      "Therefore,\n",
      "\n",
      "as desired.\n",
      "\n",
      "\f",
      "The following corollary is immediate from Proposition 5.1.\n",
      "\n",
      "Corollary 5.2. If π\n",
      "\n",
      "Unif(RP(λ)) and i, j\n",
      "\n",
      "[n], then\n",
      "\n",
      "∼\n",
      "\n",
      "∈\n",
      "\n",
      "1\n",
      "\n",
      "E(πij )\n",
      "\n",
      "≤\n",
      "\n",
      "λn+1\n",
      "\n",
      "i −\n",
      "\n",
      "−\n",
      "\n",
      ".\n",
      "\n",
      "i + 1\n",
      "\n",
      "We also have the following result. Let λ′\n",
      "\n",
      "Corollary 5.3. Let π\n",
      "and λn+1\n",
      "i2 ≥\n",
      "\n",
      "−\n",
      "\n",
      "Unif(RP(λ)). Suppose that i1, i2, j1, j2 ∈\n",
      "\n",
      "j1. If λi1 = λi2 and λ′j1 = λ′j2 , then E(πi1j1 ) = E(πi2j2 ).\n",
      "\n",
      "∼\n",
      "\n",
      "∈ Dn denote the conjugate partition of λ.\n",
      "i1 ≥\n",
      "\n",
      "[n] satisfy λn+1\n",
      "\n",
      "−\n",
      "\n",
      "j1\n",
      "\n",
      "Proof. We proceed in a similar manner to how we proved Proposition 5.1(b). Namely, let\n",
      "µ(1) (and µ(2)) denote the resulting partition from removing the (i1, j1) (and resp. (i2, j2))\n",
      "box (together with the row and the column) from λ. It is not hard to see that µ(1) = µ(2).\n",
      "(cid:3)\n",
      "This ﬁnishes the proof.\n",
      "\n",
      "Like before, we may think of λ as a Dyck path from (0, n) to (n, 0), which we can write\n",
      "\n",
      "as the following concatenation\n",
      "\n",
      "λ = Rr1Dd1Rr2Dd2\n",
      "\n",
      ",\n",
      "\n",
      "· · ·\n",
      "\n",
      "where R denotes a unit step to the right, and D denotes a unit step down. The equation\n",
      "above means that λ starts by going r1 steps to the right, and then d1 steps down, and so\n",
      "as the minimum run of λ, denoted\n",
      "on. Let us refer to the quantity min\n",
      "r1, d1, r2, d2, . . .\n",
      "{\n",
      "mr(λ). For instance, since λ\n",
      "n, where the equality is attained if and\n",
      "∈ Dn, we have mr(λ)\n",
      "only if λ = RnDn.\n",
      "\n",
      "≤\n",
      "\n",
      "}\n",
      "\n",
      "Proposition 5.4. Let π\n",
      "any t diﬀerent boxes b1 = (i1, j1), b2 = (i2, j2), . . ., bt = (it, jt)\n",
      "is 1 in all these t boxes is\n",
      "\n",
      "Unif(RP(λ)). Let t\n",
      "\n",
      "≤\n",
      "\n",
      "∼\n",
      "\n",
      "∈\n",
      "\n",
      "mr(λ) be a positive integer. Then, for\n",
      "[n]2, the probability that π\n",
      "\n",
      "E(πb1 πb2 · · ·\n",
      "\n",
      "πbt )\n",
      "\n",
      "≤\n",
      "\n",
      "mr(λ)\n",
      "\n",
      "(mr(λ)\n",
      "\n",
      "·\n",
      "\n",
      "(mr(λ)\n",
      "\n",
      "t + 1)\n",
      "\n",
      "−\n",
      "\n",
      "· · ·\n",
      "\n",
      "−\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "1)\n",
      "\n",
      "Proof. If any of the t boxes is “outside” the Young diagram of λ, we are done. Suppose that\n",
      "j1 and so on). Since removing\n",
      "all these boxes are inside the Young diagram (i.e., λn+1\n",
      "a box (together with its row and its column) reduces the minimum run by at most 1, it\n",
      "suﬃces to show that if we remove one box b (together with its row and its column) from λ\n",
      "and obtain a new partition µ, then\n",
      "\n",
      "i1 ≥\n",
      "\n",
      "−\n",
      "\n",
      "# RP(µ)\n",
      "# RP(λ) ≤\n",
      "\n",
      "1\n",
      "mr(λ)\n",
      "\n",
      ".\n",
      "\n",
      "(25)\n",
      "\n",
      "(26)\n",
      "\n",
      "Consider a box b = (i, j) such that λn+1\n",
      "λn+1\n",
      "deﬁnition of the minimum run, we have\n",
      "\n",
      "i = λn+1\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "i ≥\n",
      "−\n",
      "i′ . Let i′′ denote the largest index for which λn+1\n",
      "\n",
      "j. Let i′ denote the smallest index for which\n",
      "i′′ . By the\n",
      "\n",
      "i = λn+1\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "By Corollaries 5.2 and 5.3, we have\n",
      "\n",
      "i′′\n",
      "\n",
      "i′ + 1\n",
      "\n",
      "mr(λ).\n",
      "\n",
      "−\n",
      "\n",
      "≥\n",
      "\n",
      "(27)\n",
      "\n",
      "E(πij) = E(πi′j)\n",
      "\n",
      "which implies (25).\n",
      "\n",
      "1\n",
      "\n",
      "≤\n",
      "\n",
      "λn+1\n",
      "\n",
      "i′\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "=\n",
      "\n",
      "i′ + 1\n",
      "\n",
      "(λn+1\n",
      "\n",
      "1\n",
      "i′′) + i′′ −\n",
      "\n",
      "i′′\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "(26)\n",
      "\n",
      "1\n",
      "mr(λ)\n",
      "\n",
      ",\n",
      "\n",
      "i′ + 1\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "Since the minimum run also grows as we dilate partitions, we immediately have the\n",
      "\n",
      "following corollary.\n",
      "\n",
      "Corollary 5.5. Let t and N be positive integers such that t\n",
      "λ)). Then, for any t diﬀerent boxes b1, b2, . . . , bt ∈\n",
      "Unif(RP(N\n",
      "\n",
      "⊙\n",
      "\n",
      "N . Suppose that π\n",
      "\n",
      "≤\n",
      "[nN ]2, we have\n",
      "\n",
      "∼\n",
      "\n",
      "E(πb1 πb2 · · ·\n",
      "\n",
      "πbt )\n",
      "\n",
      "1\n",
      "\n",
      "≤\n",
      "\n",
      "N (N\n",
      "17\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "· · ·\n",
      "\n",
      "(N\n",
      "\n",
      "t + 1)\n",
      "\n",
      "−\n",
      "\n",
      ".\n",
      "\n",
      "\f",
      "The proposition below shows that these marginal probabilities P(πi,j = 1) behave nicely\n",
      "\n",
      "in the following sense, when we dilate partitions.\n",
      "\n",
      "Proposition 5.6. If π\n",
      "[nN ], we have\n",
      "\n",
      "∼\n",
      "\n",
      "Unif(RP(N\n",
      "\n",
      "λ)) and π↓\n",
      "\n",
      "Unif(RP(λ)). Then, for any i, j\n",
      "\n",
      "⊙\n",
      "\n",
      "∼\n",
      "\n",
      "Proof. This follows from a direct computation using Proposition 5.1(b).\n",
      "\n",
      "E(πi,j) =\n",
      "\n",
      "1\n",
      "N ·\n",
      "\n",
      "E\n",
      "\n",
      "π↓\n",
      "⌈\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "i/N\n",
      "\n",
      ",\n",
      "\n",
      "j/N\n",
      "\n",
      "⌉\n",
      "\n",
      "⌈\n",
      "\n",
      "⌉\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "∈\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "5.2. Cumulative X-rays. Let n be a positive integer. Suppose that a permutation π\n",
      "is given. The cumulative X-ray of π is the piecewise constant function ξπ : [0, 2n]\n",
      "by\n",
      "\n",
      "→\n",
      "\n",
      "Sn\n",
      "∈\n",
      "R given\n",
      "\n",
      "ξπ(t) :=\n",
      "\n",
      "πij .\n",
      "\n",
      "[n]\n",
      "Xi,j\n",
      "∈\n",
      "t\n",
      "i+j\n",
      "≤\n",
      "\n",
      "ξπ(t) :=\n",
      "\n",
      "ξπ(nt).\n",
      "e\n",
      "\n",
      "1\n",
      "n ·\n",
      "\n",
      "→\n",
      "\n",
      "We also deﬁne the normalized version of cumulative X-rays. The normalized cumulative\n",
      "X-ray of π\n",
      "\n",
      "Sn is the piecewise constant function\n",
      "\n",
      "[0, 1] given by\n",
      "\n",
      "ξπ : [0, 2]\n",
      "\n",
      "∈\n",
      "\n",
      "The following is a counting lemma which is easy to prove.\n",
      "\n",
      "Lemma 5.7. For each real number φ\n",
      "\n",
      "R and each positive integer N\n",
      "\n",
      "Z\n",
      "\n",
      "∈\n",
      "\n",
      "≥\n",
      "\n",
      "1, let\n",
      "\n",
      "S(φ; N ) := #\n",
      "\n",
      "[N ]\n",
      "\n",
      "[N ]\n",
      "\n",
      "x + y\n",
      "\n",
      "∈\n",
      "\n",
      "×\n",
      "\n",
      "|\n",
      "\n",
      "φN\n",
      "\n",
      ".\n",
      "\n",
      "}\n",
      "\n",
      "≤\n",
      "\n",
      "e\n",
      "\n",
      "∈\n",
      "(x, y)\n",
      "{\n",
      "\n",
      "Then, we have the following.\n",
      "\n",
      "(a) If φ\n",
      "(b) If 0\n",
      "\n",
      "0, then S(φ; N ) = 0.\n",
      "φ\n",
      "\n",
      "1, then\n",
      "\n",
      "≤\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "S(φ; N ) =\n",
      "\n",
      "=\n",
      "\n",
      "N 2 + O(φN ).\n",
      "\n",
      "φN\n",
      "2\n",
      "\n",
      "⌋\n",
      "\n",
      "⌊\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "φ2\n",
      "2\n",
      "\n",
      "(c) If 1\n",
      "\n",
      "φ\n",
      "\n",
      "2, then\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "S(φ; N ) = N 2\n",
      "\n",
      "2N\n",
      "\n",
      "− ⌊\n",
      "\n",
      "φN\n",
      "2\n",
      "\n",
      "⌋\n",
      "\n",
      "+ 1\n",
      "\n",
      "φ2\n",
      "2\n",
      "\n",
      "=\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "−\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "−\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "+ 2φ\n",
      "\n",
      "1\n",
      "\n",
      "N 2 + O(φN ).\n",
      "\n",
      "−\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(d) If φ\n",
      "\n",
      "2, then S(φ; N ) = N 2.\n",
      "In (b) and (c), the implicit constants are absolute.\n",
      "\n",
      "≥\n",
      "\n",
      "For convenience, let us reserve the symbol c. In the following, we let c denote the function\n",
      "\n",
      "c : R\n",
      "\n",
      "R given by\n",
      "\n",
      "→\n",
      "\n",
      "0\n",
      "t2\n",
      "2\n",
      "\n",
      "if t\n",
      "0,\n",
      "≤\n",
      "if 0 < t\n",
      "if 1 < t\n",
      "2.\n",
      "if t\n",
      "\n",
      "1\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "t2\n",
      "2 + 2t\n",
      "\n",
      "c(t) := \n",
      "\n",
      "\n",
      "S(φ; N ) = c(φ)N 2 + O(N ),\n",
      "\n",
      "≤\n",
      "≤\n",
      "\n",
      "≥\n",
      "\n",
      "−\n",
      "\n",
      "1,\n",
      "2, and\n",
      "\n",
      "∈ Dn, let us deﬁne a function mλ : [0, 2]\n",
      "dx dy,\n",
      "\n",
      "mλ(t) :=\n",
      "\n",
      "E\n",
      "\n",
      "→\n",
      "\n",
      "π\n",
      "⌈\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      ",\n",
      "⌉\n",
      "\n",
      "⌈\n",
      "\n",
      "⌉\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "[0, 1] by\n",
      "\n",
      "Z Z\n",
      "x,y\n",
      "∈\n",
      "x+y\n",
      "\n",
      "(0,n]\n",
      "nt\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "18\n",
      "\n",
      "Thus, Lemma 5.7 says that\n",
      "\n",
      "for positive integers N .\n",
      "For each partition λ\n",
      "\n",
      "(28)\n",
      "\n",
      "(29)\n",
      "\n",
      "where π\n",
      "\n",
      "Unif(RP(λ)).\n",
      "\n",
      "∼\n",
      "\n",
      "\f",
      "Proposition 5.8. Let λ\n",
      "for any real number t\n",
      "\n",
      "∈ Dn and N\n",
      "[0, 2], we have\n",
      "\n",
      "∈\n",
      "\n",
      "Z\n",
      "\n",
      "∈\n",
      "\n",
      "≥\n",
      "\n",
      "1. Suppose that π\n",
      "\n",
      "Unif(RP(N\n",
      "\n",
      "λ)). Then,\n",
      "\n",
      "∼\n",
      "\n",
      "⊙\n",
      "\n",
      "Proof. From Proposition 5.6, we have\n",
      "\n",
      "e\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "E\n",
      "\n",
      "ξπ(t)\n",
      "\n",
      "= mλ(t) + Oλ\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      ".\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(30)\n",
      "\n",
      "(31)\n",
      "\n",
      "(32)\n",
      "\n",
      "(33)\n",
      "\n",
      "(34)\n",
      "\n",
      "(35)\n",
      "\n",
      "(36)\n",
      "\n",
      "1\n",
      "nN 2\n",
      "\n",
      "E\n",
      "\n",
      "ξπ(t)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "e\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "nN 2\n",
      "\n",
      "E\n",
      "\n",
      "π↓\n",
      "⌈\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "i/N\n",
      "\n",
      ",\n",
      "\n",
      "j/N\n",
      "\n",
      "⌉\n",
      "\n",
      "⌈\n",
      "\n",
      "⌉\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "[nN ]\n",
      "Xi,j\n",
      "∈\n",
      "nN t\n",
      "i+j\n",
      "≤\n",
      "n\n",
      "n\n",
      "\n",
      "a=1\n",
      "X\n",
      "\n",
      "Xb=1\n",
      "\n",
      "[nN ]\n",
      "nN t\n",
      "\n",
      "Xi,j\n",
      "∈\n",
      "i+j\n",
      "≤\n",
      "j/N\n",
      ",\n",
      "\n",
      "⌉\n",
      "\n",
      "(\n",
      "\n",
      "i/N\n",
      "\n",
      "⌈\n",
      "\n",
      "⌉\n",
      "\n",
      "⌈\n",
      "\n",
      ")=(a,b)\n",
      "\n",
      "E\n",
      "\n",
      "π↓a,b\n",
      "\n",
      ".\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "Note that the innermost summation in the last expression above is over pairs (i, j) of\n",
      "aN , and\n",
      "bN . By translation, the number of such pairs is exactly the number\n",
      "b + 2)N . By our discussion above, the number\n",
      "\n",
      "positive integers such that (i) i, j\n",
      "(iv) bN\n",
      "of (x, y)\n",
      "is exactly S(nt\n",
      "\n",
      "N + 1\n",
      "≤\n",
      "[N ]2 such that x + y\n",
      "b + 2; N ).\n",
      "\n",
      "nN , (ii) i + j\n",
      "\n",
      "nN t, (iii) aN\n",
      "\n",
      "N + 1\n",
      "\n",
      "−\n",
      "∈\n",
      "\n",
      "(nt\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "−\n",
      "\n",
      "a\n",
      "\n",
      "j\n",
      "\n",
      "i\n",
      "\n",
      "a\n",
      "Therefore, Equation (28) gives\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "E\n",
      "\n",
      "ξπ(t)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "e\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "\n",
      "n\n",
      "\n",
      "E\n",
      "\n",
      "E\n",
      "\n",
      "a=1\n",
      "X\n",
      "n\n",
      "\n",
      "Xb=1\n",
      "n\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17) (cid:18)\n",
      "\n",
      "π↓a,b\n",
      "\n",
      "c(nt\n",
      "\n",
      "a\n",
      "\n",
      "b + 2) + O\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "π↓a,b\n",
      "\n",
      "c(nt\n",
      "\n",
      "a\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "b + 2) + Oλ\n",
      "\n",
      ".\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "a=1\n",
      "X\n",
      "\n",
      "Xb=1\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)(cid:19)\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      "On the other hand, by the deﬁnition of mλ, we have\n",
      "n\n",
      "\n",
      "n\n",
      "\n",
      "b\n",
      "\n",
      "a\n",
      "\n",
      "mλ(t) =\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "π↓a,b\n",
      "(cid:16)\n",
      "\n",
      "π↓a,b\n",
      "(cid:16)\n",
      "\n",
      "E\n",
      "\n",
      "E\n",
      "\n",
      "E\n",
      "\n",
      "π↓a,b\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "a=1\n",
      "X\n",
      "n\n",
      "\n",
      "Xb=1\n",
      "n\n",
      "\n",
      "a=1\n",
      "X\n",
      "n\n",
      "\n",
      "Xb=1\n",
      "n\n",
      "\n",
      "a=1\n",
      "X\n",
      "\n",
      "Xb=1\n",
      "\n",
      "b\n",
      "(cid:17) Z\n",
      "−\n",
      "1\n",
      "\n",
      "1 Z\n",
      "a\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "−\n",
      "\n",
      "0 Z\n",
      "\n",
      "0\n",
      "\n",
      "(cid:17) Z\n",
      "\n",
      "c(nt\n",
      "\n",
      "a\n",
      "\n",
      "b + 2),\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "1(x + y\n",
      "\n",
      "nt) dx dy\n",
      "\n",
      "≤\n",
      "\n",
      "1(x + y\n",
      "\n",
      "nt\n",
      "\n",
      "a\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "≤\n",
      "\n",
      "b + 2) dx dy\n",
      "\n",
      "where 1 denotes the indicator function, and Equation (35) follows from simple changes of\n",
      "1. Combining Equations (31) and (36) ﬁnishes the\n",
      "variables x\n",
      "(cid:3)\n",
      "proof.\n",
      "\n",
      "1 and y\n",
      "\n",
      "x + a\n",
      "\n",
      "y + b\n",
      "\n",
      "7→\n",
      "\n",
      "7→\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "E\n",
      "\n",
      "In particular, Proposition 5.8 implies a convergence of expectations. If for each N , we\n",
      "[0, 2], the sequence\n",
      "\n",
      "have a random variable π(N )\n",
      "\n",
      "λ)), then for any ﬁxed t\n",
      "\n",
      "Unif(RP(N\n",
      "\n",
      "∞\n",
      "\n",
      "∼\n",
      "converges to mλ(t).\n",
      "\n",
      "⊙\n",
      "\n",
      "n\n",
      "\n",
      "N =1\n",
      "\n",
      "ξπ(N ) (t)\n",
      "The author of the present paper gives the following conjecture about this function mλ.\n",
      "(cid:16)\n",
      "e\n",
      "∈ Dn be a ﬁxed partition. Fix a positive real number ε > 0. Suppose\n",
      "λ)). Then,\n",
      "\n",
      "Conjecture 5.9. Let λ\n",
      "that π\n",
      "\n",
      "Unif(RP(N\n",
      "\n",
      "(cid:17)o\n",
      "\n",
      "∈\n",
      "\n",
      "∼\n",
      "\n",
      "⊙\n",
      "\n",
      "as N\n",
      "\n",
      ".\n",
      "→ ∞\n",
      "\n",
      "random permutations.\n",
      "\n",
      "P\n",
      "\n",
      "sup\n",
      "[0,2]\n",
      "\n",
      " \n",
      "t\n",
      "∈\n",
      "\n",
      "ξπ(t)\n",
      "\n",
      "−\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)e\n",
      "\n",
      "mλ(t)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "< ε\n",
      "\n",
      "1,\n",
      "\n",
      "! →\n",
      "\n",
      "19\n",
      "\n",
      "In the next subsection, we give a proof of Conjecture 5.9 in the special case of uniformly\n",
      "\n",
      "\f",
      "5.3. Limit shape for normalized cumulative X-rays of random permutations. Let\n",
      "N be a positive integer, and let π\n",
      "Unif(SN ) be a uniformly random permutation. For\n",
      "[N + 1], we let\n",
      "each k\n",
      "\n",
      "∼\n",
      "\n",
      "∈\n",
      "\n",
      "Let’s also deﬁne, for each k = 2, 3, . . . , 2N , the kth X-ray component\n",
      "\n",
      "Xk := ξπ(k).\n",
      "\n",
      "xk :=\n",
      "\n",
      "πij .\n",
      "\n",
      "[N ]\n",
      "Xi,j\n",
      "∈\n",
      "i+j=k\n",
      "\n",
      "Indeed, there is a simple relation between these notations: Xk = x2 + x3 +\n",
      "\n",
      "+ xk.\n",
      "\n",
      "The goal of this subsection is to prove the following result.\n",
      "\n",
      "· · ·\n",
      "\n",
      "Theorem 5.10. Let ε > 0 be any ﬁxed positive real number. Then,\n",
      "\n",
      "P\n",
      "\n",
      "k\n",
      "\n",
      "[N + 1],\n",
      "\n",
      "∀\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "∈\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "< εN\n",
      "\n",
      "= 1\n",
      "\n",
      "Oε\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "−\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "log N\n",
      "N log log N\n",
      "\n",
      ",\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "Xk −\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "as N\n",
      "\n",
      ".\n",
      "→ ∞\n",
      "\n",
      "As a corollary of Theorem 5.10, we obtain a proof of Conjecture 5.9 in the very special\n",
      "case when λ = (cid:3) is a partition with one box. In this case, the function mλ coincides with\n",
      "|[0,2]. From the deﬁnition of c, we see that the graph of this function is a concatenation of\n",
      "c\n",
      "two parabolas.\n",
      "Here is our rough strategy for proving the theorem. First, we show that with high\n",
      "probability the largest X-ray component maxk xk is small. Second, we give an upper bound\n",
      "on the size of the variance Var(Xk). Third, we argue that since the X-ray components are\n",
      "small with high probability, it suﬃces to establish the bound\n",
      "\n",
      "with high probability for all k simultaneously in a certain subset of [N + 1], instead of the\n",
      "whole [N + 1]. Fourth, we use Chebyshev’s tail bound to show that we have the bound\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "Xk −\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "< εN\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "Xk −\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "< εN\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "for all k in the mentioned subset of [N + 1] simultaneously with high probability. This\n",
      "ﬁnishes the proof.\n",
      "\n",
      "Now we begin the ﬁrst step of our strategy.\n",
      "\n",
      "Proposition 5.11. Let t\n",
      "\n",
      "N be a positive integer. We have\n",
      "\n",
      "≤\n",
      "\n",
      "P(xk ≥\n",
      "\n",
      "t)\n",
      "\n",
      "≤\n",
      "\n",
      "N + 1\n",
      "(t + 1)!\n",
      "\n",
      ".\n",
      "\n",
      "N +1\n",
      "\n",
      "Xk=2\n",
      "\n",
      "Proof. The event xk ≥\n",
      "of indices in [k\n",
      "\n",
      "1] such that\n",
      "\n",
      "−\n",
      "\n",
      "t is equivalent to the event that there exists a t-subset\n",
      "\n",
      "i1, i2, . . . , it}\n",
      "\n",
      "{\n",
      "\n",
      "πi1,k\n",
      "\n",
      "i1 = πi2,k\n",
      "\n",
      "i2 =\n",
      "\n",
      "= πit,k\n",
      "\n",
      "it = 1.\n",
      "\n",
      "−\n",
      "\n",
      "· · ·\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "Since\n",
      "\n",
      "(37)\n",
      "\n",
      "(38)\n",
      "\n",
      "we have\n",
      "\n",
      "(39)\n",
      "\n",
      "P(πi1,k\n",
      "\n",
      "−\n",
      "\n",
      "i1 = πi2,k\n",
      "\n",
      "i2 =\n",
      "\n",
      "−\n",
      "\n",
      "· · ·\n",
      "\n",
      "= πit,k\n",
      "\n",
      "it = 1) = E(πi1,k\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "i1 πi2,k\n",
      "1\n",
      "\n",
      "i2 · · ·\n",
      "\n",
      "−\n",
      "\n",
      "πit,k\n",
      "\n",
      "it )\n",
      "\n",
      "−\n",
      "\n",
      ",\n",
      "\n",
      "N (N\n",
      "\n",
      "1)\n",
      "\n",
      "(N\n",
      "\n",
      "t + 1)\n",
      "\n",
      "−\n",
      "\n",
      "· · ·\n",
      "\n",
      "−\n",
      "\n",
      "=\n",
      "\n",
      "P(xk ≥\n",
      "\n",
      "t)\n",
      "\n",
      "≤\n",
      "\n",
      "1\n",
      "t! ·\n",
      "\n",
      "(k\n",
      "−\n",
      "N (N\n",
      "20\n",
      "\n",
      "1)(k\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "· · ·\n",
      "\n",
      "2)\n",
      "(N\n",
      "\n",
      "· · ·\n",
      "\n",
      "(k\n",
      "\n",
      "t)\n",
      "−\n",
      "t + 1)\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "\f",
      "Therefore, by telescoping, we obtain\n",
      "\n",
      "N +1\n",
      "\n",
      "Xk=2\n",
      "\n",
      "P(xk ≥\n",
      "\n",
      "t)\n",
      "\n",
      "≤\n",
      "\n",
      "(k\n",
      "−\n",
      "N (N\n",
      "\n",
      "1)(k\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "· · ·\n",
      "\n",
      "2)\n",
      "(N\n",
      "\n",
      "· · ·\n",
      "\n",
      "(k\n",
      "\n",
      "t)\n",
      "−\n",
      "t + 1)\n",
      "\n",
      "N +1\n",
      "\n",
      "Xk=2\n",
      "1\n",
      "t!\n",
      "\n",
      "N +1\n",
      "\n",
      "k(k\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "Xk=2\n",
      "N + 1\n",
      "(t + 1)!\n",
      "\n",
      ",\n",
      "\n",
      "(40)\n",
      "\n",
      "(41)\n",
      "\n",
      "(42)\n",
      "\n",
      "as desired.\n",
      "\n",
      "By using the bound\n",
      "\n",
      "−\n",
      "\n",
      "1)\n",
      "\n",
      "(k\n",
      "· · ·\n",
      "(t + 1)\n",
      "\n",
      "t)\n",
      "−\n",
      "N (N\n",
      "\n",
      "(k\n",
      "\n",
      "1)(k\n",
      "\n",
      "−\n",
      "1)\n",
      "\n",
      "−\n",
      "(N\n",
      "\n",
      "−\n",
      "\n",
      "· · ·\n",
      "\n",
      "(k\n",
      "· · ·\n",
      "t + 1)\n",
      "\n",
      "2)\n",
      "\n",
      "−\n",
      "\n",
      "t\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "·\n",
      "\n",
      "(43)\n",
      "\n",
      "P(xk ≥\n",
      "together with Stirling’s formula, we obtain the following corollary.\n",
      "\n",
      "x2, x3, . . . , xN +1} ≥\n",
      "\n",
      "P(max\n",
      "{\n",
      "\n",
      "Xk=2\n",
      "\n",
      "≤\n",
      "\n",
      "t)\n",
      "\n",
      "t) ,\n",
      "\n",
      "N +1\n",
      "\n",
      "Corollary 5.12. For all suﬃciently large positive integers N , we have\n",
      "\n",
      "Next is the second step of the strategy. For each 2\n",
      "\n",
      "k\n",
      "\n",
      "N + 1, it is easy to see that\n",
      "\n",
      "max\n",
      "\n",
      "x2, x3, . . . , xN +1} ≥\n",
      "\n",
      "{\n",
      "\n",
      "3 log N\n",
      "log log N\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      ".\n",
      "\n",
      "≤\n",
      "\n",
      "P\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "E(Xk) = k(k\n",
      "\n",
      "1)\n",
      "2N .\n",
      "−\n",
      "Proposition 5.13. Let N\n",
      "\n",
      "Proof. Observe that\n",
      "\n",
      "(44)\n",
      "\n",
      "2 be a positive integer. For 2\n",
      "\n",
      "k\n",
      "\n",
      "N + 1, we have\n",
      "\n",
      "≥\n",
      "X 2\n",
      "k\n",
      "\n",
      "=\n",
      "\n",
      "E\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "k(k\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "+\n",
      "\n",
      "−\n",
      "\n",
      "1)(k\n",
      "−\n",
      "12N (N\n",
      "\n",
      "≤\n",
      "\n",
      "5)\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "≤\n",
      "2)(3k\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "k\n",
      "\n",
      "E\n",
      "\n",
      "X 2\n",
      "k\n",
      "\n",
      "=\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "Xℓ=2 Xi,j\n",
      "[N ]\n",
      "∈\n",
      "\n",
      "i+j=ℓ Xi\n",
      "\n",
      ",j\n",
      "∈\n",
      "′\n",
      "′\n",
      "+j\n",
      "i\n",
      "\n",
      "[N ]\n",
      "k\n",
      "\n",
      "′\n",
      "\n",
      "′\n",
      "\n",
      "≤\n",
      "\n",
      "E(πij πi′j′ ) .\n",
      "\n",
      "For the innermost summation on the right-hand side above, there are three cases. First,\n",
      "= (i, j) but (i′, j′) is on either\n",
      "2 such\n",
      "\n",
      "if (i′, j′) = (i, j), then E(πij πi′j′ ) = 1/N . Second, if (i′, j′)\n",
      "the same row or the same column as (i, j), then E(πij πi′j′ ) = 0. There are 2k\n",
      "ordered pairs. For the rest, the expectation E(πijπi′j′ ) is\n",
      "1) . Therefore,\n",
      "\n",
      "1\n",
      "N (N\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "ℓ\n",
      "\n",
      "(45)\n",
      "\n",
      "k\n",
      "\n",
      "E\n",
      "\n",
      "X 2\n",
      "k\n",
      "\n",
      "=\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "Xℓ=2 Xi,j\n",
      "[N ]\n",
      "∈\n",
      "i+j=ℓ\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "Simplify to ﬁnish.\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      "+\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "2\n",
      "\n",
      "−\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "2k + ℓ + 1\n",
      "\n",
      "−\n",
      "\n",
      "·\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "N (N\n",
      "\n",
      ".\n",
      "\n",
      "1)\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "−\n",
      "\n",
      "Proposition 5.14. Let N\n",
      "\n",
      "2 be a positive integer. For 2\n",
      "\n",
      "k\n",
      "\n",
      "N + 1, we have\n",
      "\n",
      "Var(Xk) <\n",
      "\n",
      "k2\n",
      "2N\n",
      "\n",
      ".\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "Proof. From Proposition 5.13 and some algebraic manipulation, we obtain\n",
      "\n",
      "(46)\n",
      "\n",
      "Var(Xk) = E\n",
      "\n",
      "X 2\n",
      "k\n",
      "\n",
      "(EXk)2 =\n",
      "\n",
      "It is not hard to see that\n",
      "\n",
      "(cid:0)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "2N −\n",
      "\n",
      "−\n",
      "\n",
      "k(k\n",
      "\n",
      "1)(4k\n",
      "\n",
      "5)\n",
      "\n",
      "−\n",
      "6N (N\n",
      "\n",
      "−\n",
      "1)\n",
      "\n",
      "+\n",
      "\n",
      "k2(k\n",
      "−\n",
      "4N 2(N\n",
      "\n",
      "1)2\n",
      "1)\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "k(k\n",
      "\n",
      "1)(4k\n",
      "\n",
      "5)\n",
      "\n",
      "−\n",
      "6N (N\n",
      "\n",
      "−\n",
      "1)\n",
      "\n",
      "≥\n",
      "\n",
      "k2(k\n",
      "−\n",
      "4N 2(N\n",
      "\n",
      "1)2\n",
      "1)\n",
      "\n",
      ",\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "whence Var(Xk)\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "2N < k2\n",
      "2N .\n",
      "\n",
      "−\n",
      "\n",
      "≤\n",
      "\n",
      "21\n",
      "\n",
      "≥\n",
      "\n",
      "−\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "6\n",
      "\f",
      "(47)\n",
      "\n",
      "(48)\n",
      "\n",
      "then\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "k′\n",
      "(cid:12)\n",
      "|\n",
      "\n",
      "−\n",
      "\n",
      "|\n",
      "\n",
      "(49)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(50)\n",
      "\n",
      "(51)\n",
      "\n",
      "We now turn to the third step of the described strategy. For each real number ε > 0, we\n",
      "\n",
      "let Eε denote the event\n",
      "\n",
      "k\n",
      "\n",
      "∀\n",
      "\n",
      "∈ {\n",
      "\n",
      "2, 3, . . . , N + 1\n",
      "\n",
      ",\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "< εN.\n",
      "\n",
      "For each positive integer g\n",
      "\n",
      "N + 1, we let Eg,ε denote the event\n",
      "\n",
      "≤\n",
      "\n",
      "gZ\n",
      "\n",
      "k\n",
      "\n",
      "∀\n",
      "\n",
      "∈\n",
      "\n",
      "∩\n",
      "\n",
      "[N + 1],\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "< εN.\n",
      "\n",
      "}\n",
      "\n",
      "Xk −\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Xk −\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Here, gZ\n",
      "\n",
      "∩\n",
      "\n",
      "[N + 1] refers to the set\n",
      "\n",
      "g, 2g, . . . ,\n",
      "\n",
      "N +1\n",
      "g\n",
      "\n",
      "g\n",
      "\n",
      ".\n",
      "\n",
      "j\n",
      "\n",
      "k\n",
      "\n",
      "o\n",
      "\n",
      "n\n",
      "\n",
      "Proposition 5.15. Let ε > 0 be a positive real number. For all suﬃciently large positive\n",
      "integers N , if\n",
      "\n",
      "g <\n",
      "\n",
      "ε\n",
      "8 ·\n",
      "\n",
      "N log log N\n",
      "log N\n",
      "\n",
      ",\n",
      "\n",
      "P(Eε)\n",
      "\n",
      "P\n",
      "\n",
      "Eg,ε/2\n",
      "\n",
      "≥\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "3 log N\n",
      "Proof. Let A denote the event that max\n",
      "log log N . By Corollary 5.12,\n",
      "it suﬃces to show that for all suﬃciently large positive integers N , we have the inclusion\n",
      "Eg,ε ⊆\n",
      "\n",
      "Eε ∪\n",
      "\n",
      "A.\n",
      "\n",
      "{\n",
      "\n",
      "′\n",
      "\n",
      "′\n",
      "\n",
      "(cid:0)\n",
      "(cid:1)\n",
      "x2, x3, . . . , xN +1} ≥\n",
      "\n",
      "Consider any event in Eg,ε/2 \\\n",
      "\n",
      "(k\n",
      "−\n",
      "2N\n",
      "log log N , for any ℓ. We claim that for any k\n",
      "\n",
      "A. In this case,\n",
      "\n",
      "−\n",
      "\n",
      "k\n",
      "\n",
      "1)\n",
      "\n",
      "< εN\n",
      "\n",
      "2 for any k′ divisible\n",
      ", we have\n",
      "\n",
      "2, 3, . . . , N + 1\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "∈ {\n",
      "(cid:12)\n",
      "\n",
      "}\n",
      "\n",
      "Xk′\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "by g, and xℓ < 3 log N\n",
      "1)\n",
      "Xk −\n",
      "\n",
      "< εN .\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "k(k\n",
      "\n",
      "Note that we can ﬁnd an integer k′\n",
      "\n",
      "2, 3, . . . , N + 1\n",
      "\n",
      "which is a multiple of g such that\n",
      "\n",
      "k\n",
      "\n",
      "< g. By the triangle inequality, we have\n",
      "\n",
      "∈ {\n",
      "\n",
      "}\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "Xk −\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "Xk −\n",
      "\n",
      "≤ |\n",
      "\n",
      "Xk′\n",
      "\n",
      "+\n",
      "\n",
      "Xk′\n",
      "\n",
      "|\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "+\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "k′(k′\n",
      "\n",
      "1)\n",
      "\n",
      "k′(k′\n",
      "\n",
      "1)\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "The ﬁrst term on the right-hand side is\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      ".\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Xk −\n",
      "\n",
      "|\n",
      "\n",
      "Xk′\n",
      "\n",
      "| ≤\n",
      "\n",
      "g\n",
      "\n",
      "·\n",
      "\n",
      "3 log N\n",
      "log log N\n",
      "\n",
      "<\n",
      "\n",
      "εN.\n",
      "\n",
      "The second term is less than εN\n",
      "\n",
      "2 . The third term is\n",
      "\n",
      "k′(k′\n",
      "\n",
      "1)\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "=\n",
      "\n",
      "(k′\n",
      "\n",
      "−\n",
      "\n",
      "k)(k′ + k\n",
      "2N\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "\n",
      "g\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "1\n",
      "8\n",
      "\n",
      "εN,\n",
      "\n",
      "for all suﬃciently large N . Therefore,\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "Xk −\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "for all suﬃciently large positive integers N . This shows that Eg,ε/2 \\\n",
      "(cid:12)\n",
      "proof.\n",
      "\n",
      "< εN,\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "A\n",
      "\n",
      "Eε, ﬁnishing the\n",
      "(cid:3)\n",
      "\n",
      "⊆\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "3\n",
      "8\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "We have arrived at the ﬁnal step of our strategy.\n",
      "\n",
      "22\n",
      "\n",
      "\f",
      "Proof of Theorem 5.10. Let Ec\n",
      "shev’s tail bound, we have, for all suﬃciently large positive integers N ,\n",
      "\n",
      "g,ε/2 denote the complement of the event Eg,ε/2. By Cheby-\n",
      "\n",
      "(52)\n",
      "\n",
      "P\n",
      "\n",
      "Ec\n",
      "\n",
      "g,ε/2\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "gZ\n",
      "Xk\n",
      "∩\n",
      "∈\n",
      "\n",
      "[N +1]\n",
      "\n",
      "P\n",
      "\n",
      "Xk −\n",
      "\n",
      "(cid:18)(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "Var(Xk)\n",
      "(cid:12)\n",
      "(εN/2)2\n",
      "\n",
      "k(k\n",
      "\n",
      "1)\n",
      "\n",
      "−\n",
      "2N\n",
      "\n",
      "εN\n",
      "2\n",
      "\n",
      "≥\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(53)\n",
      "\n",
      "(54)\n",
      "\n",
      "(55)\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "<\n",
      "\n",
      "[N +1]\n",
      "\n",
      "gZ\n",
      "Xk\n",
      "∩\n",
      "∈\n",
      "2\n",
      "ε2N 3\n",
      "\n",
      "[N +1]\n",
      "\n",
      "gZ\n",
      "Xk\n",
      "∩\n",
      "∈\n",
      "log N\n",
      "N log log N\n",
      "\n",
      ".\n",
      "\n",
      "3\n",
      "ε3 ·\n",
      "\n",
      "k2\n",
      "\n",
      "(by Proposition 5.14)\n",
      "\n",
      "Combining this with Proposition 5.15, we ﬁnish the proof.\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "I would like to thank Morris Ang, Alexei Borodin, Matthew Nicoletti, Alex Postnikov,\n",
      "I would like to thank\n",
      "Sahana Vasudevan, and Wijit Yangjit for insightful discussions.\n",
      "Alex Postnikov speciﬁcally for telling me about Proposition 4.6 and showing his proof to\n",
      "me, which led to the discussion of waterfalls in this paper. I am grateful for Alex Post-\n",
      "nikov and Alexei Borodin speciﬁcally for their encouragement. I would also like to thank\n",
      "Richard Kenyon for sharing with me a copy of the slides from his “permutons” talk. I would\n",
      "like to thank Sorawee Porncharoenwase for algorithmic insights and technical help. I used\n",
      "Polymake, R, Racket, and Wolfram Alpha to help with computations.\n",
      "\n",
      "References\n",
      "\n",
      "[AM14]\n",
      "\n",
      "[Bar21]\n",
      "\n",
      "[BF14]\n",
      "\n",
      "Mahshid Atapour and Neal Madras. Large deviations and ratio limit theorems for pattern-\n",
      "avoiding permutations. Combin. Probab. Comput., 23(2):161–200, 2014.\n",
      "Kenneth Barrese. A graph theory of rook placements. Electron. J. Combin., 28(4):Paper No.\n",
      "4.13, 26, 2021.\n",
      "Richard A. Brualdi and Eliseu Fritscher. Hankel and Toeplitz X-rays of permutations. Linear\n",
      "Algebra Appl., 449:350–380, 2014.\n",
      "\n",
      "[BLRS14] Kenneth Barrese, Nicholas Loehr, Jeﬀrey Remmel, and Bruce E. Sagan. m-level rook placements.\n",
      "\n",
      "J. Combin. Theory Ser. A, 124:130–165, 2014.\n",
      "\n",
      "[BLRS16] Kenneth Barrese, Nicholas Loehr, Jeﬀrey Remmel, and Bruce E. Sagan. Bijections on m-level\n",
      "\n",
      "rook placements. European J. Combin., 57:13–35, 2016.\n",
      "\n",
      "[BMPS05] Cecilia Bebeacua, Touﬁk Mansour, Alex Postnikov, and Simone Severini. On the X-rays of\n",
      "permutations. In Proceedings of the Workshop on Discrete Tomography and its Applications,\n",
      "volume 20 of Electron. Notes Discrete Math., pages 193–203. Elsevier Sci. B. V., Amsterdam,\n",
      "2005.\n",
      "Karen S. Briggs and Jeﬀrey B. Remmel. m-rook numbers and a generalization of a formula of\n",
      "Frobenius to Cm ≀ Sn. J. Combin. Theory Ser. A, 113(6):1138–1171, 2006.\n",
      "\n",
      "[BR06]\n",
      "\n",
      "[GGKK15] Roman Glebov, Andrzej Grzesik, Tereza Klimoˇsov´a, and Daniel Kr´al’. Finitely forcible graphons\n",
      "\n",
      "and permutons. J. Combin. Theory Ser. B, 110:112–135, 2015.\n",
      "\n",
      "[GX06]\n",
      "\n",
      "[GHK+17] Roman Glebov, Carlos Hoppen, Tereza Klimoˇsov´a, Yoshiharu Kohayakawa, Daniel Kr´al’, and\n",
      "Hong Liu. Densities in large permutations and parameter testing. European J. Combin., 60:89–\n",
      "99, 2017.\n",
      "Ira M. Gessel and Guoce Xin. The generating function of ternary trees and continued fractions.\n",
      "Electron. J. Combin., 13(1):Research Paper 53, 48, 2006.\n",
      "Gabor T. Herman and Attila Kuba, editors. Discrete tomography. Applied and Numerical Har-\n",
      "monic Analysis. Birkh¨auser Boston, Inc., Boston, MA, 1999. Foundations, algorithms, and ap-\n",
      "plications.\n",
      "\n",
      "[HK99]\n",
      "\n",
      "[HKM+13] Carlos Hoppen, Yoshiharu Kohayakawa, Carlos Gustavo Moreira, Bal´azs R´ath, and Rudini\n",
      "Menezes Sampaio. Limits of permutation sequences. J. Combin. Theory Ser. B, 103(1):93–113,\n",
      "2013.\n",
      "\n",
      "[KKRW20] Richard Kenyon, Daniel Kr´aˇl, Charles Radin, and Peter Winkler. Permutations with ﬁxed pat-\n",
      "\n",
      "tern densities. Random Structures Algorithms, 56(1):220–250, 2020.\n",
      "\n",
      "23\n",
      "\n",
      "\f",
      "[MV07]\n",
      "\n",
      "[Nor08]\n",
      "[OEI]\n",
      "\n",
      "[Pos09]\n",
      "\n",
      "[Rio02]\n",
      "\n",
      "[Sta99]\n",
      "\n",
      "[Sta12]\n",
      "\n",
      "(2022), The On-Line Encyclopedia\n",
      "\n",
      "Hugh L. Montgomery and Robert C. Vaughan. Multiplicative number theory. I. Classical the-\n",
      "ory, volume 97 of Cambridge Studies in Advanced Mathematics. Cambridge University Press,\n",
      "Cambridge, 2007.\n",
      "Gustav Nordh. Perfect Skolem sets. Discrete Math., 308(9):1653–1664, 2008.\n",
      "OEIS Foundation Inc.\n",
      "https://oeis.org/.\n",
      "Alexander Postnikov. Permutohedra, associahedra, and beyond. Int. Math. Res. Not. IMRN,\n",
      "(6):1026–1106, 2009.\n",
      "John Riordan. An introduction to combinatorial analysis. Dover Publications, Inc., Mineola,\n",
      "NY, 2002.\n",
      "Richard P. Stanley. Enumerative combinatorics. Vol. 2, volume 62 of Cambridge Studies in\n",
      "Advanced Mathematics. Cambridge University Press, Cambridge, 1999. With a foreword by\n",
      "Gian-Carlo Rota and appendix 1 by Sergey Fomin.\n",
      "Richard P. Stanley. Enumerative combinatorics. Volume 1, volume 49 of Cambridge Studies in\n",
      "Advanced Mathematics. Cambridge University Press, Cambridge, second edition, 2012.\n",
      "\n",
      "Integer Sequences.\n",
      "\n",
      "of\n",
      "\n",
      "Department of Mathematics, Massachusetts Institute of Technology, Cambridge, MA 02139\n",
      "Email address, P. Jiradilok: pakawut@mit.edu\n",
      "\n",
      "24\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "\n",
    "print(array_authors[1])\n",
    "print(array_pdf_text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1220700",
   "metadata": {},
   "source": [
    "### Extract all word after the term: \"References\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e6468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n6\\n1\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nSimplicial Embeddings in Self-Supervised Learning\\nand Downstream Classiﬁcation\\n\\nSamuel Lavoie(cid:5)†, Christos Tsirigotis(cid:5)†, Max Schwarzer(cid:5)†, Kenji Kawaguchi‡, Ankit Vani(cid:5)†,\\nAaron Courville(cid:5)†♣\\n(cid:5) Mila, † Université de Montréal, ‡ National University of Singapore, ♣ CIFAR Fellow\\n{samuel.lavoie.m,aaron.courville}@gmail.com\\n{christos.tsirigotis,max.schwarzer,ankit.vani}@umontreal.ca\\nkenji@comp.nus.edu.sg\\n\\nAbstract\\n\\nWe introduce Simplicial Embeddings (SEMs) as a way to constrain the encoded\\nrepresentations of a self-supervised model to L simplices of V dimensions each\\nusing a Softmax operation. This procedure imposes a structure on the represen-\\ntations that reduce their expressivity for training downstream classiﬁers, which\\nhelps them generalize better. Speciﬁcally, we show that the temperature τ of the\\nSoftmax operation controls for the SEM representation’s expressivity, allowing us\\nto derive a tighter downstream classiﬁer generalization bound than that for classi-\\nﬁers using unnormalized representations. We empirically demonstrate that SEMs\\nconsiderably improve generalization on natural image datasets such as CIFAR-100\\nand ImageNet. Finally, we also present evidence of the emergence of semantically\\nrelevant features in SEMs, a pattern that is absent from baseline self-supervised\\nmodels.\\n\\n1\\n\\nIntroduction\\n\\nSelf-supervised learning (SSL) is an emerging family of methods that aims to learn an embedding of\\nthe data without manual supervision, such as class labels. Those methods embed the data in some\\nrepresentations that render themselves amenable to ﬁtting a linear classiﬁer, as demonstrated in Hjelm\\net al. [2019]; Grill et al. [2020]; Saeed et al. [2020]; You et al. [2020]. This observation demonstrates\\nthat the representation learned by those SSL methods encodes the semantic content necessary to learn\\na classiﬁer as a linear combination of the features.\\n\\nIn this work, we propose to embed the latent representation of the data into L simplices of V\\ndimensions each by using a Softmax operation. We refer to the normalized embeddings as Simplicial\\nEmbeddings (SEMs) due to the geometrical structure of the representation induced by the Softmax.\\nThe SEMs have an effect both while training the representation and on the training of the downstream\\nclassiﬁer. For the former, the SEM is an inductive bias to ﬁt the data in a more constrained space\\nthat may lead to a simpler representation. For the latter, the Softmax allows us to control for the\\nexpressivity of the representation. This control gives us a better generalization bound for training\\ndownstream classiﬁers.\\n\\nWe demonstrate that the proposed SEMs improve the generalization of downstream classiﬁers trained\\nwith BYOL [Grill et al., 2020] and MoCo [He et al., 2020] on CIFAR-100 and ImageNet. We also\\nshow an improvement in transfer learning and robustness to out-of-distribution datasets. Finally,\\nwe present evidence that individual features of the SEMs encode semantical content related to\\nour intuitive notion of the semantics in CIFAR-100. In contrast, we argue that the baseline SSL\\nmethods may learn the semantics related to the classes as a linear combination of the features in the\\nrepresentation but not at the individual features’ level.\\n\\nPreprint. Under review.\\n\\n\\x0cConcretely, this work makes the following contributions:\\n\\n1. Propose the Simplicial Embeddings.\\n\\n2. Derive a generalization bound for downstream classiﬁers trained on the Simplicial Embed-\\n\\n3. Empirically studies the Simplicial Embeddings and its effect on the generalization of\\n\\ndings.\\n\\ndownstream classiﬁers.\\n\\n1.1 Related works\\n\\nThe use of Softmax as an inductive bias has been studied in other contexts, notably as an architectural\\ncomponent for models to attend to context-dependent queries via, for example, attention mecha-\\nnisms [Bahdanau et al., 2016; Vaswani et al., 2017] or memory augmented networks [Graves et al.,\\n2014]. Different from these, our method places the Softmax at the output of an encoder to constrain\\nthe representation and to allow control of the expressivity of the representation for downstream\\nclassiﬁers.\\n\\nOur work builds on top of the literature on self-supervised learning. Notably, we demonstrate the effect\\nof the SEM on contrastive approaches using the noise contrastive estimation (NCE) objective [Hjelm\\net al., 2019; Chen et al., 2020b] with memory banks [He et al., 2020] and on the bootstrapping\\napproaches [Grill et al., 2020; Chen and He, 2020]. Related, some works explicitly induce clustering\\nof the representation [Caron et al., 2019; Ym et al., 2019; Caron et al., 2020]. Contrary to these\\nworks, we do not explicitly induce clustering on the representation.\\n\\nIn the realm of improving the generalization of SSL methods, Wang et al. [2021] propose a method\\nto iteratively select a partition of the data and use this partition to minimize an IRM regularizer\\n[Arjovsky et al., 2020] with an SSL objective. Lee et al. [2021] present an objective to minimize\\nthe conditional entropy bottleneck. Contrary to these works, our methods do not require additional\\nobjectives as it is merely an inductive bias in the SSL models.\\n\\n2 Background on self-supervised learning\\n\\nModels trained with a contrastive objective learn to embed samples x ∈ X into representations\\nz ∈ Z, where Z is a bounded metric space. The aim is to both minimize the distance between\\nthe representation of a sample zi = fθ(xi) : x ∈ X and the representation of a positive sample\\nzj = fθ(xj), and to maximize the distance between zi and the representation of negative samples\\nfθ(x(cid:48)) : x(cid:48) ∈ X \\\\ xi. While the positive samples are typically augmented samples of xi, other\\nstrategies can be decided, such as choosing samples from the same labelled category [Khosla et al.,\\n2020]. A common contrastive objective is Noise Contrastive Estimation (NCE) [Hjelm et al., 2019;\\nChen et al., 2020b], which is deﬁned as\\n\\nLnce := − log\\n\\nexp(d(zi, zj)/t)\\n¯x∈X \\\\x exp(d(zi, ¯z)/t)\\n\\n,\\n\\n(cid:80)\\n\\n(1)\\n\\nwhere d is often taken to be the cosine similarity: d(x, y) := x(cid:62)y/(cid:107)x(cid:107)2(cid:107)y(cid:107)2 and t > 0 is a\\nhyper-parameter that denotes a temperature.\\n\\nUnlike most contrastive methods, BYOL [Grill et al., 2020] does not require negative samples.\\nInstead, it introduces a target network in which the parameters ξ are taken as an exponential moving\\naverage of the embedding function parameters, θ. More precisely, ξ ← αξ + (1 − α)θ, with\\nα ∈ [0, 1]. The authors deﬁne the anchor and positive samples as zθ = fθ(t(x)) and zξ = fξ(t(cid:48)((x))\\nrespectively, where t, t(cid:48) ∼ T are augmentations sampled from a set of possible augmentations deﬁned\\nby the practitioner. To prevent degenerate solutions, they re-normalize the representation using batch\\nnormalization [Ioffe and Szegedy, 2015], and utilize a stop-gradient operation on zξ that prevents the\\ngradient from back-propagating through the target network. They also introduce a prediction head\\nthat maps the representation to a prediction: zθ (cid:55)→ qθ. The BYOL objective is deﬁned as\\n\\nLbyol := 2 − 2 · d(qθ, zξ),\\n\\n(2)\\n\\nwhere d is chosen to be the cosine similarity.\\n\\n2\\n\\n\\x0c(a)\\n\\n(b)\\n\\nFigure 1: (a) Illustration of the proposed Simplicial Embeddings (SEM). στ represents the Softmax\\noperation with τ . We assume that z decomposes into L vectors in RV . (b) Histogram of the entropies\\nH(¯z(x)\\n) at the end of the pre-training phase, for a given temperature τ , of each simplex for each\\ni\\ntraining sample in CIFAR-100. (c) Integration of the SEM with BYOL [Grill et al., 2020].\\n\\n3 Simplcial Embeddings\\n\\nWe illustrate the proposed Simplicial Embeddings (SEMs) in Figure 1a. An encoder embeds a sample\\nx into a L × V representation z. A temperature parameter τ then scales the logits z ∈ RL×V before\\nre-normalizing each row via L independent Softmax operations. Then, the normalized vectors are\\nconcatenated to produce ¯z ∈ RLV . Concretely, the logits are re-normalized as follows:\\n\\n¯zi := [στ (zi1),\\n\\n. . . , στ (ziV )], στ (zij) =\\n\\n, ¯z := Concat(¯z1, . . . , ¯zL),\\n\\n(3)\\n\\nezij /τ\\nk=1 ezik/τ\\n\\n(cid:80)V\\n\\nfor all i ∈ [L] and j ∈ [V ].\\n\\nThe SEMs can be integrated easily into a NCE model [Hjelm et al., 2019; Chen et al., 2020b] or\\nBYOL [Grill et al., 2020]. We insert it after the encoder and before the projector in our experiments.\\nFigure 1c depicts how we use the SEMs in BYOL. The embedding ¯z is passed into the projector\\nmodule, which we deﬁne as a linear layer or a small MLP. Beyond this small modiﬁcation, the SSL\\nmethod considered remains unchanged.\\n\\n3.1\\n\\nInductive bias of the SEMs\\n\\ni\\n\\nj=1 p(¯z(x)\\n\\n) where (cid:80)V\\n\\nij ) = 1 and p(¯z(x)\\n\\nij ) ≥ 0 ∀j. Here, ¯z(x)\\n\\nWe now describe at a high level the inductive bias of the SEMs during the self-supervised learning\\nphase. We note that each simplex can be interpreted as representing a probability mass function\\np(¯z(x)\\nrepresents the simplex i for a\\ni\\nsample x. The simplex puts a constrain on how the its elements may organize: they may interpolate\\nbetween being a sparse vector and being a constant vector. The state of a simplex can be quantiﬁed\\n) that we denote as follows: H(¯zi) := − (cid:80)V\\nusing the entropy of p(¯z(x)\\nij ). That\\nis, if H(¯z(x)\\nWhile we may argue that the temperature parameter τ , which merely induces a scaling of the logit,\\nmay be subsumed during training, we demonstrate in Figure 1b that this temperature is an important\\ninitial condition for determining the state to which the simplex will converge. Here, we plot the\\nhistogram of the entropies H(¯z(x)\\n), for a given τ , of each simplex for each sample x in the training\\nset of CIFAR-100. The temperature parameter dictates in which state the representation will converge:\\na small τ will induce a sparse representation, and a large τ will induce a constant representation.\\n\\nj=1 p(¯z(x)\\n) = log(V ) then the vector is constant.\\n\\n) = 0 then the vector is sparse and if H(¯z(x)\\n\\nij ) log p(¯z(x)\\n\\ni\\n\\ni\\n\\ni\\n\\ni\\n\\n(c)\\n\\n3\\n\\nEncoderConcat(0.3, 0.3, 0.4)(0.1, 0.7, 0.2)(0.1, 0.1, 0.8)SEM......0.00.51.01.52.02.5Entropy H(z(x)i)0123456: 0.01: 0.1: 0.5: 1.0: 10.0EncoderEncoderSEMSEMPredictionProjectionSSL  lossstop gradProjection\\x0cInterestingly, for an intermediate temperature, the distribution of entropies is more spread out, rather\\nthan having the same variance smoothly translated toward the center of the histogram.\\n\\nThe above observation gives an intuition about how the induce a bias of the SEMs on the learned\\nrepresentation during SSL. Besides the qualitative properties of the vectors that the SEM may induce,\\nthis embedding has a particular structure that we may leverage for learning a classiﬁer with a better\\ngeneralization bound. Next, we theoretically present how the SEMs allow for a better generalization\\nof a downstream classiﬁer and derive a generalization bound for the classiﬁers trained on such\\nrepresentation.\\n\\n3.2 Theoretical bound on the downstream classiﬁer\\n\\nIn this subsection, we mathematically analyze the SEM to understand its beneﬁt and the effect of\\nthe hyper-parameter τ . We show that: (1) there is a trade-off between the training loss and the\\ngeneralization gap, which is controlled by the value of τ , and (2) the SEM can improve the base\\nmodel performance when we attain good balance in this trade-off.\\n\\nLet g represent the layer(s) after the normalization. With this notation, we can deﬁne a baseline\\nmodel without normalization as fbase(z) = g(z) and the corresponding model with normalization\\nas fSEM(τ )(z) = (g ◦ στ )(z). We consider a training dataset S = (zi, yi)n\\ni=1 of n samples that is\\nused for supervised training of a classiﬁer using the representations z, which are extracted from the\\nself-supervised encoder. To understand the quality of the ﬁnal model after supervised training of\\nthe classiﬁer, we analyze the generalization gap Ez,y[l(f (z), y)] − 1\\ni=1 l(f (z(i)), y(i)) for each\\nn\\nbase}, where l : R × Y → R≥0 is the per-sample loss.\\nf ∈ {f S\\n\\nSEM(τ ), f S\\n\\n(cid:80)n\\n\\nTo simplify the notation, we consider the normalization to [−1, +1]; i.e., z ∈ Z = [−1, +1]L×V .\\nWe assume that there exists ∆ > 0 such that for any i ∈ [L], if k = arg maxj∈[V ] zij, then\\nzik ≥ zij + ∆ for any j (cid:54)= k. Since ∆ can be arbitrarily small (e.g., much smaller than machine\\nprecision), this assumption typically holds in practice. Next, we deﬁne B to be the upper bound\\non the per-sample loss such that l(f (z), y) ≤ B for all f ∈ H and for all (z, y) ∈ Z × Y,\\nwhere H is the union of the hypothesis spaces of fSEM(τ ) and fbase. For example, B = 1\\nfor the 0-1 loss. We also use Qi = {q ∈ [−1, +1]V : i = arg maxj∈[V ] qj}, with the fol-\\nlowing two deﬁnitions: ϕ(f S\\n2, and ϕ(f S\\nSEM(τ )) =\\nsupi∈[V ] supq,q(cid:48)∈Qi\\nt=1 eqt/τ for j = 1, . . . , V . Next,\\nwe deﬁne GS to be the set of g returned by the training algorithm using dataset S, and R to be the\\nLipschitz constant of ly ◦ g for all y ∈ Y and g ∈ GS; i.e., |(ly ◦ g)(z) − (ly ◦ g)(z(cid:48))| ≤ R(cid:107)z − z(cid:48)(cid:107)F ,\\nwhere ly(q) = l(q, y). Finally, let c > 0 be a universal constant in (n, f, H, δ, H, τ, S).\\n\\n2, where στ (q)j = eqj /τ\\n\\nbase) = supi∈[V ] supq,q(cid:48)∈Qi\\n\\nt=1 (cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n\\nt=1 (cid:107)q − q(cid:48)(cid:107)2\\n\\n(cid:80)n\\n\\n(cid:80)n\\n\\n(cid:80)V\\n\\nUsing the established notation, Theorem 1 illuminates the advantage of the SEM and the effect of the\\nhyper-parameter τ on the performance of the downstream classiﬁer:\\nTheorem 1. Let V ≥ 2. For any δ > 0, with probability at least 1 − δ, the following holds for any\\nfS ∈ {f S\\n\\nSEM(τ ), f S\\n\\nbase}:\\n\\nEz,y[l(fS(z), y)] ≤\\n\\nl(fS(z(i)), y(i)) + R\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:114)\\n\\n(cid:114)\\n\\nLϕ(fS)\\nn\\n\\n+ c\\n\\nln(2/δ)\\nn\\n\\n.\\n\\nMoreover,\\n\\nϕ(f S\\n\\nSEM(τ )) → 0 as τ → 0\\n\\nand ϕ(f S\\n\\nSEM(τ )) − ϕ(f S\\n\\nbase) ≤\\n\\n(1 − V ) < 0 ∀τ > 0.\\n\\n3n\\n4\\n\\nThe ﬁrst statement of Theorem 1 shows that the expected loss is bounded by the three terms: training\\nloss 1\\nn\\n\\ni=1 l(fS(z(i)), y(i)), the second term R\\n\\n(cid:113) Lϕ(fS )\\nn\\n\\n(cid:113) ln(2/δ)\\nn\\n\\n. Since c is\\n\\n(cid:80)n\\n\\na universal constant in (n, f, H, δ, H, τ, S), the third term c\\ngoes to zero as n → ∞ and is\\nthe same for both models with and without soft-discretization. Thus, for the purpose of comparing the\\nmodels with and without soft-discretization, we can focus on the second term, where the difference\\narises.\\n\\n, and the third term c\\n(cid:113) ln(2/δ)\\nn\\n\\nTheorem 1 shows that the second term R\\nSEM(τ )) →\\n0 as τ → 0. Also, for any τ > 0, the second term with soft-discretization is strictly smaller than\\n\\ngoes to zero with the SEM; i.e., ϕ(f S\\n\\n(cid:113) Lϕ(fS )\\nn\\n\\n4\\n\\n\\x0cthat without soft-discretization as ϕ(f S\\nimprovement due to soft-discretization is expected to be higher as V increases.\\n\\nSEM(τ )) − ϕ(f S\\n\\nbase) ≤ 3n\\n\\n4 (1 − V ) < 0. This shows that the\\n\\nOverall, Theorem 1 shows the beneﬁt of the SEM as well as the trade-off with τ . When τ → 0, the\\nsecond term goes to zero, but the training loss (the ﬁrst term) can increase due to the reduction in\\nexpressivity and increased difﬁculty in optimization. Thus, we assert that the best τ is the one that\\nbalances this trade-off.\\n\\n4 Experiments\\n\\nWe study the effect of the Simplicial Embeddings on the generalization of self-supervised learn-\\ning methods1. We demonstrate that the Simplicial Embeddings improve the test set accuracy on\\nCIFAR-100 and ImageNet. On CIFAR-100, we also study the different properties of the SEM, and\\nwe demonstrate the emergence of semantic features relevant to the classes in the representation\\nfeature. On ImageNet, we show that the Simplicial Embeddings improve the test accuracy on several\\nrobustness test sets and the accuracy on transfer learning datasets.\\n\\n4.1 The effect of the SEM on downstream classiﬁcation\\n\\nMethod\\nSimCLR†\\nMOCO†\\nSWAV†\\nDINO†\\nBYOL\\nBYOL + SEM\\n\\nAccuracy\\n65.78\\n69.89\\n64.88\\n66.76\\n70.46\\n74.36\\n\\nMethod\\nSimCLR‡\\nBYOL\\nBYOL*\\nBYOL + SEM\\n\\nAccuracy\\n68.73\\n74.28\\n73.33\\n77.05\\n\\nMethod\\nSIMCLR‡\\nMOCO‡\\nMOCO + SEM\\nSIMSIAM(cid:5)\\nBYOL(cid:5)\\nBYOL + SEM\\n\\nAccuracy\\n63.1\\n67.3\\n69.0\\n70.0\\n70.6\\n72.8\\n\\n(a) CIFAR-100 on ResNet18\\n\\n(b) CIFAR-100 on ResNet50\\n\\n(c) ImageNet on ResNet50\\n\\nFigure 2: Accuracy on (a) CIFAR-100 with ResNet18 for 1000 epochs. (b) CIFAR-100 with ResNet50\\nfor 1000 epochs. (c) ImageNet with ResNet50 for 200 epochs. *Denotes the accuracy obtained when\\ntraining BYOL with a representation the same size as SEM. † Results taken from da Costa et al.\\n[2021]. ‡ Results taken from Wang et al. [2021]. (cid:5) Results taken from [Chen et al., 2020b]. Boldface\\nindicates highest accuracy. Green rows indicate a SSL method + SEM.\\n\\nComparison study. We ﬁrst compare the effect of using the SEM in a BYOL model with related\\nSSL approaches in the literature. We take a standard BYOL model, as implemented in the Solo-Learn\\nlibrary [da Costa et al., 2021], and implement the Simplicial Embeddings after the encoder. We test\\nour approach with a ResNet18 and ResNet50 on CIFAR-100 and with a ResNet50 for ImageNet [He\\net al., 2015]. Our models are trained with Stochastic Gradient Descent [Bottou et al., 2018] with a\\ncosine decay scheduler on the learning rate, as done in previous works [Grill et al., 2020; Chen et al.,\\n2020b]. We use a batch size of 256 for all of our models and train on a single A100 GPU. We selected\\nthe parameters of the SEM by performing a grid search over several values using a validation set and\\nre-trained our model using all the training data to evaluate the test set. We did not modify the default\\nhyper-parameters of the method, demonstrating that the gain in accuracy is a product of the SEM. We\\npresent the hyper-parameters used in the Appendix. We evaluate all of our models by training a linear\\nclassiﬁer, using the training data on top of the learned representations as it is typically done.\\n\\nWe compare our approach on CIFAR-100 and ImageNet in Table 2b and Table 2c respectively.\\nCompared with prior models, our approach improves the baseline methods by a considerable margin.\\nOn CIFAR-100, we compare with several baselines, such as DINO and SwaV. We also trained BYOL\\nwith the same representation size as what we used in the SEM, without the embedding, and observed a\\nmarginal performance decrease. As demonstrated in Zbontar et al. [2021], BYOL does not seemingly\\nbeneﬁt from large representations.\\n\\nThe SEM also presents a noticeable improvement compared to the baselines on ImageNet when\\ntrained for 200 epochs. Here, we trained our model on both BYOL and MOCO [He et al., 2020] to\\ndemonstrate that the effect of the SEM is not limited to BYOL.\\n\\n1We provide the code to reproduce the experiments: https://github.com/lavoiems/simplicial-embeddings\\n\\n5\\n\\n\\x0cFigure 3: Study of the effect of the parameters of the SEM. We plot the accuracy obtained for several\\ndownstream classiﬁcation SEM’s temperature (fSEM(τ )) and without SEM (fbase) described in the\\nlegend. We performed the training with a ResNet18 on CIFAR-100. Interpolation of Left: τ during\\nthe training of the SSL model. Middle: V . Right: L\\n\\nStudy of the SEM parameters. We study the effect of each of the parameters of the SEM and\\nevaluate their effect on the validation accuracy in Figure 3. We trained each model with a ResNet18\\non CIFAR-100 using the BYOL training procedure. We keep the other parameters constant to their\\ndefault value for each parameter that we study. The default value of τ is 1, V is 13 and L is 5000. For\\neach pre-trained SSL model, we trained 5 downstream classiﬁers, one on the unnormalized features\\ndenoted fbase and one on the normalized features for τ ∈ {0.01, 0.1, 1, 10}.\\n\\nWe observe that the temperature used to normalized the embedding before training the downstream\\nclassiﬁer, fSEM(τ ), is important for the downstream classiﬁcation and is generally better than training\\na classiﬁer on the unnormalized features (fbase) as predicted in Theorem 1. We observe the trade-off,\\nas presented in Section 3.2, between having a small and a large τ .\\n\\nWe also observe a trade-off between having a large and a small temperature when training the SSL\\nmodel. As demonstrated in Figure 1b, the temperature parameter has an impact on whether the\\nsimplicies will represent a sparse or a constant vector. We demonstrated that a small temperature\\nyields a set of sparse vectors while a large temperature yields a constant vector. Here, we observe\\nthat the temperature yielding the better validation accuracy offers a trade-off between a sparse and a\\nconstant vector. We hypothesize that a sparse vector leads to harder training but a smaller expressivity.\\nThus, the better temperature during the training of the SSL model is the one that offers a trade-off\\nbetween a sparse but trainable representation.\\n\\nIn Theorem 1, we demonstrated theoretically that the second term was more sensitive to the tem-\\nperature as V increased. This prediction is empirically veriﬁed in Figure 3 where we evaluate the\\nvalidation accuracy for several V . As V increases, the validation accuracy drops for larger τ s. For\\nexample, the validation accuracy drops when interpolating between V = 13 and V = 34 for τ = 10,\\nstays constant for τ = 1 and increases for the smaller temperatures.\\n\\nFinally, we interpolate the L parameter and demonstrate that larger L yields increased normalized\\nfeatures’ validation accuracy. As expected, the effect of ϕ(fS) grows with larger L, and thus we\\nwould expect a bigger difference between fbase and fSEM(τ ). This demonstrates empirically and\\ntheoretically that the SEM may scale the representation of SSL methods to a larger representation\\nand thus potentially increasing the scaling capability of these methods.\\n\\n4.2 Emergence of semantically relevant features\\n\\nIn this subsection, we investigate the semantic content held by the most predictive features of an\\nembedding. To make this study, we consider an encoder pretrained on CIFAR-100, using BYOL with\\nand without SEM, and a downstream linear classiﬁer trained on the embedding of the CIFAR-100\\nsamples. Consider the trained linear classiﬁer with a weight matrix W ∈ RN ×C, where N denotes\\nthe number of features, and C denotes the number of classes. This classiﬁer is trained by minimizing\\nthe cross-entropy loss between the predicted class and the given label.\\n\\nHere, we study the semantic relevance of the top K features for each class. Consider the weight\\nmatrix W . By preserving the top K parameters of this weight matrix for each class and pruning\\nthe features predictive for only one class, we create a bipartite graph between two set of nodes: the\\n\\n6\\n\\nfbasefSEM(=0.01)fSEM(=0.1)fSEM(=1)fSEM(=10)0.010.10.5110SSL 506070Valid accuracy23581334V62.565.067.570.072.5Valid accuracy5010050010005000L62.565.067.570.072.5Valid accuracy\\x0c(a)\\n\\n(b)\\n\\nFigure 4: Semantic relevance of the features. (a) Subset of WK, the bipartite graph of the most\\nimportant features shared between at least two classes of a classiﬁer trained on BYOL + SEMs\\nfeatures. The connected components emerge without additional interventions. (b) Relevance of the\\ntop K features to the semantics of the super-class of the categories of CIFAR-100. It is taken as the\\nnumber of pairwise categories in the same super-class for which a feature is among its top K most\\npredictive features over the total number of pairwise categories.\\n\\ncategories and the features. We denote this graph WK. We plot a subset W5, obtained when taking\\nthe top 5 features for each class, on the SEM representations in Figure 4a and the full bipartite graph\\non the SEM and the one obtained when applying the procedure on the representation obtained with\\nan unnormalized BYOL in the Appendix. In the graph obtained with the SEM, we observe that a\\nset of connected components emerge, and the connected components of the graph are semantically\\nrelated. For example, the ﬁrst set of connected components are fruits and vegetables, and the second\\nset of connected components are aquatic mammals. The same observation does not occur when this\\nexperiment is performed on the baseline BYOL and BYOL, with a large representation model. In\\nparticular, we do not see a small number of semantically related connected components. Instead, we\\nsee a large fully connected graphs. This observation suggests that the features learned by the baseline\\nmodel do not hold the same amount of semantic information. Instead, the semantic information could\\nbe encoded as a linear combination of several features, for example.\\n\\nWe also study more quantitatively the semantic relevance of the features in CIFAR-100. Two\\ncategories share a predictive feature on WK if they are 2-neighbour, that is they share a common\\npredictive feature. Let N (ci) returns all pairs (ci, cj) for all j 2-neighbour of ci. Moreover, deﬁne\\nthe operation is_super(ci, cj) which returns 1 if ci and cj are from the same CIFAR-100 superclass\\nand 0 otherwise. We reproduce the superclass of CIFAR-100 in Table 5. We deﬁne the semantic\\nrelevance as follows:\\n\\nRelevance(WK) :=\\n\\n(cid:80)\\n\\nC\\n(cid:88)\\n\\ni=1\\n\\n(ci,cj )∈N (ci) is_super(ci, cj)\\n|N (ci)|\\n\\n,\\n\\n(4)\\n\\nwhere C = 100 for CIFAR-100 and | · | is the cardinality of a set.\\n\\nWe compare the semantic relevance of BYOL+SEM with the control experiments BYOL and BYOL\\nwith a representation of the same size as BYOL+SEM but without the normalization. We observe\\nthat using the SEM yields more semantically relevant features than the baseline. This observation is\\nconsistent with the qualitative experiments presented earlier and indicates that the semantics encoded\\n\\nBYOL\\n\\nIN\\n68.3\\nBYOL + SEM 70.6\\n66.7\\nMOCO + SEM 68.0\\n\\nMOCO\\n\\nIN-V2\\n55.3\\n57.9\\n53.4\\n55.0\\n\\n100%\\nIN-R\\n16.5\\n18.1\\n14.0\\n15.21\\n\\nIN-A IN-C\\n35.4\\n0.68\\n38.9\\n0.77\\n0.69\\n31.1\\n33.8\\n0.61\\n\\nIN\\n46.8\\n47.9\\n43.5\\n44.1\\n\\nIN-V2\\n37.5\\n38.5\\n34.2\\n35.9\\n\\n1%\\nIN-R IN-A IN-C\\n0.71\\n12.2\\n25.0\\n25.3\\n12.2\\n0.65\\n0.51\\n20.1\\n8.7\\n21.4\\n0.51\\n9.1\\n\\nTable 1: Test accuracies of a linear probe trained with 100% and 1% of the IMAGENET samples on\\na pre-trained representation trained for 100 epochs. Boldface indicates the maximal value for each\\nevaluation set and each base model type (BYOL or MoCo).\\n\\n7\\n\\norangeappleS190sweet_pepperS181S196pearS29S173S13sharkS355S146S383turtleS185raywhaledolphinS363S338orchidpoppyroseS294tulipS311S343S13601020304050Top K features0.10.20.30.40.50.60.70.8Semantic relevanceBYOLBYOL + large repr.BYOL+SEM\\x0cFOOD CIFAR10 CIFAR-100 SUN DTD PETS FLOWERS CALTECH CARS\\n45.7\\n57.6 71.5 85.4\\nBYOL\\n71.3\\n57.3\\n60.5 72.5 87.1\\nBYOL + SEM 74.1\\n57.6 70.9 82.3\\n39.8\\nMOCO\\n70.6\\nMOCO + SEM 71.0\\n45.2\\n58.6 70.9 83.8\\nTable 2: Transfer learning accuracy by training a linear probe on a pre-trained representation with\\nIMAGENET for 100 epochss. Boldface indicates the maximal value for each transfer dataset and each\\nbase model type (BYOL or MoCo).\\n\\n77.8\\n82.4\\n74.3\\n77.5\\n\\n89.5\\n92.0\\n88.6\\n89.6\\n\\n84.6\\n88.6\\n81.5\\n84.5\\n\\n71.4\\n76.3\\n69.5\\n72.8\\n\\nin the baseline representation may follow a more complicated syntactic structure than those encoded\\nwith the SEM features.\\n\\n4.3 Out-of-distribution evaluation on ImageNet\\n\\nRobustness to out-of-distribution test sets on ImageNet. We perform a comparative study using\\nseveral robustness evaluation sets. Speciﬁcally, we use the validation set provided in IMAGENET;\\nIMAGENET-C, which exhibits a set of common image corruptions [Hendrycks and Dietterich, 2018];\\nIMAGENET-A [Chen et al., 2020a], which contains a set of natural adversarial examples that are\\nmisclassiﬁed by a Resnet-50 classiﬁer; IMAGENET-R [Hendrycks et al., 2021], which consists of\\ndifferent renderings for several ImageNet classes; and IMAGENET-V2 [Recht et al., 2019], a distinct\\ntest set for ImageNet collected using the same process. We use the methodology proposed in Djolonga\\net al. [2020, 2021] along with their software to perform our experiments.\\n\\nTable 1 shows the performance on these test sets using a linear probe trained with 100% of ImageNet’s\\ndata and 1% of ImageNet’s data. Using the SEM generally leads to an improvement in the in-\\ndistribution and out-of-distribution generalization. Notably, we observe a 2% improvement on BYOL\\ndue to the SEM on in-distribution IMAGENET. On average, there is an improvement of 2% and 0.5%\\nin the 100% and 1% data regimes respectively for BYOL. For MOCO, the average improvement due\\nto the SEM is 1.5% and 0.8% for the 100% and 1% data regimes respectively.\\n\\nTransfer learning on ImageNet. We probe the effect of inducing the SEM in BYOL and MoCo\\non the transfer accuracy to other classiﬁcation tasks from representations trained on IMAGENET.\\nWe follow the linear evaluation methodology described in previous works [Grill et al., 2020; Lee\\net al., 2021], which entails training a linear classiﬁer on the embeddings of the samples for each\\ndataset. We perform our transfer learning experiments on the following datasets: Food [Bossard et al.,\\n2014], CIFAR-10 [Krizhevsky, 2009], CIFAR-100 [Krizhevsky, 2009], SUN [Xiao et al., 2010],\\nDTD [Cimpoi et al., 2014], Pets [Parkhi et al., 2012], Flowers [Nilsback and Zisserman, 2008],\\nCalTech [Fei-Fei et al., 2004] and Cars [Krause et al., 2013].\\n\\nThis task evaluates the generality of the encoder as it has to encode samples from various out-of-\\ndistribution domains with categories that it may not have seen during training. We present our results\\nin Table 2 and observe that the SEM improves the transfer accuracy over the baseline for every\\ndataset.\\n\\n5 Conclusion\\n\\nThis work introduces the Simplicial Embeddings (SEM) as a simple and effective drop-in module\\nfor self-supervised learning that leads to representation with better generalization. Our theoretical\\ninsights demonstrate that the temperature parameter of the SEM allows for control over the trade-off\\nbetween the training loss and expressivity on downstream classiﬁers; we also observe that controlling\\nthe expressivity via the temperature parameter. We validate our theoretical prediction with a set\\nof controlled experiments. Moreover, we empirically demonstrate that the SEM improves the in-\\ndistribution test accuracy and out-of-distribution accuracy on several robustness test sets and transfer\\nlearning datasets.\\n\\nWe have also demonstrated that the SEM leads to more semantically relevant features for predicting\\nthe categories of a dataset compared to the baseline method. Thus, the SEM embedding may be\\nsimpler than the un-normalized embedding, leading to more interpretable representations. We want\\n\\n8\\n\\n\\x0cto study this in more depth in future works. Related, we would also like to investigate further why the\\nSEM leads to such representations.\\n\\nAcknowledgments and Disclosure of Funding\\n\\nThe authors are grateful for the insightful discussions with Xavier Bouthillier, Michael Noukhovitch,\\nHattie Zhou, Sébastien Lachapelle, Yuchen Lu, Eeshan Dhekane, and Devon Hjelm. We acknowledge\\nfunding support from Samsung and Hitachi, as well as support from Aaron Courville’s CIFAR CCAI\\nchair. We also wish to acknowledge Mila and Compute Canada for providing the computing infras-\\ntructure that enabled this project. Finally, this project would not have been possible without the con-\\ntribution of the following open source projects: Pytorch [Paszke et al., 2019], Orion [Bouthillier et al.,\\n2022], Solo-Learn [da Costa et al., 2021], Scikit-Learn [Pedregosa et al., 2011], and Numpy [Harris\\net al., 2020].\\n\\nReferences\\n\\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization.\\narXiv:1907.02893 [cs, stat], March 2020. URL http://arxiv.org/abs/1907.02893. arXiv:\\n1907.02893.\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly\\nLearning to Align and Translate. arXiv:1409.0473 [cs, stat], May 2016. URL http://arxiv.\\norg/abs/1409.0473. arXiv: 1409.0473.\\n\\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative com-\\nponents with random forests. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,\\neditors, Computer Vision – ECCV 2014, pages 446–461, Cham, 2014. Springer International\\nPublishing. ISBN 978-3-319-10599-4.\\n\\nLéon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization Methods for Large-Scale Machine\\nLearning. arXiv:1606.04838 [cs, math, stat], February 2018. URL http://arxiv.org/abs/\\n1606.04838. arXiv: 1606.04838.\\n\\nXavier Bouthillier, Christos Tsirigotis, François Corneau-Tremblay, Thomas Schweizer, Lin Dong,\\nPierre Delaunay, Fabrice Normandin, Mirko Bronzi, Dendi Suhubdy, Reyhane Askari, Michael\\nNoukhovitch, Chao Xue, Satya Ortiz-Gagné, Olivier Breuleux, Arnaud Bergeron, Olexa Bilaniuk,\\nSteven Bocco, Hadrien Bertrand, Guillaume Alain, Dmitriy Serdyuk, Peter Henderson, Pascal\\nLamblin, and Christopher Beckham. Epistimio/orion: Asynchronous Distributed Hyperparameter\\nOptimization, March 2022. URL https://doi.org/10.5281/zenodo.3478592.\\n\\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep Clustering for\\nUnsupervised Learning of Visual Features. arXiv:1807.05520 [cs], March 2019. URL http:\\n//arxiv.org/abs/1807.05520. arXiv: 1807.05520.\\n\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\\n\\nUnsupervised learning of visual features by contrasting cluster assignments. 2020.\\n\\nTianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial\\nRobustness: From Self-Supervised Pre-Training to Fine-Tuning. pages 699–708, 2020a. URL\\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Adversarial_\\nRobustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_\\npaper.html.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\\ncontrastive learning of visual representations. In Hal Daumé III and Aarti Singh, editors, Proceed-\\nings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of\\nMachine Learning Research, pages 1597–1607. PMLR, 13–18 Jul 2020b.\\n\\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint\\n\\narXiv:2011.10566, 2020.\\n\\n9\\n\\n\\x0cMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-\\nscribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 3606–3613, 2014.\\n\\nVictor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. Solo-learn: A library\\nof self-supervised methods for visual representation learning, 2021. URL https://github.com/\\nvturrisi/solo-learn.\\n\\nJosip Djolonga, Frances Hubis, Matthias Minderer, Zachary Nado, Jeremy Nixon, Rob Romijn-\\nders, Dustin Tran, and Mario Lucic. Robustness Metrics, 2020. URL https://github.com/\\ngoogle-research/robustness_metrics.\\n\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Syl-\\nvain Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On Robustness and Transferabil-\\nity of Convolutional Neural Networks. arXiv:2007.08558 [cs], March 2021. URL http:\\n//arxiv.org/abs/2007.08558. arXiv: 2007.08558.\\n\\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training\\nexamples: An incremental bayesian approach tested on 101 object categories. In 2004 conference\\non computer vision and pattern recognition workshop, pages 178–178. IEEE, 2004.\\n\\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv:1410.5401 [cs],\\n\\nDecember 2014. URL http://arxiv.org/abs/1410.5401. arXiv: 1410.5401.\\n\\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\\nBilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent -\\na new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\\nBalcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\\npages 21271–21284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\\npaper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\\n\\nCharles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\\nCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti\\nPicus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández\\ndel Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy,\\nWarren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming\\nwith NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2.\\nURL https://doi.org/10.1038/s41586-020-2649-2.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\\nRecognition. arXiv:1512.03385 [cs], December 2015. URL http://arxiv.org/abs/1512.\\n03385. arXiv: 1512.03385.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for\\nUnsupervised Visual Representation Learning. arXiv:1911.05722 [cs], March 2020. URL http:\\n//arxiv.org/abs/1911.05722. arXiv: 1911.05722.\\n\\nDan Hendrycks and Thomas Dietterich. Benchmarking Neural Network Robustness to Common\\nCorruptions and Perturbations. September 2018. URL https://openreview.net/forum?id=\\nHJz6tiCqYm.\\n\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\\nThe Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization.\\narXiv:2006.16241 [cs, stat], July 2021. URL http://arxiv.org/abs/2006.16241. arXiv:\\n2006.16241.\\n\\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\\nand maximization. In International Conference on Learning Representations, 2019. URL https:\\n//openreview.net/forum?id=Bklr3j0cKX.\\n\\n10\\n\\n\\x0cSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd\\nInternational Conference on Machine Learning, volume 37 of Proceedings of Machine Learning\\nResearch, pages 448–456, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.\\nmlr.press/v37/ioffe15.html.\\n\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,\\nAaron Maschinot, Ce Liu, and Dilip Krishnan.\\nIn\\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Ad-\\nvances in Neural Information Processing Systems, volume 33, pages 18661–18673. Cur-\\nran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\\nd89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf.\\n\\nSupervised contrastive learning.\\n\\nAlexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representa-\\n\\ntion learning. CoRR, abs/1901.09005, 2019. URL http://arxiv.org/abs/1901.09005.\\n\\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained\\ncategorization. In Proceedings of the IEEE international conference on computer vision workshops,\\npages 554–561, 2013.\\n\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\n\\nKuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive\\nVisual Representations. arXiv:2109.12909 [cs, math], September 2021. URL http://arxiv.\\norg/abs/2109.12909. arXiv: 2109.12909.\\n\\nMaria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number\\nof classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing,\\npages 722–729, 2008. doi: 10.1109/ICVGIP.2008.47.\\n\\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In 2012\\nIEEE Conference on Computer Vision and Pattern Recognition, pages 3498–3505, 2012. doi:\\n10.1109/CVPR.2012.6248092.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\\nhigh-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\'Alché-\\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,\\npages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\\npdf.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\\nE. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,\\n12:2825–2830, 2011.\\n\\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classiﬁers\\nIn Proceedings of the 36th International Conference on Machine\\nGeneralize to ImageNet?\\nLearning, pages 5389–5400. PMLR, May 2019. URL https://proceedings.mlr.press/\\nv97/recht19a.html. ISSN: 2640-3498.\\n\\nAaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive Learning of General-Purpose Audio\\nRepresentations. arXiv:2010.10915 [cs, eess], October 2020. URL http://arxiv.org/abs/\\n2010.10915. arXiv: 2010.10915.\\n\\nAad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. Springer\\nNew York, 1996. doi: 10.1007/978-1-4757-2545-2. URL https://doi.org/10.1007%\\n2F978-1-4757-2545-2.\\n\\n11\\n\\n\\x0cAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762 [cs], December 2017.\\nURL http://arxiv.org/abs/1706.03762. arXiv: 1706.03762 version: 5.\\n\\nTan Wang, Zhongqi Yue, Jianqiang Huang, Qianru Sun, and Hanwang Zhang. Self-Supervised\\nLearning Disentangled Group Representation as Feature. May 2021. URL https://openreview.\\nnet/forum?id=RQfcckT1M_4.\\n\\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\\nLarge-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on\\ncomputer vision and pattern recognition, pages 3485–3492. IEEE, 2010.\\n\\nAsano Ym, Rupprecht C, and Vedaldi A. Self-labelling via simultaneous clustering and representation\\n\\nlearning. September 2019. URL https://openreview.net/forum?id=Hyx-jyBFPr.\\n\\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\\ncontrastive learning with augmentations. CoRR, abs/2010.13902, 2020. URL https://arxiv.\\norg/abs/2010.13902.\\n\\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\\n\\nlearning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.\\n\\n12\\n\\n\\x0cA Proof of Theorem 1\\n\\nLet us introduce additional notations used in the proofs. Deﬁne r = (z, y) ∈ R, (cid:96)(f, r) = l(f (z), y),\\n\\n˜Cy,k1,...,kL = {(z, ˆy) ∈ Z × Y : ˆy = y, kj = arg max\\n\\nzj,t ∀j ∈ [L]},\\n\\nt∈[V ]\\n\\nand\\n\\n˜Zk1,...,kL = {z ∈ Z : kj = arg max\\n\\nzj,t ∀j ∈ [L]}.\\n\\nt∈[V ]\\n\\nto be\\n\\nthen deﬁne Ck\\n\\n=\\nWe\\nthe ﬂatten version of\\n{ ˜Cy,k1,...,kL,y}y∈Y,k1,...,kL∈[V ] with C1 = ˜C1,1,...,1, C2 = ˜C2,1,...,1, C|Y| = ˜C|Y|,1,...,1, C|Y|+1 =\\n˜C1,2,1,...,1, C2|Y| = ˜C|Y|,2,1,...,1, and so on. Similarly, deﬁne Zk to be the ﬂatten version of ˜Zk1,...,kL.\\nWe also use Qi = {q ∈ [−1, +1]V : i = arg maxj∈[V ] qj}, Ik := I S\\nk := {i ∈ [n] : ri ∈ Ck}, and\\n(cid:80)n\\nαk(h) := Er[(cid:96)(h, r)|r ∈ Ck]. Moreover, we deﬁne ϕ(f S\\nt=1 (cid:107)q − q(cid:48)(cid:107)2\\n2,\\neqj /τ\\nand ϕ(f S\\nt=1 eqt/τ for\\nj = 1, . . . , V .\\n\\nSEM(τ )) = supi∈[V ] supq,q(cid:48)∈Qi\\n\\nbase) = supi∈[V ] supq,q(cid:48)∈Qi\\n\\nt=1 (cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n\\n2 where στ (q)j =\\n\\n(cid:80)n\\n\\ni.e.,\\n\\nk=1\\n\\n(cid:80)V\\n\\n{Ck}K\\n\\n˜Cy,k1,...,kL;\\n\\nWe ﬁrst decompose the generalization gap into two terms using the following lemma:\\nLemma 1. For any δ > 0, with probability at least 1 − δ,the following holds for all h ∈ H:\\n\\nEr[(cid:96)(h, r)] −\\n\\n(cid:96)(h, ri) ≤\\n\\n|Ik|\\n\\n\\uf8edαk(h) −\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8 + c\\n\\n\\uf8eb\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8f6\\n\\n(cid:114)\\n\\nln(2/δ)\\nn\\n\\n.\\n\\nProof. We ﬁrst write the expected error as the sum of the conditional expected error:\\n\\nEr[(cid:96)(h, r)] =\\n\\nEr[(cid:96)(h, r)|r ∈ Ck] Pr(r ∈ Ck) =\\n\\nErk [(cid:96)(h, rk)] Pr(r ∈ Ck),\\n\\nwhere rk is the random variable for the conditional with r ∈ Ck. Using this, we decompose the\\ngeneralization error into two terms:\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nEr[(cid:96)(h, r)] −\\n\\n(cid:96)(h, ri)\\n\\n=\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nErk [(cid:96)(h, rk)]\\n\\nPr(r ∈ Ck) −\\n\\nErk [(cid:96)(h, rk)]\\n\\n(cid:19)\\n\\n|Ik|\\nn\\n\\n\\uf8eb\\n\\n+\\n\\n\\uf8ed\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n|Ik|\\nn\\n\\n−\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n\\uf8f6\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8 .\\n\\nThe second term in the right-hand side of (5) is further simpliﬁed by using\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:18)\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:96)(h, ri) =\\n\\n(cid:96)(h, ri),\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\n(cid:88)\\n\\nk=1\\n\\ni∈Ik\\n\\nas\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nErk [(cid:96)(h, rk)]\\n\\n|Ik|\\nn\\n\\n−\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:96)(h, ri) =\\n\\n|Ik|\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8eb\\n\\uf8edErk [(cid:96)(h, rk)] −\\n\\n\\uf8f6\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\nSubstituting these into equation (5) yields\\nn\\n(cid:88)\\n\\nEr[(cid:96)(h, r)] −\\n\\n(cid:96)(h, ri)\\n\\n1\\nn\\n\\ni=1\\n\\n(cid:18)\\n\\n=\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nErk [(cid:96)(h, rk)]\\n\\nPr(r ∈ Ck) −\\n\\n(cid:19)\\n\\n+\\n\\n|Ik|\\nn\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8eb\\n\\uf8edErk [(cid:96)(h, rk)] −\\n\\n|Ik|\\n\\n≤ B\\n\\nPr(r ∈ Ck) −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n|Ik|\\nn\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8eb\\n\\uf8edErk [(cid:96)(h, rk)] −\\n\\n|Ik|\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\uf8f6\\n\\n(5)\\n\\n(6)\\n\\n\\uf8f6\\n\\n13\\n\\n\\x0cBy using the Bretagnolle-Huber-Carol inequality [van der Vaart and Wellner, 1996, A6.6 Proposition],\\nwe have that for any δ > 0, with probability at least 1 − δ,\\n(cid:114)\\nK\\n(cid:88)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nk=1\\n\\nPr(r ∈ Ck) −\\n\\n|Ik|\\nn\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n≤\\n\\n2K ln(2/δ)\\nn\\n\\n.\\n\\n(cid:12)\\nHere, notice that the term of (cid:80)K\\n(cid:12)\\n(cid:12) does not depend on h ∈ H. Moreover,\\nnote that for any (f, h, M ) such that M > 0 and B ≥ 0 for all X, we have that P(f (X) ≥ M ) ≥\\nP(f (X) > M ) ≥ P(Bf (X) + h(X) > BM + h(X)), where the probability is with respect to the\\nrandomness of X. Thus, by combining (6) and (7), we have that for any h ∈ H, for any δ > 0, with\\nprobability at least 1 − δ, the following holds for all h ∈ H,\\n\\n(cid:12)\\n(cid:12)Pr(r ∈ Ck) − |Ik|\\n(cid:12)\\n\\nk=1\\n\\nn\\n\\n(7)\\n\\nEr[(cid:96)(h, r)] −\\n\\n(cid:96)(h, ri) ≤\\n\\n|Ik|\\n\\n\\uf8edαk(h) −\\n\\n(cid:96)(h, ri)\\n\\n\\uf8f8 + c\\n\\n\\uf8eb\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n\\uf8f6\\n\\n(cid:114)\\n\\nln(2/δ)\\nn\\n\\n.\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\nIn particular, the ﬁrst term from the previous lemma will be bounded with the following lemma:\\nLemma 2. For any f ∈ {f S\\n\\nSEM(τ ), f S\\n\\uf8eb\\n\\nbase},\\n\\n|Ik|\\n\\n\\uf8edαk(f ) −\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\uf8f6\\n\\n(cid:114)\\n\\n(cid:96)(f, ri)\\n\\n\\uf8f8 ≤ R\\n\\nLϕ(f )\\nn\\n\\n.\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\nProof. By using the triangle inequality,\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n≤\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n\\uf8eb\\n\\uf8edEr[(cid:96)(f, r)|r ∈ Ck] −\\n\\n|Ik|\\n\\n\\uf8f6\\n\\n(cid:96)(f, ri)\\n\\n\\uf8f8\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n|Ik|\\n\\nEr[(cid:96)(f, r)|r ∈ Ck] −\\n\\n(cid:96)(f, ri)\\n\\n.\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nFurthermore, by using the triangle inequality,\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:96)(f, ri)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nEr[(cid:96)(f, r)|r ∈ Ck] −\\n\\nEr[(cid:96)(f, r)|r ∈ Ck] −\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:96)(f, ri)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n|Ik|\\n\\n1\\n|Ik|\\n\\n=\\n\\n≤\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n(cid:88)\\n\\n≤ sup\\n\\nr,r(cid:48)∈Ck\\n\\n(cid:12)Er[(cid:96)(f, r)|r ∈ Ck] − (cid:96)(f, ri)(cid:12)\\n(cid:12)\\n(cid:12)\\n\\ni∈Ik\\n(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\\n(cid:12)\\n(cid:12) .\\n\\nSEM(τ ) ◦στ , since gS\\n\\nSEM(τ ) ∈ GS, by using the Lipschitz continuity, boundedness,\\n\\nIf f = f S\\nand non-negativity,\\n\\nSEM(τ ) = gS\\n\\n(cid:12)\\n(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\\n\\nsup\\nr,r(cid:48)∈Ck\\n\\n|(ly ◦ gS\\n\\nSEM(τ ))(στ (z)) − (ly ◦ gS\\n\\nSEM(τ ))(στ (z(cid:48)))|\\n\\nsup\\nz,z(cid:48)∈Zk\\n\\n(cid:12) = sup\\ny∈Y\\n≤ R sup\\n\\nz,z(cid:48)∈Zk\\n\\n(cid:107)στ (z) − στ (z(cid:48))(cid:107)F\\n\\n(cid:118)\\n(cid:117)\\n(cid:117)\\n(cid:116)\\n\\nL\\n(cid:88)\\n\\nV\\n(cid:88)\\n\\nt=1\\n\\nj=1\\n\\n(στ (zt,j) − στ (z(cid:48)\\n\\nt,j))2\\n2\\n\\nsup\\ni∈[V ]\\n\\nsup\\nq,q(cid:48)∈Qi\\n\\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n2\\n\\n= R sup\\n\\nz,z(cid:48)∈Zk\\n\\nL\\n(cid:88)\\n\\nt=1\\n\\n(cid:118)\\n(cid:117)\\n(cid:117)\\n(cid:116)\\n\\n(cid:115)\\n\\n≤ R\\n\\n= R\\n\\nLϕ(f S\\n\\nSEM(τ ))\\nn\\n\\n14\\n\\n\\x0cbase = gS\\n\\nSimilarly, if f = f S\\nand non-negativity,\\n(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\\n(cid:12)\\n\\nbase, since gS\\n\\nsup\\nr,r(cid:48)∈Ck\\n\\nbase ∈ GS, by using the Lipschitz continuity, boundedness,\\n\\n|(ly ◦ gS\\n\\nbase)(z) − (ly ◦ gS\\n\\nbase)(z(cid:48))|\\n\\nsup\\nz,z(cid:48)∈Zk\\n\\n(cid:12) = sup\\ny∈Y\\n≤ R sup\\n\\n(cid:107)z − z(cid:48)(cid:107)F\\n\\nz,z(cid:48)∈Zk\\n(cid:114)\\n\\nLϕ(f S\\nbase)\\nn\\n\\n.\\n\\n≤ R\\n\\nTherefore, for any f ∈ {f S\\n\\nSEM(τ ), f S\\n\\nbase},\\n\\n\\uf8eb\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n|Ik|\\n\\n\\uf8edαk(f ) −\\n\\n(cid:96)(f, ri)\\n\\n\\uf8f8 ≤\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni∈Ik\\n\\n\\uf8f6\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n|Ik|R(cid:112)Lϕ(f ) = R\\n\\n(cid:114)\\n\\nLϕ(f )\\nn\\n\\n.\\n\\nCombining Lemma 1 and Lemma 2, we obtain the following upper bound on the gap:\\nLemma 3. For any δ > 0, with probability at least 1 − δ,the following holds for any f ∈\\n{f S\\n\\nSEM(τ ), f S\\n\\nbase}:\\n\\nEr[(cid:96)(f, r)] −\\n\\n(cid:96)(f, ri) ≤ R\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:114)\\n\\n(cid:114)\\n\\nLϕ(f )\\nn\\n\\n+ c\\n\\nln(2/δ)\\nn\\n\\n.\\n\\nProof. This follows directly from combining Lemma 1 and Lemma 2.\\n\\nWe now provide an upper bound on ϕ(f S\\n\\nSEM(τ )) in the following lemma:\\n\\nLemma 4. For any τ > 0,\\n\\nϕ(f S\\n\\nSEM(τ ))\\nn\\n\\n≤\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+ (V − 1)\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\nProof. Recall the deﬁnition:\\n\\nϕ(f S\\n\\nSEM(τ ))\\nn\\n\\n= sup\\ni∈[V ]\\n\\nsup\\nq,q(cid:48)∈Qi\\n\\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n2.\\n\\nστ (q)j =\\n\\neqj /τ\\nt=1 eqt/τ\\n\\n,\\n\\n(cid:80)V\\n\\nwhere\\n\\nand\\n\\nfor j = 1, . . . , V . By the symmetry and independence over i ∈ [V ] inside of the ﬁrst supremum, we\\nhave\\n\\nϕ(f S\\n\\nSEM(τ ))\\nn\\n\\n= sup\\n\\nq,q(cid:48)∈Q1\\n\\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n2.\\n\\nFor any q, q(cid:48) ∈ Q1 and i ∈ {2, . . . , V } (with q = (q1, . . . , qV ) and q(cid:48) = (q(cid:48)\\nδi, δ(cid:48)\\n\\ni > 0 such that\\n\\n1, . . . , q(cid:48)\\n\\nV )), there exists\\n\\nHere, since zik − ∆ ≥ zij from the assumption, we have that for all i ∈ {2, . . . , V },\\n\\nqi = q1 − δi\\n\\ni = q(cid:48)\\nq(cid:48)\\n\\n1 − δ(cid:48)\\ni.\\n\\nδi, δ(cid:48)\\n\\ni ≥ ∆ > 0.\\n\\n15\\n\\n\\x0cThus, we can rewrite\\n\\nSimilarly,\\n\\nUsing these,\\n\\nV\\n(cid:88)\\n\\nt=1\\n\\nV\\n(cid:88)\\n\\nt=1\\n\\neqt/τ = eq1/τ +\\n\\ne(q1−δi)/τ\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\n= eq1/τ + eq1/τ\\n\\ne−δi/τ\\n\\n= eq1/τ\\n\\n\\uf8ed1 +\\n\\ne−δi/τ\\n\\n\\uf8eb\\n\\n\\uf8eb\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\n\\uf8f6\\n\\n\\uf8f8\\n\\n\\uf8f6\\n\\neq(cid:48)\\n\\nt/τ = eq(cid:48)\\n\\n1/τ\\n\\n\\uf8ed1 +\\n\\ne−δ(cid:48)\\n\\ni/τ\\n\\n\\uf8f8 .\\n\\nστ (q)1 =\\n\\neq1/τ\\nt=1 eqt/τ\\n\\n(cid:80)V\\n\\n=\\n\\neq1/τ\\n1 + (cid:80)V\\n\\n(cid:16)\\n\\neq1/τ\\n\\ni=2 e−δi/τ\\n\\n(cid:17) =\\n\\n1\\n1 + (cid:80)V\\ni=2 e−δi/τ\\n\\nand for all j ∈ {2, . . . , V },\\n\\nστ (q)j =\\n\\neqj /τ\\nt=1 eqt/τ\\n\\n(cid:80)V\\n\\n=\\n\\n=\\n\\n=\\n\\ne(q1−δj )/τ\\n1 + (cid:80)V\\n\\n(cid:16)\\n\\n(cid:17)\\n\\ni=2 e−δi/τ\\n\\neq1/τ\\n\\ne−δj /τ\\n\\n1 + (cid:80)V\\n\\ni=2 e−δi/τ\\n1\\n1 + eδj /τ + (cid:80)V\\n\\ne(δj −δi)/τ\\n\\ni∈Ij\\n\\nστ (q(cid:48))1 =\\n\\n1\\n1 + (cid:80)V\\ni=2 e−δ(cid:48)\\n\\ni/τ\\n\\n,\\n\\nστ (q(cid:48))j =\\n\\n1\\nj /τ + (cid:80)V\\n\\n1 + eδ(cid:48)\\n\\ne(δ(cid:48)\\n\\nj −δ(cid:48)\\n\\ni)/τ\\n\\ni∈Ij\\n\\n.\\n\\nwhere Ij := {2, . . . , V } \\\\ {j}. Similarly,\\n\\nand for all j ∈ {2, . . . , V },\\n\\nUsing these, for any q, q(cid:48) ∈ Q1,\\n\\n|στ (q)1 − στ (q(cid:48))1| =\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n≤\\n\\n=\\n\\n1\\n1 + (cid:80)V\\ni=2 e−δi/τ\\n1\\n1 + (cid:80)V\\ni=2 e−2/τ\\n\\n−\\n\\n−\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\ni/τ\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1 + (cid:80)V\\ni=2 e−δ(cid:48)\\n1\\n1 + (cid:80)V\\ni=2 e−∆/τ\\n1\\n1 + (V − 1)e−∆/τ\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n,\\n\\n16\\n\\n\\x0c(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n≤\\n\\n=\\n\\n=\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nand for all j ∈ {2, . . . , V },\\n\\n|στ (q)j − στ (q(cid:48))j| =\\n\\n1\\n1 + eδj /τ + (cid:80)V\\n\\ne(δj −δi)/τ\\n\\ni∈Ij\\n\\n−\\n\\n1\\nj /τ + (cid:80)V\\n\\n1 + eδ(cid:48)\\n\\ne(δ(cid:48)\\n\\nj −δ(cid:48)\\n\\ni)/τ\\n\\ni∈Ij\\n\\n1\\n1 + e∆/τ + (cid:80)V\\n\\ne(∆−2)/τ\\n\\ni∈Ij\\n\\n−\\n\\n1\\n1 + e2/τ + (cid:80)V\\n\\ne(2−∆)/τ\\n\\ni∈Ij\\n\\n1\\n1 + e∆/τ + (V − 2)e(∆−2)/τ\\n\\n1\\n1 + e2/τ + (V − 2)e(2−∆)/τ\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n.\\n\\n−\\n\\n−\\n\\nBy combining these,\\n\\nsup\\nq,q(cid:48)∈Q1\\n\\n(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\\n2\\n\\n= sup\\n\\nq,q(cid:48)∈Q1\\n\\nV\\n(cid:88)\\n\\nj=1\\n\\n|στ (q)j − στ (q(cid:48))j|2\\n\\n≤\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\n+ (V − 1)\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\nUsing the previous lemma, we will conclude the asymptotic behavior of ϕ(f S\\nlemma:\\nLemma 5. It holds that\\n\\nSEM(τ )) in the following\\n\\nϕ(f S\\n\\nSEM(τ )) → 0 as τ → 0.\\n\\nProof. Using Lemma 4,\\n\\nlim\\nτ →0\\n\\nϕ(f S\\n\\nSEM(τ )) ≤ lim\\nτ →0\\n\\nn\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+ n(V − 1) lim\\nτ →0\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\nlim\\nτ →0\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n=\\n\\n−\\n\\n= 0,\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1\\n\\nlim\\nτ →0\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n= |0 − 0|2 = 0.\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nMoreover,\\n\\nand\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nTherefore,\\n\\nSince ϕ(f S\\n\\nSEM(τ )) ≥ 0, this implies the statement of this lemma.\\n\\nlim\\nτ →0\\n\\nϕ(f S\\n\\nSEM(τ )) ≤ 0.\\n\\n17\\n\\n\\x0cAs we have analyzed ϕ(f S\\nSEM(τ )) and ϕ(f S\\nϕ(f S\\nLemma 6. For any τ > 0,\\n\\nSEM(τ )) in the previous two lemmas, we are now ready to compare\\n\\nbase), which is done in the following lemma:\\n\\nϕ(f S\\n\\nSEM(τ )) − ϕ(f S\\n\\nbase) ≤\\n\\n(1 − V ) < 0.\\n\\n3n\\n4\\n\\nProof. From Lemma 4, for any τ > 0,\\n\\nϕ(f S\\n\\nSEM(τ )) ≤ n\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)e−∆/τ\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n1\\n(cid:12)\\n(cid:12)\\n1\\n(cid:12)\\n(cid:18) 1\\n1\\n\\n+ n(V − 1)\\n\\n1\\n1 + e∆/τ (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2)e−∆/τ )\\n\\n≤ n\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n1\\n1 + (V − 1)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+ n(V − 1)\\n\\n1\\n1 + (1 + (V − 2)e−2/τ )\\n\\n−\\n\\n1\\n1 + e2/τ (1 + (V − 2))\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n= n\\n\\n1\\n1 + (V − 1)e−2/τ\\n\\n−\\n\\n+ n(V − 1)\\n\\n1\\n2 + (V − 2)e−2/τ\\n\\n−\\n\\n1\\n1 + e2/τ (V − 1)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n≤ n\\n\\n−\\n\\n+ n(V − 1)\\n\\n= n\\n\\n−\\n\\n+ n(V − 1)\\n\\n1\\nV\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:19)2\\n\\n1\\nV\\n\\n1\\nV\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n2\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n− 0\\n(cid:12)\\n(cid:12)\\n\\n1\\n4\\n\\n.\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nRecall the deﬁnition of\\n\\nϕ(f S\\n\\nbase) = sup\\ni∈[V ]\\n\\nsup\\nq,q(cid:48)∈Qi\\n\\nn(cid:107)q − q(cid:48)(cid:107)2\\n2.\\n\\nBy choosing an element in the set over which the supremum is taken, for any δ ≥ ∆ > 0,\\n\\nϕ(f S\\n\\nbase) ≥ sup\\n\\nn(cid:107)q − q(cid:48)(cid:107)2\\n\\n2 ≥ n(cid:107)ˆq − ˆq(cid:48)(cid:107)2\\n\\n2 = n\\n\\n(ˆqj − ˆq(cid:48)\\n\\nj)2\\n\\n2 = n(2 − δ)2V,\\n\\nq,q(cid:48)∈Q1\\n\\nV\\n(cid:88)\\n\\nj=1\\n\\nwhere ˆq1 = 1, ˆqj = 1 − δ for j ∈ {2, . . . , V }, ˆq(cid:48)\\nBy combining those, for for any τ > 0 and δ ≥ ∆ > 0,\\n\\n1 = δ − 1, and ˆq(cid:48)\\n\\nj = −1 for j ∈ {2, . . . , V }.\\n\\nϕ(f S\\n\\nSEM(τ )) − ϕ(f S\\nn\\n\\nbase)\\n\\n≤\\n\\n(cid:18) 1\\n1\\n\\n(cid:19)2\\n\\n1\\nV\\n\\n+ (V − 1)\\n\\n− (2 − δ)2V\\n\\n1\\n4\\n\\n≤ 1 +\\n\\nV −\\n\\n− (2 − δ)2V\\n\\n1\\n4\\n\\n=\\n\\n=\\n\\n+\\n\\n− V\\n\\nV − (2 − δ)2V\\n(cid:18)\\n\\n(2 − δ)2 −\\n\\n(cid:19)\\n\\n1\\n4\\n\\n(cid:18)\\n\\n≤\\n\\n− V\\n\\n1 −\\n\\n(cid:19)\\n\\n1\\n4\\n\\n=\\n\\n(1 − V )\\n\\n3\\n4\\n3\\n4\\n3\\n4\\n3\\n4\\n\\n−\\n\\n1\\n4\\n1\\n4\\n\\n18\\n\\n\\x0cWe combine the lemmas above to prove Theorem 1, which is restated below with its proof:\\n\\nTheorem 1. Let V ≥ 2. For any δ > 0, with probability at least 1 − δ, the following holds for any\\nfS ∈ {f S\\n\\nSEM(τ ), f S\\n\\nbase}:\\n\\nEz,y[l(fS(z), y)] ≤\\n\\nl(fS(z(i)), y(i)) + R\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:114)\\n\\n(cid:114)\\n\\nLϕ(fS)\\nn\\n\\n+ c\\n\\nln(2/δ)\\nn\\n\\n.\\n\\nMoreover,\\n\\nϕ(f S\\n\\nSEM(τ )) → 0 as τ → 0\\n\\nand ϕ(f S\\n\\nSEM(τ )) − ϕ(f S\\n\\nbase) ≤\\n\\n(1 − V ) < 0 ∀τ > 0.\\n\\n3n\\n4\\n\\nProof. The ﬁrst statement directly follows from Lemma 3. The second statement is proven by\\nLemma 5 and Lemma 6.\\n\\nB Hyperparameters\\n\\nWe present the hype-parameters used to train for BYOL+SEM on CIFAR100. The same parameters\\nwere used for ResNet18 and ResNet50.\\n\\nLearning rate\\nWeight-decay\\nOptimizer\\nBYOL EMA\\nVocabulary size (V)\\nMessage length (L)\\nτ online network\\nτ target network\\n\\n0.5\\n1e-4\\nAdamW\\n0.99\\n13\\n5000\\n0.5\\n0.5\\n\\n(a) BYOL+SEM hyper-parameters.\\n\\nC Experiment details for ImageNet\\n\\nC.1\\n\\nImage augmentation\\n\\nWe follow the same procedure as [Grill et al., 2020] for the image augmentation procedure. The\\naugmentation applied in order during training are:\\n\\n• Random Resize crop to a 224 × 224 image. A random patch of the image is selected and\\n\\nresized to a 224 × 224 image.\\n\\n• Random color jitter. Modifying the brightness, the contrast, the saturation and the hue.\\n\\n• Random gray scale. Randomly applying a gray scale ﬁlter to the image\\n\\n• Random gaussian blur. Randomly applying a gaussian blue ﬁlter.\\n\\n• Random solarization. Randomly applying a solarization ﬁlter.\\n\\nAt validation and test time, we resize the images to 256 × 256 and then center crop a patch of\\n224 × 224.\\n\\nFor both training and evaluation, we re-normalize the image using the statistic of the training set.\\n\\nC.2 Hyper-parameters\\n\\nWe summarize the hyper-parameters for BYOL with SEM and MoCo with SEM in table 4.\\n\\n19\\n\\n\\x0cLearning rate\\nBatch size\\nWeight-decay\\nOptimizer\\nEpochs\\nBase momentum\\nVocabulary size (V)\\nMessage length (L)\\nτ online network\\nτ target network\\n\\n0.9\\n256\\n1e-6\\nSGD with lars\\n100\\n0.99\\n29\\n465\\n2.397\\n2.386\\n\\n(a) BYOL+SEM hyper-parameters.\\n\\nLearning rate\\nBatch size\\nWeight-decay\\nOptimizer\\nEpochs\\nMoCo’s EMA\\nVocabulary size (V)\\nMessage Length (L)\\nτ online network\\nτ target network\\n\\n0.6\\n256\\n3e-5\\nSGD with lars\\n100\\n0.1\\n12\\n512\\n1.35\\n1.2\\n\\n(b) MoCo+SEM hyper-parameters.\\n\\nTable 4: ImageNet’s experiment hyper-parameters.\\n\\nC.3 Linear evaluation\\n\\nWe follow the evaluation protocol from [Chen et al., 2020b]. The linear evaluation is done by training\\na linear classiﬁer on the frozen representation of the ImageNet training samples. We train a linear\\nclassiﬁer with a cross-entropy objective for 100 epochs using SGD with nesterov and a batch size of\\n512. During training, we apply random resized crop and random horizontal ﬂip.\\n\\nC.4 Semi-supervised learning\\n\\nWe perform semi-supervised experiments by training a linear classiﬁer on top of a frozen repre-\\nsentation. The procedure is the same as the linear evaluation procedure with the exception that we\\ntrain with 1% of the training sample. That training samples are taken according to the split deﬁned\\nin [Chen et al., 2020b].\\n\\nC.5 Robustness experiments\\n\\nWe follow the evaluation procedure from [Lee et al., 2021]. We treated the robustness datasets as\\nadditional \"test sets\" in that we simply evaluated them using the evaluation procedure described\\nabove. The images were resized to a 256 × 256 before being center cropped to a 224 × 224 image.\\nThe evaluation procedure was performed using the public robustness benchmark evaluation code\\nof [Djolonga et al., 2020]2.\\n\\nC.6 Transfer learning experiments\\n\\nWe follow the linear evaluation protocol of [Kolesnikov et al., 2019; Chen et al., 2020b] We train a\\nlinear classiﬁer using a regularized multinomial logistic regression from the scikit-learn package [Pe-\\ndregosa et al., 2011]. The representation is frozen, so that we do not train the encoder backbone nor\\n\\n2https://github.com/google-research/robustness_metrics\\n\\n20\\n\\n\\x0cSuperclass\\naquatic mammals\\nﬁsh\\nﬂowers\\nfood containers\\nfruit and vegetables\\nhousehold electrical devices\\nhousehold furniture\\ninsects\\nlarge carnivores\\nlarge man-made outdoor things\\nlarge natural outdoor scenes\\nlarge omnivores and herbivores\\nmedium-sized mammals\\nnon-insect invertebrates\\npeople\\nreptiles\\nsmall mammals\\ntrees\\nvehicles 1\\nvehicles 2\\n\\nClasses\\nbeaver, dolphin, otter, seal, whale\\naquarium ﬁsh, ﬂatﬁsh, ray, shark, trout\\norchids, poppies, roses, sunﬂowers, tulips\\nbottles, bowls, cans, cups, plates\\napples, mushrooms, oranges, pears, sweet peppers\\nclock, computer keyboard, lamp, telephone, television\\nbed, chair, couch, table, wardrobe\\nbee, beetle, butterﬂy, caterpillar, cockroach\\nbear, leopard, lion, tiger, wolf\\nbridge, castle, house, road, skyscraper\\ncloud, forest, mountain, plain, sea\\ncamel, cattle, chimpanzee, elephant, kangaroo\\nfox, porcupine, possum, raccoon, skunk\\ncrab, lobster, snail, spider, worm\\nbaby, boy, girl, man, woman\\ncrocodile, dinosaur, lizard, snake, turtle\\nhamster, mouse, rabbit, shrew, squirrel\\nmaple, oak, palm, pine, willow\\nbicycle, bus, motorcycle, pickup truck, train\\nlawn-mower, rocket, streetcar, tank, tractor\\n\\nTable 5: Set of classes for each superclass on CIFAR-100.\\n\\nthe batch-normalization statistics. We do not perform any augmentations and the images are resized\\nto 224 pixels using bicubic resampling and the normalized using the statistics on ImageNet’s training\\nset. We tune the regularizer term from a range of 11 logarithmically-spaced values between 10−6 and\\n105 using a small validation set and re-train using the full training set.\\n\\nD CIFAR100 superclass\\n\\nThe 100 classes of CIFAR-100 [Krizhevsky, 2009] are grouped into 20 superclasses. The list of\\nsuperclass for each class in Table 5\\n\\nE Additional CIFAR-100 relevance graphs\\n\\n21\\n\\n\\x0c(a) BYOL baseline\\n\\n(b) BYOL baseline with a large rep-\\nresentation\\n\\n(c) BYOL + SEM\\n\\nFigure 5: Comparison of the full relevance graph W5 between BYOL and BYOL + SEM.\\n\\n22\\n\\nS363otterkangaroobottleS382lampS225beartankS144streetcarroadS249S167trainS94lobsterS273boygirlhousewomanS29S106cockroachS6appledolphinsealwhaleaquarium_fishflatfishraysharkorchidpoppyrosesunflowertulipbowlcancupplatemushroomorangepearsweet_pepperclockcomputer_keyboardtelephonetelevisionbedchaircouchtablewardrobebeebeetlebutterflycaterpillarleopardtigerwolfbridgecastleskyscrapercloudforestmountainplainseacamelcattlechimpanzeefoxpossumraccoonskunkcrabsnailwormbabymanlizardsnaketurtlehamstermouserabbitshrewsquirrelmaple_treeoak_treepalm_treewillow_treebicyclebusmotorcyclepickup_trucklawn_mowerrockettractorS55S63S47S279S162S69S134S242S34S241S0S98S158S361S91S66S12S95S388S53S374S190S22S59S183S261S1S375S27S35S130S110S127S43S111S258S380S214S30S85S292S193S238S336S128S132S284S14S99S93S332S236S298S260S281S68S204S161S182S177S381S64S227S320S318S266S230S196S126S181S184S114S187flatfishtableS16troutcaterpillarS194cattleS276orchidcomputer_keyboardS229palm_treeS184cloudrocketskunkchimpanzeeS256S187pickup_trucksnailcameltigerspiderS307dolphinsweet_pepperS85lionS434S415lobstersunflowermushroomS66orangesquirrelS189S319manS190snakeS195babywardrobewolfsealwillow_treecanS323maple_treeS252S404turtleleopardhamsterrabbitcockroachS353clockforestS87S61pine_treeS369couchS397tankotterwomanbicycleS365S148bedbeavertulipbeelawn_mowercrocodilelamptelephoneS435tractorS422oak_treeS233motorcycleshrewS284S33S146S13plainS78whalebusbeeS248spiderbeetlecockroachS178S240bearkangarootigerS393leopardhousecastleS40roadS107plainS403motorcyclelawn_mowertankS350tractorsealS61beaverS284porcupinebedS168couchS360chairlionfoxS216wolfS272skyscraperS227mountainS78seasnakeS98S49S219S2lizardwormorangeS185S30sweet_pepperpearS198S193applebusS18S133streetcarS370S106pickup_trucktrainroseS306poppyS289S329tulipS334S135orchidS210cancupS31S259S321S291bowlplatebottlewillow_treepine_treemaple_treeoak_treeforestS71palm_treeS138S327S405dolphinrayS151S347S17S379turtlesharkS176S189S355whaleS381S221S200S22boyS157girlwomanbabyS204flatfishmanmouseraccoonS336S271S300possumsquirrelS239S128shrewhamsterrabbit\\x0c'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with one pdf first\n",
    "\n",
    "mypdftext=array_pdf_text[0]\n",
    "mypdftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a99fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization.\n",
      "arXiv:1907.02893 [cs, stat], March 2020. URL http://arxiv.org/abs/1907.02893. arXiv:\n",
      "1907.02893.\n",
      "\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly\n",
      "Learning to Align and Translate. arXiv:1409.0473 [cs, stat], May 2016. URL http://arxiv.\n",
      "org/abs/1409.0473. arXiv: 1409.0473.\n",
      "\n",
      "Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative com-\n",
      "ponents with random forests. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,\n",
      "editors, Computer Vision – ECCV 2014, pages 446–461, Cham, 2014. Springer International\n",
      "Publishing. ISBN 978-3-319-10599-4.\n",
      "\n",
      "Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization Methods for Large-Scale Machine\n",
      "Learning. arXiv:1606.04838 [cs, math, stat], February 2018. URL http://arxiv.org/abs/\n",
      "1606.04838. arXiv: 1606.04838.\n",
      "\n",
      "Xavier Bouthillier, Christos Tsirigotis, François Corneau-Tremblay, Thomas Schweizer, Lin Dong,\n",
      "Pierre Delaunay, Fabrice Normandin, Mirko Bronzi, Dendi Suhubdy, Reyhane Askari, Michael\n",
      "Noukhovitch, Chao Xue, Satya Ortiz-Gagné, Olivier Breuleux, Arnaud Bergeron, Olexa Bilaniuk,\n",
      "Steven Bocco, Hadrien Bertrand, Guillaume Alain, Dmitriy Serdyuk, Peter Henderson, Pascal\n",
      "Lamblin, and Christopher Beckham. Epistimio/orion: Asynchronous Distributed Hyperparameter\n",
      "Optimization, March 2022. URL https://doi.org/10.5281/zenodo.3478592.\n",
      "\n",
      "Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep Clustering for\n",
      "Unsupervised Learning of Visual Features. arXiv:1807.05520 [cs], March 2019. URL http:\n",
      "//arxiv.org/abs/1807.05520. arXiv: 1807.05520.\n",
      "\n",
      "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\n",
      "\n",
      "Unsupervised learning of visual features by contrasting cluster assignments. 2020.\n",
      "\n",
      "Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial\n",
      "Robustness: From Self-Supervised Pre-Training to Fine-Tuning. pages 699–708, 2020a. URL\n",
      "https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Adversarial_\n",
      "Robustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_\n",
      "paper.html.\n",
      "\n",
      "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\n",
      "contrastive learning of visual representations. In Hal Daumé III and Aarti Singh, editors, Proceed-\n",
      "ings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of\n",
      "Machine Learning Research, pages 1597–1607. PMLR, 13–18 Jul 2020b.\n",
      "\n",
      "Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint\n",
      "\n",
      "arXiv:2011.10566, 2020.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-\n",
      "scribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and\n",
      "Pattern Recognition, pages 3606–3613, 2014.\n",
      "\n",
      "Victor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. Solo-learn: A library\n",
      "of self-supervised methods for visual representation learning, 2021. URL https://github.com/\n",
      "vturrisi/solo-learn.\n",
      "\n",
      "Josip Djolonga, Frances Hubis, Matthias Minderer, Zachary Nado, Jeremy Nixon, Rob Romijn-\n",
      "ders, Dustin Tran, and Mario Lucic. Robustness Metrics, 2020. URL https://github.com/\n",
      "google-research/robustness_metrics.\n",
      "\n",
      "Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\n",
      "Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Syl-\n",
      "vain Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On Robustness and Transferabil-\n",
      "ity of Convolutional Neural Networks. arXiv:2007.08558 [cs], March 2021. URL http:\n",
      "//arxiv.org/abs/2007.08558. arXiv: 2007.08558.\n",
      "\n",
      "Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training\n",
      "examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference\n",
      "on computer vision and pattern recognition workshop, pages 178–178. IEEE, 2004.\n",
      "\n",
      "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv:1410.5401 [cs],\n",
      "\n",
      "December 2014. URL http://arxiv.org/abs/1410.5401. arXiv: 1410.5401.\n",
      "\n",
      "Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\n",
      "Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\n",
      "Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent -\n",
      "a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\n",
      "Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\n",
      "pages 21271–21284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\n",
      "paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\n",
      "\n",
      "Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\n",
      "Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti\n",
      "Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández\n",
      "del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy,\n",
      "Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming\n",
      "with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2.\n",
      "URL https://doi.org/10.1038/s41586-020-2649-2.\n",
      "\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\n",
      "Recognition. arXiv:1512.03385 [cs], December 2015. URL http://arxiv.org/abs/1512.\n",
      "03385. arXiv: 1512.03385.\n",
      "\n",
      "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for\n",
      "Unsupervised Visual Representation Learning. arXiv:1911.05722 [cs], March 2020. URL http:\n",
      "//arxiv.org/abs/1911.05722. arXiv: 1911.05722.\n",
      "\n",
      "Dan Hendrycks and Thomas Dietterich. Benchmarking Neural Network Robustness to Common\n",
      "Corruptions and Perturbations. September 2018. URL https://openreview.net/forum?id=\n",
      "HJz6tiCqYm.\n",
      "\n",
      "Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\n",
      "Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\n",
      "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization.\n",
      "arXiv:2006.16241 [cs, stat], July 2021. URL http://arxiv.org/abs/2006.16241. arXiv:\n",
      "2006.16241.\n",
      "\n",
      "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\n",
      "Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\n",
      "and maximization. In International Conference on Learning Representations, 2019. URL https:\n",
      "//openreview.net/forum?id=Bklr3j0cKX.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\n",
      "reducing internal covariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd\n",
      "International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning\n",
      "Research, pages 448–456, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.\n",
      "mlr.press/v37/ioffe15.html.\n",
      "\n",
      "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,\n",
      "Aaron Maschinot, Ce Liu, and Dilip Krishnan.\n",
      "In\n",
      "H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Ad-\n",
      "vances in Neural Information Processing Systems, volume 33, pages 18661–18673. Cur-\n",
      "ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n",
      "d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf.\n",
      "\n",
      "Supervised contrastive learning.\n",
      "\n",
      "Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representa-\n",
      "\n",
      "tion learning. CoRR, abs/1901.09005, 2019. URL http://arxiv.org/abs/1901.09005.\n",
      "\n",
      "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained\n",
      "categorization. In Proceedings of the IEEE international conference on computer vision workshops,\n",
      "pages 554–561, 2013.\n",
      "\n",
      "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n",
      "\n",
      "Kuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive\n",
      "Visual Representations. arXiv:2109.12909 [cs, math], September 2021. URL http://arxiv.\n",
      "org/abs/2109.12909. arXiv: 2109.12909.\n",
      "\n",
      "Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number\n",
      "of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing,\n",
      "pages 722–729, 2008. doi: 10.1109/ICVGIP.2008.47.\n",
      "\n",
      "Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In 2012\n",
      "IEEE Conference on Computer Vision and Pattern Recognition, pages 3498–3505, 2012. doi:\n",
      "10.1109/CVPR.2012.6248092.\n",
      "\n",
      "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\n",
      "Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\n",
      "Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\n",
      "Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\n",
      "high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-\n",
      "Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,\n",
      "pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n",
      "9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\n",
      "pdf.\n",
      "\n",
      "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\n",
      "hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\n",
      "E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,\n",
      "12:2825–2830, 2011.\n",
      "\n",
      "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classiﬁers\n",
      "In Proceedings of the 36th International Conference on Machine\n",
      "Generalize to ImageNet?\n",
      "Learning, pages 5389–5400. PMLR, May 2019. URL https://proceedings.mlr.press/\n",
      "v97/recht19a.html. ISSN: 2640-3498.\n",
      "\n",
      "Aaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive Learning of General-Purpose Audio\n",
      "Representations. arXiv:2010.10915 [cs, eess], October 2020. URL http://arxiv.org/abs/\n",
      "2010.10915. arXiv: 2010.10915.\n",
      "\n",
      "Aad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. Springer\n",
      "New York, 1996. doi: 10.1007/978-1-4757-2545-2. URL https://doi.org/10.1007%\n",
      "2F978-1-4757-2545-2.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
      "Kaiser, and Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762 [cs], December 2017.\n",
      "URL http://arxiv.org/abs/1706.03762. arXiv: 1706.03762 version: 5.\n",
      "\n",
      "Tan Wang, Zhongqi Yue, Jianqiang Huang, Qianru Sun, and Hanwang Zhang. Self-Supervised\n",
      "Learning Disentangled Group Representation as Feature. May 2021. URL https://openreview.\n",
      "net/forum?id=RQfcckT1M_4.\n",
      "\n",
      "Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\n",
      "Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on\n",
      "computer vision and pattern recognition, pages 3485–3492. IEEE, 2010.\n",
      "\n",
      "Asano Ym, Rupprecht C, and Vedaldi A. Self-labelling via simultaneous clustering and representation\n",
      "\n",
      "learning. September 2019. URL https://openreview.net/forum?id=Hyx-jyBFPr.\n",
      "\n",
      "Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\n",
      "contrastive learning with augmentations. CoRR, abs/2010.13902, 2020. URL https://arxiv.\n",
      "org/abs/2010.13902.\n",
      "\n",
      "Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\n",
      "\n",
      "learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "A Proof of Theorem 1\n",
      "\n",
      "Let us introduce additional notations used in the proofs. Deﬁne r = (z, y) ∈ R, (cid:96)(f, r) = l(f (z), y),\n",
      "\n",
      "˜Cy,k1,...,kL = {(z, ˆy) ∈ Z × Y : ˆy = y, kj = arg max\n",
      "\n",
      "zj,t ∀j ∈ [L]},\n",
      "\n",
      "t∈[V ]\n",
      "\n",
      "and\n",
      "\n",
      "˜Zk1,...,kL = {z ∈ Z : kj = arg max\n",
      "\n",
      "zj,t ∀j ∈ [L]}.\n",
      "\n",
      "t∈[V ]\n",
      "\n",
      "to be\n",
      "\n",
      "then deﬁne Ck\n",
      "\n",
      "=\n",
      "We\n",
      "the ﬂatten version of\n",
      "{ ˜Cy,k1,...,kL,y}y∈Y,k1,...,kL∈[V ] with C1 = ˜C1,1,...,1, C2 = ˜C2,1,...,1, C|Y| = ˜C|Y|,1,...,1, C|Y|+1 =\n",
      "˜C1,2,1,...,1, C2|Y| = ˜C|Y|,2,1,...,1, and so on. Similarly, deﬁne Zk to be the ﬂatten version of ˜Zk1,...,kL.\n",
      "We also use Qi = {q ∈ [−1, +1]V : i = arg maxj∈[V ] qj}, Ik := I S\n",
      "k := {i ∈ [n] : ri ∈ Ck}, and\n",
      "(cid:80)n\n",
      "αk(h) := Er[(cid:96)(h, r)|r ∈ Ck]. Moreover, we deﬁne ϕ(f S\n",
      "t=1 (cid:107)q − q(cid:48)(cid:107)2\n",
      "2,\n",
      "eqj /τ\n",
      "and ϕ(f S\n",
      "t=1 eqt/τ for\n",
      "j = 1, . . . , V .\n",
      "\n",
      "SEM(τ )) = supi∈[V ] supq,q(cid:48)∈Qi\n",
      "\n",
      "base) = supi∈[V ] supq,q(cid:48)∈Qi\n",
      "\n",
      "t=1 (cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "\n",
      "2 where στ (q)j =\n",
      "\n",
      "(cid:80)n\n",
      "\n",
      "i.e.,\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "{Ck}K\n",
      "\n",
      "˜Cy,k1,...,kL;\n",
      "\n",
      "We ﬁrst decompose the generalization gap into two terms using the following lemma:\n",
      "Lemma 1. For any δ > 0, with probability at least 1 − δ,the following holds for all h ∈ H:\n",
      "\n",
      "Er[(cid:96)(h, r)] −\n",
      "\n",
      "(cid:96)(h, ri) ≤\n",
      "\n",
      "|Ik|\n",
      "\n",
      "αk(h) −\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      " + c\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "Proof. We ﬁrst write the expected error as the sum of the conditional expected error:\n",
      "\n",
      "Er[(cid:96)(h, r)] =\n",
      "\n",
      "Er[(cid:96)(h, r)|r ∈ Ck] Pr(r ∈ Ck) =\n",
      "\n",
      "Erk [(cid:96)(h, rk)] Pr(r ∈ Ck),\n",
      "\n",
      "where rk is the random variable for the conditional with r ∈ Ck. Using this, we decompose the\n",
      "generalization error into two terms:\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "Er[(cid:96)(h, r)] −\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "=\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "Erk [(cid:96)(h, rk)]\n",
      "\n",
      "Pr(r ∈ Ck) −\n",
      "\n",
      "Erk [(cid:96)(h, rk)]\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "\n",
      "\n",
      "+\n",
      "\n",
      "\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      " .\n",
      "\n",
      "The second term in the right-hand side of (5) is further simpliﬁed by using\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:96)(h, ri) =\n",
      "\n",
      "(cid:96)(h, ri),\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "as\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "Erk [(cid:96)(h, rk)]\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:96)(h, ri) =\n",
      "\n",
      "|Ik|\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "Erk [(cid:96)(h, rk)] −\n",
      "\n",
      "\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "Substituting these into equation (5) yields\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "Er[(cid:96)(h, r)] −\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "=\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "Erk [(cid:96)(h, rk)]\n",
      "\n",
      "Pr(r ∈ Ck) −\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "+\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "Erk [(cid:96)(h, rk)] −\n",
      "\n",
      "|Ik|\n",
      "\n",
      "≤ B\n",
      "\n",
      "Pr(r ∈ Ck) −\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "+\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "Erk [(cid:96)(h, rk)] −\n",
      "\n",
      "|Ik|\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "By using the Bretagnolle-Huber-Carol inequality [van der Vaart and Wellner, 1996, A6.6 Proposition],\n",
      "we have that for any δ > 0, with probability at least 1 − δ,\n",
      "(cid:114)\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "k=1\n",
      "\n",
      "Pr(r ∈ Ck) −\n",
      "\n",
      "|Ik|\n",
      "n\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "≤\n",
      "\n",
      "2K ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "(cid:12)\n",
      "Here, notice that the term of (cid:80)K\n",
      "(cid:12)\n",
      "(cid:12) does not depend on h ∈ H. Moreover,\n",
      "note that for any (f, h, M ) such that M > 0 and B ≥ 0 for all X, we have that P(f (X) ≥ M ) ≥\n",
      "P(f (X) > M ) ≥ P(Bf (X) + h(X) > BM + h(X)), where the probability is with respect to the\n",
      "randomness of X. Thus, by combining (6) and (7), we have that for any h ∈ H, for any δ > 0, with\n",
      "probability at least 1 − δ, the following holds for all h ∈ H,\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)Pr(r ∈ Ck) − |Ik|\n",
      "(cid:12)\n",
      "\n",
      "k=1\n",
      "\n",
      "n\n",
      "\n",
      "(7)\n",
      "\n",
      "Er[(cid:96)(h, r)] −\n",
      "\n",
      "(cid:96)(h, ri) ≤\n",
      "\n",
      "|Ik|\n",
      "\n",
      "αk(h) −\n",
      "\n",
      "(cid:96)(h, ri)\n",
      "\n",
      " + c\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "In particular, the ﬁrst term from the previous lemma will be bounded with the following lemma:\n",
      "Lemma 2. For any f ∈ {f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "\n",
      "base},\n",
      "\n",
      "|Ik|\n",
      "\n",
      "αk(f ) −\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "(cid:96)(f, ri)\n",
      "\n",
      " ≤ R\n",
      "\n",
      "Lϕ(f )\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "Proof. By using the triangle inequality,\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "≤\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "\n",
      "Er[(cid:96)(f, r)|r ∈ Ck] −\n",
      "\n",
      "|Ik|\n",
      "\n",
      "\n",
      "\n",
      "(cid:96)(f, ri)\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "|Ik|\n",
      "\n",
      "Er[(cid:96)(f, r)|r ∈ Ck] −\n",
      "\n",
      "(cid:96)(f, ri)\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Furthermore, by using the triangle inequality,\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:96)(f, ri)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Er[(cid:96)(f, r)|r ∈ Ck] −\n",
      "\n",
      "Er[(cid:96)(f, r)|r ∈ Ck] −\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:96)(f, ri)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "=\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "≤ sup\n",
      "\n",
      "r,r(cid:48)∈Ck\n",
      "\n",
      "(cid:12)Er[(cid:96)(f, r)|r ∈ Ck] − (cid:96)(f, ri)(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "i∈Ik\n",
      "(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\n",
      "(cid:12)\n",
      "(cid:12) .\n",
      "\n",
      "SEM(τ ) ◦στ , since gS\n",
      "\n",
      "SEM(τ ) ∈ GS, by using the Lipschitz continuity, boundedness,\n",
      "\n",
      "If f = f S\n",
      "and non-negativity,\n",
      "\n",
      "SEM(τ ) = gS\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\n",
      "\n",
      "sup\n",
      "r,r(cid:48)∈Ck\n",
      "\n",
      "|(ly ◦ gS\n",
      "\n",
      "SEM(τ ))(στ (z)) − (ly ◦ gS\n",
      "\n",
      "SEM(τ ))(στ (z(cid:48)))|\n",
      "\n",
      "sup\n",
      "z,z(cid:48)∈Zk\n",
      "\n",
      "(cid:12) = sup\n",
      "y∈Y\n",
      "≤ R sup\n",
      "\n",
      "z,z(cid:48)∈Zk\n",
      "\n",
      "(cid:107)στ (z) − στ (z(cid:48))(cid:107)F\n",
      "\n",
      "(cid:118)\n",
      "(cid:117)\n",
      "(cid:117)\n",
      "(cid:116)\n",
      "\n",
      "L\n",
      "(cid:88)\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "j=1\n",
      "\n",
      "(στ (zt,j) − στ (z(cid:48)\n",
      "\n",
      "t,j))2\n",
      "2\n",
      "\n",
      "sup\n",
      "i∈[V ]\n",
      "\n",
      "sup\n",
      "q,q(cid:48)∈Qi\n",
      "\n",
      "(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "2\n",
      "\n",
      "= R sup\n",
      "\n",
      "z,z(cid:48)∈Zk\n",
      "\n",
      "L\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:118)\n",
      "(cid:117)\n",
      "(cid:117)\n",
      "(cid:116)\n",
      "\n",
      "(cid:115)\n",
      "\n",
      "≤ R\n",
      "\n",
      "= R\n",
      "\n",
      "Lϕ(f S\n",
      "\n",
      "SEM(τ ))\n",
      "n\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "base = gS\n",
      "\n",
      "Similarly, if f = f S\n",
      "and non-negativity,\n",
      "(cid:12)(cid:96)(f, r) − (cid:96)(f, r(cid:48))(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "base, since gS\n",
      "\n",
      "sup\n",
      "r,r(cid:48)∈Ck\n",
      "\n",
      "base ∈ GS, by using the Lipschitz continuity, boundedness,\n",
      "\n",
      "|(ly ◦ gS\n",
      "\n",
      "base)(z) − (ly ◦ gS\n",
      "\n",
      "base)(z(cid:48))|\n",
      "\n",
      "sup\n",
      "z,z(cid:48)∈Zk\n",
      "\n",
      "(cid:12) = sup\n",
      "y∈Y\n",
      "≤ R sup\n",
      "\n",
      "(cid:107)z − z(cid:48)(cid:107)F\n",
      "\n",
      "z,z(cid:48)∈Zk\n",
      "(cid:114)\n",
      "\n",
      "Lϕ(f S\n",
      "base)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "≤ R\n",
      "\n",
      "Therefore, for any f ∈ {f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "base},\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "|Ik|\n",
      "\n",
      "αk(f ) −\n",
      "\n",
      "(cid:96)(f, ri)\n",
      "\n",
      " ≤\n",
      "\n",
      "1\n",
      "|Ik|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i∈Ik\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "k=1\n",
      "\n",
      "|Ik|R(cid:112)Lϕ(f ) = R\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "Lϕ(f )\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "Combining Lemma 1 and Lemma 2, we obtain the following upper bound on the gap:\n",
      "Lemma 3. For any δ > 0, with probability at least 1 − δ,the following holds for any f ∈\n",
      "{f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "base}:\n",
      "\n",
      "Er[(cid:96)(f, r)] −\n",
      "\n",
      "(cid:96)(f, ri) ≤ R\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "Lϕ(f )\n",
      "n\n",
      "\n",
      "+ c\n",
      "\n",
      "ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "Proof. This follows directly from combining Lemma 1 and Lemma 2.\n",
      "\n",
      "We now provide an upper bound on ϕ(f S\n",
      "\n",
      "SEM(τ )) in the following lemma:\n",
      "\n",
      "Lemma 4. For any τ > 0,\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ ))\n",
      "n\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "+ (V − 1)\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      ".\n",
      "\n",
      "Proof. Recall the deﬁnition:\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ ))\n",
      "n\n",
      "\n",
      "= sup\n",
      "i∈[V ]\n",
      "\n",
      "sup\n",
      "q,q(cid:48)∈Qi\n",
      "\n",
      "(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "2.\n",
      "\n",
      "στ (q)j =\n",
      "\n",
      "eqj /τ\n",
      "t=1 eqt/τ\n",
      "\n",
      ",\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "where\n",
      "\n",
      "and\n",
      "\n",
      "for j = 1, . . . , V . By the symmetry and independence over i ∈ [V ] inside of the ﬁrst supremum, we\n",
      "have\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ ))\n",
      "n\n",
      "\n",
      "= sup\n",
      "\n",
      "q,q(cid:48)∈Q1\n",
      "\n",
      "(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "2.\n",
      "\n",
      "For any q, q(cid:48) ∈ Q1 and i ∈ {2, . . . , V } (with q = (q1, . . . , qV ) and q(cid:48) = (q(cid:48)\n",
      "δi, δ(cid:48)\n",
      "\n",
      "i > 0 such that\n",
      "\n",
      "1, . . . , q(cid:48)\n",
      "\n",
      "V )), there exists\n",
      "\n",
      "Here, since zik − ∆ ≥ zij from the assumption, we have that for all i ∈ {2, . . . , V },\n",
      "\n",
      "qi = q1 − δi\n",
      "\n",
      "i = q(cid:48)\n",
      "q(cid:48)\n",
      "\n",
      "1 − δ(cid:48)\n",
      "i.\n",
      "\n",
      "δi, δ(cid:48)\n",
      "\n",
      "i ≥ ∆ > 0.\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Thus, we can rewrite\n",
      "\n",
      "Similarly,\n",
      "\n",
      "Using these,\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "eqt/τ = eq1/τ +\n",
      "\n",
      "e(q1−δi)/τ\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "i=2\n",
      "\n",
      "= eq1/τ + eq1/τ\n",
      "\n",
      "e−δi/τ\n",
      "\n",
      "= eq1/τ\n",
      "\n",
      "1 +\n",
      "\n",
      "e−δi/τ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "i=2\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "i=2\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "i=2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "eq(cid:48)\n",
      "\n",
      "t/τ = eq(cid:48)\n",
      "\n",
      "1/τ\n",
      "\n",
      "1 +\n",
      "\n",
      "e−δ(cid:48)\n",
      "\n",
      "i/τ\n",
      "\n",
      " .\n",
      "\n",
      "στ (q)1 =\n",
      "\n",
      "eq1/τ\n",
      "t=1 eqt/τ\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "=\n",
      "\n",
      "eq1/τ\n",
      "1 + (cid:80)V\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "eq1/τ\n",
      "\n",
      "i=2 e−δi/τ\n",
      "\n",
      "(cid:17) =\n",
      "\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−δi/τ\n",
      "\n",
      "and for all j ∈ {2, . . . , V },\n",
      "\n",
      "στ (q)j =\n",
      "\n",
      "eqj /τ\n",
      "t=1 eqt/τ\n",
      "\n",
      "(cid:80)V\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "e(q1−δj )/τ\n",
      "1 + (cid:80)V\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "i=2 e−δi/τ\n",
      "\n",
      "eq1/τ\n",
      "\n",
      "e−δj /τ\n",
      "\n",
      "1 + (cid:80)V\n",
      "\n",
      "i=2 e−δi/τ\n",
      "1\n",
      "1 + eδj /τ + (cid:80)V\n",
      "\n",
      "e(δj −δi)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "στ (q(cid:48))1 =\n",
      "\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−δ(cid:48)\n",
      "\n",
      "i/τ\n",
      "\n",
      ",\n",
      "\n",
      "στ (q(cid:48))j =\n",
      "\n",
      "1\n",
      "j /τ + (cid:80)V\n",
      "\n",
      "1 + eδ(cid:48)\n",
      "\n",
      "e(δ(cid:48)\n",
      "\n",
      "j −δ(cid:48)\n",
      "\n",
      "i)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      ".\n",
      "\n",
      "where Ij := {2, . . . , V } \\ {j}. Similarly,\n",
      "\n",
      "and for all j ∈ {2, . . . , V },\n",
      "\n",
      "Using these, for any q, q(cid:48) ∈ Q1,\n",
      "\n",
      "|στ (q)1 − στ (q(cid:48))1| =\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "≤\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−δi/τ\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "i/τ\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−δ(cid:48)\n",
      "1\n",
      "1 + (cid:80)V\n",
      "i=2 e−∆/τ\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      ",\n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "≤\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "and for all j ∈ {2, . . . , V },\n",
      "\n",
      "|στ (q)j − στ (q(cid:48))j| =\n",
      "\n",
      "1\n",
      "1 + eδj /τ + (cid:80)V\n",
      "\n",
      "e(δj −δi)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "j /τ + (cid:80)V\n",
      "\n",
      "1 + eδ(cid:48)\n",
      "\n",
      "e(δ(cid:48)\n",
      "\n",
      "j −δ(cid:48)\n",
      "\n",
      "i)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "1\n",
      "1 + e∆/τ + (cid:80)V\n",
      "\n",
      "e(∆−2)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ + (cid:80)V\n",
      "\n",
      "e(2−∆)/τ\n",
      "\n",
      "i∈Ij\n",
      "\n",
      "1\n",
      "1 + e∆/τ + (V − 2)e(∆−2)/τ\n",
      "\n",
      "1\n",
      "1 + e2/τ + (V − 2)e(2−∆)/τ\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      ".\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "By combining these,\n",
      "\n",
      "sup\n",
      "q,q(cid:48)∈Q1\n",
      "\n",
      "(cid:107)στ (q) − στ (q(cid:48))(cid:107)2\n",
      "2\n",
      "\n",
      "= sup\n",
      "\n",
      "q,q(cid:48)∈Q1\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "|στ (q)j − στ (q(cid:48))j|2\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      ".\n",
      "\n",
      "+ (V − 1)\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "Using the previous lemma, we will conclude the asymptotic behavior of ϕ(f S\n",
      "lemma:\n",
      "Lemma 5. It holds that\n",
      "\n",
      "SEM(τ )) in the following\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) → 0 as τ → 0.\n",
      "\n",
      "Proof. Using Lemma 4,\n",
      "\n",
      "lim\n",
      "τ →0\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) ≤ lim\n",
      "τ →0\n",
      "\n",
      "n\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "+ n(V − 1) lim\n",
      "τ →0\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      ".\n",
      "\n",
      "lim\n",
      "τ →0\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "=\n",
      "\n",
      "−\n",
      "\n",
      "= 0,\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "1\n",
      "\n",
      "lim\n",
      "τ →0\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "= |0 − 0|2 = 0.\n",
      "\n",
      "(cid:12)\n",
      "2\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Moreover,\n",
      "\n",
      "and\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Therefore,\n",
      "\n",
      "Since ϕ(f S\n",
      "\n",
      "SEM(τ )) ≥ 0, this implies the statement of this lemma.\n",
      "\n",
      "lim\n",
      "τ →0\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) ≤ 0.\n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "As we have analyzed ϕ(f S\n",
      "SEM(τ )) and ϕ(f S\n",
      "ϕ(f S\n",
      "Lemma 6. For any τ > 0,\n",
      "\n",
      "SEM(τ )) in the previous two lemmas, we are now ready to compare\n",
      "\n",
      "base), which is done in the following lemma:\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) − ϕ(f S\n",
      "\n",
      "base) ≤\n",
      "\n",
      "(1 − V ) < 0.\n",
      "\n",
      "3n\n",
      "4\n",
      "\n",
      "Proof. From Lemma 4, for any τ > 0,\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) ≤ n\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−∆/τ\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "1\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "1\n",
      "(cid:12)\n",
      "(cid:18) 1\n",
      "1\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "1\n",
      "1 + e∆/τ (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2)e−∆/τ )\n",
      "\n",
      "≤ n\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + (V − 1)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "1\n",
      "1 + (1 + (V − 2)e−2/τ )\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (1 + (V − 2))\n",
      "\n",
      "(cid:12)\n",
      "2\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "= n\n",
      "\n",
      "1\n",
      "1 + (V − 1)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "1\n",
      "2 + (V − 2)e−2/τ\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "1 + e2/τ (V − 1)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "≤ n\n",
      "\n",
      "−\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "= n\n",
      "\n",
      "−\n",
      "\n",
      "+ n(V − 1)\n",
      "\n",
      "1\n",
      "V\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:19)2\n",
      "\n",
      "1\n",
      "V\n",
      "\n",
      "1\n",
      "V\n",
      "\n",
      "(cid:12)\n",
      "2\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "− 0\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      ".\n",
      "\n",
      "(cid:12)\n",
      "2\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "Recall the deﬁnition of\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "base) = sup\n",
      "i∈[V ]\n",
      "\n",
      "sup\n",
      "q,q(cid:48)∈Qi\n",
      "\n",
      "n(cid:107)q − q(cid:48)(cid:107)2\n",
      "2.\n",
      "\n",
      "By choosing an element in the set over which the supremum is taken, for any δ ≥ ∆ > 0,\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "base) ≥ sup\n",
      "\n",
      "n(cid:107)q − q(cid:48)(cid:107)2\n",
      "\n",
      "2 ≥ n(cid:107)ˆq − ˆq(cid:48)(cid:107)2\n",
      "\n",
      "2 = n\n",
      "\n",
      "(ˆqj − ˆq(cid:48)\n",
      "\n",
      "j)2\n",
      "\n",
      "2 = n(2 − δ)2V,\n",
      "\n",
      "q,q(cid:48)∈Q1\n",
      "\n",
      "V\n",
      "(cid:88)\n",
      "\n",
      "j=1\n",
      "\n",
      "where ˆq1 = 1, ˆqj = 1 − δ for j ∈ {2, . . . , V }, ˆq(cid:48)\n",
      "By combining those, for for any τ > 0 and δ ≥ ∆ > 0,\n",
      "\n",
      "1 = δ − 1, and ˆq(cid:48)\n",
      "\n",
      "j = −1 for j ∈ {2, . . . , V }.\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) − ϕ(f S\n",
      "n\n",
      "\n",
      "base)\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:18) 1\n",
      "1\n",
      "\n",
      "(cid:19)2\n",
      "\n",
      "1\n",
      "V\n",
      "\n",
      "+ (V − 1)\n",
      "\n",
      "− (2 − δ)2V\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      "≤ 1 +\n",
      "\n",
      "V −\n",
      "\n",
      "− (2 − δ)2V\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "+\n",
      "\n",
      "− V\n",
      "\n",
      "V − (2 − δ)2V\n",
      "(cid:18)\n",
      "\n",
      "(2 − δ)2 −\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "≤\n",
      "\n",
      "− V\n",
      "\n",
      "1 −\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      "=\n",
      "\n",
      "(1 − V )\n",
      "\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "4\n",
      "1\n",
      "4\n",
      "\n",
      "18\n",
      "\n",
      "\f",
      "We combine the lemmas above to prove Theorem 1, which is restated below with its proof:\n",
      "\n",
      "Theorem 1. Let V ≥ 2. For any δ > 0, with probability at least 1 − δ, the following holds for any\n",
      "fS ∈ {f S\n",
      "\n",
      "SEM(τ ), f S\n",
      "\n",
      "base}:\n",
      "\n",
      "Ez,y[l(fS(z), y)] ≤\n",
      "\n",
      "l(fS(z(i)), y(i)) + R\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "Lϕ(fS)\n",
      "n\n",
      "\n",
      "+ c\n",
      "\n",
      "ln(2/δ)\n",
      "n\n",
      "\n",
      ".\n",
      "\n",
      "Moreover,\n",
      "\n",
      "ϕ(f S\n",
      "\n",
      "SEM(τ )) → 0 as τ → 0\n",
      "\n",
      "and ϕ(f S\n",
      "\n",
      "SEM(τ )) − ϕ(f S\n",
      "\n",
      "base) ≤\n",
      "\n",
      "(1 − V ) < 0 ∀τ > 0.\n",
      "\n",
      "3n\n",
      "4\n",
      "\n",
      "Proof. The ﬁrst statement directly follows from Lemma 3. The second statement is proven by\n",
      "Lemma 5 and Lemma 6.\n",
      "\n",
      "B Hyperparameters\n",
      "\n",
      "We present the hype-parameters used to train for BYOL+SEM on CIFAR100. The same parameters\n",
      "were used for ResNet18 and ResNet50.\n",
      "\n",
      "Learning rate\n",
      "Weight-decay\n",
      "Optimizer\n",
      "BYOL EMA\n",
      "Vocabulary size (V)\n",
      "Message length (L)\n",
      "τ online network\n",
      "τ target network\n",
      "\n",
      "0.5\n",
      "1e-4\n",
      "AdamW\n",
      "0.99\n",
      "13\n",
      "5000\n",
      "0.5\n",
      "0.5\n",
      "\n",
      "(a) BYOL+SEM hyper-parameters.\n",
      "\n",
      "C Experiment details for ImageNet\n",
      "\n",
      "C.1\n",
      "\n",
      "Image augmentation\n",
      "\n",
      "We follow the same procedure as [Grill et al., 2020] for the image augmentation procedure. The\n",
      "augmentation applied in order during training are:\n",
      "\n",
      "• Random Resize crop to a 224 × 224 image. A random patch of the image is selected and\n",
      "\n",
      "resized to a 224 × 224 image.\n",
      "\n",
      "• Random color jitter. Modifying the brightness, the contrast, the saturation and the hue.\n",
      "\n",
      "• Random gray scale. Randomly applying a gray scale ﬁlter to the image\n",
      "\n",
      "• Random gaussian blur. Randomly applying a gaussian blue ﬁlter.\n",
      "\n",
      "• Random solarization. Randomly applying a solarization ﬁlter.\n",
      "\n",
      "At validation and test time, we resize the images to 256 × 256 and then center crop a patch of\n",
      "224 × 224.\n",
      "\n",
      "For both training and evaluation, we re-normalize the image using the statistic of the training set.\n",
      "\n",
      "C.2 Hyper-parameters\n",
      "\n",
      "We summarize the hyper-parameters for BYOL with SEM and MoCo with SEM in table 4.\n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "Learning rate\n",
      "Batch size\n",
      "Weight-decay\n",
      "Optimizer\n",
      "Epochs\n",
      "Base momentum\n",
      "Vocabulary size (V)\n",
      "Message length (L)\n",
      "τ online network\n",
      "τ target network\n",
      "\n",
      "0.9\n",
      "256\n",
      "1e-6\n",
      "SGD with lars\n",
      "100\n",
      "0.99\n",
      "29\n",
      "465\n",
      "2.397\n",
      "2.386\n",
      "\n",
      "(a) BYOL+SEM hyper-parameters.\n",
      "\n",
      "Learning rate\n",
      "Batch size\n",
      "Weight-decay\n",
      "Optimizer\n",
      "Epochs\n",
      "MoCo’s EMA\n",
      "Vocabulary size (V)\n",
      "Message Length (L)\n",
      "τ online network\n",
      "τ target network\n",
      "\n",
      "0.6\n",
      "256\n",
      "3e-5\n",
      "SGD with lars\n",
      "100\n",
      "0.1\n",
      "12\n",
      "512\n",
      "1.35\n",
      "1.2\n",
      "\n",
      "(b) MoCo+SEM hyper-parameters.\n",
      "\n",
      "Table 4: ImageNet’s experiment hyper-parameters.\n",
      "\n",
      "C.3 Linear evaluation\n",
      "\n",
      "We follow the evaluation protocol from [Chen et al., 2020b]. The linear evaluation is done by training\n",
      "a linear classiﬁer on the frozen representation of the ImageNet training samples. We train a linear\n",
      "classiﬁer with a cross-entropy objective for 100 epochs using SGD with nesterov and a batch size of\n",
      "512. During training, we apply random resized crop and random horizontal ﬂip.\n",
      "\n",
      "C.4 Semi-supervised learning\n",
      "\n",
      "We perform semi-supervised experiments by training a linear classiﬁer on top of a frozen repre-\n",
      "sentation. The procedure is the same as the linear evaluation procedure with the exception that we\n",
      "train with 1% of the training sample. That training samples are taken according to the split deﬁned\n",
      "in [Chen et al., 2020b].\n",
      "\n",
      "C.5 Robustness experiments\n",
      "\n",
      "We follow the evaluation procedure from [Lee et al., 2021]. We treated the robustness datasets as\n",
      "additional \"test sets\" in that we simply evaluated them using the evaluation procedure described\n",
      "above. The images were resized to a 256 × 256 before being center cropped to a 224 × 224 image.\n",
      "The evaluation procedure was performed using the public robustness benchmark evaluation code\n",
      "of [Djolonga et al., 2020]2.\n",
      "\n",
      "C.6 Transfer learning experiments\n",
      "\n",
      "We follow the linear evaluation protocol of [Kolesnikov et al., 2019; Chen et al., 2020b] We train a\n",
      "linear classiﬁer using a regularized multinomial logistic regression from the scikit-learn package [Pe-\n",
      "dregosa et al., 2011]. The representation is frozen, so that we do not train the encoder backbone nor\n",
      "\n",
      "2https://github.com/google-research/robustness_metrics\n",
      "\n",
      "20\n",
      "\n",
      "\f",
      "Superclass\n",
      "aquatic mammals\n",
      "ﬁsh\n",
      "ﬂowers\n",
      "food containers\n",
      "fruit and vegetables\n",
      "household electrical devices\n",
      "household furniture\n",
      "insects\n",
      "large carnivores\n",
      "large man-made outdoor things\n",
      "large natural outdoor scenes\n",
      "large omnivores and herbivores\n",
      "medium-sized mammals\n",
      "non-insect invertebrates\n",
      "people\n",
      "reptiles\n",
      "small mammals\n",
      "trees\n",
      "vehicles 1\n",
      "vehicles 2\n",
      "\n",
      "Classes\n",
      "beaver, dolphin, otter, seal, whale\n",
      "aquarium ﬁsh, ﬂatﬁsh, ray, shark, trout\n",
      "orchids, poppies, roses, sunﬂowers, tulips\n",
      "bottles, bowls, cans, cups, plates\n",
      "apples, mushrooms, oranges, pears, sweet peppers\n",
      "clock, computer keyboard, lamp, telephone, television\n",
      "bed, chair, couch, table, wardrobe\n",
      "bee, beetle, butterﬂy, caterpillar, cockroach\n",
      "bear, leopard, lion, tiger, wolf\n",
      "bridge, castle, house, road, skyscraper\n",
      "cloud, forest, mountain, plain, sea\n",
      "camel, cattle, chimpanzee, elephant, kangaroo\n",
      "fox, porcupine, possum, raccoon, skunk\n",
      "crab, lobster, snail, spider, worm\n",
      "baby, boy, girl, man, woman\n",
      "crocodile, dinosaur, lizard, snake, turtle\n",
      "hamster, mouse, rabbit, shrew, squirrel\n",
      "maple, oak, palm, pine, willow\n",
      "bicycle, bus, motorcycle, pickup truck, train\n",
      "lawn-mower, rocket, streetcar, tank, tractor\n",
      "\n",
      "Table 5: Set of classes for each superclass on CIFAR-100.\n",
      "\n",
      "the batch-normalization statistics. We do not perform any augmentations and the images are resized\n",
      "to 224 pixels using bicubic resampling and the normalized using the statistics on ImageNet’s training\n",
      "set. We tune the regularizer term from a range of 11 logarithmically-spaced values between 10−6 and\n",
      "105 using a small validation set and re-train using the full training set.\n",
      "\n",
      "D CIFAR100 superclass\n",
      "\n",
      "The 100 classes of CIFAR-100 [Krizhevsky, 2009] are grouped into 20 superclasses. The list of\n",
      "superclass for each class in Table 5\n",
      "\n",
      "E Additional CIFAR-100 relevance graphs\n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "(a) BYOL baseline\n",
      "\n",
      "(b) BYOL baseline with a large rep-\n",
      "resentation\n",
      "\n",
      "(c) BYOL + SEM\n",
      "\n",
      "Figure 5: Comparison of the full relevance graph W5 between BYOL and BYOL + SEM.\n",
      "\n",
      "22\n",
      "\n",
      "S363otterkangaroobottleS382lampS225beartankS144streetcarroadS249S167trainS94lobsterS273boygirlhousewomanS29S106cockroachS6appledolphinsealwhaleaquarium_fishflatfishraysharkorchidpoppyrosesunflowertulipbowlcancupplatemushroomorangepearsweet_pepperclockcomputer_keyboardtelephonetelevisionbedchaircouchtablewardrobebeebeetlebutterflycaterpillarleopardtigerwolfbridgecastleskyscrapercloudforestmountainplainseacamelcattlechimpanzeefoxpossumraccoonskunkcrabsnailwormbabymanlizardsnaketurtlehamstermouserabbitshrewsquirrelmaple_treeoak_treepalm_treewillow_treebicyclebusmotorcyclepickup_trucklawn_mowerrockettractorS55S63S47S279S162S69S134S242S34S241S0S98S158S361S91S66S12S95S388S53S374S190S22S59S183S261S1S375S27S35S130S110S127S43S111S258S380S214S30S85S292S193S238S336S128S132S284S14S99S93S332S236S298S260S281S68S204S161S182S177S381S64S227S320S318S266S230S196S126S181S184S114S187flatfishtableS16troutcaterpillarS194cattleS276orchidcomputer_keyboardS229palm_treeS184cloudrocketskunkchimpanzeeS256S187pickup_trucksnailcameltigerspiderS307dolphinsweet_pepperS85lionS434S415lobstersunflowermushroomS66orangesquirrelS189S319manS190snakeS195babywardrobewolfsealwillow_treecanS323maple_treeS252S404turtleleopardhamsterrabbitcockroachS353clockforestS87S61pine_treeS369couchS397tankotterwomanbicycleS365S148bedbeavertulipbeelawn_mowercrocodilelamptelephoneS435tractorS422oak_treeS233motorcycleshrewS284S33S146S13plainS78whalebusbeeS248spiderbeetlecockroachS178S240bearkangarootigerS393leopardhousecastleS40roadS107plainS403motorcyclelawn_mowertankS350tractorsealS61beaverS284porcupinebedS168couchS360chairlionfoxS216wolfS272skyscraperS227mountainS78seasnakeS98S49S219S2lizardwormorangeS185S30sweet_pepperpearS198S193applebusS18S133streetcarS370S106pickup_trucktrainroseS306poppyS289S329tulipS334S135orchidS210cancupS31S259S321S291bowlplatebottlewillow_treepine_treemaple_treeoak_treeforestS71palm_treeS138S327S405dolphinrayS151S347S17S379turtlesharkS176S189S355whaleS381S221S200S22boyS157girlwomanbabyS204flatfishmanmouseraccoonS336S271S300possumsquirrelS239S128shrewhamsterrabbit\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#First filter to have only the References from a pdf\n",
    "\n",
    "def after_references(mypdftext): \n",
    "    keyword1 = 'References'\n",
    "    keyword2 = 'REFERENCES'\n",
    "    keyword3 = 'R EFERENCES'\n",
    "    keyword4 = 'Reference'\n",
    "    keyword5='[1]' \n",
    "\n",
    "    if keyword1 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword1)\n",
    "    elif keyword2 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword2)\n",
    "    elif keyword3 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword3)\n",
    "    elif keyword4 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword4)\n",
    "    elif keyword5 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword5)\n",
    "    else:\n",
    "        after_keyword = mypdftext[:10000]\n",
    "    return after_keyword\n",
    "\n",
    "#All references in a variable\n",
    "\n",
    "references=after_references(mypdftext)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439d489",
   "metadata": {},
   "source": [
    "## Preprocess to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e325730d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First cleaning\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns): \n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s) \n",
    "        return s\n",
    "\n",
    "replacer=RegexpReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e46e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess a text before using IA\n",
    "\n",
    "def preprocess_text(test):\n",
    "\n",
    "    #Removing Numbers\n",
    "    test=re.sub(r'\\d+','',test)\n",
    "\n",
    "    #Removing Letter alone\n",
    "    test = re.sub(r'\\b\\w\\b', ' ', test)\n",
    "\n",
    "    #Removing white spaces\n",
    "    test=test.strip()\n",
    "    \n",
    "    #Replacer replace\n",
    "    text_replaced = replacer.replace(test)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text_replaced)\n",
    "\n",
    "    #Tokenize words\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = tokenizer.tokenize(sentences[i])\n",
    "\n",
    "    #Remove stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stops=set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [word for word in sentences[i] if word not in stops]\n",
    "\n",
    "    #Lemmatize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer_output=WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            sentences[i][j] = lemmatizer_output.lemmatize(sentences[i][j])\n",
    "\n",
    "\n",
    "    #Join the words back into a sentence.\n",
    "    a=[' '.join(s) for s in sentences]\n",
    "    b=', '.join(a)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c077ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Arjovsky Léon Bottou Ishaan Gulrajani David Lopez Paz, Invariant Risk Minimization, arXiv, c stat March, URL http arxiv org ab arXiv Dzmitry Bahdanau Kyunghyun Cho Yoshua Bengio, Neural Machine Translation Jointly Learning Align Translate, arXiv, c stat May, URL http arxiv, org ab arXiv Lukas Bossard Matthieu Guillaumin Luc Van Gool, Food mining discriminative com ponents random forest, In David Fleet Tomas Pajdla Bernt Schiele Tinne Tuytelaars editor Computer Vision ECCV page Cham, Springer International Publishing, ISBN, Léon Bottou Frank, Curtis Jorge Nocedal, Optimization Methods Large Scale Machine Learning, arXiv, c math stat February, URL http arxiv org ab arXiv Xavier Bouthillier Christos Tsirigotis François Corneau Tremblay Thomas Schweizer Lin Dong Pierre Delaunay Fabrice Normandin Mirko Bronzi Dendi Suhubdy Reyhane Askari Michael Noukhovitch Chao Xue Satya Ortiz Gagné Olivier Breuleux Arnaud Bergeron Olexa Bilaniuk Steven Bocco Hadrien Bertrand Guillaume Alain Dmitriy Serdyuk Peter Henderson Pascal Lamblin Christopher Beckham, Epistimio orion Asynchronous Distributed Hyperparameter Optimization March, URL http doi org zenodo Mathilde Caron Piotr Bojanowski Armand Joulin Matthijs Douze, Deep Clustering Unsupervised Learning Visual Features, arXiv, c March, URL http arxiv org ab arXiv Mathilde Caron Ishan Misra Julien Mairal Priya Goyal Piotr Bojanowski Armand Joulin, Unsupervised learning visual feature contrasting cluster assignment, , Tianlong Chen Sijia Liu Shiyu Chang Yu Cheng Lisa Amini Zhangyang Wang, Adversarial Robustness From Self Supervised Pre Training Fine Tuning, page, URL http openaccess thecvf com content_CVPR_ html Chen_Adversarial_ Robustness_From_Self Supervised_Pre Training_to_Fine Tuning_CVPR__ paper html, Ting Chen Simon Kornblith Mohammad Norouzi Geoffrey Hinton, simple framework contrastive learning visual representation, In Hal Daumé III Aarti Singh editor Proceed ings th International Conference Machine Learning volume Proceedings Machine Learning Research page, PMLR Jul, Xinlei Chen Kaiming He, Exploring simple siamese representation learning, arXiv preprint arXiv, Mircea Cimpoi Subhransu Maji Iasonas Kokkinos Sammy Mohamed Andrea Vedaldi, De scribing texture wild, In Proceedings IEEE Conference Computer Vision Pattern Recognition page, Victor, Turrisi da Costa Enrico Fini Moin Nabi Nicu Sebe Elisa Ricci, Solo learn library self supervised method visual representation learning, URL http github com vturrisi solo learn, Josip Djolonga Frances Hubis Matthias Minderer Zachary Nado Jeremy Nixon Rob Romijn ders Dustin Tran Mario Lucic, Robustness Metrics, URL http github com google research robustness_metrics, Josip Djolonga Jessica Yung Michael Tschannen Rob Romijnders Lucas Beyer Alexander Kolesnikov Joan Puigcerver Matthias Minderer Alexander Amour Dan Moldovan Syl vain Gelly Neil Houlsby Xiaohua Zhai Mario Lucic, On Robustness Transferabil ity Convolutional Neural Networks, arXiv, c March, URL http arxiv org ab arXiv Li Fei Fei Rob Fergus Pietro Perona, Learning generative visual model training example An incremental bayesian approach tested object category, In conference computer vision pattern recognition workshop page, IEEE, Alex Graves Greg Wayne Ivo Danihelka, Neural Turing Machines, arXiv, c December, URL http arxiv org ab arXiv Jean Bastien Grill Florian Strub Florent Altché Corentin Tallec Pierre Richemond Elena Buchatskaya Carl Doersch Bernardo Avila Pires Zhaohan Guo Mohammad Gheshlaghi Azar Bilal Piot koray kavukcuoglu Remi Munos Michal Valko, Bootstrap latent new approach self supervised learning, In, Larochelle, Ranzato, Hadsell, , Balcan, Lin editor Advances Neural Information Processing Systems volume page, Curran Associates Inc, URL http proceeding neurips cc paper file fadadceebbbe Paper pdf, Charles, Harris, Jarrod Millman Stéfan, van der Walt Ralf Gommers Pauli Virtanen David Cournapeau Eric Wieser Julian Taylor Sebastian Berg Nathaniel, Smith Robert Kern Matti Picus Stephan Hoyer Marten, van Kerkwijk Matthew Brett Allan Haldane Jaime Fernández del Río Mark Wiebe Pearu Peterson Pierre Gérard Marchant Kevin Sheppard Tyler Reddy Warren Weckesser Hameer Abbasi Christoph Gohlke Travis, Oliphant, Array programming NumPy, Nature September, doi, URL http doi org, Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun, Deep Residual Learning Image Recognition, arXiv, c December, URL http arxiv org ab, , arXiv Kaiming He Haoqi Fan Yuxin Wu Saining Xie Ross Girshick, Momentum Contrast Unsupervised Visual Representation Learning, arXiv, c March, URL http arxiv org ab arXiv Dan Hendrycks Thomas Dietterich, Benchmarking Neural Network Robustness Common Corruptions Perturbations, September, URL http openreview net forum id HJztiCqYm, Dan Hendrycks Steven Basart Norman Mu Saurav Kadavath Frank Wang Evan Dorundo Rahul Desai Tyler Zhu Samyak Parajuli Mike Guo Dawn Song Jacob Steinhardt Justin Gilmer, The Many Faces Robustness Critical Analysis Out Distribution Generalization, arXiv, c stat July, URL http arxiv org ab arXiv Devon Hjelm Alex Fedorov Samuel Lavoie Marchildon Karan Grewal Phil Bachman Adam Trischler Yoshua Bengio, Learning deep representation mutual information estimation maximization, In International Conference Learning Representations, URL http openreview net forum id BklrjcKX, Sergey Ioffe Christian Szegedy, Batch normalization Accelerating deep network training reducing internal covariate shift, In Francis Bach David Blei editor Proceedings nd International Conference Machine Learning volume Proceedings Machine Learning Research page Lille France Jul, PMLR, URL http proceeding, mlr press ioffe html, Prannay Khosla Piotr Teterwak Chen Wang Aaron Sarna Yonglong Tian Phillip Isola Aaron Maschinot Ce Liu Dilip Krishnan, In, Larochelle, Ranzato, Hadsell, , Balcan, Lin editor Ad vances Neural Information Processing Systems volume page, Cur ran Associates Inc, URL http proceeding neurips cc paper file daccabbdbabfaaaf Paper pdf, Supervised contrastive learning, Alexander Kolesnikov Xiaohua Zhai Lucas Beyer, Revisiting self supervised visual representa tion learning, CoRR ab, URL http arxiv org ab Jonathan Krause Michael Stark Jia Deng Li Fei Fei, object representation ﬁne grained categorization, In Proceedings IEEE international conference computer vision workshop page, Alex Krizhevsky, Learning multiple layer feature tiny image, Technical report, Kuang Huei Lee Anurag Arnab Sergio Guadarrama John Canny Ian Fischer, Compressive Visual Representations, arXiv, c math September, URL http arxiv, org ab arXiv Maria Elena Nilsback Andrew Zisserman, Automated ﬂower classiﬁcation large number class, In Sixth Indian Conference Computer Vision Graphics Image Processing page, doi ICVGIP Omkar Parkhi Andrea Vedaldi Andrew Zisserman, , Jawahar, Cats dog, In IEEE Conference Computer Vision Pattern Recognition page, doi CVPR Adam Paszke Sam Gross Francisco Massa Adam Lerer James Bradbury Gregory Chanan Trevor Killeen Zeming Lin Natalia Gimelshein Luca Antiga Alban Desmaison Andreas Kopf Edward Yang Zachary DeVito Martin Raison Alykhan Tejani Sasank Chilamkurthy Benoit Steiner Lu Fang Junjie Bai Soumith Chintala, Pytorch An imperative style high performance deep learning library, In, Wallach, Larochelle, Beygelzimer, Alché Buc, Fox, Garnett editor Advances Neural Information Processing Systems page, Curran Associates Inc, URL http paper neurips cc paper pytorch imperative style high performance deep learning library, pdf, , Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Pretten hofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, Scikit learn Machine learning Python, Journal Machine Learning Research, Benjamin Recht Rebecca Roelofs Ludwig Schmidt Vaishaal Shankar, Do ImageNet Classiﬁers In Proceedings th International Conference Machine Generalize ImageNet, Learning page, PMLR May, URL http proceeding mlr press rechta html, ISSN, Aaqib Saeed David Grangier Neil Zeghidour, Contrastive Learning General Purpose Audio Representations, arXiv, c ee October, URL http arxiv org ab arXiv Aad, van der Vaart Jon, Wellner, Weak Convergence Empirical Processes, Springer New York, doi, URL http doi org, Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan, Gomez Lukasz Kaiser Illia Polosukhin, Attention Is All You Need, arXiv, c December, URL http arxiv org ab arXiv, version, Tan Wang Zhongqi Yue Jianqiang Huang Qianru Sun Hanwang Zhang, Self Supervised Learning Disentangled Group Representation Feature, May, URL http openreview, net forum id RQfcckTM_, Jianxiong Xiao James Hays Krista Ehinger Aude Oliva Antonio Torralba, Sun database Large scale scene recognition abbey zoo, In IEEE computer society conference computer vision pattern recognition page, IEEE, Asano Ym Rupprecht Vedaldi, Self labelling via simultaneous clustering representation learning, September, URL http openreview net forum id Hyx jyBFPr, Yuning You Tianlong Chen Yongduo Sui Ting Chen Zhangyang Wang Yang Shen, Graph contrastive learning augmentation, CoRR ab, URL http arxiv, org ab Jure Zbontar Li Jing Ishan Misra Yann LeCun Stéphane Deny, Barlow twin Self supervised learning via redundancy reduction, arXiv preprint arXiv, Proof Theorem Let u introduce additional notation used proof, Deﬁne cid Cy kL ˆy ˆy kj arg max zj Zk kL kj arg max zj, deﬁne Ck We ﬂatten version Cy kL kL, Similarly deﬁne Zk ﬂatten version Zk kL, We also use Qi arg maxj qj Ik ri Ck cid αk Er cid Ck, Moreover deﬁne cid cid cid eqj eqt, , , , SEM supi supq cid Qi base supi supq cid Qi cid στ στ cid cid στ cid, cid Ck Cy kL We ﬁrst decompose generalization gap two term using following lemma Lemma, For probability least following hold Er cid cid ri Ik αk cid ri cid cid ln, Proof, We ﬁrst write expected error sum conditional expected error Er cid Er cid Ck Pr Ck Erk cid rk Pr Ck rk random variable conditional Ck, Using decompose generalization error two term Ik cid Ik cid Er cid cid ri cid Erk cid rk Pr Ck Erk cid rk cid Ik cid Ik cid cid ri, The second term right hand side simpliﬁed using cid cid cid cid cid cid ri cid ri cid cid Ik cid Erk cid rk Ik cid cid ri Ik cid Erk cid rk cid ri Ik cid Ik Substituting equation yield cid Er cid cid ri cid cid Erk cid rk Pr Ck cid Ik cid Erk cid rk Ik Pr Ck cid cid cid cid cid Ik cid cid cid cid cid Erk cid rk Ik Ik cid Ik cid ri cid ri Ik cid Ik By using Bretagnolle Huber Carol inequality van der Vaart Wellner, Proposition probability least cid cid cid cid cid cid Pr Ck Ik cid cid cid cid ln, cid Here notice term cid cid cid depend, Moreover note Bf BM probability respect randomness, Thus combining probability least following hold cid cid Pr Ck Ik cid Er cid cid ri Ik αk cid ri cid Ik cid Ik cid ln, cid In particular ﬁrst term previous lemma bounded following lemma Lemma, For SEM base Ik αk cid cid cid ri Lϕ, Ik cid Ik Proof, By using triangle inequality cid cid cid cid cid cid cid cid Er cid Ck Ik cid ri Ik cid Ik Ik Er cid Ck cid ri, Ik cid Ik cid cid cid cid cid cid Furthermore using triangle inequality cid cid cid cid cid cid Ik cid Ik cid cid cid cid ri cid cid cid Er cid Ck Er cid Ck Ik cid Ik cid cid cid cid ri cid cid cid cid cid cid cid cid cid Ik Ik cid Ik cid sup cid Ck cid Er cid Ck cid ri cid cid cid Ik cid cid cid cid cid cid cid, SEM στ since gS SEM GS using Lipschitz continuity boundedness If non negativity SEM gS cid cid cid cid cid cid sup cid Ck ly gS SEM στ ly gS SEM στ cid sup cid Zk cid sup sup cid Zk cid στ στ cid cid cid cid cid cid cid cid στ zt στ cid sup sup cid Qi cid στ στ cid cid sup cid Zk cid cid cid cid cid cid Lϕ SEM base gS Similarly non negativity cid cid cid cid cid cid base since gS sup cid Ck base GS using Lipschitz continuity boundedness ly gS base ly gS base cid sup cid Zk cid sup sup cid cid cid cid Zk cid Lϕ base, Therefore SEM base cid Ik αk cid ri Ik cid Ik cid Ik cid Lϕ cid Lϕ, Combining Lemma Lemma obtain following upper bound gap Lemma, For probability least following hold SEM base Er cid cid ri cid cid cid Lϕ ln, Proof, This follows directly combining Lemma Lemma, We provide upper bound SEM following lemma Lemma, For SEM cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid, Proof, Recall deﬁnition SEM sup sup cid Qi cid στ στ cid cid, στ eqj eqt cid, , , , By symmetry independence inside ﬁrst supremum SEM sup cid cid στ στ cid cid, For cid, , , , , , qV cid cid δi cid, , , cid exists Here since zik zij assumption, , , qi δi cid cid cid, δi cid, Thus rewrite Similarly Using cid cid eqt eq δi cid eq eq δi eq δi cid cid cid eq cid eq cid cid, στ eq eqt cid eq cid cid eq δi cid cid δi, , , στ eqj eqt cid δj cid cid cid δi eq δj cid δi eδj cid δj δi Ij στ cid cid cid στ cid cid eδ cid cid cid Ij, Ij, , , , Similarly, , , Using cid στ στ cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid δi cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid, , , στ στ cid eδj cid δj δi Ij cid eδ cid cid cid Ij cid Ij cid Ij, By combining sup cid cid στ στ cid cid sup cid cid στ στ cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid, Using previous lemma conclude asymptotic behavior lemma Lemma, It hold SEM following SEM, Proof, Using Lemma lim SEM lim cid cid cid cid cid lim cid cid cid cid cid cid cid cid cid cid, lim cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid lim, cid cid cid cid cid Moreover cid cid cid cid cid Therefore Since SEM implies statement lemma, lim SEM, As analyzed SEM Lemma, For SEM previous two lemma ready compare base done following lemma SEM base, Proof, From Lemma SEM cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid cid, cid cid cid cid cid cid cid cid cid cid Recall deﬁnition base sup sup cid Qi cid cid cid, By choosing element set supremum taken base sup cid cid cid cid ˆq ˆq cid cid ˆqj ˆq cid cid cid ˆq ˆqj, , , ˆq cid By combining ˆq cid, , , , SEM base cid cid cid cid cid cid We combine lemma prove Theorem restated proof Theorem, Let, For probability least following hold fS SEM base Ez fS fS cid cid cid Lϕ fS ln, Moreover SEM SEM base, Proof, The ﬁrst statement directly follows Lemma, The second statement proven Lemma Lemma, Hyperparameters We present hype parameter used train BYOL SEM CIFAR, The parameter used ResNet ResNet, Learning rate Weight decay Optimizer BYOL EMA Vocabulary size Message length online network target network, AdamW, , , BYOL SEM hyper parameter, Experiment detail ImageNet, Image augmentation We follow procedure Grill et al image augmentation procedure, The augmentation applied order training Random Resize crop image, random patch image selected resized image, Random color jitter, Modifying brightness contrast saturation hue, Random gray scale, Randomly applying gray scale ﬁlter image Random gaussian blur, Randomly applying gaussian blue ﬁlter, Random solarization, Randomly applying solarization ﬁlter, At validation test time resize image center crop patch, For training evaluation normalize image using statistic training set, , Hyper parameter We summarize hyper parameter BYOL SEM MoCo SEM table, Learning rate Batch size Weight decay Optimizer Epochs Base momentum Vocabulary size Message length online network target network, SGD lars, , , BYOL SEM hyper parameter, Learning rate Batch size Weight decay Optimizer Epochs MoCo EMA Vocabulary size Message Length online network target network, SGD lars, , , MoCo SEM hyper parameter, Table ImageNet experiment hyper parameter, , Linear evaluation We follow evaluation protocol Chen et al, The linear evaluation done training linear classiﬁer frozen representation ImageNet training sample, We train linear classiﬁer cross entropy objective epoch using SGD nesterov batch size, During training apply random resized crop random horizontal ﬂip, , Semi supervised learning We perform semi supervised experiment training linear classiﬁer top frozen repre sentation, The procedure linear evaluation procedure exception train training sample, That training sample taken according split deﬁned Chen et al, , Robustness experiment We follow evaluation procedure Lee et al, We treated robustness datasets additional test set simply evaluated using evaluation procedure described, The image resized center cropped image, The evaluation procedure performed using public robustness benchmark evaluation code Djolonga et al, , Transfer learning experiment We follow linear evaluation protocol Kolesnikov et al Chen et al We train linear classiﬁer using regularized multinomial logistic regression scikit learn package Pe dregosa et al, The representation frozen train encoder backbone http github com google research robustness_metrics Superclass aquatic mammal ﬁsh ﬂowers food container fruit vegetable household electrical device household furniture insect large carnivore large man made outdoor thing large natural outdoor scene large omnivore herbivore medium sized mammal non insect invertebrate people reptile small mammal tree vehicle vehicle Classes beaver dolphin otter seal whale aquarium ﬁsh ﬂatﬁsh ray shark trout orchid poppy rose sunﬂowers tulip bottle bowl can cup plate apple mushroom orange pear sweet pepper clock computer keyboard lamp telephone television bed chair couch table wardrobe bee beetle butterﬂy caterpillar cockroach bear leopard lion tiger wolf bridge castle house road skyscraper cloud forest mountain plain sea camel cattle chimpanzee elephant kangaroo fox porcupine possum raccoon skunk crab lobster snail spider worm baby boy girl man woman crocodile dinosaur lizard snake turtle hamster mouse rabbit shrew squirrel maple oak palm pine willow bicycle bus motorcycle pickup truck train lawn mower rocket streetcar tank tractor Table Set class superclass CIFAR, batch normalization statistic, We perform augmentation image resized pixel using bicubic resampling normalized using statistic ImageNet training set, We tune regularizer term range logarithmically spaced value using small validation set train using full training set, CIFAR superclass The class CIFAR Krizhevsky grouped superclass, The list superclass class Table Additional CIFAR relevance graph BYOL baseline BYOL baseline large rep resentation BYOL SEM Figure Comparison full relevance graph BYOL BYOL SEM, SotterkangaroobottleSlampSbeartankSstreetcarroadSStrainSlobsterSboygirlhousewomanSScockroachSappledolphinsealwhaleaquarium_fishflatfishraysharkorchidpoppyrosesunflowertulipbowlcancupplatemushroomorangepearsweet_pepperclockcomputer_keyboardtelephonetelevisionbedchaircouchtablewardrobebeebeetlebutterflycaterpillarleopardtigerwolfbridgecastleskyscrapercloudforestmountainplainseacamelcattlechimpanzeefoxpossumraccoonskunkcrabsnailwormbabymanlizardsnaketurtlehamstermouserabbitshrewsquirrelmaple_treeoak_treepalm_treewillow_treebicyclebusmotorcyclepickup_trucklawn_mowerrockettractorSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSflatfishtableStroutcaterpillarScattleSorchidcomputer_keyboardSpalm_treeScloudrocketskunkchimpanzeeSSpickup_trucksnailcameltigerspiderSdolphinsweet_pepperSlionSSlobstersunflowermushroomSorangesquirrelSSmanSsnakeSbabywardrobewolfsealwillow_treecanSmaple_treeSSturtleleopardhamsterrabbitcockroachSclockforestSSpine_treeScouchStankotterwomanbicycleSSbedbeavertulipbeelawn_mowercrocodilelamptelephoneStractorSoak_treeSmotorcycleshrewSSSSplainSwhalebusbeeSspiderbeetlecockroachSSbearkangarootigerSleopardhousecastleSroadSplainSmotorcyclelawn_mowertankStractorsealSbeaverSporcupinebedScouchSchairlionfoxSwolfSskyscraperSmountainSseasnakeSSSSlizardwormorangeSSsweet_pepperpearSSapplebusSSstreetcarSSpickup_trucktrainroseSpoppySStulipSSorchidScancupSSSSbowlplatebottlewillow_treepine_treemaple_treeoak_treeforestSpalm_treeSSSdolphinraySSSSturtlesharkSSSwhaleSSSSboySgirlwomanbabySflatfishmanmouseraccoonSSSpossumsquirrelSSshrewhamsterrabbit\n"
     ]
    }
   ],
   "source": [
    "references_clean= preprocess_text(references)\n",
    "print(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5af5e9",
   "metadata": {},
   "source": [
    "## Get_human_names Algorithme using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa296d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    \n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    for person in person_list:\n",
    "        person_split = person.split(\" \")\n",
    "        for name in person_split:\n",
    "            if wordnet.synsets(name):\n",
    "                if(name in person):\n",
    "                    person_names.remove(person)\n",
    "                    break\n",
    "    return person_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "864d30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 52 and without 196\n",
      "Ex: Arjovsky Léon Bottou Ishaan Gulrajani\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "\n",
    "print(\"Len with preprocess\",len(get_human_names(references_clean)), \"and without\",len(get_human_names(references)))\n",
    "print(\"Ex:\",get_human_names(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8dd9ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arjovsky Léon Bottou Ishaan Gulrajani',\n",
       " 'Mathilde Caron Piotr Bojanowski Armand Joulin Matthijs Douze',\n",
       " 'Armand Joulin',\n",
       " 'Chen_Adversarial_ Robustness_From_Self Supervised_Pre',\n",
       " 'Aarti Singh',\n",
       " 'Mircea Cimpoi Subhransu Maji Iasonas Kokkinos Sammy Mohamed Andrea Vedaldi',\n",
       " 'Dustin Tran Mario Lucic',\n",
       " 'Remi Munos Michal Valko',\n",
       " 'Jarrod Millman Stéfan',\n",
       " 'Sergey Ioffe Christian Szegedy',\n",
       " 'Alex Krizhevsky',\n",
       " 'Alché Buc',\n",
       " 'Aaqib Saeed',\n",
       " 'Vaart Jon',\n",
       " 'Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit',\n",
       " 'Jianxiong Xiao',\n",
       " 'Asano Ym Rupprecht Vedaldi',\n",
       " 'Ishan Misra Yann',\n",
       " 'Cy kL',\n",
       " 'Cy kL kL',\n",
       " 'Vaart Wellner',\n",
       " 'Optimizer BYOL',\n",
       " 'Léon Bottou',\n",
       " 'Ishaan Gulrajani',\n",
       " 'Dzmitry Bahdanau',\n",
       " 'Kyunghyun Cho',\n",
       " 'Yoshua Bengio',\n",
       " 'Lukas Bossard',\n",
       " 'Matthieu Guillaumin',\n",
       " 'Tomas Pajdla',\n",
       " 'Bernt Schiele',\n",
       " 'Tinne Tuytelaars',\n",
       " 'Jorge Nocedal',\n",
       " 'Christos Tsirigotis',\n",
       " 'François Corneau-Tremblay',\n",
       " 'Fabrice Normandin',\n",
       " 'Mirko Bronzi',\n",
       " 'Dendi Suhubdy',\n",
       " 'Reyhane Askari',\n",
       " 'Chao Xue',\n",
       " 'Satya Ortiz-Gagné',\n",
       " 'Arnaud Bergeron',\n",
       " 'Olexa Bilaniuk',\n",
       " 'Steven Bocco',\n",
       " 'Hadrien Bertrand',\n",
       " 'Guillaume Alain',\n",
       " 'Dmitriy Serdyuk',\n",
       " 'Mathilde Caron',\n",
       " 'Piotr Bojanowski',\n",
       " 'Matthijs Douze',\n",
       " 'Ishan Misra',\n",
       " 'Julien Mairal',\n",
       " 'Priya Goyal',\n",
       " 'Sijia Liu',\n",
       " 'Yu Cheng',\n",
       " 'Lisa Amini',\n",
       " 'Zhangyang Wang',\n",
       " 'Geoffrey Hinton',\n",
       " 'Mircea Cimpoi',\n",
       " 'Subhransu Maji',\n",
       " 'Iasonas Kokkinos',\n",
       " 'Sammy Mohamed',\n",
       " 'Andrea Vedaldi',\n",
       " 'Enrico Fini',\n",
       " 'Moin Nabi',\n",
       " 'Josip Djolonga',\n",
       " 'Matthias Minderer',\n",
       " 'Zachary Nado',\n",
       " 'Dustin Tran',\n",
       " 'Mario Lucic',\n",
       " 'Jessica Yung',\n",
       " 'Joan Puigcerver',\n",
       " 'Neil Houlsby',\n",
       " 'Xiaohua Zhai',\n",
       " 'Pietro Perona',\n",
       " 'Ivo Danihelka',\n",
       " 'Florian Strub',\n",
       " 'Florent Altché',\n",
       " 'Corentin Tallec',\n",
       " 'Elena Buchatskaya',\n",
       " 'Carl Doersch',\n",
       " 'Bernardo Avila Pires',\n",
       " 'Zhaohan Guo',\n",
       " 'Bilal Piot',\n",
       " 'Remi Munos',\n",
       " 'Michal Valko',\n",
       " 'Stéfan J.',\n",
       " 'Ralf Gommers',\n",
       " 'Eric Wieser',\n",
       " 'Stephan Hoyer',\n",
       " 'Jaime Fernández',\n",
       " 'Pearu Peterson',\n",
       " 'Kevin Sheppard',\n",
       " 'Hameer Abbasi',\n",
       " 'Christoph Gohlke',\n",
       " 'Travis E. Oliphant',\n",
       " 'Xiangyu Zhang',\n",
       " 'Shaoqing Ren',\n",
       " 'Dan Hendrycks',\n",
       " 'Steven Basart',\n",
       " 'Saurav Kadavath',\n",
       " 'Evan Dorundo',\n",
       " 'Rahul Desai',\n",
       " 'Samyak Parajuli',\n",
       " 'Alex Fedorov',\n",
       " 'Karan Grewal',\n",
       " 'Phil Bachman',\n",
       " 'Sergey Ioffe',\n",
       " 'Prannay Khosla',\n",
       " 'Piotr Teterwak',\n",
       " 'Yonglong Tian',\n",
       " 'Phillip Isola',\n",
       " 'Dilip Krishnan',\n",
       " 'Jia Deng',\n",
       " 'Anurag Arnab',\n",
       " 'Sergio Guadarrama',\n",
       " 'Francisco Massa',\n",
       " 'Trevor Killeen',\n",
       " 'Natalia Gimelshein',\n",
       " 'Luca Antiga',\n",
       " 'Alban Desmaison',\n",
       " 'Andreas Kopf',\n",
       " 'Zachary DeVito',\n",
       " 'Alykhan Tejani',\n",
       " 'Sasank Chilamkurthy',\n",
       " 'Soumith Chintala',\n",
       " 'J. Vanderplas',\n",
       " 'Neil Zeghidour',\n",
       " 'Aad W.',\n",
       " 'Jon A. Wellner',\n",
       " 'Noam Shazeer',\n",
       " 'Niki Parmar',\n",
       " 'Jakob Uszkoreit',\n",
       " 'Aidan N. Gomez',\n",
       " 'Illia Polosukhin',\n",
       " 'Jianqiang Huang',\n",
       " 'Hanwang Zhang',\n",
       " 'Aude Oliva',\n",
       " 'Antonio Torralba',\n",
       " 'Asano Ym',\n",
       " 'Vedaldi A.',\n",
       " 'Yongduo Sui',\n",
       " 'Jure Zbontar',\n",
       " 'Yann LeCun',\n",
       " 'Springer International Publishing',\n",
       " 'Deep Clustering',\n",
       " 'Xinlei Chen',\n",
       " 'Robustness Metrics',\n",
       " 'Alex Graves Greg Wayne Ivo Danihelka',\n",
       " 'Smith Robert Kern Matti Picus Stephan Hoyer Marten',\n",
       " 'Haoqi Fan Yuxin Wu',\n",
       " 'Adam Trischler Yoshua Bengio',\n",
       " 'Jonathan Krause Michael Stark Jia Deng Li Fei Fei',\n",
       " 'Journal Machine Learning Research',\n",
       " 'Purpose Audio Representations',\n",
       " 'Gomez Lukasz Kaiser Illia Polosukhin',\n",
       " 'Chen Zhangyang Wang Yang Shen',\n",
       " 'Bf BM',\n",
       " 'Optimizer Epochs',\n",
       " 'David Fleet Tomas Pajdla Bernt Schiele Tinne Tuytelaars',\n",
       " 'Curtis Jorge Nocedal',\n",
       " 'Tianlong Chen Sijia Liu Shiyu Chang Yu Cheng Lisa Amini Zhangyang Wang',\n",
       " 'Pattern Recognition',\n",
       " 'Josip Djolonga Jessica Yung Michael Tschannen Rob Romijnders Lucas Beyer',\n",
       " 'Gelly Neil Houlsby Xiaohua Zhai Mario Lucic',\n",
       " 'David Cournapeau Eric Wieser Julian Taylor Sebastian Berg Nathaniel',\n",
       " 'Río Mark Wiebe Pearu Peterson Pierre Gérard Marchant Kevin Sheppard Tyler Reddy Warren Weckesser Hameer Abbasi Christoph Gohlke Travis',\n",
       " 'Ross Girshick',\n",
       " 'Benchmarking Neural Network Robustness Common Corruptions Perturbations',\n",
       " 'David Blei',\n",
       " 'Associates Inc',\n",
       " 'Kuang Huei Lee Anurag Arnab Sergio Guadarrama',\n",
       " 'Benjamin Recht Rebecca Roelofs Ludwig Schmidt Vaishaal Shankar',\n",
       " 'Weak Convergence Empirical Processes',\n",
       " 'Tan Wang Zhongqi Yue Jianqiang Huang Qianru Sun Hanwang Zhang',\n",
       " 'Zbontar Li',\n",
       " 'Lemma Lemma',\n",
       " 'Random Resize',\n",
       " 'Message Length',\n",
       " 'Table Additional']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the result\n",
    "get_human_names(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fcb76",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "656d73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "\n",
    "def nlp_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    PROPN=[token.lemma_ for token in doc if token.pos_ == \"PROPN\"]\n",
    "    \n",
    "#     Remove duplicate\n",
    "    PROPN = list(dict.fromkeys(PROPN))\n",
    "#     Remove word with first letter as lowercase \n",
    "    for word in PROPN:\n",
    "        if word[0].islower():\n",
    "            PROPN.remove(word)\n",
    "    return PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4230cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775\n"
     ]
    }
   ],
   "source": [
    "final_names_prep_spacy=nlp_entities(references_clean)\n",
    "final_names_spacy=nlp_entities(references)\n",
    "print(len(final_names_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d123b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An other function with more accuracy\n",
    "\n",
    "def extract(text:str) :\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    doc = spacy_nlp(text.strip())\n",
    "    named_entities = []\n",
    "    \n",
    "    for i in doc.ents:\n",
    "        entry = str(i.lemma_).lower()\n",
    "        text = text.replace(str(i).lower(), \"\")\n",
    "        if i.label_ in [\"ART\", \"EVE\", \"NAT\", \"PERSON\"]:\n",
    "            named_entities.append(entry.title().replace(\" \", \"_\").replace(\"\\n\",\"_\"))\n",
    "        named_entities = list(dict.fromkeys(named_entities))\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f8b14fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 171 and without 275\n",
      "Ex: Martin_Arjovsky_Léon_Bottou\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "\n",
    "print(\"Len with preprocess\",len(extract(references_clean)), \"and without\",len(extract(references)))\n",
    "print(\"Ex:\",extract(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "167b9906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Martin_Arjovsky_Léon_Bottou',\n",
       " 'David_Lopez_Paz',\n",
       " 'Invariant_Risk_Minimization',\n",
       " 'Kyunghyun_Cho',\n",
       " 'Yoshua_Bengio',\n",
       " 'Lukas_Bossard_Matthieu',\n",
       " 'David_Fleet_Tomas_Pajdla_Bernt_Schiele',\n",
       " 'Jorge_Nocedal',\n",
       " 'Xavier_Bouthillier',\n",
       " 'Thomas_Schweizer',\n",
       " 'Lin_Dong_Pierre_Delaunay',\n",
       " 'Michael_Noukhovitch',\n",
       " 'Chao_Xue',\n",
       " 'Satya_Ortiz',\n",
       " 'Peter_Henderson',\n",
       " 'Lamblin_Christopher_Beckham',\n",
       " 'Mathilde_Caron_Piotr',\n",
       " 'Joulin_Matthijs_Douze',\n",
       " 'Mathilde_Caron_Ishan_Misra',\n",
       " 'Tianlong_Chen_Sijia',\n",
       " 'Liu_Shiyu_Chang_Yu',\n",
       " 'Cheng',\n",
       " 'Lisa_Amini',\n",
       " 'Zhangyang_Wang',\n",
       " 'Ting_Chen_Simon_Kornblith_Mohammad_Norouzi_Geoffrey_Hinton',\n",
       " 'Hal_Daumé_Iii_Aarti_Singh',\n",
       " 'Xinlei_Chen_Kaiming',\n",
       " 'Cimpoi_Subhransu',\n",
       " 'Sammy_Mohamed_Andrea_Vedaldi',\n",
       " 'Victor',\n",
       " 'Elisa_Ricci',\n",
       " 'Vturrisi',\n",
       " 'Djolonga_Frances',\n",
       " 'Rob_Romijn',\n",
       " 'Dustin_Tran',\n",
       " 'Mario_Lucic',\n",
       " 'Djolonga_Jessica',\n",
       " 'Yung_Michael',\n",
       " 'Alexander_Kolesnikov',\n",
       " 'Dan_Moldovan_Syl',\n",
       " 'Gelly_Neil',\n",
       " 'Li_Fei_Fei_Rob_Fergus_Pietro_Perona',\n",
       " 'Alex_Graves_Greg_Wayne_Ivo_Danihelka',\n",
       " 'Jean_Bastien_Grill',\n",
       " 'Corentin_Tallec',\n",
       " 'Pierre_Richemond_Elena_Buchatskaya',\n",
       " 'Remi_Munos_Michal_Valko',\n",
       " 'Lin',\n",
       " 'Jarrod_Millman_Stéfan',\n",
       " 'Walt_Ralf',\n",
       " 'David_Cournapeau',\n",
       " 'Eric_Wieser',\n",
       " 'Sebastian_Berg_Nathaniel',\n",
       " 'Robert_Kern_Matti',\n",
       " 'Kerkwijk_Matthew_Brett',\n",
       " 'Allan_Haldane',\n",
       " 'Pierre_Gérard_Marchant',\n",
       " 'Kevin_Sheppard_Tyler',\n",
       " 'Abbasi_Christoph_Gohlke_Travis',\n",
       " 'Kaiming_He_Xiangyu_Zhang',\n",
       " 'Jian_Sun',\n",
       " 'Kaime_He',\n",
       " 'Wu_Saining_Xie_Ross_Girshick',\n",
       " 'Dan_Hendrycks_Thomas_Dietterich',\n",
       " 'Dan_Hendrycks',\n",
       " 'Norman_Mu_Saurav',\n",
       " 'Kadavath_Frank',\n",
       " 'Zhu_Samyak',\n",
       " 'Mike_Guo_Dawn_Song_Jacob_Steinhardt_Justin_Gilmer',\n",
       " 'Devon_Hjelm',\n",
       " 'Alex_Fedorov_Samuel_Lavoie_Marchildon',\n",
       " 'Phil_Bachman',\n",
       " 'Adam_Trischler',\n",
       " 'Francis_Bach',\n",
       " 'Proceedings',\n",
       " 'Prannay_Khosla_Piotr',\n",
       " 'Chen_Wang',\n",
       " 'Liu_Dilip_Krishnan',\n",
       " 'Alexander_Kolesnikov_Xiaohua',\n",
       " 'Jonathan_Krause',\n",
       " 'Michael_Stark',\n",
       " 'Jia_Deng_Li_Fei_Fei',\n",
       " 'Alex_Krizhevsky',\n",
       " 'Kuang_Huei_Lee_Anurag_Arnab',\n",
       " 'John_Canny_Ian_Fischer',\n",
       " 'Maria_Elena_Nilsback_Andrew_Zisserman',\n",
       " 'Andrea_Vedaldi_Andrew_Zisserman',\n",
       " 'Jawahar',\n",
       " 'Sam_Gross_Francisco_Massa',\n",
       " 'James_Bradbury',\n",
       " 'Killeen_Zeming_Lin',\n",
       " 'Edward_Yang',\n",
       " 'Devito_Martin',\n",
       " 'Benoit_Steiner',\n",
       " 'Fang_Junjie_Bai_Soumith',\n",
       " 'Alché_Buc',\n",
       " 'Garnett',\n",
       " 'Benjamin_Recht',\n",
       " 'Rebecca_Roelofs',\n",
       " 'David_Grangier_Neil_Zeghidour',\n",
       " 'Vaart_Jon',\n",
       " 'Wellner',\n",
       " 'Parmar_Jakob',\n",
       " 'Jones_Aidan',\n",
       " 'Gomez_Lukasz_Kaiser',\n",
       " 'Illia_Polosukhin',\n",
       " 'Tan_Wang_Zhongqi',\n",
       " 'Yue_Jianqiang_Huang_Qianru',\n",
       " 'Sun_Hanwang_Zhang',\n",
       " 'Self_Supervised',\n",
       " 'Jianxiong_Xiao',\n",
       " 'James_Hays',\n",
       " 'Antonio_Torralba',\n",
       " 'Abbey_Zoo',\n",
       " 'Asano_Ym_Rupprecht_Vedaldi',\n",
       " 'Hyx_Jybfpr',\n",
       " 'Yune_You',\n",
       " 'Yongduo_Sui_Ting_Chen_Zhangyang',\n",
       " 'Wang_Yang_Shen',\n",
       " 'Graph',\n",
       " 'Self',\n",
       " 'Proof_Theorem',\n",
       " 'Cy_Kl',\n",
       " 'Zk_Kl',\n",
       " 'Ck',\n",
       " 'Cy_Kl_Kl',\n",
       " 'Eqj_Eqt',\n",
       " 'Qi',\n",
       " 'Στ_Στ',\n",
       " 'Ck_Cy_Kl',\n",
       " 'Proof',\n",
       " 'Ck_Pr',\n",
       " 'Pr_Ck',\n",
       " 'Pr_Ck_Erk',\n",
       " 'Vaart_Wellner',\n",
       " 'Pr_Ck_Ik',\n",
       " 'Lipschitz',\n",
       " 'Στ_Zt_Στ',\n",
       " 'Lϕ_Sem',\n",
       " 'Lemma_Lemma',\n",
       " 'Στ_Eqj_Eqt',\n",
       " 'Qv',\n",
       " 'Στ_Eq_Eqt',\n",
       " 'Στ_Στ_Cid_Eδj',\n",
       " 'Lim_Sem',\n",
       " 'Sem_Lemma',\n",
       " 'Lemma_Sem',\n",
       " 'ˆQ_ˆQj',\n",
       " 'Random_Resize',\n",
       " 'Random',\n",
       " 'Random_Gaussian',\n",
       " 'Gaussian_Blue_Filter',\n",
       " 'Hyper',\n",
       " 'Batch',\n",
       " 'Optimizer_Epochs_Base',\n",
       " 'Optimizer_Epochs_Moco',\n",
       " 'Chen',\n",
       " 'Semi',\n",
       " 'Chen_Et_Al',\n",
       " 'Robustness',\n",
       " 'Lee',\n",
       " 'Djolonga_Et_Al',\n",
       " 'Transfer',\n",
       " 'Kolesnikov',\n",
       " 'Al_Chen',\n",
       " 'Class',\n",
       " 'Mouse',\n",
       " 'Squirrel_Maple',\n",
       " 'Table_Set',\n",
       " 'Krizhevsky',\n",
       " 'Byol_Byol_Sem']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Result\n",
    "extract(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dbd17",
   "metadata": {},
   "source": [
    "## TextBlob : TextBlob est une bibliothèque python et propose une API simple pour accéder à ses méthodes et effectuer des tâches NLP de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86155dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_names_TextBlob= TextBlob(references_clean)\n",
    "PROPN=[words for words, tag in final_names_TextBlob.tags if tag == \"NNP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cc696d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 1216\n",
      "Ex: Martin\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess\n",
    "final_names_TextBlob=PROPN\n",
    "#With preprocess\n",
    "print(\"Len with preprocess\",len(final_names_TextBlob))\n",
    "print(\"Ex:\",final_names_TextBlob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60bbf0",
   "metadata": {},
   "source": [
    "Also efficence but not useful here because spacy hasa better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f8eab",
   "metadata": {},
   "source": [
    "## API using NLTK package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccb418",
   "metadata": {},
   "source": [
    "### Service Docker (de cette API):\n",
    "\n",
    "Go in the main folder and type to build the image:\n",
    "\n",
    "    $ docker build -t pfr .\n",
    "\n",
    "Then run the container:\n",
    "\n",
    "    $ docker run -d --name mycontainer -p 80:80 pfr\n",
    "\n",
    "Interactive API docs: \n",
    "\n",
    "Now you can go to http://127.0.0.1/docs and try the API to analyse a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73cd41c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=80): Max retries exceeded with url: /NLTK/NER?text=Aux%20groupes%20pr%C3%A9sents%20depuis%20le%20Pal%C3%A9olithique%20et%20le%20N%C3%A9olithique,%20sont%20venues%20s'ajouter,%20%C3%A0%20l'%C3%82ge%20du%20bronze%20et%20%C3%A0%20l'%C3%82ge%20du%20fer,%20des%20vagues%20successives%20de%20Celtes,%20puis%20au%20iiie%20si%C3%A8cle%20de%20peuples%20germains%20(Francs,%20Wisigoths,%20Alamans,%20Burgondes)%20et%20au%20ixe%20si%C3%A8cle%20de%20scandinaves%20appel%C3%A9s%20Normands. (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000289BE57FB50>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    170\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_default_user_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1254\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    182\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x00000289BE57FB50>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    756\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=80): Max retries exceeded with url: /NLTK/NER?text=Aux%20groupes%20pr%C3%A9sents%20depuis%20le%20Pal%C3%A9olithique%20et%20le%20N%C3%A9olithique,%20sont%20venues%20s'ajouter,%20%C3%A0%20l'%C3%82ge%20du%20bronze%20et%20%C3%A0%20l'%C3%82ge%20du%20fer,%20des%20vagues%20successives%20de%20Celtes,%20puis%20au%20iiie%20si%C3%A8cle%20de%20peuples%20germains%20(Francs,%20Wisigoths,%20Alamans,%20Burgondes)%20et%20au%20ixe%20si%C3%A8cle%20de%20scandinaves%20appel%C3%A9s%20Normands. (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000289BE57FB50>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a143ce0e3a8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Aux groupes présents depuis le Paléolithique et le Néolithique, sont venues s'ajouter, à l'Âge du bronze et à l'Âge du fer, des vagues successives de Celtes, puis au iiie siècle de peuples germains (Francs, Wisigoths, Alamans, Burgondes) et au ixe siècle de scandinaves appelés Normands.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'http://127.0.0.1/NLTK/NER?text={text}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=80): Max retries exceeded with url: /NLTK/NER?text=Aux%20groupes%20pr%C3%A9sents%20depuis%20le%20Pal%C3%A9olithique%20et%20le%20N%C3%A9olithique,%20sont%20venues%20s'ajouter,%20%C3%A0%20l'%C3%82ge%20du%20bronze%20et%20%C3%A0%20l'%C3%82ge%20du%20fer,%20des%20vagues%20successives%20de%20Celtes,%20puis%20au%20iiie%20si%C3%A8cle%20de%20peuples%20germains%20(Francs,%20Wisigoths,%20Alamans,%20Burgondes)%20et%20au%20ixe%20si%C3%A8cle%20de%20scandinaves%20appel%C3%A9s%20Normands. (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000289BE57FB50>: Failed to establish a new connection: [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "text=\"Aux groupes présents depuis le Paléolithique et le Néolithique, sont venues s'ajouter, à l'Âge du bronze et à l'Âge du fer, des vagues successives de Celtes, puis au iiie siècle de peuples germains (Francs, Wisigoths, Alamans, Burgondes) et au ixe siècle de scandinaves appelés Normands.\"\n",
    "\n",
    "response = requests.get(f'http://127.0.0.1/NLTK/NER?text={text}')\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f'http://127.0.0.1/NLTK/NER?text={references_clean}')\n",
    "result_API=response.json()\n",
    "result_API=print(result_API['Names entities in text from NLTK '])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb67165",
   "metadata": {},
   "source": [
    "## Apply Spacy functions on all pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b585bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n6\\n1\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nSimplicial Embeddings in Self-Supervised Learning\\nand Downstream Classi\\xef\\xac\\x81cation\\n\\nSamuel Lavoie(cid:5)\\xe2\\x80\\xa0, Christos Tsirigotis(cid:5)\\xe2\\x80\\xa0, Max Schwarzer(cid:5)\\xe2\\x80\\xa0, Kenji Kawaguchi\\xe2\\x80\\xa1, Ankit Vani(cid:5)\\xe2\\x80\\xa0,\\nAaron Courville(cid:5)\\xe2\\x80\\xa0\\xe2\\x99\\xa3\\n(cid:5) Mila, \\xe2\\x80\\xa0 Universit\\xc3\\xa9 de Montr\\xc3\\xa9al, \\xe2\\x80\\xa1 National University of Singapore, \\xe2\\x99\\xa3 CIFAR Fellow\\n{samuel.lavoie.m,aaron.courville}@gmail.com\\n{christos.tsirigotis,max.schwarzer,ankit.vani}@umontreal.ca\\nkenji@comp.nus.edu.sg\\n\\nAbstract\\n\\nWe introduce Simplicial Embeddings (SEMs) as a way to constrain the encoded\\nrepresentations of a self-supervised model to L simplices of V dimensions each\\nusing a Softmax operation. This procedure imposes a structure on the represen-\\ntations that reduce their expressivity for training downstream classi\\xef\\xac\\x81ers, which\\nhelps them generalize better. Speci\\xef\\xac\\x81cally, we show that the temperature \\xcf\\x84 of the\\nSoftmax operation controls for the SEM representation\\xe2\\x80\\x99s expressivity, allowing us\\nto derive a tighter downstream classi\\xef\\xac\\x81er generalization bound than that for classi-\\n\\xef\\xac\\x81ers using unnormalized representations. We empirically demonstrate that SEMs\\nconsiderably improve generalization on natural image datasets such as CIFAR-100\\nand ImageNet. Finally, we also present evidence of the emergence of semantically\\nrelevant features in SEMs, a pattern that is absent from baseline self-supervised\\nmodels.\\n\\n1\\n\\nIntroduction\\n\\nSelf-supervised learning (SSL) is an emerging family of methods that aims to learn an embedding of\\nthe data without manual supervision, such as class labels. Those methods embed the data in some\\nrepresentations that render themselves amenable to \\xef\\xac\\x81tting a linear classi\\xef\\xac\\x81er, as demonstrated in Hjelm\\net al. [2019]; Grill et al. [2020]; Saeed et al. [2020]; You et al. [2020]. This observation demonstrates\\nthat the representation learned by those SSL methods encodes the semantic content necessary to learn\\na classi\\xef\\xac\\x81er as a linear combination of the features.\\n\\nIn this work, we propose to embed the latent representation of the data into L simplices of V\\ndimensions each by using a Softmax operation. We refer to the normalized embeddings as Simplicial\\nEmbeddings (SEMs) due to the geometrical structure of the representation induced by the Softmax.\\nThe SEMs have an effect both while training the representation and on the training of the downstream\\nclassi\\xef\\xac\\x81er. For the former, the SEM is an inductive bias to \\xef\\xac\\x81t the data in a more constrained space\\nthat may lead to a simpler representation. For the latter, the Softmax allows us to control for the\\nexpressivity of the representation. This control gives us a better generalization bound for training\\ndownstream classi\\xef\\xac\\x81ers.\\n\\nWe demonstrate that the proposed SEMs improve the generalization of downstream classi\\xef\\xac\\x81ers trained\\nwith BYOL [Grill et al., 2020] and MoCo [He et al., 2020] on CIFAR-100 and ImageNet. We also\\nshow an improvement in transfer learning and robustness to out-of-distribution datasets. Finally,\\nwe present evidence that individual features of the SEMs encode semantical content related to\\nour intuitive notion of the semantics in CIFAR-100. In contrast, we argue that the baseline SSL\\nmethods may learn the semantics related to the classes as a linear combination of the features in the\\nrepresentation but not at the individual features\\xe2\\x80\\x99 level.\\n\\nPreprint. Under review.\\n\\n\\x0cConcretely, this work makes the following contributions:\\n\\n1. Propose the Simplicial Embeddings.\\n\\n2. Derive a generalization bound for downstream classi\\xef\\xac\\x81ers trained on the Simplicial Embed-\\n\\n3. Empirically studies the Simplicial Embeddings and its effect on the generalization of\\n\\ndings.\\n\\ndownstream classi\\xef\\xac\\x81ers.\\n\\n1.1 Related works\\n\\nThe use of Softmax as an inductive bias has been studied in other contexts, notably as an architectural\\ncomponent for models to attend to context-dependent queries via, for example, attention mecha-\\nnisms [Bahdanau et al., 2016; Vaswani et al., 2017] or memory augmented networks [Graves et al.,\\n2014]. Different from these, our method places the Softmax at the output of an encoder to constrain\\nthe representation and to allow control of the expressivity of the representation for downstream\\nclassi\\xef\\xac\\x81ers.\\n\\nOur work builds on top of the literature on self-supervised learning. Notably, we demonstrate the effect\\nof the SEM on contrastive approaches using the noise contrastive estimation (NCE) objective [Hjelm\\net al., 2019; Chen et al., 2020b] with memory banks [He et al., 2020] and on the bootstrapping\\napproaches [Grill et al., 2020; Chen and He, 2020]. Related, some works explicitly induce clustering\\nof the representation [Caron et al., 2019; Ym et al., 2019; Caron et al., 2020]. Contrary to these\\nworks, we do not explicitly induce clustering on the representation.\\n\\nIn the realm of improving the generalization of SSL methods, Wang et al. [2021] propose a method\\nto iteratively select a partition of the data and use this partition to minimize an IRM regularizer\\n[Arjovsky et al., 2020] with an SSL objective. Lee et al. [2021] present an objective to minimize\\nthe conditional entropy bottleneck. Contrary to these works, our methods do not require additional\\nobjectives as it is merely an inductive bias in the SSL models.\\n\\n2 Background on self-supervised learning\\n\\nModels trained with a contrastive objective learn to embed samples x \\xe2\\x88\\x88 X into representations\\nz \\xe2\\x88\\x88 Z, where Z is a bounded metric space. The aim is to both minimize the distance between\\nthe representation of a sample zi = f\\xce\\xb8(xi) : x \\xe2\\x88\\x88 X and the representation of a positive sample\\nzj = f\\xce\\xb8(xj), and to maximize the distance between zi and the representation of negative samples\\nf\\xce\\xb8(x(cid:48)) : x(cid:48) \\xe2\\x88\\x88 X \\\\ xi. While the positive samples are typically augmented samples of xi, other\\nstrategies can be decided, such as choosing samples from the same labelled category [Khosla et al.,\\n2020]. A common contrastive objective is Noise Contrastive Estimation (NCE) [Hjelm et al., 2019;\\nChen et al., 2020b], which is de\\xef\\xac\\x81ned as\\n\\nLnce := \\xe2\\x88\\x92 log\\n\\nexp(d(zi, zj)/t)\\n\\xc2\\xafx\\xe2\\x88\\x88X \\\\x exp(d(zi, \\xc2\\xafz)/t)\\n\\n,\\n\\n(cid:80)\\n\\n(1)\\n\\nwhere d is often taken to be the cosine similarity: d(x, y) := x(cid:62)y/(cid:107)x(cid:107)2(cid:107)y(cid:107)2 and t > 0 is a\\nhyper-parameter that denotes a temperature.\\n\\nUnlike most contrastive methods, BYOL [Grill et al., 2020] does not require negative samples.\\nInstead, it introduces a target network in which the parameters \\xce\\xbe are taken as an exponential moving\\naverage of the embedding function parameters, \\xce\\xb8. More precisely, \\xce\\xbe \\xe2\\x86\\x90 \\xce\\xb1\\xce\\xbe + (1 \\xe2\\x88\\x92 \\xce\\xb1)\\xce\\xb8, with\\n\\xce\\xb1 \\xe2\\x88\\x88 [0, 1]. The authors de\\xef\\xac\\x81ne the anchor and positive samples as z\\xce\\xb8 = f\\xce\\xb8(t(x)) and z\\xce\\xbe = f\\xce\\xbe(t(cid:48)((x))\\nrespectively, where t, t(cid:48) \\xe2\\x88\\xbc T are augmentations sampled from a set of possible augmentations de\\xef\\xac\\x81ned\\nby the practitioner. To prevent degenerate solutions, they re-normalize the representation using batch\\nnormalization [Ioffe and Szegedy, 2015], and utilize a stop-gradient operation on z\\xce\\xbe that prevents the\\ngradient from back-propagating through the target network. They also introduce a prediction head\\nthat maps the representation to a prediction: z\\xce\\xb8 (cid:55)\\xe2\\x86\\x92 q\\xce\\xb8. The BYOL objective is de\\xef\\xac\\x81ned as\\n\\nLbyol := 2 \\xe2\\x88\\x92 2 \\xc2\\xb7 d(q\\xce\\xb8, z\\xce\\xbe),\\n\\n(2)\\n\\nwhere d is chosen to be the cosine similarity.\\n\\n2\\n\\n\\x0c(a)\\n\\n(b)\\n\\nFigure 1: (a) Illustration of the proposed Simplicial Embeddings (SEM). \\xcf\\x83\\xcf\\x84 represents the Softmax\\noperation with \\xcf\\x84 . We assume that z decomposes into L vectors in RV . (b) Histogram of the entropies\\nH(\\xc2\\xafz(x)\\n) at the end of the pre-training phase, for a given temperature \\xcf\\x84 , of each simplex for each\\ni\\ntraining sample in CIFAR-100. (c) Integration of the SEM with BYOL [Grill et al., 2020].\\n\\n3 Simplcial Embeddings\\n\\nWe illustrate the proposed Simplicial Embeddings (SEMs) in Figure 1a. An encoder embeds a sample\\nx into a L \\xc3\\x97 V representation z. A temperature parameter \\xcf\\x84 then scales the logits z \\xe2\\x88\\x88 RL\\xc3\\x97V before\\nre-normalizing each row via L independent Softmax operations. Then, the normalized vectors are\\nconcatenated to produce \\xc2\\xafz \\xe2\\x88\\x88 RLV . Concretely, the logits are re-normalized as follows:\\n\\n\\xc2\\xafzi := [\\xcf\\x83\\xcf\\x84 (zi1),\\n\\n. . . , \\xcf\\x83\\xcf\\x84 (ziV )], \\xcf\\x83\\xcf\\x84 (zij) =\\n\\n, \\xc2\\xafz := Concat(\\xc2\\xafz1, . . . , \\xc2\\xafzL),\\n\\n(3)\\n\\nezij /\\xcf\\x84\\nk=1 ezik/\\xcf\\x84\\n\\n(cid:80)V\\n\\nfor all i \\xe2\\x88\\x88 [L] and j \\xe2\\x88\\x88 [V ].\\n\\nThe SEMs can be integrated easily into a NCE model [Hjelm et al., 2019; Chen et al., 2020b] or\\nBYOL [Grill et al., 2020]. We insert it after the encoder and before the projector in our experiments.\\nFigure 1c depicts how we use the SEMs in BYOL. The embedding \\xc2\\xafz is passed into the projector\\nmodule, which we de\\xef\\xac\\x81ne as a linear layer or a small MLP. Beyond this small modi\\xef\\xac\\x81cation, the SSL\\nmethod considered remains unchanged.\\n\\n3.1\\n\\nInductive bias of the SEMs\\n\\ni\\n\\nj=1 p(\\xc2\\xafz(x)\\n\\n) where (cid:80)V\\n\\nij ) = 1 and p(\\xc2\\xafz(x)\\n\\nij ) \\xe2\\x89\\xa5 0 \\xe2\\x88\\x80j. Here, \\xc2\\xafz(x)\\n\\nWe now describe at a high level the inductive bias of the SEMs during the self-supervised learning\\nphase. We note that each simplex can be interpreted as representing a probability mass function\\np(\\xc2\\xafz(x)\\nrepresents the simplex i for a\\ni\\nsample x. The simplex puts a constrain on how the its elements may organize: they may interpolate\\nbetween being a sparse vector and being a constant vector. The state of a simplex can be quanti\\xef\\xac\\x81ed\\n) that we denote as follows: H(\\xc2\\xafzi) := \\xe2\\x88\\x92 (cid:80)V\\nusing the entropy of p(\\xc2\\xafz(x)\\nij ). That\\nis, if H(\\xc2\\xafz(x)\\nWhile we may argue that the temperature parameter \\xcf\\x84 , which merely induces a scaling of the logit,\\nmay be subsumed during training, we demonstrate in Figure 1b that this temperature is an important\\ninitial condition for determining the state to which the simplex will converge. Here, we plot the\\nhistogram of the entropies H(\\xc2\\xafz(x)\\n), for a given \\xcf\\x84 , of each simplex for each sample x in the training\\nset of CIFAR-100. The temperature parameter dictates in which state the representation will converge:\\na small \\xcf\\x84 will induce a sparse representation, and a large \\xcf\\x84 will induce a constant representation.\\n\\nj=1 p(\\xc2\\xafz(x)\\n) = log(V ) then the vector is constant.\\n\\n) = 0 then the vector is sparse and if H(\\xc2\\xafz(x)\\n\\nij ) log p(\\xc2\\xafz(x)\\n\\ni\\n\\ni\\n\\ni\\n\\ni\\n\\n(c)\\n\\n3\\n\\nEncoderConcat(0.3, 0.3, 0.4)(0.1, 0.7, 0.2)(0.1, 0.1, 0.8)SEM......0.00.51.01.52.02.5Entropy H(z(x)i)0123456: 0.01: 0.1: 0.5: 1.0: 10.0EncoderEncoderSEMSEMPredictionProjectionSSL  lossstop gradProjection\\x0cInterestingly, for an intermediate temperature, the distribution of entropies is more spread out, rather\\nthan having the same variance smoothly translated toward the center of the histogram.\\n\\nThe above observation gives an intuition about how the induce a bias of the SEMs on the learned\\nrepresentation during SSL. Besides the qualitative properties of the vectors that the SEM may induce,\\nthis embedding has a particular structure that we may leverage for learning a classi\\xef\\xac\\x81er with a better\\ngeneralization bound. Next, we theoretically present how the SEMs allow for a better generalization\\nof a downstream classi\\xef\\xac\\x81er and derive a generalization bound for the classi\\xef\\xac\\x81ers trained on such\\nrepresentation.\\n\\n3.2 Theoretical bound on the downstream classi\\xef\\xac\\x81er\\n\\nIn this subsection, we mathematically analyze the SEM to understand its bene\\xef\\xac\\x81t and the effect of\\nthe hyper-parameter \\xcf\\x84 . We show that: (1) there is a trade-off between the training loss and the\\ngeneralization gap, which is controlled by the value of \\xcf\\x84 , and (2) the SEM can improve the base\\nmodel performance when we attain good balance in this trade-off.\\n\\nLet g represent the layer(s) after the normalization. With this notation, we can de\\xef\\xac\\x81ne a baseline\\nmodel without normalization as fbase(z) = g(z) and the corresponding model with normalization\\nas fSEM(\\xcf\\x84 )(z) = (g \\xe2\\x97\\xa6 \\xcf\\x83\\xcf\\x84 )(z). We consider a training dataset S = (zi, yi)n\\ni=1 of n samples that is\\nused for supervised training of a classi\\xef\\xac\\x81er using the representations z, which are extracted from the\\nself-supervised encoder. To understand the quality of the \\xef\\xac\\x81nal model after supervised training of\\nthe classi\\xef\\xac\\x81er, we analyze the generalization gap Ez,y[l(f (z), y)] \\xe2\\x88\\x92 1\\ni=1 l(f (z(i)), y(i)) for each\\nn\\nbase}, where l : R \\xc3\\x97 Y \\xe2\\x86\\x92 R\\xe2\\x89\\xa50 is the per-sample loss.\\nf \\xe2\\x88\\x88 {f S\\n\\nSEM(\\xcf\\x84 ), f S\\n\\n(cid:80)n\\n\\nTo simplify the notation, we consider the normalization to [\\xe2\\x88\\x921, +1]; i.e., z \\xe2\\x88\\x88 Z = [\\xe2\\x88\\x921, +1]L\\xc3\\x97V .\\nWe assume that there exists \\xe2\\x88\\x86 > 0 such that for any i \\xe2\\x88\\x88 [L], if k = arg maxj\\xe2\\x88\\x88[V ] zij, then\\nzik \\xe2\\x89\\xa5 zij + \\xe2\\x88\\x86 for any j (cid:54)= k. Since \\xe2\\x88\\x86 can be arbitrarily small (e.g., much smaller than machine\\nprecision), this assumption typically holds in practice. Next, we de\\xef\\xac\\x81ne B to be the upper bound\\non the per-sample loss such that l(f (z), y) \\xe2\\x89\\xa4 B for all f \\xe2\\x88\\x88 H and for all (z, y) \\xe2\\x88\\x88 Z \\xc3\\x97 Y,\\nwhere H is the union of the hypothesis spaces of fSEM(\\xcf\\x84 ) and fbase. For example, B = 1\\nfor the 0-1 loss. We also use Qi = {q \\xe2\\x88\\x88 [\\xe2\\x88\\x921, +1]V : i = arg maxj\\xe2\\x88\\x88[V ] qj}, with the fol-\\nlowing two de\\xef\\xac\\x81nitions: \\xcf\\x95(f S\\n2, and \\xcf\\x95(f S\\nSEM(\\xcf\\x84 )) =\\nsupi\\xe2\\x88\\x88[V ] supq,q(cid:48)\\xe2\\x88\\x88Qi\\nt=1 eqt/\\xcf\\x84 for j = 1, . . . , V . Next,\\nwe de\\xef\\xac\\x81ne GS to be the set of g returned by the training algorithm using dataset S, and R to be the\\nLipschitz constant of ly \\xe2\\x97\\xa6 g for all y \\xe2\\x88\\x88 Y and g \\xe2\\x88\\x88 GS; i.e., |(ly \\xe2\\x97\\xa6 g)(z) \\xe2\\x88\\x92 (ly \\xe2\\x97\\xa6 g)(z(cid:48))| \\xe2\\x89\\xa4 R(cid:107)z \\xe2\\x88\\x92 z(cid:48)(cid:107)F ,\\nwhere ly(q) = l(q, y). Finally, let c > 0 be a universal constant in (n, f, H, \\xce\\xb4, H, \\xcf\\x84, S).\\n\\n2, where \\xcf\\x83\\xcf\\x84 (q)j = eqj /\\xcf\\x84\\n\\nbase) = supi\\xe2\\x88\\x88[V ] supq,q(cid:48)\\xe2\\x88\\x88Qi\\n\\nt=1 (cid:107)\\xcf\\x83\\xcf\\x84 (q) \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (q(cid:48))(cid:107)2\\n\\nt=1 (cid:107)q \\xe2\\x88\\x92 q(cid:48)(cid:107)2\\n\\n(cid:80)n\\n\\n(cid:80)n\\n\\n(cid:80)V\\n\\nUsing the established notation, Theorem 1 illuminates the advantage of the SEM and the effect of the\\nhyper-parameter \\xcf\\x84 on the performance of the downstream classi\\xef\\xac\\x81er:\\nTheorem 1. Let V \\xe2\\x89\\xa5 2. For any \\xce\\xb4 > 0, with probability at least 1 \\xe2\\x88\\x92 \\xce\\xb4, the following holds for any\\nfS \\xe2\\x88\\x88 {f S\\n\\nSEM(\\xcf\\x84 ), f S\\n\\nbase}:\\n\\nEz,y[l(fS(z), y)] \\xe2\\x89\\xa4\\n\\nl(fS(z(i)), y(i)) + R\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:114)\\n\\n(cid:114)\\n\\nL\\xcf\\x95(fS)\\nn\\n\\n+ c\\n\\nln(2/\\xce\\xb4)\\nn\\n\\n.\\n\\nMoreover,\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x86\\x92 0 as \\xcf\\x84 \\xe2\\x86\\x92 0\\n\\nand \\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x88\\x92 \\xcf\\x95(f S\\n\\nbase) \\xe2\\x89\\xa4\\n\\n(1 \\xe2\\x88\\x92 V ) < 0 \\xe2\\x88\\x80\\xcf\\x84 > 0.\\n\\n3n\\n4\\n\\nThe \\xef\\xac\\x81rst statement of Theorem 1 shows that the expected loss is bounded by the three terms: training\\nloss 1\\nn\\n\\ni=1 l(fS(z(i)), y(i)), the second term R\\n\\n(cid:113) L\\xcf\\x95(fS )\\nn\\n\\n(cid:113) ln(2/\\xce\\xb4)\\nn\\n\\n. Since c is\\n\\n(cid:80)n\\n\\na universal constant in (n, f, H, \\xce\\xb4, H, \\xcf\\x84, S), the third term c\\ngoes to zero as n \\xe2\\x86\\x92 \\xe2\\x88\\x9e and is\\nthe same for both models with and without soft-discretization. Thus, for the purpose of comparing the\\nmodels with and without soft-discretization, we can focus on the second term, where the difference\\narises.\\n\\n, and the third term c\\n(cid:113) ln(2/\\xce\\xb4)\\nn\\n\\nTheorem 1 shows that the second term R\\nSEM(\\xcf\\x84 )) \\xe2\\x86\\x92\\n0 as \\xcf\\x84 \\xe2\\x86\\x92 0. Also, for any \\xcf\\x84 > 0, the second term with soft-discretization is strictly smaller than\\n\\ngoes to zero with the SEM; i.e., \\xcf\\x95(f S\\n\\n(cid:113) L\\xcf\\x95(fS )\\nn\\n\\n4\\n\\n\\x0cthat without soft-discretization as \\xcf\\x95(f S\\nimprovement due to soft-discretization is expected to be higher as V increases.\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x88\\x92 \\xcf\\x95(f S\\n\\nbase) \\xe2\\x89\\xa4 3n\\n\\n4 (1 \\xe2\\x88\\x92 V ) < 0. This shows that the\\n\\nOverall, Theorem 1 shows the bene\\xef\\xac\\x81t of the SEM as well as the trade-off with \\xcf\\x84 . When \\xcf\\x84 \\xe2\\x86\\x92 0, the\\nsecond term goes to zero, but the training loss (the \\xef\\xac\\x81rst term) can increase due to the reduction in\\nexpressivity and increased dif\\xef\\xac\\x81culty in optimization. Thus, we assert that the best \\xcf\\x84 is the one that\\nbalances this trade-off.\\n\\n4 Experiments\\n\\nWe study the effect of the Simplicial Embeddings on the generalization of self-supervised learn-\\ning methods1. We demonstrate that the Simplicial Embeddings improve the test set accuracy on\\nCIFAR-100 and ImageNet. On CIFAR-100, we also study the different properties of the SEM, and\\nwe demonstrate the emergence of semantic features relevant to the classes in the representation\\nfeature. On ImageNet, we show that the Simplicial Embeddings improve the test accuracy on several\\nrobustness test sets and the accuracy on transfer learning datasets.\\n\\n4.1 The effect of the SEM on downstream classi\\xef\\xac\\x81cation\\n\\nMethod\\nSimCLR\\xe2\\x80\\xa0\\nMOCO\\xe2\\x80\\xa0\\nSWAV\\xe2\\x80\\xa0\\nDINO\\xe2\\x80\\xa0\\nBYOL\\nBYOL + SEM\\n\\nAccuracy\\n65.78\\n69.89\\n64.88\\n66.76\\n70.46\\n74.36\\n\\nMethod\\nSimCLR\\xe2\\x80\\xa1\\nBYOL\\nBYOL*\\nBYOL + SEM\\n\\nAccuracy\\n68.73\\n74.28\\n73.33\\n77.05\\n\\nMethod\\nSIMCLR\\xe2\\x80\\xa1\\nMOCO\\xe2\\x80\\xa1\\nMOCO + SEM\\nSIMSIAM(cid:5)\\nBYOL(cid:5)\\nBYOL + SEM\\n\\nAccuracy\\n63.1\\n67.3\\n69.0\\n70.0\\n70.6\\n72.8\\n\\n(a) CIFAR-100 on ResNet18\\n\\n(b) CIFAR-100 on ResNet50\\n\\n(c) ImageNet on ResNet50\\n\\nFigure 2: Accuracy on (a) CIFAR-100 with ResNet18 for 1000 epochs. (b) CIFAR-100 with ResNet50\\nfor 1000 epochs. (c) ImageNet with ResNet50 for 200 epochs. *Denotes the accuracy obtained when\\ntraining BYOL with a representation the same size as SEM. \\xe2\\x80\\xa0 Results taken from da Costa et al.\\n[2021]. \\xe2\\x80\\xa1 Results taken from Wang et al. [2021]. (cid:5) Results taken from [Chen et al., 2020b]. Boldface\\nindicates highest accuracy. Green rows indicate a SSL method + SEM.\\n\\nComparison study. We \\xef\\xac\\x81rst compare the effect of using the SEM in a BYOL model with related\\nSSL approaches in the literature. We take a standard BYOL model, as implemented in the Solo-Learn\\nlibrary [da Costa et al., 2021], and implement the Simplicial Embeddings after the encoder. We test\\nour approach with a ResNet18 and ResNet50 on CIFAR-100 and with a ResNet50 for ImageNet [He\\net al., 2015]. Our models are trained with Stochastic Gradient Descent [Bottou et al., 2018] with a\\ncosine decay scheduler on the learning rate, as done in previous works [Grill et al., 2020; Chen et al.,\\n2020b]. We use a batch size of 256 for all of our models and train on a single A100 GPU. We selected\\nthe parameters of the SEM by performing a grid search over several values using a validation set and\\nre-trained our model using all the training data to evaluate the test set. We did not modify the default\\nhyper-parameters of the method, demonstrating that the gain in accuracy is a product of the SEM. We\\npresent the hyper-parameters used in the Appendix. We evaluate all of our models by training a linear\\nclassi\\xef\\xac\\x81er, using the training data on top of the learned representations as it is typically done.\\n\\nWe compare our approach on CIFAR-100 and ImageNet in Table 2b and Table 2c respectively.\\nCompared with prior models, our approach improves the baseline methods by a considerable margin.\\nOn CIFAR-100, we compare with several baselines, such as DINO and SwaV. We also trained BYOL\\nwith the same representation size as what we used in the SEM, without the embedding, and observed a\\nmarginal performance decrease. As demonstrated in Zbontar et al. [2021], BYOL does not seemingly\\nbene\\xef\\xac\\x81t from large representations.\\n\\nThe SEM also presents a noticeable improvement compared to the baselines on ImageNet when\\ntrained for 200 epochs. Here, we trained our model on both BYOL and MOCO [He et al., 2020] to\\ndemonstrate that the effect of the SEM is not limited to BYOL.\\n\\n1We provide the code to reproduce the experiments: https://github.com/lavoiems/simplicial-embeddings\\n\\n5\\n\\n\\x0cFigure 3: Study of the effect of the parameters of the SEM. We plot the accuracy obtained for several\\ndownstream classi\\xef\\xac\\x81cation SEM\\xe2\\x80\\x99s temperature (fSEM(\\xcf\\x84 )) and without SEM (fbase) described in the\\nlegend. We performed the training with a ResNet18 on CIFAR-100. Interpolation of Left: \\xcf\\x84 during\\nthe training of the SSL model. Middle: V . Right: L\\n\\nStudy of the SEM parameters. We study the effect of each of the parameters of the SEM and\\nevaluate their effect on the validation accuracy in Figure 3. We trained each model with a ResNet18\\non CIFAR-100 using the BYOL training procedure. We keep the other parameters constant to their\\ndefault value for each parameter that we study. The default value of \\xcf\\x84 is 1, V is 13 and L is 5000. For\\neach pre-trained SSL model, we trained 5 downstream classi\\xef\\xac\\x81ers, one on the unnormalized features\\ndenoted fbase and one on the normalized features for \\xcf\\x84 \\xe2\\x88\\x88 {0.01, 0.1, 1, 10}.\\n\\nWe observe that the temperature used to normalized the embedding before training the downstream\\nclassi\\xef\\xac\\x81er, fSEM(\\xcf\\x84 ), is important for the downstream classi\\xef\\xac\\x81cation and is generally better than training\\na classi\\xef\\xac\\x81er on the unnormalized features (fbase) as predicted in Theorem 1. We observe the trade-off,\\nas presented in Section 3.2, between having a small and a large \\xcf\\x84 .\\n\\nWe also observe a trade-off between having a large and a small temperature when training the SSL\\nmodel. As demonstrated in Figure 1b, the temperature parameter has an impact on whether the\\nsimplicies will represent a sparse or a constant vector. We demonstrated that a small temperature\\nyields a set of sparse vectors while a large temperature yields a constant vector. Here, we observe\\nthat the temperature yielding the better validation accuracy offers a trade-off between a sparse and a\\nconstant vector. We hypothesize that a sparse vector leads to harder training but a smaller expressivity.\\nThus, the better temperature during the training of the SSL model is the one that offers a trade-off\\nbetween a sparse but trainable representation.\\n\\nIn Theorem 1, we demonstrated theoretically that the second term was more sensitive to the tem-\\nperature as V increased. This prediction is empirically veri\\xef\\xac\\x81ed in Figure 3 where we evaluate the\\nvalidation accuracy for several V . As V increases, the validation accuracy drops for larger \\xcf\\x84 s. For\\nexample, the validation accuracy drops when interpolating between V = 13 and V = 34 for \\xcf\\x84 = 10,\\nstays constant for \\xcf\\x84 = 1 and increases for the smaller temperatures.\\n\\nFinally, we interpolate the L parameter and demonstrate that larger L yields increased normalized\\nfeatures\\xe2\\x80\\x99 validation accuracy. As expected, the effect of \\xcf\\x95(fS) grows with larger L, and thus we\\nwould expect a bigger difference between fbase and fSEM(\\xcf\\x84 ). This demonstrates empirically and\\ntheoretically that the SEM may scale the representation of SSL methods to a larger representation\\nand thus potentially increasing the scaling capability of these methods.\\n\\n4.2 Emergence of semantically relevant features\\n\\nIn this subsection, we investigate the semantic content held by the most predictive features of an\\nembedding. To make this study, we consider an encoder pretrained on CIFAR-100, using BYOL with\\nand without SEM, and a downstream linear classi\\xef\\xac\\x81er trained on the embedding of the CIFAR-100\\nsamples. Consider the trained linear classi\\xef\\xac\\x81er with a weight matrix W \\xe2\\x88\\x88 RN \\xc3\\x97C, where N denotes\\nthe number of features, and C denotes the number of classes. This classi\\xef\\xac\\x81er is trained by minimizing\\nthe cross-entropy loss between the predicted class and the given label.\\n\\nHere, we study the semantic relevance of the top K features for each class. Consider the weight\\nmatrix W . By preserving the top K parameters of this weight matrix for each class and pruning\\nthe features predictive for only one class, we create a bipartite graph between two set of nodes: the\\n\\n6\\n\\nfbasefSEM(=0.01)fSEM(=0.1)fSEM(=1)fSEM(=10)0.010.10.5110SSL 506070Valid accuracy23581334V62.565.067.570.072.5Valid accuracy5010050010005000L62.565.067.570.072.5Valid accuracy\\x0c(a)\\n\\n(b)\\n\\nFigure 4: Semantic relevance of the features. (a) Subset of WK, the bipartite graph of the most\\nimportant features shared between at least two classes of a classi\\xef\\xac\\x81er trained on BYOL + SEMs\\nfeatures. The connected components emerge without additional interventions. (b) Relevance of the\\ntop K features to the semantics of the super-class of the categories of CIFAR-100. It is taken as the\\nnumber of pairwise categories in the same super-class for which a feature is among its top K most\\npredictive features over the total number of pairwise categories.\\n\\ncategories and the features. We denote this graph WK. We plot a subset W5, obtained when taking\\nthe top 5 features for each class, on the SEM representations in Figure 4a and the full bipartite graph\\non the SEM and the one obtained when applying the procedure on the representation obtained with\\nan unnormalized BYOL in the Appendix. In the graph obtained with the SEM, we observe that a\\nset of connected components emerge, and the connected components of the graph are semantically\\nrelated. For example, the \\xef\\xac\\x81rst set of connected components are fruits and vegetables, and the second\\nset of connected components are aquatic mammals. The same observation does not occur when this\\nexperiment is performed on the baseline BYOL and BYOL, with a large representation model. In\\nparticular, we do not see a small number of semantically related connected components. Instead, we\\nsee a large fully connected graphs. This observation suggests that the features learned by the baseline\\nmodel do not hold the same amount of semantic information. Instead, the semantic information could\\nbe encoded as a linear combination of several features, for example.\\n\\nWe also study more quantitatively the semantic relevance of the features in CIFAR-100. Two\\ncategories share a predictive feature on WK if they are 2-neighbour, that is they share a common\\npredictive feature. Let N (ci) returns all pairs (ci, cj) for all j 2-neighbour of ci. Moreover, de\\xef\\xac\\x81ne\\nthe operation is_super(ci, cj) which returns 1 if ci and cj are from the same CIFAR-100 superclass\\nand 0 otherwise. We reproduce the superclass of CIFAR-100 in Table 5. We de\\xef\\xac\\x81ne the semantic\\nrelevance as follows:\\n\\nRelevance(WK) :=\\n\\n(cid:80)\\n\\nC\\n(cid:88)\\n\\ni=1\\n\\n(ci,cj )\\xe2\\x88\\x88N (ci) is_super(ci, cj)\\n|N (ci)|\\n\\n,\\n\\n(4)\\n\\nwhere C = 100 for CIFAR-100 and | \\xc2\\xb7 | is the cardinality of a set.\\n\\nWe compare the semantic relevance of BYOL+SEM with the control experiments BYOL and BYOL\\nwith a representation of the same size as BYOL+SEM but without the normalization. We observe\\nthat using the SEM yields more semantically relevant features than the baseline. This observation is\\nconsistent with the qualitative experiments presented earlier and indicates that the semantics encoded\\n\\nBYOL\\n\\nIN\\n68.3\\nBYOL + SEM 70.6\\n66.7\\nMOCO + SEM 68.0\\n\\nMOCO\\n\\nIN-V2\\n55.3\\n57.9\\n53.4\\n55.0\\n\\n100%\\nIN-R\\n16.5\\n18.1\\n14.0\\n15.21\\n\\nIN-A IN-C\\n35.4\\n0.68\\n38.9\\n0.77\\n0.69\\n31.1\\n33.8\\n0.61\\n\\nIN\\n46.8\\n47.9\\n43.5\\n44.1\\n\\nIN-V2\\n37.5\\n38.5\\n34.2\\n35.9\\n\\n1%\\nIN-R IN-A IN-C\\n0.71\\n12.2\\n25.0\\n25.3\\n12.2\\n0.65\\n0.51\\n20.1\\n8.7\\n21.4\\n0.51\\n9.1\\n\\nTable 1: Test accuracies of a linear probe trained with 100% and 1% of the IMAGENET samples on\\na pre-trained representation trained for 100 epochs. Boldface indicates the maximal value for each\\nevaluation set and each base model type (BYOL or MoCo).\\n\\n7\\n\\norangeappleS190sweet_pepperS181S196pearS29S173S13sharkS355S146S383turtleS185raywhaledolphinS363S338orchidpoppyroseS294tulipS311S343S13601020304050Top K features0.10.20.30.40.50.60.70.8Semantic relevanceBYOLBYOL + large repr.BYOL+SEM\\x0cFOOD CIFAR10 CIFAR-100 SUN DTD PETS FLOWERS CALTECH CARS\\n45.7\\n57.6 71.5 85.4\\nBYOL\\n71.3\\n57.3\\n60.5 72.5 87.1\\nBYOL + SEM 74.1\\n57.6 70.9 82.3\\n39.8\\nMOCO\\n70.6\\nMOCO + SEM 71.0\\n45.2\\n58.6 70.9 83.8\\nTable 2: Transfer learning accuracy by training a linear probe on a pre-trained representation with\\nIMAGENET for 100 epochss. Boldface indicates the maximal value for each transfer dataset and each\\nbase model type (BYOL or MoCo).\\n\\n77.8\\n82.4\\n74.3\\n77.5\\n\\n89.5\\n92.0\\n88.6\\n89.6\\n\\n84.6\\n88.6\\n81.5\\n84.5\\n\\n71.4\\n76.3\\n69.5\\n72.8\\n\\nin the baseline representation may follow a more complicated syntactic structure than those encoded\\nwith the SEM features.\\n\\n4.3 Out-of-distribution evaluation on ImageNet\\n\\nRobustness to out-of-distribution test sets on ImageNet. We perform a comparative study using\\nseveral robustness evaluation sets. Speci\\xef\\xac\\x81cally, we use the validation set provided in IMAGENET;\\nIMAGENET-C, which exhibits a set of common image corruptions [Hendrycks and Dietterich, 2018];\\nIMAGENET-A [Chen et al., 2020a], which contains a set of natural adversarial examples that are\\nmisclassi\\xef\\xac\\x81ed by a Resnet-50 classi\\xef\\xac\\x81er; IMAGENET-R [Hendrycks et al., 2021], which consists of\\ndifferent renderings for several ImageNet classes; and IMAGENET-V2 [Recht et al., 2019], a distinct\\ntest set for ImageNet collected using the same process. We use the methodology proposed in Djolonga\\net al. [2020, 2021] along with their software to perform our experiments.\\n\\nTable 1 shows the performance on these test sets using a linear probe trained with 100% of ImageNet\\xe2\\x80\\x99s\\ndata and 1% of ImageNet\\xe2\\x80\\x99s data. Using the SEM generally leads to an improvement in the in-\\ndistribution and out-of-distribution generalization. Notably, we observe a 2% improvement on BYOL\\ndue to the SEM on in-distribution IMAGENET. On average, there is an improvement of 2% and 0.5%\\nin the 100% and 1% data regimes respectively for BYOL. For MOCO, the average improvement due\\nto the SEM is 1.5% and 0.8% for the 100% and 1% data regimes respectively.\\n\\nTransfer learning on ImageNet. We probe the effect of inducing the SEM in BYOL and MoCo\\non the transfer accuracy to other classi\\xef\\xac\\x81cation tasks from representations trained on IMAGENET.\\nWe follow the linear evaluation methodology described in previous works [Grill et al., 2020; Lee\\net al., 2021], which entails training a linear classi\\xef\\xac\\x81er on the embeddings of the samples for each\\ndataset. We perform our transfer learning experiments on the following datasets: Food [Bossard et al.,\\n2014], CIFAR-10 [Krizhevsky, 2009], CIFAR-100 [Krizhevsky, 2009], SUN [Xiao et al., 2010],\\nDTD [Cimpoi et al., 2014], Pets [Parkhi et al., 2012], Flowers [Nilsback and Zisserman, 2008],\\nCalTech [Fei-Fei et al., 2004] and Cars [Krause et al., 2013].\\n\\nThis task evaluates the generality of the encoder as it has to encode samples from various out-of-\\ndistribution domains with categories that it may not have seen during training. We present our results\\nin Table 2 and observe that the SEM improves the transfer accuracy over the baseline for every\\ndataset.\\n\\n5 Conclusion\\n\\nThis work introduces the Simplicial Embeddings (SEM) as a simple and effective drop-in module\\nfor self-supervised learning that leads to representation with better generalization. Our theoretical\\ninsights demonstrate that the temperature parameter of the SEM allows for control over the trade-off\\nbetween the training loss and expressivity on downstream classi\\xef\\xac\\x81ers; we also observe that controlling\\nthe expressivity via the temperature parameter. We validate our theoretical prediction with a set\\nof controlled experiments. Moreover, we empirically demonstrate that the SEM improves the in-\\ndistribution test accuracy and out-of-distribution accuracy on several robustness test sets and transfer\\nlearning datasets.\\n\\nWe have also demonstrated that the SEM leads to more semantically relevant features for predicting\\nthe categories of a dataset compared to the baseline method. Thus, the SEM embedding may be\\nsimpler than the un-normalized embedding, leading to more interpretable representations. We want\\n\\n8\\n\\n\\x0cto study this in more depth in future works. Related, we would also like to investigate further why the\\nSEM leads to such representations.\\n\\nAcknowledgments and Disclosure of Funding\\n\\nThe authors are grateful for the insightful discussions with Xavier Bouthillier, Michael Noukhovitch,\\nHattie Zhou, S\\xc3\\xa9bastien Lachapelle, Yuchen Lu, Eeshan Dhekane, and Devon Hjelm. We acknowledge\\nfunding support from Samsung and Hitachi, as well as support from Aaron Courville\\xe2\\x80\\x99s CIFAR CCAI\\nchair. We also wish to acknowledge Mila and Compute Canada for providing the computing infras-\\ntructure that enabled this project. Finally, this project would not have been possible without the con-\\ntribution of the following open source projects: Pytorch [Paszke et al., 2019], Orion [Bouthillier et al.,\\n2022], Solo-Learn [da Costa et al., 2021], Scikit-Learn [Pedregosa et al., 2011], and Numpy [Harris\\net al., 2020].\\n\\nReferences\\n\\nMartin Arjovsky, L\\xc3\\xa9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization.\\narXiv:1907.02893 [cs, stat], March 2020. URL http://arxiv.org/abs/1907.02893. arXiv:\\n1907.02893.\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly\\nLearning to Align and Translate. arXiv:1409.0473 [cs, stat], May 2016. URL http://arxiv.\\norg/abs/1409.0473. arXiv: 1409.0473.\\n\\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \\xe2\\x80\\x93 mining discriminative com-\\nponents with random forests. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,\\neditors, Computer Vision \\xe2\\x80\\x93 ECCV 2014, pages 446\\xe2\\x80\\x93461, Cham, 2014. Springer International\\nPublishing. ISBN 978-3-319-10599-4.\\n\\nL\\xc3\\xa9on Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization Methods for Large-Scale Machine\\nLearning. arXiv:1606.04838 [cs, math, stat], February 2018. URL http://arxiv.org/abs/\\n1606.04838. arXiv: 1606.04838.\\n\\nXavier Bouthillier, Christos Tsirigotis, Fran\\xc3\\xa7ois Corneau-Tremblay, Thomas Schweizer, Lin Dong,\\nPierre Delaunay, Fabrice Normandin, Mirko Bronzi, Dendi Suhubdy, Reyhane Askari, Michael\\nNoukhovitch, Chao Xue, Satya Ortiz-Gagn\\xc3\\xa9, Olivier Breuleux, Arnaud Bergeron, Olexa Bilaniuk,\\nSteven Bocco, Hadrien Bertrand, Guillaume Alain, Dmitriy Serdyuk, Peter Henderson, Pascal\\nLamblin, and Christopher Beckham. Epistimio/orion: Asynchronous Distributed Hyperparameter\\nOptimization, March 2022. URL https://doi.org/10.5281/zenodo.3478592.\\n\\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep Clustering for\\nUnsupervised Learning of Visual Features. arXiv:1807.05520 [cs], March 2019. URL http:\\n//arxiv.org/abs/1807.05520. arXiv: 1807.05520.\\n\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\\n\\nUnsupervised learning of visual features by contrasting cluster assignments. 2020.\\n\\nTianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial\\nRobustness: From Self-Supervised Pre-Training to Fine-Tuning. pages 699\\xe2\\x80\\x93708, 2020a. URL\\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Adversarial_\\nRobustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_\\npaper.html.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\\ncontrastive learning of visual representations. In Hal Daum\\xc3\\xa9 III and Aarti Singh, editors, Proceed-\\nings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of\\nMachine Learning Research, pages 1597\\xe2\\x80\\x931607. PMLR, 13\\xe2\\x80\\x9318 Jul 2020b.\\n\\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint\\n\\narXiv:2011.10566, 2020.\\n\\n9\\n\\n\\x0cMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-\\nscribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 3606\\xe2\\x80\\x933613, 2014.\\n\\nVictor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. Solo-learn: A library\\nof self-supervised methods for visual representation learning, 2021. URL https://github.com/\\nvturrisi/solo-learn.\\n\\nJosip Djolonga, Frances Hubis, Matthias Minderer, Zachary Nado, Jeremy Nixon, Rob Romijn-\\nders, Dustin Tran, and Mario Lucic. Robustness Metrics, 2020. URL https://github.com/\\ngoogle-research/robustness_metrics.\\n\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D\\xe2\\x80\\x99Amour, Dan Moldovan, Syl-\\nvain Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On Robustness and Transferabil-\\nity of Convolutional Neural Networks. arXiv:2007.08558 [cs], March 2021. URL http:\\n//arxiv.org/abs/2007.08558. arXiv: 2007.08558.\\n\\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training\\nexamples: An incremental bayesian approach tested on 101 object categories. In 2004 conference\\non computer vision and pattern recognition workshop, pages 178\\xe2\\x80\\x93178. IEEE, 2004.\\n\\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv:1410.5401 [cs],\\n\\nDecember 2014. URL http://arxiv.org/abs/1410.5401. arXiv: 1410.5401.\\n\\nJean-Bastien Grill, Florian Strub, Florent Altch\\xc3\\xa9, Corentin Tallec, Pierre Richemond, Elena\\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\\nBilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent -\\na new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\\nBalcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,\\npages 21271\\xe2\\x80\\x9321284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\\npaper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.\\n\\nCharles R. Harris, K. Jarrod Millman, St\\xc3\\xa9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\\nCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti\\nPicus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\\xc3\\xa1ndez\\ndel R\\xc3\\xado, Mark Wiebe, Pearu Peterson, Pierre G\\xc3\\xa9rard-Marchant, Kevin Sheppard, Tyler Reddy,\\nWarren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming\\nwith NumPy. Nature, 585(7825):357\\xe2\\x80\\x93362, September 2020. doi: 10.1038/s41586-020-2649-2.\\nURL https://doi.org/10.1038/s41586-020-2649-2.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\\nRecognition. arXiv:1512.03385 [cs], December 2015. URL http://arxiv.org/abs/1512.\\n03385. arXiv: 1512.03385.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for\\nUnsupervised Visual Representation Learning. arXiv:1911.05722 [cs], March 2020. URL http:\\n//arxiv.org/abs/1911.05722. arXiv: 1911.05722.\\n\\nDan Hendrycks and Thomas Dietterich. Benchmarking Neural Network Robustness to Common\\nCorruptions and Perturbations. September 2018. URL https://openreview.net/forum?id=\\nHJz6tiCqYm.\\n\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\\nThe Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization.\\narXiv:2006.16241 [cs, stat], July 2021. URL http://arxiv.org/abs/2006.16241. arXiv:\\n2006.16241.\\n\\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\\nand maximization. In International Conference on Learning Representations, 2019. URL https:\\n//openreview.net/forum?id=Bklr3j0cKX.\\n\\n10\\n\\n\\x0cSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd\\nInternational Conference on Machine Learning, volume 37 of Proceedings of Machine Learning\\nResearch, pages 448\\xe2\\x80\\x93456, Lille, France, 07\\xe2\\x80\\x9309 Jul 2015. PMLR. URL https://proceedings.\\nmlr.press/v37/ioffe15.html.\\n\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,\\nAaron Maschinot, Ce Liu, and Dilip Krishnan.\\nIn\\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Ad-\\nvances in Neural Information Processing Systems, volume 33, pages 18661\\xe2\\x80\\x9318673. Cur-\\nran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\\nd89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf.\\n\\nSupervised contrastive learning.\\n\\nAlexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representa-\\n\\ntion learning. CoRR, abs/1901.09005, 2019. URL http://arxiv.org/abs/1901.09005.\\n\\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for \\xef\\xac\\x81ne-grained\\ncategorization. In Proceedings of the IEEE international conference on computer vision workshops,\\npages 554\\xe2\\x80\\x93561, 2013.\\n\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\n\\nKuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive\\nVisual Representations. arXiv:2109.12909 [cs, math], September 2021. URL http://arxiv.\\norg/abs/2109.12909. arXiv: 2109.12909.\\n\\nMaria-Elena Nilsback and Andrew Zisserman. Automated \\xef\\xac\\x82ower classi\\xef\\xac\\x81cation over a large number\\nof classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing,\\npages 722\\xe2\\x80\\x93729, 2008. doi: 10.1109/ICVGIP.2008.47.\\n\\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In 2012\\nIEEE Conference on Computer Vision and Pattern Recognition, pages 3498\\xe2\\x80\\x933505, 2012. doi:\\n10.1109/CVPR.2012.6248092.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\\nhigh-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\'Alch\\xc3\\xa9-\\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,\\npages 8024\\xe2\\x80\\x938035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\\npdf.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\\nE. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,\\n12:2825\\xe2\\x80\\x932830, 2011.\\n\\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classi\\xef\\xac\\x81ers\\nIn Proceedings of the 36th International Conference on Machine\\nGeneralize to ImageNet?\\nLearning, pages 5389\\xe2\\x80\\x935400. PMLR, May 2019. URL https://proceedings.mlr.press/\\nv97/recht19a.html. ISSN: 2640-3498.\\n\\nAaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive Learning of General-Purpose Audio\\nRepresentations. arXiv:2010.10915 [cs, eess], October 2020. URL http://arxiv.org/abs/\\n2010.10915. arXiv: 2010.10915.\\n\\nAad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. Springer\\nNew York, 1996. doi: 10.1007/978-1-4757-2545-2. URL https://doi.org/10.1007%\\n2F978-1-4757-2545-2.\\n\\n11\\n\\n\\x0cAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762 [cs], December 2017.\\nURL http://arxiv.org/abs/1706.03762. arXiv: 1706.03762 version: 5.\\n\\nTan Wang, Zhongqi Yue, Jianqiang Huang, Qianru Sun, and Hanwang Zhang. Self-Supervised\\nLearning Disentangled Group Representation as Feature. May 2021. URL https://openreview.\\nnet/forum?id=RQfcckT1M_4.\\n\\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\\nLarge-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on\\ncomputer vision and pattern recognition, pages 3485\\xe2\\x80\\x933492. IEEE, 2010.\\n\\nAsano Ym, Rupprecht C, and Vedaldi A. Self-labelling via simultaneous clustering and representation\\n\\nlearning. September 2019. URL https://openreview.net/forum?id=Hyx-jyBFPr.\\n\\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph\\ncontrastive learning with augmentations. CoRR, abs/2010.13902, 2020. URL https://arxiv.\\norg/abs/2010.13902.\\n\\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\\xc3\\xa9phane Deny. Barlow twins: Self-supervised\\n\\nlearning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.\\n\\n12\\n\\n\\x0cA Proof of Theorem 1\\n\\nLet us introduce additional notations used in the proofs. De\\xef\\xac\\x81ne r = (z, y) \\xe2\\x88\\x88 R, (cid:96)(f, r) = l(f (z), y),\\n\\n\\xcb\\x9cCy,k1,...,kL = {(z, \\xcb\\x86y) \\xe2\\x88\\x88 Z \\xc3\\x97 Y : \\xcb\\x86y = y, kj = arg max\\n\\nzj,t \\xe2\\x88\\x80j \\xe2\\x88\\x88 [L]},\\n\\nt\\xe2\\x88\\x88[V ]\\n\\nand\\n\\n\\xcb\\x9cZk1,...,kL = {z \\xe2\\x88\\x88 Z : kj = arg max\\n\\nzj,t \\xe2\\x88\\x80j \\xe2\\x88\\x88 [L]}.\\n\\nt\\xe2\\x88\\x88[V ]\\n\\nto be\\n\\nthen de\\xef\\xac\\x81ne Ck\\n\\n=\\nWe\\nthe \\xef\\xac\\x82atten version of\\n{ \\xcb\\x9cCy,k1,...,kL,y}y\\xe2\\x88\\x88Y,k1,...,kL\\xe2\\x88\\x88[V ] with C1 = \\xcb\\x9cC1,1,...,1, C2 = \\xcb\\x9cC2,1,...,1, C|Y| = \\xcb\\x9cC|Y|,1,...,1, C|Y|+1 =\\n\\xcb\\x9cC1,2,1,...,1, C2|Y| = \\xcb\\x9cC|Y|,2,1,...,1, and so on. Similarly, de\\xef\\xac\\x81ne Zk to be the \\xef\\xac\\x82atten version of \\xcb\\x9cZk1,...,kL.\\nWe also use Qi = {q \\xe2\\x88\\x88 [\\xe2\\x88\\x921, +1]V : i = arg maxj\\xe2\\x88\\x88[V ] qj}, Ik := I S\\nk := {i \\xe2\\x88\\x88 [n] : ri \\xe2\\x88\\x88 Ck}, and\\n(cid:80)n\\n\\xce\\xb1k(h) := Er[(cid:96)(h, r)|r \\xe2\\x88\\x88 Ck]. Moreover, we de\\xef\\xac\\x81ne \\xcf\\x95(f S\\nt=1 (cid:107)q \\xe2\\x88\\x92 q(cid:48)(cid:107)2\\n2,\\neqj /\\xcf\\x84\\nand \\xcf\\x95(f S\\nt=1 eqt/\\xcf\\x84 for\\nj = 1, . . . , V .\\n\\nSEM(\\xcf\\x84 )) = supi\\xe2\\x88\\x88[V ] supq,q(cid:48)\\xe2\\x88\\x88Qi\\n\\nbase) = supi\\xe2\\x88\\x88[V ] supq,q(cid:48)\\xe2\\x88\\x88Qi\\n\\nt=1 (cid:107)\\xcf\\x83\\xcf\\x84 (q) \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (q(cid:48))(cid:107)2\\n\\n2 where \\xcf\\x83\\xcf\\x84 (q)j =\\n\\n(cid:80)n\\n\\ni.e.,\\n\\nk=1\\n\\n(cid:80)V\\n\\n{Ck}K\\n\\n\\xcb\\x9cCy,k1,...,kL;\\n\\nWe \\xef\\xac\\x81rst decompose the generalization gap into two terms using the following lemma:\\nLemma 1. For any \\xce\\xb4 > 0, with probability at least 1 \\xe2\\x88\\x92 \\xce\\xb4,the following holds for all h \\xe2\\x88\\x88 H:\\n\\nEr[(cid:96)(h, r)] \\xe2\\x88\\x92\\n\\n(cid:96)(h, ri) \\xe2\\x89\\xa4\\n\\n|Ik|\\n\\n\\xef\\xa3\\xad\\xce\\xb1k(h) \\xe2\\x88\\x92\\n\\n(cid:96)(h, ri)\\n\\n\\xef\\xa3\\xb8 + c\\n\\n\\xef\\xa3\\xab\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\xef\\xa3\\xb6\\n\\n(cid:114)\\n\\nln(2/\\xce\\xb4)\\nn\\n\\n.\\n\\nProof. We \\xef\\xac\\x81rst write the expected error as the sum of the conditional expected error:\\n\\nEr[(cid:96)(h, r)] =\\n\\nEr[(cid:96)(h, r)|r \\xe2\\x88\\x88 Ck] Pr(r \\xe2\\x88\\x88 Ck) =\\n\\nErk [(cid:96)(h, rk)] Pr(r \\xe2\\x88\\x88 Ck),\\n\\nwhere rk is the random variable for the conditional with r \\xe2\\x88\\x88 Ck. Using this, we decompose the\\ngeneralization error into two terms:\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nEr[(cid:96)(h, r)] \\xe2\\x88\\x92\\n\\n(cid:96)(h, ri)\\n\\n=\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nErk [(cid:96)(h, rk)]\\n\\nPr(r \\xe2\\x88\\x88 Ck) \\xe2\\x88\\x92\\n\\nErk [(cid:96)(h, rk)]\\n\\n(cid:19)\\n\\n|Ik|\\nn\\n\\n\\xef\\xa3\\xab\\n\\n+\\n\\n\\xef\\xa3\\xad\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n|Ik|\\nn\\n\\n\\xe2\\x88\\x92\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n\\xef\\xa3\\xb6\\n\\n(cid:96)(h, ri)\\n\\n\\xef\\xa3\\xb8 .\\n\\nThe second term in the right-hand side of (5) is further simpli\\xef\\xac\\x81ed by using\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:18)\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:96)(h, ri) =\\n\\n(cid:96)(h, ri),\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\n(cid:88)\\n\\nk=1\\n\\ni\\xe2\\x88\\x88Ik\\n\\nas\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nErk [(cid:96)(h, rk)]\\n\\n|Ik|\\nn\\n\\n\\xe2\\x88\\x92\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:96)(h, ri) =\\n\\n|Ik|\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\xef\\xa3\\xab\\n\\xef\\xa3\\xadErk [(cid:96)(h, rk)] \\xe2\\x88\\x92\\n\\n\\xef\\xa3\\xb6\\n\\n(cid:96)(h, ri)\\n\\n\\xef\\xa3\\xb8\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\nSubstituting these into equation (5) yields\\nn\\n(cid:88)\\n\\nEr[(cid:96)(h, r)] \\xe2\\x88\\x92\\n\\n(cid:96)(h, ri)\\n\\n1\\nn\\n\\ni=1\\n\\n(cid:18)\\n\\n=\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nErk [(cid:96)(h, rk)]\\n\\nPr(r \\xe2\\x88\\x88 Ck) \\xe2\\x88\\x92\\n\\n(cid:19)\\n\\n+\\n\\n|Ik|\\nn\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\xef\\xa3\\xab\\n\\xef\\xa3\\xadErk [(cid:96)(h, rk)] \\xe2\\x88\\x92\\n\\n|Ik|\\n\\n\\xe2\\x89\\xa4 B\\n\\nPr(r \\xe2\\x88\\x88 Ck) \\xe2\\x88\\x92\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n|Ik|\\nn\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\xef\\xa3\\xab\\n\\xef\\xa3\\xadErk [(cid:96)(h, rk)] \\xe2\\x88\\x92\\n\\n|Ik|\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\n(cid:96)(h, ri)\\n\\n\\xef\\xa3\\xb8\\n\\n(cid:96)(h, ri)\\n\\n\\xef\\xa3\\xb8\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\xef\\xa3\\xb6\\n\\n(5)\\n\\n(6)\\n\\n\\xef\\xa3\\xb6\\n\\n13\\n\\n\\x0cBy using the Bretagnolle-Huber-Carol inequality [van der Vaart and Wellner, 1996, A6.6 Proposition],\\nwe have that for any \\xce\\xb4 > 0, with probability at least 1 \\xe2\\x88\\x92 \\xce\\xb4,\\n(cid:114)\\nK\\n(cid:88)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nk=1\\n\\nPr(r \\xe2\\x88\\x88 Ck) \\xe2\\x88\\x92\\n\\n|Ik|\\nn\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n\\xe2\\x89\\xa4\\n\\n2K ln(2/\\xce\\xb4)\\nn\\n\\n.\\n\\n(cid:12)\\nHere, notice that the term of (cid:80)K\\n(cid:12)\\n(cid:12) does not depend on h \\xe2\\x88\\x88 H. Moreover,\\nnote that for any (f, h, M ) such that M > 0 and B \\xe2\\x89\\xa5 0 for all X, we have that P(f (X) \\xe2\\x89\\xa5 M ) \\xe2\\x89\\xa5\\nP(f (X) > M ) \\xe2\\x89\\xa5 P(Bf (X) + h(X) > BM + h(X)), where the probability is with respect to the\\nrandomness of X. Thus, by combining (6) and (7), we have that for any h \\xe2\\x88\\x88 H, for any \\xce\\xb4 > 0, with\\nprobability at least 1 \\xe2\\x88\\x92 \\xce\\xb4, the following holds for all h \\xe2\\x88\\x88 H,\\n\\n(cid:12)\\n(cid:12)Pr(r \\xe2\\x88\\x88 Ck) \\xe2\\x88\\x92 |Ik|\\n(cid:12)\\n\\nk=1\\n\\nn\\n\\n(7)\\n\\nEr[(cid:96)(h, r)] \\xe2\\x88\\x92\\n\\n(cid:96)(h, ri) \\xe2\\x89\\xa4\\n\\n|Ik|\\n\\n\\xef\\xa3\\xad\\xce\\xb1k(h) \\xe2\\x88\\x92\\n\\n(cid:96)(h, ri)\\n\\n\\xef\\xa3\\xb8 + c\\n\\n\\xef\\xa3\\xab\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\n\\xef\\xa3\\xb6\\n\\n(cid:114)\\n\\nln(2/\\xce\\xb4)\\nn\\n\\n.\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\nIn particular, the \\xef\\xac\\x81rst term from the previous lemma will be bounded with the following lemma:\\nLemma 2. For any f \\xe2\\x88\\x88 {f S\\n\\nSEM(\\xcf\\x84 ), f S\\n\\xef\\xa3\\xab\\n\\nbase},\\n\\n|Ik|\\n\\n\\xef\\xa3\\xad\\xce\\xb1k(f ) \\xe2\\x88\\x92\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\xef\\xa3\\xb6\\n\\n(cid:114)\\n\\n(cid:96)(f, ri)\\n\\n\\xef\\xa3\\xb8 \\xe2\\x89\\xa4 R\\n\\nL\\xcf\\x95(f )\\nn\\n\\n.\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\nProof. By using the triangle inequality,\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n\\xe2\\x89\\xa4\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n\\xef\\xa3\\xab\\n\\xef\\xa3\\xadEr[(cid:96)(f, r)|r \\xe2\\x88\\x88 Ck] \\xe2\\x88\\x92\\n\\n|Ik|\\n\\n\\xef\\xa3\\xb6\\n\\n(cid:96)(f, ri)\\n\\n\\xef\\xa3\\xb8\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\n|Ik|\\n\\nEr[(cid:96)(f, r)|r \\xe2\\x88\\x88 Ck] \\xe2\\x88\\x92\\n\\n(cid:96)(f, ri)\\n\\n.\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nFurthermore, by using the triangle inequality,\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:96)(f, ri)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nEr[(cid:96)(f, r)|r \\xe2\\x88\\x88 Ck] \\xe2\\x88\\x92\\n\\nEr[(cid:96)(f, r)|r \\xe2\\x88\\x88 Ck] \\xe2\\x88\\x92\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:96)(f, ri)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n|Ik|\\n\\n1\\n|Ik|\\n\\n=\\n\\n\\xe2\\x89\\xa4\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\n(cid:88)\\n\\n\\xe2\\x89\\xa4 sup\\n\\nr,r(cid:48)\\xe2\\x88\\x88Ck\\n\\n(cid:12)Er[(cid:96)(f, r)|r \\xe2\\x88\\x88 Ck] \\xe2\\x88\\x92 (cid:96)(f, ri)(cid:12)\\n(cid:12)\\n(cid:12)\\n\\ni\\xe2\\x88\\x88Ik\\n(cid:12)(cid:96)(f, r) \\xe2\\x88\\x92 (cid:96)(f, r(cid:48))(cid:12)\\n(cid:12)\\n(cid:12) .\\n\\nSEM(\\xcf\\x84 ) \\xe2\\x97\\xa6\\xcf\\x83\\xcf\\x84 , since gS\\n\\nSEM(\\xcf\\x84 ) \\xe2\\x88\\x88 GS, by using the Lipschitz continuity, boundedness,\\n\\nIf f = f S\\nand non-negativity,\\n\\nSEM(\\xcf\\x84 ) = gS\\n\\n(cid:12)\\n(cid:12)(cid:96)(f, r) \\xe2\\x88\\x92 (cid:96)(f, r(cid:48))(cid:12)\\n\\nsup\\nr,r(cid:48)\\xe2\\x88\\x88Ck\\n\\n|(ly \\xe2\\x97\\xa6 gS\\n\\nSEM(\\xcf\\x84 ))(\\xcf\\x83\\xcf\\x84 (z)) \\xe2\\x88\\x92 (ly \\xe2\\x97\\xa6 gS\\n\\nSEM(\\xcf\\x84 ))(\\xcf\\x83\\xcf\\x84 (z(cid:48)))|\\n\\nsup\\nz,z(cid:48)\\xe2\\x88\\x88Zk\\n\\n(cid:12) = sup\\ny\\xe2\\x88\\x88Y\\n\\xe2\\x89\\xa4 R sup\\n\\nz,z(cid:48)\\xe2\\x88\\x88Zk\\n\\n(cid:107)\\xcf\\x83\\xcf\\x84 (z) \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (z(cid:48))(cid:107)F\\n\\n(cid:118)\\n(cid:117)\\n(cid:117)\\n(cid:116)\\n\\nL\\n(cid:88)\\n\\nV\\n(cid:88)\\n\\nt=1\\n\\nj=1\\n\\n(\\xcf\\x83\\xcf\\x84 (zt,j) \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (z(cid:48)\\n\\nt,j))2\\n2\\n\\nsup\\ni\\xe2\\x88\\x88[V ]\\n\\nsup\\nq,q(cid:48)\\xe2\\x88\\x88Qi\\n\\n(cid:107)\\xcf\\x83\\xcf\\x84 (q) \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (q(cid:48))(cid:107)2\\n2\\n\\n= R sup\\n\\nz,z(cid:48)\\xe2\\x88\\x88Zk\\n\\nL\\n(cid:88)\\n\\nt=1\\n\\n(cid:118)\\n(cid:117)\\n(cid:117)\\n(cid:116)\\n\\n(cid:115)\\n\\n\\xe2\\x89\\xa4 R\\n\\n= R\\n\\nL\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 ))\\nn\\n\\n14\\n\\n\\x0cbase = gS\\n\\nSimilarly, if f = f S\\nand non-negativity,\\n(cid:12)(cid:96)(f, r) \\xe2\\x88\\x92 (cid:96)(f, r(cid:48))(cid:12)\\n(cid:12)\\n\\nbase, since gS\\n\\nsup\\nr,r(cid:48)\\xe2\\x88\\x88Ck\\n\\nbase \\xe2\\x88\\x88 GS, by using the Lipschitz continuity, boundedness,\\n\\n|(ly \\xe2\\x97\\xa6 gS\\n\\nbase)(z) \\xe2\\x88\\x92 (ly \\xe2\\x97\\xa6 gS\\n\\nbase)(z(cid:48))|\\n\\nsup\\nz,z(cid:48)\\xe2\\x88\\x88Zk\\n\\n(cid:12) = sup\\ny\\xe2\\x88\\x88Y\\n\\xe2\\x89\\xa4 R sup\\n\\n(cid:107)z \\xe2\\x88\\x92 z(cid:48)(cid:107)F\\n\\nz,z(cid:48)\\xe2\\x88\\x88Zk\\n(cid:114)\\n\\nL\\xcf\\x95(f S\\nbase)\\nn\\n\\n.\\n\\n\\xe2\\x89\\xa4 R\\n\\nTherefore, for any f \\xe2\\x88\\x88 {f S\\n\\nSEM(\\xcf\\x84 ), f S\\n\\nbase},\\n\\n\\xef\\xa3\\xab\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n|Ik|\\n\\n\\xef\\xa3\\xad\\xce\\xb1k(f ) \\xe2\\x88\\x92\\n\\n(cid:96)(f, ri)\\n\\n\\xef\\xa3\\xb8 \\xe2\\x89\\xa4\\n\\n1\\n|Ik|\\n\\n(cid:88)\\n\\ni\\xe2\\x88\\x88Ik\\n\\n\\xef\\xa3\\xb6\\n\\n1\\nn\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n|Ik|R(cid:112)L\\xcf\\x95(f ) = R\\n\\n(cid:114)\\n\\nL\\xcf\\x95(f )\\nn\\n\\n.\\n\\nCombining Lemma 1 and Lemma 2, we obtain the following upper bound on the gap:\\nLemma 3. For any \\xce\\xb4 > 0, with probability at least 1 \\xe2\\x88\\x92 \\xce\\xb4,the following holds for any f \\xe2\\x88\\x88\\n{f S\\n\\nSEM(\\xcf\\x84 ), f S\\n\\nbase}:\\n\\nEr[(cid:96)(f, r)] \\xe2\\x88\\x92\\n\\n(cid:96)(f, ri) \\xe2\\x89\\xa4 R\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:114)\\n\\n(cid:114)\\n\\nL\\xcf\\x95(f )\\nn\\n\\n+ c\\n\\nln(2/\\xce\\xb4)\\nn\\n\\n.\\n\\nProof. This follows directly from combining Lemma 1 and Lemma 2.\\n\\nWe now provide an upper bound on \\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) in the following lemma:\\n\\nLemma 4. For any \\xcf\\x84 > 0,\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 ))\\nn\\n\\n\\xe2\\x89\\xa4\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+ (V \\xe2\\x88\\x92 1)\\n\\n1\\n1 + e\\xe2\\x88\\x86/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x922/\\xcf\\x84 )\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + e2/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84 )\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\nProof. Recall the de\\xef\\xac\\x81nition:\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 ))\\nn\\n\\n= sup\\ni\\xe2\\x88\\x88[V ]\\n\\nsup\\nq,q(cid:48)\\xe2\\x88\\x88Qi\\n\\n(cid:107)\\xcf\\x83\\xcf\\x84 (q) \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (q(cid:48))(cid:107)2\\n2.\\n\\n\\xcf\\x83\\xcf\\x84 (q)j =\\n\\neqj /\\xcf\\x84\\nt=1 eqt/\\xcf\\x84\\n\\n,\\n\\n(cid:80)V\\n\\nwhere\\n\\nand\\n\\nfor j = 1, . . . , V . By the symmetry and independence over i \\xe2\\x88\\x88 [V ] inside of the \\xef\\xac\\x81rst supremum, we\\nhave\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 ))\\nn\\n\\n= sup\\n\\nq,q(cid:48)\\xe2\\x88\\x88Q1\\n\\n(cid:107)\\xcf\\x83\\xcf\\x84 (q) \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (q(cid:48))(cid:107)2\\n2.\\n\\nFor any q, q(cid:48) \\xe2\\x88\\x88 Q1 and i \\xe2\\x88\\x88 {2, . . . , V } (with q = (q1, . . . , qV ) and q(cid:48) = (q(cid:48)\\n\\xce\\xb4i, \\xce\\xb4(cid:48)\\n\\ni > 0 such that\\n\\n1, . . . , q(cid:48)\\n\\nV )), there exists\\n\\nHere, since zik \\xe2\\x88\\x92 \\xe2\\x88\\x86 \\xe2\\x89\\xa5 zij from the assumption, we have that for all i \\xe2\\x88\\x88 {2, . . . , V },\\n\\nqi = q1 \\xe2\\x88\\x92 \\xce\\xb4i\\n\\ni = q(cid:48)\\nq(cid:48)\\n\\n1 \\xe2\\x88\\x92 \\xce\\xb4(cid:48)\\ni.\\n\\n\\xce\\xb4i, \\xce\\xb4(cid:48)\\n\\ni \\xe2\\x89\\xa5 \\xe2\\x88\\x86 > 0.\\n\\n15\\n\\n\\x0cThus, we can rewrite\\n\\nSimilarly,\\n\\nUsing these,\\n\\nV\\n(cid:88)\\n\\nt=1\\n\\nV\\n(cid:88)\\n\\nt=1\\n\\neqt/\\xcf\\x84 = eq1/\\xcf\\x84 +\\n\\ne(q1\\xe2\\x88\\x92\\xce\\xb4i)/\\xcf\\x84\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\n= eq1/\\xcf\\x84 + eq1/\\xcf\\x84\\n\\ne\\xe2\\x88\\x92\\xce\\xb4i/\\xcf\\x84\\n\\n= eq1/\\xcf\\x84\\n\\n\\xef\\xa3\\xad1 +\\n\\ne\\xe2\\x88\\x92\\xce\\xb4i/\\xcf\\x84\\n\\n\\xef\\xa3\\xab\\n\\n\\xef\\xa3\\xab\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\nV\\n(cid:88)\\n\\ni=2\\n\\n\\xef\\xa3\\xb6\\n\\n\\xef\\xa3\\xb8\\n\\n\\xef\\xa3\\xb6\\n\\neq(cid:48)\\n\\nt/\\xcf\\x84 = eq(cid:48)\\n\\n1/\\xcf\\x84\\n\\n\\xef\\xa3\\xad1 +\\n\\ne\\xe2\\x88\\x92\\xce\\xb4(cid:48)\\n\\ni/\\xcf\\x84\\n\\n\\xef\\xa3\\xb8 .\\n\\n\\xcf\\x83\\xcf\\x84 (q)1 =\\n\\neq1/\\xcf\\x84\\nt=1 eqt/\\xcf\\x84\\n\\n(cid:80)V\\n\\n=\\n\\neq1/\\xcf\\x84\\n1 + (cid:80)V\\n\\n(cid:16)\\n\\neq1/\\xcf\\x84\\n\\ni=2 e\\xe2\\x88\\x92\\xce\\xb4i/\\xcf\\x84\\n\\n(cid:17) =\\n\\n1\\n1 + (cid:80)V\\ni=2 e\\xe2\\x88\\x92\\xce\\xb4i/\\xcf\\x84\\n\\nand for all j \\xe2\\x88\\x88 {2, . . . , V },\\n\\n\\xcf\\x83\\xcf\\x84 (q)j =\\n\\neqj /\\xcf\\x84\\nt=1 eqt/\\xcf\\x84\\n\\n(cid:80)V\\n\\n=\\n\\n=\\n\\n=\\n\\ne(q1\\xe2\\x88\\x92\\xce\\xb4j )/\\xcf\\x84\\n1 + (cid:80)V\\n\\n(cid:16)\\n\\n(cid:17)\\n\\ni=2 e\\xe2\\x88\\x92\\xce\\xb4i/\\xcf\\x84\\n\\neq1/\\xcf\\x84\\n\\ne\\xe2\\x88\\x92\\xce\\xb4j /\\xcf\\x84\\n\\n1 + (cid:80)V\\n\\ni=2 e\\xe2\\x88\\x92\\xce\\xb4i/\\xcf\\x84\\n1\\n1 + e\\xce\\xb4j /\\xcf\\x84 + (cid:80)V\\n\\ne(\\xce\\xb4j \\xe2\\x88\\x92\\xce\\xb4i)/\\xcf\\x84\\n\\ni\\xe2\\x88\\x88Ij\\n\\n\\xcf\\x83\\xcf\\x84 (q(cid:48))1 =\\n\\n1\\n1 + (cid:80)V\\ni=2 e\\xe2\\x88\\x92\\xce\\xb4(cid:48)\\n\\ni/\\xcf\\x84\\n\\n,\\n\\n\\xcf\\x83\\xcf\\x84 (q(cid:48))j =\\n\\n1\\nj /\\xcf\\x84 + (cid:80)V\\n\\n1 + e\\xce\\xb4(cid:48)\\n\\ne(\\xce\\xb4(cid:48)\\n\\nj \\xe2\\x88\\x92\\xce\\xb4(cid:48)\\n\\ni)/\\xcf\\x84\\n\\ni\\xe2\\x88\\x88Ij\\n\\n.\\n\\nwhere Ij := {2, . . . , V } \\\\ {j}. Similarly,\\n\\nand for all j \\xe2\\x88\\x88 {2, . . . , V },\\n\\nUsing these, for any q, q(cid:48) \\xe2\\x88\\x88 Q1,\\n\\n|\\xcf\\x83\\xcf\\x84 (q)1 \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (q(cid:48))1| =\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n\\xe2\\x89\\xa4\\n\\n=\\n\\n1\\n1 + (cid:80)V\\ni=2 e\\xe2\\x88\\x92\\xce\\xb4i/\\xcf\\x84\\n1\\n1 + (cid:80)V\\ni=2 e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\ni/\\xcf\\x84\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1 + (cid:80)V\\ni=2 e\\xe2\\x88\\x92\\xce\\xb4(cid:48)\\n1\\n1 + (cid:80)V\\ni=2 e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n,\\n\\n16\\n\\n\\x0c(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n\\xe2\\x89\\xa4\\n\\n=\\n\\n=\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nand for all j \\xe2\\x88\\x88 {2, . . . , V },\\n\\n|\\xcf\\x83\\xcf\\x84 (q)j \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (q(cid:48))j| =\\n\\n1\\n1 + e\\xce\\xb4j /\\xcf\\x84 + (cid:80)V\\n\\ne(\\xce\\xb4j \\xe2\\x88\\x92\\xce\\xb4i)/\\xcf\\x84\\n\\ni\\xe2\\x88\\x88Ij\\n\\n\\xe2\\x88\\x92\\n\\n1\\nj /\\xcf\\x84 + (cid:80)V\\n\\n1 + e\\xce\\xb4(cid:48)\\n\\ne(\\xce\\xb4(cid:48)\\n\\nj \\xe2\\x88\\x92\\xce\\xb4(cid:48)\\n\\ni)/\\xcf\\x84\\n\\ni\\xe2\\x88\\x88Ij\\n\\n1\\n1 + e\\xe2\\x88\\x86/\\xcf\\x84 + (cid:80)V\\n\\ne(\\xe2\\x88\\x86\\xe2\\x88\\x922)/\\xcf\\x84\\n\\ni\\xe2\\x88\\x88Ij\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + e2/\\xcf\\x84 + (cid:80)V\\n\\ne(2\\xe2\\x88\\x92\\xe2\\x88\\x86)/\\xcf\\x84\\n\\ni\\xe2\\x88\\x88Ij\\n\\n1\\n1 + e\\xe2\\x88\\x86/\\xcf\\x84 + (V \\xe2\\x88\\x92 2)e(\\xe2\\x88\\x86\\xe2\\x88\\x922)/\\xcf\\x84\\n\\n1\\n1 + e2/\\xcf\\x84 + (V \\xe2\\x88\\x92 2)e(2\\xe2\\x88\\x92\\xe2\\x88\\x86)/\\xcf\\x84\\n\\n1\\n1 + e\\xe2\\x88\\x86/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x922/\\xcf\\x84 )\\n\\n1\\n1 + e2/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84 )\\n\\n.\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nBy combining these,\\n\\nsup\\nq,q(cid:48)\\xe2\\x88\\x88Q1\\n\\n(cid:107)\\xcf\\x83\\xcf\\x84 (q) \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (q(cid:48))(cid:107)2\\n2\\n\\n= sup\\n\\nq,q(cid:48)\\xe2\\x88\\x88Q1\\n\\nV\\n(cid:88)\\n\\nj=1\\n\\n|\\xcf\\x83\\xcf\\x84 (q)j \\xe2\\x88\\x92 \\xcf\\x83\\xcf\\x84 (q(cid:48))j|2\\n\\n\\xe2\\x89\\xa4\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\n+ (V \\xe2\\x88\\x92 1)\\n\\n1\\n1 + e\\xe2\\x88\\x86/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x922/\\xcf\\x84 )\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + e2/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84 )\\n\\nUsing the previous lemma, we will conclude the asymptotic behavior of \\xcf\\x95(f S\\nlemma:\\nLemma 5. It holds that\\n\\nSEM(\\xcf\\x84 )) in the following\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x86\\x92 0 as \\xcf\\x84 \\xe2\\x86\\x92 0.\\n\\nProof. Using Lemma 4,\\n\\nlim\\n\\xcf\\x84 \\xe2\\x86\\x920\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x89\\xa4 lim\\n\\xcf\\x84 \\xe2\\x86\\x920\\n\\nn\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+ n(V \\xe2\\x88\\x92 1) lim\\n\\xcf\\x84 \\xe2\\x86\\x920\\n\\n1\\n1 + e\\xe2\\x88\\x86/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x922/\\xcf\\x84 )\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + e2/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84 )\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\nlim\\n\\xcf\\x84 \\xe2\\x86\\x920\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n=\\n\\n\\xe2\\x88\\x92\\n\\n= 0,\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n1\\n\\nlim\\n\\xcf\\x84 \\xe2\\x86\\x920\\n\\n1\\n1 + e\\xe2\\x88\\x86/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x922/\\xcf\\x84 )\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + e2/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84 )\\n\\n= |0 \\xe2\\x88\\x92 0|2 = 0.\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nMoreover,\\n\\nand\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nTherefore,\\n\\nSince \\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x89\\xa5 0, this implies the statement of this lemma.\\n\\nlim\\n\\xcf\\x84 \\xe2\\x86\\x920\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x89\\xa4 0.\\n\\n17\\n\\n\\x0cAs we have analyzed \\xcf\\x95(f S\\nSEM(\\xcf\\x84 )) and \\xcf\\x95(f S\\n\\xcf\\x95(f S\\nLemma 6. For any \\xcf\\x84 > 0,\\n\\nSEM(\\xcf\\x84 )) in the previous two lemmas, we are now ready to compare\\n\\nbase), which is done in the following lemma:\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x88\\x92 \\xcf\\x95(f S\\n\\nbase) \\xe2\\x89\\xa4\\n\\n(1 \\xe2\\x88\\x92 V ) < 0.\\n\\n3n\\n4\\n\\nProof. From Lemma 4, for any \\xcf\\x84 > 0,\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x89\\xa4 n\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n1\\n(cid:12)\\n(cid:12)\\n1\\n(cid:12)\\n(cid:18) 1\\n1\\n\\n+ n(V \\xe2\\x88\\x92 1)\\n\\n1\\n1 + e\\xe2\\x88\\x86/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x922/\\xcf\\x84 )\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + e2/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x92\\xe2\\x88\\x86/\\xcf\\x84 )\\n\\n\\xe2\\x89\\xa4 n\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n+ n(V \\xe2\\x88\\x92 1)\\n\\n1\\n1 + (1 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x922/\\xcf\\x84 )\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + e2/\\xcf\\x84 (1 + (V \\xe2\\x88\\x92 2))\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n= n\\n\\n1\\n1 + (V \\xe2\\x88\\x92 1)e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n+ n(V \\xe2\\x88\\x92 1)\\n\\n1\\n2 + (V \\xe2\\x88\\x92 2)e\\xe2\\x88\\x922/\\xcf\\x84\\n\\n\\xe2\\x88\\x92\\n\\n1\\n1 + e2/\\xcf\\x84 (V \\xe2\\x88\\x92 1)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n\\xe2\\x89\\xa4 n\\n\\n\\xe2\\x88\\x92\\n\\n+ n(V \\xe2\\x88\\x92 1)\\n\\n= n\\n\\n\\xe2\\x88\\x92\\n\\n+ n(V \\xe2\\x88\\x92 1)\\n\\n1\\nV\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:19)2\\n\\n1\\nV\\n\\n1\\nV\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\n2\\n\\n2\\n\\n(cid:12)\\n(cid:12)\\n\\xe2\\x88\\x92 0\\n(cid:12)\\n(cid:12)\\n\\n1\\n4\\n\\n.\\n\\n(cid:12)\\n2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nRecall the de\\xef\\xac\\x81nition of\\n\\n\\xcf\\x95(f S\\n\\nbase) = sup\\ni\\xe2\\x88\\x88[V ]\\n\\nsup\\nq,q(cid:48)\\xe2\\x88\\x88Qi\\n\\nn(cid:107)q \\xe2\\x88\\x92 q(cid:48)(cid:107)2\\n2.\\n\\nBy choosing an element in the set over which the supremum is taken, for any \\xce\\xb4 \\xe2\\x89\\xa5 \\xe2\\x88\\x86 > 0,\\n\\n\\xcf\\x95(f S\\n\\nbase) \\xe2\\x89\\xa5 sup\\n\\nn(cid:107)q \\xe2\\x88\\x92 q(cid:48)(cid:107)2\\n\\n2 \\xe2\\x89\\xa5 n(cid:107)\\xcb\\x86q \\xe2\\x88\\x92 \\xcb\\x86q(cid:48)(cid:107)2\\n\\n2 = n\\n\\n(\\xcb\\x86qj \\xe2\\x88\\x92 \\xcb\\x86q(cid:48)\\n\\nj)2\\n\\n2 = n(2 \\xe2\\x88\\x92 \\xce\\xb4)2V,\\n\\nq,q(cid:48)\\xe2\\x88\\x88Q1\\n\\nV\\n(cid:88)\\n\\nj=1\\n\\nwhere \\xcb\\x86q1 = 1, \\xcb\\x86qj = 1 \\xe2\\x88\\x92 \\xce\\xb4 for j \\xe2\\x88\\x88 {2, . . . , V }, \\xcb\\x86q(cid:48)\\nBy combining those, for for any \\xcf\\x84 > 0 and \\xce\\xb4 \\xe2\\x89\\xa5 \\xe2\\x88\\x86 > 0,\\n\\n1 = \\xce\\xb4 \\xe2\\x88\\x92 1, and \\xcb\\x86q(cid:48)\\n\\nj = \\xe2\\x88\\x921 for j \\xe2\\x88\\x88 {2, . . . , V }.\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x88\\x92 \\xcf\\x95(f S\\nn\\n\\nbase)\\n\\n\\xe2\\x89\\xa4\\n\\n(cid:18) 1\\n1\\n\\n(cid:19)2\\n\\n1\\nV\\n\\n+ (V \\xe2\\x88\\x92 1)\\n\\n\\xe2\\x88\\x92 (2 \\xe2\\x88\\x92 \\xce\\xb4)2V\\n\\n1\\n4\\n\\n\\xe2\\x89\\xa4 1 +\\n\\nV \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92 (2 \\xe2\\x88\\x92 \\xce\\xb4)2V\\n\\n1\\n4\\n\\n=\\n\\n=\\n\\n+\\n\\n\\xe2\\x88\\x92 V\\n\\nV \\xe2\\x88\\x92 (2 \\xe2\\x88\\x92 \\xce\\xb4)2V\\n(cid:18)\\n\\n(2 \\xe2\\x88\\x92 \\xce\\xb4)2 \\xe2\\x88\\x92\\n\\n(cid:19)\\n\\n1\\n4\\n\\n(cid:18)\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\x92 V\\n\\n1 \\xe2\\x88\\x92\\n\\n(cid:19)\\n\\n1\\n4\\n\\n=\\n\\n(1 \\xe2\\x88\\x92 V )\\n\\n3\\n4\\n3\\n4\\n3\\n4\\n3\\n4\\n\\n\\xe2\\x88\\x92\\n\\n1\\n4\\n1\\n4\\n\\n18\\n\\n\\x0cWe combine the lemmas above to prove Theorem 1, which is restated below with its proof:\\n\\nTheorem 1. Let V \\xe2\\x89\\xa5 2. For any \\xce\\xb4 > 0, with probability at least 1 \\xe2\\x88\\x92 \\xce\\xb4, the following holds for any\\nfS \\xe2\\x88\\x88 {f S\\n\\nSEM(\\xcf\\x84 ), f S\\n\\nbase}:\\n\\nEz,y[l(fS(z), y)] \\xe2\\x89\\xa4\\n\\nl(fS(z(i)), y(i)) + R\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:114)\\n\\n(cid:114)\\n\\nL\\xcf\\x95(fS)\\nn\\n\\n+ c\\n\\nln(2/\\xce\\xb4)\\nn\\n\\n.\\n\\nMoreover,\\n\\n\\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x86\\x92 0 as \\xcf\\x84 \\xe2\\x86\\x92 0\\n\\nand \\xcf\\x95(f S\\n\\nSEM(\\xcf\\x84 )) \\xe2\\x88\\x92 \\xcf\\x95(f S\\n\\nbase) \\xe2\\x89\\xa4\\n\\n(1 \\xe2\\x88\\x92 V ) < 0 \\xe2\\x88\\x80\\xcf\\x84 > 0.\\n\\n3n\\n4\\n\\nProof. The \\xef\\xac\\x81rst statement directly follows from Lemma 3. The second statement is proven by\\nLemma 5 and Lemma 6.\\n\\nB Hyperparameters\\n\\nWe present the hype-parameters used to train for BYOL+SEM on CIFAR100. The same parameters\\nwere used for ResNet18 and ResNet50.\\n\\nLearning rate\\nWeight-decay\\nOptimizer\\nBYOL EMA\\nVocabulary size (V)\\nMessage length (L)\\n\\xcf\\x84 online network\\n\\xcf\\x84 target network\\n\\n0.5\\n1e-4\\nAdamW\\n0.99\\n13\\n5000\\n0.5\\n0.5\\n\\n(a) BYOL+SEM hyper-parameters.\\n\\nC Experiment details for ImageNet\\n\\nC.1\\n\\nImage augmentation\\n\\nWe follow the same procedure as [Grill et al., 2020] for the image augmentation procedure. The\\naugmentation applied in order during training are:\\n\\n\\xe2\\x80\\xa2 Random Resize crop to a 224 \\xc3\\x97 224 image. A random patch of the image is selected and\\n\\nresized to a 224 \\xc3\\x97 224 image.\\n\\n\\xe2\\x80\\xa2 Random color jitter. Modifying the brightness, the contrast, the saturation and the hue.\\n\\n\\xe2\\x80\\xa2 Random gray scale. Randomly applying a gray scale \\xef\\xac\\x81lter to the image\\n\\n\\xe2\\x80\\xa2 Random gaussian blur. Randomly applying a gaussian blue \\xef\\xac\\x81lter.\\n\\n\\xe2\\x80\\xa2 Random solarization. Randomly applying a solarization \\xef\\xac\\x81lter.\\n\\nAt validation and test time, we resize the images to 256 \\xc3\\x97 256 and then center crop a patch of\\n224 \\xc3\\x97 224.\\n\\nFor both training and evaluation, we re-normalize the image using the statistic of the training set.\\n\\nC.2 Hyper-parameters\\n\\nWe summarize the hyper-parameters for BYOL with SEM and MoCo with SEM in table 4.\\n\\n19\\n\\n\\x0cLearning rate\\nBatch size\\nWeight-decay\\nOptimizer\\nEpochs\\nBase momentum\\nVocabulary size (V)\\nMessage length (L)\\n\\xcf\\x84 online network\\n\\xcf\\x84 target network\\n\\n0.9\\n256\\n1e-6\\nSGD with lars\\n100\\n0.99\\n29\\n465\\n2.397\\n2.386\\n\\n(a) BYOL+SEM hyper-parameters.\\n\\nLearning rate\\nBatch size\\nWeight-decay\\nOptimizer\\nEpochs\\nMoCo\\xe2\\x80\\x99s EMA\\nVocabulary size (V)\\nMessage Length (L)\\n\\xcf\\x84 online network\\n\\xcf\\x84 target network\\n\\n0.6\\n256\\n3e-5\\nSGD with lars\\n100\\n0.1\\n12\\n512\\n1.35\\n1.2\\n\\n(b) MoCo+SEM hyper-parameters.\\n\\nTable 4: ImageNet\\xe2\\x80\\x99s experiment hyper-parameters.\\n\\nC.3 Linear evaluation\\n\\nWe follow the evaluation protocol from [Chen et al., 2020b]. The linear evaluation is done by training\\na linear classi\\xef\\xac\\x81er on the frozen representation of the ImageNet training samples. We train a linear\\nclassi\\xef\\xac\\x81er with a cross-entropy objective for 100 epochs using SGD with nesterov and a batch size of\\n512. During training, we apply random resized crop and random horizontal \\xef\\xac\\x82ip.\\n\\nC.4 Semi-supervised learning\\n\\nWe perform semi-supervised experiments by training a linear classi\\xef\\xac\\x81er on top of a frozen repre-\\nsentation. The procedure is the same as the linear evaluation procedure with the exception that we\\ntrain with 1% of the training sample. That training samples are taken according to the split de\\xef\\xac\\x81ned\\nin [Chen et al., 2020b].\\n\\nC.5 Robustness experiments\\n\\nWe follow the evaluation procedure from [Lee et al., 2021]. We treated the robustness datasets as\\nadditional \"test sets\" in that we simply evaluated them using the evaluation procedure described\\nabove. The images were resized to a 256 \\xc3\\x97 256 before being center cropped to a 224 \\xc3\\x97 224 image.\\nThe evaluation procedure was performed using the public robustness benchmark evaluation code\\nof [Djolonga et al., 2020]2.\\n\\nC.6 Transfer learning experiments\\n\\nWe follow the linear evaluation protocol of [Kolesnikov et al., 2019; Chen et al., 2020b] We train a\\nlinear classi\\xef\\xac\\x81er using a regularized multinomial logistic regression from the scikit-learn package [Pe-\\ndregosa et al., 2011]. The representation is frozen, so that we do not train the encoder backbone nor\\n\\n2https://github.com/google-research/robustness_metrics\\n\\n20\\n\\n\\x0cSuperclass\\naquatic mammals\\n\\xef\\xac\\x81sh\\n\\xef\\xac\\x82owers\\nfood containers\\nfruit and vegetables\\nhousehold electrical devices\\nhousehold furniture\\ninsects\\nlarge carnivores\\nlarge man-made outdoor things\\nlarge natural outdoor scenes\\nlarge omnivores and herbivores\\nmedium-sized mammals\\nnon-insect invertebrates\\npeople\\nreptiles\\nsmall mammals\\ntrees\\nvehicles 1\\nvehicles 2\\n\\nClasses\\nbeaver, dolphin, otter, seal, whale\\naquarium \\xef\\xac\\x81sh, \\xef\\xac\\x82at\\xef\\xac\\x81sh, ray, shark, trout\\norchids, poppies, roses, sun\\xef\\xac\\x82owers, tulips\\nbottles, bowls, cans, cups, plates\\napples, mushrooms, oranges, pears, sweet peppers\\nclock, computer keyboard, lamp, telephone, television\\nbed, chair, couch, table, wardrobe\\nbee, beetle, butter\\xef\\xac\\x82y, caterpillar, cockroach\\nbear, leopard, lion, tiger, wolf\\nbridge, castle, house, road, skyscraper\\ncloud, forest, mountain, plain, sea\\ncamel, cattle, chimpanzee, elephant, kangaroo\\nfox, porcupine, possum, raccoon, skunk\\ncrab, lobster, snail, spider, worm\\nbaby, boy, girl, man, woman\\ncrocodile, dinosaur, lizard, snake, turtle\\nhamster, mouse, rabbit, shrew, squirrel\\nmaple, oak, palm, pine, willow\\nbicycle, bus, motorcycle, pickup truck, train\\nlawn-mower, rocket, streetcar, tank, tractor\\n\\nTable 5: Set of classes for each superclass on CIFAR-100.\\n\\nthe batch-normalization statistics. We do not perform any augmentations and the images are resized\\nto 224 pixels using bicubic resampling and the normalized using the statistics on ImageNet\\xe2\\x80\\x99s training\\nset. We tune the regularizer term from a range of 11 logarithmically-spaced values between 10\\xe2\\x88\\x926 and\\n105 using a small validation set and re-train using the full training set.\\n\\nD CIFAR100 superclass\\n\\nThe 100 classes of CIFAR-100 [Krizhevsky, 2009] are grouped into 20 superclasses. The list of\\nsuperclass for each class in Table 5\\n\\nE Additional CIFAR-100 relevance graphs\\n\\n21\\n\\n\\x0c(a) BYOL baseline\\n\\n(b) BYOL baseline with a large rep-\\nresentation\\n\\n(c) BYOL + SEM\\n\\nFigure 5: Comparison of the full relevance graph W5 between BYOL and BYOL + SEM.\\n\\n22\\n\\nS363otterkangaroobottleS382lampS225beartankS144streetcarroadS249S167trainS94lobsterS273boygirlhousewomanS29S106cockroachS6appledolphinsealwhaleaquarium_fishflatfishraysharkorchidpoppyrosesunflowertulipbowlcancupplatemushroomorangepearsweet_pepperclockcomputer_keyboardtelephonetelevisionbedchaircouchtablewardrobebeebeetlebutterflycaterpillarleopardtigerwolfbridgecastleskyscrapercloudforestmountainplainseacamelcattlechimpanzeefoxpossumraccoonskunkcrabsnailwormbabymanlizardsnaketurtlehamstermouserabbitshrewsquirrelmaple_treeoak_treepalm_treewillow_treebicyclebusmotorcyclepickup_trucklawn_mowerrockettractorS55S63S47S279S162S69S134S242S34S241S0S98S158S361S91S66S12S95S388S53S374S190S22S59S183S261S1S375S27S35S130S110S127S43S111S258S380S214S30S85S292S193S238S336S128S132S284S14S99S93S332S236S298S260S281S68S204S161S182S177S381S64S227S320S318S266S230S196S126S181S184S114S187flatfishtableS16troutcaterpillarS194cattleS276orchidcomputer_keyboardS229palm_treeS184cloudrocketskunkchimpanzeeS256S187pickup_trucksnailcameltigerspiderS307dolphinsweet_pepperS85lionS434S415lobstersunflowermushroomS66orangesquirrelS189S319manS190snakeS195babywardrobewolfsealwillow_treecanS323maple_treeS252S404turtleleopardhamsterrabbitcockroachS353clockforestS87S61pine_treeS369couchS397tankotterwomanbicycleS365S148bedbeavertulipbeelawn_mowercrocodilelamptelephoneS435tractorS422oak_treeS233motorcycleshrewS284S33S146S13plainS78whalebusbeeS248spiderbeetlecockroachS178S240bearkangarootigerS393leopardhousecastleS40roadS107plainS403motorcyclelawn_mowertankS350tractorsealS61beaverS284porcupinebedS168couchS360chairlionfoxS216wolfS272skyscraperS227mountainS78seasnakeS98S49S219S2lizardwormorangeS185S30sweet_pepperpearS198S193applebusS18S133streetcarS370S106pickup_trucktrainroseS306poppyS289S329tulipS334S135orchidS210cancupS31S259S321S291bowlplatebottlewillow_treepine_treemaple_treeoak_treeforestS71palm_treeS138S327S405dolphinrayS151S347S17S379turtlesharkS176S189S355whaleS381S221S200S22boyS157girlwomanbabyS204flatfishmanmouseraccoonS336S271S300possumsquirrelS239S128shrewhamsterrabbit\\x0c', b'2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\n\\nO\\nC\\n.\\nh\\nt\\na\\nm\\n\\n[\\n \\n \\n1\\nv\\n5\\n1\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nLARGE-SCALE ROOK PLACEMENTS\\n\\nPAKAWUT JIRADILOK\\n\\nAbstract. For each certain \\xe2\\x80\\x9cnice\\xe2\\x80\\x9d piecewise linear function f : [0, 1] \\xe2\\x86\\x92 [0, 1], we con-\\nsider a family of growing Young diagrams {\\xce\\xbb(f, N )}\\xe2\\x88\\x9e\\nN=1 by enlarging the region under\\nthe graph of f . We compute asymptotic formulas for the number of rook placements\\nof the shape \\xce\\xbb(f, N ). We prove that the normalized cumulative X-ray of a uniformly\\nrandom permutation, as the size of the permutation grows, exhibits a limit shape phe-\\nnomenon.\\n\\n1. Introduction\\n\\nWhile rook placements are classical objects in combinatorics (cf. e.g. [Rio02, Chapter 7]\\nand [Sta12, Chapter 2]), there are many recent works in the literature studying them (e.g.\\n[BR06, BLRS14, BLRS16, Bar21]). Enumerative combinatorics of rook placements deals\\nwith problems of counting the number of ways to place a certain number of non-attacking\\nrooks on a certain subset of a chessboard. In many cases, one obtains nice formulas. For\\ninstance, it is a well-known elementary exercise that if the subset of the chessboard takes\\nthe shape of a Young diagram of a partition, then the number of rook placements has a nice\\nproduct formula.\\n\\nIn this paper, we study large-scale rook placements. We are interested in the family of\\nrook placements when the board on which non-attacking rooks are placed grows in size in\\nthe following manner. We de\\xef\\xac\\x81ne a class\\nof \\xe2\\x80\\x9cnice\\xe2\\x80\\x9d piecewise linear functions from [0, 1] to\\nbelongs to this class.\\n[0, 1] (see the exact de\\xef\\xac\\x81nition in Subsection 3.1). Suppose that f\\nWe obtain a family of Young diagrams by dilating the region under the graph of f from the\\n[0, N ] for each positive integer N . More precisely, we let\\nunit square [0, 1]\\n\\xce\\xbb(f, N ) be the Young diagram with N rows whose ith row has (\\xce\\xbb(f, N ))i :=\\nf (i/N )\\n\\xe2\\x8c\\x89\\nboxes (see Subsection 3.2 for details). Let RP(\\xce\\xbb(f, N )) denote the set of rook placements of\\nthe shape \\xce\\xbb(f, N ). Our \\xef\\xac\\x81rst main result of this paper, Theorem 3.7, says that the cardinality\\nof RP(\\xce\\xbb(f, N )) behaves well asymptotically:\\n\\n[0, 1] to [0, N ]\\n\\n\\xe2\\x88\\x88 P\\n\\nN\\n\\n\\xc3\\x97\\n\\n\\xc3\\x97\\n\\nP\\n\\n\\xe2\\x8c\\x88\\n\\n\\xc2\\xb7\\n\\nlog (# RP(\\xce\\xbb(f, N ))) = N log N + Bf \\xc2\\xb7\\n\\nN +\\n\\nlog N + Of (1),\\n\\n1\\n2\\n\\nfor positive integers N which are multiples of a certain integer depending on f (see the\\nstatement of Theorem 3.7 for details), and we establish the following integral formula for\\nthe coe\\xef\\xac\\x83cient Bf :\\n\\n1\\n\\nBf =\\n\\nlog(f (x) + x\\n\\n1) dx.\\n\\n\\xe2\\x88\\x92\\n\\n0\\nZ\\nOur next stop is a special subclass\\n\\nof the function class\\n\\nas\\nthe \\xe2\\x80\\x9ccombinatorial\\xe2\\x80\\x9d class, since it contains, rather naturally, many familiar objects from\\nis a countable\\nalgebraic combinatorics such as Dyck paths and Motzkin paths. The class\\nPk (see Section 4 for the precise de\\xef\\xac\\x81nition).\\nunion of \\xef\\xac\\x81nite families of functions:\\nBijective combinatorics in\\nis noteworthy, and so we spend Subsection 4.1 discussing it.\\ne\\nThis subsection contains a bijective-combinatorics \\xef\\xac\\x82avor which seems less analytic than its\\n\\n. We might refer to\\n\\ne\\n=\\n\\n\\xe2\\x88\\x9ek=1\\n\\nS\\n\\nP\\n\\nP\\n\\nP\\n\\nP\\n\\nP\\n\\nP\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\nDate: April 4, 2022.\\n2020 Mathematics Subject Classi\\xef\\xac\\x81cation. 05A16 (Primary) 05A05, 05A19, 05A20, 60F05 (Secondary).\\nKey words and phrases. large-scale rook placement, Young diagram, partition, dilation, asymptotic for-\\nmula, integral formula, Dyck path, Motzkin path, Schr\\xc2\\xa8oder number, ground bump, waterfall, combinatorial\\ninequality, permutation, X-ray, cumulative X-ray, limit shape, random permutation.\\n\\n1\\n\\n\\x0cneighboring parts. For example, we provide bijective arguments resulting in Corollary 4.2\\nwhich states\\n\\n=\\n\\nPk|\\n\\n|\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n\\xe2\\x88\\x92\\n1\\n\\xe2\\x88\\x92\\n\\n.\\n\\n(cid:19)\\nThe functions in\\nPk are in one-to-one correspondence with combinatorial objects which we\\ncall \\xe2\\x80\\x9cwaterfalls\\xe2\\x80\\x9d (see Subsection 4.1 for details). Studying waterfalls yields the following\\ncurious combinatorial formula, given in Proposition 4.9:\\ne\\n\\n(cid:18)\\n\\ne\\n\\nwt(D) =\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n\\xe2\\x88\\x92\\n1\\n\\xe2\\x88\\x92\\n\\n,\\n(cid:19)\\n\\n(cid:18)\\n\\nDyck(k)\\n\\nXD\\n\\xe2\\x88\\x88\\n\\nwhere Dyck(k) denotes the family of Dyck paths of length 2k \\xe2\\x80\\x94 lattice paths from (0, k)\\nto (k, 0) with k unit steps to the right and k unit steps down which never go below the line\\nX + Y = k \\xe2\\x80\\x94 and the weight wt(D) is de\\xef\\xac\\x81ned as\\n\\nwt(D) :=\\n\\n#\\n\\nj\\n\\n{\\n\\n\\xe2\\x88\\x88\\n\\nZ\\n\\n|\\n\\ni + j > k and (i, j)\\n\\nD\\n\\n.\\n\\n}\\n\\n\\xe2\\x88\\x88\\n\\nk\\n\\n1\\n\\n\\xe2\\x88\\x92\\n\\ni=1\\nY\\n\\nHaving visited waterfalls, we proceed to Subsection 4.2. Functions in the combinatorial\\nclass allow for even more precise asymptotics for the number of rook placements. Our second\\nmain result, Theorem 4.10, provides the following asymptotic formula for f\\nPk as follows.\\nWe have\\ne\\nlog N + Df + Of (1/N ),\\n\\nlog (# RP(\\xce\\xbb(f, N ))) = N log N + Bf \\xc2\\xb7\\n\\nN +\\n\\n1\\n2\\n\\n\\xe2\\x88\\x88\\n\\nfor positive integers N\\nthe following integral formula for the coe\\xef\\xac\\x83cient Df :\\n\\n\\xe2\\x88\\x88\\n\\nkZ, where the coe\\xef\\xac\\x83cient Bf is the same as before, and we give\\n\\nDf :=\\n\\nlog(2\\xcf\\x80) +\\n\\n1\\n2\\n\\n1\\n\\n1\\n2\\n\\n0\\nZ\\n\\nxf \\xe2\\x80\\xb2(x)\\nx(f (x) + x\\n\\nf (x) + 1\\n1)\\n\\n\\xe2\\x88\\x92\\n\\ndx.\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x88\\n\\nNow that for each function f\\n\\nPk, there are two coe\\xef\\xac\\x83cients Bf and Df associated to\\nit, one might wonder about the possible ranges of these numbers. Proposition 4.16 states\\nthat\\ne\\n\\n1\\nk \\xe2\\x89\\xa4\\nBoth upper bound and lower bound are tight. Each of them is attained by exactly one\\nfunction in\\n\\nBf \\xe2\\x89\\xa4 \\xe2\\x88\\x92\\n\\nlog k\\n\\n1.\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nSimilarly, we have tight bounds for Df . Proposition 4.17 states that\\n\\nPk.\\ne\\n\\n1\\n2\\n\\nlog\\n\\n2\\xcf\\x80\\nk\\n\\nDf \\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\n1\\n2\\n\\nlog\\n\\n2k\\xcf\\x80\\nk\\n\\n.\\n\\n(cid:18)\\nThe upper bound is attained by exactly one function in\\nPk. The equality cases for the lower\\nbound is rather interesting: the lower bound is attained by a Catalan-numerous family of\\ne\\nfunctions inside\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n(cid:19)\\n\\nEach rook placement \\xe2\\x80\\x94 or, more generally, each permutation \\xe2\\x80\\x94 comes with a certain\\nsequence of non-negative integers called the X-ray. An object which appears in the \\xef\\xac\\x81eld of\\ndiscrete tomography (cf. e.g. [HK99]), the X-ray of permutation has been investigated from\\nalgebraic and combinatorial points of view (cf. e.g. [BF14, BMPS05]). It is related to other\\nobjects in combinatorics such as Skolem sets (cf. e.g. [Nor08]) and permutohedra (cf. e.g.\\n[Pos09]). For each permutation \\xcf\\x80\\nn permutation matrix,\\nthe cumulative X-ray of \\xcf\\x80 is the function \\xce\\xbe\\xcf\\x80 : [0, 2n]\\n\\nSn, which we consider as an n\\n\\n[0, n] given by\\n\\n\\xc3\\x97\\n\\n\\xe2\\x88\\x88\\n\\nPk.\\ne\\n\\n\\xce\\xbe\\xcf\\x80(t) :=\\n\\n\\xe2\\x86\\x92\\n\\xcf\\x80ij ,\\n\\n[n]\\nXi,j\\n\\xe2\\x88\\x88\\ni+j\\nt\\n\\xe2\\x89\\xa4\\n2\\n\\n\\x0c\\xe2\\x88\\x88\\n\\nSn is the function\\n\\n[0, 1] given by\\n\\xce\\xbe\\xcf\\x80(nt). Thus, the graph of the normalized cumulative X-ray is simply the graph\\n\\nand the normalized cumulative X-ray of \\xcf\\x80\\n\\xce\\xbe\\xcf\\x80(t) := 1\\nn \\xc2\\xb7\\nof the cumulative X-ray rescaled from the rectangle [0, 2n]\\nWe have arrived at our last stop, where we consider the X-ray of a random large rook\\ne\\nplacement. For each a partition \\xce\\xbb, we let N\\n\\xce\\xbb be the partition obtained from magnifying\\n\\xe2\\x8a\\x99\\n\\xce\\xbb by a factor of N (see Section 2 for the precise de\\xef\\xac\\x81nition). Conjecture 5.9 predicts that the\\n\\xce\\xbe\\xcf\\x80 exhibits a limit shape phenomenon: for a \\xef\\xac\\x81xed real \\xce\\xb5 > 0,\\nnormalized cumulative X-ray\\nif \\xcf\\x80 is a uniformly random rook placement of the shape N\\n\\n[0, n] to [0, 2]\\n\\n\\xce\\xbe\\xcf\\x80 : [0, 2]\\n\\n\\xce\\xbb, then\\n\\n[0, 1].\\n\\n\\xe2\\x86\\x92\\n\\n\\xc3\\x97\\n\\n\\xc3\\x97\\n\\ne\\n\\n\\xe2\\x8a\\x99\\n\\nm\\xce\\xbb(t)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nsup\\n[0,2]\\n\\n\\xce\\xbe\\xcf\\x80(t)\\n\\n\\xe2\\x88\\x92\\n\\n< \\xce\\xb5\\n\\n1,\\n\\n! \\xe2\\x86\\x92\\n\\nas N\\n\\xe2\\x86\\x92\\nEquation (29) in Subsection 5.2 provides a formula for this function.\\n\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)e\\n[0, 1] is a certain function depending on the shape \\xce\\xbb.\\n\\n \\nt\\n\\xe2\\x88\\x88\\n, where m\\xce\\xbb : [0, 2]\\n\\nOur third main result of this paper, Theorem 5.10, proves this conjecture in the special\\ncase when \\xce\\xbb = (cid:3) is a partition with one box. In other words, it says that the normalized\\ncumulative X-ray of a uniformly random permutation, as the size of the permutation grows,\\nexhibits a limit shape phenomenon in the above sense. We note that it is easy to compute\\nthe limit shape for the permutation case explicitly:\\n\\ne\\n\\nP\\n\\nm(cid:3)(t) :=\\n\\nt2\\n2\\n\\n(\\n\\nt2\\n2 + 2t\\n\\n1\\n\\nif 0 < t\\nif 1 < t\\n\\n1, and\\n2.\\n\\n\\xe2\\x89\\xa4\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\x92\\nWhile Theorem 5.10 proves Conjecture 5.9 for only one very special case, we hope that one\\nproof technique is applicable, perhaps with some more work, for other shapes \\xce\\xbb as well.\\n\\n\\xe2\\x88\\x92\\n\\nWe remark that since rook placements can be considered as permutations, our work in\\nthis paper is closely related to an active and exciting \\xef\\xac\\x81eld of research on large permutations\\nand \\xe2\\x80\\x9cpermutons\\xe2\\x80\\x9d (cf. e.g. [HKM+13, AM14, GGKK15, GHK+17, KKRW20]). For example,\\nour construction of the normalized cumulative X-ray is reminiscent of that of permutons. It\\nwould be interesting, in the author\\xe2\\x80\\x99s opinion, to see how tools from the permuton literature\\ncan be applied to better understand large rook placements.\\n\\nOutline. In Section 2, we give some de\\xef\\xac\\x81nitions and present some elementary facts about\\nof \\xe2\\x80\\x9cnice\\xe2\\x80\\x9d piecewise linear functions. It\\nrook placements. Section 3 focuses on the class\\ncontains Theorem 3.7, our \\xef\\xac\\x81rst main result. Section 4 focuses on the \\xe2\\x80\\x9ccombinatorial\\xe2\\x80\\x9d class\\n. We discuss some bijective combinatorics in Subsection 4.1. We establish Theorem 4.10,\\nP\\nour second main result, in Subsection 4.2. We give some properties of the coe\\xef\\xac\\x83cient Df in\\nSubsection 4.3. We prove inequalities on the coe\\xef\\xac\\x83cients Bf and Df in Subsection 4.4. In\\ne\\nSection 5, we discuss probabilities and X-rays. It contains Conjecture 5.9. We deduce this\\nconjecture in the special case of random permutations from Theorem 5.10, our third main\\nresult, in Subsection 5.3.\\n\\nP\\n\\nFor each positive integer n, let Sn denote the set of permutations of [n] :=\\n{\\nn matrix (\\xe2\\x80\\x9cpermutation matrix\\xe2\\x80\\x9d)\\n\\nWe think of a permutation \\xcf\\x80\\n\\nSn as an n\\n\\n1, 2, . . . , n\\n\\n.\\n\\n}\\n\\n2. Rook Placements\\n\\n\\xc3\\x97\\n\\xcf\\x8011\\n\\xcf\\x8012\\n\\xcf\\x8021\\n\\xcf\\x8022\\n...\\n...\\n\\xcf\\x80n1 \\xcf\\x80n2\\n\\n\\xe2\\x88\\x88\\n\\n\\xcf\\x80 = \\xef\\xa3\\xae\\n\\xef\\xa3\\xaf\\n\\xef\\xa3\\xaf\\n\\xef\\xa3\\xaf\\n\\xef\\xa3\\xb0\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n. . .\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n\\xcf\\x801n\\n\\xcf\\x802n\\n...\\n\\xcf\\x80nn\\n\\n\\xef\\xa3\\xb9\\n\\n,\\n\\n\\xef\\xa3\\xba\\n\\xef\\xa3\\xba\\n\\xef\\xa3\\xba\\n\\xef\\xa3\\xbb\\n\\nwhere each entry \\xcf\\x80ij is either 0 or 1, each row has exactly one 1, and each column has\\nexactly one 1. A partition is a \\xef\\xac\\x81nite sequence of weakly decreasing positive integers. A\\npartition \\xce\\xbb is said to have (exactly) n parts if the length of \\xce\\xbb, as a \\xef\\xac\\x81nite sequence, is n.\\n3\\n\\n\\x0cLet us denote by Par the set of all partitions. By convention, we also include the empty\\npartition [ ] in Par. Consider the set\\n\\nIn other words,\\na partition \\xce\\xbb\\n\\n{\\n\\n[\\xce\\xbb1, \\xce\\xbb2, . . . , \\xce\\xbbn]\\n\\nBn :=\\nBn is the set of partitions \\xce\\xbb with exactly n parts such that \\xce\\xbb1 = n. Given\\n\\n\\xce\\xbb2 \\xe2\\x89\\xa5 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xe2\\x89\\xa5\\n\\nn = \\xce\\xbb1 \\xe2\\x89\\xa5\\n\\n\\xce\\xbbn > 0\\n\\nPar\\n\\n\\xe2\\x88\\x88\\n\\n}\\n\\n|\\n\\n.\\n\\n\\xe2\\x88\\x88 Bn, a rook placement of the shape \\xce\\xbb is a permutation \\xcf\\x80\\n[n], if j > \\xce\\xbbn+1\\n\\ni, then \\xcf\\x80ij = 0.\\n\\nfor any i, j\\n\\n\\xe2\\x88\\x88\\n\\nSn such that\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x92\\n\\nWe use the notation RP(\\xce\\xbb) to denote the set of rook placements of the shape \\xce\\xbb. The\\ncardinality of RP(\\xce\\xbb) has a well-known and easy-to-prove formula: for any \\xce\\xbb\\n\\n\\xe2\\x88\\x88 Bn,\\n\\n(1)\\n\\nn\\n\\n# RP(\\xce\\xbb) =\\n\\n(\\xce\\xbbi \\xe2\\x88\\x92\\n\\n(n\\n\\n\\xe2\\x88\\x92\\n\\ni)).\\n\\ni=1\\nY\\nOne particular point to notice about the product formula above that is particularly beautiful,\\nin the author\\xe2\\x80\\x99s opinion, is that the formula holds even when there are no rook placements\\n\\xe2\\x88\\x88 Bn such that RP(\\xce\\xbb) = \\xe2\\x88\\x85, the right-hand\\nof the shape \\xce\\xbb. In other words, for partitions \\xce\\xbb\\nside of the formula becomes zero (not some negative integer).\\nIt is well-known that for \\xce\\xbb\\n\\xe2\\x88\\x88 Bn,\\nIt\\ni.\\nn + 1\\n[n], we have \\xce\\xbbi \\xe2\\x89\\xa5\\n\\xe2\\x88\\x92\\nis the\\n|Dn|\\n\\nRP(\\xce\\xbb) is not empty.\\n.\\n}\\nDn if and only if for all i\\nis the central binomial coe\\xef\\xac\\x83cient\\n\\nthe partition \\xce\\xbb belongs to\\nis also well-known that\\n|Bn|\\nnth-Catalan number Cn := 1\\nn+1\\n\\nDn :=\\n\\nWe de\\xef\\xac\\x81ne\\n\\nand that\\n\\n\\xe2\\x88\\x88 Bn |\\n\\n2\\n\\xe2\\x88\\x92\\n1\\n\\xe2\\x88\\x92\\n\\n2n\\nn\\n\\n2n\\nn\\n\\n\\xe2\\x88\\x88\\n\\n\\xce\\xbb\\n\\n{\\n\\n.\\n\\n(cid:0)\\n\\nPar(n) and let m be a positive integer. We de\\xef\\xac\\x81ne the partition m\\n\\nFor each non-negative integer n, let Par(n) denote the set of all partitions \\xce\\xbb such that\\nthe sum of all parts of \\xce\\xbb is n. Now we describe how we dilate partitions. Suppose that\\nPar(m2n) as\\n\\xce\\xbb\\nfollows. Imagine starting with the Young diagram of \\xce\\xbb, and then replacing each of the n\\nboxes of \\xce\\xbb with an m\\nm array of boxes. The resulting diagram is the Young diagram of\\nm\\n\\n\\xce\\xbb.\\n\\n\\xc3\\x97\\n\\n\\xe2\\x8a\\x99\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x88\\n\\n\\xce\\xbb\\n\\n(cid:1)\\n\\n(cid:0)\\n\\n(cid:1)\\n\\n\\xce\\xbb) is an immediate consequence (and also\\n\\n\\xe2\\x8a\\x99\\nThe following formula for the size of RP(m\\n\\na mild generalization) of Equation (1).\\n\\n\\xe2\\x8a\\x99\\n\\nProposition 2.1. Let m and n be positive integers. For any partition \\xce\\xbb = [\\xce\\xbb1, \\xce\\xbb2, . . . , \\xce\\xbbn]\\nBn, we have m\\n\\n\\xe2\\x88\\x88 Bmn and\\n\\n\\xe2\\x8a\\x99\\n\\n\\xce\\xbb\\n\\n\\xe2\\x88\\x88\\n\\n# RP(m\\n\\n\\xce\\xbb) = m!n\\n\\n\\xe2\\x8a\\x99\\n\\nn\\n\\n\\xc2\\xb7\\n\\ni=1 (cid:18)\\nY\\n\\n(n\\nm(\\xce\\xbbi \\xe2\\x88\\x92\\nm\\n\\n\\xe2\\x88\\x92\\n\\ni))\\n\\n.\\n(cid:19)\\n\\nHere, the binomial coe\\xef\\xac\\x83cient is de\\xef\\xac\\x81ned for a\\n\\nZ and b\\n\\nZ\\n\\n1 as\\n\\na\\nb\\n\\n:= a(a\\n\\n\\xe2\\x88\\x92\\n\\n1)\\n\\n(a\\n\\nb+1)\\n\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\nb!\\n\\n\\xe2\\x88\\x92\\n\\n.\\n\\nIt is easy to see that for \\xce\\xbb\\npositive integer m, the partition m\\n\\n\\xe2\\x88\\x88\\n\\xe2\\x88\\x88 Bn, the partition \\xce\\xbb belongs to\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x89\\xa5\\n\\n\\xce\\xbb belongs to\\n\\nDmn.\\n\\n(cid:1)\\n\\n(cid:0)\\nDn if and only if for any\\n\\n3. The class\\n\\nof piecewise linear functions\\n\\n3.1. The functions and their lofts. Consider the class\\nwith the following properties:\\n\\nP\\n\\nof functions f : [0, 1]\\n\\n[0, 1]\\n\\n\\xe2\\x86\\x92\\n\\nf is weakly decreasing,\\nf is piecewise linear with a \\xef\\xac\\x81nite number of non-di\\xef\\xac\\x80erentiable points,\\nall the non-di\\xef\\xac\\x80erentiable points of f are rational numbers in [0, 1],\\nthere exists \\xce\\xb5 > 0 such that for any 0\\nf (1) > 0, and\\nfor any a\\n\\nx < \\xce\\xb5, we have f (x) = 1,\\n\\n(0, 1), we have limx\\n\\na f (x) > 1\\n\\na.\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\x88\\n\\n\\xd6\\x81\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\n4\\n\\n\\xe2\\x8a\\x99\\n\\nP\\n\\n\\x0cExample 3.1. An example of a function in\\ngiven by\\n\\nP\\n\\nis the following function f : [0, 1]\\n\\n[0, 1]\\n\\n\\xe2\\x86\\x92\\n\\nf (x) := \\xef\\xa3\\xb1\\n\\xef\\xa3\\xb4\\xef\\xa3\\xb2\\n\\n1\\n1\\n\\xe2\\x88\\x9a2\\n1\\n\\xe2\\x88\\x9a2 \\xe2\\x88\\x92\\n\\nx\\n\\xe2\\x88\\x9a7\\n\\nif x < 1\\n2 ,\\nif x = 1\\n2 ,\\nif x > 1\\n2 .\\n\\n\\xef\\xa3\\xb4\\xef\\xa3\\xb3\\n\\nare satis\\xef\\xac\\x81ed.\\nIt is straightforward to check that all the conditions for functions to be in\\nNote that while we require the non-di\\xef\\xac\\x80erentiable points to be rational numbers in [0, 1], it\\nis \\xef\\xac\\x81ne for the values of the function at the non-di\\xef\\xac\\x80erentiable points to be irrational. In our\\nexample here, the value of the function at the non-di\\xef\\xac\\x80erentiable point 1/2 is 1/\\xe2\\x88\\x9a2, which\\nis irrational. Moreover, it is also \\xef\\xac\\x81ne for the slope of some piece of the function to be\\n1/\\xe2\\x88\\x9a7, which\\nirrational. In our example here, the slope of the function when x\\nis irrational.\\n\\n(1/2, 1] is\\n\\n\\xe2\\x88\\x92\\n\\nP\\n\\n\\xe2\\x88\\x88\\n\\nExample 3.2. Here we present a non-example. A function that does not belong to\\nfunction g : [0, 1]\\n\\n[0, 1] given by\\n\\nP\\n\\nis the\\n\\n\\xe2\\x86\\x92\\n\\ng(x) :=\\n\\n1\\n1\\n2\\n\\n(\\n\\n1\\n2 ,\\nif x\\n\\xe2\\x89\\xa4\\nif x > 1\\n2 .\\n\\n.\\n\\nP\\n\\nP\\n\\nNote that even though g(x) > 1\\nviolates the last condition for functions to belong to\\n\\nx for all x\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x88\\n\\n(0, 1], the limit limx\\n\\n(1/2) g(x) = 1/2. This\\n\\n\\xd6\\x81\\n\\nThe following proposition gives some basic properties of functions in\\n\\n. These properties\\n\\ncan be proved immediately from the de\\xef\\xac\\x81nition of\\n\\n, so we omit the proof.\\n\\nProposition 3.3. Let f\\n\\n. Then,\\n\\n(a) for every x\\n(b) for every a\\n(c) for every \\xce\\xb5\\n\\n[0, 1], we have 0 < f (x)\\n\\xe2\\x89\\xa4\\na f (x) > 1\\n(0, 1], we have f (a) > 1\\n\\xe2\\x88\\x92\\n(0, 1], there exists \\xce\\xb4 > 0 such that for every x\\n\\n1.\\na and limx\\n\\n\\xd6\\x80\\n\\na.\\n[\\xce\\xb5, 1], we have the\\n\\ninequality f (x) + x\\n\\n1 > \\xce\\xb4.\\n\\nEach function f\\n\\ncomes with a useful quantity we call the loft of f de\\xef\\xac\\x81ned as follows.\\n\\n\\xe2\\x88\\x88 P\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x88\\n\\xe2\\x88\\x88\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x88 P\\n\\nDe\\xef\\xac\\x81nition 3.4. For each function f\\n\\n, de\\xef\\xac\\x81ne the loft of f as\\n\\n\\xe2\\x88\\x88 P\\n\\nloft(f ) := sup\\n\\n\\xce\\xb5\\n\\n[0, 1] :\\n\\n[0, \\xce\\xb5], f (x) = 1, and\\n\\n(\\xce\\xb5, 1], f (x) > 1\\n\\nx + \\xce\\xb5\\n\\n.\\n\\n{\\n\\n\\xe2\\x88\\x88\\n\\nx\\n\\n\\xe2\\x88\\x80\\n\\n\\xe2\\x88\\x88\\n\\nx\\n\\n\\xe2\\x88\\x80\\n\\n\\xe2\\x88\\x88\\n\\nThe following proposition gives some basic properties of the loft of a function in\\n\\n\\xe2\\x88\\x92\\n\\n}\\n\\n.\\n\\nP\\n\\nP\\n\\n\\xe2\\x88\\x92\\n\\xe2\\x88\\x88\\n\\nProposition 3.5. Let f\\n\\n. Then,\\n\\n\\xe2\\x88\\x88 P\\n\\n(a) its loft is strictly positive: 0 < loft(f )\\n\\xe2\\x89\\xa4\\nx\\n(b) for every real number x such that 0\\n(c) for every real number x such that loft(f )\\n(d) for every x\\n\\n[0, 1], we have\\n\\n\\xe2\\x89\\xa4\\n\\n1.\\n\\n\\xe2\\x89\\xa4\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\x88\\n\\nloft(f ), we have f (x) = 1.\\n1, we have f (x) + x\\nx\\n\\n\\xe2\\x89\\xa4\\n\\n1\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa5\\n\\nloft(f ).\\n\\n\\xe2\\x89\\xa5\\nProof. Consider any function f\\n\\nx\\n\\nf (x) + x\\n\\n1\\n\\n\\xe2\\x88\\x92\\n\\n. Let\\n\\nmin\\n\\nx, loft(f )\\n}\\n\\xe2\\x89\\xa5\\ndenote the set from De\\xef\\xac\\x81nition 3.4:\\n\\n{\\n\\n.\\n\\n:=\\n\\n\\xce\\xb5\\n\\n{\\n\\n\\xe2\\x88\\x88\\n\\nX\\n\\n[0, 1] :\\n\\nx\\n\\n\\xe2\\x88\\x80\\n\\n\\xe2\\x88\\x88\\n\\nx\\n\\n\\xe2\\x88\\x80\\n\\n\\xe2\\x88\\x88\\n\\n(\\xce\\xb5, 1], f (x) > 1\\n\\nx + \\xce\\xb5\\n\\n.\\n\\n\\xe2\\x88\\x92\\n\\n}\\n\\n\\xe2\\x88\\x88 P\\n[0, \\xce\\xb5], f (x) = 1, and\\n\\nX\\n\\n(a) It su\\xef\\xac\\x83ces to show that\\n\\n(0, 1]\\nthat f (a) = 1. By Proposition 3.3(c), there exists b > 0 such that for every x\\na, b\\nhave f (x) + x\\n\\n, there exists some a > 0 such\\n[a, 1], we\\n\\n(0, 1]. We claim that c\\n\\nX \\xe2\\x88\\xa9\\n\\n\\xe2\\x88\\x88 P\\n\\n\\xe2\\x88\\x88\\n\\n= \\xe2\\x88\\x85. Since f\\n\\n\\xe2\\x88\\x92\\n\\n1 > b. Take c := min\\n{\\n[0, c], we have 1\\n\\xe2\\x89\\xa5\\na, then f (x) = 1 > 1\\n\\n} \\xe2\\x88\\x88\\nf (x)\\n\\n\\xe2\\x88\\x88\\n(c, 1]. If c < x\\n\\n\\xe2\\x89\\xa5\\n\\nf (c)\\n\\n.\\nf (a) = 1 and so f (x) = 1. Second,\\nc\\n\\nx+c. If x > a, then f (x)+x\\n\\n1 > b\\n\\n\\xe2\\x88\\x88 X\\n\\n\\xe2\\x89\\xa5\\n\\nFirst, for any x\\n\\nsuppose x\\nand thus f (x) > 1\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\x92\\n\\nx + c. This shows that c\\n5\\n\\n\\xe2\\x88\\x88 X\\n\\n\\xe2\\x88\\x92\\n.\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa5\\n\\n6\\n\\x0c(b) It su\\xef\\xac\\x83ces to show that f (loft(f )) = 1. If loft(f )\\n\\nthen for any positive integer n, there exists an \\xe2\\x88\\x88 X\\nloft(f ) > an, we have that\\n\\n\\xe2\\x88\\x88 X\\nwith loft(f )\\n\\n, we are done. If loft(f ) /\\n\\n,\\n\\xe2\\x88\\x88 X\\n1\\nn < an < loft(f ). Since\\n\\n\\xe2\\x88\\x92\\n\\nf (loft(f )) > 1\\n\\nloft(f ) + an > 1\\n\\n1\\nn\\n\\n.\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\nSince n is arbitrary, we have f (loft(f )) = 1.\\n(c) This is similar to part (b). If loft(f )\\n\\n, we have f (x) > 1\\n\\nX\\nf (loft(f )) + loft(f )\\n\\n\\xe2\\x88\\x92\\n1 = loft(f ).\\n\\nx + loft(f ). If loft(f )\\n\\n\\xe2\\x88\\x88 X\\n\\n\\xe2\\x88\\x88 X\\n\\n\\xe2\\x88\\x92\\n\\nand x > loft(f ), then by the de\\xef\\xac\\x81nition of\\n1 =\\n\\nand x = loft(f ), then f (x) + x\\n\\n\\xe2\\x88\\x92\\n\\nOn the other hand, if loft(f ) /\\n\\n\\xe2\\x88\\x88 X\\n1\\nn < an < loft(f ). For every real number x\\n\\n, then for any positive integer n, there exists an \\xe2\\x88\\x88 X\\nloft(f ), we then have x > an,\\n\\nwith loft(f )\\nand thus\\n\\n\\xe2\\x88\\x92\\n\\nSince n is arbitrary, we have f (x)\\n\\n1\\n\\nx + loft(f ).\\n\\n(d) This part follows from parts (b) and (c).\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa5\\n\\n\\xe2\\x88\\x92\\n\\nf (x) > 1\\n\\nx + an > 1\\n\\nx + loft(f )\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa5\\n\\n1\\nn\\n\\n.\\n\\n\\xe2\\x88\\x92\\n\\n(cid:3)\\n\\nProposition 3.5(d) is an analytically useful property of the loft of a function in\\n\\n. It\\n[0, 1] is far enough from 0, the point (x, f (x)) on the graph of\\n\\nP\\n\\nsays roughly that once x\\n\\xe2\\x88\\x88\\nthe function is far enough from the line X + Y = 1.\\n\\n\\xe2\\x88\\x88 P\\n\\n.\\n3.2. An asymptotic formula for the number of rook placements for functions in\\nP\\nSuppose that a function f\\nis given. Let \\xcf\\x811, \\xcf\\x812, . . . , \\xcf\\x81m be the non-di\\xef\\xac\\x80erentiable points\\nof f inside the open interval (0, 1), listed in increasing order. (Here m is a non-negative\\ninteger. We use the convention that m = 0 if and only if f is di\\xef\\xac\\x80erentiable on (0, 1), which\\nx < 1.) For convenience, we de\\xef\\xac\\x81ne \\xcf\\x810 := 0 and \\xcf\\x81m+1 := 1.\\nis when f (x) = 1 for all 0\\n\\xe2\\x89\\xa4\\n1, \\xcf\\x81i). De\\xef\\xac\\x81ne\\nNote that for each i\\n[m + 1], the function f is linear on the open interval (\\xcf\\x81i\\nR to be the unique linear extension of f\\nthe function fi : [\\xcf\\x81i\\n1, \\xcf\\x81i)\\n1, \\xcf\\x81i]. There exist a non-positive real number \\xc2\\xb5i and a real number \\xce\\xb2i such that\\nto [\\xcf\\x81i\\nfi(x) = \\xc2\\xb5ix + \\xce\\xb2i for x\\n\\n|(\\xcf\\x81i\\xe2\\x88\\x921,\\xcf\\x81i) from (\\xcf\\x81i\\n\\n\\xe2\\x88\\x88\\n1, \\xcf\\x81i]\\n\\n1, \\xcf\\x81i].\\n\\n[\\xcf\\x81i\\n\\n\\xe2\\x86\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nNote that since we de\\xef\\xac\\x81ne fi on the closed interval [\\xcf\\x81i\\n\\nhave di\\xef\\xac\\x80erent values at \\xcf\\x81i\\ninterior of the interval. Note also that f1(x)\\n\\n1, \\xcf\\x81i], the functions fi and f might\\n1 and \\xcf\\x81i. On the other hand, the two functions agree in the\\n1 (i.e., \\xc2\\xb51 = 0 and \\xce\\xb21 = 1).\\n\\nTake any positive integer N such that N \\xcf\\x81i \\xe2\\x88\\x88\\n\\n\\xe2\\x89\\xa1\\nPar to be the partition with exactly N parts whose ith part is given by\\n\\nZ for every i. We de\\xef\\xac\\x81ne the partition\\n\\n\\xce\\xbb(f, N )\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x92\\n\\n(\\xce\\xbb(f, N ))i :=\\n\\nN\\n\\n\\xe2\\x8c\\x88\\n\\nf (i/N )\\n\\xe2\\x8c\\x89\\n\\n.\\n\\n\\xc2\\xb7\\n\\nOur de\\xef\\xac\\x81nition of\\nwords, RP(\\xce\\xbb(f, N )) is always non-empty.\\n\\n\\xe2\\x88\\x88 DN ; in other\\nOur goal of this subsection is to compute an asymptotic formula for # RP(\\xce\\xbb(f, N )) of\\n\\nguarantees that, as one may readily verify, \\xce\\xbb(f, N )\\n\\nP\\n\\n\\xe2\\x88\\x88\\n\\nthe form\\n\\nlog (# RP(\\xce\\xbb(f, N ))) = Af \\xc2\\xb7\\nfor positive integers N such that N \\xcf\\x81i \\xe2\\x88\\x88\\nthe implicit constant depends only on the function f .\\nBy Equation (1), we can write\\n\\nN log N + Bf \\xc2\\xb7\\nZ for every i. Here, the notation Of means that\\n\\nlog N + Of (1),\\n\\nN + Cf \\xc2\\xb7\\n\\n(2)\\n\\nlog (# RP(\\xce\\xbb(f, N ))) =\\n\\nlog (N f (n/N ) + n\\n\\nN )\\n\\n+ R(f, N ),\\n\\n\\xef\\xa3\\xab\\n\\n\\xef\\xa3\\xad\\n\\nN\\nX0<n\\n\\xe2\\x89\\xa4\\n\\nwhere R(f, N ) is the discrepancy from rounding:\\n\\n(3)\\n\\nR(f, N ) :=\\n\\nN\\nX0<n\\n\\xe2\\x89\\xa4\\nProposition 3.6. We have R(f, N )\\n\\n\\xe2\\x8c\\x88\\n\\nlog\\n\\nN f (n/N )\\n+ n\\n\\xe2\\x8c\\x89\\nN f (n/N ) + n\\n\\n\\xe2\\x88\\x92\\n\\xe2\\x88\\x92\\n0 and R(f, N ) = Of (1).\\n\\n(cid:18)\\n\\nN\\nN\\n\\n\\xe2\\x89\\xa5\\n\\n6\\n\\n\\xe2\\x88\\x92\\n\\n\\xef\\xa3\\xb6\\n\\n\\xef\\xa3\\xb8\\n\\n.\\n\\n(cid:19)\\n\\n\\x0cProof. The \\xef\\xac\\x81rst item R(f, N )\\n0 is clear from Equation (3). We proceed to show that\\nR(f, N ) = Of (1). Notice that for 0 < n < \\xcf\\x811N , we have f (n/N ) = 1. Therefore, we can\\nwrite\\n\\n\\xe2\\x89\\xa5\\n\\n(4)\\n\\nR(f, N ) = log\\n\\n\\xe2\\x8c\\x88\\n\\nN f (\\xcf\\x811)\\n+ \\xcf\\x811N\\n\\xe2\\x8c\\x89\\nN f (\\xcf\\x811) + \\xcf\\x811N\\n\\nN\\nN\\n\\n\\xe2\\x88\\x92\\n\\xe2\\x88\\x92\\n\\n+\\n\\nlog\\n\\n\\xe2\\x8c\\x88\\n\\nN f (n/N )\\n+ n\\n\\xe2\\x8c\\x89\\nN f (n/N ) + n\\n\\nN\\nN\\n\\n\\xe2\\x88\\x92\\n\\xe2\\x88\\x92\\n\\n.\\n\\n(cid:18)\\n\\n(cid:19)\\n(0, 1], by Proposition 3.3(c), there exists a > 0 such that for every x\\nlog(x) < 1/x,\\n\\n1 > a. Using this with the inequality log(\\n\\xe2\\x8c\\x88\\n\\nX\\xcf\\x811N <n\\n\\xe2\\x89\\xa4\\n\\nx\\n\\xe2\\x8c\\x89\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n(cid:18)\\n\\nN\\n\\n)\\n\\n\\xe2\\x88\\x88\\n\\xe2\\x88\\x80\\n\\n(cid:19)\\n[\\xcf\\x811, 1] we\\nx > 0, we\\n\\nSince \\xcf\\x811 \\xe2\\x88\\x88\\nhave f (x) + x\\nobtain\\n\\n1\\n(f (n/N ) + (n/N )\\n\\nN\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n\\nR(f, N ) <\\n\\n+\\n\\n+\\n\\n1\\nN (f (\\xcf\\x811) + \\xcf\\x811 \\xe2\\x88\\x92\\n1\\nN (f (\\xcf\\x811) + \\xcf\\x811 \\xe2\\x88\\x92\\n1\\nf (\\xcf\\x811) + \\xcf\\x811 \\xe2\\x88\\x92\\n\\n1)\\n\\n1)\\n\\n1\\nN\\n\\n\\xe2\\x89\\xa4\\n\\n=\\n\\nX\\xcf\\x811N <n\\n\\xe2\\x89\\xa4\\n\\nN\\n\\nN\\n\\nX\\xcf\\x811N <n\\n\\xe2\\x89\\xa4\\n\\xcf\\x811\\n1\\n\\xe2\\x88\\x92\\na\\n\\n.\\n\\n+\\n\\n\\xc2\\xb7\\n1\\n\\n\\xc2\\xb7\\n\\nN\\n\\na\\n\\n1 \\xc2\\xb7\\nSince \\xcf\\x811 and a depend only on f (and not N ), the quantity above is Of (1).\\n\\n(cid:3)\\n\\nRecall that the function f is linear on each open interval (\\xcf\\x81i\\n\\nthere might be a \\xe2\\x80\\x9cjump.\\xe2\\x80\\x9d For instance, in Example 3.1 above, the three values limx\\nf (\\xcf\\x811), and limx\\njumps do not have a huge e\\xef\\xac\\x80ect on the summation in Equation (2):\\n\\n1, \\xcf\\x81i), while, at each \\xcf\\x81i,\\n\\xcf\\x811 f (x),\\n\\xcf\\x811 f (x) are all di\\xef\\xac\\x80erent. It is not hard to see, however, that these possible\\n\\n\\xd6\\x80\\n\\n\\xd6\\x81\\n\\n\\xe2\\x88\\x92\\n\\n(5)\\n\\n(6)\\n\\n(9)\\n\\n(10)\\n\\n(11)\\n\\n(12)\\n\\nlog (N f (n/N ) + n\\n\\nN )\\n\\n\\xe2\\x88\\x92\\n\\nN\\n\\nX0<n\\n\\xe2\\x89\\xa4\\nm+1\\n\\n=\\n\\n\\xef\\xa3\\xae\\nX\\xcf\\x81i\\xe2\\x88\\x921N <n\\n\\xe2\\x89\\xa4\\n\\xef\\xa3\\xb0\\n\\xef\\xa3\\xbb\\nCombining this with Equation (2) and Proposition 3.6, we obtain\\n\\ni=1\\nX\\n\\n\\xcf\\x81iN\\n\\n\\xef\\xa3\\xb9\\n\\nlog((\\xc2\\xb5i + 1)n + (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n1)N )\\n\\n+ Of (1).\\n\\nm+1\\n\\n(7)\\n\\nlog (# RP (\\xce\\xbb(f, N ))) =\\n\\n\\xef\\xa3\\xae\\nX\\xcf\\x81i\\xe2\\x88\\x921N <n\\n\\xe2\\x89\\xa4\\n\\xef\\xa3\\xb0\\nWe break the outer summation on the right-hand side above into when i = 1 and when\\ni\\n\\ni=1\\nX\\n\\n\\xcf\\x81iN\\n\\n\\xef\\xa3\\xbb\\n\\n\\xef\\xa3\\xb9\\n\\nlog((\\xc2\\xb5i + 1)n + (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n1)N )\\n\\n+ Of (1).\\n\\n\\xe2\\x89\\xa5\\n(8)\\n\\n2. When i = 1, we have, by Stirling\\xe2\\x80\\x99s formula,\\nlog((\\xc2\\xb5i + 1)n + (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n\\xcf\\x81iN\\n\\nX\\xcf\\x81i\\xe2\\x88\\x921N <n\\n\\xe2\\x89\\xa4\\nN log N + (\\xcf\\x811 log \\xcf\\x811 \\xe2\\x88\\x92\\n= \\xcf\\x811 \\xc2\\xb7\\n\\xc2\\xb7\\nm + 1, observe that the function\\n\\n\\xcf\\x811)\\n\\n1)N ) = log((\\xcf\\x811N )!)\\n\\nN +\\n\\nlog N + Of (1).\\n\\n1\\n2\\n\\nWhen 2\\n\\ni\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\nis well-de\\xef\\xac\\x81ned on the whole closed interval [\\xcf\\x81i\\nmation formula (cf. [MV07, Appendix B]) with this function, we write\\n\\n1N, \\xcf\\x81iN ]. Using the Euler-Maclaurin sum-\\n\\n\\xe2\\x88\\x92\\n\\nx\\n\\nlog((\\xc2\\xb5i + 1)x + (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n7\\xe2\\x86\\x92\\n\\n1)N )\\n\\nlog((\\xc2\\xb5i + 1)n+ (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n1)N ) =\\n\\nlog((\\xc2\\xb5i + 1)x+ (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n1)N )dx+ Of (1).\\n\\n\\xcf\\x81iN\\n\\n\\xcf\\x81i\\xe2\\x88\\x921N\\n\\nZ\\n\\nX\\xcf\\x81i\\xe2\\x88\\x921N <n\\n\\xe2\\x89\\xa4\\n\\n\\xcf\\x81iN\\n\\nBy the change of variables x\\n\\nN\\n\\nx\\xe2\\x80\\xb2, we have\\n\\n\\xc2\\xb7\\n\\n7\\xe2\\x86\\x92\\nlog((\\xc2\\xb5i + 1)x + (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n1)N )dx\\n\\n\\xcf\\x81iN\\n\\n\\xcf\\x81i\\xe2\\x88\\x921N\\n\\nZ\\n\\n= (\\xcf\\x81i \\xe2\\x88\\x92\\n\\n\\xcf\\x81i\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n\\n\\xc2\\xb7\\n\\nN log N +\\n\\nlog(f (x) + x\\n\\n1) dx\\n\\nN.\\n\\n\\xe2\\x88\\x92\\n\\n) \\xc2\\xb7\\n\\n\\xcf\\x81i\\n\\n(Z\\n\\n\\xcf\\x81i\\xe2\\x88\\x921\\n\\n7\\n\\n\\x0cThe following is the main theorem of this section.\\n\\nTheorem 3.7. Let f\\n\\n. Let \\xcf\\x810, \\xcf\\x811, . . . , \\xcf\\x81m+1 be as de\\xef\\xac\\x81ned above. We have\\n\\n\\xe2\\x88\\x88 P\\n\\nlog (# RP(\\xce\\xbb(f, N ))) = N log N + Bf \\xc2\\xb7\\n\\nlog N + Of (1),\\n\\n1\\n2\\nZ for every i, where\\n\\nN +\\n\\nfor positive integers N such that N \\xcf\\x81i \\xe2\\x88\\x88\\n\\nNote that the integral is improper at x = 0. We interpret it as\\n\\n1\\n\\n0\\nZ\\n\\n1\\n\\nBf =\\n\\nlog(f (x) + x\\n\\n1) dx.\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nlog(f (x) + x\\n\\n1) dx.\\n\\nlim\\n0\\n\\xce\\xb5\\n\\xd6\\x81\\n\\n\\xce\\xb5\\nZ\\n\\nProof of Theorem 3.7. This result is immediate from combining Equations (8)-(12) and ob-\\n(cid:3)\\n1) dx = lim\\xce\\xb5\\nserving that lim\\xce\\xb5\\n\\nlog(f (x) + x\\n\\n\\xcf\\x811.\\n\\n0\\n\\n0\\n\\n\\xcf\\x811\\n\\xce\\xb5\\n\\n\\xd6\\x81\\n\\nlog(x) dx = \\xcf\\x811 log \\xcf\\x811 \\xe2\\x88\\x92\\n\\n\\xcf\\x811\\n\\xce\\xb5\\n\\n\\xd6\\x81\\n\\n\\xe2\\x88\\x92\\n\\nR\\n\\n3.3. Properties of Bf . Let p\\nnorm of functions in\\nby (\\nfunctions f, g\\nfunctions.\\n\\n\\xe2\\x88\\x88 P\\n\\nP\\n\\nP\\n\\nk\\n\\nf\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa5\\n, we make the class\\n\\n, Lp). A technical remark is that in the construction of (\\ng\\n\\nfor which\\n\\nR\\n\\n1 be any positive real number. By considering the Lp-\\na metric space. Let us denote this metric space\\n, Lp), we identify any two\\n, Lp) is an equivalence class of\\n\\nkp = 0. An element in (\\nP\\n\\nP\\n\\nP\\n\\nNevertheless, it is easy to see that the map f\\n, Lp)\\nLet (R, Euclid) denote the set of real numbers equipped with the usual Euclidean metric.\\nWe have the following topological property of B, when considered as a function from (\\n, Lp)\\nto (R, Euclid).\\nProposition 3.8. The map B : (\\n\\nBf induces a well-de\\xef\\xac\\x81ned map\\nR.\\n\\n(R, Euclid) is discontinuous everywhere on\\n\\nB : (\\n\\n, Lp)\\n\\n7\\xe2\\x86\\x92\\n\\n\\xe2\\x86\\x92\\n\\nP\\n\\nP\\n\\n.\\n\\nP\\n\\n\\xe2\\x86\\x92\\n\\nbe an arbitrary function. For each positive integer n such that n\\xe2\\x88\\x92\\n\\nP\\n1 +\\n\\nProof. Let f\\n2\\xe2\\x88\\x92\\n\\nn2\\n\\n< loft(f ), de\\xef\\xac\\x81ne\\n\\n\\xe2\\x88\\x88 P\\n\\nObserve that gn converges to f in Lp, as n\\nhave\\n\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\n. However, by the triangle inequality, we\\n\\ngn(x) :=\\n\\nf (x)\\n1\\n\\n\\xe2\\x88\\x92\\n\\n(\\n\\nx + 2\\xe2\\x88\\x92\\n\\nn2\\n\\nif x\\n1\\n\\xe2\\x89\\xa4\\nif x > 1\\n\\n1\\nn ,\\n1\\nn .\\n\\n\\xe2\\x88\\x92\\n\\xe2\\x88\\x92\\n\\nBgn |\\n\\n|\\n\\n=\\n\\nlog(gn(x) + x\\n\\nI1| \\xe2\\x88\\x92 |\\n\\nI2| \\xe2\\x88\\x92 |\\n\\nI3|\\n\\n,\\n\\n\\xe2\\x89\\xa5 |\\n\\n1\\n\\n0\\nZ\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nI1 :=\\n\\n\\xe2\\x88\\x92\\n\\n1) dx\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\nlog(gn(x) + x\\n\\n1) dx,\\n\\n\\xe2\\x88\\x92\\n\\nI2 :=\\n\\nlog(f (x) + x\\n\\n1) dx,\\n\\n1\\n\\n1\\nZ\\n\\n\\xe2\\x88\\x92\\n\\n1\\nn\\n1\\n\\n0\\n\\nZ\\n\\n1\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nwhere\\n\\nand\\n\\nI3 :=\\n\\nlog(f (x) + x\\n\\n1) dx.\\n\\n1\\nZ\\nNote that\\nBf |\\n= (log 2)\\nof functions converging to f in Lp, but\\n\\nI2|\\n\\nI1|\\n\\nn,\\n\\n=\\n\\n|\\n\\n|\\n\\n\\xc2\\xb7\\n\\n|\\n\\n1\\nn\\n\\xe2\\x88\\x92\\n, and\\n\\nI3| \\xe2\\x86\\x92\\n|\\nBgn | \\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\n|\\n\\n0, as n\\n, as n\\n\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n.\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\n. Hence,\\n\\ngn}\\n\\n{\\n\\nis a sequence\\n(cid:3)\\n\\nThe following proposition is clear from the integral formula of Bf .\\n, we have Bf \\xe2\\x89\\xa4 \\xe2\\x88\\x92\\n\\n(a) For every function f\\n\\nis tight. The equality is attained if and only if f (x) = 1 for every x\\n\\nProposition 3.9.\\n\\n\\xe2\\x88\\x88 P\\n\\n(b) For any functions f, g\\n\\nsuch that\\n\\n[0, 1], f (x)\\n\\n1. The upper bound\\n[0, 1).\\ng(x), we have Bf \\xe2\\x89\\xa5\\n\\nBg.\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x89\\xa5\\n\\n\\xe2\\x88\\x88 P\\n\\nx\\n\\n\\xe2\\x88\\x80\\n\\n\\xe2\\x88\\x88\\n\\n8\\n\\n\\x0c4. The class\\n\\nP\\n\\nof piecewise linear functions\\nPk to be the class of functions f\\ne\\nf (i/k)\\n\\nZ,\\n\\nFor each positive integer k, de\\xef\\xac\\x81ne\\n\\ne\\nthe following additional properties:\\n[k], we have k\\n\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\n\\xe2\\x88\\x88\\n\\nfor each i\\nf is upper-semicontinuous, and\\nfor each i\\nslope.\\n\\n[k], the restriction f\\n\\n\\xe2\\x88\\x88\\n\\n\\xc2\\xb7\\n\\n\\xe2\\x88\\x88\\n\\n|((i\\n\\xe2\\x88\\x92\\n\\n1)/k,i/k) is linear with a non-positive integer\\n\\nwhich satisfy\\n\\n\\xe2\\x88\\x88 P\\n\\na\\n\\nPk. One important property about function f\\nWe let\\n:=\\n(0, 1], we have limx\\n\\xd6\\x80\\ne\\n\\na f (x) = f (a).\\n\\n\\xe2\\x88\\x9ek=1\\n\\nP\\n\\n\\xe2\\x88\\x88\\n\\nP\\n\\ne\\n\\n\\xe2\\x88\\x88\\n\\ne\\n\\nS\\n4.1. Bijective Combinatorics in\\nsize has a nice product formula.\\n\\nPk. It is easy to see that\\ne\\n\\nPk is a \\xef\\xac\\x81nite set. In fact, its\\ne\\n\\n2, the sizes of the following sets are equal:\\n\\nProposition 4.1. For each positive integer k\\n\\nis that for every\\n\\n3)-tuples (y1, y2, . . . , y2k\\n\\n3) of non-negative integers such that for\\n\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\nthe class\\nthe set\\neach m\\n\\nPk,\\nTk of (2k\\n[2k\\ne\\n\\xe2\\x88\\x92\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x92\\n3], we have\\n\\n\\xe2\\x89\\xa5\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x80\\xa2\\n\\nthe set\\nhalf-plane\\n\\ny1 + y2 +\\n\\n+ ym \\xe2\\x89\\xa4\\nGk of lattice paths from (0, 0) to (2k\\n.\\n2y\\n}\\n\\n(x, y) : x\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n\\xe2\\x89\\xa5\\n\\n{\\n\\nl\\n\\xe2\\x88\\x92\\n\\n,\\n\\nm\\n2\\nm\\n1, k\\n\\n\\xe2\\x88\\x92\\n\\nProof. We construct the following bijections.\\n\\nthat 1\\n\\nFirst,\\n\\nPk \\xe2\\x86\\x92 Tk. Given f\\n1, let\\nh\\n\\xe2\\x89\\xa4\\ne\\n\\n\\xe2\\x88\\x92\\n\\nk\\n\\n\\xe2\\x89\\xa4\\n\\nand for each \\xe2\\x84\\x93 such that 1\\n\\n\\xe2\\x88\\x88\\n\\nPk, we de\\xef\\xac\\x81ne (y1, y2, . . . , y2k\\ne\\n1 := k\\n\\nf (x)\\n\\nf\\n\\n\\xe2\\x88\\x92\\n\\nh + 1\\nk\\n\\n,\\n\\n(cid:19)(cid:19)\\n\\n\\xe2\\x88\\x92\\n\\n(cid:18)\\n\\ny2h\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x84\\x93\\n\\nk\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\x92\\n\\n\\xc2\\xb7\\n\\nh/k\\n\\nlim\\nx\\n(cid:18)\\n\\xd6\\x81\\n2, let\\n\\ny2\\xe2\\x84\\x93 := k\\n\\n\\xe2\\x84\\x93 + 1\\nk\\n\\nf\\n(cid:18)\\n\\nSecond,\\n\\n\\xe2\\x88\\x92\\nTk \\xe2\\x86\\x92 Gk. Send the tuple (y1, . . . , y2k\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n\\xc2\\xb7\\n\\n\\xe2\\x88\\x92\\n\\nx\\n\\n\\xd6\\x81\\n3)\\n\\nE2N y1EN y2EN y3\\n\\nEN y2k\\xe2\\x88\\x923EN k\\n\\nlim\\n(\\xe2\\x84\\x93+1)/k\\n\\nf (x)\\n\\n.\\n\\n(cid:19)\\n\\n\\xe2\\x88\\x88 Tk to the path\\ny2\\xe2\\x88\\x92\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\xe2\\x88\\x92\\ny1\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n1\\n\\ny2k\\xe2\\x88\\x923,\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\nwhere E denotes the step (1, 0) and N denotes the step (0, 1).\\n\\nCorollary 4.2. For every positive integer k, we have\\n\\n|\\nNote that this sequence appears as A006013 on the OEIS [OEI].\\n\\n(cid:19)\\n\\n(cid:18)\\n\\ne\\n\\n=\\n\\nPk|\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n\\xe2\\x88\\x92\\n1\\n\\xe2\\x88\\x92\\n\\n.\\n\\n1) which are contained in the\\n\\n3) as follows. For each h such\\n\\n(cid:3)\\n\\nProof of Corollary 4.2. With Proposition 4.1, it su\\xef\\xac\\x83ces to show that\\nis well-known: see, for example, the sequence\\nSection 3], or references in A006013 on the OEIS [OEI].\\n\\n. This\\n|Gk|\\nin the work of Gessel and Xin [GX06,\\n(cid:3)\\n\\nbn}\\n\\n{\\n\\n(cid:0)\\n\\n(cid:1)\\n\\n= 1\\nk\\n\\n3k\\nk\\n\\n2\\n\\xe2\\x88\\x92\\n1\\n\\xe2\\x88\\x92\\n\\nIn the following discussion, by a lattice path, we mean the image of an injective continuous\\nR2 (under the usual Euclidean topology for both spaces) that is also a \\xef\\xac\\x81nite\\n\\nfunction [0, 1]\\nunion of segments s1, . . . , sn such that both end points of each si are lattice points.\\n\\n\\xe2\\x86\\x92\\n\\nFunctions in\\n\\nvertical segments. Formally, suppose a function f\\nof f into the square [0, k]\\n\\nPk can be seen as lattice paths, by dilating their graphs and then adding\\nPk is given. We \\xef\\xac\\x81rst dilate the graph\\ne\\ne\\n\\n[0, k] by\\n\\n\\xc3\\x97\\n\\n\\xe2\\x88\\x88\\n\\n\\xce\\x93f :=\\n\\n(x, y)\\n\\nR\\n\\nR :\\n\\n\\xe2\\x88\\x88\\n\\n\\xc3\\x97\\n\\nn\\n\\n= f\\n\\ny\\nk\\n\\n9\\n\\nx\\nk\\n\\n(cid:16)\\n\\n(cid:17)o\\n\\n[0, k]\\n\\n[0, k].\\n\\n\\xe2\\x8a\\x86\\n\\n\\xc3\\x97\\n\\n\\x0cThen, we take the closure \\xce\\x93f of \\xce\\x93f with respect to the usual Euclidean topology on R2.\\nNote that the closure simply adds a \\xef\\xac\\x81nite number of points into the set \\xce\\x93f . Then, our path\\nPath(f ) is given by\\n\\nR2\\n\\n|\\n\\ny\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x89\\xa4\\n\\nPath(f ) :=\\n\\nFor any f\\nde\\xef\\xac\\x81ne\\n\\nthere exist y1, y2 such that y1 \\xe2\\x89\\xa4\\n\\n.\\n(x, y)\\nPk, the path Path(f ) is a lattice path with endpoints (0, k) and (k, 0). Let us\\n(cid:9)\\n(cid:8)\\n\\xe2\\x88\\x88\\nPk).\\nLk := Path(\\ne\\nThe map Path :\\ne\\nconsider the map Func :\\n\\nPk \\xe2\\x86\\x92 Lk is a bijection. To go back from lattice paths to functions,\\ne\\n\\ny2 and (x, y1), (x, y2)\\n\\nPk given by\\nLk \\xe2\\x86\\x92\\n(Func(\\xce\\xb3)) (x) := sup\\n\\xe2\\x88\\x88\\ne\\n\\xe2\\x88\\x88 Lk. This map simply shrinks the path back and then removes\\nLk as going from (0, k) to (k, 0), then they are exactly the lattice\\n\\ny : (kx, ky)\\n{\\n\\nfor any x\\nvertical segments. It is straightforward to see that Path and Func are inverses.\\n\\nIf we think of paths in\\n\\n[0, 1], for any \\xce\\xb3\\n\\n\\xce\\x93f\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x88\\n\\n\\xce\\xb3\\n\\n}\\n\\n,\\n\\npaths \\xce\\xb3 with the following properties:\\n\\nthe path \\xce\\xb3 starts at (0, k) and ends at (k, 0),\\neach step is either (1,\\n0 or (0,\\nthe path \\xce\\xb3 intersects with the diagonal X + Y = k exactly at its two endpoints.\\n\\n\\xe2\\x84\\x93) for \\xe2\\x84\\x93\\n\\n1),\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nZ\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x89\\xa5\\n\\nThis class\\n\\nLk of lattice paths contains many familiar paths in algebraic combinatorics\\nsuch as Dyck paths and Motzkin paths. It is also closely related to plane S-trees, paren-\\nthesizations, and dissections of a convex polygon. See Stanley\\xe2\\x80\\x99s text [Sta99, Chapter 6] for\\ndetails.\\n\\nFor the following discussion, a Dyck path from (0, k) to (k, 0) is a lattice path starting\\n1), and ending at (k, 0) that never crosses (but might\\nfrom (0, k), using steps (1, 0) and (0,\\ntouch) the line X + Y = k. We let Dyck(k) denote the set of Dyck paths from (0, k) to\\n(k, 0).\\n\\n\\xe2\\x88\\x92\\n\\nSince paths in\\n\\n(0, k) and (k, 0), the Dyck paths in\\n(k, 1). Note that Dyck paths in\\nfunctions in\\n.\\ninto\\n\\nLk can intersect with the line X + Y = k only at the two endpoints\\nLk are in bijection with the Dyck paths from (1, k) to\\nPk) to piecewise constant\\nLk \\xe2\\x86\\x92\\nPk. We have thus obtained one trivial embedding of a Catalan-numerous family\\ne\\ne\\n\\nLk correspond (under Func :\\n\\nProposition 4.3. For each positive integer k, the number of piecewise constant functions\\nin\\n\\nP\\n\\ne\\n\\nPk is exactly the (k\\ne\\n\\n\\xe2\\x88\\x92\\n\\n1)st Catalan number\\n1\\nk\\n\\n1 =\\n\\nCk\\n\\n\\xe2\\x88\\x92\\n\\n2k\\nk\\n\\n2\\n\\xe2\\x88\\x92\\n1\\n\\xe2\\x88\\x92\\n\\n.\\n(cid:19)\\n\\n(cid:18)\\n\\nSimilarly, we have a Motzkin-numerous class of functions in\\n\\nas follows.\\n\\nP\\n\\nProposition 4.4. For each positive integer k\\nclass of all functions f\\n\\n2, de\\xef\\xac\\x81ne the subset\\ne\\n\\xe2\\x89\\xa5\\nPk which satisfy the following conditions:\\ne\\n\\n1,\\n(0, 1) of f , we have\\n\\neach linear piece of f either is constant or has slope\\nfor each non-di\\xef\\xac\\x80erentiable point a\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x88\\n\\nMOk \\xe2\\x8a\\x86\\n\\nPk to be the\\ne\\n\\n\\xe2\\x88\\x88\\nf (x)\\n\\nk\\n\\nf (a)\\n\\nk\\n\\n\\xc2\\xb7\\n\\n\\xe2\\x89\\xa1\\n\\n\\xe2\\x89\\xa1\\n\\xc2\\xb7\\nf (1) is an even integer.\\n\\nlim\\na\\nx\\n\\xd6\\x81\\n\\nk\\n\\n(1\\n\\na)\\n\\n(mod 2),\\n\\n\\xc2\\xb7\\n\\n\\xe2\\x88\\x92\\n\\nthe number k\\n\\n\\xc2\\xb7\\n\\nThen, the size of\\nMotzkin numbers, we recommend Stanley\\xe2\\x80\\x99s text [Sta99, Exercises 6.37 and 6.38].)\\n\\nMOk is the (k\\n\\n2. (For more details about the\\n\\n2)nd Motzkin number Mk\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nIf we drop the last two conditions about parity, we obtain Schr\\xc2\\xa8oder numbers.\\n\\nProposition 4.5. For each positive integer k, de\\xef\\xac\\x81ne the subset\\nof all functions f\\nThen, the size of\\nSchr\\xc2\\xa8oder numbers, we recommend Stanley\\xe2\\x80\\x99s text [Sta99, Section 6.2 and Exercises 6.39].)\\n10\\n\\nPk such that each linear piece of f either is constant or has slope\\ne\\n\\nPk to be the class\\n1.\\n\\xe2\\x88\\x92\\ne\\n1. (For more details about the\\n\\n1)st Schr\\xc2\\xa8oder number rk\\n\\n\\xe2\\x88\\x88\\nSCk is the (k\\n\\nSCk \\xe2\\x8a\\x86\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\n\\xe2\\x80\\xa2\\n\\n\\x0cThere is another embedding of a Catalan-numerous family in\\n\\n. The following proposi-\\n\\ntion is observed and proved by Alex Postnikov.\\n\\nProposition 4.6. Let k be a positive integer. The number of continuous functions in\\nis exactly the kth Catalan number\\n\\nP\\n\\ne\\n\\nPk\\n\\ne\\n\\nCk =\\n\\n1\\nk + 1\\n\\n2k\\nk\\n\\n.\\n(cid:19)\\n\\n(cid:18)\\n\\nProof. We construct an explicit bijection from the set of continuous functions in\\nPk to\\nDyck(k), the set of Dyck paths from (0, k) to (k, 0). Note that for each continuous function\\ne\\nf\\n[0, k], is a continuous lattice path from (0, k)\\nPk, the graph of f , after dilating to [0, k]\\nk\\ni=1 \\xce\\xb3i,\\nto (k, 0) without a vertical step. We can write this path as the union of k segments\\ne\\nwhere\\n\\n\\xc3\\x97\\n\\n\\xe2\\x88\\x88\\n\\n\\xce\\xb3i :=\\n\\ni\\n(cid:20)(cid:18)\\n\\n\\xe2\\x88\\x92\\n\\n1, k\\n\\nf\\n\\n\\xc2\\xb7\\n\\ni\\n\\n1\\n\\n\\xe2\\x88\\x92\\nk\\n\\n,\\n\\ni, k\\n\\nf\\n\\n\\xc2\\xb7\\n\\ni\\nk\\n\\n(cid:18)\\nReplace each segment \\xce\\xb3i with an L-shaped broken segment with the same endpoints:\\ni\\nk\\n\\n(cid:19)(cid:19)(cid:21)\\n\\n1, k\\n\\n1, k\\n\\n(cid:19)(cid:19)\\n\\ni, k\\n\\n\\xe2\\x88\\x92\\nk\\n\\ni\\nk\\n\\ni\\nk\\n\\n(cid:18)\\n\\n(cid:18)\\n\\n1\\n\\nf\\n\\nf\\n\\nf\\n\\nf\\n\\ni\\n\\n,\\n\\n,\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\xaa\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n(cid:19)(cid:19)\\n\\n(cid:18)\\nk\\ni=1 \\xce\\xb3\\xe2\\x80\\xb2i is the desired Dyck path in Dyck(k).\\n\\n(cid:19)(cid:19)(cid:21)\\n\\n(cid:18)\\n\\n(cid:18)\\n\\n(cid:19)(cid:19)\\n\\n(cid:18)\\n\\n1, k\\n\\n\\xce\\xb3\\xe2\\x80\\xb2i :=\\n\\ni\\n(cid:20)(cid:18)\\nThen, the union\\n\\n\\xe2\\x88\\x92\\n\\ni\\n(cid:20)(cid:18)\\n\\ni\\n(cid:18)\\n\\n.\\n\\n(cid:18)\\n\\n(cid:19)(cid:19)(cid:21)\\n(cid:3)\\n\\nS\\n\\nRemark 4.7. From Proposition 4.6, we quickly obtain yet another Catalan-numerous fam-\\n1)st\\nily. The number of continuous functions f\\nCatalan number Ck\\n1. This is because under the bijection in the proof of Proposition 4.6,\\nthese functions become Dyck paths in Dyck(k) that visit (k\\n\\nPk for which f (1) = 1/k is the (k\\ne\\n\\n1, 1).\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x92\\n\\nS\\n\\n\\xe2\\x88\\x92\\n\\nThe bijection in the proof of Proposition 4.6 that sends continuous functions to Dyck\\npaths might be extended to the whole\\nPk. The image of the extended map can be understood\\nas Dyck paths with certain marks on vertical segments. These are combinatorial objects\\nwhich we call waterfalls.\\ne\\n\\nDe\\xef\\xac\\x81nition 4.8. A waterfall of size k is a Dyck path \\xce\\xb3\\nDyck(k) together with a choice of\\ncoloring of every unit segment in \\xce\\xb3 so that each segment is colored one of either green or\\nblue with the following rules:\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\xe2\\x80\\xa2\\n\\nevery horizontal segment is colored blue,\\nevery vertical segment on the line x = k is colored green,\\nevery vertical segment with an endpoint on the line x + y = k is colored green, and\\nif s1 and s2 are vertical segments such that s1 is immediately above s2 and s2 is\\ncolored blue, then s1 must also be colored blue.\\n\\nLet WT(k) denote the set of waterfalls of size k. From our discussion above, we have\\n\\nWe obtain the following curious combinatorial formula.\\n\\nWT(k)\\n|\\n\\n|\\n\\n=\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n\\xe2\\x88\\x92\\n1\\n\\xe2\\x88\\x92\\n\\n.\\n(cid:19)\\n\\n(cid:18)\\n\\nProposition 4.9. Let k be a positive integer. For each Dyck path D\\nde\\xef\\xac\\x81ne the weight of D to be\\n\\n\\xe2\\x88\\x88\\n\\nDyck(k), let us\\n\\nThen,\\n\\nwt(D) :=\\n\\n#\\n\\nj\\n\\n{\\n\\n\\xe2\\x88\\x88\\n\\nZ\\n\\n|\\n\\ni + j > k and (i, j)\\n\\nD\\n\\n.\\n\\n}\\n\\n\\xe2\\x88\\x88\\n\\nk\\n\\n1\\n\\n\\xe2\\x88\\x92\\n\\ni=1\\nY\\n\\nwt(D) =\\n\\n1\\nk\\n\\n3k\\nk\\n\\n2\\n\\xe2\\x88\\x92\\n1\\n\\xe2\\x88\\x92\\n\\n.\\n(cid:19)\\n\\n(cid:18)\\n\\nDyck(k)\\n\\nXD\\n\\xe2\\x88\\x88\\n\\nProof. Note that the weight wt(D) is the number of waterfalls whose underlying Dyck paths\\nDyck(k) wt(D) counts the total number of waterfalls in WT(k). (cid:3)\\nare D. Therefore,\\n\\nD\\n\\n\\xe2\\x88\\x88\\n\\nP\\n\\n11\\n\\n\\x0c(14)\\n\\n(15)\\n\\n(16)\\n\\n(17)\\n\\n4.2. A precise asymptotic formula for the number of rook placements for func-\\ntions in\\nPk. The goal of this subsection is to compute a precise asymptotic formula for\\n# RP(\\xce\\xbb(f, N )), for each f\\ne\\n\\nlog (# RP(\\xce\\xbb(f, N ))) = Af \\xc2\\xb7\\n{\\n\\nfor positive integers N\\nAf , Bf , and Cf are the same as before.\\n\\nN log N + Bf \\xc2\\xb7\\nk, 2k, 3k, . . .\\n}\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x88\\n\\nPk, of the form\\ne\\nkZ :=\\n\\nN + Cf \\xc2\\xb7\\n\\n. Since\\n\\nlog N + Df + Of (1/N ),\\n\\n, the quantities\\n\\nIn this subsection, we rede\\xef\\xac\\x81ne our notations \\xc2\\xb5i and \\xce\\xb2i. These notations now have slightly\\n[k],\\nPk and for any i\\n\\xe2\\x88\\x88\\n1)/k, i/k]. Let \\xc2\\xb5i and \\xce\\xb2i\\ne\\n\\ndi\\xef\\xac\\x80erent meanings from what they meant in Subsection 3.2. For f\\nwe know that f is a linear function on the half-open interval ((i\\nbe such that for x\\nBecause f\\n\\nFurthermore, the e\\xef\\xac\\x80ect from jumps (as in Equation (6)) is also zero. Therefore, for f\\nand for any positive integer N\\n\\nPk, the discrepancy from rounding, R(f, N ) (as in Proposition 3.6), is zero.\\nPk\\ne\\n\\n1)/k, i/k], we have f (x) = \\xc2\\xb5ix + \\xce\\xb2i.\\n\\nkZ, we have\\n\\n((i\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x88\\n\\nPk is a subclass of\\ne\\n\\nP\\n\\n\\xe2\\x88\\x88\\n\\nk\\n\\ne\\n\\n(13)\\n\\nlog (# RP(\\xce\\xbb(f, N ))) =\\n\\nlog((\\xc2\\xb5i + 1)n + (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n1)N ).\\n\\ni\\xe2\\x88\\x921\\nk N <n\\nX\\nOnce again, we break the outer summation on the right-hand side above into when i = 1\\nand when i\\n\\n2. When i = 1, we have, by Stirling\\xe2\\x80\\x99s formula,\\n\\ni=1\\nX\\n\\ni\\nk N\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa5\\n\\nN\\nk\\n\\nX0<n\\n\\xe2\\x89\\xa4\\n1\\nk \\xc2\\xb7\\n\\n=\\n\\nlog((\\xc2\\xb5i + 1)n + (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n1)N ) =\\n\\nlog(n) = log((N/k)!)\\n\\nX0<n\\n\\xe2\\x89\\xa4\\n\\nN\\nk\\n\\nN log N\\n\\nN +\\n\\nlog N +\\n\\nlog\\n\\n+ O(k/N ),\\n\\n(log k) + 1\\nk\\n\\n\\xc2\\xb7\\n\\n1\\n2\\n\\n1\\n2\\n\\n2\\xcf\\x80\\nk\\n\\n(cid:18)\\n\\n(cid:19)\\n\\nWhen 2\\n\\ni\\n\\nk, we use the Euler-Maclaurin summation formula (cf.\\n\\n[MV07, Appen-\\n\\nfor positive integers N\\n\\n\\xe2\\x89\\xa4\\ndix B]) to obtain\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\x92\\nkZ.\\n\\n\\xe2\\x88\\x88\\n\\n=\\n\\nN log N +\\n\\nlog(f (x) + x\\n\\n1) dx\\n\\nN\\n\\nlog((\\xc2\\xb5i + 1)n + (\\xce\\xb2i \\xe2\\x88\\x92\\n\\n1)N )\\n\\ni\\xe2\\x88\\x921\\nk N <n\\nX\\n\\n\\xe2\\x89\\xa4\\n\\ni\\nk N\\n\\n1\\nk \\xc2\\xb7\\n\\ni/k\\n\\n1)/k\\n\\ni\\n\\n(Z\\n(i\\n\\xe2\\x88\\x92\\n(\\xc2\\xb5i + 1)\\n(\\xc2\\xb5i + 1)\\n\\n\\xe2\\x88\\x92\\n\\n) \\xc2\\xb7\\n\\n+ Of (1/N ).\\n\\n(18)\\n\\n1)\\nk + (\\xce\\xb2i \\xe2\\x88\\x92\\n1) !\\nk + (\\xce\\xb2i \\xe2\\x88\\x92\\nCombining these terms, we obtain the following theorem.\\n\\n1\\n2 \\xc2\\xb7\\n\\n\\xc2\\xb7\\ni\\n\\xe2\\x88\\x92\\n\\nlog\\n\\n \\n\\n+\\n\\n1\\n\\n\\xc2\\xb7\\n\\nTheorem 4.10. Let k be a positive integer. Let f\\n\\nlog (# RP(\\xce\\xbb(f, N ))) = N log N + Bf \\xc2\\xb7\\n\\nfor positive integers N\\n\\nkZ, where Bf is as given in Theorem 3.7, and\\n\\n\\xe2\\x88\\x88\\nN +\\n\\nPk. We have\\n1\\ne\\n2\\n\\nlog N + Df + Of (1/N ),\\n\\n\\xe2\\x88\\x88\\n\\nDf :=\\n\\nlog(2\\xcf\\x80) +\\n\\n1\\n2\\n\\n1\\n\\n1\\n2\\n\\n0\\nZ\\n\\nxf \\xe2\\x80\\xb2(x)\\nx(f (x) + x\\n\\nf (x) + 1\\n1)\\n\\n\\xe2\\x88\\x92\\n\\ndx.\\n\\n\\xe2\\x88\\x92\\n\\nNote that one has to be careful about the integral in the formula of Df . Since in general\\nf has a number of non-di\\xef\\xac\\x80erentiable points, the derivative f \\xe2\\x80\\xb2 might be unde\\xef\\xac\\x81ned for some\\nvalues of x. By the integral as expressed, we mean\\n\\n1\\n\\n0\\nZ\\n\\nxf \\xe2\\x80\\xb2(x)\\nx(f (x) + x\\n\\nf (x) + 1\\n1)\\n\\n\\xe2\\x88\\x92\\n\\ndx =\\n\\n\\xe2\\x88\\x92\\n\\nk\\n\\ni/k\\n\\ni=1 Z\\nX\\n\\n(i\\n\\n1)/k\\n\\n\\xe2\\x88\\x92\\n\\nxf \\xe2\\x80\\xb2(x)\\nx(f (x) + x\\n\\nf (x) + 1\\n1)\\n\\n\\xe2\\x88\\x92\\n\\ndx.\\n\\n\\xe2\\x88\\x92\\n\\nSince f is linear in ((i\\n\\n\\xe2\\x88\\x92\\n\\n1)/k, i/k), the sum of integrals on the right-hand side is well-de\\xef\\xac\\x81ned.\\nIn the following examples, we\\n\\nAn illustration of Theorem 4.10 is given in Figure 1.\\ncompute explicit asymptotic formulas for certain functions.\\n\\n12\\n\\n\\x0cExample 4.11. Suppose that k is a positive integer. Let f : [0, 1]\\n\\n[0, 1] be given as\\n\\n\\xe2\\x86\\x92\\n\\n\\xe2\\x86\\x92\\n\\n(cid:0)\\n\\nN\\n\\n(cid:17)\\n\\nfor all x\\n\\xe2\\x88\\x88\\nTherefore,\\n\\n[0, 1]. Note that f\\n\\n1\\n\\nk and Df = 1\\n\\n2 log(2\\xcf\\x80/k).\\n\\n1\\nk \\xe2\\x88\\x92\\n\\nf (x) := min\\n\\n1 +\\n\\nx, 1\\n\\n,\\n\\n(cid:26)\\nPk. We have Bf =\\ne\\n\\n\\xe2\\x88\\x88\\n\\n(cid:27)\\n\\nlog k\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n2\\xcf\\x80\\nk \\xc2\\xb7\\n\\n\\xe2\\x88\\xbc r\\n\\nN N + 1\\n\\n2\\n\\n(k\\xe2\\x88\\x92\\n\\n1e\\xe2\\x88\\x92\\n\\n1/k)N ,\\n\\n\\xc2\\xb7\\n\\n# RP(\\xce\\xbb(f, N ))\\n\\nas N\\n\\n, N\\n\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\n\\xe2\\x88\\x88\\n\\nkZ.\\n\\nExample 4.12. Suppose that k is a positive integer. Let f : [0, 1]\\n\\n[0, 1] be given as\\n\\nf (x) := min\\n\\n1 +\\n\\n(cid:26)\\n\\n2\\n\\n\\xe2\\x88\\x92 \\xe2\\x8c\\x88\\nk\\n\\nkx\\n\\xe2\\x8c\\x89\\n\\n, 1\\n\\n,\\n\\n(cid:27)\\n\\n\\xe2\\x88\\x92\\n\\nfor all x\\n1\\nDk =\\n\\n[0, 1]. Note that f\\n2 log 2 + 1\\n\\n2 log k + k\\n\\n\\xe2\\x88\\x88\\n2 log \\xcf\\x80. Therefore,\\n\\n\\xe2\\x88\\x88\\n\\nPk. We have Bf =\\ne\\n\\n\\xe2\\x88\\x92\\n\\nlog k +\\n\\n2\\n\\nlog 2\\n\\n1 and\\n\\n\\xe2\\x88\\x92\\n\\n2\\nk\\n\\n\\xe2\\x88\\x92\\n\\n(cid:1)\\n\\n# RP(\\xce\\xbb(f, N ))\\n\\nN N + 1\\n\\n2\\n\\n22\\n\\n\\xe2\\x88\\x92\\n\\n2/k k\\xe2\\x88\\x92\\n\\n1 e\\xe2\\x88\\x92\\n\\n1\\n\\n,\\n\\n2k\\xcf\\x80\\nk \\xc2\\xb7\\n\\n\\xe2\\x88\\xbc r\\n\\n\\xc2\\xb7\\n\\n(cid:16)\\n\\nas N\\n\\n, N\\n\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\n\\xe2\\x88\\x88\\n\\nkZ.\\n\\nAs another application of our result, we can detect the number of ground bumps of Dyck\\nDyck(k) is a Dyck path from (0, k) to (k, 0), then a ground bump\\npaths analytically. If D\\nof \\xce\\xb3 is an intersection between \\xce\\xb3 and the open line segment from (0, k) to (k, 0). Recall\\nthat a partition \\xce\\xbb\\nIn the\\nfollowing proposition, a ground bump of \\xce\\xbb is de\\xef\\xac\\x81ned as a ground bump of the Dyck path\\ncorresponding to \\xce\\xbb.\\n\\n\\xe2\\x88\\x88 Dk can be thought of as a Dyck path from (0, k) to (k, 0).\\n\\n\\xe2\\x88\\x88\\n\\nProposition 4.13. Let \\xce\\xbb be any nonempty partition such that RP(\\xce\\xbb)\\nexist positive real numbers \\xce\\xb11, \\xce\\xb12, \\xce\\xb13, \\xce\\xb14 > 0 such that\\n\\n= \\xe2\\x88\\x85. Then, there\\n\\n\\xe2\\x8a\\x99\\n. Furthermore, \\xce\\xb11 = \\xce\\xbb1 and\\n\\n\\xe2\\x88\\xbc\\n\\n# RP(N\\n\\n\\xce\\xbb)\\n\\nN \\xce\\xb11N +\\xce\\xb12\\n\\n\\xce\\xb1N\\n\\n3 \\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xce\\xb14,\\n\\nas N\\n\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\n\\xce\\xb12 =\\n\\n(#ground bumps of \\xce\\xbb) + 1\\n2\\n\\n.\\n\\nProof. Since RP(\\xce\\xbb)\\nas a concatenation \\xce\\xbb = \\xce\\xbb(1)\\nground bumps. Observe that\\n\\n\\xe2\\x88\\x97\\n\\n= \\xe2\\x88\\x85, we have that \\xce\\xbb\\n\\n\\xce\\xbb(2)\\n\\n\\xe2\\x88\\x97 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xe2\\x88\\x97\\n\\n\\xe2\\x88\\x88 D\\xce\\xbb1 . As a Dyck path, \\xce\\xbb can be uniquely written\\n\\xce\\xbb(p) such that each \\xce\\xbb(i) is a Dyck path without\\n\\n(19)\\n\\n(20)\\n\\nand that\\n\\n# RP(N\\n\\n\\xce\\xbb) =\\n\\n# RP(N\\n\\n\\xe2\\x8a\\x99\\n\\n\\xce\\xbb(i)),\\n\\n\\xe2\\x8a\\x99\\n\\np\\n\\ni=1\\nY\\n\\np\\n\\ni=1\\nX\\n\\n\\xce\\xbb1 =\\n\\n\\xce\\xbb(i)\\n1 ,\\n\\nwhere \\xce\\xbb(i)\\n\\n1 denotes the \\xef\\xac\\x81rst part of the partition \\xce\\xbb(i).\\n\\nFor each i\\n\\n[p], there is a unique corresponding function f (i)\\n\\n. We have that for\\n\\nany positive integer N ,\\n\\n\\xe2\\x88\\x88\\n\\nP\\xce\\xbb(i)\\n\\n1\\n\\n\\xe2\\x88\\x88\\n\\ne\\n\\n# RP(N\\n\\n\\xe2\\x8a\\x99\\n\\n\\xce\\xbb(i)) = # RP(\\xce\\xbb(f (i), N \\xce\\xbb(i)\\n\\n1 )).\\n\\n13\\n\\n6\\n6\\n\\x0c\\xe2\\x88\\x9a2\\xcf\\x80\\n\\nN N +1/2\\n\\n(e\\xe2\\x88\\x92\\n\\n1)N\\n\\n\\xe2\\x88\\xbc\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n4\\n3 \\xcf\\x80\\n\\n2\\n3 \\xcf\\x80\\n\\n8\\n3 \\xcf\\x80\\n\\n4\\n3 \\xcf\\x80\\n\\n4\\n3 \\xcf\\x80\\n\\n2\\n3 \\xcf\\x80\\n\\n\\xe2\\x88\\xbc\\n\\nq\\n\\n\\xe2\\x88\\xbc\\n\\nq\\n\\n\\xe2\\x88\\xbc\\n\\nq\\n\\n\\xe2\\x88\\xbc\\n\\nq\\n\\n\\xe2\\x88\\xbc\\n\\nq\\n\\n\\xe2\\x88\\xbc\\n\\nq\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\nN\\n\\nN\\n\\nN\\n\\n(cid:1)\\n\\n(cid:1)\\n\\n(cid:1)\\n\\nN\\n\\nN\\n\\n(cid:1)\\n\\n(cid:1)\\n\\nN N +1/2\\n\\n21 3\\xe2\\x88\\x92\\n\\n1 e\\xe2\\x88\\x92\\n\\n2/3\\n\\nN N +1/2\\n\\n24/3 3\\xe2\\x88\\x92\\n\\n1 e\\xe2\\x88\\x92\\n\\n1\\n\\nN N +1/2\\n\\n24/3 3\\xe2\\x88\\x92\\n\\n1 e\\xe2\\x88\\x92\\n\\n1\\n\\nN N +1/2\\n\\n22/3 3\\xe2\\x88\\x92\\n\\n1 e\\xe2\\x88\\x92\\n\\n2/3\\n\\nN N +1/2\\n\\n22/3 3\\xe2\\x88\\x92\\n\\n1 e\\xe2\\x88\\x92\\n\\n2/3\\n\\nN N +1/2\\n\\n3\\xe2\\x88\\x92\\n\\n1 e\\xe2\\x88\\x92\\n\\n1/3\\n\\nN\\n\\n(cid:1)\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n\\xc2\\xb7\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(cid:0)\\n\\n(22)\\n\\n(24)\\n\\nFigure 1. The seven functions in the class\\nasymptotic formulas for # RP(\\xce\\xbb(f, N )), as N\\n\\nP3 and their corresponding\\n3Z.\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\ne\\n\\n, N\\n\\n\\xe2\\x88\\x88\\n\\n(Note that the notation \\xce\\xbb on the right-hand side of the equation above is an operator, not\\na partition.) Now, Theorem 4.10 gives\\n\\n(21)\\n\\nlog(# RP(N\\n\\n\\xe2\\x8a\\x99\\n\\n1 N log N + (\\xce\\xbb(i)\\n\\xce\\xbbi)) = \\xce\\xbb(i)\\n1\\n2\\n\\nlog \\xce\\xbb(i)\\n\\n+\\n\\n1 + Df (i) + O\\xce\\xbb(1/N ).\\n\\n1 log \\xce\\xbb(i)\\n\\n1 )N + Bf (i) \\xce\\xbb(i)\\n\\n1 N +\\n\\nlog N\\n\\n1\\n2\\n\\nCombining Equations (19), (20), and (21), we obtain\\n\\n(23)\\n\\nlog(# RP(N\\n\\n\\xce\\xbb)) = \\xce\\xbb1N log N +\\n\\n\\xe2\\x8a\\x99\\n\\np\\n\\ni=1 (cid:16)\\nX\\np\\n1\\n2\\n\\ni=1 (cid:18)\\nX\\n\\n\\xce\\xbb(i)\\n1 log \\xce\\xbb(i)\\n\\n1 + Bf (i) \\xce\\xbb(i)\\n\\ni\\n\\nN\\n\\n\\xc2\\xb7\\n\\n(cid:17)\\n\\nlog \\xce\\xbb(i)\\n\\n1 + Df (i)\\n\\n+ O\\xce\\xbb(1/N ),\\n\\n(cid:19)\\n\\n+\\n\\nlog N +\\n\\np\\n2\\n\\nfor positive integers N . Since p\\nthe proof.\\n\\n1 is the number of ground bumps of \\xce\\xbb, we have \\xef\\xac\\x81nished\\n(cid:3)\\n\\n\\xe2\\x88\\x92\\n\\n4.3. Properties of Df . Theorem 4.10 gives an integral formula for Df . In applications,\\nit is also useful to have the following formula, which is immediate from Equations (15) and\\n(17).\\n\\nProposition 4.14. For any function f\\n1\\n2\\n\\nlog(2\\xcf\\x80f (1)) +\\n\\nDf =\\n\\n1\\n2\\n\\n, we have\\n\\nlog\\n\\nlimx\\n\\n\\xd6\\x81\\n\\n(cid:18)\\n\\nf (a) + a\\n\\n1\\na f (x) + x\\n\\n\\xe2\\x88\\x92\\n\\n.\\n\\n1\\n\\n(cid:19)\\n\\n\\xe2\\x88\\x92\\n\\nNote that the sum on the right-hand side is \\xef\\xac\\x81nite, since there are only \\xef\\xac\\x81nitely many discon-\\ntinuous points for f .\\n\\n\\xe2\\x88\\x88\\n\\nP\\n\\ne\\n(0,1)\\nXa\\n\\xe2\\x88\\x88\\n\\n14\\n\\n\\x0c, Lp) be the metric space obtained from\\n\\nDf induces a well-de\\xef\\xac\\x81ned map\\n\\nLet p\\nendowing\\n\\n\\xe2\\x89\\xa5\\nP\\n\\n1 be any positive real number. Let (\\nwith the Lp norm. The map f\\nD : (\\n\\n7\\xe2\\x86\\x92\\n, Lp)\\n\\nP\\n\\ne\\n\\xe2\\x86\\x92\\n\\ne\\n\\nR.\\nIn Proposition 3.8, we have seen that the map B : (\\nP\\ne\\neverywhere. The following proposition says that D, considered as a function from (\\nto (R, Euclid), exhibits a similar topological property.\\n\\n(R, Euclid) is discontinuous\\n, Lp)\\n\\n, Lp)\\n\\n\\xe2\\x86\\x92\\n\\nP\\n\\nP\\n\\nProposition 4.15. The map D : (\\n\\n, Lp)\\n\\ne\\n(R, Euclid) is discontinuous everywhere on\\n\\n.\\n\\nbe arbitrary. Let k be a positive integer for which f\\n\\n\\xe2\\x86\\x92\\n\\nP\\n\\ne\\n\\nProof. Let f\\nP\\npositive integer n\\ne\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x89\\xa5\\n\\nk + 1, de\\xef\\xac\\x81ne\\n\\n\\xe2\\x88\\x88\\n\\nP\\nPk. For each\\ne\\ne\\n\\ngn(x) =\\n\\nf (x)\\nn\\xe2\\x88\\x92\\n\\n(\\n\\n1 + n\\xe2\\x88\\x92\\n\\n2\\n\\nif x\\n1\\n\\xe2\\x89\\xa4\\nif x > 1\\n\\n1\\nn ,\\n1\\nn .\\n\\n\\xe2\\x88\\x92\\n\\xe2\\x88\\x92\\n\\nNote that\\nhand, we obtain from Proposition 4.14 that\\n\\nis a sequence of functions in\\n\\ngn}\\n\\n{\\n\\nP\\n\\nthat converges in Lp to f . On the other\\n\\nDgn =\\n\\ne\\n1\\nlog(n + 1) + Df +\\n2\\n\\n1\\n2\\n\\nf (1\\n\\n1\\nn )\\n\\xe2\\x88\\x92\\nf (1)\\n\\n1\\nn\\n\\n\\xe2\\x88\\x92\\n\\n.\\n\\n(cid:19)\\n\\nlog\\n\\n(cid:18)\\n\\n\\xd6\\x80\\n\\n, as n\\n\\n, as n\\n\\n.\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\n1 f (x) = f (1), the third term on the right-hand side converges to 0 as n\\n\\nSince limx\\nThe second term does not depend on n. The \\xef\\xac\\x81rst term goes to +\\nDgn \\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n4.4. Bounds for Bf and Df . In this subsection, we determine the extremal values for\\nPk.\\nboth Bf and Df among all functions f\\ne\\n1\\nk \\xe2\\x89\\xa4\\nBoth bounds are tight. The lower bound is attained if and only if f is the function in\\nExample 4.11. The upper bound is attained if and only if f (x) = 1 for every x\\n\\nProposition 4.16. Let k be a positive integer. Let f\\n\\nPk. Then,\\n1.\\ne\\n\\n.\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n. Therefore,\\n(cid:3)\\n\\n\\xe2\\x88\\x88\\nBf \\xe2\\x89\\xa4 \\xe2\\x88\\x92\\n\\n[0, 1].\\n\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\nlog k\\n\\n\\xe2\\x88\\x9e\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x88\\n\\nProof. The upper bound follows from Proposition 3.9(a). For the equality case of the upper\\nbound, note that since limx\\n[0, 1), then f (1) must\\nalso be 1.\\n\\n1 f (x) = f (1), if f (x) = 1 for all x\\n\\n\\xe2\\x88\\x88\\n\\n\\xd6\\x80\\n\\nFor the lower bound, note that for any function f\\n\\n1/k. Therefore,\\n\\nby Proposition 3.5, we have\\n\\n1\\n\\n\\xe2\\x88\\x88\\n\\nPk, we have loft(f )\\ne\\n\\n1\\n\\n\\xe2\\x89\\xa5\\n\\n1/k\\n\\nBf =\\n\\nlog(f (x) + x\\n\\n1) dx\\n\\nlog(x) dx +\\n\\nlog(1/k) dx\\n\\n0\\nZ\\n\\n0\\nZ\\nComputing the last expression yields the desired lower bound. The equality case happens\\n1 = 1/k on (1/k, 1]. This is exactly if and only if\\nwhen f (x) = 1 on [0, 1/k] and f (x) + x\\n(cid:3)\\nf is the function in Example 4.11.\\n\\n1/k\\n\\n\\xe2\\x88\\x92\\n\\nZ\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa5\\n\\nProposition 4.17. Let k be a positive integer. Let f\\n\\n\\xe2\\x88\\x88\\n\\nPk. Then,\\n2k\\xcf\\x80\\n.\\ne\\nk\\n\\n1\\n2\\n\\n1\\n2\\n\\n2\\xcf\\x80\\nk\\n\\nlog\\n\\nDf \\xe2\\x89\\xa4\\nBoth bounds are tight. The lower bound is attained if and only if f is continuous and\\nf (1) = 1/k (i.e., if and only if f is in the Catalan-numerous family discussed in Remark 4.7).\\nThe upper bound is attained if and only if f is the function in Example 4.12.\\n\\nlog\\n\\n\\xe2\\x89\\xa4\\n\\n(cid:19)\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n(cid:18)\\n\\nProof. For the lower bound, note that by the formula in Proposition 4.14, it is immediate\\nthat\\n\\nDf \\xe2\\x89\\xa5\\n\\n1\\n2\\n\\n15\\n\\nlog(2\\xcf\\x80f (1)).\\n\\n\\x0cSince f\\n1/k. Combining the two inequalities yields the desired lower\\nbound. The equality is attained if and only if f (1) = 1/k and there are no \\xe2\\x80\\x9cjumps.\\xe2\\x80\\x9d In\\nother words, f is continuous and f (1) = 1/k.\\n\\nPk, we have f (1)\\ne\\n\\n\\xe2\\x89\\xa5\\n\\n\\xe2\\x88\\x88\\n\\nFor the upper bound, we use the following strategy. We start with an arbitrary function\\nf\\nPk, and then we keep transforming the function (if possible) in a number of steps so that\\nin each step Df becomes larger. We claim that we can always end at the unique extremal\\nfunction in Example 4.12.\\ne\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nFirst, start with any function f\\n\\nPk. Consider whether f has a linear piece with a\\n1)/k, i/k], f has a negative slope \\xe2\\x80\\x93 modify the\\nstrictly negative slope. If so \\xe2\\x80\\x93 say over ((i\\nfunction f so that over ((i\\n1)/k f (x)\\n1)/k, i/k], it becomes constant with the value limx\\ne\\nPk, and the value Df strictly increases.\\ninstead. The new function remains in\\ne\\n\\nSecond, now assume that the function f is already piecewise constant. Consider the\\n1/k, 1]. If it is strictly greater than 2/k, change the value to 2/k. This\\nvalue of f over (1\\nchange strictly increases Df . Then, consider the value of f over (1\\n1/k]. If it is\\nstrictly greater than 3/k, change the value to 3/k. Keep going in this manner from the right\\nto the left. The resulting function is the unique function in Example 4.12. This proves the\\nupper bound.\\n\\n2/k, 1\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xd6\\x81\\n\\n\\xe2\\x88\\x92\\n\\n(i\\n\\nNote that in each step, if a change is made, the value of Df increases strictly. This shows\\nthat the equality case for the upper bound happens if and only if f is the unique function\\n(cid:3)\\nin Example 4.12.\\n\\n5. Cumulative X-rays of rook placements\\n\\n\\xe2\\x88\\x88\\nLet n be a positive integer. Let \\xce\\xbb\\n\\n5.1. Marginal Probabilities. In the following, for a \\xef\\xac\\x81nite nonempty set S, we denote\\nby Unif(S) the uniform distribution on S. The notation X\\nUnif(S) means that X is a\\n1.\\nuniform random variable so that\\n\\nS, P(X = s) =\\n\\xe2\\x88\\x88 Dn be a partition. In what follows, let us consider\\nour partitions in the French notation so that the boxes of \\xce\\xbb are bottom- and left-aligned\\nand there are \\xce\\xbb1 boxes on the bottom row, \\xce\\xbb2 boxes on the second row from the bottom,\\nand so on.\\n\\ns\\n\\xe2\\x88\\x80\\n\\n\\xe2\\x88\\xbc\\n\\nS\\n\\n\\xe2\\x88\\x92\\n\\n|\\n\\n|\\n\\nProposition 5.1. Let \\xcf\\x80\\n\\nUnif(RP(\\xce\\xbb)). Let i, j\\n\\n[n].\\n\\n\\xe2\\x88\\x88\\n\\n(a) If \\xce\\xbbn+1\\n(b) If \\xce\\xbbn+1\\n\\ni < j, then E(\\xcf\\x80ij ) = 0.\\n\\xe2\\x88\\x92\\nj, and suppose i\\xe2\\x80\\xb2\\ni \\xe2\\x89\\xa5\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\xbc\\n\\n[n] is the smallest index such that \\xce\\xbbn+1\\n\\nj, then\\n\\ni\\xe2\\x80\\xb2\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa5\\n\\nE(\\xcf\\x80ij ) =\\n\\n\\xce\\xbbn+1\\n\\nt \\xe2\\x88\\x92\\n\\nt\\nt + 1 \\xef\\xa3\\xb6\\n\\n1\\n\\n.\\n\\n\\xe2\\x88\\x92\\nt \\xe2\\x88\\x92\\nProof. (a) It follows immediately from the de\\xef\\xac\\x81nition of RP(\\xce\\xbb) (cf. the beginning of Sec-\\ntion 2) that \\xcf\\x80ij = 0.\\n\\nYi\\xe2\\x80\\xb2\\n\\n\\xce\\xbbn+1\\n\\n\\xce\\xbbn+1\\n\\ni + 1\\n\\ni \\xe2\\x88\\x92\\n\\n\\xef\\xa3\\xad\\n\\n\\xef\\xa3\\xb8\\n\\n\\xef\\xa3\\xab\\n\\nt<i\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa4\\n\\n\\xc2\\xb7\\n\\n(b) Suppose that \\xc2\\xb5 is the partition obtained by removing the ith row from the top (the\\ni) and the jth column from the left from the Young diagram of\\n\\nrow corresponding to \\xce\\xbbn+1\\n\\xce\\xbb. Observe that the probability that \\xcf\\x80ij = 1 is # RP(\\xc2\\xb5)/# RP(\\xce\\xbb).\\n\\n\\xe2\\x88\\x92\\n\\nThe formula in Equation (1) gives\\n\\n# RP(\\xc2\\xb5) =\\n\\n(\\xce\\xbbn+1\\n\\nt \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n(t\\n\\n1))\\n\\n\\xe2\\x88\\x92\\n\\n(\\xce\\xbbn+1\\n\\nt \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nt)\\n\\n.\\n\\n \\n\\nYt<i\\xe2\\x80\\xb2 or t>i\\n\\n! \\xef\\xa3\\xab\\n\\xef\\xa3\\xad\\n\\nt<i\\n\\nYi\\xe2\\x80\\xb2\\n\\n\\xe2\\x89\\xa4\\n\\n\\xef\\xa3\\xb6\\n\\n\\xef\\xa3\\xb8\\n\\n1\\n\\nE(\\xcf\\x80ij ) = P(\\xcf\\x80ij = 1) =\\n\\n# RP(\\xc2\\xb5)\\n# RP(\\xce\\xbb)\\n\\n=\\n\\n\\xce\\xbbn+1\\n\\n\\xce\\xbbn+1\\n\\n\\xe2\\x88\\x92\\nt \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nt \\xe2\\x88\\x92\\n\\nt\\nt + 1 \\xef\\xa3\\xb6\\n\\n\\xc2\\xb7\\n\\n\\xce\\xbbn+1\\n\\ni \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n,\\n\\ni + 1\\n\\n\\xef\\xa3\\xb8\\n\\n(cid:3)\\n\\n\\xef\\xa3\\xab\\n\\n\\xef\\xa3\\xad\\n\\nt<i\\n\\nYi\\xe2\\x80\\xb2\\n\\n\\xe2\\x89\\xa4\\n\\n16\\n\\nTherefore,\\n\\nas desired.\\n\\n\\x0cThe following corollary is immediate from Proposition 5.1.\\n\\nCorollary 5.2. If \\xcf\\x80\\n\\nUnif(RP(\\xce\\xbb)) and i, j\\n\\n[n], then\\n\\n\\xe2\\x88\\xbc\\n\\n\\xe2\\x88\\x88\\n\\n1\\n\\nE(\\xcf\\x80ij )\\n\\n\\xe2\\x89\\xa4\\n\\n\\xce\\xbbn+1\\n\\ni \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n.\\n\\ni + 1\\n\\nWe also have the following result. Let \\xce\\xbb\\xe2\\x80\\xb2\\n\\nCorollary 5.3. Let \\xcf\\x80\\nand \\xce\\xbbn+1\\ni2 \\xe2\\x89\\xa5\\n\\n\\xe2\\x88\\x92\\n\\nUnif(RP(\\xce\\xbb)). Suppose that i1, i2, j1, j2 \\xe2\\x88\\x88\\n\\nj1. If \\xce\\xbbi1 = \\xce\\xbbi2 and \\xce\\xbb\\xe2\\x80\\xb2j1 = \\xce\\xbb\\xe2\\x80\\xb2j2 , then E(\\xcf\\x80i1j1 ) = E(\\xcf\\x80i2j2 ).\\n\\n\\xe2\\x88\\xbc\\n\\n\\xe2\\x88\\x88 Dn denote the conjugate partition of \\xce\\xbb.\\ni1 \\xe2\\x89\\xa5\\n\\n[n] satisfy \\xce\\xbbn+1\\n\\n\\xe2\\x88\\x92\\n\\nj1\\n\\nProof. We proceed in a similar manner to how we proved Proposition 5.1(b). Namely, let\\n\\xc2\\xb5(1) (and \\xc2\\xb5(2)) denote the resulting partition from removing the (i1, j1) (and resp. (i2, j2))\\nbox (together with the row and the column) from \\xce\\xbb. It is not hard to see that \\xc2\\xb5(1) = \\xc2\\xb5(2).\\n(cid:3)\\nThis \\xef\\xac\\x81nishes the proof.\\n\\nLike before, we may think of \\xce\\xbb as a Dyck path from (0, n) to (n, 0), which we can write\\n\\nas the following concatenation\\n\\n\\xce\\xbb = Rr1Dd1Rr2Dd2\\n\\n,\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\nwhere R denotes a unit step to the right, and D denotes a unit step down. The equation\\nabove means that \\xce\\xbb starts by going r1 steps to the right, and then d1 steps down, and so\\nas the minimum run of \\xce\\xbb, denoted\\non. Let us refer to the quantity min\\nr1, d1, r2, d2, . . .\\n{\\nmr(\\xce\\xbb). For instance, since \\xce\\xbb\\nn, where the equality is attained if and\\n\\xe2\\x88\\x88 Dn, we have mr(\\xce\\xbb)\\nonly if \\xce\\xbb = RnDn.\\n\\n\\xe2\\x89\\xa4\\n\\n}\\n\\nProposition 5.4. Let \\xcf\\x80\\nany t di\\xef\\xac\\x80erent boxes b1 = (i1, j1), b2 = (i2, j2), . . ., bt = (it, jt)\\nis 1 in all these t boxes is\\n\\nUnif(RP(\\xce\\xbb)). Let t\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\xbc\\n\\n\\xe2\\x88\\x88\\n\\nmr(\\xce\\xbb) be a positive integer. Then, for\\n[n]2, the probability that \\xcf\\x80\\n\\nE(\\xcf\\x80b1 \\xcf\\x80b2 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n\\xcf\\x80bt )\\n\\n\\xe2\\x89\\xa4\\n\\nmr(\\xce\\xbb)\\n\\n(mr(\\xce\\xbb)\\n\\n\\xc2\\xb7\\n\\n(mr(\\xce\\xbb)\\n\\nt + 1)\\n\\n\\xe2\\x88\\x92\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n\\xe2\\x88\\x92\\n\\n.\\n\\n1\\n1)\\n\\nProof. If any of the t boxes is \\xe2\\x80\\x9coutside\\xe2\\x80\\x9d the Young diagram of \\xce\\xbb, we are done. Suppose that\\nj1 and so on). Since removing\\nall these boxes are inside the Young diagram (i.e., \\xce\\xbbn+1\\na box (together with its row and its column) reduces the minimum run by at most 1, it\\nsu\\xef\\xac\\x83ces to show that if we remove one box b (together with its row and its column) from \\xce\\xbb\\nand obtain a new partition \\xc2\\xb5, then\\n\\ni1 \\xe2\\x89\\xa5\\n\\n\\xe2\\x88\\x92\\n\\n# RP(\\xc2\\xb5)\\n# RP(\\xce\\xbb) \\xe2\\x89\\xa4\\n\\n1\\nmr(\\xce\\xbb)\\n\\n.\\n\\n(25)\\n\\n(26)\\n\\nConsider a box b = (i, j) such that \\xce\\xbbn+1\\n\\xce\\xbbn+1\\nde\\xef\\xac\\x81nition of the minimum run, we have\\n\\ni = \\xce\\xbbn+1\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\ni \\xe2\\x89\\xa5\\n\\xe2\\x88\\x92\\ni\\xe2\\x80\\xb2 . Let i\\xe2\\x80\\xb2\\xe2\\x80\\xb2 denote the largest index for which \\xce\\xbbn+1\\n\\nj. Let i\\xe2\\x80\\xb2 denote the smallest index for which\\ni\\xe2\\x80\\xb2\\xe2\\x80\\xb2 . By the\\n\\ni = \\xce\\xbbn+1\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nBy Corollaries 5.2 and 5.3, we have\\n\\ni\\xe2\\x80\\xb2\\xe2\\x80\\xb2\\n\\ni\\xe2\\x80\\xb2 + 1\\n\\nmr(\\xce\\xbb).\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa5\\n\\n(27)\\n\\nE(\\xcf\\x80ij) = E(\\xcf\\x80i\\xe2\\x80\\xb2j)\\n\\nwhich implies (25).\\n\\n1\\n\\n\\xe2\\x89\\xa4\\n\\n\\xce\\xbbn+1\\n\\ni\\xe2\\x80\\xb2\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n=\\n\\ni\\xe2\\x80\\xb2 + 1\\n\\n(\\xce\\xbbn+1\\n\\n1\\ni\\xe2\\x80\\xb2\\xe2\\x80\\xb2) + i\\xe2\\x80\\xb2\\xe2\\x80\\xb2 \\xe2\\x88\\x92\\n\\ni\\xe2\\x80\\xb2\\xe2\\x80\\xb2\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n(26)\\n\\n1\\nmr(\\xce\\xbb)\\n\\n,\\n\\ni\\xe2\\x80\\xb2 + 1\\n\\n\\xe2\\x89\\xa4\\n\\n(cid:3)\\n\\nSince the minimum run also grows as we dilate partitions, we immediately have the\\n\\nfollowing corollary.\\n\\nCorollary 5.5. Let t and N be positive integers such that t\\n\\xce\\xbb)). Then, for any t di\\xef\\xac\\x80erent boxes b1, b2, . . . , bt \\xe2\\x88\\x88\\nUnif(RP(N\\n\\n\\xe2\\x8a\\x99\\n\\nN . Suppose that \\xcf\\x80\\n\\n\\xe2\\x89\\xa4\\n[nN ]2, we have\\n\\n\\xe2\\x88\\xbc\\n\\nE(\\xcf\\x80b1 \\xcf\\x80b2 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n\\xcf\\x80bt )\\n\\n1\\n\\n\\xe2\\x89\\xa4\\n\\nN (N\\n17\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n(N\\n\\nt + 1)\\n\\n\\xe2\\x88\\x92\\n\\n.\\n\\n\\x0cThe proposition below shows that these marginal probabilities P(\\xcf\\x80i,j = 1) behave nicely\\n\\nin the following sense, when we dilate partitions.\\n\\nProposition 5.6. If \\xcf\\x80\\n[nN ], we have\\n\\n\\xe2\\x88\\xbc\\n\\nUnif(RP(N\\n\\n\\xce\\xbb)) and \\xcf\\x80\\xe2\\x86\\x93\\n\\nUnif(RP(\\xce\\xbb)). Then, for any i, j\\n\\n\\xe2\\x8a\\x99\\n\\n\\xe2\\x88\\xbc\\n\\nProof. This follows from a direct computation using Proposition 5.1(b).\\n\\nE(\\xcf\\x80i,j) =\\n\\n1\\nN \\xc2\\xb7\\n\\nE\\n\\n\\xcf\\x80\\xe2\\x86\\x93\\n\\xe2\\x8c\\x88\\n\\n(cid:16)\\n\\ni/N\\n\\n,\\n\\nj/N\\n\\n\\xe2\\x8c\\x89\\n\\n\\xe2\\x8c\\x88\\n\\n\\xe2\\x8c\\x89\\n\\n(cid:17)\\n\\n\\xe2\\x88\\x88\\n\\n(cid:3)\\n\\n5.2. Cumulative X-rays. Let n be a positive integer. Suppose that a permutation \\xcf\\x80\\nis given. The cumulative X-ray of \\xcf\\x80 is the piecewise constant function \\xce\\xbe\\xcf\\x80 : [0, 2n]\\nby\\n\\n\\xe2\\x86\\x92\\n\\nSn\\n\\xe2\\x88\\x88\\nR given\\n\\n\\xce\\xbe\\xcf\\x80(t) :=\\n\\n\\xcf\\x80ij .\\n\\n[n]\\nXi,j\\n\\xe2\\x88\\x88\\nt\\ni+j\\n\\xe2\\x89\\xa4\\n\\n\\xce\\xbe\\xcf\\x80(t) :=\\n\\n\\xce\\xbe\\xcf\\x80(nt).\\ne\\n\\n1\\nn \\xc2\\xb7\\n\\n\\xe2\\x86\\x92\\n\\nWe also de\\xef\\xac\\x81ne the normalized version of cumulative X-rays. The normalized cumulative\\nX-ray of \\xcf\\x80\\n\\nSn is the piecewise constant function\\n\\n[0, 1] given by\\n\\n\\xce\\xbe\\xcf\\x80 : [0, 2]\\n\\n\\xe2\\x88\\x88\\n\\nThe following is a counting lemma which is easy to prove.\\n\\nLemma 5.7. For each real number \\xcf\\x86\\n\\nR and each positive integer N\\n\\nZ\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x89\\xa5\\n\\n1, let\\n\\nS(\\xcf\\x86; N ) := #\\n\\n[N ]\\n\\n[N ]\\n\\nx + y\\n\\n\\xe2\\x88\\x88\\n\\n\\xc3\\x97\\n\\n|\\n\\n\\xcf\\x86N\\n\\n.\\n\\n}\\n\\n\\xe2\\x89\\xa4\\n\\ne\\n\\n\\xe2\\x88\\x88\\n(x, y)\\n{\\n\\nThen, we have the following.\\n\\n(a) If \\xcf\\x86\\n(b) If 0\\n\\n0, then S(\\xcf\\x86; N ) = 0.\\n\\xcf\\x86\\n\\n1, then\\n\\n\\xe2\\x89\\xa4\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\nS(\\xcf\\x86; N ) =\\n\\n=\\n\\nN 2 + O(\\xcf\\x86N ).\\n\\n\\xcf\\x86N\\n2\\n\\n\\xe2\\x8c\\x8b\\n\\n\\xe2\\x8c\\x8a\\n(cid:18)\\n\\n(cid:19)\\n\\n\\xcf\\x862\\n2\\n\\n(c) If 1\\n\\n\\xcf\\x86\\n\\n2, then\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\nS(\\xcf\\x86; N ) = N 2\\n\\n2N\\n\\n\\xe2\\x88\\x92 \\xe2\\x8c\\x8a\\n\\n\\xcf\\x86N\\n2\\n\\n\\xe2\\x8c\\x8b\\n\\n+ 1\\n\\n\\xcf\\x862\\n2\\n\\n=\\n\\n(cid:19)\\n\\n\\xe2\\x88\\x92\\n\\n(cid:18)\\n\\n\\xe2\\x88\\x92\\n\\n(cid:18)\\n\\n+ 2\\xcf\\x86\\n\\n1\\n\\nN 2 + O(\\xcf\\x86N ).\\n\\n\\xe2\\x88\\x92\\n\\n(cid:19)\\n\\n(d) If \\xcf\\x86\\n\\n2, then S(\\xcf\\x86; N ) = N 2.\\nIn (b) and (c), the implicit constants are absolute.\\n\\n\\xe2\\x89\\xa5\\n\\nFor convenience, let us reserve the symbol c. In the following, we let c denote the function\\n\\nc : R\\n\\nR given by\\n\\n\\xe2\\x86\\x92\\n\\n0\\nt2\\n2\\n\\nif t\\n0,\\n\\xe2\\x89\\xa4\\nif 0 < t\\nif 1 < t\\n2.\\nif t\\n\\n1\\n\\n\\xe2\\x88\\x92\\n1\\n\\nt2\\n2 + 2t\\n\\nc(t) := \\xef\\xa3\\xb1\\n\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb2\\n\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb4\\xef\\xa3\\xb3\\nS(\\xcf\\x86; N ) = c(\\xcf\\x86)N 2 + O(N ),\\n\\n\\xe2\\x89\\xa4\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa5\\n\\n\\xe2\\x88\\x92\\n\\n1,\\n2, and\\n\\n\\xe2\\x88\\x88 Dn, let us de\\xef\\xac\\x81ne a function m\\xce\\xbb : [0, 2]\\ndx dy,\\n\\nm\\xce\\xbb(t) :=\\n\\nE\\n\\n\\xe2\\x86\\x92\\n\\n\\xcf\\x80\\n\\xe2\\x8c\\x88\\n\\nx\\n\\ny\\n\\n,\\n\\xe2\\x8c\\x89\\n\\n\\xe2\\x8c\\x88\\n\\n\\xe2\\x8c\\x89\\n\\n1\\nn\\n\\n[0, 1] by\\n\\nZ Z\\nx,y\\n\\xe2\\x88\\x88\\nx+y\\n\\n(0,n]\\nnt\\n\\n\\xe2\\x89\\xa4\\n\\n(cid:0)\\n\\n(cid:1)\\n\\n18\\n\\nThus, Lemma 5.7 says that\\n\\nfor positive integers N .\\nFor each partition \\xce\\xbb\\n\\n(28)\\n\\n(29)\\n\\nwhere \\xcf\\x80\\n\\nUnif(RP(\\xce\\xbb)).\\n\\n\\xe2\\x88\\xbc\\n\\n\\x0cProposition 5.8. Let \\xce\\xbb\\nfor any real number t\\n\\n\\xe2\\x88\\x88 Dn and N\\n[0, 2], we have\\n\\n\\xe2\\x88\\x88\\n\\nZ\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x89\\xa5\\n\\n1. Suppose that \\xcf\\x80\\n\\nUnif(RP(N\\n\\n\\xce\\xbb)). Then,\\n\\n\\xe2\\x88\\xbc\\n\\n\\xe2\\x8a\\x99\\n\\nProof. From Proposition 5.6, we have\\n\\ne\\n\\n(cid:16)\\n\\n(cid:17)\\n\\nE\\n\\n\\xce\\xbe\\xcf\\x80(t)\\n\\n= m\\xce\\xbb(t) + O\\xce\\xbb\\n\\n1\\nN\\n\\n.\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n(30)\\n\\n(31)\\n\\n(32)\\n\\n(33)\\n\\n(34)\\n\\n(35)\\n\\n(36)\\n\\n1\\nnN 2\\n\\nE\\n\\n\\xce\\xbe\\xcf\\x80(t)\\n\\n=\\n\\n(cid:16)\\n\\ne\\n\\n(cid:17)\\n\\n=\\n\\n1\\nnN 2\\n\\nE\\n\\n\\xcf\\x80\\xe2\\x86\\x93\\n\\xe2\\x8c\\x88\\n\\n(cid:16)\\n\\ni/N\\n\\n,\\n\\nj/N\\n\\n\\xe2\\x8c\\x89\\n\\n\\xe2\\x8c\\x88\\n\\n\\xe2\\x8c\\x89\\n\\n(cid:17)\\n\\n[nN ]\\nXi,j\\n\\xe2\\x88\\x88\\nnN t\\ni+j\\n\\xe2\\x89\\xa4\\nn\\nn\\n\\na=1\\nX\\n\\nXb=1\\n\\n[nN ]\\nnN t\\n\\nXi,j\\n\\xe2\\x88\\x88\\ni+j\\n\\xe2\\x89\\xa4\\nj/N\\n,\\n\\n\\xe2\\x8c\\x89\\n\\n(\\n\\ni/N\\n\\n\\xe2\\x8c\\x88\\n\\n\\xe2\\x8c\\x89\\n\\n\\xe2\\x8c\\x88\\n\\n)=(a,b)\\n\\nE\\n\\n\\xcf\\x80\\xe2\\x86\\x93a,b\\n\\n.\\n\\n(cid:16)\\n\\n(cid:17)\\n\\nNote that the innermost summation in the last expression above is over pairs (i, j) of\\naN , and\\nbN . By translation, the number of such pairs is exactly the number\\nb + 2)N . By our discussion above, the number\\n\\npositive integers such that (i) i, j\\n(iv) bN\\nof (x, y)\\nis exactly S(nt\\n\\nN + 1\\n\\xe2\\x89\\xa4\\n[N ]2 such that x + y\\nb + 2; N ).\\n\\nnN , (ii) i + j\\n\\nnN t, (iii) aN\\n\\nN + 1\\n\\n\\xe2\\x88\\x92\\n\\xe2\\x88\\x88\\n\\n(nt\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x88\\x92\\n\\na\\n\\nj\\n\\ni\\n\\na\\nTherefore, Equation (28) gives\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nE\\n\\n\\xce\\xbe\\xcf\\x80(t)\\n\\n=\\n\\n(cid:16)\\n\\ne\\n\\n(cid:17)\\n\\n=\\n\\n1\\nn\\n\\n1\\nn\\n\\nn\\n\\nn\\n\\nE\\n\\nE\\n\\na=1\\nX\\nn\\n\\nXb=1\\nn\\n\\n(cid:16)\\n\\n(cid:17) (cid:18)\\n\\n\\xcf\\x80\\xe2\\x86\\x93a,b\\n\\nc(nt\\n\\na\\n\\nb + 2) + O\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xcf\\x80\\xe2\\x86\\x93a,b\\n\\nc(nt\\n\\na\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nb + 2) + O\\xce\\xbb\\n\\n.\\n\\n(cid:18)\\n\\n(cid:19)\\n\\na=1\\nX\\n\\nXb=1\\n\\n(cid:16)\\n\\n(cid:17)\\n\\n(cid:18)\\n\\n(cid:19)(cid:19)\\n\\n1\\nN\\n\\n1\\nN\\n\\nOn the other hand, by the de\\xef\\xac\\x81nition of m\\xce\\xbb, we have\\nn\\n\\nn\\n\\nb\\n\\na\\n\\nm\\xce\\xbb(t) =\\n\\n1\\nn\\n\\n1\\nn\\n\\n1\\nn\\n\\n=\\n\\n=\\n\\n\\xcf\\x80\\xe2\\x86\\x93a,b\\n(cid:16)\\n\\n\\xcf\\x80\\xe2\\x86\\x93a,b\\n(cid:16)\\n\\nE\\n\\nE\\n\\nE\\n\\n\\xcf\\x80\\xe2\\x86\\x93a,b\\n(cid:16)\\n\\n(cid:17)\\n\\na=1\\nX\\nn\\n\\nXb=1\\nn\\n\\na=1\\nX\\nn\\n\\nXb=1\\nn\\n\\na=1\\nX\\n\\nXb=1\\n\\nb\\n(cid:17) Z\\n\\xe2\\x88\\x92\\n1\\n\\n1 Z\\na\\n1\\n\\n1\\n\\n\\xe2\\x88\\x92\\n\\n0 Z\\n\\n0\\n\\n(cid:17) Z\\n\\nc(nt\\n\\na\\n\\nb + 2),\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n1(x + y\\n\\nnt) dx dy\\n\\n\\xe2\\x89\\xa4\\n\\n1(x + y\\n\\nnt\\n\\na\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa4\\n\\nb + 2) dx dy\\n\\nwhere 1 denotes the indicator function, and Equation (35) follows from simple changes of\\n1. Combining Equations (31) and (36) \\xef\\xac\\x81nishes the\\nvariables x\\n(cid:3)\\nproof.\\n\\n1 and y\\n\\nx + a\\n\\ny + b\\n\\n7\\xe2\\x86\\x92\\n\\n7\\xe2\\x86\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nE\\n\\nIn particular, Proposition 5.8 implies a convergence of expectations. If for each N , we\\n[0, 2], the sequence\\n\\nhave a random variable \\xcf\\x80(N )\\n\\n\\xce\\xbb)), then for any \\xef\\xac\\x81xed t\\n\\nUnif(RP(N\\n\\n\\xe2\\x88\\x9e\\n\\n\\xe2\\x88\\xbc\\nconverges to m\\xce\\xbb(t).\\n\\n\\xe2\\x8a\\x99\\n\\nn\\n\\nN =1\\n\\n\\xce\\xbe\\xcf\\x80(N ) (t)\\nThe author of the present paper gives the following conjecture about this function m\\xce\\xbb.\\n(cid:16)\\ne\\n\\xe2\\x88\\x88 Dn be a \\xef\\xac\\x81xed partition. Fix a positive real number \\xce\\xb5 > 0. Suppose\\n\\xce\\xbb)). Then,\\n\\nConjecture 5.9. Let \\xce\\xbb\\nthat \\xcf\\x80\\n\\nUnif(RP(N\\n\\n(cid:17)o\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\xbc\\n\\n\\xe2\\x8a\\x99\\n\\nas N\\n\\n.\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\nrandom permutations.\\n\\nP\\n\\nsup\\n[0,2]\\n\\n \\nt\\n\\xe2\\x88\\x88\\n\\n\\xce\\xbe\\xcf\\x80(t)\\n\\n\\xe2\\x88\\x92\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)e\\n\\nm\\xce\\xbb(t)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n< \\xce\\xb5\\n\\n1,\\n\\n! \\xe2\\x86\\x92\\n\\n19\\n\\nIn the next subsection, we give a proof of Conjecture 5.9 in the special case of uniformly\\n\\n\\x0c5.3. Limit shape for normalized cumulative X-rays of random permutations. Let\\nN be a positive integer, and let \\xcf\\x80\\nUnif(SN ) be a uniformly random permutation. For\\n[N + 1], we let\\neach k\\n\\n\\xe2\\x88\\xbc\\n\\n\\xe2\\x88\\x88\\n\\nLet\\xe2\\x80\\x99s also de\\xef\\xac\\x81ne, for each k = 2, 3, . . . , 2N , the kth X-ray component\\n\\nXk := \\xce\\xbe\\xcf\\x80(k).\\n\\nxk :=\\n\\n\\xcf\\x80ij .\\n\\n[N ]\\nXi,j\\n\\xe2\\x88\\x88\\ni+j=k\\n\\nIndeed, there is a simple relation between these notations: Xk = x2 + x3 +\\n\\n+ xk.\\n\\nThe goal of this subsection is to prove the following result.\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\nTheorem 5.10. Let \\xce\\xb5 > 0 be any \\xef\\xac\\x81xed positive real number. Then,\\n\\nP\\n\\nk\\n\\n[N + 1],\\n\\n\\xe2\\x88\\x80\\n\\n(cid:18)\\n\\n\\xe2\\x88\\x88\\n\\nk(k\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n< \\xce\\xb5N\\n\\n= 1\\n\\nO\\xce\\xb5\\n\\n(cid:19)\\n\\n\\xe2\\x88\\x92\\n\\n(cid:18)\\n\\nlog N\\nN log log N\\n\\n,\\n\\n(cid:19)\\n\\nXk \\xe2\\x88\\x92\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nas N\\n\\n.\\n\\xe2\\x86\\x92 \\xe2\\x88\\x9e\\n\\nAs a corollary of Theorem 5.10, we obtain a proof of Conjecture 5.9 in the very special\\ncase when \\xce\\xbb = (cid:3) is a partition with one box. In this case, the function m\\xce\\xbb coincides with\\n|[0,2]. From the de\\xef\\xac\\x81nition of c, we see that the graph of this function is a concatenation of\\nc\\ntwo parabolas.\\nHere is our rough strategy for proving the theorem. First, we show that with high\\nprobability the largest X-ray component maxk xk is small. Second, we give an upper bound\\non the size of the variance Var(Xk). Third, we argue that since the X-ray components are\\nsmall with high probability, it su\\xef\\xac\\x83ces to establish the bound\\n\\nwith high probability for all k simultaneously in a certain subset of [N + 1], instead of the\\nwhole [N + 1]. Fourth, we use Chebyshev\\xe2\\x80\\x99s tail bound to show that we have the bound\\n\\nk(k\\n\\n1)\\n\\nXk \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n< \\xce\\xb5N\\n\\nk(k\\n\\n1)\\n\\nXk \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n< \\xce\\xb5N\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nfor all k in the mentioned subset of [N + 1] simultaneously with high probability. This\\n\\xef\\xac\\x81nishes the proof.\\n\\nNow we begin the \\xef\\xac\\x81rst step of our strategy.\\n\\nProposition 5.11. Let t\\n\\nN be a positive integer. We have\\n\\n\\xe2\\x89\\xa4\\n\\nP(xk \\xe2\\x89\\xa5\\n\\nt)\\n\\n\\xe2\\x89\\xa4\\n\\nN + 1\\n(t + 1)!\\n\\n.\\n\\nN +1\\n\\nXk=2\\n\\nProof. The event xk \\xe2\\x89\\xa5\\nof indices in [k\\n\\n1] such that\\n\\n\\xe2\\x88\\x92\\n\\nt is equivalent to the event that there exists a t-subset\\n\\ni1, i2, . . . , it}\\n\\n{\\n\\n\\xcf\\x80i1,k\\n\\ni1 = \\xcf\\x80i2,k\\n\\ni2 =\\n\\n= \\xcf\\x80it,k\\n\\nit = 1.\\n\\n\\xe2\\x88\\x92\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nSince\\n\\n(37)\\n\\n(38)\\n\\nwe have\\n\\n(39)\\n\\nP(\\xcf\\x80i1,k\\n\\n\\xe2\\x88\\x92\\n\\ni1 = \\xcf\\x80i2,k\\n\\ni2 =\\n\\n\\xe2\\x88\\x92\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n= \\xcf\\x80it,k\\n\\nit = 1) = E(\\xcf\\x80i1,k\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\ni1 \\xcf\\x80i2,k\\n1\\n\\ni2 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n\\xe2\\x88\\x92\\n\\n\\xcf\\x80it,k\\n\\nit )\\n\\n\\xe2\\x88\\x92\\n\\n,\\n\\nN (N\\n\\n1)\\n\\n(N\\n\\nt + 1)\\n\\n\\xe2\\x88\\x92\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n\\xe2\\x88\\x92\\n\\n=\\n\\nP(xk \\xe2\\x89\\xa5\\n\\nt)\\n\\n\\xe2\\x89\\xa4\\n\\n1\\nt! \\xc2\\xb7\\n\\n(k\\n\\xe2\\x88\\x92\\nN (N\\n20\\n\\n1)(k\\n1)\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n2)\\n(N\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n(k\\n\\nt)\\n\\xe2\\x88\\x92\\nt + 1)\\n\\n.\\n\\n\\xe2\\x88\\x92\\n\\n\\x0cTherefore, by telescoping, we obtain\\n\\nN +1\\n\\nXk=2\\n\\nP(xk \\xe2\\x89\\xa5\\n\\nt)\\n\\n\\xe2\\x89\\xa4\\n\\n(k\\n\\xe2\\x88\\x92\\nN (N\\n\\n1)(k\\n1)\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n2)\\n(N\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n(k\\n\\nt)\\n\\xe2\\x88\\x92\\nt + 1)\\n\\nN +1\\n\\nXk=2\\n1\\nt!\\n\\nN +1\\n\\nk(k\\n\\n=\\n\\n=\\n\\nXk=2\\nN + 1\\n(t + 1)!\\n\\n,\\n\\n(40)\\n\\n(41)\\n\\n(42)\\n\\nas desired.\\n\\nBy using the bound\\n\\n\\xe2\\x88\\x92\\n\\n1)\\n\\n(k\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n(t + 1)\\n\\nt)\\n\\xe2\\x88\\x92\\nN (N\\n\\n(k\\n\\n1)(k\\n\\n\\xe2\\x88\\x92\\n1)\\n\\n\\xe2\\x88\\x92\\n(N\\n\\n\\xe2\\x88\\x92\\n\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\n\\n(k\\n\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7\\nt + 1)\\n\\n2)\\n\\n\\xe2\\x88\\x92\\n\\nt\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\xc2\\xb7\\n\\n(43)\\n\\nP(xk \\xe2\\x89\\xa5\\ntogether with Stirling\\xe2\\x80\\x99s formula, we obtain the following corollary.\\n\\nx2, x3, . . . , xN +1} \\xe2\\x89\\xa5\\n\\nP(max\\n{\\n\\nXk=2\\n\\n\\xe2\\x89\\xa4\\n\\nt)\\n\\nt) ,\\n\\nN +1\\n\\nCorollary 5.12. For all su\\xef\\xac\\x83ciently large positive integers N , we have\\n\\nNext is the second step of the strategy. For each 2\\n\\nk\\n\\nN + 1, it is easy to see that\\n\\nmax\\n\\nx2, x3, . . . , xN +1} \\xe2\\x89\\xa5\\n\\n{\\n\\n3 log N\\nlog log N\\n\\n1\\nN\\n\\n.\\n\\n\\xe2\\x89\\xa4\\n\\nP\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\nE(Xk) = k(k\\n\\n1)\\n2N .\\n\\xe2\\x88\\x92\\nProposition 5.13. Let N\\n\\nProof. Observe that\\n\\n(44)\\n\\n2 be a positive integer. For 2\\n\\nk\\n\\nN + 1, we have\\n\\n\\xe2\\x89\\xa5\\nX 2\\nk\\n\\n=\\n\\nE\\n\\n(cid:0)\\n\\n(cid:1)\\n\\nk(k\\n\\n1)\\n\\nk(k\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n+\\n\\n\\xe2\\x88\\x92\\n\\n1)(k\\n\\xe2\\x88\\x92\\n12N (N\\n\\n\\xe2\\x89\\xa4\\n\\n5)\\n\\n.\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa4\\n2)(3k\\n1)\\n\\n\\xe2\\x88\\x92\\n\\nk\\n\\nE\\n\\nX 2\\nk\\n\\n=\\n\\n(cid:0)\\n\\n(cid:1)\\n\\nX\\xe2\\x84\\x93=2 Xi,j\\n[N ]\\n\\xe2\\x88\\x88\\n\\ni+j=\\xe2\\x84\\x93 Xi\\n\\n,j\\n\\xe2\\x88\\x88\\n\\xe2\\x80\\xb2\\n\\xe2\\x80\\xb2\\n+j\\ni\\n\\n[N ]\\nk\\n\\n\\xe2\\x80\\xb2\\n\\n\\xe2\\x80\\xb2\\n\\n\\xe2\\x89\\xa4\\n\\nE(\\xcf\\x80ij \\xcf\\x80i\\xe2\\x80\\xb2j\\xe2\\x80\\xb2 ) .\\n\\nFor the innermost summation on the right-hand side above, there are three cases. First,\\n= (i, j) but (i\\xe2\\x80\\xb2, j\\xe2\\x80\\xb2) is on either\\n2 such\\n\\nif (i\\xe2\\x80\\xb2, j\\xe2\\x80\\xb2) = (i, j), then E(\\xcf\\x80ij \\xcf\\x80i\\xe2\\x80\\xb2j\\xe2\\x80\\xb2 ) = 1/N . Second, if (i\\xe2\\x80\\xb2, j\\xe2\\x80\\xb2)\\nthe same row or the same column as (i, j), then E(\\xcf\\x80ij \\xcf\\x80i\\xe2\\x80\\xb2j\\xe2\\x80\\xb2 ) = 0. There are 2k\\nordered pairs. For the rest, the expectation E(\\xcf\\x80ij\\xcf\\x80i\\xe2\\x80\\xb2j\\xe2\\x80\\xb2 ) is\\n1) . Therefore,\\n\\n1\\nN (N\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x84\\x93\\n\\n(45)\\n\\nk\\n\\nE\\n\\nX 2\\nk\\n\\n=\\n\\n(cid:0)\\n\\n(cid:1)\\n\\nX\\xe2\\x84\\x93=2 Xi,j\\n[N ]\\n\\xe2\\x88\\x88\\ni+j=\\xe2\\x84\\x93\\n\\n(cid:26)\\n\\nSimplify to \\xef\\xac\\x81nish.\\n\\n1\\nN\\n\\n+\\n\\nk(k\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n2\\n\\n\\xe2\\x88\\x92\\n\\n(cid:18)\\n\\n2k + \\xe2\\x84\\x93 + 1\\n\\n\\xe2\\x88\\x92\\n\\n\\xc2\\xb7\\n\\n(cid:19)\\n\\n1\\nN (N\\n\\n.\\n\\n1)\\n\\n(cid:27)\\n\\n\\xe2\\x88\\x92\\n\\nProposition 5.14. Let N\\n\\n2 be a positive integer. For 2\\n\\nk\\n\\nN + 1, we have\\n\\nVar(Xk) <\\n\\nk2\\n2N\\n\\n.\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\nProof. From Proposition 5.13 and some algebraic manipulation, we obtain\\n\\n(46)\\n\\nVar(Xk) = E\\n\\nX 2\\nk\\n\\n(EXk)2 =\\n\\nIt is not hard to see that\\n\\n(cid:0)\\n\\n(cid:1)\\n\\nk(k\\n\\n1)\\n2N \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nk(k\\n\\n1)(4k\\n\\n5)\\n\\n\\xe2\\x88\\x92\\n6N (N\\n\\n\\xe2\\x88\\x92\\n1)\\n\\n+\\n\\nk2(k\\n\\xe2\\x88\\x92\\n4N 2(N\\n\\n1)2\\n1)\\n\\n.\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nk(k\\n\\n1)(4k\\n\\n5)\\n\\n\\xe2\\x88\\x92\\n6N (N\\n\\n\\xe2\\x88\\x92\\n1)\\n\\n\\xe2\\x89\\xa5\\n\\nk2(k\\n\\xe2\\x88\\x92\\n4N 2(N\\n\\n1)2\\n1)\\n\\n,\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n\\nwhence Var(Xk)\\n\\nk(k\\n\\n1)\\n\\n2N < k2\\n2N .\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa4\\n\\n21\\n\\n\\xe2\\x89\\xa5\\n\\n\\xe2\\x88\\x92\\n\\n(cid:3)\\n\\n(cid:3)\\n\\n(cid:3)\\n\\n6\\n\\x0c(47)\\n\\n(48)\\n\\nthen\\n\\n(cid:12)\\n(cid:12)\\nk\\xe2\\x80\\xb2\\n(cid:12)\\n|\\n\\n\\xe2\\x88\\x92\\n\\n|\\n\\n(49)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(50)\\n\\n(51)\\n\\nWe now turn to the third step of the described strategy. For each real number \\xce\\xb5 > 0, we\\n\\nlet E\\xce\\xb5 denote the event\\n\\nk\\n\\n\\xe2\\x88\\x80\\n\\n\\xe2\\x88\\x88 {\\n\\n2, 3, . . . , N + 1\\n\\n,\\n\\nk(k\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n< \\xce\\xb5N.\\n\\nFor each positive integer g\\n\\nN + 1, we let Eg,\\xce\\xb5 denote the event\\n\\n\\xe2\\x89\\xa4\\n\\ngZ\\n\\nk\\n\\n\\xe2\\x88\\x80\\n\\n\\xe2\\x88\\x88\\n\\n\\xe2\\x88\\xa9\\n\\n[N + 1],\\n\\nk(k\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n< \\xce\\xb5N.\\n\\n}\\n\\nXk \\xe2\\x88\\x92\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nXk \\xe2\\x88\\x92\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nHere, gZ\\n\\n\\xe2\\x88\\xa9\\n\\n[N + 1] refers to the set\\n\\ng, 2g, . . . ,\\n\\nN +1\\ng\\n\\ng\\n\\n.\\n\\nj\\n\\nk\\n\\no\\n\\nn\\n\\nProposition 5.15. Let \\xce\\xb5 > 0 be a positive real number. For all su\\xef\\xac\\x83ciently large positive\\nintegers N , if\\n\\ng <\\n\\n\\xce\\xb5\\n8 \\xc2\\xb7\\n\\nN log log N\\nlog N\\n\\n,\\n\\nP(E\\xce\\xb5)\\n\\nP\\n\\nEg,\\xce\\xb5/2\\n\\n\\xe2\\x89\\xa5\\n\\n1\\nN\\n\\n.\\n\\n\\xe2\\x88\\x92\\n\\n3 log N\\nProof. Let A denote the event that max\\nlog log N . By Corollary 5.12,\\nit su\\xef\\xac\\x83ces to show that for all su\\xef\\xac\\x83ciently large positive integers N , we have the inclusion\\nEg,\\xce\\xb5 \\xe2\\x8a\\x86\\n\\nE\\xce\\xb5 \\xe2\\x88\\xaa\\n\\nA.\\n\\n{\\n\\n\\xe2\\x80\\xb2\\n\\n\\xe2\\x80\\xb2\\n\\n(cid:0)\\n(cid:1)\\nx2, x3, . . . , xN +1} \\xe2\\x89\\xa5\\n\\nConsider any event in Eg,\\xce\\xb5/2 \\\\\\n\\n(k\\n\\xe2\\x88\\x92\\n2N\\nlog log N , for any \\xe2\\x84\\x93. We claim that for any k\\n\\nA. In this case,\\n\\n\\xe2\\x88\\x92\\n\\nk\\n\\n1)\\n\\n< \\xce\\xb5N\\n\\n2 for any k\\xe2\\x80\\xb2 divisible\\n, we have\\n\\n2, 3, . . . , N + 1\\n\\n(cid:12)\\n(cid:12)\\n\\xe2\\x88\\x88 {\\n(cid:12)\\n\\n}\\n\\nXk\\xe2\\x80\\xb2\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nby g, and x\\xe2\\x84\\x93 < 3 log N\\n1)\\nXk \\xe2\\x88\\x92\\n\\n< \\xce\\xb5N .\\n\\n\\xe2\\x88\\x92\\n2N\\n\\nk(k\\n\\nNote that we can \\xef\\xac\\x81nd an integer k\\xe2\\x80\\xb2\\n\\n2, 3, . . . , N + 1\\n\\nwhich is a multiple of g such that\\n\\nk\\n\\n< g. By the triangle inequality, we have\\n\\n\\xe2\\x88\\x88 {\\n\\n}\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nk(k\\n\\n1)\\n\\nXk \\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n2N\\n\\nXk \\xe2\\x88\\x92\\n\\n\\xe2\\x89\\xa4 |\\n\\nXk\\xe2\\x80\\xb2\\n\\n+\\n\\nXk\\xe2\\x80\\xb2\\n\\n|\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n+\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n2N\\n\\nk\\xe2\\x80\\xb2(k\\xe2\\x80\\xb2\\n\\n1)\\n\\nk\\xe2\\x80\\xb2(k\\xe2\\x80\\xb2\\n\\n1)\\n\\nk(k\\n\\n1)\\n\\nThe \\xef\\xac\\x81rst term on the right-hand side is\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n.\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nXk \\xe2\\x88\\x92\\n\\n|\\n\\nXk\\xe2\\x80\\xb2\\n\\n| \\xe2\\x89\\xa4\\n\\ng\\n\\n\\xc2\\xb7\\n\\n3 log N\\nlog log N\\n\\n<\\n\\n\\xce\\xb5N.\\n\\nThe second term is less than \\xce\\xb5N\\n\\n2 . The third term is\\n\\nk\\xe2\\x80\\xb2(k\\xe2\\x80\\xb2\\n\\n1)\\n\\nk(k\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n\\xe2\\x88\\x92\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n=\\n\\n(k\\xe2\\x80\\xb2\\n\\n\\xe2\\x88\\x92\\n\\nk)(k\\xe2\\x80\\xb2 + k\\n2N\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n\\ng\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\n1\\n8\\n\\n\\xce\\xb5N,\\n\\nfor all su\\xef\\xac\\x83ciently large N . Therefore,\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nk(k\\n\\n1)\\n\\nXk \\xe2\\x88\\x92\\n(cid:12)\\n(cid:12)\\n(cid:12)\\nfor all su\\xef\\xac\\x83ciently large positive integers N . This shows that Eg,\\xce\\xb5/2 \\\\\\n(cid:12)\\nproof.\\n\\n< \\xce\\xb5N,\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nA\\n\\nE\\xce\\xb5, \\xef\\xac\\x81nishing the\\n(cid:3)\\n\\n\\xe2\\x8a\\x86\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n3\\n8\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nWe have arrived at the \\xef\\xac\\x81nal step of our strategy.\\n\\n22\\n\\n\\x0cProof of Theorem 5.10. Let Ec\\nshev\\xe2\\x80\\x99s tail bound, we have, for all su\\xef\\xac\\x83ciently large positive integers N ,\\n\\ng,\\xce\\xb5/2 denote the complement of the event Eg,\\xce\\xb5/2. By Cheby-\\n\\n(52)\\n\\nP\\n\\nEc\\n\\ng,\\xce\\xb5/2\\n\\n(cid:16)\\n\\n\\xe2\\x89\\xa4\\n\\n(cid:17)\\n\\ngZ\\nXk\\n\\xe2\\x88\\xa9\\n\\xe2\\x88\\x88\\n\\n[N +1]\\n\\nP\\n\\nXk \\xe2\\x88\\x92\\n\\n(cid:18)(cid:12)\\n(cid:12)\\n(cid:12)\\nVar(Xk)\\n(cid:12)\\n(\\xce\\xb5N/2)2\\n\\nk(k\\n\\n1)\\n\\n\\xe2\\x88\\x92\\n2N\\n\\n\\xce\\xb5N\\n2\\n\\n\\xe2\\x89\\xa5\\n\\n(cid:19)\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n(53)\\n\\n(54)\\n\\n(55)\\n\\n\\xe2\\x89\\xa4\\n\\n\\xe2\\x89\\xa4\\n\\n<\\n\\n[N +1]\\n\\ngZ\\nXk\\n\\xe2\\x88\\xa9\\n\\xe2\\x88\\x88\\n2\\n\\xce\\xb52N 3\\n\\n[N +1]\\n\\ngZ\\nXk\\n\\xe2\\x88\\xa9\\n\\xe2\\x88\\x88\\nlog N\\nN log log N\\n\\n.\\n\\n3\\n\\xce\\xb53 \\xc2\\xb7\\n\\nk2\\n\\n(by Proposition 5.14)\\n\\nCombining this with Proposition 5.15, we \\xef\\xac\\x81nish the proof.\\n\\n(cid:3)\\n\\nAcknowledgments\\n\\nI would like to thank Morris Ang, Alexei Borodin, Matthew Nicoletti, Alex Postnikov,\\nI would like to thank\\nSahana Vasudevan, and Wijit Yangjit for insightful discussions.\\nAlex Postnikov speci\\xef\\xac\\x81cally for telling me about Proposition 4.6 and showing his proof to\\nme, which led to the discussion of waterfalls in this paper. I am grateful for Alex Post-\\nnikov and Alexei Borodin speci\\xef\\xac\\x81cally for their encouragement. I would also like to thank\\nRichard Kenyon for sharing with me a copy of the slides from his \\xe2\\x80\\x9cpermutons\\xe2\\x80\\x9d talk. I would\\nlike to thank Sorawee Porncharoenwase for algorithmic insights and technical help. I used\\nPolymake, R, Racket, and Wolfram Alpha to help with computations.\\n\\nReferences\\n\\n[AM14]\\n\\n[Bar21]\\n\\n[BF14]\\n\\nMahshid Atapour and Neal Madras. Large deviations and ratio limit theorems for pattern-\\navoiding permutations. Combin. Probab. Comput., 23(2):161\\xe2\\x80\\x93200, 2014.\\nKenneth Barrese. A graph theory of rook placements. Electron. J. Combin., 28(4):Paper No.\\n4.13, 26, 2021.\\nRichard A. Brualdi and Eliseu Fritscher. Hankel and Toeplitz X-rays of permutations. Linear\\nAlgebra Appl., 449:350\\xe2\\x80\\x93380, 2014.\\n\\n[BLRS14] Kenneth Barrese, Nicholas Loehr, Je\\xef\\xac\\x80rey Remmel, and Bruce E. Sagan. m-level rook placements.\\n\\nJ. Combin. Theory Ser. A, 124:130\\xe2\\x80\\x93165, 2014.\\n\\n[BLRS16] Kenneth Barrese, Nicholas Loehr, Je\\xef\\xac\\x80rey Remmel, and Bruce E. Sagan. Bijections on m-level\\n\\nrook placements. European J. Combin., 57:13\\xe2\\x80\\x9335, 2016.\\n\\n[BMPS05] Cecilia Bebeacua, Tou\\xef\\xac\\x81k Mansour, Alex Postnikov, and Simone Severini. On the X-rays of\\npermutations. In Proceedings of the Workshop on Discrete Tomography and its Applications,\\nvolume 20 of Electron. Notes Discrete Math., pages 193\\xe2\\x80\\x93203. Elsevier Sci. B. V., Amsterdam,\\n2005.\\nKaren S. Briggs and Je\\xef\\xac\\x80rey B. Remmel. m-rook numbers and a generalization of a formula of\\nFrobenius to Cm \\xe2\\x89\\x80 Sn. J. Combin. Theory Ser. A, 113(6):1138\\xe2\\x80\\x931171, 2006.\\n\\n[BR06]\\n\\n[GGKK15] Roman Glebov, Andrzej Grzesik, Tereza Klimo\\xcb\\x87sov\\xc2\\xb4a, and Daniel Kr\\xc2\\xb4al\\xe2\\x80\\x99. Finitely forcible graphons\\n\\nand permutons. J. Combin. Theory Ser. B, 110:112\\xe2\\x80\\x93135, 2015.\\n\\n[GX06]\\n\\n[GHK+17] Roman Glebov, Carlos Hoppen, Tereza Klimo\\xcb\\x87sov\\xc2\\xb4a, Yoshiharu Kohayakawa, Daniel Kr\\xc2\\xb4al\\xe2\\x80\\x99, and\\nHong Liu. Densities in large permutations and parameter testing. European J. Combin., 60:89\\xe2\\x80\\x93\\n99, 2017.\\nIra M. Gessel and Guoce Xin. The generating function of ternary trees and continued fractions.\\nElectron. J. Combin., 13(1):Research Paper 53, 48, 2006.\\nGabor T. Herman and Attila Kuba, editors. Discrete tomography. Applied and Numerical Har-\\nmonic Analysis. Birkh\\xc2\\xa8auser Boston, Inc., Boston, MA, 1999. Foundations, algorithms, and ap-\\nplications.\\n\\n[HK99]\\n\\n[HKM+13] Carlos Hoppen, Yoshiharu Kohayakawa, Carlos Gustavo Moreira, Bal\\xc2\\xb4azs R\\xc2\\xb4ath, and Rudini\\nMenezes Sampaio. Limits of permutation sequences. J. Combin. Theory Ser. B, 103(1):93\\xe2\\x80\\x93113,\\n2013.\\n\\n[KKRW20] Richard Kenyon, Daniel Kr\\xc2\\xb4a\\xcb\\x87l, Charles Radin, and Peter Winkler. Permutations with \\xef\\xac\\x81xed pat-\\n\\ntern densities. Random Structures Algorithms, 56(1):220\\xe2\\x80\\x93250, 2020.\\n\\n23\\n\\n\\x0c[MV07]\\n\\n[Nor08]\\n[OEI]\\n\\n[Pos09]\\n\\n[Rio02]\\n\\n[Sta99]\\n\\n[Sta12]\\n\\n(2022), The On-Line Encyclopedia\\n\\nHugh L. Montgomery and Robert C. Vaughan. Multiplicative number theory. I. Classical the-\\nory, volume 97 of Cambridge Studies in Advanced Mathematics. Cambridge University Press,\\nCambridge, 2007.\\nGustav Nordh. Perfect Skolem sets. Discrete Math., 308(9):1653\\xe2\\x80\\x931664, 2008.\\nOEIS Foundation Inc.\\nhttps://oeis.org/.\\nAlexander Postnikov. Permutohedra, associahedra, and beyond. Int. Math. Res. Not. IMRN,\\n(6):1026\\xe2\\x80\\x931106, 2009.\\nJohn Riordan. An introduction to combinatorial analysis. Dover Publications, Inc., Mineola,\\nNY, 2002.\\nRichard P. Stanley. Enumerative combinatorics. Vol. 2, volume 62 of Cambridge Studies in\\nAdvanced Mathematics. Cambridge University Press, Cambridge, 1999. With a foreword by\\nGian-Carlo Rota and appendix 1 by Sergey Fomin.\\nRichard P. Stanley. Enumerative combinatorics. Volume 1, volume 49 of Cambridge Studies in\\nAdvanced Mathematics. Cambridge University Press, Cambridge, second edition, 2012.\\n\\nInteger Sequences.\\n\\nof\\n\\nDepartment of Mathematics, Massachusetts Institute of Technology, Cambridge, MA 02139\\nEmail address, P. Jiradilok: pakawut@mit.edu\\n\\n24\\n\\n\\x0c', b'On the Importance of Asymmetry for Siamese Representation Learning\\n\\nXiao Wang\\xe2\\x88\\x97,\\xe2\\x80\\xa0 Haoqi Fan1,\\xe2\\x80\\xa0 Yuandong Tian1 Daisuke Kihara2 Xinlei Chen1\\n\\n1Facebook AI Research (FAIR)\\n2Purdue University\\nCode: https://github.com/facebookresearch/asym-siam\\n\\n2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n3\\n1\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nMany recent self-supervised frameworks for visual rep-\\nresentation learning are based on certain forms of Siamese\\nnetworks. Such networks are conceptually symmetric with\\ntwo parallel encoders, but often practically asymmetric as\\nnumerous mechanisms are devised to break the symmetry.\\nIn this work, we conduct a formal study on the importance\\nof asymmetry by explicitly distinguishing the two encoders\\nwithin the network \\xe2\\x80\\x93 one produces source encodings and the\\nother targets. Our key insight is keeping a relatively lower\\nvariance in target than source generally bene\\xef\\xac\\x81ts learning.\\nThis is empirically justi\\xef\\xac\\x81ed by our results from \\xef\\xac\\x81ve case\\nstudies covering different variance-oriented designs, and\\nis aligned with our preliminary theoretical analysis on the\\nbaseline. Moreover, we \\xef\\xac\\x81nd the improvements from asym-\\nmetric designs generalize well to longer training schedules,\\nmultiple other frameworks and newer backbones. Finally,\\nthe combined effect of several asymmetric designs achieves\\na state-of-the-art accuracy on ImageNet linear probing and\\ncompetitive results on downstream transfer. We hope our\\nexploration will inspire more research in exploiting asym-\\nmetry for Siamese representation learning.\\n\\n1. Introduction\\n\\nDespite different motivations and formulations, many re-\\ncent un-/self-supervised methods for visual representation\\nlearning [1, 6\\xe2\\x80\\x938, 18, 19, 44] are based on certain forms of\\nSiamese networks [4]. Siamese networks are inherently\\nsymmetric, as the two encoders within such networks share\\nmany aspects in design. For example, their model architec-\\ntures (e.g., ResNet [20]) are usually the same; their network\\nweights are often copied over; their input distributions \\xe2\\x80\\x93\\ntypically compositions of multiple data augmentations [8] \\xe2\\x80\\x93\\nare by default identical; and their outputs are encouraged to\\nbe similar for the same image. Such a symmetric structure\\nnot only enables straightforward adaptation from off-the-\\nshelf, supervised learning architectures to self-supervised\\nlearning, but also introduces a minimal inductive bias to\\n\\nFigure 1. Asymmetry for Siamese representation learning. For\\nthe two encoders in a Siamese network, we treat one as a source\\nencoder, and the other as a target encoder. We \\xef\\xac\\x81nd it generally\\nbene\\xef\\xac\\x81cial to have relatively lower variance in target than source.\\n\\nlearn representations invariant w.r.t. various transformations\\nin computer vision [10].\\n\\nHowever, symmetry is not the only theme in these frame-\\nworks.\\nIn fact, numerous mechanisms were proposed to\\nbreak the conceptual symmetry. For example, BYOL [18]\\nand SimSiam [10] place a special predictor head on one\\nof the encoders, so architecture-wise they are no longer\\nsymmetric; MoCo [19] introduces momentum encoder, in\\nwhich the weights are computed with moving-averages in-\\nstead of directly copied; SwAV [6] and DINO [7] addition-\\nally adopt a multi-crop [27] strategy to enhance the augmen-\\ntation on one side, shifting the data distribution asymmetric\\nbetween encoders; even the InfoNCE loss [28] treats out-\\nputs from two encoders differently \\xe2\\x80\\x93 one is positive-only\\nand the other also involves negatives. Among them, some\\nspeci\\xef\\xac\\x81c asymmetric designs are crucial and well-studied\\n(e.g., stop-gradient to prevent collapse [10]), but the gen-\\neral role of asymmetry for Siamese representation learning\\nis yet to be better understood.\\n\\nIn this paper, we conduct a more formal study on the\\nimportance of asymmetry for Siamese learning. Deviat-\\ning from the original meaning of \\xe2\\x80\\x98Siamese\\xe2\\x80\\x99, we explic-\\nitly mark the two encoders within the network function-\\nally different: a source encoder and a target encoder.1 The\\n\\n1Depending on the context, source has also been referred as query/on-\\n\\n\\xe2\\x88\\x97: work done during internship at FAIR. \\xe2\\x80\\xa0: equal contribution.\\n\\nline/student; and target as key/teacher in the literature [18, 19, 32].\\n\\n1\\n\\nlowervariancehighervariancelosssourcetargetx\\x0csource encoder generates source encodings, and updates its\\nweights via normal gradient-based optimization like in su-\\npervised learning. The target encoder updates its weights\\nonly with their source counterparts, and outputs target en-\\ncodings which in turn judge the quality of sources. This\\nasymmetric encoder formulation also covers symmetric en-\\ncoders (e.g., in SimCLR [8]), where the target weights can\\nbe simply viewed as source duplicates.\\n\\nWith this distinction, our key insight is that keeping a rel-\\natively lower variance in target encodings than source can\\nhelp representation learning (illustrated in Fig. 1). We sys-\\ntematically study this phenomenon with our MoCo v2 [9]\\nvariant beyond existing \\xe2\\x80\\x93 but scattered \\xe2\\x80\\x93 evidence in the\\nliterature [5, 6, 19, 24, 37]. Speci\\xef\\xac\\x81cally, given a variance-\\noriented design, we \\xef\\xac\\x81rst quantify its encoding variance with\\nour baseline model, and then apply it to source or target (or\\nboth) encoders and examine the in\\xef\\xac\\x82uence on learned repre-\\nsentations. In total, we have conducted \\xef\\xac\\x81ve case studies to\\nexplore various design spaces, ranging from encoder inputs,\\nto intermediate layers and all the way to network outputs.\\nThe results are well-aligned with our insight: designs that\\nincrease encoding variance generally help when applied to\\nsource encoders, whereas ones that decrease variance favor\\ntarget. We additionally provide a preliminary theoretical\\nanalysis taking MoCo pre-training objective as an example,\\naimed at revealing the underlying cause.\\n\\nOur observation generalizes well. First, we show the\\nimprovements from asymmetry \\xe2\\x80\\x93 lower variance in target\\nthan source \\xe2\\x80\\x93 can hold with longer pre-training schedules,\\nsuggesting they are not simply an outcome of faster con-\\nvergence. Second, directly applying proper asymmetric\\ndesigns from MoCo v2 to a variety of other frameworks\\n(e.g., BYOL [18], Barlow Twins [44]) also works well,\\ndespite notable changes in objective function (contrastive\\nor non-contrastive), model optimization (large-batch train-\\ning [43] or not), etc. Third, using MoCo v3 [11], we\\nalso experimented a more recent backbone \\xe2\\x80\\x93 Vision Trans-\\nformer (ViT) [14] \\xe2\\x80\\x93 and \\xef\\xac\\x81nd the generalization still holds\\nwell. Finally, several asymmetric designs are fairly com-\\npositional: their combined effect enables single-node pre-\\ntrained MoCo v2 to reach a top-1 linear probing accuracy\\nof 75.6% on ImageNet, a state-of-the-art with ResNet-50\\nbackbone. This model also demonstrates good transferring\\nability to other downstream classi\\xef\\xac\\x81cation tasks [8, 15, 18].\\n\\nIn summary, our study reveals an intriguing correlation\\nbetween the relative source-target variance and the learned\\nrepresentation quality. We have to note that such correla-\\ntion has limitations, especially as self-supervised learning\\nfollows a staged evaluation paradigm and the \\xef\\xac\\x81nal result is\\ninevitably in\\xef\\xac\\x82uenced by many other factors. Nonetheless,\\nwe hope our exploration will raise the awareness of the im-\\nportant role played by asymmetry for Siamese representa-\\ntion learning, and inspire more research in this direction.\\n\\n2. Related Work\\n\\nSiamese networks are weight-sharing networks [4] that\\nprocess multiple inputs and produce multiple outputs in par-\\nallel. It has been widely used in computer vision [3,4,31,38]\\nand has recently caught attention in self-supervised learn-\\ning [8, 10]. This can be explained by the design of Siamese\\nnetworks, which can conveniently learn invariance in a\\ndata-driven fashion \\xe2\\x80\\x93 a widely acknowledged property for\\nuseful visual representations [10]. While a na\\xc2\\xa8\\xc4\\xb1ve applica-\\ntion of Siamese network can incur collapse, various formu-\\nlations and mechanisms (e.g., contrastive learning [8, 19],\\nonline balanced clustering [6, 7], extra predictor [10, 18],\\nvariance reduction loss [1, 44]) \\xe2\\x80\\x93 many of them asymmetric\\n\\xe2\\x80\\x93 have been proposed to maintain healthy learning dynam-\\nics. Our focus is not on collapse prevention. Instead, we\\nstudy generic designs that change encoding variance, ana-\\nlyze their effect on the output representations, and show that\\nan asymmetry between source and target helps learning.\\n\\nSymmetry for Siamese learning. While the theme of the\\npaper is asymmetry, symmetry is also a powerful concept\\nin Siamese learning. One advantage of symmetry is in re-\\nducing the computation cost when source and target en-\\ncoders share the same backbone weights. In such frame-\\nworks [8,10], source features can be reused for targets, sav-\\ning the extra need to compute with a second encoder. Re-\\ncently, symmetric designs alone are also shown to yield the\\nsame level of performance as asymmetric methods [1, 44].\\nInterestingly, there is often an attempt to symmetrize the\\nloss by forwarding image views once as source and once\\nas target [11, 18], even when the encoder weights are not\\nshared (e.g., in case of a momentum encoder [19]). Com-\\npared to using a single asymmetric loss but training for 2\\xc3\\x97\\nas long, this practice has the same number of forward/back-\\nward passes and we empirically verify it generates similar\\nresults across frameworks (see Sec. 6.2) [10]. Therefore, we\\nbelieve loss symmetrization is not essential beyond plausi-\\nble better performance at the \\xe2\\x80\\x98same\\xe2\\x80\\x99 training epochs.\\n\\nAsymmetric source-target variance. Asymmetry in vari-\\nance is already serving self-supervised learning in implicit\\nways. MoCo [19] itself is a successful example: by smooth-\\ning its target encoder, the memory bank stores consistent\\nkeys with smaller variance across training iterations. Mo-\\nmentum update has been extended to normalization statis-\\ntics to further reduce variance [5, 24], again applied on tar-\\ngets. State-of-the-art on ImageNet [37, 41, 47] is held by\\nusing high-variance, strong augmentations on source views.\\nSiamese networks are also popular in semi-supervised\\nlearning, where some examples are unlabeled. To create\\nmore reliable pseudo labels, the common practice is to aver-\\nage predicted labels over augmented views [2,30,36], which\\neffectively reduces variance on target. Such evidences are\\nscattered in the literature, and we analyze it systematically.\\n\\n2\\n\\n\\x0c(a) MultiCrop (Sec. 4.1)\\n\\n(b) ScaleMix (Sec. 4.2)\\n\\n(c) AsymAug (Sec. 4.3)\\n\\n(d) SyncBN (Sec. 4.4)\\n\\n(e) MeanEnc (Sec. 4.5)\\n\\nFigure 2. We present \\xef\\xac\\x81ve case studies exploring different variance-oriented designs for source and target encoders. For each column, we\\nshow the speci\\xef\\xac\\x81c design on the top, and its in\\xef\\xac\\x82uence on the encoding variance (both the cumulative distribution function and the mean on\\nthe validation set as our empirical reference) at the bottom. Each design is then applied to either the source, the target, or both encoders.\\nThe resulting representation is evaluated by linear probing on ImageNet. Best viewed on a screen and zoomed in. See Sec. 4 for details.\\n\\n3. Methodology Overview\\n\\nIn this section we give an overview for our methodology\\nto systematically study variance-oriented encoder designs.\\nFirst, we specify our variance of interest. While exactly\\nquantifying such variance during training is hard, we pro-\\nvide an approximate reference for such variance using our\\nbaseline model. Now, for each design we can then compute\\nits variance reference and quantify the relative change in\\ncomparison to a vanilla encoder. Regardless of the change\\n(higher or lower), we plug-in the design to either the source,\\nthe target, or both encoders and see its in\\xef\\xac\\x82uence on result-\\ning representations after pre-training. The in\\xef\\xac\\x82uence is mea-\\nsured by linear probing on ImageNet [13]. For a particular\\ndesign, if applying it to both (or neither) encoders is bet-\\nter, then it implies maintaining symmetry is important; if it\\nprefers either source or target, then it means asymmetry is\\nbene\\xef\\xac\\x81cial. In such cases, we also check whether the change\\nin variance is correlated with the encoder preference.\\n\\nIn total, we have conducted \\xef\\xac\\x81ve case studies exploring\\nvarious design spaces, ranging from encoder inputs (i.e.,\\ndata augmentations), to intermediate layers (i.e., different\\nbatch sizes for Batch Normalization [21]) all the way to net-\\nwork outputs (i.e., averaging multiple encodings to reduce\\nvariance). Fig. 2 shows these designs and their variance\\nplots in conjunction with our baseline. We detail our base-\\nline and each case study in Sec. 4, and \\xef\\xac\\x81rst motivate our\\nvariance of interest and its reference in the following.\\n\\nVariance of interest. As each encoding is the encoder out-\\nput of an augmented view from an image, the total variance\\ni) changes\\nin encodings mainly comes from three types:\\nto the encoder, ii) changes across images, and iii) changes\\nwithin a single image. For type i), MoCo [19] with its mo-\\nmentum encoder is already a major, well-studied asymmet-\\n\\nric design that intuitively reduces the target variance across\\ntraining iterations. For type ii), as Siamese representation\\nlearning encourages uniformity [10, 35], the cross-image\\nvariance quickly converges to a constant dependent only on\\nencoding dimensions (evidenced in Appendix A).2 There-\\nfore, we focus on type iii), i.e., intra-image variance as the\\nmain subject of our study. Note that it does not restrict us\\nto design input augmentations as the only means to adjust\\nvariance, as will be discussed in Secs. 4.4 and 4.5.\\n\\nVariance reference. Exactly quantifying intra-image vari-\\nance requires sampling all possible augmentations of all im-\\nages and forward all of them to obtain encodings for all\\ntraining steps. Even if possible, this process is highly ex-\\npensive and also probably unnecessary. Therefore, we re-\\nsort to an approximation with the goal of keeping a refer-\\nence to characterize the encoding variance when changed.\\n\\nTo this end, we simply augment each image in the val-\\nidation set r times and feed them to a pre-trained baseline\\nencoder. The output encodings are then used to compute\\nthe per-image, intra-sample variance, which jointly form a\\ndistribution. All variances across the entire set are then av-\\neraged to a single value v, the reference variance used to\\nmeasure different designs. More details are listed in Sec. 7.\\n\\n4. Case Studies for Source-Target Variance\\n\\nIn this section, we introduce our baseline and perform\\n\\xef\\xac\\x81ve empirical case studies exploring the impact of differ-\\nent designs. For each one of them, we record its corre-\\nsponding variance reference v, and linear-probing accura-\\ncies when placed on encoders with different con\\xef\\xac\\x81gurations\\n\\n2If encodings are uniformly distributed on the unit hypersphere (due to\\n(cid:96)2 normalization), their variance is 1/d where d is the encoding dimension.\\n\\n3\\n\\n\\x0cwithout preset bias. Since our goal is to analyze the behav-\\nior, all models in this section are pre-trained for 100 epochs,\\nwith the generalization toward longer schedules deferred\\nto Sec. 6.1 after we draw the connection between variance\\nchange and encoder preference in Sec. 4.6.\\n\\nBaseline. Our baseline is an improved variant of MoCo\\nv2 [9], which itself is an improved baseline over origi-\\nnal MoCo [19].\\nIt consists of a gradient-updated source\\nencoder fs, a momentum-updated target encoder ft, and\\nan encoding-updated memory bank [40].\\nInspired by\\nSimCLR [8], each MoCo v2 encoder further uses a pro-\\njection head (projector), which is a 2-layer MLP without\\nBatch Normalization (BN) [21] in-between. Our baseline\\nadds an additional fully connected layer (2048-d, with BN)\\nbefore the 2-layer MLP. Inherited from MoCo v1, all BNs\\nin fs are performed per GPU device, and all BNs in ft are\\nshuf\\xef\\xac\\x82ed [19]. All the output encodings z are (cid:96)2 normalized\\nto unit-length vectors before InfoNCE loss [28]. We do not\\nemploy any loss symmetrization [6,18] in this baseline, thus\\none source/target pair only contributes to the loss once.\\n\\nCompared to vanilla MoCo v2 [9], our baseline is gen-\\nerally better in linear probing on ImageNet [13] (detailed\\nin Sec. 7). The table below summarizes the top-1 accuracy\\n(%) using ResNet-50 [20] and the same evaluation protocol:\\n\\nMoCo v2 [9]\\nMoCo v2, ours\\n\\n100 ep\\n64.7\\n65.8\\n\\n200 ep\\n67.9\\n69.0\\n\\n400 ep\\n69.6\\n70.5\\n\\n800 ep\\n70.7\\n71.9\\n\\nThe improvement (\\xe2\\x88\\xbc1 percent) is consistent across different\\nnumber of training epochs. We also notice no degradation\\nin object detection transfer on VOC [16] \\xe2\\x80\\x93 e.g., achieving\\n57.4 mAP at 800 pre-training epochs, same as original [9].\\nThe variance reference for our baseline v0 is 8.5 (\\xc3\\x9710\\xe2\\x88\\x924).\\n\\n4.1. Study 1: MultiCrop Augmentation\\n\\nWe begin our study with an existing design in the litera-\\nture \\xe2\\x80\\x93 multi-crop augmentation (or \\xe2\\x80\\x98MultiCrop\\xe2\\x80\\x99) [6, 7, 27].\\nBesides the two basic views needed for Siamese learning,\\nMultiCrop takes additional views from each image per iter-\\nation. To alleviate the added computation cost, a common\\nstrategy is to have m low-resolution crops (e.g., 96\\xc3\\x9796 [6])\\ninstead of standard-resolution crops (224\\xc3\\x97224) as added\\nviews (illustrated in Fig. 2a top for m=4). As a side effect,\\ninputting small crops can potentially increase the variance\\nfor an encoder due to the size and crop-distribution changes.\\nThis is con\\xef\\xac\\x81rmed in Fig. 2a bottom, where we compare\\nthe variance distribution of MultiCrop to our baseline on\\nthe ImageNet val set. We show the cumulative distribution\\nfunction in solid lines with increasing per-image variances\\nfrom left to right, and the mean variances v and v0 in dotted\\nvertical lines. MultiCrop has signi\\xef\\xac\\x81cantly higher variance\\nthan our baseline: v=38.0 vs. 8.5 (\\xc3\\x9710\\xe2\\x88\\x924).\\n\\nWe plug-in MultiCrop to either the source, the target, or\\nboth encoders (detailed in Appendix D). The table below\\n\\nsummarizes the corresponding top-1 accuracy and change\\n(\\xe2\\x88\\x86) to the baseline in linear probing:\\n\\n+MultiCrop ( \\xe2\\x86\\x91 )\\n\\nneither\\n\\nsource\\n\\naccuracy (%)\\n\\xe2\\x88\\x86 (%)\\n\\n65.8\\n/\\n\\n69.9\\n+4.1\\n\\ntarget\\n\\n57.1\\n-8.7\\n\\nboth\\n\\n61.7\\n-4.1\\n\\nAs a design that increases variance (indicated by \\xe2\\x80\\x98 \\xe2\\x86\\x91 \\xe2\\x80\\x99\\nin table), MultiCrop improves the accuracy substantially\\n(+4.1%) when applied to the source encoder, and hurts\\nwhen applied to the target. When applied to both, the per-\\nformance also degenerates signi\\xef\\xac\\x81cantly (-4.1%), even with\\nmore crops processed per training iteration than to source\\nalone. These results indicate that the source encoder is the\\npreferred place of applying MultiCrop (column shaded in\\ngray ) \\xe2\\x80\\x93 which also matches the common protocols in the\\nliterature when multi-crop augmentation is used [6, 7, 27].\\n\\n4.2. Study 2: ScaleMix Augmentation\\n\\nNext, we introduce and study a different type of augmen-\\ntation called \\xe2\\x80\\x98ScaleMix\\xe2\\x80\\x99, illustrated in Fig. 2b top (more\\ndetails are found in Appendix B). As the name suggests,\\nit generates new views of an image by mixing two views\\nof potentially different scales together via binary masking.\\nThe masking strategy follows CutMix [29], where an entire\\nregion \\xe2\\x80\\x93 denoted by a box with randomly sampled coordi-\\nnates \\xe2\\x80\\x93 is cropped and pasted. Unlike CutMix, ScaleMix\\nonly operates on views from the same image, and the out-\\nput is a single view of standard size (224\\xc3\\x97224). This single\\nview can be regarded as an ef\\xef\\xac\\x81cient approximation of mul-\\ntiple crops in MultiCrop, without the need to process small\\ncrops separately. Like MultiCrop, ScaleMix also introduces\\nextra variance to the encoding space (as shown in Fig. 2b\\nbottom), with a mean variance of v=29.5 (\\xc3\\x9710\\xe2\\x88\\x924).\\n\\nAgain, we apply ScaleMix augmentation to the source,\\nthe target, or both encoders without preset preference. The\\nresults for linear probing are summarized in the table below:\\n\\n+ScaleMix ( \\xe2\\x86\\x91 )\\n\\nneither\\n\\nsource\\n\\naccuracy (%)\\n\\xe2\\x88\\x86 (%)\\n\\n65.8\\n/\\n\\n67.3\\n+1.5\\n\\ntarget\\n\\n52.8\\n-13.0\\n\\nboth\\n\\n64.8\\n-1.0\\n\\nWe observe a similar trend as the MultiCrop case: ScaleMix\\nbene\\xef\\xac\\x81ts source encoders, harms target encoders, and the ef-\\nfect neutralizes when applied to both. This suggests source\\nencoder is again the preferred choice for ScaleMix.\\n\\n4.3. Study 3: General Asymmetric Augmentations\\n\\nThe original v2 recipe is symmetric:\\n\\nMultiCrop and ScaleMix are mostly on geometric trans-\\nformations of images. Next, we study the behavior by vary-\\ning other ingredients in the MoCo v2 augmentation recipe.\\nthe same set of\\naugmentations (e.g., random resized cropping, color jitter-\\ning [40], blurring [8]) is used for both source and target.\\nIn this case study, we add or remove augmentations (be-\\nyond geometric ones), and present two more recipes: one\\n\\n4\\n\\n\\x0cdeemed stronger (\\xe2\\x80\\x98StrongerAug\\xe2\\x80\\x99), and the other weaker\\n(\\xe2\\x80\\x98WeakerAug\\xe2\\x80\\x99) compared to the original one (detailed in\\nAppendix D). Together, they can form general asymmet-\\nric augmentation recipes for source and target. Comply-\\ning with the intuition, we \\xef\\xac\\x81nd StrongerAug has higher vari-\\nance 19.7 (\\xc3\\x9710\\xe2\\x88\\x924), and WeakerAug has lower variance 6.9\\n(\\xc3\\x9710\\xe2\\x88\\x924) w.r.t. to the baseline v0 (shown in Fig. 2c bottom).\\nThe results are split into three tables for clarity. The in-\\n\\n\\xef\\xac\\x82uence of WeakerAug is summarized \\xef\\xac\\x81rst:\\n\\n+WeakerAug ( \\xe2\\x86\\x93 )\\n\\nneither\\n\\nsource\\n\\naccuracy (%)\\n\\xe2\\x88\\x86 (%)\\n\\n65.8\\n/\\n\\n51.0\\n-14.8\\n\\ntarget\\n\\n67.2\\n+1.4\\n\\nboth\\n\\n46.8\\n-19.0\\n\\nInterestingly, the effect of WeakerAug on source/target en-\\ncoder is opposite compared to the previous studies:\\nit\\nhurts source but helps target (referred as \\xe2\\x80\\x98AsymAug\\xe2\\x80\\x99). A\\nsymmetric WeakerAug on both does not work, suggesting\\nthe heavy reliance of Siamese learning on augmentation\\nrecipes [8, 18]. On the StrongerAug side:\\n\\n+StrongerAug ( \\xe2\\x86\\x91 )\\n\\nneither\\n\\nsource\\n\\naccuracy (%)\\n\\xe2\\x88\\x86 (%)\\n\\n65.8\\n/\\n\\n66.7\\n+0.9\\n\\ntarget\\n\\n62.2\\n-3.6\\n\\nboth\\n\\n66.2\\n+0.4\\n\\nIt helps most when used only on source, but harms accu-\\nracy when used only on target. For completeness, we also\\nexperimented changing augmentation strength in opposite\\ndirections for source and target:\\n\\nStronger & Weaker\\n\\nsource \\xe2\\x86\\x91 target \\xe2\\x86\\x93\\n\\nsource \\xe2\\x86\\x93 target \\xe2\\x86\\x91\\n\\naccuracy (%)\\n\\xe2\\x88\\x86 (%)\\n\\n67.2\\n+1.4\\n\\n44.3\\n-21.5\\n\\nCompared to having WeakerAug on target alone (67.2%),\\nfurther adding StrongerAug on source does not bring ex-\\ntra gains. In contrast, stronger augmentations on target and\\nweaker augmentations on source results in the worst perfor-\\nmance in all the cases we have studied.\\n\\n4.4. Study 4: Sync BatchNorm\\n\\nAlthough input data augmentation is a major source of\\nintra-image variance, it is not the only cause of such vari-\\nance within output encodings. One notable source lies in\\nintermediate BN layers [21], a popular normalization tech-\\nnique in modern vision architectures [20]. During training,\\nthe statistics for BN are computed per-batch, which means\\nif other images within the batch are replaced, the output will\\nlikely change even if the current image stays the same. As\\na result, the magnitude of this variance is largely controlled\\nby the batch size: a suf\\xef\\xac\\x81ciently large size can provide nearly\\nstable statistics, whereas for small batches (e.g., below 16)\\nthe estimation is generally less accurate [39]. For MoCo\\nv2, its effective batch size is 32, because the default BN\\nperforms normalization only on the same device (256 im-\\nages/8 GPUs).3 A natural alternative is to employ SyncBN\\n\\n3MoCo v2 inherits MoCo v1 and uses \\xe2\\x80\\x98shuf\\xef\\xac\\x82ed BN\\xe2\\x80\\x99 in ft. It shuf\\xef\\xac\\x82es\\nthe input to avoid cheating but the normalization still happens per-device.\\n\\nthat normalizes over all devices, so the batch size is 256 (il-\\nlustrated in Fig. 2d top for 4 devices). From the zoomed-in\\nvariance plot (Fig. 2d bottom), SyncBN leads to a slight de-\\ncrease in variance from 8.5 to 8.3 (\\xc3\\x9710\\xe2\\x88\\x924) in this case \\xe2\\x80\\x93\\nsuggesting 32 is already suf\\xef\\xac\\x81ciently stable in our baseline.\\nFor ef\\xef\\xac\\x81ciency and generalizability, we replace the single\\nBN in our 3-layer projector with SyncBN.4 As before, we\\ntried different combinations on encoders and the results are:\\n+SyncBN ( \\xe2\\x86\\x93 )\\n\\nneither\\n\\nsource\\n\\ntarget\\n\\nboth\\n\\naccuracy (%)\\n\\xe2\\x88\\x86 (%)\\n\\n65.8\\n/\\n\\n64.7\\n-0.9\\n\\n66.5\\n+0.7\\n\\n66.0\\n+0.2\\n\\nDespite the seemly minor modi\\xef\\xac\\x81cation, SyncBN still leads\\nto a notable improvement when applied to target (referred\\nas \\xe2\\x80\\x98AsymBN\\xe2\\x80\\x99) and degeneration to source. SyncBN on both\\nencoders is at-par with the baseline per-device BNs.\\n\\n4.5. Study 5: Mean Encoding\\n\\nIn this last study we focus on the encoder output. Ac-\\ncording to basic statistics, a direct approach to reduce the\\nvariance of a random variable is to perform i.i.d. sam-\\npling multiple times and take the mean as the new variable.\\nSpeci\\xef\\xac\\x81cally for v, we can reduce it by a factor of \\xe2\\x88\\xbcn if the\\noutput encoding z is averaged from n separate encodings\\n{z1, . . . , zn} (illustrated in Fig. 2e top for n=2).5 These\\nencodings can be simply generated by running the same en-\\ncoder on n augmented views of the same image (detailed\\nin Appendix D). For example, we show v is 4.2 (\\xc3\\x9710\\xe2\\x88\\x924),\\nabout half of v0 when two encodings are averaged in Fig. 2e\\nbottom. We name this design \\xe2\\x80\\x98MeanEnc\\xe2\\x80\\x99 for an encoder.\\n\\nAs discussed in our Sec. 2 (also shown in [10]), increas-\\ning the number of views per training iteration can lead to\\nbetter performance by itself. To minimize this effect, we\\nconduct our main analysis of MeanEnc by \\xef\\xac\\x81xing the total\\nnumber of views to 4 per training iteration. The 4 views are\\nsplit between source (ns) and target (nt) encoders, shown\\nin the \\xef\\xac\\x81rst 3 result columns below:\\n\\n+MeanEnc ( \\xe2\\x86\\x93 )\\n\\naccuracy (%)\\n\\xe2\\x88\\x86 (%)\\n\\nns =1\\nnt =3\\n67.9\\n+2.1\\n\\nns =2\\nnt =2\\n67.1\\n+1.3\\n\\nns =3\\nnt =1\\n59.9\\n-5.9\\n\\nns =1\\nnt =2\\n67.5\\n+1.7\\n\\nWith more views in the target encoder (and simultane-\\nously fewer views in source), we observe a trend for better\\naccuracy. Having 2 views in both encoders still keeps sym-\\nmetry, so its improvement over baseline (65.8%) is an out-\\ncome of more views. For simplicity, we also experimented\\nMeanEnc with 2 views in the target encoder alone (last col-\\numn). The result strikes a better balance between speed and\\naccuracy, so we pick this setting as default for MeanEnc.\\n\\n4Replacing all BNs including ones in ResNet also exhibits the same\\npattern. Replacing BNs in projector only is noticeably faster, and general-\\nizes to other BN-free backbones such as ViT [14].\\n\\n5Here the reduction is approximate because we jointly forward multiple\\nviews which doubles or triples the batch size in BN; and encodings are\\nfurther (cid:96)2 normalized before calculating v.\\n\\n5\\n\\n\\x0cvariance change\\nencoder preference\\n\\nMultiCrop\\n(Sec. 4.1)\\n\\xe2\\x86\\x91\\nsource\\n\\nScaleMix\\n(Sec. 4.2)\\n\\xe2\\x86\\x91\\nsource\\n\\nWeakerAug\\n(Sec. 4.3)\\n\\xe2\\x86\\x93\\ntarget\\n\\nStrongerAug\\n(Sec. 4.3)\\n\\xe2\\x86\\x91\\nsource\\n\\nSyncBN\\n(Sec. 4.4)\\n\\xe2\\x86\\x93\\ntarget\\n\\nMeanEnc\\n(Sec. 4.5)\\n\\xe2\\x86\\x93\\ntarget\\n\\nTable 1. Summary of the 6 designs covered in our case studies. For each design, we list its qualitative change in intra-image variance v,\\nand its preferred encoder. We see a consistent pattern that higher-variance designs prefer source, whilst lower-variance ones prefer target.\\n\\n4.6. Summary of Studies\\n\\nwrite the gradient \\xef\\xac\\x82ow of W as:\\n\\nIn total, we covered 6 variance-oriented designs in the\\n5 case studies described above. Interestingly, none of them\\nachieves best result when designs are symmetrically applied\\nto both (or neither) encoders. Instead, all of them have a\\nsingle preferred encoder in the Siamese network. This phe-\\nnomenon directly supports the importance of asymmetry for\\nSiamese representation learning.\\n\\nMoreover, we observe a consistent pattern: designs that\\nintroduce higher encoding variance generally help when\\nplaced on source encoders, whereas designs that decrease\\nvariance favor target encoders. We summarize the relation\\nbetween: i) change of variance and ii) encoder preference\\nin Tab. 1. This is well-aligned with our insight: the speci\\xef\\xac\\x81c\\nasymmetry of a relatively lower variance in target encod-\\nings than source can bene\\xef\\xac\\x81t Siamese representation learn-\\ning, and not the other way around.\\n\\nFrom the results, we do have to note that such a pattern\\nholds within a reasonable range of v, and more extreme\\nasymmetry does not always lead to better performance (e.g.,\\nwhen further increasing source augmentation strength while\\nhaving WeakerAug in target). Moreover, asymmetry is usu-\\nally not the only factor in play for self-supervised frame-\\nworks; other factors (e.g. the number of views in MeanEnc)\\ncan also in\\xef\\xac\\x82uence the \\xef\\xac\\x81nal outcome of our pipelines.\\n\\n5. Theoretical Analysis for Variance\\n\\nHere we aim to provide a preliminary theoretical analysis\\nfor MoCo following [33, 34] (More details in Appendix C).\\nConsider the following simpli\\xef\\xac\\x81ed InfoNCE objective:6\\n\\nL = \\xe2\\x88\\x92\\n\\n1\\nN\\n\\nN\\n(cid:88)\\n\\ni=1\\n\\nlog\\n\\n(cid:80)\\n\\nexp(Sii(cid:48)/\\xcf\\x84 )\\nj(cid:54)=i exp(Sij(cid:48)/\\xcf\\x84 )\\n\\n,\\n\\n(1)\\n\\ni z(cid:48)\\n\\nwhere N is batch size, \\xcf\\x84 is temperature, Sii(cid:48)=z(cid:62)\\ni and\\nSij(cid:48)=z(cid:62)\\nj are pairwise similarities between source encod-\\nings zi and targets z(cid:48)\\ni (target weights and encodings all\\ncome with prime (cid:48)). For MoCo, gradients are only back-\\npropagated through the source zi, but not z(cid:48)\\n\\ni z(cid:48)\\n\\ni or z(cid:48)\\nj.\\n\\nNow, let\\xe2\\x80\\x99s take the last linear layer immediately before z\\nas an example for analysis. Let f be the input features of this\\nlayer, W be its weight matrix (so z=W f ), and denotes co-\\nef\\xef\\xac\\x81cients \\xce\\xb1ij(cid:48)= exp(Sij(cid:48)/\\xcf\\x84 )/ (cid:80)\\nk(cid:54)=i exp(Sik(cid:48)/\\xcf\\x84 ), we can\\n\\n6We make two simpli\\xef\\xac\\x81cations to InfoNCE [28] by ignoring (cid:96)2 normal-\\n\\nization and the positive term exp(Sii(cid:48) /\\xcf\\x84 ) in the denominator [42].\\n\\ndL\\ndW\\n\\n= W (cid:48) 1\\n\\xcf\\x84 N\\n\\nN\\n(cid:88)\\n\\n(cid:88)\\n\\ni=1\\n\\nj(cid:54)=i\\n\\n\\xce\\xb1ij(cid:48)(f (cid:48)\\n\\nj \\xe2\\x88\\x92 f (cid:48)\\n\\ni )f (cid:62)\\ni .\\n\\n(2)\\n\\nTo study the behavior of gradients especially w.r.t. our\\nvariance of interest, we can model intra-image variance as\\nan additive noise in f (and f (cid:48)) that affects training. Specif-\\nically, let \\xcb\\x9cf be the feature corresponding to the original im-\\nage, we can assume:\\n\\ni]=\\xce\\xa3(cid:48).\\n\\ni =\\xcb\\x9cfi+e(cid:48)\\n\\ni]=\\xc2\\xafe(cid:48) and V[e(cid:48)\\n\\n\\xe2\\x80\\xa2 Source features fi=\\xcb\\x9cfi+ei, with E[ei]=\\xc2\\xafe and V[ei]=\\xce\\xa3;\\ni, with E[e(cid:48)\\n\\xe2\\x80\\xa2 Target side f (cid:48)\\nE[\\xc2\\xb7] computes expectation and V[\\xc2\\xb7] outputs variance.\\nNote that \\xcb\\x9cfi and \\xcb\\x9cfj are from different images, while ei, e(cid:48)\\ni\\nand e(cid:48)\\nj model intra-sample variance that comes from mul-\\ntiple sources, e.g., input augmentations, BNs with different\\nbatch sizes (Sec. 4.4), etc. Due to the independent augmen-\\ntation process, these noises are modeled as independent of\\neach other.\\n\\nUnder such setting, we can arrive at the following result\\n(detailed derivations in Appendix C) to better understand\\nour observation from a theoretical perspective:\\n\\nHigher variance on the target side is not necessary and\\ncan be less stable. With higher variance on the target side\\n(i.e., \\xce\\xa3(cid:48) has larger eigenvalues), the variance of the gradi-\\nent w.r.t. W , V[dL/dW ], will become larger without affect-\\ning its expectation E[dL/dW ]. Intuitively, this asymmetry\\ncomes from an asymmetric structure in Eq. (2): there is a\\nsubtraction term (f (cid:48)\\ni ) on the target side, but not on the\\nsource side (fi). To make the training dynamics more sta-\\nble, maintaining a relative lower variance on the target side\\nthan source is preferred.\\n\\nj\\xe2\\x88\\x92f (cid:48)\\n\\n6. Generalization Studies and Results\\n\\nThe keyword of this section is generalization, for which\\nwe study our insight for Siamese learning under various\\nconditions. Speci\\xef\\xac\\x81cally for MoCo v2, we study the behav-\\nior of asymmetric designs by training with longer sched-\\nules, and by composing multiple designs together. As a by-\\nproduct, our \\xef\\xac\\x81nal model achieves state-of-the-art on Ima-\\ngeNet, and performs well beyond when transferred to other\\ndatasets. Besides MoCo v2, we seek generalizations across\\nmore frameworks and backbones and \\xef\\xac\\x81nd it also holds well.\\nUnless otherwise speci\\xef\\xac\\x81ed, all the evaluations are top-1 lin-\\near probing accuracy on ImageNet [13].\\n\\n6\\n\\n\\x0cFigure 3. Generalization to longer pre-training. Here y-axis is accuracy (%) and x-axis is number of epochs (log-scale). Asymmetric\\ndesigns consistently outperform the baseline. MultiCrop as the single strongest one reaches 73.7% at 800-ep without loss symmetrization.\\n\\n(%)\\n\\nMoCo v3 [11]\\nasym., 2\\xc3\\x97 / \\xe2\\x88\\x86\\nSimCLR [8]\\nasym., 2\\xc3\\x97 / \\xe2\\x88\\x86\\nBYOL [18]\\nasym., 2\\xc3\\x97 / \\xe2\\x88\\x86\\nSimSiam [10]\\nasym., 2\\xc3\\x97 / \\xe2\\x88\\x86\\nBarlow Twins [44]\\nasym. / \\xe2\\x88\\x86\\n\\nbaseline\\n69.9\\n69.7\\n65.0\\n65.0\\n69.5\\n69.0\\n67.8\\n67.4\\n66.8\\n66.4\\n\\nScaleMix AsymBN MeanEnc\\n70.1\\n+0.4\\n65.8\\n+0.8\\n69.9\\n+0.9\\n68.0\\n+0.6\\n66.6\\n+0.2\\n\\n70.6\\n+0.9\\n66.4\\n+1.4\\n69.7\\n+0.7\\n68.0\\n+0.6\\n67.1\\n+0.7\\n\\n70.7\\n+1.0\\n66.3\\n+1.3\\n70.4\\n+1.4\\n68.7\\n+1.3\\n67.3\\n+0.9\\n\\nTable 2. Generalization to more frameworks. We cover 5 of them\\nand convert each to and asymmetric one \\xef\\xac\\x81rst. In the second col-\\numn, we show similar results using our asymmetric versions com-\\npared to the original ones at 100-ep (in gray), optionally with 2\\xc3\\x97\\ntraining schedules.7 On top of these, we \\xef\\xac\\x81nd asymmetric designs\\nhelp learning across the board: third to \\xef\\xac\\x81fth columns list accura-\\ncies and improvements over the asymmetric baseline.\\n\\n6.1. Longer Training\\n\\nThe \\xef\\xac\\x81rst generalization is to longer training schedules.\\nMost Siamese learning frameworks [6, 8, 18], including\\nour baseline MoCo v2, produce substantially better results\\nin linear probing with more training epochs. Meanwhile,\\nlower variance in target \\xe2\\x80\\x93 in the extreme a \\xef\\xac\\x81xed target per\\nimage, could result in faster convergence closer to super-\\nvised learning where longer training is not as helpful [20].\\nWe run our baseline with the \\xef\\xac\\x81ve asymmetric setups studied\\nin Sec. 4 for 200, 400 and 800 epochs to check the behav-\\niors, and put the trends in Fig. 3. Overall, all the asymmetric\\nmodels outperform the baseline across different epoch num-\\nbers. The maintained gap suggests the gain from asymmetry\\ncannot be simply explained away by faster convergence.\\n\\n6.2. More Frameworks\\n\\nNext we examine the generalization to other frameworks.\\nRoughly ranked by its similarity to our baseline MoCo v2\\nfrom closest to furthest, they are: i) MoCo v3 [11], where\\nthe memory bank is replaced by large batch sizes [43];\\nii) SimCLR [8], where no momentum encoder is needed;\\niii) BYOL [18], where the contrastive formulation is chal-\\nlenged by learning only on comparing positive pairs; iv)\\nSimSiam [10], where neither momentum encoder nor nega-\\ntive pairs are required; and v) Barlow Twins [44], where a\\nfully symmetric pipeline for Siamese learning is discovered.\\nNote that we only outlined major differences above and\\nmore subtleties (including detailed setup for each frame-\\nwork in this paper) are found in Appendix D.\\n\\n(%)\\nMoCo v3, ViT [11]\\nasym., 2\\xc3\\x97 / \\xe2\\x88\\x86\\n\\nbaseline\\n69.1\\n68.7\\n\\nScaleMix AsymBN MeanEnc\\n69.4\\n+0.7\\n\\n69.4\\n+0.7\\n\\n69.1\\n+0.4\\n\\nTable 3. Generalization to ViT [14], a new architecture gaining\\npopularity in vision and is recently studied in MoCo v3 [11]. The\\nprocedure and table format follow Tab. 2.\\n\\nFor ease of applying asymmetric designs to these frame-\\nworks, we \\xef\\xac\\x81rst convert their symmetrized components to an\\nasymmetric form following our source-target formulation.\\nA popular one is loss symmetrization, used by all except\\nBarlow Twins. We remove it by only forwarding a pair of\\nviews through the network once (instead of twice) per it-\\neration. Intuitively, training 2\\xc3\\x97 as long can roughly com-\\npensate for the symmetrized loss with fair amount of com-\\npute, as discussed in Sec. 2 and analyzed in [10]. More-\\nover, methods without momentum encoders [8,10,44] reuse\\nsource encoders for targets.\\nIn such cases, we explic-\\nitly maintain a target encoder by using an online clone of\\nthe source one, and stopping gradients from \\xef\\xac\\x82owing into\\nthe branch \\xe2\\x80\\x93 a choice deviated from SimCLR and Barlow\\nTwins [8, 44]. We show in Tab. 2 (second column) that our\\nasymmetric versions work similarly in accuracy compared\\nto the original ones, despite the above modi\\xef\\xac\\x81cations.7\\n\\nWe pick ScaleMix, AsymBN and MeanEnc as three rep-\\nresentative designs which range from encoder inputs to\\noutputs. MultiCrop is relatively well studied in the liter-\\nature [6, 7] and we \\xef\\xac\\x81nd it non-trivial to train MultiCrop\\nwith large batch sizes [8, 11, 18, 44]. More recent frame-\\nworks [11,18,44] already employ stronger asymmetric aug-\\nmentation recipes [18] like AsymAug. Thus we did not in-\\nclude them in our comparisons listed in Tab. 2 (last three\\ncolumns). Our asymmetric source-target designs generalize\\nwell beyond MoCo v2, showing consistent improvements\\nacross the board with same number of pre-training epochs.\\n\\n6.3. ViT Backbone\\n\\nWith MoCo v3, we also benchmarked a newly proposed\\nbackbone: ViT [14]. We follow the same procedure by \\xef\\xac\\x81rst\\nbuilding an asymmetric baseline and then applying different\\ndesigns (detailed in Appendix D). Again, we \\xef\\xac\\x81nd asymme-\\ntry works well (Tab. 3). The only notable difference is the\\nreduced gap for ScaleMix, which is likely related to patches\\nfed for ViT not aligned with ScaleMix masks [22].\\n\\n7We keep all the optimization hyper-parameters the same when running\\nthe asymmetric version. The results can be further improved when e.g.\\nlearning rate is adjusted following the batch size change [17].\\n\\n7\\n\\n100200400800657065.869.969.071.870.572.871.973.7MultiCropbaseline100200400800657065.867.369.070.070.572.071.973.2ScaleMixbaseline100200400800657065.867.269.069.770.571.371.972.5AsymAugbaseline100200400800657065.866.469.069.370.571.271.972.3AsymBNbaseline100200400800657065.867.569.070.270.571.871.972.6MeanEncbaseline\\x0cSupervised\\nSimCLR [8]\\nBYOL [18]\\nNNCLR [15]\\nOurs, 1600-ep\\n\\nFood-101 CIFAR-10 CIFAR-100 Birdsnap SUN-397\\n78.3\\n71.6\\n78.4\\n79.0\\n77.8\\n\\n93.6\\n90.6\\n91.3\\n93.7\\n92.8\\n\\n53.7\\n37.4\\n57.2\\n61.4\\n58.5\\n\\n61.9\\n58.8\\n62.2\\n62.5\\n67.8\\n\\n72.3\\n68.4\\n75.3\\n76.7\\n79.4\\n\\nCars\\n66.7\\n50.3\\n67.8\\n67.1\\n69.7\\n\\nAircraft VOC-07 DTD\\n74.9\\n87.5\\n74.5\\n85.5\\n75.5\\n82.5\\n75.5\\n83.0\\n80.2\\n93.8\\n\\n61.0\\n50.3\\n60.6\\n64.1\\n59.3\\n\\nPets\\n91.5\\n83.6\\n90.4\\n91.8\\n87.2\\n\\nCaltech-101 Flowers\\n\\n94.5\\n90.3\\n94.2\\n91.3\\n93.1\\n\\n94.7\\n91.2\\n96.1\\n95.1\\n92.5\\n\\nTable 4. Generalization by transferring our model to 12 different downstream datasets with linear probing. We follow the protocol\\nof [15, 18] and report results on the test set. For VOC-07, we cite the improved numbers from [44] for fair comparisons. Our 1600-ep\\nmodel achieves best results on 5 out of 12, while being less competitive on tasks with iconic images (such as CIFAR [23] and Aircraft [26]).\\n\\n6.4. Design Compositions\\n\\nAs another aspect for generalization, we compose mul-\\ntiple asymmetric designs together and check their joint ef-\\nfect on representation quality. To this end, we fall back to\\nour MoCo v2 baseline (100-ep) and start from our strongest\\nsingle asymmetric design, MultiCrop. When pairing it with\\nother two input designs (ScaleMix an AsymAug), we \\xef\\xac\\x81nd\\ntheir added value has mostly diminished so we did not in-\\nclude them. On the target side, we \\xef\\xac\\x81rst enabled SyncBN,\\nand then enabled MeanEnc (nt =2) to reduce variance, and\\nboth designs further improved performance:\\n+MultiCrop +MultiCrop\\n+AsymBN\\n\\ncompositions\\n\\nnone\\n\\n65.8\\n-\\n\\n70.4\\n+4.6\\n\\naccuracy (%)\\n69.9\\n\\xe2\\x88\\x86 (%)\\n+4.1\\nWhile our exploration on this front is preliminary and im-\\nprovement is not guaranteed (as discussed in Sec. 4.6), it in-\\ndicates different asymmetric designs can be compositional.\\nFinally, we pre-train our best composition (shaded col-\\numn above) for 1600 epochs to check its limit. We arrive at\\n75.6% on ImageNet linear probing (more details in Sec. 7).\\nThis puts us in the state-of-the-art cohort [37, 41, 47] with\\nsingle-node training and no other bells or whistles.\\n\\n+MultiCrop\\n+AsymBN\\n+MeanEnc\\n71.3\\n+5.5\\n\\n6.5. Transfer Learning\\n\\nIn Tab. 4, we show transfer learning results of our \\xef\\xac\\x81nal\\nImageNet 1600-ep model to 12 standard downstream classi-\\n\\xef\\xac\\x81cation tasks for linear probing [8,15,18]. For each dataset,\\nwe search the learning rate on the validation set and report\\nresults on the test set, following the protocol of [15,18] (see\\nAppendix D). Our model performs competitively against\\nthe most recent NNCLR [15]), achieving best on 5 tasks but\\nlags behind on ones with iconic images. We hypothesis it\\xe2\\x80\\x99s\\ndue to MultiCrop which used local small crops. We further\\ntransferred to Places-205 [46], which focuses on scene-level\\nunderstanding. We \\xef\\xac\\x81nd our model indeed achieves state-of-\\nthe-art (56.8%), slightly better than SwAV [6] which also\\nused MultiCrop. These results verify our learned represen-\\ntation is effective beyond ImageNet.\\n\\n7. Implementation Details\\n\\nWe list the most important implementation details for our\\n\\npaper below. Other subtleties are found in Appendix D.\\n\\nVariance reference. We use ImageNet val set (50k images\\nin total), r=32 views, and the 800-ep pre-trained baseline\\nsource encoder for variance calculation.8 Encodings are\\n(cid:96)2 normalized. To fully mimic the pre-training setting, we\\nuse online per-batch statistics for BN, not recorded moving-\\naverage ones from the training set.\\n\\nPre-training. By default, we adopt the same MoCo v2\\nsetup (e.g., augmentation recipe, SGD optimizer etc.) for\\nexperiments on our baseline. A half-cycle cosine learning\\nrate decay schedule [25] is used given the number of pre-\\ntraining epochs. Mixed-precision is enabled for ef\\xef\\xac\\x81ciency.\\n\\nLinear probing. Linear probing freezes backbone after\\npre-training, and only trains a linear classi\\xef\\xac\\x81er on top of\\nthe global image features to test the representation qual-\\nity. By default on ImageNet, we use LARS [43] opti-\\nmizer with batch size 4096, initial learning rate lr=1.6 (lin-\\nearly scaled [17]), weight decay 0 and train the classi\\xef\\xac\\x81er\\nfor 90 epochs with a half-cycle cosine schedule following\\nSimSiam [10]. We choose LARS over SGD as the former\\nshows better adaptation for explorations, without the need\\nto search hyper-parameters (e.g. lr) extensively for good\\nperformance. For our \\xef\\xac\\x81nal model, we switched back to\\nSGD optimizer following MoCo [20], with an initial learn-\\ning rate of 120 and batch size of 256.\\n\\n8. Conclusion\\n\\nThrough systematic studies, we have revealed an inter-\\nesting correlation between the asymmetry of source-target\\nvariance and the representation quality for Siamese learn-\\ning methods. While such a correlation is conditioned on\\nother factors and certainly not universal, we \\xef\\xac\\x81nd as guide-\\nline it is generally applicable to various training sched-\\nules, frameworks and backbones. Composing asymmet-\\nric designs helps us achieve state-of-the-art with MoCo v2,\\nand the learned representation transfers well to other down-\\nstream classi\\xef\\xac\\x81cation tasks. We hope our work will inspire\\nmore research exploiting the importance of asymmetry for\\nSiamese learning, e.g. for object detection transfer [19] or\\nspeeding up model convergence for carbon neutral training.\\n\\n8A potential concern is the variance reference being biased by out-of-\\ndistribution views, since the baseline model has not seen certain data (e.g.,\\nsmall crops) during training. To address this, we also experimented with a\\nmodel pre-trained with all the asymmetric designs. The trends still hold.\\n\\n8\\n\\n\\x0cAcknowledgements. XC would like to thank Kaiming He\\non helpful discussions through this project. XW would like\\nto thank Yutong Bai on helpful discussions through this\\nproject.\\n\\nA. Cross-Image Variance\\n\\nIn this section, we show evidence with our MoCo v2\\nbaseline that cross-image variance quickly converges to a\\nconstant that only depends on the encoding dimension d.\\nThis is through a monitor installed on the output encodings\\nduring training. Speci\\xef\\xac\\x81cally, for each iteration, we compute\\nthe variance of the output (cid:96)2-normalized vectors from the\\nsource encoder along the batch axis and average them over\\nthe channel axis. Since each training batch contains differ-\\nent images rather than different views of the same image,\\nthe resulting value re\\xef\\xac\\x82ects the cross-image variance. Three\\nencoding dimensions, d\\xe2\\x88\\x88{64, 128, 256} are experimented,\\nand their variances during the 100-epoch training process\\nare separately recorded in Fig. 4.\\n\\nFrom the plot, we \\xef\\xac\\x81nd that all the variances quickly and\\nseparately converge to 1/d. For example, when the encod-\\ning dimension d is 128 (default), the variance converges to\\n1/128; when d is 64, it converges to 1/64. The same obser-\\nvations are made regardless of other designs for the encoder\\n(e.g., MultiCrop or SyncBN). We believe it is a natural out-\\ncome of Siamese representation learning which generally\\nencourages uniformity [10, 35] \\xe2\\x80\\x93 encodings of different im-\\nages distribute uniformly on the unit hypersphere. There-\\nfore, cross-image variance is deemed not an ideal reference\\nto distinguish designs. Instead, we use intra-image variance\\nwhich has a much smaller magnitude (\\xc3\\x9710\\xe2\\x88\\x924), but carries\\nuseful signals to tell different designs apart (see Fig. 2).\\n\\nB. ScaleMix\\n\\nThe goal of ScaleMix is to generate a new view vs by\\ncombining two random sampled views of the same size\\n(height H and width W ): v1 and v2. The generated new\\nview is treated as a normal view of the input image x and\\nused for Siamese learning. Speci\\xef\\xac\\x81cally, following the pro-\\ntocol of [29], we de\\xef\\xac\\x81ne the combining operation as:\\n\\nvs = M \\xc2\\xb7 v1 + (1 \\xe2\\x88\\x92 M ) \\xc2\\xb7 v2,\\n\\nwhere M \\xe2\\x88\\x88{0, 1}H\\xc3\\x97W denotes a binary mask indicating\\nwhere to use pixels from which view, and \\xc2\\xb7 is an element-\\nwise multiplication. Note that different from other mixing\\noperations [29,45], we do not mix outputs as both views are\\nfrom the same image.\\n\\nThe binary values in M are determined by bounding box\\ncoordinates B= (x, y, w, h), where (x, y) is the box center,\\nand (w, h) is the box size. Given B, its corresponding re-\\ngion in M is set to all 0 and otherwise all 1. Intuitively,\\n\\nFigure 4. Cross-image variance tracked during the 100-epoch\\ntraining process for our MoCo v2 baseline, with three encoding\\ndimension options: d\\xe2\\x88\\x88{64, 128, 256}. All of them quickly con-\\nverge to 1/d (dotted lines).\\n\\nthis means the region B in v1 is removed and \\xef\\xac\\x81lled with the\\npatch cropped from B of v2.\\n\\nThe box coordinates B are randomly sampled. We keep\\nthe aspect ratio of B \\xef\\xac\\x81xed and the same as the input views,\\nand only vary the size of the box according to a random vari-\\nable \\xce\\xbb uniformly drawn from (0, 1): w=W\\n\\xce\\xbb.\\nBox centers (x, y) are again uniformly sampled.\\n\\n\\xce\\xbb, h=H\\n\\n\\xe2\\x88\\x9a\\n\\n\\xe2\\x88\\x9a\\n\\nC. Detailed Theoretical Analysis\\n\\nGiven the outputs: z from the source encoder and z(cid:48) from\\nthe target encoder (prime (cid:48) indicates target-related), the In-\\nfoNCE [28] loss used by MoCo is de\\xef\\xac\\x81ned as:\\n\\nL := \\xe2\\x88\\x92\\n\\nlog\\n\\n1\\nN\\n\\nN\\n(cid:88)\\n\\ni=1\\n\\nexp(Sii(cid:48)/\\xcf\\x84 )\\n\\n(cid:15) exp(Sii(cid:48)/\\xcf\\x84 ) + (cid:80)\\n\\nj(cid:54)=i exp(Sij(cid:48)/\\xcf\\x84 )\\n\\n,\\n\\ni z(cid:48)\\n\\n(3)\\nwhere N is batch size, \\xcf\\x84 is temperature, Sii(cid:48)=z(cid:62)\\ni and\\nSij(cid:48)=z(cid:62)\\nj are pairwise similarities between source and tar-\\nget encodings. We additionally introduce the parameter (cid:15)\\nthat controls the weight for the positive term in the denom-\\ninator, where for standard loss (cid:15)=1.\\n\\ni z(cid:48)\\n\\nFor MoCo, only the source encoder receives gradient,\\n\\nand we take derivatives only for zi:\\n\\n\\xe2\\x88\\x82L\\n\\xe2\\x88\\x82zi\\n\\n=\\n\\n1\\n\\xcf\\x84\\n\\n(cid:88)\\n\\nj(cid:54)=i\\n\\n\\xce\\xb1ii(cid:48)j(cid:48)(z(cid:48)\\n\\nj \\xe2\\x88\\x92 z(cid:48)\\n\\ni),\\n\\nwhere\\n\\n\\xce\\xb1ii(cid:48)j(cid:48) =\\n\\nexp(Sij(cid:48)/\\xcf\\x84 \\xe2\\x88\\x92 Sii(cid:48)/\\xcf\\x84 )\\nk(cid:54)=i exp(Sik(cid:48)/\\xcf\\x84 \\xe2\\x88\\x92 Sii(cid:48)/\\xcf\\x84 )\\n\\n.\\n\\n(cid:15) + (cid:80)\\n\\nFor the simpli\\xef\\xac\\x81ed case where (cid:15)=0 [42], we can have:\\n\\n\\xce\\xb1ii(cid:48)j(cid:48) = \\xce\\xb1ij(cid:48) =\\n\\nexp(Sij(cid:48)/\\xcf\\x84 )\\nk(cid:54)=i exp(Sik(cid:48)/\\xcf\\x84 )\\n\\n,\\n\\n(cid:80)\\n\\nwhich is independent of target encoding z(cid:48)\\ni.\\n\\nNow, let\\xe2\\x80\\x99s consider the last linear layer immediately be-\\nfore z as an example for analysis. Let f be the input features\\n\\n(4)\\n\\n(5)\\n\\n(6)\\n\\n9\\n\\n020406080100Epoch12561128164Varianced=64d=128d=256\\x0cof this layer, W be its weight matrix (so z=W f and we do\\nnot consider (cid:96)2 normalization applied to z). In this case, we\\ncan write down the dynamics of the source weight W based\\non the gradient descent rule:\\n\\n\\xcb\\x99W := \\xe2\\x88\\x92\\n\\n= \\xe2\\x88\\x92\\n\\n1\\nN\\n\\nN\\n(cid:88)\\n\\ni=1\\n\\n\\xe2\\x88\\x82L\\n\\xe2\\x88\\x82zi\\n\\nf (cid:62)\\ni\\n\\n\\xe2\\x88\\x82L\\n\\xe2\\x88\\x82W\\n\\n1\\n\\xcf\\x84 N\\n\\nN\\n(cid:88)\\n\\n(cid:88)\\n\\ni=1\\n\\nj(cid:54)=i\\n\\n= \\xe2\\x88\\x92\\n\\n\\xce\\xb1ij(cid:48)(z(cid:48)\\n\\nj \\xe2\\x88\\x92 z(cid:48)\\n\\ni)f (cid:62)\\ni ,\\n\\n(7)\\n\\n(8)\\n\\nwhere \\xcb\\x99W is a simpli\\xef\\xac\\x81ed notion of the change to w.r.t. W fol-\\nlowing gradient decent. Since both z(cid:48)\\ni come from the\\ni=W (cid:48)f (cid:48)\\nj and z(cid:48)\\ntarget encoder weight W (cid:48), we have z(cid:48)\\ni\\nand thus:\\n\\nj and z(cid:48)\\nj=W (cid:48)f (cid:48)\\n\\n\\xcb\\x99W = \\xe2\\x88\\x92W (cid:48) 1\\n\\xcf\\x84 N\\n\\nN\\n(cid:88)\\n\\n(cid:88)\\n\\ni=1\\n\\nj(cid:54)=i\\n\\n\\xce\\xb1ij(cid:48)(f (cid:48)\\n\\nj \\xe2\\x88\\x92 f (cid:48)\\n\\ni )f (cid:62)\\ni\\n\\n(9)\\n\\nWe de\\xef\\xac\\x81ne \\xc2\\xaff :=E[f ] to be the mean of the input feature and\\n\\xce\\xa3f :=V[f ] to be the co-variance matrix of the input feature f ,\\nwhere E[\\xc2\\xb7] computes expectation and V[\\xc2\\xb7] outputs variance.\\nThese two quantities will be used later.\\n\\nNow let\\xe2\\x80\\x99s consider how intra-image variance in both tar-\\nget and source sides affect training. To reach a clear con-\\nclusion, we now make two assumptions.\\n\\nAssumption 1: additive noise. We can model the intra-\\nimage variance as additive noise. Speci\\xef\\xac\\x81cally, let \\xcb\\x9cf be the\\nfeature corresponding to the original image, we can assume:\\n\\xe2\\x80\\xa2 fi=\\xcb\\x9cfi+ei. That is, the input feature of the last layer fi\\nreceives source noise ei with E[ei]=\\xc2\\xafe and V[ei]=\\xce\\xa3;\\nj=\\xcb\\x9cfj+e(cid:48)\\nj receives target\\nj]=\\xc2\\xafe(cid:48) and V[e(cid:48)\\nj]=\\xce\\xa3(cid:48). Note that for\\nnoise e(cid:48)\\nthe feature of a different image f (cid:48)\\ni , it also undergoes\\nthe same process on the target side and thus we have\\ni =\\xcb\\x9cfi+e(cid:48)\\nf (cid:48)\\ni.\\n\\nj. That is, the input feature f (cid:48)\\nj with E[e(cid:48)\\n\\n\\xe2\\x80\\xa2 f (cid:48)\\n\\nNote that the noise is not necessarily zero mean-ed. Since\\nthe augmentations of fi and f (cid:48)\\ni are\\nindependent of each other: P(ei, e(cid:48)\\ni). Same for\\nei and ej where i(cid:54)=j.\\n\\ni are independent, ei and e(cid:48)\\ni)=P(ei)P(e(cid:48)\\n\\nAssumption 2: all \\xce\\xb1ij(cid:48) are constant and independent\\nof f . Alternatively, if we consider the quadratic loss (i.e.,\\nLq= (cid:80)\\nj(cid:54)=i (Sij(cid:48)\\xe2\\x88\\x92Sii(cid:48))), then all \\xce\\xb1ij(cid:48) are constant and this\\nassumption holds true. For InfoNCE this may not hold, and\\nwe leverage this assumption for simplicity of derivations.\\n\\nUnder these two assumptions, we now compute Ef [ \\xcb\\x99W ],\\nthe expectation of the weight gradient over input feature f\\nof the last layer. This gets rid of inter-image variance, and\\nfocuses on intra-image variance only:\\n\\nEf [ \\xcb\\x99W ] =\\n\\nW (cid:48)(\\xce\\xa3f \\xe2\\x88\\x92 R).\\n\\n(10)\\n\\n1\\n\\xcf\\x84\\n\\nHere the residual term R is as follows:\\n\\nR := \\xe2\\x88\\x92\\n\\nN\\n(cid:88)\\n\\n1\\nN\\n\\ni(\\xc2\\xaff + ei)(cid:62),\\n\\xcb\\x86e(cid:48)\\n\\ni=1\\nj\\xe2\\x88\\x92e(cid:48)\\nwhere \\xcb\\x86e(cid:48)\\ni\\nj and e(cid:48)\\nwhich is a weighted sum of e(cid:48)\\ni.\\n\\nj(cid:54)=i \\xce\\xb1ij(cid:48)e(cid:48)\\n\\ni:= (cid:80)\\n\\nis also a random variable\\n\\nFrom the de\\xef\\xac\\x81nition (Eq. (5)), we have (cid:80)\\n\\nj(cid:54)=i \\xce\\xb1ij(cid:48)=1. e(cid:48)\\nj\\ni are independent. Therefore we can compute the\\n\\nand e(cid:48)\\nmean and variance of \\xcb\\x86e(cid:48)\\n\\ni as:\\n\\nE[\\xcb\\x86e(cid:48)\\ni := V[\\xcb\\x86e(cid:48)\\n\\ni] = 0,\\ni] = (1 +\\n\\n\\xcb\\x86\\xce\\xa3(cid:48)\\n\\nij(cid:48))\\xce\\xa3(cid:48).\\n\\xce\\xb12\\n\\n(cid:88)\\n\\nj(cid:54)=i\\n\\n(11)\\n\\n(12)\\n\\n(13)\\n\\nNow for the residual term R, we also have Ee[R]=0.\\n\\nTherefore, the full expectation for \\xcb\\x99W can be written as:\\n\\nE[ \\xcb\\x99W ] := Ee[Ef [ \\xcb\\x99W ]] =\\n\\nW (cid:48)\\xce\\xa3f .\\n\\n(14)\\n\\n1\\n\\xcf\\x84\\n\\nThis means the source weight will grow along the direc-\\ntion that maximizes the distance between different images.\\nMore precisely, it grows along the eigenvector that corre-\\nsponds to the maximal eigenvalue of \\xce\\xa3f .\\n\\nNow we can check the in\\xef\\xac\\x82uence of intra-image variance\\nfrom source and target encoders. The in\\xef\\xac\\x82uence can be char-\\nacterized by the term Ve[Ef [ \\xcb\\x99W ]]. For simplicity, we can\\ncompute Ve[Ef [tr(R)]] \\xe2\\x80\\x93 i.e. the variance on the trace of R,\\nsince \\xce\\xa3f remains constant for intra-image variance.\\n\\nLeveraging the independence of {\\xcb\\x86e(cid:48)\\n\\ni, ei} among different\\n\\nimages, we can arrive at:\\n\\nVe[Ef [tr(R)]] = tr\\n\\n(cid:104) \\xcb\\x86\\xce\\xa3(cid:48)(\\xc2\\xaff\\xc2\\xaff (cid:62) + \\xc2\\xafe\\xc2\\xafe(cid:62) + \\xce\\xa3)\\n\\n(cid:105)\\n\\n,\\n\\n(15)\\n\\nwhere \\xcb\\x86\\xce\\xa3(cid:48):= 1\\nN\\n\\n(cid:80)N\\n\\ni=1\\n\\n\\xcb\\x86\\xce\\xa3(cid:48)\\n\\ni is the mean of all variances of \\xcb\\x86e(cid:48)\\ni.\\n\\nFrom Eq. (15) we can notice that: i) if there is large mag-\\nnitude of source feature mean \\xc2\\xaff and/or source noise mean \\xc2\\xafe,\\nthen the variance will be large; ii) this effect will be magni-\\n\\xef\\xac\\x81ed with more target-side variance (i.e., larger eigenvalues\\nof \\xce\\xa3(cid:48) and thus \\xcb\\x86\\xce\\xa3(cid:48)), without affecting the average gradient;\\niii) large magnitude of feature mean and/or noise mean on\\nthe target side does not in\\xef\\xac\\x82uence the variance. This asym-\\nmetry between source and target suggests that the training\\nprocedure an be negatively affected if the target variance is\\ntoo large, coupled by \\xc2\\xaff\\xc2\\xaff (cid:62) and \\xc2\\xafe\\xc2\\xafe(cid:62) in Eq. (15).\\n\\nj \\xe2\\x88\\x92 f (cid:48)\\n\\nThe intuition why there is such an asymmetry is the\\nfollowing: in Eq. (9), while the target side has a subtrac-\\ntion f (cid:48)\\ni which cancels out the mean, the source side fi\\ndoesn\\xe2\\x80\\x99t. This leads to the mean values being kept on the\\nsource side which couples with the target variance, whereas\\nno mean values from the target side are kept.\\n\\nTherefore, we can infer that higher variance on the target\\nside is less necessary compared to the source side \\xe2\\x80\\x93 it will\\nincur more instability during training without affecting the\\nmean of gradients.\\n\\n10\\n\\n\\x0cD. More Implementation Details\\n\\nMultiCrop. Our MultiCrop recipe largely follows the work\\nof SwAV [6]. Speci\\xef\\xac\\x81cally, 224-sized crops are sampled\\nwith a scale range of (0.14, 1), and 96-sized small crops are\\nsampled from (0.05, 0.14). We use m=6 small crops by\\ndefault, and each is forwarded separately with the encoder.\\nWhen applied to one encoder, all (1+6)=7 encodings are\\ncompared against the single encoding from the other side;\\nwhen applied jointly, (7\\xc3\\x972)=14 encodings are paired by\\ncrop size to compute loss terms. Unlike the practice in\\nSwAV, no loss symmetrization is employed and the 6 losses\\nfrom small crops are averaged before adding to the stan-\\ndard loss. When target encoder is involved in MultiCrop,\\nwe also create a separate memory bank [19] dedicated to\\nsmall crops, updated with 1 out of the 6 crops.\\n\\nAsymAug. For StrongerAug, we use additional augmenta-\\ntions from RandAug [12], same as [37]. For WeakerAug,\\nwe simply remove all the color- and blur-related augmenta-\\ntions and only keep geometric ones in the MoCo v2 recipe.\\nThis leaves us with random resized cropping and \\xef\\xac\\x82ipping.\\n\\nMeanEnc. Deviating from MultiCrop, augmentations used\\nfor computing the mean are forwarded jointly through the\\nencoder thanks to the uniform size of 224\\xc3\\x97224. Joint for-\\nwarding enlarges the batch size in BN, which further re-\\nduces the variance. The output encodings are averaged be-\\nfore (cid:96)2 normalization.\\nOther frameworks. Different from MoCo v2 which uses\\nshuf\\xef\\xac\\x82e BN [19] across 8 GPUs, all the frameworks stud-\\nied in Sec. 6.2 use SyncBN by default. Therefore, when\\napplying AsymBN to them, we keep the target encoder un-\\ntouched and change the BNs in the source encoder instead.\\nTo minimize the impact from the number of GPU devices\\n(e.g., MoCo v3 uses 16 GPUs to \\xef\\xac\\x81t a batch size of 4096\\nfor ResNet; whereas for ViT it uses 32 GPUs), we always\\ndivide the full batch into 8 groups and the normalization is\\nperformed within each group \\xe2\\x80\\x93 this mimics the per-device\\nBN operation in MoCo v2 while being more general.\\n\\nMoreover, for MoCo v2 we only convert the single BN in\\nthe target projector to SyncBN. This has minimal in\\xef\\xac\\x82uence\\non ef\\xef\\xac\\x81ciency as SyncBN can be expensive and converting\\nall of them (including ones in the encoder) can signi\\xef\\xac\\x81cantly\\nslow down training. Now since we are converting SyncBN\\nback, we choose to convert all BNs in the source encoder\\nwhenever possible to reduce inter-device communications\\nfor ef\\xef\\xac\\x81ciency purposes.\\n\\nMore recent frameworks [11, 44] adopt the asymmetric\\naugmentation recipe in BYOL [18], in such cases, we use\\none composition for source and the other for target half the\\ntime during pre-training, and swap them in the other half.\\n\\nTo have a fair comparison with frameworks pre-trained\\nfor 100 epochs, we optionally train 2\\xc3\\x97 as long when the\\ndefault loss is symmetrized and ours is asymmetric.\\n\\nUnless otherwise speci\\xef\\xac\\x81ed, we follow the same design\\nchoices in MoCo v2 when applying ScaleMix and MeanEnc\\nto other frameworks. In addition, there are subtleties asso-\\nciated with each individual framework listed below:\\n\\n\\xe2\\x80\\xa2 MoCo v3 [11]. Since MoCo v3 also employs an addi-\\ntional predictor on the source side, we involve both the\\npredictor and the backbone when applying AsymBN.\\n\\n\\xe2\\x80\\xa2 SimCLR [8]. The original SimCLR uses 2\\xc3\\x97N \\xe2\\x88\\x922\\nnegative examples for contrastive learning [8], which\\nincludes all the other images in the same batch, mul-\\ntiplied by 2 for the two augmentations per image.\\nAfter converting to the asymmetric version, we only\\nuse N \\xe2\\x88\\x921 negative samples \\xe2\\x80\\x93 same as in MoCo v3 \\xe2\\x80\\x93\\nand it causes a gap. We \\xef\\xac\\x81nd a simple change of In-\\nfoNCE [28] temperature from 0.1 to 0.2 can roughly\\ncompensate for this gap. For AsymBN, we convert all\\nthe BNs in the source encoder, not just the ones in the\\nprojector. For ScaleMix, we apply this augmentation\\nhalf the time \\xe2\\x80\\x93 we empirically \\xef\\xac\\x81nd applying ScaleMix\\nall the time will cause a considerable drop in perfor-\\nmance compared to the asymmetric baseline, for rea-\\nsons yet to be understood.\\n\\n\\xe2\\x80\\xa2 BYOL [18]. BYOL initiated the additional predictor\\nwhich also has BNs. We convert all the BNs in the\\nsource encoder when AsymBN is used, not just ones\\nin the projector.\\n\\n\\xe2\\x80\\xa2 SimSiam [10]. Additional predictor is again used in\\nSimSiam and plays an important role in collapse pre-\\nvention. We convert all the BNs in the source encoder\\nafter the conversion to an asymmetric version.\\n\\n\\xe2\\x80\\xa2 Barlow Twins [44]. This is a fully symmetric frame-\\nwork and no loss symmetrization is used by default.\\nTherefore, we also pre-train the asymmetric version\\nfor 100 epochs, not 2\\xc3\\x97 as long. Same as SimCLR,\\nScaleMix is applied with half the frequency. All the\\nencoder BNs are converted when AsymBN is used.\\n\\nViT backbone. MoCo v3 [11] with its default hyper-\\nparameters for ViT backbone is used. ViT as a backbone\\ndoes not have BN. Therefore we convert BNs in the projec-\\ntor and predictor when using AsymBN.\\n\\nTransfer learning. We follow the linear probing protocol\\nto evaluate our model on transfer learning tasks. Different\\nfrom ImageNet, we use SGD optimizer with momentum 0.9\\nand weight decay 0 for training. The learning rate is ad-\\njusted via grid search on the validation set, and the \\xef\\xac\\x81nal\\nresults are reported on the test set. All models are trained\\nfor 100 epochs, with a half-cycle cosine decaying schedule\\nfor learning rate.\\n\\n11\\n\\n\\x0cReferences\\n\\n[1] Adrien Bardes,\\n\\nJean Ponce, and Yann LeCun.\\n\\nVi-\\ncreg: Variance-invariance-covariance regularization for self-\\nsupervised learning. arXiv preprint arXiv:2105.04906, 2021.\\n1, 2\\n\\n[2] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas\\nPapernot, Avital Oliver, and Colin A Raffel. Mixmatch: A\\nholistic approach to semi-supervised learning. In NeurIPS,\\n2019. 2\\n\\n[3] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea\\nVedaldi, and Philip HS Torr. Fully-convolutional siamese\\nnetworks for object tracking. In ECCV, 2016. 2\\n\\n[4] Jane Bromley,\\n\\nIsabelle Guyon, Yann LeCun, Eduard\\nS\\xc2\\xa8ackinger, and Roopak Shah. Signature veri\\xef\\xac\\x81cation using\\na \\xe2\\x80\\x9cSiamese\\xe2\\x80\\x9d time delay neural network. In NeurIPS, 1994.\\n1, 2\\n\\n[5] Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Char-\\nless Fowlkes, Zhuowen Tu, and Stefano Soatto. Exponential\\nmoving average normalization for self-supervised and semi-\\nsupervised learning. In CVPR, 2021. 2\\n\\n[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\\notr Bojanowski, and Armand Joulin. Unsupervised learning\\nof visual features by contrasting cluster assignments. arXiv\\npreprint arXiv:2006.09882, 2020. 1, 2, 4, 7, 8, 11\\n\\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\\xc2\\xb4e J\\xc2\\xb4egou,\\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\\ning properties in self-supervised vision transformers. arXiv\\npreprint arXiv:2104.14294, 2021. 1, 2, 4, 7\\n\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. arXiv preprint arXiv:2002.05709,\\n2020. 1, 2, 4, 5, 7, 8, 11\\n\\n[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\\nImproved baselines with momentum contrastive learning.\\narXiv preprint arXiv:2003.04297, 2020. 2, 4\\n\\n[10] Xinlei Chen and Kaiming He. Exploring simple siamese rep-\\n\\nresentation learning. In CVPR, 2021. 1, 2, 3, 5, 7, 8, 9, 11\\n\\n[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical\\nstudy of training self-supervised vision transformers. arXiv\\npreprint arXiv:2104.02057, 2021. 2, 7, 11\\n\\n[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\\nLe. Randaugment: Practical automated data augmentation\\nwith a reduced search space. In CVPRW, 2020. 11\\n\\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\ndatabase. In CVPR, 2009. 3, 4, 6\\n\\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. In ICLR, 2021. 2, 5, 7\\n\\n[15] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre\\nSermanet, and Andrew Zisserman. With a little help from\\nmy friends: Nearest-neighbor contrastive learning of visual\\nrepresentations. arXiv preprint arXiv:2104.14548, 2021. 2,\\n8\\n\\n[16] Mark Everingham, Luc Van Gool, Christopher KI Williams,\\nJohn Winn, and Andrew Zisserman. The pascal visual object\\nclasses (voc) challenge. IJCV, 2010. 4\\n\\n[17] Priya Goyal, Piotr Doll\\xc2\\xb4ar, Ross Girshick, Pieter Noord-\\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\\nYangqing Jia, and Kaiming He. Accurate, large minibatch\\nSGD: Training ImageNet in 1 hour. arXiv:1706.02677, 2017.\\n7, 8\\n\\n[18] Jean-Bastien Grill, Florian Strub, Florent Altch\\xc2\\xb4e, Corentin\\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\\nnew approach to self-supervised learning. arXiv preprint\\narXiv:2006.07733, 2020. 1, 2, 4, 5, 7, 8, 11\\n\\n[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\nGirshick. Momentum contrast for unsupervised visual rep-\\nresentation learning. In CVPR, 2020. 1, 2, 3, 4, 8, 11\\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nIn CVPR,\\n\\nDeep residual learning for image recognition.\\n2016. 1, 4, 5, 7, 8\\n\\n[21] Sergey Ioffe and Christian Szegedy. Batch normalization:\\nAccelerating deep network training by reducing internal co-\\nvariate shift. In ICML, 2015. 3, 4, 5\\n\\n[22] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi,\\nXiaojie Jin, Anran Wang, and Jiashi Feng. All tokens matter:\\nToken labeling for training better vision transformers. arXiv\\npreprint arXiv:2104.10858, 2021. 7\\n\\n[23] Alex Krizhevsky. Learning multiple layers of features from\\n\\ntiny images. Tech Report, 2009. 8\\n\\n[24] Zeming Li, Songtao Liu, and Jian Sun. Momentum2\\nteacher: Momentum teacher with momentum statistics for\\nself-supervised learning. arXiv preprint arXiv:2101.07525,\\n2021. 2\\n\\nSgdr:\\n\\nStochas-\\narXiv preprint\\n\\n[25] Ilya Loshchilov and Frank Hutter.\\n\\ntic gradient descent with warm restarts.\\narXiv:1608.03983, 2016. 8\\n[26] Subhransu Maji, Esa Rahtu,\\n\\nJuho Kannala, Matthew\\nBlaschko, and Andrea Vedaldi. Fine-grained visual classi-\\n\\xef\\xac\\x81cation of aircraft. arXiv preprint arXiv:1306.5151, 2013.\\n8\\n\\n[27] Ishan Misra and Laurens van der Maaten. Self-supervised\\nlearning of pretext-invariant representations. In CVPR, 2020.\\n1, 4\\n\\n[28] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\\nsentation learning with contrastive predictive coding. arXiv\\npreprint arXiv:1807.03748, 2018. 1, 4, 6, 9, 11\\n\\n[29] Yun Sangdoo, Han Dongyoon, Oh Seong, Joon, Chun\\nSanghyuk, Choe Junsuk, and Yoo Youngjoon. Cutmix: Reg-\\nularization strategy to train strong classi\\xef\\xac\\x81ers with localizable\\nfeatures. In ICCV, 2019. 4, 9\\n\\n[30] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao\\nZhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han\\nZhang, and Colin Raffel. Fixmatch: Simplifying semi-\\nsupervised learning with consistency and con\\xef\\xac\\x81dence. arXiv\\npreprint arXiv:2001.07685, 2020. 2\\n\\n[31] Yaniv Taigman, Ming Yang, Marc\\xe2\\x80\\x99Aurelio Ranzato, and Lior\\nWolf. Deepface: Closing the gap to human-level perfor-\\nmance in face veri\\xef\\xac\\x81cation. In CVPR, 2014. 2\\n\\n12\\n\\n\\x0c[32] Antti Tarvainen and Harri Valpola. Mean teachers are better\\nrole models: Weight-averaged consistency targets improve\\nsemi-supervised deep learning results. In NeurIPS, 2017. 1\\n[33] Yuandong Tian, Xinlei Chen, and Surya Ganguli. Un-\\nderstanding self-supervised learning dynamics without con-\\ntrastive pairs. arXiv preprint arXiv:2102.06810, 2021. 6\\n[34] Yuandong Tian, Lantao Yu, Xinlei Chen, and Surya Gan-\\nguli. Understanding self-supervised learning with dual deep\\nnetworks. arXiv preprint arXiv:2010.00578, 2020. 6\\n[35] Tongzhou Wang and Phillip Isola. Understanding contrastive\\nrepresentation learning through alignment and uniformity on\\nthe hypersphere. In ICML, 2020. 3, 9\\n\\n[36] Xiao Wang, Daisuke Kihara, Jiebo Luo, and Guo-Jun\\nEnaet: Self-trained ensemble autoencoding trans-\\narXiv preprint\\n\\nQi.\\nformations for semi-supervised learning.\\narXiv:1911.09265, 2019. 2\\n\\n[37] Xiao Wang and Guo-Jun Qi. Contrastive learning with\\nstronger augmentations. arXiv preprint arXiv:2104.07713,\\n2021. 2, 8, 11\\n\\n[38] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey,\\nHongyuan Zha, Le Song, and Shu-Tao Xia. Iterative learning\\nwith open-set noisy labels. In CVPR, 2018. 2\\n\\n[39] Yuxin Wu and Kaiming He. Group normalization. In ECCV,\\n\\n2018. 5\\n\\n[40] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\nUnsupervised feature learning via non-parametric instance\\ndiscrimination. In CVPR, 2018. 4\\n\\n[41] Haohang Xu, Xiaopeng Zhang, Hao Li, Lingxi Xie, Hongkai\\nXiong, and Qi Tian. Seed the views: Hierarchical seman-\\ntic alignment for contrastive representation learning. arXiv\\npreprint arXiv:2012.02733, 2020. 2, 8\\n\\n[42] Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh\\nLiu, Yubei Chen, and Yann LeCun. Decoupled contrastive\\nlearning. arXiv preprint arXiv:2110.06848, 2021. 6, 9\\n[43] Yang You, Igor Gitman, and Boris Ginsburg. Large batch\\ntraining of convolutional networks. arXiv:1708.03888, 2017.\\n2, 7, 8\\n\\n[44] Jure Zbontar, Li Jing,\\n\\nIshan Misra, Yann LeCun, and\\nSt\\xc2\\xb4ephane Deny. Barlow twins: Self-supervised learning via\\narXiv preprint arXiv:2103.03230,\\nredundancy reduction.\\n2021. 1, 2, 7, 8, 11\\n\\n[45] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\\ntion. arXiv preprint arXiv:1710.09412, 2017. 9\\n\\n[46] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-\\nralba, and Aude Oliva. Learning deep features for scene\\nrecognition using places database. In NeurIPS, 2014. 8\\n[47] Pan Zhou, Caiming Xiong, Xiao-Tong Yuan, and Steven\\nA theory-driven self-labeling re\\xef\\xac\\x81nement method\\narXiv preprint\\n\\nHoi.\\nfor contrastive representation learning.\\narXiv:2106.14749, 2021. 2, 8\\n\\n13\\n\\n\\x0c', b'2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\nI\\n\\nA\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n7\\n0\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nFROM STATISTICAL TO CAUSAL LEARNING\\n\\nBernhard Sch\\xc2\\xa8olkopf\\nMax Planck Institute for Intelligent Systems, T\\xc2\\xa8ubingen, Germany\\nbs@tuebingen.mpg.de\\n\\nJulius von K \\xc2\\xa8ugelgen\\nMax Planck Institute for Intelligent Systems, T\\xc2\\xa8ubingen, Germany\\nUniversity of Cambridge, United Kingdom\\njvk@tuebingen.mpg.de\\n\\nApril 4, 2022\\n\\nABSTRACT\\n\\nWe describe basic ideas underlying research to build and understand arti\\xef\\xac\\x81cially intelligent systems:\\nfrom symbolic approaches via statistical learning to interventional models relying on concepts of\\ncausality. Some of the hard open problems of machine learning and AI are intrinsically related\\nto causality, and progress may require advances in our understanding of how to model and infer\\ncausality from data.*\\n\\nMathematics Subject Classi\\xef\\xac\\x81cation 2020\\n\\nPrimary 68T05; Secondary 68Q32, 68T01, 68T10, 68T30, 68T37\\n\\nCausal inference, machine learning, causal representation learning\\n\\nKeywords\\n\\n1\\n\\nIntroduction\\n\\nIn 1958, the New York Times reported on a new machine called the perceptron. Frank Rosenblatt, its inventor,\\ndemonstrated that the perceptron was able to learn from experience. He predicted that later perceptrons would be able\\nto recognize people, or instantly translate spoken language. Now a reality, this must have sounded like distant science\\n\\xef\\xac\\x81ction at the time. In hindsight, we may consider it the birth of machine learning, the \\xef\\xac\\x81eld fueling most of the current\\nadvances in arti\\xef\\xac\\x81cial intelligence (AI).\\n\\nAround the same time, another equally revolutionary development took place: scientists understood that computers\\ncould do more than compute numbers:\\nthey can process symbols. Although this insight was also motivated by\\narti\\xef\\xac\\x81cial intelligence, in hindsight it was the birth of the \\xef\\xac\\x81eld of computer science. There was great optimism that the\\nmanipulation of symbols, in programs written by humans, implementing rules designed by humans, should be enough\\nto generate intelligence. Below, we shall refer to this as the symbol-rule hypothesis.1\\n\\nThere was initially encouraging progress on seemingly hard problems such as automatic theorem proving and\\ncomputer chess. One of the fathers of the \\xef\\xac\\x81eld, Herb Simon, predicted in 1956 that \\xe2\\x80\\x9cmachines will be capable,\\nwithin twenty years, of doing any work a man can do.\\xe2\\x80\\x9d However, problems that appeared simple, such as most things\\nanimals could do, turned out to be hard. This came to be known as Moravec\\xe2\\x80\\x98s paradox. When IBM\\xe2\\x80\\x99s Deep Blue\\n\\n*To appear in the Proceedings of the International Congress of Mathematicians 2022, EMS Press. Both authors contributed\\n\\nequally to this work; names listed in alphabetical order.\\n\\n1The term should be taken with a grain of salt, since it suggests a separation between representations and computations which\\n\\nis hard to uphold in practice.\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nchess computer beat Garry Kasparov in 1997, Kasparov was physically facing a human during the match: while\\nDeep Blue was capable of analyzing the game\\xe2\\x80\\x99s search tree in unprecedented detail, it was unable to recognize and\\nphysically move chess pieces, so this task had to be relegated to a human, in an inversion of the famous mechanical\\nturk.2 In the years to follow, the \\xef\\xac\\x81eld of AI entered what came to be known as the AI winter. The community got\\ndisillusioned with the lack of progress and prospects, and interest greatly declined. However, largely independently of\\nthe \\xef\\xac\\x81eld of classic AI, machine learning eventually started to boom. Like Rosenblatt\\xe2\\x80\\x99s early work, it was built on the\\nobservation that all existing examples of truly intelligent systems\\xe2\\x80\\x94i.e., animals, including humans\\xe2\\x80\\x94were not built\\non the symbol-rule hypothesis: both the representations and the rules implemented by natural intelligent systems are\\nacquired from experience, through processes of evolution and learning.\\n\\nRather than exploring the well-known dichotomy between rule-based and learning-based approaches, we will explore\\nthe less known questions of causality and interventions. While the \\xef\\xac\\x81eld of causality in computer science was initially\\nstrongly linked to classic AI, recent years have witnessed great interest in connecting it to machine learning [112].\\nBelow, we explore some of these connections, drawing from [126, 131]. We will argue that the causal view is rel-\\nevant when it comes to addressing crucial open problems of machine learning, related to notions of robustness and\\ngeneralization beyond the training distribution.\\n\\nOverview In statistical learning, our starting point is a joint distribution p(X) generating the observable data. Here,\\nX is a random vector, and we are usually given a dataset x1, . . . , xm sampled i.i.d. from p. We are often interested in\\nestimating properties of conditionals of some components of X given others, e.g., a classi\\xef\\xac\\x81er (which may be obtained\\nby thresholding a conditional at 0.5). This is a nontrivial inverse problem, giving rise to statistical learning theory (\\xc2\\xa7 2).\\n\\nCausal learning is motivated by shortcomings of statistical learning (\\xc2\\xa7 3). Its starting point is a structural causal model\\n(SCM) [105] (\\xc2\\xa7 4). In an SCM, the components X1, . . . , Xn of X are identi\\xef\\xac\\x81ed with vertices of a directed graph whose\\narrows represent direct causal in\\xef\\xac\\x82uences, and there is a random variable Ui for each vertex, along with a function fi\\nwhich computes Xi from its graph parents PAi and Ui, i.e.,\\n\\nXi := fi(PAi, Ui).\\n\\n(1.1)\\n\\nGiven a distribution over the Ui, which are assumed independent, this also gives rise to a probabilistic model p(X).\\nHowever, the model in (1.1) is more structured: the graph connectivity and the functions fi create particular depen-\\ndences between the observables. Moreover, it describes how the system behaves under intervention: by replacing\\nfunctions by constants, we can compute the effect of setting some variables to speci\\xef\\xac\\x81c values.\\n\\nCausal learning builds on assumptions different from standard machine learning (\\xc2\\xa7 5), and addresses a different level\\nin the modeling hierarchy (\\xc2\\xa7 6). It also comes with new problems, such as causal discovery, where we seek to infer\\nproperties of graph and functions from data (\\xc2\\xa7 7). In some cases, conditional independences among the Xi contain\\ninformation about the graph [145]; but novel assumptions let us handle some cases that were previously unsolvable\\n[64]. Those assumptions have nontrivial implications for machine learning tasks such as semi-supervised learning,\\ncovariate shift adaptation and transfer learning [130] (\\xc2\\xa7 8). Once provided with a causal model, causal reasoning (\\xc2\\xa7 9)\\nallows us to identify and estimate certain causal queries of interest from observational data. We conclude with a list of\\nsome current and open problems (\\xc2\\xa7 10), with a particular emphasis on the topic of causal representation learning.\\n\\nThe presentation and notation will be somewhat informal in several respects. We generally assume that all distribu-\\ntions possess densities (w.r.t. a suitable reference measure). We sometimes write p(x) for the distribution (or density)\\nof a random variable X. Accordingly, the same p can denote another distribution p(y), distinguished by the argument\\nof p(\\xc2\\xb7). We also sometimes use summation for marginalization which supposes discrete variables; the corresponding\\nexpressions for continuous quantities would use integrals.\\n\\n2 Statistical Learning Theory\\n\\nSuppose we have measured two statistically dependent observables and found the points to lie approximately on\\na straight line. An empirical scientist might be willing to hypothesize a corresponding law of nature (see Fig. 1).\\nHowever, already Leibniz pointed out that if we scatter spots of ink randomly on a piece of paper by shaking a quill\\npen, we can also \\xef\\xac\\x81nd a mathematical equation satis\\xef\\xac\\x81ed by these points [81]. He argued that we would not call this a\\nlaw of nature, because no matter how the points are distributed, there always exists such an equation; we would only\\ncall it a law of nature only if the equation is simple. This raises the question of what makes an equation simple. The\\nphysicist Rutherford took the pragmatic view that if there is a law, it should be directly evident from the data: \\xe2\\x80\\x9cif your\\nexperiment needs statistics, you ought to have done a better experiment.\\xe2\\x80\\x9d3 This view may have been a healthy one\\n\\n2https://en.wikipedia.org/wiki/Mechanical_Turk\\n3Cited after http://www.warwick.ac.uk/statsdept/staff/JEHS/data/jehsquot.pdf.\\n\\n2\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 1: Given a small number of observations, how do\\nwe \\xef\\xac\\x81nd a law underlying them? Leibniz argued that even\\nif we generate a random set of points, we can always \\xef\\xac\\x81nd a\\nmathematical equation satis\\xef\\xac\\x81ed by these points.\\n\\nwhen faced with low-dimensional inference problems where regularities are immediately obvious; however, modern\\nAI is facing inference problems that are harder: they are often high-dimensional and nonlinear, yet we may have little\\nprior knowledge about the underlying regularity (e.g., for medical data, we usually do not have a mechanistic model).\\n\\nStatistical learning theory studies the problem of how to still perform valid inference, provided that we have suf\\xef\\xac\\x81-\\nciently large datasets and the computational means to process them. Let us look at some theoretical results for the\\nsimplest learning scenario, drawing from [133]; for details, see [153]. Suppose we are given empirical observations,\\n\\n(x1, y1), . . . , (xm, ym) \\xe2\\x88\\x88 X \\xc3\\x97 Y,\\nwhere X is some nonempty set from which the inputs come, and Y = {\\xc2\\xb11} is the output set, in our case consisting\\nof just two classes. This situation is called pattern recognition, and our goal is to use the training data (2.1) to infer\\na function f : X \\xe2\\x86\\x92 {\\xc2\\xb11} (from some function class chosen a priori) which will produce the correct output for a\\nnew input x which we may not have seen before. To formalize what we mean by \\xe2\\x80\\x9ccorrect\\xe2\\x80\\x9d, we make the assumption\\nthat all observations (xi, yi) have been generated independently by performing a random experiment described by an\\nunknown probability distribution p(x, y)\\xe2\\x80\\x94a setting referred to as i.i.d. (independent and identically distributed) data.\\nOur goal will be to minimize the expected error (or risk)\\n\\n(2.1)\\n\\nR[f ] =\\n\\nc(y, f (x)) dp(x, y),\\n\\n(2.2)\\n\\n(cid:90)\\n\\nX\\xc3\\x97Y\\n\\nwhere c is a so-called loss function, e.g., the misclassi\\xef\\xac\\x81cation error c(y, f (x)) = 1\\nwhenever f (x) = y and 1 otherwise.\\n\\n2 |f (x) \\xe2\\x88\\x92 y| taking the value 0\\n\\nThe dif\\xef\\xac\\x81culty of the task stems from the fact that we are trying to minimize a quantity that we cannot evaluate: since\\nwe do not know p, we cannot compute (2.2). We do know, however, the training data (2.1) sampled from p. We can\\nthus try to infer a function f from the training sample whose risk is close to the minimum of (2.2). To this end, we\\nneed what is called an induction principle.\\n\\nOne way to proceed is to use the training sample to approximate (2.2) by a \\xef\\xac\\x81nite sum, referred to as the empirical risk\\n\\nRemp[f ] =\\n\\nc(xi, yi, f (xi)).\\n\\n(2.3)\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nThe empirical risk minimization (ERM) induction principle recommends that we choose (or \\xe2\\x80\\x9clearn\\xe2\\x80\\x9d) an f that mini-\\nmizes (2.3). We can then ask whether the ERM principle is statistically consistent: in the limit of in\\xef\\xac\\x81nitely many data\\npoints, will ERM lead to a solution which will do as well as possible on future data generated by p?\\n\\nIt turns out that if the function class over which we minimize (2.3) is too large, then ERM is not consistent. Hence,\\nwe need to suitably restrict the class of possible functions. For instance, ERM is consistent for all probability\\ndistributions, provided that the VC dimension of the function class is \\xef\\xac\\x81nite. The VC dimension is an example of a\\ncapacity measure. It is de\\xef\\xac\\x81ned as the maximal number of points that can be separated (classi\\xef\\xac\\x81ed) in all possible ways\\nusing functions from the class. E.g., using linear classi\\xef\\xac\\x81ers (separating classes by straight lines) on R2, we can realize\\nall possible classi\\xef\\xac\\x81cations for 3 suitably chosen points, but we can no longer do this once we have 4 points, no matter\\nhow they are placed (see Fig. 2). This means that the VC dimension of this function class is 3. More generally, for\\nlinear separations in Rd, the VC dimension is d + 1.\\n\\nWhenever the VC dimension is \\xef\\xac\\x81nite, our class of functions (or explanations) becomes falsi\\xef\\xac\\x81able in the sense that start-\\ning from a certain number of observations, no longer all possible labelings of the points can be explained (cf. Fig. 2).\\nIf we can nevertheless explain a suf\\xef\\xac\\x81ciently large set of observed data, we thus have reason to believe that this is a\\nmeaningful \\xef\\xac\\x81nding.\\n\\n3\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 2: Using straight lines, we can separate three points in all possible ways; we cannot do this for four points,\\nno matter how they are placed. The class of linear separations is not \\xe2\\x80\\x9cfalsi\\xef\\xac\\x81able\\xe2\\x80\\x9d using three points, but it becomes\\nfalsi\\xef\\xac\\x81able once we have four or more points.\\n\\nMuch of machine learning research is concerned with restrictions on classes of functions to make inference possible,\\nbe it by imposing prior distributions on function classes, through other constraints, or by designing self-regularizing\\nlearning procedures, e.g., gradient descent methods for neural networks [79]. While there is a solid theoretical under-\\nstanding of supervised machine learning as described above (i.e., function learning from input-output examples), there\\nare still details under investigation, such as the recently observed phenomenon of \\xe2\\x80\\x9cdouble descent\\xe2\\x80\\x9d [7].\\n\\nA popular constraint, implemented in the Support Vector Machine (SVM) [153, 133], is to consider linear separations\\nwith large margin: it turns out that for large margin separations in high-dimensional (or in\\xef\\xac\\x81nite-dimensional) spaces,\\nthe capacity can be much smaller than the dimensionality, making learning possible in situations where it would\\notherwise fail.\\n\\nFor some learning algorithms, including SVMs and nearest neighbor classi\\xef\\xac\\x81ers, there are strong universal consistency\\nresults, guaranteeing convergence of the algorithm to the lowest achievable risk, for any problem to be learned [28,\\n153, 133, 147]. Note, however, that this convergence can be arbitrarily slow.\\n\\nFor a given sample size, it will depend on the problem being learned whether we achieve low expected error. In\\naddition to asymptotic consistency statements, learning theory makes \\xef\\xac\\x81nite sample size statements: one can prove that\\nwith probability at least 1 \\xe2\\x88\\x92 \\xce\\xb4 (for \\xce\\xb4 > 0), for all functions f in a class of functions with VC dimension h,\\n\\nR[f ] \\xe2\\x89\\xa4 Remp[f ] +\\n\\nh (log(2m/h) + 1) + log\\n\\n(2.4)\\n\\n(cid:115)\\n\\n(cid:18)\\n\\n1\\nm\\n\\n(cid:19)\\n\\n.\\n\\n4\\n\\xce\\xb4\\n\\nThis is an example of a class of results that relate the training error Remp[f ] and the test error R[f ] using a con\\xef\\xac\\x81dence\\ninterval (the square root term) depending on a capacity measure of a function class (here, its VC dimension h). It\\nsays that with high probability, the expected error R[f ] on future observations generated by the unknown probability\\ndistribution is small, provided the two terms on the right hand side are small: the training error Remp[f ] (i.e., the error\\non the examples we have already seen), and the square root term, which will be small whenever the capacity h is small\\ncompared to the number of training observations m. If, on the other hand, we try to learn something that may not\\nmake sense, such as the mapping from the name of people to their telephone number, we would \\xef\\xac\\x81nd that to explain\\nall the training data (i.e., to obtain a small Remp[f ]), we need a model whose capacity h is large, and the second term\\nbecomes large. In any case, it is crucial for both consistency results and \\xef\\xac\\x81nite sample error bounds such as (2.4) that\\nwe have i.i.d. data.\\n\\nKernel Methods A symmetric function k : X2 \\xe2\\x86\\x92 R, where X is a nonempty set, is called a positive de\\xef\\xac\\x81nite (pd)\\nkernel if for arbitrary points x1, . . . , xm \\xe2\\x88\\x88 X and coef\\xef\\xac\\x81cients a1, . . . , am \\xe2\\x88\\x88 R:\\n\\nThe kernel is called strictly positive de\\xef\\xac\\x81nite if for pairwise distinct points, the implication (cid:80)\\n\\xe2\\x88\\x80i : ai = 0 is valid. Any positive de\\xef\\xac\\x81nite kernel induces a mapping\\n\\ni,j aiajk(xi, xj) = 0 =\\xe2\\x87\\x92\\n\\ninto a reproducing kernel Hilbert space (RKHS) H satisfying\\n\\n(2.5)\\n\\n(2.6)\\n\\n(cid:88)\\n\\ni,j\\n\\naiajk(xi, xj) \\xe2\\x89\\xa5 0.\\n\\n\\xce\\xa6 : x (cid:55)\\xe2\\x86\\x92 k(x, .)\\n\\n(cid:104)k(x, .), k(x(cid:48), .)(cid:105) = k(x, x(cid:48))\\n\\n4\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nfor all x, x(cid:48) \\xe2\\x88\\x88 X. Although H may be in\\xef\\xac\\x81nite-dimensional, we can construct practical classi\\xef\\xac\\x81cation algorithms in H\\nprovided that all computational steps are carried out in terms of scalar products, since those can be reduced to kernel\\nevaluations (2.6).\\n\\nIn the SVM algorithm, the capacity of the function class is restricted by enforcing a large margin of class separation\\nin H via a suitable RKHS regularization term. The solution can be shown to take the form\\n\\nf (x) = sgn\\n\\n\\xce\\xb1ik(xi, x) + b\\n\\n,\\n\\n(cid:19)\\n\\n(cid:18) (cid:88)\\n\\ni\\n\\n(2.7)\\n\\n(2.8)\\n\\nwhere the learned parameters \\xce\\xb1i and b are the solution of a convex quadratic optimization problem. A similar ex-\\npansion of the solution in terms of kernel functions evaluated at training points holds true for a larger class of kernel\\nalgorithms beyond SVMs, regularized by an RKHS norm [127].\\n\\nIn kernel methods, the kernel plays three roles which are crucial for machine learning: it acts as a similarity measure\\nfor data points, induces a representation in a linear space4 via (2.5), and parametrizes the function class within which\\nthe solution is sought, cf. (2.7).\\n\\nKernel Mean Embeddings Consider two sets of points X := {x1, . . . , xm} \\xe2\\x8a\\x82 X and Y := {y1, . . . , yn} \\xe2\\x8a\\x82 X. We\\nde\\xef\\xac\\x81ne the mean map \\xc2\\xb5 as [133]\\n\\n\\xc2\\xb5(X) =\\n\\nk(xi, \\xc2\\xb7).\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nFor polynomial kernels k(x, x(cid:48)) = ((cid:104)x, x(cid:48)(cid:105) + 1)d, we have \\xc2\\xb5(X) = \\xc2\\xb5(Y ) if all empirical moments up to order d\\ncoincide. For strictly pd kernels, the means coincide only if X = Y , rendering \\xc2\\xb5 injective [134]. The mean map\\nhas some other interesting properties [144], e.g., \\xc2\\xb5(X) represents the operation of taking a mean of a function on the\\nsample X:\\n\\n(cid:104)\\xc2\\xb5(X), f (cid:105) =\\n\\nk(xi, \\xc2\\xb7), f\\n\\n=\\n\\nf (xi)\\n\\n(cid:28) 1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\n(cid:29)\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nMoreover, we have\\n\\n(cid:107)\\xc2\\xb5(X) \\xe2\\x88\\x92 \\xc2\\xb5(Y )(cid:107) = sup\\n(cid:107)f (cid:107)\\xe2\\x89\\xa41\\n\\n|(cid:104)\\xc2\\xb5(X) \\xe2\\x88\\x92 \\xc2\\xb5(Y ), f (cid:105)| = sup\\n(cid:107)f (cid:107)\\xe2\\x89\\xa41\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nf (xi) \\xe2\\x88\\x92\\n\\nf (yi)\\n\\n.\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\nIf Ex,x(cid:48)\\xe2\\x88\\xbcp[k(x, x(cid:48))], Ex,x(cid:48)\\xe2\\x88\\xbcq[k(x, x(cid:48))] < \\xe2\\x88\\x9e, then the above statements, including the injectivity of \\xc2\\xb5, generalize to\\nBorel measures p, q, if we de\\xef\\xac\\x81ne the mean map as\\n\\n\\xc2\\xb5 : p (cid:55)\\xe2\\x86\\x92 Ex\\xe2\\x88\\xbcp[k(x, \\xc2\\xb7)],\\n\\nand replace the notion of strictly pd kernels by that of characteristic kernels [33]. This means that we do not lose\\ninformation when representing a probability distribution in the RKHS. This enables us to work with distributions using\\nHilbert space methods, and construct practical algorithms analyzing distributions using scalar product evaluations.\\n\\nNote that the mean map \\xc2\\xb5 can be viewed as a generalization of the moment generating function Mp of a random\\nvariable x with distribution p,\\n\\nMp(.) = Ex\\xe2\\x88\\xbcp\\n\\n(cid:104)\\n\\ne(cid:104)x, \\xc2\\xb7 (cid:105)(cid:105)\\n\\n.\\n\\nThe map \\xc2\\xb5 has applications in a number of tasks including computing functions of random variables [132], testing\\nof homogeneity [41], and of independence [43]. The latter will be of particular interest to causal inference: we\\ncan develop a kernel-based independence test by computing the distance between sample-based embeddings of a\\njoint distribution p(X, Y ) and the product of its marginals p(X), p(Y ) [42, 44, 43, 165, 115], and generalize it to\\nconditional independence testing [33, 101], as required for certain causal discovery methods (see \\xc2\\xa7 7).\\n\\n3 From Statistical to Causal Models\\n\\nMethods Relying on i.i.d. Data In current successes of machine learning [79], we generally (i) have large amounts\\nof data, often from simulations or large-scale human labeling, (ii) use high capacity machine learning models (e.g.,\\nneural networks with many adjustable parameters), and (iii) employ high performance computing. Statistical learning\\n\\n4Note that the data domain X need not have any structure other than being a nonempty set.\\n\\n5\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 3: Measurements of two genes (x-axis), gene A (left) and gene B (right), show the same strong positive\\ncorrelation with a phenotype (y-axis). However, this statistical information alone is insuf\\xef\\xac\\x81cient to predict the outcome\\nof a knock-out experiment where the activity of a gene is set to zero (vertical lines at x = 0). Answering such\\ninterventional questions requires additional causal knowledge (inset causal graphs): knocking out gene A, which is a\\ndirect cause, would lead to a reduction in phenotype, whereas knocking out gene B, which shares a common cause, or\\nconfounder, with the phenotype but has no causal effect on it, would leave the phenotype unaffected. This shows that\\ncorrelation alone is not enough to predict the outcome of perturbations to a system (toy data, \\xef\\xac\\x81gure from [112]).\\n\\ntheory offers a partial explanation for recent successes of learning: huge datasets enable training complex models and\\nthus solving increasingly dif\\xef\\xac\\x81cult tasks.\\n\\nHowever, a crucial aspect that is often ignored is that we (iv) assume that the data are i.i.d. This assumption is crucial\\nfor good performance in practice, and it underlies theoretical statements such as (2.4). When faced with problems\\nthat violate the i.i.d. assumption, all bets are off. Vision systems can be grossly misled if an object that is normally\\nrecognized with high accuracy is placed in a context that in the training set may be negatively correlated with the\\npresence of the object. For instance, such a system may fail to recognize a cow standing on the beach. In order to\\nsuccessfully generalize in such settings, we would need to construct systems which do not merely rely on statistical\\ndependences, but instead model mechanisms that are robust across certain violations of the i.i.d. assumption. As we\\nwill argue, causality provides a natural framework for capturing such stable mechanisms and reasoning about different\\ntypes of distribution shifts.\\n\\nCorrelation vs Causation It is a commonplace that correlation does not imply causation. Two popular and\\nillustrative examples are the positive correlation between chocolate consumption and nobel prizes per capita [91],\\nand that between the number of stork breeding pairs and human birth rates [89], neither of which admit a sensible\\ninterpretation in terms of direct causation. These examples naturally lead to the following questions: What exactly\\ndo we mean by \\xe2\\x80\\x9ccausation\\xe2\\x80\\x9d? What is its relationship to correlation? And, if correlation alone is not enough, what is\\nneeded to infer causation?\\n\\nHere, we adopt a notion of causality based on manipulability [159] and intervention [105] which has proven useful in\\n\\xef\\xac\\x81elds such as agriculture [161], econometrics [46, 52] and epidemiology [119].\\n\\nDe\\xef\\xac\\x81nition 3.1 (Causal effect). We say that a random variable X has a causal effect on a random variable Y if there\\nexist x (cid:54)= x(cid:48) s.t. the distribution of Y after intervening on X and setting it to x differs from the distribution of Y after\\nsetting X to x(cid:48).\\n\\nInherent to the notion of causation, there is a directionality and asymmetry which does not exist for correlation: if\\nX is correlated with Y , then Y is equally correlated with X; but, if X has a causal effect on Y , the converse (in the\\ngeneric case) does not hold.\\n\\nWe illustrate the intervention-based notion of causation and its difference from correlation (or, more generally, sta-\\ntistical dependence) in Fig. 3. Here, knocking out two genes XA and XB that are indistinguishable based on their\\ncorrelation with a phenotype Y would have very different effects. Only intervening on XA would change the dis-\\ntribution of Y , whereas XB does not have a causal effect on Y \\xe2\\x80\\x94instead, their correlation arises from a different\\n(confounded) causal structure. Such causal relationships are most commonly represented in the form of causal graphs\\nwhere directed arrows indicate a direct causal effect.\\n\\n6\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nX\\n\\nY\\n\\nX\\n\\nY\\n\\nX\\n\\nY\\n\\n(a)\\n\\n(b)\\n\\nZ\\n\\n(c)\\n\\nFigure 4: Reichenbach\\xe2\\x80\\x99s common cause principle [117] postulates that statistical dependence between two random\\nvariables X and Y has three elementary possible causal explanations shown as causal graphs in (a)\\xe2\\x80\\x93(c). It thus states\\nthat association is always induced by an underlying causal process. In (a) the common cause Z coincides with X, and\\nin (b) it coincides with Y . Grey nodes indicate observed and white nodes unobserved variables.\\n\\nThe example in Fig. 3 shows that the same correlation can be explained by multiple causal graphs which lead to\\ndifferent experimental outcomes, i.e., correlation does not imply causation. However, there is a connection between\\ncorrelation and causation, expressed by Reichenbach [117] as the Common Cause Principle, see Fig. 4.\\nPrinciple 3.2 (Common Cause). If two random variables X and Y are statistically dependent (X (cid:54)\\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 Y ), then there\\nexists a random variable Z which causally in\\xef\\xac\\x82uences both of them and which explains all their dependence in the\\nsense of rendering them conditionally independent (X \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 Y | Z). As a special case, Z may coincide with X or Y .\\n\\nAccording to Principle 3.2, statistical dependence always results from underlying causal relationships by which vari-\\nables, including potentially unobserved ones, in\\xef\\xac\\x82uence each other. Correlation is thus an epiphenomenon, the by-\\nproduct of a causal process.\\n\\nFor the example of chocolate consumption (X) and Nobel laureates (Y ), common sense suggests that neither of the two\\nvariables should have a causal effect on the other, i.e., neither chocolate consumption driving scienti\\xef\\xac\\x81c success (X \\xe2\\x86\\x92\\nY ; Fig. 4a) nor Nobel laureates increasing chocolate consumption (Y \\xe2\\x86\\x92 X; Fig. 4c) seem plausible. Principle 3.2\\nthen tells us that the observed correlation must be explained by a common cause Z as in Fig. 4c. A plausible candidate\\nfor such a confounder could, for example, be economic factors driving both consumer spending and investment in\\neducation and science.\\n\\nWithout such background knowledge or additional assumptions, however, we cannot distinguish the three cases\\nin Fig. 4 through passive observation, i.e., in a purely data-driven way: the class of observational distributions over X\\nand Y that can be realized by these models is the same in all three cases.\\n\\nTo be clear, this does not mean that correlation cannot be useful, nor that causal insight is always required. Both genes\\nin Fig. 3 remain useful features for making predictions in a passive, or observational, setting in which we measure the\\nactivities of certain genes and are asked to predict the phenotype. Similarly, chocolate consumption remains predictive\\nof winning Nobel prizes. However, if we want to answer interventional questions, such as the outcome of a gene-\\nknockout experiment or the effect of a policy enforcing higher chocolate consumption, we need more than correlation:\\na causal model.\\n\\n4 Causal Modeling Frameworks\\n\\nCausal inference has a long history in a variety of disciplines, including statistics, econometrics, epidemiology, and\\nAI. As a result, different frameworks for causal modeling have emerged over the years and coexist today. The \\xef\\xac\\x81rst\\nframework described below (CGM) starts from the distribution of the observables, combining it with a directed graph\\nto endow it with causal semantics. The second one (SCM) starts from a graph and a set of functional assignments,\\nand generates the observed distribution as the push-forward of an unobserved noise distribution. Finally, we cover a\\nnon-graphical approach (PO) popular in statistics.\\n\\nCausal Graphical Models (CGMs) The graphical models framework [78, 75] provides a compact way of represent-\\ning joint probability distributions by encoding the dependence structure between variables in graphical form. Directed\\ngraphical models are also known as Bayesian networks [102]. While they do not offer a causal interpretation per se\\xe2\\x80\\x94\\nindeed, different graphical models can be compatible with the same distribution (cf. Principle 3.2)\\xe2\\x80\\x94when edges are\\nendowed with the notion of direct causal effect (Defn. 3.1), we refer to them as causal graphical models (CGM) [145].\\nDe\\xef\\xac\\x81nition 4.1 (CGM). A CGM M = (G, p) over n random variables X1, . . . , Xn consists of: (i) a directed acyclic\\ngraph (DAG) G in which directed edges (Xj \\xe2\\x86\\x92 Xi) represent a direct causal effect of Xj on Xi; and (ii) a joint\\ndistribution p(X1, . . . , Xn) which is Markovian w.r.t. G:\\n\\np(X1, . . . , Xn) =\\n\\np(Xi | PAi)\\n\\n(4.1)\\n\\nn\\n(cid:89)\\n\\ni=1\\n\\n7\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nX1\\n\\nX2\\n\\nX3\\n\\n(a) G\\n\\nX1\\n\\nx2\\n\\nX3\\n\\n(b) G(cid:48)\\n\\nX1\\n\\nX2\\n\\nx3\\n\\n(c) G(cid:48)(cid:48)\\n\\nFigure 5: (a) A directed acyclic graph (DAG) G over three variables. A causal graphical model (G, p) with causal\\ngraph G and observational distribution p can be used to answer interventional queries using the concept of graph\\nsurgery: when a variable is intervened upon and set to a constant (white diamonds), this removes any in\\xef\\xac\\x82uence from\\nother variables, captured graphically by removing all incoming edges. (b) and (c) show post-intervention graphs G(cid:48)\\nand G(cid:48)(cid:48) for do(X2 := x2) and do(X3 := x3), respectively. (An intervention on X1 would leave the graph unaffected.)\\n\\nwhere PAi = {Xj : (Xj \\xe2\\x86\\x92 Xi) \\xe2\\x88\\x88 G} denotes the set of parents, or direct causes, of Xi in G.\\n\\nWe will refer to (4.1) as the causal (or disentangled) factorization. While many other entangled factorizations are\\npossible, e.g.,\\n\\np(X1, . . . , Xn) =\\n\\np(Xi | Xi+1, . . . , Xn),\\n\\n(4.2)\\n\\nn\\n(cid:89)\\n\\ni=1\\n\\nonly (4.1) decomposes the joint distribution into causal conditionals, or causal mechanisms, p(Xi | PAi), which can\\nhave a meaningful physical interpretation, rather than being mere mathematical objects such as the factors on the RHS\\nof (4.2).\\n\\nIt turns out that (4.1) is equivalent to the following condition.\\nDe\\xef\\xac\\x81nition 4.2 (Causal Markov condition). A distribution p satis\\xef\\xac\\x81es the causal Markov condition w.r.t. a DAG G if\\nevery variable is conditionally independent of its non-descendants in G given its parents in G.\\n\\nDefn. 4.2 can equivalently be expressed in terms of d-separation, a graphical criterion for directed graphs [105], by\\nsaying that d-separation in G implies (conditional) independence in p. The causal Markov condition thus provides a\\nlink between properties of p and G.\\n\\nWhat makes CGMs causal is the interpretation of edges as cause-effect relationships which enables reasoning about\\nthe outcome of interventions using the do-operator [105] and the concept of graph surgery [145]. The central idea is\\nthat intervening on a variable, say by externally forcing it to take on a particular value, renders it independent of its\\ncauses and breaks their causal in\\xef\\xac\\x82uence on it, see Fig. 5 for an illustration. For example, if a gene is knocked out, it\\nis no longer in\\xef\\xac\\x82uenced by other genes that were previously regulating it; instead, its activity is now solely determined\\nby the intervention. This is fundamentally different from conditioning since passively observing the activity of a gene\\nprovides information about its driving factors (i.e., its direct causes).\\n\\nTo emphasize this difference between passive observation and active intervention, Pearl [105] introduced the notation\\ndo(X := x) to denote an intervention by which variable X is set to value x. The term graph surgery refers to the idea\\nthat the effect of such an intervention can be captured in the form of a modi\\xef\\xac\\x81cation to the original graph by removing\\nall incoming edges to the intervened variable. Interventional queries can then be answered by performing probabilistic\\ninference in the modi\\xef\\xac\\x81ed post-intervention graph which typically implies additional (conditional) independences due\\nto the removed edges.\\nExample 4.3. The interventional distribution p(X3 | do(X2 = x2)) for the CGM in Fig. 5 is obtained via probabilistic\\ninference w.r.t. the post-intervention graph G(cid:48) where X1 \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 X2:\\n\\np(X3|do(X2 := x2)) =\\n\\np(x1)p(X3|x1, x2)\\n\\n(cid:54)=\\n\\np(x1 | x2)p(X3 | x1, x2) = p(X3 | x2)\\n\\n(4.3)\\n\\n(4.4)\\n\\nIt differs from the conditional p(X3 | x2) for which inference in done over G where X1 (cid:54)\\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 X2. Note the marginal\\np(x1) in (4.3), in contrast to the conditional p(x1 | x2) in (4.4): this is precisely the link which is broken by the\\nintervention do(X2 := x2), see Fig. 5b. The RHS of (4.3) is an example of covariate adjustment: it controls for the\\nconfounder X1 of the causal effect of X2 on X3, see \\xc2\\xa7 9 for more details on adjustment and computing interventions.\\n\\n(cid:88)\\n\\nx1\\xe2\\x88\\x88X1\\n(cid:88)\\n\\nx1\\xe2\\x88\\x88X1\\n\\n8\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nCGMs have been widely used in constraint- and score-based approaches to causal discovery [145, 47] which we\\nwill discuss in \\xc2\\xa7 7. Due to their conceptual simplicity, they are a useful and intuitive model for reasoning about\\ninterventions. However, their capacity as a causal model is limited in that they do not support counterfactual reasoning,\\nwhich is better addressed by the two causal modeling frameworks which we will discuss next.\\n\\nStructural Causal Models (SCMs) Structural Causal Models, also referred to as functional causal models or\\nnon-parametric structural equation models, have ties to the graphical approach presented above, but rely on using\\ndirected functional parent-child relationships rather than causal conditionals. While conceptually simple in hindsight,\\nthis constituted a major step in the understanding of causality, as later expressed by [105] (p. 104):\\n\\n\\xe2\\x80\\x9cWe played around with the possibility of replacing the parents-child relationship p(Xi | PAi)\\nwith its functional counterpart Xi = fi(PAi, Ui) and, suddenly, everything began to fall into place:\\nWe \\xef\\xac\\x81nally had a mathematical object to which we could attribute familiar properties of physical\\nmechanisms instead of those slippery epistemic probabilities p(Xi | PAi) with which we had been\\nworking so long in the study of Bayesian networks.\\xe2\\x80\\x9d\\n\\nDe\\xef\\xac\\x81nition 4.4 (SCM). An SCM M = (F, pU) over a set X of n random variables X1, . . . , Xn consists of (i) a set F\\nof n assignments (the structural equations),\\n\\nF = {Xi := fi(PAi, Ui)}n\\nwhere fi are deterministic functions computing each variable Xi from its causal parents PAi \\xe2\\x8a\\x86 X \\\\ {Xi} and an\\nexogenous noise variable Ui; and (ii) a joint distribution pU(U1, . . . , Un) over the exogenous noise variables.\\n\\n(4.5)\\n\\ni=1\\n\\nThe paradigm of SCMs views the processes fi by which each observable Xi is generated from others as a physical\\nmechanism. All randomness comes from the unobserved (also referred to as unexplained) noise terms Ui which\\ncapture both possible stochasticity of the process, as well as uncertainty due to unmeasured parts of the system.\\n\\nNote also the assignment symbol \\xe2\\x80\\x9c:=\\xe2\\x80\\x9d which is used instead of an equality sign to indicate the asymmetry of the causal\\nrelationship: the LHS quantity is de\\xef\\xac\\x81ned to take on the RHS value. For example, we cannot simply rewrite a structural\\nequation X2 := f2(X1, U2) as X1 = g(X2, U2) for some g, as would be the case for a standard (invertible) equation.\\n\\nIn parametric, linear form (i.e., with linear fi), SCMs are also known as structural equation models and have a long\\nhistory in path analysis [161] and economics [46, 52].\\n\\nEach SCM induces a corresponding causal graph via the input variables to the structural equations which is useful as\\na representation and provides a link to CGMs.\\nDe\\xef\\xac\\x81nition 4.5 (Induced causal graph). The causal graph G induced by an SCM M is the directed graph with vertex\\nset X and a directed edge from each vertex in PAi to Xi for all i.\\nExample 4.6. Consider an SCM over X = {X1, X2, X3} with some pU(U1, U2, U3) and\\n\\nX1 := f1(U1),\\n\\nX2 := f2(X1, U2),\\n\\nX3 := f3(X1, X2, U3).\\n\\nFollowing Defn. 4.5, the induced graph then corresponds to G in Fig. 5.\\n\\nDefn. 4.4 allows for a rich class of causal models, including ones with cyclic causal relations and ones which do not\\nobey the causal Markov condition (Defn. 4.2) due to complex covariance structures between the noise terms. While\\nwork exists on such cyclic or confounded SCMs [13], it is common to make the following two assumptions.\\nAssumption 4.7 (Acyclicity). The induced graph G is a DAG: it does not contain cycles.\\nAssumption 4.8 (Causal suf\\xef\\xac\\x81ciency/no hidden confounders). The Ui are jointly independent, i.e., their distribution\\nfactorises: pU(U1, . . . , Un) = pU1(U1) \\xc3\\x97 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xc3\\x97 pUn (Un).\\n\\nAssumption 4.7 implies5 the existence of a well-de\\xef\\xac\\x81ned, unique (observational) distribution over X from which we\\ncan draw via ancestral sampling:6 \\xef\\xac\\x81rst, we draw the noise variables from pU, and then we iteratively compute the\\ncorresponding Xi\\xe2\\x80\\x99s in topological order of the induced DAG (i.e., starting at the root node of the graph), substituting\\npreviously computed Xi into the structural equations where necessary.\\nFormally, the (observational) distribution\\np(X1, . . . , Xn) induced by an SCM under Assumption 4.7 is de\\xef\\xac\\x81ned as the push-forward of the noise distribution pU\\nthrough the structural equations F. Under Assumption 4.8, the causal conditionals are thus given by\\n\\np(Xi | PAi = pai) := pUi(f \\xe2\\x88\\x921\\npai\\n\\n(Xi))\\n\\nfor\\n\\ni = 1, . . . , n,\\n\\n(4.6)\\n\\n5Acyclicity is a suf\\xef\\xac\\x81cient, but not a necessary condition.\\n6Since neither F nor p are known a priori, ancestral sampling should be seen as a hypothetical sampling procedure; inference\\n\\nand learning are generally still necessary.\\n\\n9\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nwhere f \\xe2\\x88\\x921\\npai\\n\\n(Xi) denotes the pre-image of Xi under fi for \\xef\\xac\\x81xed PAi = pai.\\n\\nAssumption 4.8 rules out the existence of hidden confounders because any unmeasured variables affecting more than\\none of the Xi simultaneously would constitute a dependence between some of the noise terms (which account for any\\nexternal, or exogenous, in\\xef\\xac\\x82uences not explained by the observed Xi). In combination with Assumption 4.7, Assump-\\ntion 4.8 (also known as causal suf\\xef\\xac\\x81ciency), thus ensures that the distribution induced by an SCM factorises according\\nto its induced causal graph as in (4.1). In other words, it guarantees that the causal Markov condition is satis\\xef\\xac\\x81ed w.r.t.\\nthe induced causal graph [105]. Below, unless explicitly stated otherwise, we will assume causal suf\\xef\\xac\\x81ciency.\\n\\nDue to the conceptual similarity between interventions and the assignment character of structural equations, the com-\\nputation of interventional distributions \\xef\\xac\\x81ts in naturally into the SCM framework. To model an intervention, we simply\\nreplace the corresponding structural equation and consider the resulting entailed distribution.\\nDe\\xef\\xac\\x81nition 4.9 (Interventions in SCMs). An intervention do(Xi := xi) in an SCM M = (F, pU ) is modeled by\\nreplacing the ith structural equation in F by Xi := xi, yielding the intervened SCM Mdo(Xi:=xi) = (F(cid:48), pU ). The\\ninterventional distribution p(X\\xe2\\x88\\x92i | do(Xi = xi)), where X\\xe2\\x88\\x92i = X \\\\ {Xi}, and intervention graph G(cid:48) are those\\ninduced by Mdo(Xi=xi).\\n\\nThis way of handling interventions coincides with that for CGMs: e.g., after performing do(X2 := x2) in Ex. 4.6, X1\\nno longer appears in the structural equation for X2, and the edge X1 \\xe2\\x86\\x92 X2 hence disappears in the intervened graph,\\nas is the case for G(cid:48) in Fig. 5.\\n\\nIn contrast to CGMs, SCMs also provide a framework for counterfactual reasoning. While (i) observations describe\\nwhat is passively seen or measured and (ii) interventions describe active external manipulation or experimentation, (iii)\\ncounterfactuals are statements about what would or could have been, given that something else was in fact observed.\\nThese three modes of reasoning are sometimes referred to as the three rungs of the \\xe2\\x80\\x9cladder of causation\\xe2\\x80\\x9d [108]. As an\\nexample, consider the following counterfactual query:\\n\\nGiven that patient X received treatment A and their health got worse, what would have happened if\\nthey had been given treatment B instead, all else being equal?\\n\\nThe \\xe2\\x80\\x9call else being equal\\xe2\\x80\\x9d part highlights the difference between interventions and counterfactuals: observing the\\nfactual outcome (i.e., what actually happened) provides information about the background state of the system (as\\ncaptured by the noise terms in SCMs) which can be used to reason about alternative, counterfactual, outcomes. This\\ndiffers from an intervention where such background information is not available. For example, observing that treatment\\nA did not work may tell us that the patient has a rare condition and that treatment B would have therefore worked.\\nHowever, given that treatment A has been prescribed, the patient\\xe2\\x80\\x99s condition may have changed, and B may no longer\\nwork in a future intervention.\\n\\nNote that counterfactuals cannot be observed empirically by their very de\\xef\\xac\\x81nition and are therefore unfalsi\\xef\\xac\\x81able. Some\\ntherefore consider them unscienti\\xef\\xac\\x81c [116] or at least problematic [26]. On the other hand, humans seem to perform\\ncounterfactual reasoning in practice, developing this ability in early childhood [14].\\n\\nCounterfactuals are computed in SCMs through the following three-step procedure:\\n\\n1. Update the noise distribution to its posterior given the observed evidence (\\xe2\\x80\\x9cabduction\\xe2\\x80\\x9d).\\n2. Manipulate the structural equations to capture the hypothetical intervention (\\xe2\\x80\\x9caction\\xe2\\x80\\x9d).\\n3. Use the modi\\xef\\xac\\x81ed SCM to infer the quantity of interest (\\xe2\\x80\\x9cprediction\\xe2\\x80\\x9d).\\n\\nDe\\xef\\xac\\x81nition 4.10 (Counterfactuals in SCMs). Given evidence X = x observed from an SCM M = (F, pU ), the\\ncounterfactual SCM MX=x is obtained by updating pU with its posterior: MX=x = (F, pU |X=x). Counterfactuals\\nare then computed by performing interventions in the counterfactual SCM MX=x, see Defn. 4.9.\\n\\nNote that while computing interventions only involved manipulating the structural equations, counterfactuals also\\ninvolve updating the noise distribution, highlighting the conceptual difference between the two. Updating pU requires\\nknowledge of the interaction between noise and observed variables, i.e., of the structural equations, which explains\\nwhy additional assumptions are necessary. Note that the updated noise variables no longer need to be independent,\\neven if the original system was causally suf\\xef\\xac\\x81cient (Assumption 4.8).\\nExample 4.11 (Computing counterfactuals with SCMs). Consider an SCM M de\\xef\\xac\\x81ned by\\n\\nX := UX ,\\n\\nY := 3X + UY ,\\n\\nUX , UY \\xe2\\x88\\xbc N(0, 1).\\n\\n(4.7)\\n\\nSuppose we observe X = 2 and Y = 6.5 and want to answer the counterfactual \\xe2\\x80\\x9cwhat would Y have been, had\\nX = 1?\\xe2\\x80\\x9d, i.e., we are interested in p(YX=1 | X = 2, Y = 6.5). Updating the noise using the observed evidence\\n\\n10\\n\\n\\x0cTable 1: Causal inference as a missing data problem: for each individ-\\nual i (rows), only the PO Yi(Ti) corresponding to the assigned treat-\\nment Ti is observed; the other PO is a counterfactual. Hence, the unit-\\nlevel causal effect \\xcf\\x84i = Yi(1) \\xe2\\x88\\x92 Yi(0) is non-identi\\xef\\xac\\x81able.\\n\\nFROM STATISTICAL TO CAUSAL LEARNING\\n\\ni\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n\\nTi\\n1\\n0\\n1\\n1\\n0\\n0\\n\\nYi(1)\\n7\\n?\\n3\\n6\\n?\\n?\\n\\nYi(0)\\n?\\n8\\n?\\n?\\n4\\n1\\n\\n\\xcf\\x84i\\n?\\n?\\n?\\n?\\n?\\n?\\n\\nvia (4.7), we obtain the counterfactual SCM MX=2,Y =6.5,\\n\\nX := UX ,\\n\\nY := 3X + UY ,\\n\\nUX \\xe2\\x88\\xbc \\xce\\xb4(2),\\n\\nUY \\xe2\\x88\\xbc \\xce\\xb4(0.5),\\n\\n(4.8)\\n\\nwhere \\xce\\xb4(\\xc2\\xb7) denotes the Dirac delta measure. Performing the intervention do(X := 1) in (4.8) then gives the\\nresult p(YX=1 | X = 2, Y = 6.5) = \\xce\\xb4(3.5), i.e., \\xe2\\x80\\x9cY would have been 3.5\\xe2\\x80\\x9d. This differs from the interventional\\ndistribution p(Y | do(X = 1)) = N(3, 1), since the factual observation helped determine the background\\nstate (UX = 2, UY = 0.5).\\n\\nThe SCM viewpoint is intuitive and lends itself well to studying restrictions on function classes to enable induction\\n(\\xc2\\xa7 2). For this reason, we will mostly focus on SCMs in the subsequent sections.\\n\\nPotential Outcomes (PO) The potential outcomes framework was initially proposed by Neyman [98] for random-\\nized studies [31], and later popularized and extended to observational settings by Rubin [125] and others. It is popular\\nwithin statistics and epidemiology and perhaps best understood in the context of the latter. This is also re\\xef\\xac\\x82ected in\\nits terminology: in the most common setting, we consider a binary treatment variable T , with T = 1 and T = 0\\ncorresponding to treatment and control, respectively, whose causal effect on an outcome variable Y (often a measure\\nof health) is of interest.\\n\\nOne interpretation of the PO framework consistent with its roots in statistics is to view causal inference as a missing\\ndata problem. In the PO framework, for each individual (or unit) i and treatment value t there is a PO, or potential\\nresponse, denoted Yi(t) capturing what would happen if individual i received treatment t. The POs are considered\\ndeterministic quantities in the sense that for a given individual i, Yi(1) and Yi(0) are \\xef\\xac\\x81xed and all randomness in the\\nrealized outcome Yi stems from randomness in the treatment assignment:\\n\\nTo decide whether patient i should receive treatment, we need to reason about the individualized treatment effect (ITE)\\n\\xcf\\x84i as captured by the difference of the two POs.\\nDe\\xef\\xac\\x81nition 4.12 (ITE). The ITE for individual i under a binary treatment is de\\xef\\xac\\x81ned as\\n\\nThe \\xe2\\x80\\x9cfundamental problem of causal inference\\xe2\\x80\\x9d [51] is that only one of the POs is ever observed for each i. The other,\\nunobserved PO becomes a counterfactual:\\n\\nConsequently, \\xcf\\x84i can never be measured or computed from data, i.e., it is not identi\\xef\\xac\\x81able (without further assumptions),\\nas illustrated in Tab. 1.\\n\\nImplicit in the form of (4.9) and (4.11) are the following two assumptions.\\nAssumption 4.13 (Stable unit treatment value; SUTVA). The observation on one unit should be unaffected by the\\nparticular assignment of treatments to the other units [23].\\nAssumption 4.14 (Consistency). If individual i receives treatment t, then the observed outcome is Yi = Yi(t), i.e.,\\nthe potential outcome for t.\\n\\nAssumption 4.13 is usually understood as (i) units do not interfere, and (ii) there is only one treatment level per group\\n(treated or control) leading to well-de\\xef\\xac\\x81ned POs [61]. It can be violated, e.g., through (i) population dynamics such\\nas herd immunity from vaccination or (ii) technical errors or varying within-group dosage, respectively. However, for\\nmany situations such as controlled studies it can be a reasonable assumption, and we can then view different units as\\nindependent samples from a population.\\n\\n11\\n\\nYi = T Yi(1) + (1 \\xe2\\x88\\x92 T )Yi(0).\\n\\n\\xcf\\x84i = Yi(1) \\xe2\\x88\\x92 Yi(0).\\n\\nY CF\\ni = (1 \\xe2\\x88\\x92 T )Yi(1) + T Yi(0).\\n\\n(4.9)\\n\\n(4.10)\\n\\n(4.11)\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nSo far, we have considered POs for a given unit as deterministic quantities. However, most times it is impossible to\\nfully characterize a unit, e.g., when dealing with complex subjects such as humans. Such lack of complete information\\nintroduces uncertainty, so that POs are often instead treated as random variables. This parallels the combination of\\ndeterministic structural equations with exogenous noise variables in SCMs.7 Indeed, there is a equivalence between\\nPOs and SCMs [105]:\\n\\nYi(t) = Y | do(T = t)\\n\\nin an SCM with\\n\\nU = ui,\\n\\nAn individual in the PO framework thus corresponds to a particular instantiation of the Uj in an SCM: the outcome is\\ndeterministic given U, but since we do not observe ui (nor can we characterize a given individual based on observed\\ncovariates), the counterfactual outcome is treated as a random variable. In practice, all we observe is a featurised\\ndescription xi of an individual i and have to reason about expected POs, E[Y (1), Y (0) | xi].\\nAnother common assumption is that of no hidden confounders which we have already encountered in form of the\\ncausal Markov condition (Defn. 4.2) for CGMs and causal suf\\xef\\xac\\x81ciency (Assumption 4.8) for SCMs. In the PO frame-\\nwork this becomes no hidden confounding between treatment and outcome and is referred to as (conditional) ignora-\\nbility.\\nAssumption 4.15 (Conditional ignorability). Given a treatment T \\xe2\\x88\\x88 {0, 1}, potential outcomes Y (0), Y (1), and\\nobserved covariates X, we have:\\n\\nY (0) \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 T | X and Y (1) \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 T | X.\\n\\n(4.12)\\n\\nThe PO framework is tailored toward studying the (confounded) effect of a typically binary treatment variable on an\\noutcome and is mostly used for causal reasoning, i.e., estimating individual and population level causal effects (\\xc2\\xa7 9).\\nIn this context, it is sometimes seen as an advantage that an explicit graphical representation is not needed. At the same\\ntime, the lack of a causal graph and the need for special treatment and outcome variables make POs rather unsuitable\\nfor causal discovery where other frameworks prevail.\\n\\n5\\n\\nIndependent Causal Mechanisms\\n\\nLet us consider the disentangled factorization (4.1) of the joint distribution p(X1, . . . , Xn). This factorization ac-\\ncording to the causal graph is possible whenever the Ui are independent. We now consider an additional notion of\\nindependence, concerning how the factors in (4.1) relate to one another.\\n\\nConsider a dataset that consists of altitude A and average annual temperature T of weather stations [112]. Supposed\\nwe \\xef\\xac\\x81nd that A and T are correlated, which we attribute due to the fact that the altitude has a causal effect on the\\ntemperature. Suppose we had two such datasets, one for Austria and one for Switzerland. The two joint distributions\\nmay be different, since the marginal distributions p(A) over altitudes will likely differ. The conditionals p(T | A),\\nhowever, may be rather similar, since they re\\xef\\xac\\x82ect physical mechanisms generating temperature from altitude. The\\ncausal factorization p(A)p(T | A) thus contains a component p(T | A) that generalizes across countries, while the\\nentangled factorization p(T )p(A | T ) does not. A similar reasoning applies when we consider interventions in a\\nsystem. For a model to correctly predict the effect of interventions, it needs to have components that are robust when\\nmoving from an observational distribution to certain interventional distributions.\\n\\nThe above insights can be stated as follows [130, 112]:\\nPrinciple 5.1 (Independent Causal Mechanisms (ICM)). The causal generative process of a system\\xe2\\x80\\x99s variables is\\ncomposed of autonomous modules that do not inform or in\\xef\\xac\\x82uence each other. In the probabilistic case, this means\\nthat the conditional distribution of each variable given its causes (i.e., its mechanism) does not inform or in\\xef\\xac\\x82uence the\\nother mechanisms.\\n\\nThis principle subsumes several notions important to causality, including separate intervenability of causal variables,\\nmodularity and autonomy of subsystems, and invariance [105, 112]. In the two-variable case, it reduces to an inde-\\npendence between the cause distribution and the mechanism producing the effect from the cause.\\n\\nApplied to the causal factorization (4.1), the principle tells us that the factors should be independent in two senses:\\n\\n(in\\xef\\xac\\x82uence)\\n\\n(information)\\n\\nchanging (or intervening upon) one mechanism p(Xi | PAi) does not change the other mechanisms\\np(Xj | PAj) (i (cid:54)= j), and\\nknowing some mechanisms p(Xi | PAi) (i (cid:54)= j) does not give us information about a mechanism\\np(Xj | PAj).\\n\\n7When all noise variables in an SCM are \\xef\\xac\\x81xed, the other variables are uniquely determined; without complete background\\n\\nknowledge, on the other hand, they are random.\\n\\n12\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nIf p(x) and f are\\nFigure 6:\\nchosen independently,\\nthen\\npeaks of p(y) tend to occur\\nin regions where f has small\\nslope. As a result, p(y) con-\\ntains information about f \\xe2\\x88\\x921\\n(\\xef\\xac\\x81gure from [112]).\\n\\ny\\n\\nf (x)\\n\\np\\n(\\ny\\n)\\n\\nx\\n\\np(x)\\n\\nWe view any real-world distribution as a product of causal mechanisms. A change in such a distribution (e.g., when\\nmoving from one setting/domain to a related one) will always be due to a change in at least one of those mechanisms.\\nConsistent with Principle 5.1, we hypothesize [131]:\\n\\nPrinciple 5.2 (Sparse Mechanism Shift (SMS)). Small distribution changes tend to manifest themselves in a sparse or\\nlocal way in the causal/disentangled factorization (4.1), i.e., they should usually not affect all factors simultaneously.\\n\\nIn contrast, in a non-causal factorization, e.g., (4.2), many terms will be affected simultaneously if we change one\\nof the physical mechanisms responsible for a system\\xe2\\x80\\x99s statistical dependences. Such a factorization may thus be\\ncalled entangled. The notion of disentanglement has recently gained popularity in machine learning [9, 50, 82, 148],\\nsometimes loosely identi\\xef\\xac\\x81ed with statistical independence. The notion of invariant, autonomous, and independent\\nmechanisms has appeared in various guises throughout the history of causality research, see [1, 105, 130, 112, 131].\\n\\nMeasures of Dependence of Mechanisms Note that the dependence of two mechanisms p(Xi | PAi) and p(Xj |\\nPAj) does not coincide with the statistical dependence of the random variables Xi and Xj. Indeed, in a causal graph,\\nmany of the random variables will be dependent even if all the mechanisms are independent.\\n\\nConsider two variables and structural assignments X := U and Y := f (X). I.e., the cause X is a noise variable (with\\ndensity p(x)), and the effect Y is a deterministic function of the cause. Let us moreover assume that the ranges of X\\nand Y are both [0, 1], and f is strictly monotonically increasing. The ICM principle then reduces to an independence\\nof p(x) and f . Let us consider p(x) and the derivative f (cid:48) as random variables on the probability space [0, 1] with\\nLebesgue measure, and use their correlation as a measure of dependence of mechanisms. It can be shown that for\\nf (cid:54)= id, independence of p(x) and f (cid:48) implies dependence between p(y) and (f \\xe2\\x88\\x921)(cid:48) (see Fig. 6). Other measures are\\npossible and admit information-geometric interpretations. Intuitively, under the ICM assumption (Principle 5.1), the\\n\\xe2\\x80\\x9cirregularity\\xe2\\x80\\x9d of the effect distribution becomes a sum of (i) irregularity already present in the input distribution and (ii)\\nirregularity introduced by the mechanism f , i.e., the irregularities of the two mechanisms add up rather than (partly)\\ncompensating each other. This would not be the case in the opposite (\\xe2\\x80\\x9canticausal\\xe2\\x80\\x9d) direction (for details, see [64]).\\nOther dependence measures have been proposed for high-dimensional linear settings and time series [63, 137, 12, 68].\\n\\nAlgorithmic Independence So far, we have discussed links between causal and statistical structures. The fundamen-\\ntal of the two is the causal structure, since it captures the physical mechanisms that generate statistical dependences in\\nthe \\xef\\xac\\x81rst place. The statistical structure is an epiphenomenon that follows if we make the unexplained variables random.\\nIt is awkward to talk about the (statistical) information contained in a mechanism, since deterministic functions in the\\ngeneric case neither generate nor destroy information. This motivated us to devise an algorithmic model of causal\\nstructures in terms of Kolmogorov complexity [66]. The Kolmogorov complexity (or algorithmic information) of a bit\\nstring is essentially the length of its shortest compression on a Turing machine, and thus a measure of its information\\ncontent. Independence of mechanisms can be de\\xef\\xac\\x81ned as vanishing mutual algorithmic information: two conditionals\\nare considered independent if knowing (the shortest compression of) one does not help achieve a shorter compression\\nof the other one.\\n\\nAlgorithmic information theory is an elegant framework for non-statistical graphical models. Just like statistical CGMs\\nare obtained from SCMs by making the unexplained variables Ui random, we obtain algorithmic CGMs by turning the\\nUi into bit strings (jointly independent across nodes), and viewing the node Xi as the output of a \\xef\\xac\\x81xed Turing machine\\nrunning the program Ui with input PAi. Similar to the statistical case, one can de\\xef\\xac\\x81ne a local causal Markov condition,\\na global one in terms of d-separation, and a decomposition of the joint Kolmogorov complexity in analogy to (4.1),\\nand prove that they are implied by the SCM [66]. This approach shows that concepts of causality are not intrinsically\\ntied to statistics: causality is about mechanisms governing \\xef\\xac\\x82ow of information which may or may not be statistical.\\n\\n13\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nTable 2: Simple model taxonomy. The most detailed model (top) is a mechanistic or physical one, often in terms\\nof differential equations. At the other end of the spectrum (bottom), we have a purely statistical model; this can be\\nlearned from data and is useful for predictions but often provides little insight beyond modeling associations between\\nepiphenomena. Causal models can be seen as intermediate descriptions, abstracting away from physical realism while\\nretaining the power to answer certain interventional or counterfactual questions.\\n\\nModel\\n\\nPredict in i.i.d. Predict under distr. Answer counter-\\nfactual questions\\nshift/intervention\\n\\nsetting\\n\\nObtain\\nphysical insight\\n\\nLearn from\\ndata\\n\\nMechanistic/physical\\nStructural causal\\nCausal graphical\\nStatistical\\n\\nyes\\nyes\\nyes\\nyes\\n\\nyes\\nyes\\nyes\\nno\\n\\nyes\\nyes\\nno\\nno\\n\\nyes\\n?\\n?\\nno\\n\\n?\\n?\\n?\\nyes\\n\\nThe assumption of algorithmically independent mechanisms has interesting implications for physics:\\nit implies\\nthe second law of thermodynamics (i.e., the arrow of time). Consider a process where an incoming ordered beam\\nof photons (the cause) is scattered by an object (the mechanism). Then the outgoing beam (the effect) contains\\ninformation about the object. Microscopically, the time evolution is reversible; however, the photons contain\\ninformation about the object only after the scattering. What underlies Loschmidt\\xe2\\x80\\x99s paradox [86]?\\n\\nThe asymmetry can be explained by applying the ICM Principle 5.1 to initial state and system dynamics, postulating\\nthat the two be algorithmically independent, i.e., knowing one does not allow a shorter description of the other one.\\nThe Kolmogorov complexity of the system\\xe2\\x80\\x99s state can then be shown to be non-decreasing under time evolution [62].\\nIf we view Kolmogorov complexity as a measure of entropy, this means that the entropy of the state can only stay\\nconstant or increase, amounting to the second law of thermodynamics.\\n\\nNote that the resulting state after time evolution is clearly not independent of the system dynamic: it is precisely the\\nstate that, when fed to the inverse dynamics, would return us to the original (ordered) state.\\n\\n6 Levels of Causal Modeling\\n\\nCoupled differential equations are the canonical way of modeling physical phenomena. They allow us to predict the\\nfuture behavior of a system, to reason about the effect of interventions, and\\xe2\\x80\\x94by suitable averaging procedures\\xe2\\x80\\x94to\\npredict statistical dependences that are generated by a coupled time evolution. They also allow us to gain insight into\\na system, explain its functioning, and, in particular, read off its causal structure.\\n\\nConsider a coupled set of ordinary differential equations\\n\\ndx\\ndt\\n\\n= f (x), x \\xe2\\x88\\x88 Rd,\\n\\n(6.1)\\n\\n(6.2)\\n\\nwith initial value x(t0) = x0. We assume that they correctly describe the physical mechanisms of a system.8 The\\nPicard\\xe2\\x80\\x93Lindel\\xc2\\xa8of theorem states that, at least locally, if f is Lipschitz, there exists a unique solution x(t). This implies,\\nin particular, that the immediate future of x is implied by its past values.\\n\\nIn terms of in\\xef\\xac\\x81nitesimal differentials dt and dx = x(t + dt) \\xe2\\x88\\x92 x(t), (6.1) reads:\\n\\nx(t + dt) = x(t) + dt \\xc2\\xb7 f (x(t)).\\n\\nFrom this, we can ascertain which entries of the vector x(t) cause the future of others x(t + dt), i.e., the causal\\nstructure.\\n\\nCompared to a differential equation, a statistical model derived from the joint distribution of a set of (time-independent)\\nrandom variables is a rather super\\xef\\xac\\x81cial description of a system.\\nIt exploits that some of the variables allow the\\nprediction of others as long as the experimental conditions do not change. If we drive a differential equation system\\nwith certain types of noise, or if we average over time, statistical dependences between components of x may emerge,\\nwhich can be exploited by machine learning. In contrast to the differential equation model, such a model does not\\nallow us to predict the effect of interventions; however, its strength is that it can often be learned from data.\\n\\n8I.e., they do not merely phenomenologically describe its time evolution without capturing the underlying mechanisms (e.g.,\\n\\ndue to unobserved confounding, or a form of coarse-graining that does not preserve the causal structure [124, 131]).\\n\\n14\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nIt aims to provide understanding and predict the effect of\\nCausal modeling lies in between these two extremes.\\ninterventions. Causal discovery and learning tries to arrive at such models in a data-driven way, using only weak\\nassumptions (see Table 2, from [112, 131]).\\n\\nWhile we may naively think that causality is always about time, most existing causal models do not (and need not)\\nconsider time. For instance, returning to our example of altitude and temperature, there is an underlying dynamical\\nphysical process that results in higher places tending to be colder. On the level of microscopic equations of motion\\nfor the involved particles, there is a temporal causal structure. However, when we talk about dependence or causality\\nbetween altitude and temperature, we need not worry about the details of this temporal structure; we are given a\\ndataset where time does not appear, and we can reason about how that dataset would look if we were to intervene on\\ntemperature or altitude.\\n\\nSome work exists trying to build bridges between these different levels of description. One can derive SCMs that\\ndescribe the interventional behavior of a coupled system that is in an equilibrium state and perturbed in an adiabatic\\nway [96], with generalizations to oscillatory systems [123]. In this work, an SCM arises as a high-level abstraction of\\nan underlying system of differential equations. It can only be derived if suitable high-level variables can be de\\xef\\xac\\x81ned\\n[124], which in practice may well be the exception rather than the rule.\\n\\n7 Causal Discovery\\n\\nSometimes, domain knowledge or the temporal ordering of events can help constrain the causal relationships between\\nvariables: e.g., we may know that certain attributes like age or sex are not caused by others; treatments in\\xef\\xac\\x82uence health\\noutcomes; and events do not causally in\\xef\\xac\\x82uence their past. When such domain knowledge is unavailable or incomplete,\\nwe need to perform causal discovery: infer which variables causally in\\xef\\xac\\x82uence which others, i.e., learn the causal\\nstructure (e.g., a DAG) from data. Since experiments are often dif\\xef\\xac\\x81cult and expensive to perform while observational\\n(i.e., passively collected) data is abundant, causal discovery from observational data is of particular interest.\\n\\nAs discussed in \\xc2\\xa7 3 in the context of the Common Cause Principle 3.2, the case where we have two variables is already\\ndif\\xef\\xac\\x81cult since the same dependence can be explained by multiple different causal structures. One might thus wonder\\nif the case of more observables is completely hopeless. Surprisingly, this is not the case: the problem becomes easier\\n(in a certain sense) because there are nontrivial conditional independence properties [146, 25, 35] implied by a causal\\nstructure. We \\xef\\xac\\x81rst review two classical approaches to the multi-variate setting before returning to the two-variable case.\\n\\nConstraint-Based Methods Constraint-based approaches to causal discovery test which (conditional) indepen-\\ndences can be inferred from the data and then try to \\xef\\xac\\x81nd a graph which implies them. They are therefore also known\\nas independence-based methods. Such a procedure requires a way of linking properties of the data distribution p to\\nproperties of the underlying causal graph G. This link is known as the faithfulness assumption.\\n\\nAssumption 7.1 (Faithfulness). The only (conditional) independences satis\\xef\\xac\\x81ed by p are those implied by the causal\\nMarkov condition (Defn. 4.2).\\n\\nFaithfulness can be seen as the converse of the causal Markov condition. Together, they constitute a one-to-one\\ncorrespondence between graphical separation in G and conditional independence in p. While the causal Markov\\ncondition is satis\\xef\\xac\\x81ed by construction, faithfulness is an assumption which may be violated. A classical example for a\\nviolation of faithfulness is when causal effects along different paths cancel.\\n\\nExample 7.2 (Violation of faithfulness). Consider the SCM from Ex. 4.6 and let\\n\\nX1 := U1,\\n\\nX2 := \\xce\\xb1X1 + U2,\\n\\nX3 := \\xce\\xb2X1 + \\xce\\xb3X2 + U3\\n\\ni.i.d.\\xe2\\x88\\xbc N(0, 1). By substitution, we obtain X3 = (\\xce\\xb2 + \\xce\\xb1\\xce\\xb3)X1 + \\xce\\xb3U2 + U3. Hence X3 \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 X1 whenever\\nwith U1, U2, U3\\n\\xce\\xb2 + \\xce\\xb1\\xce\\xb3 = 0, even though this independence is not implied by the causal Markov condition over the induced causal\\ngraph G, see Fig. 5. Here, faithfulness is violated if the direct effect of X1 on X3 (\\xce\\xb2) and the indirect effect via X2\\n(\\xce\\xb1\\xce\\xb3) cancel.\\n\\nApart from relying on faithfulness, a fundamental limitation to constraint-based methods is the fact that many different\\nDAGs may encode the same d-separation / independence relations. This is referred to as Markov equivalence and\\nillustrated in Fig. 7.\\n\\nDe\\xef\\xac\\x81nition 7.3 (Markov equivalence). Two DAGs are said to be Markov equivalent if they encode the same d-\\nseparation statements. The set of all DAGs encoding the same d-separations is called a Markov equivalence class.\\n\\n15\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nX\\n\\nX\\n\\nY\\n\\nY\\n\\nZ\\n\\nZ\\n\\nY\\n\\nX\\n\\nZ\\n\\n(a) Chains\\n\\n(b) Fork\\n\\nX\\n\\nZ\\n\\nY\\n\\n(c) Collider\\n\\nFigure 7: Illustration of Markov equivalence using common graph motifs. The chains in (a) and the fork in (b) all\\nimply the relation X \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 Z | Y (and no others). They thus form a Markov equivalence class, meaning they cannot be\\ndistinguished using conditional independence testing alone. The collider, or v-structure, in (c) implies X \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 Z (but\\nX (cid:54)\\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 Z | Y ) and forms its own Markov equivalence class, so it can be uniquely identi\\xef\\xac\\x81ed from observational data.\\nFor this reason, v-structures are helpful for causal discovery. It can be shown that two graphs are Markov equivalent\\niff. they share the same skeleton and v-structures.\\n\\nConstraint-based algorithms typically \\xef\\xac\\x81rst construct an undirected graph, or skeleton, which captures the (conditional)\\nindependences found by testing, and then direct as many edges as possible using Meek\\xe2\\x80\\x99s orientation rules [90]. The\\n\\xef\\xac\\x81rst step carries most of the computational weight and various algorithms have been devised to solve it ef\\xef\\xac\\x81ciently.\\n\\nThe simplest procedure is implemented in the IC [110] and SGS [145] algorithms. For each pair of variables (X, Y ),\\nthese search through all subsets W of the remaining variables to check whether X \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 Y | W. If no such set W is\\nfound, then X and Y are connected with an edge. Since this can be slow due to the large number of subsets, the PC\\nalgorithm [145] uses a much more ef\\xef\\xac\\x81cient search procedure. It starts from a complete graph and then sequentially test\\nonly subsets of the neighbors of X or Y of increasing size, removing an edge when a separating subset is found. This\\nneighbor search is no longer guaranteed to give the right result for causally insuf\\xef\\xac\\x81cient systems, i.e., in the presence\\nof hidden confounders. The FCI (for Fast Causal Inference) algorithm [145] addresses this setting, and produces a\\npartially directed causal graph as output.\\n\\nApart from being limited to recovering a Markov equivalence class, constraint-based methods can suffer from statis-\\ntical issues. In practice, datasets are \\xef\\xac\\x81nite, and conditional independence testing is a notoriously dif\\xef\\xac\\x81cult problem,\\nespecially if conditioning sets are continuous and multi-dimensional. So while in principle, the conditional indepen-\\ndences implied by the causal Markov condition hold true irrespective of the complexity of the functions appearing\\nin an SCM, for \\xef\\xac\\x81nite datasets, conditional independence testing is hard without additional assumptions [136]. Re-\\ncent progress in (conditional) independence testing heavily relies on kernel function classes to represent probability\\ndistributions in reproducing kernel Hilbert spaces, see \\xc2\\xa7 2.\\n\\nScore-Based Methods Score-based approaches to causal discovery assign a score to each graph G from a set of\\ncandidate graphs (usually the set of all DAGs). The score S is supposed to re\\xef\\xac\\x82ect how well G explains the observed\\ndata D = {x1, . . . , xm}, and we choose the graph \\xcb\\x86G maximizing this score,\\n\\n\\xcb\\x86G = argmax\\n\\nS(G | D).\\n\\nG\\n\\nVarious score functions have been proposed, but most methods assume a parametric model which factorises according\\nto G, parametrised by \\xce\\xb8 \\xe2\\x88\\x88 \\xce\\x98. Two common choices are multinomial models for discrete data [22] and linear Gaussian\\nmodels for continuous data [34]. E.g., a penalized maximum likelihood approach using the BIC [135] as a score yields\\n\\nSBIC(G | D) = log p(D | G, \\xcb\\x86\\xce\\xb8MLE) \\xe2\\x88\\x92\\n\\nlog m\\n\\n(7.1)\\n\\nk\\n2\\n\\nwhere k is the number of parameters and \\xcb\\x86\\xce\\xb8MLE is the maximum likelihood estimate for \\xce\\xb8 to D in G. Note that k\\ngenerally increases with the number of edges in G so that the second term in (7.1) penalizes complex graphs which do\\nnot lead to substantial improvements.\\n\\nAnother choice of score function is the marginal likelihood, or evidence, in a Bayesian approach to causal discovery,\\nwhich requires specifying prior distributions over graphs and parameters, p(G, \\xce\\xb8) = p(G)p(\\xce\\xb8 | G). The score for G is\\nthen given by\\n\\nSBAYES(G | D) = p(D | G) =\\n\\np(D | G, \\xce\\xb8)p(\\xce\\xb8 | G)d \\xce\\xb8.\\n\\n(7.2)\\n\\nThis integral is intractable in general, but can be computed exactly for some models such as a Dirichlet-multinomial\\nunder some mild additional assumptions [47, 48].\\n\\n(cid:90)\\n\\n\\xce\\x98\\n\\n16\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nA major drawback of score-based approaches is the combinatorial size of the search space. The number of DAGs\\nover n random variables grows super-exponentially and can be computed recursively (to account for acyclicity con-\\nstraints) [120]. E.g., the number of DAGs for n = 5 and n = 10 nodes is 29281 and 4175098976430598143,\\nrespectively. Finding the best scoring DAG is NP-hard [20]. To overcome this problem, greedy search techniques can\\nbe applied, e.g., greedy equivalence search (GES) [21] which optimizes for the BIC.\\n\\nIn recent years, another class of methods has emerged that is based on assuming particular functional forms for the\\nSCM assignments. Those arose from studying the cause-effect inference problem, as discussed below.\\n\\nCause-Effect Inference.\\nIn the case of only two variables, the ternary concept of conditional independences col-\\nlapses and the causal Markov condition (Defn. 4.2) thus has no nontrivial implications. However, we have seen in \\xc2\\xa7 5\\nthat assuming an independence of mechanisms (Principle 5.1) lets us \\xef\\xac\\x81nd asymmetries between cause and effect, and\\nthus address the cause-effect inference problem previously considered unsolvable [64]. It turns out that this problem\\ncan be also addressed by making additional assumptions on function classes, as not only the graph topology leaves\\na footprint in the observational distribution, but so do the functions fi in an SCM. Such assumptions are typical for\\nmachine learning, where it is well-known that \\xef\\xac\\x81nite-sample generalization without assumptions on function classes is\\nimpossible, and where much attention is devoted to properties of function classes (e.g., priors or capacity measures),\\nas discussed in \\xc2\\xa7 2.\\n\\nLet us provide an intuition as to why assumptions on the functions in an SCM should help learn about them from data.\\nConsider a toy SCM with only two observables X \\xe2\\x86\\x92 Y . In this case, the structural equations (4.5) turn into\\n\\nX := U,\\n\\nY := f (X, V )\\n\\n(7.3)\\n\\n(7.4)\\n\\nwith noises U \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 V . Now think of V acting as a random selector variable choosing from among a set of functions\\nF = {fv(x) \\xe2\\x89\\xa1 f (x, v) | v \\xe2\\x88\\x88 supp(V )}. If f (x, v) depends on v in a non-smooth way, it should be hard to glean\\ninformation about the SCM from a \\xef\\xac\\x81nite dataset, given that V is not observed and it randomly switches between\\narbitrarily different fv.9 This motivates restricting the complexity with which f depends on V . A natural restriction is\\nto assume an additive noise model\\n\\nX := U,\\n\\nY := f (X) + V.\\n\\nIf f in (7.3) depends smoothly on V , and if V is relatively well concentrated, this can be motivated by a local Taylor\\nexpansion argument. Such assumptions drastically reduce the effective size of the function class\\xe2\\x80\\x94without them, the\\nlatter could depend exponentially on the cardinality of the support of V .\\n\\nRestrictions of function classes can break the symmetry between cause and effect in the two-variable case: one can\\nshow that given a distribution over X, Y generated by an additive noise model, one cannot \\xef\\xac\\x81t an additive noise model\\nin the opposite direction (i.e., with the roles of X and Y interchanged) [53, 95, 114, 76, 6]. This is subject to certain\\ngenericity assumptions, and notable exceptions include the case where U, V are Gaussian and f is linear. It general-\\nizes results of [140] for linear functions, and it can be generalized to include nonlinear rescaling [164], cycles [94],\\nconfounders [65], and multi-variable causal discovery [113]. There is now a range of methods that can detect causal\\ndirection better than chance [97].\\n\\nWe have thus gathered some evidence that ideas from machine learning can help tackle causality problems that were\\npreviously considered hard. Equally intriguing, however, is the opposite direction: can causality help us improve\\nmachine learning?\\n\\nNonstationarity-Based Methods The last family of causal discovery approaches we mention is based on ideas of\\nnonstationarity and invariance [130]. These approaches do not apply to purely observational data collected in an\\ni.i.d. setting. In contrast, they aim to leverage heterogeneity of data collected from different environments. The main\\nidea is the following: since causal systems are modular in the sense of the ICM Principle 5.1, changing one of the\\nindependent mechanisms should leave the other components, or causal conditionals, unaffected (SMS Principle 5.2).\\nA correct factorization of the joint distribution according to the underlying causal structure should thus be able to\\nexplain heterogeneity by localized changes in one (or few) of the mechanisms while the others remain invariant.\\n\\nOne of the \\xef\\xac\\x81rst works to use this idea [151] analyzed which causal structures can be distinguished given data resulting\\nfrom a set of mechanism changes. Recent work [54] additionally aims to learn a low-dimensional representation of\\nthe mechanism changes. Other works [111, 121] have proposed methods for \\xef\\xac\\x81nding the direct causes of a given target\\nvariable. Using a recent result on identi\\xef\\xac\\x81ability of non-linear ICA [59] which also relies on non-stationarity, a method\\n\\n9Suppose X and Y are binary, and U, V are uniform Bernoulli variables, the latter selecting from F = {id, not} (i.e., identity\\nand negation). In this case, the entailed distribution for Y is uniform, independent of X, even though we have X \\xe2\\x86\\x92 Y . We would\\nbe unable to discern X \\xe2\\x86\\x92 Y from data. (This would also constitute a violation of faithfulness (Assumption 7.1)).\\n\\n17\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nfor learning general non-linear SCMs was proposed [93]. Here the idea is to train a classi\\xef\\xac\\x81er to discriminate between\\nthe true value of some nonstationarity variable (such as a time-stamp or environment indicator) and a shuf\\xef\\xac\\x82ed version\\nthereof.\\n\\n8\\n\\nImplications for Machine Learning\\n\\nSemi-Supervised Learning Suppose our underlying causal graph is X \\xe2\\x86\\x92 Y , and we wish to learn a mapping\\nX \\xe2\\x86\\x92 Y . The causal factorization (4.1) in this case is\\n\\np(X, Y ) = p(X) p(Y | X).\\n\\n(8.1)\\n\\nThe ICM Principle 5.1 posits that the modules in a joint distribution\\xe2\\x80\\x99s causal factorization do not inform or in\\xef\\xac\\x82uence\\neach other. This means that, in particular, p(X) should contain no information about p(Y | X), which implies that\\nsemi-supervised learning [17] should be futile, as it is trying to use additional information about p(X) (from unlabeled\\ndata) to improve our estimate of p(Y | X = x). How about the opposite direction? Is there hope that semi-supervised\\nlearning should be possible in that case? It turns out the answer is yes, due to work on cause-effect inference using\\nthe ICM Principle 5.1 [24]. It introduced a measure of dependence between the input and the conditional of output\\ngiven input, and showed that if this dependence is zero in the causal direction, then it is strictly positive in the opposite\\ndirection. Independence of cause and mechanism in the causal direction thus implies that in the backward direction\\n(i.e., for anticausal learning), the distribution of the input variable should contain information about the conditional\\nof output given input, i.e., the quantity that machine learning is usually concerned with. This is exactly the kind of\\ninformation that semi-supervised learning requires when trying to improve the estimate of output given input by using\\nunlabeled inputs. This suggests that semi-supervised learning should be impossible for causal learning problems, but\\nfeasible otherwise, in particular for anticausal ones. A meta-analysis of published semi-supervised learning benchmark\\nstudies corroborated this prediction [130], and similar results apply for natural language processing [69]. These\\n\\xef\\xac\\x81ndings are intriguing since they provide insight into physical properties of learning problems, thus going beyond the\\nmethods and applications that machine learning studies usually provide.\\n\\nSubsequent developments include further theoretical analyses [67, 112] and a form of conditional semi-supervised\\nlearning [158]. The view of semi-supervised learning as exploiting dependences between a marginal p(x) and a\\nnon-causal conditional p(y | x) is consistent with the common assumptions employed to justify semi-supervised\\nlearning [17, 126].\\n\\nInvariance and Robustness We have discussed the shortcomings of the i.i.d. assumption, which rarely holds true\\nexactly in practice, and the fact that real-world intelligent agents need to be able to generalize not just within a single\\ni.i.d. setting, but across related problems. This notion has been termed out-of-distribution (o.o.d.) generalization,\\nattracting signi\\xef\\xac\\x81cant attention in recent years [131]. While most work so far has been empirical, statistical bounds\\nwould be desirable that generalize (2.4), including additional quantities measuring the distance between training and\\ntest distribution, incorporating meaningful assumptions [138]. Such assumptions are necessary [8], and could be\\ncausal, or related to invariance properties.\\n\\nThe recent phenomenon of \\xe2\\x80\\x9cadversarial vulnerability\\xe2\\x80\\x9d [149] shows that minuscule targeted violations of the i.i.d.\\nassumption, generated by adding suitably chosen noise to images (imperceptible to humans), can lead to dangerous\\nerrors such as confusion of traf\\xef\\xac\\x81c signs. These examples are compelling as they showcase non-robustnesses of\\narti\\xef\\xac\\x81cial systems which are not shared by human perception. Our own perception thus exhibits invariance or\\nrobustness properties that are not easily learned from a single training set.\\n\\nEarly causal work related to domain shift [130] looked at the problem of learning from multiple cause-effect datasets\\nthat share a functional mechanism but differ in noise distributions. More generally, given (data from) multiple distri-\\nbutions, one can try to identify components which are robust, and \\xef\\xac\\x81nd means to transfer them across problems [166, 4,\\n163, 36, 55]. According to the ICM Principle 5.1, invariance of conditionals or functions (also referred to as covariate\\nshift in simple settings) should only hold in the causal direction, a reversal of the impossibility described for SSL.\\n\\nBuilding on the work of [130, 111], the idea of invariance for prediction has also been used for supervised learning\\n[121, 3, 87]. In particular, \\xe2\\x80\\x9cinvariant risk minimization\\xe2\\x80\\x9d (IRM) was proposed as an alternative to ERM, cf. (2.3).\\n\\n9 Causal Reasoning\\n\\nIn contrast to causal discovery (\\xc2\\xa7 7), which aims to uncover the causal structure underlying a set of variables, causal\\nreasoning starts from a known (or postulated) causal graph and answers causal queries of interest. While causal\\ndiscovery often looks for qualitative relationships, causal reasoning usually aims to quantify them. This requires\\n\\n18\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 8: Left: Covid-19 case fatality rates (CFRs) in Italy and China by age and in aggregate (\\xe2\\x80\\x9cTotal\\xe2\\x80\\x9d), including\\nall con\\xef\\xac\\x81rmed cases and fatalities up to the time of reporting in early 2020 (see legend): for all age groups, CFRs in\\nItaly are lower than in China, but the total CFR in Italy is higher, an example of Simpson\\xe2\\x80\\x99s paradox. Right: The case\\ndemographic differs between countries: in Italy, most cases occurred in the older population (\\xef\\xac\\x81gure from [154]).\\n\\ntwo steps: (i) identifying the query, i.e., deriving an estimand for it that only involves observed quantitites; and (ii)\\nestimating this using data. Often, the quantities of interest can be described as treatment effects, i.e., contrasts between\\ntwo interventions.\\nDe\\xef\\xac\\x81nition 9.1 (Treatment effects). The conditional average treatment effect (CATE), conditioned on (a subset of)\\nfeatures x, is de\\xef\\xac\\x81ned as\\n\\n\\xcf\\x84 (x) := E[Y | x, do(T = 1)] \\xe2\\x88\\x92 E[Y | x, do(T = 0)] = E[Y (1) \\xe2\\x88\\x92 Y (0) | x].\\n\\nThe average treatment effect (ATE) is de\\xef\\xac\\x81ned as the population average of the CATE,\\n\\n\\xcf\\x84 := E[\\xcf\\x84 (X)] = E[Y | do(T = 1)] \\xe2\\x88\\x92 E[Y | do(T = 0)] = E[Y (1) \\xe2\\x88\\x92 Y (0)].\\n\\n(9.1)\\n\\n(9.2)\\n\\nWhile ITE (Defn. 4.12) and CATE (9.1) are sometimes used interchangeably, there is a conceptual difference: ITE\\nrefers to the difference of two POs and is thus bound to an individual, while CATE applies to subpopulations, e.g.,\\nthe CATE for females in their 40s. Since the ITE is fundamentally impossible to observe, it is often estimated by the\\nCATE conditional on an individual\\xe2\\x80\\x99s features xi using suitable additional assumptions.\\n\\nAs is clear from Defn. 9.1, the treatment effects we want to estimate involve interventional expressions. However, we\\nusually only have access to observational data. Causal reasoning can thus be cast as answering interventional queries\\nusing observational data and a causal model. This involves dealing with confounders, both observed and unobserved.\\n\\nBefore discussing how to identify and estimate causal effects, we illustrate why causal assumptions are necessary\\nusing a well-known statistical phenomenon.\\n\\nSimpson\\xe2\\x80\\x99s Paradox and Covid-19 Simpson\\xe2\\x80\\x99s paradox refers to the observation that aggregating data across sub-\\npopulations may yield opposite trends (and thus lead to reversed conclusions) from considering subpopulations sep-\\narately [143]. We observed a textbook example of this during the Covid-19 pandemic by comparing case fatality\\nrates (CFRs), i.e., the proportion of con\\xef\\xac\\x81rmed Covid-19 cases which end in fatality, across different countries and age\\ngroups as illustrated in Fig. 8 [154]: for all age groups, CFRs in Italy are lower than in China, but the total CFR in\\nItaly is higher.\\n\\nHow can such a pattern be explained? The case demographic (see Fig. 8, right) is rather different across the two coun-\\ntries, i.e., there is a statistical association between country and age. In particular, Italy recorded a much larger pro-\\nportion of cases in older patients who are generally at higher risk of dying from Covid-19 (see Fig. 8, left). While\\nthis provides a consistent explanation in a statistical sense, the phenomenon may still seem puzzling as it de\\xef\\xac\\x81es our\\ncausal intuition. Humans appear to naturally extrapolate conditional probabilities to read them as causal effects, which\\ncan lead to inconsistent conclusions and may leave one wondering: how can the disease in Italy be less fatal for the\\nyoung, less fatal for the old, but more fatal for the people overall? It is for this reason that the reversal of (conditional)\\nprobabilities in Fig. 8 is perceived as and referred to as a \\xe2\\x80\\x9cparadox\\xe2\\x80\\x9d [106, 49].\\n\\nIf we consider the country as treatment whose causal effect on fatality is of interest, then causal assumptions (e.g., in\\nthe form of a causal graph) are needed to decide how to handle covariates such as age that are statistically associated\\n\\n19\\n\\n0-910-1920-2930-3940-4950-5960-6970-7980+TotalAge02468101214%Case fatality rates (CFRs) by age groupChina, 17 FebruaryItaly, 9 March0-910-1920-2930-3940-4950-5960-6970-7980+Age05101520%Proportion of confirmed cases by age groupChina, 17 FebruaryItaly, 9 March\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nFigure 9: Treatment effect estimation with three observed covariates\\nX1, X2, X3: here, the valid adjustment sets for T \\xe2\\x86\\x92 Y (see Prop. 9.3)\\nare {X1}, {X2}, and {X1, X2}. Including X3 opens the non-directed\\npath T \\xe2\\x86\\x92 X3 \\xe2\\x86\\x90 X2 \\xe2\\x86\\x92 Y and lies on the directed path T \\xe2\\x86\\x92 X3 \\xe2\\x86\\x92 Y ,\\nboth of which can introduce bias.\\n\\nX1\\n\\nX2\\n\\nT\\n\\nY\\n\\nX3\\n\\nwith the treatment, e.g., whether to stratify by (i.e., adjust for) age or not. This also explains why randomized controlled\\ntrials (RCTs) [31] are the gold standard for causal reasoning: randomizing the assignment breaks any potential links\\nbetween the treatment variable and other covariates, thus eliminating potential problems of bias. However, RCTs are\\ncostly and sometimes unethical to perform, so that causal reasoning often relies on observational data only.10\\n\\nWe \\xef\\xac\\x81rst consider the simplest setting without hidden confounders and with overlap. We start with identi\\xef\\xac\\x81cation of\\ntreatment effects on the population level, and then discuss different techniques for estimating these from data.\\n\\nIdenti\\xef\\xac\\x81cation In absence of unmeasured variables (i.e., without hidden confounding), and provided we know the\\ncausal graph, it is straight-forward to compute causal effects by adjusting for covariates. A principled approach to\\ndo so for any given graph was proposed by Robins [118] and is known as the g-computation formula (where the g\\nstands for general). It is also known as truncated factorisation [105] or manipulation theorem [145]. It relies on the\\nindependence of causal mechanisms (Principle 5.1), i.e., the fact that intervening on a variable leaves the other causal\\nconditionals in (4.1) unaffected:\\n\\np(X1, . . . , Xn | do(Xi = xi)) = \\xce\\xb4(Xi = xi)\\n\\np(Xj | PAj)\\n\\n(9.3)\\n\\n(cid:89)\\n\\nj(cid:54)=i\\n\\nFrom (9.3) the interventional distribution of interest can then be obtained by marginalization. This is related to the idea\\nof graph surgery (see Fig. 5), and leads to a set of three inference rules for manipulating interventional distributions\\nknown as do-calculus [105] that have been shown to be complete for identifying causal effects [56, 141].\\n\\nNote that covariate adjustment may be needed even if there are no clear confounders directly in\\xef\\xac\\x82uencing both treatment\\nand outcome, as shown by the example in Fig. 9.\\nExample 9.2. Applying the g-computation formula (9.3) to the setting of Fig. 9, we obtain\\n\\np(y | do(t)) =\\n\\np(x1)\\n\\np(x2 | x1)\\n\\np(x3 | t, x2)p(y | t, x2, x3)\\n\\n(cid:88)\\n\\nx2\\n(cid:88)\\n\\nx2\\n\\n(cid:88)\\n\\nx1\\n(cid:88)\\n\\n=\\n\\n(a)\\n=\\n\\nx1\\n(cid:88)\\n\\nx1,x2\\n\\n(cid:88)\\n\\nx3\\n\\n(cid:88)\\n\\nx2\\n\\n(cid:88)\\n\\n(b)\\n=\\n\\nx1\\n\\np(x1, x2)p(y | t, x1, x2)\\n\\np(x1)p(y | t, x1)\\n\\np(x1)\\n\\np(x2 | x1)p(y | t, x2) =\\n\\np(x2)p(y | t, x2)\\n\\nwhere the last line follows by using the following conditional independences implied by the graph: (a) Y \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 X1 |\\n{T, X2}, and (b) X2 \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 T | X1.\\n\\nNote that both the RHS in (9.5) and both sides in (9.6) take the form\\n\\np(y | do(t)) =\\n\\np(z)p(y|t, z).\\n\\n(cid:88)\\n\\nz\\nIn this case we call Z a valid adjustment set for the effect of T on Y . Here, {X1}, {X2}, and {X1, X2} are all\\nvalid adjustment sets, but it can be shown that, e.g., {X1, X3} is not (see Fig. 9). As computing the g-formula with\\nmany covariates can be cumbersome, graphical criteria for which subsets constitute valid adjustment sets are useful\\nin practice, even in the absence of unobserved confounders.\\nProposition 9.3 ([142]). Under causal suf\\xef\\xac\\x81ciency, a set Z is a valid adjustment set for the causal effect of a singleton\\ntreatment T on an outcome Y (in the sense of (9.7)) if and only if the following two conditions hold: (i) Z contains no\\ndescendant of any node on a directed path from T to Y (except for descendants of T which are not on a directed path\\nfrom T to Y ); and (ii) Z blocks all non-directed paths from T to Y .\\n\\n10For a treatment of more general types of data fusion and transportability of experimental \\xef\\xac\\x81ndings across different populations\\n\\nwe refer to [107, 5].\\n\\n20\\n\\n(9.4)\\n\\n(9.5)\\n\\n(9.6)\\n\\n(9.7)\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nHere, a path is called directed if all directed edges on it point in the same direction, and non-directed otherwise. A\\npath is blocked (by a set of vertices Z) if it contains a triple of consecutive nodes connected in one of the following\\nthree ways: A \\xe2\\x86\\x92 B \\xe2\\x86\\x92 C with B \\xe2\\x88\\x88 Z, A \\xe2\\x86\\x90 B \\xe2\\x86\\x92 C with B \\xe2\\x88\\x88 Z, or A \\xe2\\x86\\x92 B \\xe2\\x86\\x90 C, where neither B nor any\\ndescendant of B is in Z.\\n\\nTwo well-known types of adjustment set implied by Prop. 9.3 are parent adjustment, where Z = PaT ; and the\\nbackdoor criterion, where Z is constrained to contain no descendants of T and to block all \\xe2\\x80\\x9cback-door paths\\xe2\\x80\\x9d from T\\nto Y (T \\xe2\\x86\\x90 ... Y ).\\n\\nNote that Prop. 9.3 only holds singleton treatments (i.e., interventions on a single variable). For treatments T involving\\nmultiple variables, a slightly more complicated version of Prop. 9.3 can be given in terms of proper causal paths, and\\nwe refer to [103, 109] for details.\\n\\nLet us brie\\xef\\xac\\x82y return to our earlier example of Simpson\\xe2\\x80\\x99s paradox and Covid-19. Considering a plausible causal graph\\nfor this setting [154], we \\xef\\xac\\x81nd that age A acts as a mediator C \\xe2\\x86\\x92 A \\xe2\\x86\\x92 F of the causal effect of country C on fatality F\\n(there is likely also a direct effect C \\xe2\\x86\\x92 F , potentially mediated by other, unobserved variables). If we are interested\\nin the (total) causal effect of C on F (i.e., the overall in\\xef\\xac\\x82uence of country on fatality), A should not be included for\\nadjustment according to Prop. 9.3, and, subject to causal suf\\xef\\xac\\x81ciency, the total CFRs can be interpreted causally.11 For\\nanother classic example of Simpson\\xe2\\x80\\x99s paradox in the context of kidney stone treatment [18], on the other hand, the\\nsize of the stone acts as a confounder and thus needs to be adjusted for to obtain sound causal conclusions.\\n\\nValid covariate adjustment and the g-formula tell us how to compute interventions from the observational distribution\\nwhen there are no hidden confounders. To actually identify causal effects from data, however, we need to also be\\nable to estimate the involved quantities in (9.7). This is a problem if a subgroup of the population never (or always)\\nreceives a certain treatment. We thus need the additional assumption of a non-zero probability of receiving each\\npossible treatment, referred to as overlap, or common support.\\nAssumption 9.4 (Overlap/common treatment support). For any treatment t and any con\\xef\\xac\\x81guration of features x, it\\nholds that: 0 < p(T = t | X = x) < 1.\\n\\nThe combination of overlap and ignorability (i.e., no hidden confounders\\xe2\\x80\\x94see Assumption 4.15) is also referred to\\nas strong ignorability and is a suf\\xef\\xac\\x81cient condition for identifying ATE and CATE: the absence of hidden confounders\\nguarantees the existence of a valid adjustment set Z \\xe2\\x8a\\x86 X for which p(Y | do(T = t), Z) = p(Y | T = t, Z), and\\noverlap guarantees that we can actually estimate the latter term for any z occurring with non-zero probability.12\\n\\nRegression Adjustment Having identi\\xef\\xac\\x81ed a valid adjustment set (using Prop. 9.3), regression adjustment works by\\n\\xef\\xac\\x81tting a regression function \\xcb\\x86f to E[Y | Z = z, T = t] = f (z, t) using an observational sample {(yi, ti, zi)}m\\ni=1. We\\ncan then use \\xcb\\x86f to impute counterfactual outcomes as \\xcb\\x86yCF\\ni = \\xcb\\x86f (zi, 1 \\xe2\\x88\\x92 ti) in order to estimate the CATE. The ATE is\\nthen given by the population average and can be estimated as\\n\\n\\xcb\\x86\\xcf\\x84regression-adj. =\\n\\n(cid:88)\\n\\n(cid:0)yi \\xe2\\x88\\x92 \\xcb\\x86f (zi, 0)(cid:1) +\\n\\n(cid:88)\\n\\n(cid:0) \\xcb\\x86f (zi, 1) \\xe2\\x88\\x92 yi\\n\\n(cid:1),\\n\\n(9.8)\\n\\n1\\nm1\\n\\ni : ti=1\\n\\n1\\nm0\\n\\ni : ti=0\\n\\nwhere m1 and m0 are the number of observations from the treatment and control groups, respectively. Note the\\ndifference to the RCT estimator where no adjustment is necessary,\\n1\\nm0\\n\\n\\xcb\\x86\\xcf\\x84RCT =\\n\\n1\\nm1\\n\\nyi \\xe2\\x88\\x92\\n\\n(9.9)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\nyi.\\n\\ni : ti=1\\n\\ni : ti=0\\n\\nMatching and Weighting Approaches While regression adjustment indirectly estimates ATE via CATE, matching\\nand weighting approaches aim to estimate ATE directly. The general is idea to emulate the conditions of an RCT as\\nwell as possible.\\n\\nMatching approaches work by splitting the population into subgroups based on feature similarity. This can be done on\\nan individual level (so-called one-to-one or nearest neighbor matching) by matching each individual i with the most\\nsimilar one, j(i), from the opposite treatment group (i.e., ti (cid:54)= tj(i)). The difference of their outcomes, yi \\xe2\\x88\\x92 yj(i), is\\nthen considered as a sample of the ATE, and their average taken as an estimate thereof,\\n1\\nm1\\n\\n(yi \\xe2\\x88\\x92 yj(i)) +\\n\\n\\xcb\\x86\\xcf\\x84NN-matching =\\n\\n(yj(i) \\xe2\\x88\\x92 yi).\\n\\n1\\nm0\\n\\n(9.10)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\ni : ti=1\\n\\ni : ti=0\\n\\n11Mediation analysis [104] provides tools to tease apart and quantify the direct and indirect effects; the age-speci\\xef\\xac\\x81c CFRs in Fig. 8\\n\\nthen correspond to controlled direct effects [154].\\n\\n12The overlap assumption can thus be relaxed to hold for at least one valid adjustment set.\\n\\n21\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nH\\n\\nM\\n\\nT\\n\\nY\\n\\nI\\n\\nT\\n\\nY\\n\\n(a) Front-door\\n\\n(b) IV\\n\\n(c) RDD\\n\\nH\\n\\nS\\n\\nT\\n\\nH\\n\\nY\\n\\nFigure 10: Overview of special settings which allow to estimate causal effects of treatment T on outcome Y when the\\nstrong ignorability assumption (no hidden confounding, and overlap) does not hold. In (a) the hidden confounder H is\\ndealt with by means of an observed mediator M , while (b) relies on an instrumental variable (IV) which is independent\\nof H. (c) In a regression discontinuity design (RDD), treatment assignment is a threshold function of some observed\\ndecision score S so that there is no overlap between treatment groups.\\n\\nAlternatively, the population can be split into larger subgroups with similar features (so-called strata). Each stratum is\\nthen treated as an independent RCT. If there are K strata containing m1, ..., mK observations each, the strati\\xef\\xac\\x81ed ATE\\nestimator is\\n\\n\\xcb\\x86\\xcf\\x84strati\\xef\\xac\\x81ed =\\n\\n(cid:80)K\\n\\nk=1 mk \\xcb\\x86\\xcf\\x84 (k)\\n(cid:80)K\\nk=1 mk\\n\\nRCT\\n\\n(9.11)\\n\\n(9.12)\\n\\nRCT is the estimator from (9.9) applied to observation in the kth stratum.\\n\\nwhere \\xcb\\x86\\xcf\\x84 (k)\\nWeighting approaches, on the other hand, aim to counteract the confounding bias by reweighting each observation\\nto make the population more representative of an RCT. This means that underrepresented treatment groups are up-\\nweighted and overrepresented ones downweighted. An example is the inverse probability weighting (IPW) estimator,\\n\\n\\xcb\\x86\\xcf\\x84IPW =\\n\\n1\\nm1\\n\\n(cid:88)\\n\\ni : ti=1\\n\\nyi\\np(T = 1 | Z = zi)\\n\\n\\xe2\\x88\\x92\\n\\n1\\nm0\\n\\n(cid:88)\\n\\ni : ti=0\\n\\nyi\\np(T = 0 | Z = zi)\\n\\n.\\n\\nThe treatment probability p(T = 1 | Z) is also known as propensity score. While from a theoretical point of view Z\\nshould be a valid adjustment set, practitioners sometimes use all covariates to construct a propensity score.\\n\\nPropensity Score-Methods To overcome the curse of dimensionality and gain statistical ef\\xef\\xac\\x81ciency in high-\\ndimensional, low-data regimes, propensity scores can be a useful tool, because covariates and treatment are rendered\\nconditionally independent, T \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 Z | s(z), by the propensity score s(z) := p(T = 1 | Z = z) [122]. Instead of\\nadjusting for large feature sets or performing matching in high-dimensional spaces, the scalar propensity score can be\\nused instead. Applying this idea to the above methods gives rise to propensity score adjustment and propensity score\\nmatching. For the latter, the difference in propensity scores is used as similarity between instances to \\xef\\xac\\x81nd nearest\\nneighbors or to de\\xef\\xac\\x81ne strata.\\n\\nWhile simplifying in one respect, the propensity score needs to be estimated from data which is an additional source\\nof error. The standard approach for this is to estimate s(z) by logistic regression, but more sophisticated methods\\nare also possible. However, propensity score methods still rely on having identi\\xef\\xac\\x81ed a valid adjustment set Z to give\\nunbiased results. Using all covariates to estimate s, without checking for validity as an adjustment set, can thus lead\\nto wrong results.\\n\\nNext, we consider the case of causal reasoning with unobserved confounders. While it is not possible to identify causal\\neffects in the general case, we will discuss two particular situations in which ATE can still be estimated. These are\\nshown in Fig. 10a and b.\\n\\nFront-Door Adjustment The \\xef\\xac\\x81rst situation in which identi\\xef\\xac\\x81cation is possible even though a hidden variable H\\nconfounds the effect between treatment and outcome is known as front-door adjustment. The corresponding causal\\ngraph is shown in Fig. 10a. Front-door adjustment relies on the existence of an observed variable M which blocks\\nall directed paths from T to Y , so that T only causally in\\xef\\xac\\x82uences Y through M . For this reason M is also called a\\nmediator. The other important assumption is that the hidden confounder does not in\\xef\\xac\\x82uence the mediator other than\\nthrough the treatment T , i.e., M \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 H | T . In this case, and provided p(t, m) > 0 for all t and m, the causal effect of\\nT on Y is identi\\xef\\xac\\x81able and is given by the following.\\nProposition 9.5 (Front-door adjustment). For the causal graph in Fig. 10a it holds that:\\n\\np(y | do(t)) =\\n\\np(m | t)\\n\\np(t(cid:48))p(y | m, t(cid:48)).\\n\\n(cid:88)\\n\\nm\\n\\n(cid:88)\\n\\nt(cid:48)\\n\\n(9.13)\\n\\n22\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n(cid:88)\\n\\nm\\n\\n(cid:88)\\n\\nt(cid:48)\\n\\nWe give a sketch of the derivation, and refer to [105] for a proof using the rules of do-calculus. Since M mediates the\\ncausal effect of T on Y , we have that\\n\\np(y | do(t)) =\\n\\np(m | do(t))p(y | do(m)).\\n\\n(9.14)\\n\\nSince there are no backdoor paths from T to M we have p(m | do(t)) = p(m | t).\\n\\nMoreover, {T } is a valid adjustment set for the effect of M on Y by Prop. 9.3, so\\n\\np(y | do(m)) =\\n\\np(t(cid:48))p(y | m, t(cid:48)).\\n\\n(9.15)\\n\\nSubstituting into (9.14) then yields expression (9.13).\\n\\nWe point out that the setting presented here is only the simplest form of front-door adjustment which is suf\\xef\\xac\\x81cient to\\nconvey the main idea. It can be amended to include observed covariates X as well, as long as the conditions on the\\nmediator remain satis\\xef\\xac\\x81ed.\\n\\nInstrumental Variables (IV) The second setting for causal reasoning with hidden confounders is based on the idea\\nof instrumental variables [2, 29, 160], see Fig. 10b. The IV approach relies on the existence of a special observed\\nvariable I called instrument.\\nDe\\xef\\xac\\x81nition 9.6 (IV). A variable I is a valid instrument for estimating the effect of treatment T on outcome Y con-\\nfounded by a hidden variable H if all of the following three conditions hold: (i) I \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 H; (ii) I (cid:54)\\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 T ; and (iii)\\nI \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 Y | T .\\n\\nCondition (i) states that the instrument is independent of any hidden confounders H. Since this assumption cannot be\\ntested, background knowledge is necessary to justify the use of a variable as IV in practice. Conditions (ii) and (iii)\\nstate that the instrument is correlated with treatment, and only affects the outcome through T , and are referred to as\\nrelevance and exclusion restriction, respectively.\\nGiven a valid IV, we apply a two-stage procedure: \\xef\\xac\\x81rst obtain an estimate \\xcb\\x86T of the treatment variable T that is\\nindependent of H by predicting T from I. Having thus created an unconfounded version of the treatment, a regression\\nof Y on \\xcb\\x86T then reveals the correct causal effect. We demonstrate this idea for a simple linear model with continuous\\ntreatment variable where the causal effect can be obtained by two-stage least squares (2SLS).\\nExample 9.7 (Linear IV with 2SLS). Consider the linear SCM de\\xef\\xac\\x81ned by\\n\\nT := aI + bH + UT ,\\n\\nY := cH + dT + UY .\\n\\nwith UT , UY independent noise terms. Then, since I \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 H, linear regression of T on I recovers the coef\\xef\\xac\\x81cient a via\\n\\xcb\\x86T = aI. Substituting for T in the structural equation for Y gives\\n\\nY := daI + (c + bd)H + UY + dUT .\\nA second linear regression of Y on \\xcb\\x86T = aI recovers the causal effect d because (I \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 H) =\\xe2\\x87\\x92 ( \\xcb\\x86T \\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 H), whereas a\\nnaive regression of Y on T would give a different result, as T (cid:54)\\xe2\\x8a\\xa5\\xe2\\x8a\\xa5 H.\\n\\nIVs have been studied extensively and more sophisticated versions than the simple example above exist, allowing for\\nnon-linear interactions and observed covariates.\\n\\nHaving discussed some special settings to deal with hidden confounding, we brie\\xef\\xac\\x82y present a technique to deal with\\nviolations of the overlap assumption.\\n\\nRegression Discontinuity Design In a regression discontinuity design (RDD) the treatment assignment mechanism\\nbehaves like a threshold function, i.e., the propensity score is discontinuous [60]. In the simplest setting, the assign-\\nment of treatment or control is determined by whether an observed score S is above a threshold s0, T := I{S \\xe2\\x89\\xa5 s0}.\\nThis score in turn depends on other covariates which may or may not be observed. For example, patients may be\\nassigned a risk score, and treatment is only prescribed if this score surpasses a given threshold. Since the score may\\nbe assigned by another institution, not all relevant covariates H are usually observed. However, it is assumed that\\nthe treatment decision only depends on the score, e.g., because doctors comply with the of\\xef\\xac\\x81cial rules. The causal\\ngraph for such a simple RDD setting is shown in Fig. 10c. While the score S constitutes a valid adjustment set in\\nprinciple, the problem with RDDs is the lack of overlap: patients with low scores are always assigned T = 0 and\\npatients with high scores are always assigned T = 1. Because of this, covariate adjustment, matching, or weighting\\n\\n23\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\napproaches do not apply. The general idea of an RDD is to overcome this challenge by comparing observations with\\nscore in a small neighborhood of the decision cut-off value s0, motivated by the consideration that patients with close\\nscores but on opposite sides of s0 differ only in whether they received the treatment or not. For example, if the treat-\\nment cut-off value is 0.5 for a score in [0,1], then patients with scores of 0.49 and 0.51 are comparable and can be\\ntreated as samples from an RCT. An RDD (in its simplest form) thus focuses on differences in the regression function\\nE[Y | S = s, T = t(s)] = f (s) for s \\xe2\\x88\\x88 [s0 \\xe2\\x88\\x92 (cid:15), s0 + (cid:15)], where (cid:15) > 0 is small.\\n\\nHalf-Sibling Regression and Exoplanet Detection We conclude this section with a real-world application perform-\\ning causal reasoning in a confounded additive noise model. Launched in 2009, NASA\\xe2\\x80\\x99s Kepler space telescope initially\\nobserved 150000 stars over four years, in search of exoplanet transits. These are events where a planet partially oc-\\ncludes its host star, causing a slight decrease in brightness, often orders of magnitude smaller than the in\\xef\\xac\\x82uence of\\ntelescope errors. When looking at stellar light curves, we noticed that the noise structure was often shared across\\nstars that were light years apart. Since that made direct interaction of the stars impossible, it was clear that the shared\\ninformation was due to the telescope acting as a confounder. We thus devised a method that (a) regresses a given star\\nof interest on a large set of other stars chosen such that their measurements contain no information about the star\\xe2\\x80\\x99s\\nastrophysical signal, and (b) removes that regression in order to cancel the telescope\\xe2\\x80\\x99s in\\xef\\xac\\x82uence.13 The method is\\ncalled \\xe2\\x80\\x9chalf-sibling\\xe2\\x80\\x9d regression since target and predictors share a parent, namely the telescope. The method recovers\\nthe random variable representing the astrophysical signal almost surely (up to a constant offset), for an additive noise\\nmodel (speci\\xef\\xac\\x81cally, the observed light curve is a sum of the unknown astrophysical signal and an unknown function\\nof the telescope noise), subject to the assumption that the telescope\\xe2\\x80\\x99s effect on the star is in principle predictable from\\nthe other stars [128].\\n\\nIn 2013, the Kepler spacecraft suffered a technical failure, which left it with only two functioning reaction wheels,\\ninsuf\\xef\\xac\\x81cient for the precise spatial orientation required by the original Kepler mission. NASA decided to use the\\nremaining fuel to make further observations, however the systematic error was signi\\xef\\xac\\x81cantly larger than before\\xe2\\x80\\x94a\\ngodsend for our method designed to remove exactly these errors. We augmented it with models of exoplanet tran-\\nsits and an ef\\xef\\xac\\x81cient way to search light curves, leading to the discovery of 36 planet candidates [32], of which 21\\nwere subsequently validated as bona \\xef\\xac\\x81de exoplanets [92]. Four years later, astronomers found traces of water in the\\natmosphere of the exoplanet K2-18b\\xe2\\x80\\x94the \\xef\\xac\\x81rst such discovery for an exoplanet in the habitable zone, i.e., allowing\\nfor liquid water [10, 152]. The planet turned out to be one that had been \\xef\\xac\\x81rst detected in our work [32] (exoplanet\\ncandidate EPIC 201912552).\\n\\n10 Current Research and Open Problems\\n\\nConservation of Information We have previously argued that the mechanization of information processing plays\\ncurrently plays a similar role to the mechanization of energy processing in earlier industrial revolutions [126]. Our\\npresent understanding of information is rather incomplete, as was the understanding of energy during the course of\\nthe \\xef\\xac\\x81rst two industrial revolutions. The profound modern understanding of energy came with Emmy Noether and the\\ninsight that energy conservation is due to a symmetry (or covariance) of the fundamental laws of physics: they look\\nthe same no matter how we shift time. One might argue that information, suitably conceptualized, should also be a\\nconserved quantity, and that this might also be a consequence of symmetries. The notions of invariance/independence\\ndiscussed above may be able to play a role in this respect.\\n\\nMass seemingly played two fundamentally different roles (inertia and gravitation) until Einstein furnished a deeper\\nconnection in general relativity. It is noteworthy that causality introduces a layer of complexity underlying the symmet-\\nric notion of statistical mutual information. Discussing source coding and channel coding, Shannon [139] remarked:\\nThis duality can be pursued further and is related to a duality between past and future and the notions of control and\\nknowledge. Thus we may have knowledge of the past but cannot control it; we may control the future but have no\\nknowledge of it.\\n\\nWhat is an Object? Following the i.i.d. pattern recognition paradigm, machine learning learns objects by extracting\\npatterns from many observations. An complementary view may consider objects as modules that can be separately\\nmanipulated or intervened upon [150]. The idea that objects are de\\xef\\xac\\x81ned by their behavior under transformation has\\nbeen in\\xef\\xac\\x82uential in \\xef\\xac\\x81elds ranging from psychology to mathematics [74, 88].\\n\\nCausal Representation Learning.\\nIn hindsight, it appears somewhat naive that \\xef\\xac\\x81rst attempts to build AI tried\\nto realize intelligence by programs written by humans, since existing examples of intelligent systems appear much\\n\\n13For events that are localized in time (such as exoplanet transits), we further argued that the same applies for suitably chosen\\n\\npast and future values of the star itself, which can thus also be used as predictors.\\n\\n24\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nClassic AI:\\nsymbols provided a priori;\\nrules provided a priori.\\n\\nMachine Learning:\\nrepresentations (symbols) learned from data;\\nonly include statistical information.\\n\\nCausal Modeling:\\nstructural causal models assume the causal vari-\\nables (symbols) are given.\\n\\nCausal Representation Learning:\\ncapture interventions, reasoning, planning\\xe2\\x80\\x94 \\xe2\\x80\\x9cThinking is act-\\ning in an imagined space\\xe2\\x80\\x9d (Konrad Lorenz)\\n\\nFigure 11: Causal representation learning aims to automatically learn representations that contain not just statistical\\ninformation, but support interventions, reasoning, and planning. The long-term goal of this \\xef\\xac\\x81eld is to learn causal\\nworld models supporting AI, or causal digital twins of complex systems.\\n\\ntoo complex for that. However, there is a second problem, which is just as signi\\xef\\xac\\x81cant: classic AI assumed that the\\nsymbols which were the basis of algorithms were provided a priori by humans. When building a chess program, it is\\nclear that the algorithms operate on chess board positions and chess pieces; however, if we want to solve a real-world\\nproblem in an unstructured environment (e.g., recognize spoken language), it is not clear what constitutes the basic\\nsymbols to be processed.\\n\\nTraditional causal discovery and reasoning assumed that the elementary units are random variables connected by a\\ncausal graph. Real-world observations, however, are usually not structured into such units to begin with. For instance,\\nobjects in images that permit causal reasoning \\xef\\xac\\x81rst need to be discovered [85, 157, 150, 84]. The emerging \\xef\\xac\\x81eld of\\ncausal representation learning strives to learn these variables from data, much like machine learning went beyond\\nsymbolic AI in not requiring that the symbols that algorithms manipulate be given a priori (see Fig. 11).\\n\\nDe\\xef\\xac\\x81ning objects or variables, and structural models connecting them, can sometimes be achieved by coarse-graining of\\nmicroscopic models, including microscopic SCMs [124], ordinary differential equations [123], and temporally aggre-\\ngated time series [37]. While most causal models in economics, medicine, or psychology use variables that are abstrac-\\ntions of more elementary concepts, it is challenging to state general conditions under which coarse-grained variables\\nadmit causal models with well-de\\xef\\xac\\x81ned interventions [15, 124, 16]. The task of identifying suitable units that admit\\ncausal models aligns with the general goal of modern machine learning to learn meaningful representations for data,\\nwhere meaningful can mean robust, transferable, interpretable, explainable, or fair [77, 72, 162, 71, 70, 155]. To com-\\nbine structural causal modeling (Defn. 4.4) and representation learning, we may try to devise machine learning models\\nwhose inputs may be high-dimensional and unstructured, but whose inner workings are (partly) governed by an SCM.\\n\\nSuppose that our high-dimensional, low-level observations X = (X1, ..., Xd) are explained by a small number of\\nunobserved, or latent, variables S = (S1, ..., Sn) where n (cid:28) d, in that X is generated by applying an injective map\\ng : Rn \\xe2\\x86\\x92 Rd to S (see Fig. 12c):\\n\\nX = g(S).\\n(10.1)\\nA common assumption regarding (10.1) is that the latent Si are jointly independent, e.g., for independent component\\nanalysis (ICA) [57] (where g is referred to as a mixing) or disentangled representation learning [9] (where g is called\\na decoder). Presently, however, we instead want think of the latent Si as causal variables that support interventions\\nand reasoning.\\n\\nThe Si may thus well be dependent, and possess a causal factorization (4.1),\\n\\np(S1, . . . , Sn) =\\n\\np(Si | PAi),\\n\\n(10.2)\\n\\ninduced by an underlying (acyclic) SCM M = (F, pU) with jointly independent Ui and\\n\\nF = {Si := fi(PAi, Ui)}n\\nOur goal is to learn a latent causal model consisting of (i) the causal representation S = g\\xe2\\x88\\x921(X), along with (ii) the\\ncorresponding causal graph and (iii) the mechanisms p(Si | PAi) or fi. This is a challenging task, since none of\\nthem are directly observed or known a priori; instead we typically only have access to observations of X. In fact,\\nthere is no hope in an i.i.d. setting since already the simpler case with independent Si (and n = d) is generally not\\nidenti\\xef\\xac\\x81able (i.e., for arbitrary nonlinear g in (10.1)): even independence does not suf\\xef\\xac\\x81ciently constrain the problem to\\nuniquely recover, or identify, the true Si\\xe2\\x80\\x99s up to any simple class of ambiguities such as permutations and element-wise\\ninvertible transformations of the Si [58].\\n\\n(10.3)\\n\\ni=1.\\n\\nn\\n(cid:89)\\n\\ni=1\\n\\n25\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\ng\\xe2\\x88\\x921 ?\\n\\nS1\\n\\n?\\n\\nX2\\n\\n?\\n\\nX3\\n\\nX1\\n\\n?\\n\\n(a)\\n\\nX1\\n\\nX3\\n\\nx2\\n\\n(b)\\n\\nE[X3 | do(x2)]?\\n\\nS2\\n\\nS3\\n\\ng\\n\\n(c)\\n\\nFigure 12: Overview of different causal learning tasks: (a) causal discovery (\\xc2\\xa7 7) aims to learn the causal graph (or\\nSCM) connecting a set of observed variables; (b) causal reasoning (\\xc2\\xa7 9) aims to answer interventional or counterfactual\\nqueries based on a (partial) causal model over observed variables Xi; (c) causal representation learning (\\xc2\\xa7 10) aims\\nto infer a causal model consisting of a small number of high-level, abstract causal variables Si and their relations from\\npotentially high-dimensional, low-level observations X = g(S).\\n\\nTo link causal representation learning to the well-studied ICA setting with independent latents in (10.1), we can\\nconsider the so-called reduced form of an (acyclic) SCM: by recursive substitution of the structural assignments (10.3)\\nin topological order of the causal graph, we can write the latent causal variables S as function of the noise variables\\nonly\\n\\nS = fRF(U).\\n(10.4)\\nDue to acyclicity, this mapping fRF : Rn \\xe2\\x86\\x92 Rn has a lower triangular Jacobian (possibly after re-ordering the Si\\nw.l.o.g.). However, (10.4) is strictly less informative than (10.3): while they entail the same distribution (10.2), the\\nformer no longer naturally supports interventions on the Si but only changes to the noise distribution pU (an example\\nof a so-called soft intervention [30]). At the same time, the reduced form (10.4) allows us to rewrite (10.1) as\\n\\nX = g \\xe2\\x97\\xa6 fRF(U)\\n\\n(10.5)\\n\\nThrough this lens, the task of learning the reduced form (10.4) could be seen as structured form of nonlinear ICA\\n(i.e., (10.1) with independent latents) where we additionally want to learn an intermediate representation through fRF.\\nHowever, as discussed, we cannot even solve the problem with independent latents (i.e., identify g \\xe2\\x97\\xa6 fRF in (10.5)) [58],\\nlet alone separate the SCM and mixing functions to recover the intermediate causal representation.\\n\\nIt is not surprising that is is not possible to solve the strictly harder causal representation learning problem in an i.i.d.\\nsetting and that additional causal learning signals are needed. This gives rise to the following questions: How can we\\ndevise causal training algorithms to learn the Si? And, what types of additional data, assumptions, and constraints\\nwould these algorithms require beyond the i.i.d. setting? Two general ideas are to (i) build on the ICM Principle 5.1\\nand enforce some form of (algorithmic) independence between the learned causal mechanisms p(Si | PAi) or fi, and\\n(ii) use heterogeneous (non-i.i.d.) data, e.g., from multiple views or different environments, arising from interventions\\nin the underlying latent SCM (10.3). We brie\\xef\\xac\\x82y discuss some more concrete ideas based on recent work.\\n\\nGenerative Approach: Causal Auto-Encoders. One approach is to try to learn the generative causal model (10.1)\\nand (10.3), or its reduced form (10.4), using an auto-encoder approach [73]. An auto-encoder consists of an encoder\\nfunction q : Rd \\xe2\\x86\\x92 Rn which maps X to a latent \\xe2\\x80\\x9cbottleneck\\xe2\\x80\\x9d representation (e.g., comprising the unexplained noise\\nvariables U), and a decoder function \\xcb\\x86g : Rn \\xe2\\x86\\x92 Rd mapping back to the observations. For example, the decoder\\nmay directly implement the composition \\xcb\\x86g = g \\xe2\\x97\\xa6 fRF from (10.4). Alternatively, it could consist of multiple modules,\\nimplementing (10.1) and (10.3) separately. A standard procedure to train such an auto-encoder architecture is to\\nminimise the reconstruction error, i.e., to satisfy \\xcb\\x86g \\xe2\\x97\\xa6 q \\xe2\\x89\\x88 id on a training set of observations of X. As discussed, this\\nalone is insuf\\xef\\xac\\x81cient, so to make it causal we can impose additional constraints on the structure of the decoder [80]\\nand try to make the causal mechanisms independent by ensuring that they are invariant across problems and can be\\nindependently intervened upon. For example, if we intervene on the causal variables Si or noise distribution pU\\nin our model of (10.3) or (10.4), respectively, this should still produce \\xe2\\x80\\x9cvalid\\xe2\\x80\\x9d observations, as assessed, e.g., by\\nthe discriminator of a generative adversarial network [38]. While we ideally want to manipulate the causal variables,\\nanother way to intervene is to replace noise variables with the corresponding values computed from other input images,\\na procedure that has been referred to as hybridization [11]. Alternatively, if we have access to multiple environments,\\ni.e., datasets collected under different conditions, we could rely on the Sparse Mechanism Shift Principle 5.2 by\\nrequiring that changes can be explained by shifts in only a few of the p(Si | PAi).\\n\\nDiscriminative Approach: Self-Supervised Causal Representation Learning. A different machine learning approach\\nfor unsupervised representation learning, that is not based on generative modeling but is discriminative in nature, is\\nself-supervised learning with data augmentation. Here, the main idea is to apply some hand-crafted transformations\\n\\n26\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nto the observation to generate augmented views that are thought to share the main semantic characteristics with the\\noriginal observation (e.g., random crops or blurs for images). One then directly learns a representation by maximizing\\nthe similarity across encodings of views related to each other by augmentations, while enforcing diversity across those\\nof unrelated views. In recent work [156], we set out to better understand this approach theoretically, as well as to\\ninvestigate its potential for learning causal representations. Starting from (10.1), we postulate a latent causal model of\\nthe form Sc \\xe2\\x86\\x92 Ss, where Sc is a (potentially multivariate) content variable, de\\xef\\xac\\x81ned as the high-level semantic part\\nof the representation S = (Sc, Ss) that is assumed invariant across views; and Ss is a (potentially multivariate) style\\nvariable, de\\xef\\xac\\x81ned as the remaining part of the representation that may change. Within this setting, data augmentations\\nhave a natural interpretation as counterfactuals under a hypothetical intervention on the style variables, given the\\noriginal view. It can be shown that in this case, subject to some technical assumptions, common contrastive self-\\nsupervised learning algorithms [19, 99, 45] as well as appropriately constrained generative models isolate, or recover,\\nthe true content variables Sc up to an invertible transformation. By extending this approach to use multiple augmented\\nviews of the same observation, and linking these to different counterfactuals in the underlying latent SCM, it may be\\npossible to recover a more-\\xef\\xac\\x81ne grained causal representation.\\n\\nIndependent Mechanism Analysis. We also explored [40] to what extent the ICM Principle 5.1 may be useful for un-\\nsupervised representation learning tasks such as (10.1), particularly for imposing additional constraints on the mixing\\nfunction g. It turns out that independence between p(S) and the mixing g\\xe2\\x80\\x94measured, e.g., as discussed in \\xc2\\xa7 5 in\\nthe context of Fig. 6 and [64]\\xe2\\x80\\x94does not impose nontrivial constraints when S is not observed, even when the Si are\\nassumed independent as in ICA. However, by thinking of each Si as independently in\\xef\\xac\\x82uencing the observed distri-\\nbution, we postulate another type of independence between the partial derivatives \\xe2\\x88\\x82g\\nof the mixing g which has a\\n\\xe2\\x88\\x82Si\\ngeometric interpretation as an orthogonality condition on the columns of the Jacobian of g. The resulting indepen-\\ndent mechanism analysis (IMA) approach rules out some of the common examples of non-identi\\xef\\xac\\x81ability of nonlinear\\nICA [58, 82] mentioned above. Since IMA does not require independent sources, it may also be a useful constraint for\\ncausal representation learning algorithms.\\n\\nLearning Transferable Mechanisms and Multi-Task Learning Machine learning excels in i.i.d. settings, and\\nthrough the use of high capacity learning algorithms we can achieve outstanding performance on many problems,\\nprovided we have i.i.d. data for each individual problem (\\xc2\\xa7 2). However, natural intelligence excels at generalizing\\nacross tasks and settings. Suppose we want to build a system that can solve multiple tasks in multiple environments.\\nIf we view learning as data compression, it would make sense for that system to utilize components that apply across\\ntasks and environments, and thus need to be stored only once [126].\\n\\nIndeed, an arti\\xef\\xac\\x81cial or natural agent in a complex world is faced with limited resources. This concerns training\\ndata, i.e., we only have limited data for each individual task/domain, and thus need to \\xef\\xac\\x81nd ways of pooling/re-using\\ndata, in stark contrast to the current industry practice of large-scale labelling work done by humans. It also concerns\\ncomputational resources: animals have constraints on the resources (e.g., space, energy) used by their brains, and\\nevolutionary neuroscience knows examples where brain regions get re-purposed. Similar constraints apply as machine\\nlearning systems get embedded in physical devices that may be small and battery-powered. Versatile AI models that\\nrobustly solve a range of problems in the real world will thus likely need to re-use components, which requires that\\nthe components are robust across tasks and environments [129, 131]. This calls for a structure whose modules are\\nmaximally reusable. An elegant way to do this would be to employ a modular structure that mirrors modularity that\\nIn other words, if the are mechanisms at play in the world play similar roles across a range\\nexists in the world.\\nof environments, tasks, and settings, then it would be prudent for a model to employ corresponding computational\\nmodules [39]. For instance, if variations of natural lighting (the position of the sun, clouds, etc.)\\nimply that the\\nvisual environment can appear in brightness conditions spanning several orders of magnitude, then visual processing\\nalgorithms in our nervous system should employ methods that can factor out these variations, rather than building\\nseparate sets of object recognizers for every lighting condition. If our brain were to model the lighting changes by a\\ngain control mechanism, say, then this mechanism in itself need not have anything to do with the physical mechanisms\\nbringing about brightness differences. It would, however, play a role in a modular structure that corresponds to the\\nrole the physical mechanisms play in the world\\xe2\\x80\\x99s modular structure\\xe2\\x80\\x94in other words, it would represent the physical\\nmechanism. Searching for the most versatile yet compact models would then automatically produce a bias towards\\nmodels that exhibit certain forms of structural isomorphy to a world that we cannot directly recognize.\\n\\nA sensible inductive bias to learn such models is to look for independent causal mechanisms [83], and competitive\\ntraining can play a role in this: for a pattern recognition task, learning causal models that contain independent mecha-\\nnisms helps in transferring modules across substantially different domains [100].\\n\\nInterventional World Models, Surrogate Models, Digital Twins, and Reasoning Modern representation learning\\nIt does so, however,\\nexcels at learning representations of data that preserve relevant statistical properties [9, 79].\\n\\n27\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\nwithout taking into account causal properties of the variables, i.e., it does not care about the interventional properties\\nof the variables it analyzes or reconstructs. Going forward, causality will play a major role in taking representation\\nlearning to the next level, moving beyond the representation of statistical dependence structures towards models that\\nsupport intervention, planning, and reasoning. This would realize Konrad Lorenz\\xe2\\x80\\x99 notion of thinking as acting in an\\nimagined space. It would also provide a means to learn causal digital twins that go beyond reproducing statistical\\ndependences captured by surrogate models trained using machine learning.\\n\\nThe idea of surrogate modeling is that we may have a complex phenomenon for which we have access to computa-\\ntionally expensive simulation data. If the mappings involved (e.g., from parameter settings to target quantities) can\\nbe \\xef\\xac\\x81tted from data, we can employ machine learning, which will often speed them up by orders of magnitude. Such\\na speed-up can qualitatively change the usability of a model: for instance, we have recently built a system to map\\ngravitational wave measurements to a probability distribution of physical parameters of a black hole merger event, in-\\ncluding sky position [27]. The fact that this model only requires seconds to evaluate makes it possible to immediately\\nstart electromagnetic follow-up observations using telescopes as soon as a gravitational wave event has been detected,\\nenabling analysis of transient events.\\n\\nGoing forward, we anticipate that surrogate modeling will bene\\xef\\xac\\x81t from respecting the causal factorization (4.1) de-\\ncomposing the overall dependence structure into mechanisms (i.e., causal conditionals). We can then build an overall\\nmodel of a system by modeling the mechanisms independently, each of them using the optimal method. Some of the\\nconditionals we may know analytically, some we may be able to transfer from related problems, if they are invariant.\\nFor some, we may have access to real data to estimate them, and for others, we may need to resort to simulations,\\npossibly \\xef\\xac\\x81tted using surrogate models.\\n\\nIf the model is required to fully capture the effects of all possible interventions, then all components should be \\xef\\xac\\x81tted\\nas described in the causal directions (i.e., we \\xef\\xac\\x81t the causal mechanisms). Such a model then allows to employ all\\nthe causal reasoning machinery described in \\xc2\\xa7 4 and \\xc2\\xa7 9 (e.g., computing interventional and, in the case of SCMs,\\ncounterfactual distributions). If, on the other hand, a model only needs to capture some of the possible interventions,\\nand is used in a purely predictive/observational mode for other variables, then we can get away with also using and\\n\\xef\\xac\\x81tting some non-causal modules, i.e., using a decomposition which lies in between (4.1) and (4.2).\\n\\nWe believe that this overall framework will be a principled and powerful approach to build such (causal) digital twins\\nor causal surrogate models by combining a range of methods and bringing them to bear according to their strengths.\\n\\nConcluding Remarks. Most of the discussed \\xef\\xac\\x81elds are still in their infancy, and the above account is biased by\\npersonal taste and knowledge. With the current hype around machine learning, there is much to say in favor of some\\nhumility towards what machine learning can do, and thus towards the current state of AI\\xe2\\x80\\x94the hard problems have not\\nbeen solved yet, making basic research in this \\xef\\xac\\x81eld all the more exciting.\\n\\nAckowledgements Many thanks to all past and present members of the T\\xc2\\xa8ubingen causality team, and to Cian East-\\nwood and Elias Bareinboim for feedback on the manuscript.\\n\\nReferences\\n\\n[1] J. Aldrich, Autonomy. Oxford Economic Papers 41 (1989), 15\\xe2\\x80\\x9334\\n\\n[2] J. D. Angrist, G. W. Imbens, and D. B. Rubin, Identi\\xef\\xac\\x81cation of causal effects using instrumental variables. Journal of the\\n\\nAmerican statistical Association 91 (1996), no. 434, 444\\xe2\\x80\\x93455\\n\\n[3] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz, Invariant risk minimization. arXiv preprint 1907.02893 (2019)\\n\\n[4] E. Bareinboim and J. Pearl, Transportability from multiple environments with limited experiments: Completeness results. In\\n\\nAdvances in Neural Information Processing Systems 27, pp. 280\\xe2\\x80\\x93288, 2014\\n\\n[5] E. Bareinboim and J. Pearl, Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences\\n\\n113 (2016), no. 27, 7345\\xe2\\x80\\x937352\\n\\n[6] S. Bauer, B. Sch\\xc2\\xa8olkopf, and J. Peters, The arrow of time in multivariate time series. In Proceedings of the 33nd international\\n\\nconference on machine learning, pp. 2043\\xe2\\x80\\x932051, 48, 2016\\n\\n[7] M. Belkin, D. Hsu, S. Ma, and S. Mandal, Reconciling modern machine learning practice and the bias-variance trade-off.\\n\\n2018, arXiv:1812.11118\\n\\n[8] S. Ben-David, T. Lu, T. Luu, and D. P\\xc2\\xb4al, Impossibility theorems for domain adaptation. In Proceedings of the international\\n\\nconference on arti\\xef\\xac\\x81cial intelligence and statistics 13 (AISTATS), pp. 129\\xe2\\x80\\x93136, 2010\\n\\n[9] Y. Bengio, A. Courville, and P. Vincent, Representation learning: A review and new perspectives. IEEE Transactions on\\n\\nPattern Analysis and Machine Intelligence 35 (2013), no. 8, 1798\\xe2\\x80\\x931828\\n\\n28\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[10] B. Benneke, I. Wong, C. Piaulet, H. A. Knutson, I. J. M. Cross\\xef\\xac\\x81eld, J. Lothringer, C. V. Morley, P. Gao, T. P. Greene,\\nC. Dressing, D. Dragomir, A. W. Howard, P. R. McCullough, E. M. R. K. J. J. Fortney, and J. Fraine, Water vapor on the\\nhabitable-zone exoplanet K2-18b. arXiv preprint 1909.04642 (2019)\\n\\n[11] M. Besserve, A. Mehrjou, R. Sun, and B. Sch\\xc2\\xa8olkopf, Counterfactuals uncover the modular structure of deep generative\\n\\nmodels. In International conference on learning representations, 2020\\n\\n[12] M. Besserve, N. Shajarisales, B. Sch\\xc2\\xa8olkopf, and D. Janzing, Group invariance principles for causal generative models. In\\nProceedings of the 21st international conference on arti\\xef\\xac\\x81cial intelligence and statistics (aistats), pp. 557\\xe2\\x80\\x93565, 2018\\n\\n[13] S. Bongers, P. Forr\\xc2\\xb4e, J. Peters, and J. M. Mooij, Foundations of structural causal models with cycles and latent variables. The\\n\\nAnnals of Statistics 49 (2021), no. 5, 2885\\xe2\\x80\\x932915\\n\\n[14] D. Buchsbaum, S. Bridgers, D. Skolnick Weisberg, and A. Gopnik, The power of possibility: Causal learning, counterfactual\\nreasoning, and pretend play. Philosophical Transactions of the Royal Society B: Biological Sciences 367 (2012), no. 1599,\\n2202\\xe2\\x80\\x932212\\n\\n[15] K. Chalupka, F. Eberhardt, and P. Perona, Multi-level cause-effect systems. In Arti\\xef\\xac\\x81cial intelligence and statistics, pp. 361\\xe2\\x80\\x93\\n\\n369, PMLR, 2016\\n\\n[16] K. Chalupka, F. Eberhardt, and P. Perona, Causal feature learning: an overview. Behaviormetrika 44 (2017), no. 1, 137\\xe2\\x80\\x93164\\n\\n[17] O. Chapelle, B. Sch\\xc2\\xa8olkopf, and A. Zien (eds.), Semi-supervised learning. MIT Press, Cambridge, MA, USA, 2006\\n\\n[18] C. R. Charig, D. R. Webb, S. R. Payne, and J. E. Wickham, Comparison of treatment of renal calculi by open surgery,\\npercutaneous nephrolithotomy, and extracorporeal shockwave lithotripsy. Br Med J (Clin Res Ed) 292 (1986), no. 6524,\\n879\\xe2\\x80\\x93882\\n\\n[19] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, A simple framework for contrastive learning of visual representations.\\n\\narXiv preprint 2002.05709 (2020)\\n\\n[20] D. M. Chickering, Learning bayesian networks is np-complete. In Learning from data, pp. 121\\xe2\\x80\\x93130, Springer, 1996\\n\\n[21] D. M. Chickering, Optimal structure identi\\xef\\xac\\x81cation with greedy search. Journal of machine learning research 3 (2002), no.\\n\\n[22] G. F. Cooper and E. Herskovits, A bayesian method for the induction of probabilistic networks from data. Machine learning\\n\\nNov, 507\\xe2\\x80\\x93554\\n\\n9 (1992), no. 4, 309\\xe2\\x80\\x93347\\n\\n[23] D. R. Cox, Planning of experiments (1958)\\n\\n[24] P. Daniu\\xcb\\x87sis, D. Janzing, J. M. Mooij, J. Zscheischler, B. Steudel, K. Zhang, and B. Sch\\xc2\\xa8olkopf, Inferring deterministic causal\\n\\nrelations. In Proceedings of the 26th annual conference on Uncertainty in Arti\\xef\\xac\\x81cial Intelligence (UAI), pp. 143\\xe2\\x80\\x93150, 2010\\n\\n[25] A. P. Dawid, Conditional independence in statistical theory. Journal of the Royal Statistical Society B 41 (1979), no. 1, 1\\xe2\\x80\\x9331\\n\\n[26] A. P. Dawid, Causal inference without counterfactuals. Journal of the American Statistical Association 95 (2000), no. 450,\\n\\n407\\xe2\\x80\\x93424\\n\\n[27] M. Dax, S. R. Green, J. Gair, J. H. Macke, A. Buonanno, and B. Sch\\xc2\\xa8olkopf, Real-time gravitational-wave science with neural\\n\\nposterior estimation. Physical Review Letters (2021)\\n\\n[28] L. Devroye, L. Gy\\xc2\\xa8or\\xef\\xac\\x81, and G. Lugosi, A probabilistic theory of pattern recognition. Applications of Mathematics 31,\\n\\n[29] V. Didelez, S. Meng, and N. A. Sheehan, Assumptions of IV methods for observational epidemiology. Statistical Science 25\\n\\nSpringer, New York, NY, 1996\\n\\n(2010), 22\\xe2\\x80\\x9340\\n\\n[30] F. Eberhardt and R. Scheines, Interventions and causal inference. Philosophy of Science 74 (2007), no. 5, 981\\xe2\\x80\\x93995\\n\\n[31] R. A. Fisher, The design of experiments. Oliver & Boyd, Edinburgh & London. (1937), no. 2\\n\\n[32] D. Foreman-Mackey, B. T. Montet, D. W. Hogg, T. D. Morton, D. Wang, and B. Sch\\xc2\\xa8olkopf, A systematic search for transiting\\n\\nplanets in the K2 data. The Astrophysical Journal 806 (2015), no. 2\\n\\n[33] K. Fukumizu, A. Gretton, X. Sun, and B. Sch\\xc2\\xa8olkopf, Kernel measures of conditional dependence. In Advances in neural\\n\\n[34] D. Geiger and D. Heckerman, Learning gaussian networks. In Proceedings of the tenth international conference on uncer-\\n\\ninformation processing systems, pp. 489\\xe2\\x80\\x93496, 2008\\n\\ntainty in arti\\xef\\xac\\x81cial intelligence, pp. 235\\xe2\\x80\\x93243, 1994\\n\\n[35] D. Geiger and J. Pearl, Logical and algorithmic properties of independence and their application to Bayesian networks.\\n\\nAnnals of Mathematics and Arti\\xef\\xac\\x81cial Intelligence 2 (1990), 165\\xe2\\x80\\x93178\\n\\n[36] M. Gong, K. Zhang, T. Liu, D. Tao, C. Glymour, and B. Sch\\xc2\\xa8olkopf, Domain adaptation with conditional transferable com-\\n\\nponents. In Proceedings of the 33nd international conference on machine learning, pp. 2839\\xe2\\x80\\x932848, 2016\\n\\n[37] M. Gong, K. Zhang, B. Sch\\xc2\\xa8olkopf, C. Glymour, and D. Tao, Causal discovery from temporally aggregated time series. In\\n\\nProceedings of the thirty-third conference on uncertainty in arti\\xef\\xac\\x81cial intelligence, 2017\\n\\n[38] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative\\n\\nadversarial nets. In Advances in neural information processing systems 27, pp. 2672\\xe2\\x80\\x932680, 2014\\n\\n29\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[39] A. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, Y. Bengio, and B. Sch\\xc2\\xa8olkopf, Recurrent independent mechanisms.\\n\\nIn International conference on learning representations, 2020\\n\\n[40] L. Gresele, J. von K\\xc2\\xa8ugelgen, V. Stimper, B. Sch\\xc2\\xa8olkopf, and M. Besserve, Independent mechanism analysis, a new concept?\\n\\nIn Advances in neural information processing systems 34, 2021\\n\\n[41] A. Gretton, K. M. Borgwardt, M. Rasch, B. Sch\\xc2\\xa8olkopf, and A. J. Smola, A kernel method for the two-sample-problem. In\\n\\nAdvances in neural information processing systems 19, pp. 513\\xe2\\x80\\x93520, 2007\\n\\n[42] A. Gretton, O. Bousquet, A. Smola, and B. Sch\\xc2\\xa8olkopf, Measuring statistical dependence with Hilbert-Schmidt norms. In\\n\\nAlgorithmic learning theory, pp. 63\\xe2\\x80\\x9378, Springer-Verlag, 2005\\n\\n[43] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch\\xc2\\xa8olkopf, and A. J. Smola, A kernel statistical test of independence. In\\n\\nAdvances in neural information processing systems 20, pp. 585\\xe2\\x80\\x93592, 2008\\n\\n[44] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Sch\\xc2\\xa8olkopf, Kernel methods for measuring independence. Journal\\n\\nof Machine Learning Research 6 (2005), 2075\\xe2\\x80\\x932129\\n\\n[45] U. M. Gutmann and A. Hyv\\xc2\\xa8arinen, Noise-contrastive estimation: A new estimation principle for unnormalized statistical\\n\\nmodels. In International conference on arti\\xef\\xac\\x81cial intelligence and statistics, pp. 297\\xe2\\x80\\x93304, 2010\\n\\n[46] T. Haavelmo, The probability approach in econometrics. Econometrica: Journal of the Econometric Society (1944), iii\\xe2\\x80\\x93115\\n\\n[47] D. Heckerman, D. Geiger, and D. M. Chickering, Learning bayesian networks: The combination of knowledge and statistical\\n\\ndata. Machine learning 20 (1995), no. 3, 197\\xe2\\x80\\x93243\\n\\n[48] D. Heckerman, C. Meek, and G. Cooper, A bayesian approach to causal discovery. In Innovations in machine learning, pp.\\n\\n[49] M. A. Hern\\xc2\\xb4an, D. Clayton, and N. Keiding, The Simpson\\xe2\\x80\\x99s paradox unraveled. International journal of epidemiology 40\\n\\n1\\xe2\\x80\\x9328, Springer, 2006\\n\\n(2011), no. 3, 780\\xe2\\x80\\x93785\\n\\n[50] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, Beta-VAE: Learning\\nbasic visual concepts with a constrained variational framework. In International conference on learning representations,\\n2016\\n\\n[51] P. W. Holland, Statistics and causal inference. Journal of the American statistical Association 81 (1986), no. 396, 945\\xe2\\x80\\x93960\\n\\n[52] K. D. Hoover, Causality in macroeconomics. Cambridge University Press, 2001\\n\\n[53] P. O. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Sch\\xc2\\xa8olkopf, Nonlinear causal discovery with additive noise models. In\\n\\nAdvances in Neural Information Processing Systems 21 (NIPS), pp. 689\\xe2\\x80\\x93696, 2009\\n\\n[54] B. Huang, K. Zhang, J. Zhang, J. D. Ramsey, R. Sanchez-Romero, C. Glymour, and B. Sch\\xc2\\xa8olkopf, Causal discovery from\\n\\nheterogeneous/nonstationary data. J. Mach. Learn. Res. 21 (2020), no. 89, 1\\xe2\\x80\\x9353\\n\\n[55] B. Huang, K. Zhang, J. Zhang, R. Sanchez-Romero, C. Glymour, and B. Sch\\xc2\\xa8olkopf, Behind distribution shift: Mining driving\\nforces of changes and causal arrows. In IEEE 17th international conference on data mining (icdm 2017), pp. 913\\xe2\\x80\\x93918, 2017\\n\\n[56] Y. Huang and M. Valtorta, Pearl\\xe2\\x80\\x99s calculus of intervention is complete. In Proceedings of the twenty-second conference on\\n\\nuncertainty in arti\\xef\\xac\\x81cial intelligence, pp. 217\\xe2\\x80\\x93224, 2006\\n\\n[57] A. Hyv\\xc2\\xa8arinen and E. Oja, Independent component analysis: algorithms and applications. Neural networks 13 (2000), no.\\n\\n[58] A. Hyv\\xc2\\xa8arinen and P. Pajunen, Nonlinear independent component analysis: Existence and uniqueness results. Neural networks\\n\\n[59] A. Hyvarinen, H. Sasaki, and R. Turner, Nonlinear ica using auxiliary variables and generalized contrastive learning. In The\\n\\n22nd international conference on arti\\xef\\xac\\x81cial intelligence and statistics, pp. 859\\xe2\\x80\\x93868, 2019\\n\\n[60] G. W. Imbens and T. Lemieux, Regression discontinuity designs: A guide to practice. Journal of econometrics 142 (2008),\\n\\n[61] G. W. Imbens and D. B. Rubin, Causal inference in statistics, social, and biomedical sciences. Cambridge University Press,\\n\\n4-5, 411\\xe2\\x80\\x93430\\n\\n12 (1999), no. 3, 429\\xe2\\x80\\x93439\\n\\nno. 2, 615\\xe2\\x80\\x93635\\n\\n2015\\n\\n[62] D. Janzing, R. Chaves, and B. Sch\\xc2\\xa8olkopf, Algorithmic independence of initial condition and dynamical law in thermody-\\n\\nnamics and causal inference. New Journal of Physics 18 (2016), no. 093052, 1\\xe2\\x80\\x9313\\n\\n[63] D. Janzing, P. Hoyer, and B. Sch\\xc2\\xa8olkopf, Telling cause from effect based on high-dimensional observations. In Proceedings\\nof the 27th international conference on machine learning, edited by J. F\\xc2\\xa8urnkranz and T. Joachims, pp. 479\\xe2\\x80\\x93486, 2010\\n\\n[64] D. Janzing, J. M. Mooij, K. Zhang, J. Lemeire, J. Zscheischler, P. Daniu\\xcb\\x87sis, B. Steudel, and B. Sch\\xc2\\xa8olkopf, Information-\\n\\ngeometric approach to inferring causal directions. Arti\\xef\\xac\\x81cial Intelligence 182\\xe2\\x80\\x93183 (2012), 1\\xe2\\x80\\x9331\\n\\n[65] D. Janzing, J. Peters, J. M. Mooij, and B. Sch\\xc2\\xa8olkopf, Identifying confounders using additive noise models. In Proceedings\\n\\nof the 25th annual conference on Uncertainty in Arti\\xef\\xac\\x81cial Intelligence (UAI), pp. 249\\xe2\\x80\\x93257, 2009\\n\\n[66] D. Janzing and B. Sch\\xc2\\xa8olkopf, Causal inference using the algorithmic Markov condition. IEEE Transactions on Information\\n\\nTheory 56 (2010), no. 10, 5168\\xe2\\x80\\x935194\\n\\n30\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[67] D. Janzing and B. Sch\\xc2\\xa8olkopf, Semi-supervised interpolation in an anticausal learning scenario. Journal of Machine Learning\\n\\nResearch 16 (2015), 1923\\xe2\\x80\\x931948\\n\\n[68] D. Janzing and B. Sch\\xc2\\xa8olkopf, Detecting non-causal artifacts in multivariate linear regression models. In Proceedings of the\\n\\n35th international conference on machine learning (ICML), pp. 2250\\xe2\\x80\\x932258, 2018\\n\\n[69] Z. Jin, J. von K\\xc2\\xa8ugelgen, J. Ni, T. Vaidhya, A. Kaushal, M. Sachan, and B. Sch\\xc2\\xa8olkopf, Causal direction of data collection\\nmatters: Implications of causal and anticausal learning for nlp. In Proceedings of the 2021 conference on empirical methods\\nin natural language processing (emnlp), 2021\\n\\n[70] A.-H. Karimi, B. Sch\\xc2\\xa8olkopf, and I. Valera, Algorithmic recourse: from counterfactual explanations to interventions. In\\n\\nConference on fairness, accountability, and transparency, pp. 353\\xe2\\x80\\x93362, 2021\\n\\n[71] A.-H. Karimi, J. von K\\xc2\\xa8ugelgen, B. Sch\\xc2\\xa8olkopf, and I. Valera, Algorithmic recourse under imperfect causal knowledge: a\\n\\nprobabilistic approach. In Advances in neural information processing systems 33, 2020\\n\\n[72] N. Kilbertus, M. Rojas Carulla, G. Parascandolo, M. Hardt, D. Janzing, and B. Sch\\xc2\\xa8olkopf, Avoiding discrimination through\\n\\ncausal reasoning. In Advances in neural information processing systems 30, pp. 656\\xe2\\x80\\x93666, 2017\\n\\n[73] D. P. Kingma and M. Welling, Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114 (2013)\\n\\n[74] F. Klein, Vergleichende Betrachtungen \\xc2\\xa8uber neuere geometrische Forschungen. Verlag von Andreas Deichert, Erlangen,\\n\\n1872\\n\\n[75] D. Koller and N. Friedman, Probabilistic graphical models: principles and techniques. MIT press, 2009\\n\\n[76] S. Kpotufe, E. Sgouritsa, D. Janzing, and B. Sch\\xc2\\xa8olkopf, Consistency of causal inference under the additive noise model. In\\n\\nProceedings of the 31th international conference on machine learning, pp. 478\\xe2\\x80\\x93486, 2014\\n\\n[77] M. J. Kusner, J. Loftus, C. Russell, and R. Silva, Counterfactual fairness. In Advances in neural information processing\\n\\nsystems 30, pp. 4066\\xe2\\x80\\x934076, Curran Associates, Inc., 2017\\n\\n[78] S. L. Lauritzen, Graphical models. 17, Clarendon Press, 1996\\n\\n[79] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning. Nature 521 (2015), no. 7553, 436\\xe2\\x80\\x93444\\n\\n[80] F. Leeb, Y. Annadani, S. Bauer, and B. Sch\\xc2\\xa8olkopf, Structural autoencoders improve representations for generation and\\n\\ntransfer. arXiv preprint 2006.07796 (2020)\\n\\n[81] G. W. Leibniz, Discours de m\\xc2\\xb4etaphysique. 1686, (cited after Chaitin, 2010)\\n\\n[82] F. Locatello, S. Bauer, M. Lucic, G. R\\xc2\\xa8atsch, S. Gelly, B. Sch\\xc2\\xa8olkopf, and O. Bachem, Challenging common assumptions\\nin the unsupervised learning of disentangled representations. Proceedings of the 36th International Conference on Machine\\nLearning (2019)\\n\\n[83] F. Locatello, D. Vincent, I. Tolstikhin, G. R\\xc2\\xa8atsch, S. Gelly, and B. Sch\\xc2\\xa8olkopf, Competitive training of mixtures of independent\\n\\ndeep generative models. arXiv preprint 1804.11130 (2018)\\n\\n[84] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf, Object-\\n\\ncentric learning with slot attention. In Advances in neural information processing systems, 2020\\n\\n[85] D. Lopez-Paz, R. Nishihara, S. Chintala, B. Sch\\xc2\\xa8olkopf, and L. Bottou, Discovering causal signals in images. In Ieee confer-\\n\\nence on computer vision and pattern recognition (cvpr), pp. 58\\xe2\\x80\\x9366, 2017\\n\\n[86] J. Loschmidt, \\xc2\\xa8Uber den Zustand des W\\xc2\\xa8armegleichgewichtes eines Systems von K\\xc2\\xa8orpern mit R\\xc2\\xa8ucksicht auf die Schwerkraft.\\nAkademie der Wissenschaften, Wien. Mathematisch-Naturwissenschaftliche Klasse, Sitzungsberichte 73 (1876), 128\\xe2\\x80\\x93142\\n\\n[87] C. Lu, Y. Wu, J. M. Hern\\xc2\\xb4andez-Lobato, and B. Sch\\xc2\\xa8olkopf, Nonlinear invariant risk minimization: A causal approach. 2021,\\n\\narXiv:2102.12353\\n\\n[88] S. MacLane, Categories for the working mathematician. Springer-Verlag, New York, 1971\\n\\n[89] R. Matthews, Storks deliver babies (p= 0.008). Teaching Statistics 22 (2000), no. 2, 36\\xe2\\x80\\x9338\\n\\n[90] C. Meek, Causal inference and causal explanation with background knowledge. In Proceedings of the eleventh conference\\n\\non uncertainty in arti\\xef\\xac\\x81cial intelligence, pp. 403\\xe2\\x80\\x93410, Morgan Kaufmann Publishers Inc., 1995\\n\\n[91] F. H. Messerli, Chocolate consumption, cognitive function, and nobel laureates. The New England Journal of Medicine 367\\n\\n(2012), no. 16, 1562\\xe2\\x80\\x931564\\n\\n[92] B. T. Montet, T. D. Morton, D. Foreman-Mackey, J. A. Johnson, D. W. Hogg, B. P. Bowler, D. W. Latham, A. Bieryla, and\\nA. W. Mann, Stellar and planetary properties of K2 campaign 1 candidates and validation of 17 planets, including a planet\\nreceiving earth-like insolation. The Astrophysical Journal 809 (2015), no. 1, 25\\n\\n[93] R. P. Monti, K. Zhang, and A. Hyv\\xc2\\xa8arinen, Causal discovery with general non-linear relationships using non-linear ica. In\\n\\nUncertainty in arti\\xef\\xac\\x81cial intelligence, pp. 186\\xe2\\x80\\x93195, PMLR, 2020\\n\\n[94] J. M. Mooij, D. Janzing, T. Heskes, and B. Sch\\xc2\\xa8olkopf, On causal discovery with cyclic additive noise models. In Advances\\n\\nin Neural Information Processing Systems 24 (NIPS), 2011\\n\\n[95] J. M. Mooij, D. Janzing, J. Peters, and B. Sch\\xc2\\xa8olkopf, Regression by dependence minimization and its application to causal\\n\\ninference. In Proceedings of the 26th international conference on machine learning (ICML), pp. 745\\xe2\\x80\\x93752, 2009\\n\\n31\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[96] J. M. Mooij, D. Janzing, and B. Sch\\xc2\\xa8olkopf, From ordinary differential equations to structural causal models: The deter-\\nministic case. In Proceedings of the 29th annual conference on uncertainty in arti\\xef\\xac\\x81cial intelligence (UAI), pp. 440\\xe2\\x80\\x93448,\\n2013\\n\\n[97] J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Sch\\xc2\\xa8olkopf, Distinguishing cause from effect using observational\\n\\ndata: methods and benchmarks. Journal of Machine Learning Research 17 (2016), no. 32, 1\\xe2\\x80\\x93102\\n\\n[98] J. S. Neyman, On the application of probability theory to agricultural experiments. essay on principles. section 9.(tlanslated\\nand edited by dm dabrowska and tp speed, statistical science (1990), 5, 465-480). Annals of Agricultural Sciences 10 (1923),\\n1\\xe2\\x80\\x9351\\n\\n[99] A. v. d. Oord, Y. Li, and O. Vinyals, Representation learning with contrastive predictive coding. arXiv preprint 1807.03748\\n\\n(2018)\\n\\n[100] G. Parascandolo, N. Kilbertus, M. Rojas-Carulla, and B. Sch\\xc2\\xa8olkopf, Learning independent causal mechanisms. In Proceed-\\n\\nings of the 35th international conference on machine learning, pmlr 80:4036-4044, 2018\\n\\n[101] J. Park and K. Muandet, A measure-theoretic approach to kernel conditional mean embeddings. In Advances in neural\\n\\ninformation processing systems 33 (neurips 2020), pp. 21247\\xe2\\x80\\x9321259, Curran Associates, Inc., 2020\\n\\n[102] J. Pearl, Bayesian networks: A model of self-activated memory for evidential reasoning. In Proceedings of the 7th conference\\n\\nof the cognitive science society, 1985, pp. 329\\xe2\\x80\\x93334, 1985\\n\\n[103] J. Pearl, Causal diagrams for empirical research. Biometrika 82 (1995), no. 4, 669\\xe2\\x80\\x93688\\n\\n[104] J. Pearl, Direct and indirect effects. In Proceedings of the seventeenth conference on uncertainty in arti\\xef\\xac\\x81cial intelligence, pp.\\n\\n[105] J. Pearl, Causality: Models, reasoning, and inference. 2nd edn., Cambridge University Press, New York, NY, 2009\\n\\n[106] J. Pearl, Comment: understanding simpson\\xe2\\x80\\x99s paradox. The American Statistician 68 (2014), no. 1, 8\\xe2\\x80\\x9313\\n\\n[107] J. Pearl and E. Bareinboim, External validity: From do-calculus to transportability across populations. Statistical Science 29\\n\\n411\\xe2\\x80\\x93420, 2001\\n\\n(2014), no. 4, 579\\xe2\\x80\\x93595\\n\\n[108] J. Pearl and D. Mackenzie, The book of why: the new science of cause and effect. Basic Books, 2018\\n\\n[109] J. Pearl and A. Paz, Confounding equivalence in causal inference. Journal of Causal Inference 2 (2014), no. 1, 75\\xe2\\x80\\x9393\\n\\n[110] J. Pearl and T. Verma, A theory of inferred causation. In Principles of knowledge representation and reasoning: Proceedings\\n\\nof the second international conference, p. 441, 2, 1991\\n\\n[111] J. Peters, P. B\\xc2\\xa8uhlmann, and N. Meinshausen, Causal inference by using invariant prediction: identi\\xef\\xac\\x81cation and con\\xef\\xac\\x81dence\\n\\nintervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 78 (2016), no. 5, 947\\xe2\\x80\\x931012\\n\\n[112] J. Peters, D. Janzing, and B. Sch\\xc2\\xa8olkopf, Elements of causal inference - foundations and learning algorithms. MIT Press,\\n\\nCambridge, MA, USA, 2017\\n\\n[113] J. Peters, J. M. Mooij, D. Janzing, and B. Sch\\xc2\\xa8olkopf, Identi\\xef\\xac\\x81ability of causal graphs using functional models. In Proceedings\\n\\nof the 27th annual conference on Uncertainty in Arti\\xef\\xac\\x81cial Intelligence (UAI), pp. 589\\xe2\\x80\\x93598, 2011\\n\\n[114] J. Peters, J. M. Mooij, D. Janzing, and B. Sch\\xc2\\xa8olkopf, Causal discovery with continuous additive noise models. Journal of\\n\\nMachine Learning Research 15 (2014), 2009\\xe2\\x80\\x932053\\n\\n[115] N. P\\xef\\xac\\x81ster, P. B\\xc2\\xa8uhlmann, B. Sch\\xc2\\xa8olkopf, and J. Peters, Kernel-based tests for joint independence. Journal of the Royal Statis-\\n\\ntical Society: Series B (Statistical Methodology) 80 (2018), no. 1, 5\\xe2\\x80\\x9331\\n\\n[116] K. Popper, The logic of scienti\\xef\\xac\\x81c discovery (1959)\\n\\n[117] H. Reichenbach, The direction of time. University of California Press, Berkeley, CA, 1956\\n\\n[118] J. Robins, A new approach to causal inference in mortality studies with a sustained exposure period\\xe2\\x80\\x94application to control\\n\\nof the healthy worker survivor effect. Mathematical modelling 7 (1986), no. 9-12, 1393\\xe2\\x80\\x931512\\n\\n[119] J. M. Robins, M. A. Hernan, and B. Brumback, Marginal structural models and causal inference in epidemiology. Epidemi-\\n\\nology 11 (2000), no. 5, 550\\xe2\\x80\\x93560\\n\\nmichigan, ann arbor, mich., 1971). 1973\\n\\nLearning Research 19 (2018), no. 36, 1\\xe2\\x80\\x9334\\n\\nBiometrika 70 (1983), no. 1, 41\\xe2\\x80\\x9355\\n\\n[120] R. W. Robinson, Counting labeled acyclic digraphs, new directions in the theory of graphs (proc. third ann arbor conf., univ.\\n\\n[121] M. Rojas-Carulla, B. Sch\\xc2\\xa8olkopf, R. Turner, and J. Peters, Invariant models for causal transfer learning. Journal of Machine\\n\\n[122] P. R. Rosenbaum and D. B. Rubin, The central role of the propensity score in observational studies for causal effects.\\n\\n[123] P. K. Rubenstein, S. Bongers, B. Sch\\xc2\\xa8olkopf, and J. M. Mooij, From deterministic ODEs to dynamic structural causal models.\\n\\nIn Proceedings of the 34th conference on uncertainty in arti\\xef\\xac\\x81cial intelligence (uai), 2018\\n\\n[124] P. K. Rubenstein, S. Weichwald, S. Bongers, J. M. Mooij, D. Janzing, M. Grosse-Wentrup, and B. Sch\\xc2\\xa8olkopf, Causal con-\\nsistency of structural equation models. In Proceedings of the thirty-third conference on uncertainty in arti\\xef\\xac\\x81cial intelligence,\\n2017\\n\\n32\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[125] D. B. Rubin, Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of educational\\n\\nPsychology 66 (1974), no. 5, 688\\n\\n[126] B. Sch\\xc2\\xa8olkopf, Causality for machine learning. arXiv preprint 1911.10500, to appear in: R. Dechter, J. Halpern, and H.\\n\\nGeffner. Probabilistic and Causal Inference: The Works of Judea Pearl. ACM books (2019)\\n\\n[127] B. Sch\\xc2\\xa8olkopf, R. Herbrich, and A. J. Smola, A generalized representer theorem. In Annual conference on computational\\nlearning theory, edited by D. Helmbold and R. Williamson, pp. 416\\xe2\\x80\\x93426, no. 2111 in Lecture Notes in Computer Science,\\nSpringer, Berlin, 2001\\n\\n[128] B. Sch\\xc2\\xa8olkopf, D. Hogg, D. Wang, D. Foreman-Mackey, D. Janzing, C.-J. Simon-Gabriel, and J. Peters, Modeling confound-\\ning by half-sibling regression. Proceedings of the National Academy of Science (PNAS) 113 (2016), no. 27, 7391\\xe2\\x80\\x937398\\n\\n[129] B. Sch\\xc2\\xa8olkopf, D. Janzing, and D. Lopez-Paz, Causal and statistical learning. In Oberwolfach reports, pp. 1896\\xe2\\x80\\x931899, 13(3),\\n\\n2016\\n\\n[130] B. Sch\\xc2\\xa8olkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. M. Mooij, On causal and anticausal learning. In Proceed-\\n\\nings of the 29th international conference on machine learning (ICML), pp. 1255\\xe2\\x80\\x931262, 2012\\n\\n[131] B. Sch\\xc2\\xa8olkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio, Toward causal representation\\n\\nlearning. Proceedings of the IEEE 109 (2021), no. 5, 612\\xe2\\x80\\x93634\\n\\n[132] B. Sch\\xc2\\xa8olkopf, K. Muandet, K. Fukumizu, S. Harmeling, and J. Peters, Computing functions of random variables via repro-\\n\\nducing kernel Hilbert space representations. Statistics and Computing 25 (2015), no. 4, 755\\xe2\\x80\\x93766\\n\\n[133] B. Sch\\xc2\\xa8olkopf and A. J. Smola, Learning with kernels. MIT Press, Cambridge, MA, 2002\\n\\n[134] B. Sch\\xc2\\xa8olkopf, B. K. Sriperumbudur, A. Gretton, and K. Fukumizu, RKHS representation of measures applied to homogene-\\nity, independence, and Fourier optics. In Oberwolfach reports, edited by K. Jetter, S. Smale, and D.-X. Zhou, pp. 42\\xe2\\x80\\x9344, 30,\\n2008\\n\\n[135] G. Schwarz et al., Estimating the dimension of a model. The annals of statistics 6 (1978), no. 2, 461\\xe2\\x80\\x93464\\n\\n[136] R. D. Shah and J. Peters, The hardness of conditional independence testing and the generalised covariance measure. The\\n\\nAnnals of Statistics 48 (2020), no. 3, 1514\\xe2\\x80\\x931538\\n\\n[137] N. Shajarisales, D. Janzing, B. Sch\\xc2\\xa8olkopf, and M. Besserve, Telling cause from effect in deterministic linear dynamical\\n\\nsystems. In Proceedings of the 32nd international conference on machine learning (ICML), pp. 285\\xe2\\x80\\x93294, 2015\\n\\n[138] U. Shalit, F. D. Johansson, and D. Sontag, Estimating individual treatment effect: generalization bounds and algorithms. In\\n\\nInternational conference on machine learning, pp. 3076\\xe2\\x80\\x933085, 2017\\n\\n[139] C. E. Shannon, Coding theorems for a discrete source with a \\xef\\xac\\x81delity criterion. In Ire international convention records, pp.\\n\\n142\\xe2\\x80\\x93163, 7, 1959\\n\\n[140] S. Shimizu, P. O. Hoyer, A. Hyv\\xc2\\xa8arinen, and A. J. Kerminen, A linear non-Gaussian acyclic model for causal discovery.\\n\\nJournal of Machine Learning Research 7 (2006), 2003\\xe2\\x80\\x932030\\n\\n[141] I. Shpitser and J. Pearl, Identi\\xef\\xac\\x81cation of joint interventional distributions in recursive semi-markovian causal models. In\\n\\nProceedings of the 21st national conference on arti\\xef\\xac\\x81cial intelligence, pp. 1219\\xe2\\x80\\x931226, 2006\\n\\n[142] I. Shpitser, T. VanderWeele, and J. M. Robins, On the validity of covariate adjustment for estimating causal effects. In\\nProceedings of the twenty-sixth conference on uncertainty in arti\\xef\\xac\\x81cial intelligence, pp. 527\\xe2\\x80\\x93536, AUAI Press, 2010\\n\\n[143] E. H. Simpson, The interpretation of interaction in contingency tables. Journal of the Royal Statistical Society: Series B\\n\\n(Methodological) 13 (1951), no. 2, 238\\xe2\\x80\\x93241\\n\\n[144] A. J. Smola, A. Gretton, L. Song, and B. Sch\\xc2\\xa8olkopf, A Hilbert space embedding for distributions. In Algorithmic learning\\n\\ntheory: 18th international conference, pp. 13\\xe2\\x80\\x9331, 2007\\n\\n[145] P. Spirtes, C. Glymour, and R. Scheines, Causation, prediction, and search. 2nd edn., MIT Press, Cambridge, MA, 2000\\n\\n[146] W. Spohn, Grundlagen der Entscheidungstheorie. Scriptor-Verlag, 1978\\n\\n[147] I. Steinwart and A. Christmann, Support vector machines. Springer, New York, NY, 2008\\n\\n[148] R. Suter, D. Miladinovic, B. Sch\\xc2\\xa8olkopf, and S. Bauer, Robustly disentangled causal mechanisms: Validating deep represen-\\ntations for interventional robustness. In International conference on machine learning, pp. 6056\\xe2\\x80\\x936065, PMLR, 2019\\n\\n[149] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, Intriguing properties of neural\\n\\nnetworks. arXiv preprint 1312.6199 (2013)\\n\\n[150] M. Tangemann, S. Schneider, J. von K\\xc2\\xa8ugelgen, F. Locatello, P. Gehler, T. Brox, M. K\\xc2\\xa8ummerer, M. Bethge, and B. Sch\\xc2\\xa8olkopf,\\n\\nUnsupervised object learning via common fate. arXiv preprint arXiv:2110.06562 (2021)\\n\\n[151] J. Tian and J. Pearl, Causal discovery from changes. In Proceedings of the seventeenth conference on uncertainty in arti\\xef\\xac\\x81cial\\n\\nintelligence, pp. 512\\xe2\\x80\\x93521, Morgan Kaufmann Publishers Inc., 2001\\n\\n[152] A. Tsiaras, I. Waldmann, G. Tinetti, J. Tennyson, and S. Yurchenko, Water vapour in the atmosphere of the habitable-zone\\n\\neight-earth-mass planet K2-18b. Nature Astronomy (2019)\\n\\n[153] V. N. Vapnik, Statistical learning theory. Wiley, New York, NY, 1998\\n\\n33\\n\\n\\x0cFROM STATISTICAL TO CAUSAL LEARNING\\n\\n[154] J. von K\\xc2\\xa8ugelgen, L. Gresele, and B. Sch\\xc2\\xa8olkopf, Simpson\\xe2\\x80\\x99s paradox in Covid-19 case fatality rates: a mediation analysis of\\n\\nage-related causal effects. IEEE Transactions on Arti\\xef\\xac\\x81cial Intelligence 2 (2021), no. 1, 18\\xe2\\x80\\x9327\\n\\n[155] J. von K\\xc2\\xa8ugelgen, A.-H. Karimi, U. Bhatt, I. Valera, A. Weller, and B. Sch\\xc2\\xa8olkopf, On the fairness of causal algorithmic\\n\\nrecourse. In 36th aaai conference on arti\\xef\\xac\\x81cial intelligence, 2022\\n\\n[156] J. von K\\xc2\\xa8ugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Sch\\xc2\\xa8olkopf, M. Besserve, and F. Locatello, Self-supervised learning\\nwith data augmentations provably isolates content from style. In Advances in neural information processing systems 34,\\n2021\\n\\n[157] J. von K\\xc2\\xa8ugelgen, I. Ustyuzhaninov, P. Gehler, M. Bethge, and B. Sch\\xc2\\xa8olkopf, Towards causal generative scene models via\\n\\ncompetition of experts. In ICLR 2020 workshop on causal learning for decision making, 2020\\n\\n[158] J. von K\\xc2\\xa8ugelgen, A. Mey, M. Loog, and B. Sch\\xc2\\xa8olkopf, Semi-supervised learning, causality and the conditional cluster\\n\\nassumption. Conference on Uncertainty in Arti\\xef\\xac\\x81cial Intelligence (2020)\\n\\n[159] J. Woodward, Causation and manipulability (2001)\\n\\n[160] P. G. Wright, Tariff on animal and vegetable oils. Macmillan Company, New York, 1928\\n\\n[161] S. Wright, Correlation and causation. Journal of Agricultural Research 20 (1921), 557\\xe2\\x80\\x93580\\n\\n[162] J. Zhang and E. Bareinboim, Fairness in decision-making - the causal explanation formula. In Proceedings of the thirty-\\n\\nsecond AAAI conference on arti\\xef\\xac\\x81cial intelligence, pp. 2037\\xe2\\x80\\x932045, 2018\\n\\n[163] K. Zhang, M. Gong, and B. Sch\\xc2\\xa8olkopf, Multi-source domain adaptation: A causal view. In Proceedings of the 29th aaai\\n\\nconference on arti\\xef\\xac\\x81cial intelligence, pp. 3150\\xe2\\x80\\x933157, 2015\\n\\n[164] K. Zhang and A. Hyv\\xc2\\xa8arinen, On the identi\\xef\\xac\\x81ability of the post-nonlinear causal model. In Proceedings of the 25th annual\\n\\nconference on Uncertainty in Arti\\xef\\xac\\x81cial Intelligence (UAI), pp. 647\\xe2\\x80\\x93655, 2009\\n\\n[165] K. Zhang, J. Peters, D. Janzing, and B. Sch\\xc2\\xa8olkopf, Kernel-based conditional independence test and application in causal\\ndiscovery. In Proceedings of the 27th annual conference on Uncertainty in Arti\\xef\\xac\\x81cial Intelligence (UAI), pp. 804\\xe2\\x80\\x93813, 2011\\n\\n[166] K. Zhang, B. Sch\\xc2\\xa8olkopf, K. Muandet, and Z. Wang, Domain adaptation under target and conditional shift. In Proceedings of\\n\\nthe 30th international conference on machine learning, pp. 819\\xe2\\x80\\x93827, 2013\\n\\n34\\n\\n\\x0c', b'Quantized GAN for Complex Music Generation from Dance Videos\\n\\nYe Zhu *\\nIllinois Institute of Technology\\n\\nKyle Olszewski\\nSnap Inc.\\n\\nYu Wu\\nPrinceton University\\n\\nPanos Achlioptas\\nSnap Inc.\\n\\nMenglei Chai\\nSnap Inc.\\n\\nYan Yan\\nIllinois Institute of Technology\\n\\nSergey Tulyakov\\nSnap Inc.\\n\\n2\\n2\\n0\\n2\\n \\nr\\np\\nA\\n \\n1\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n1\\nv\\n4\\n0\\n6\\n0\\n0\\n.\\n4\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe present Dance2Music-GAN (D2M-GAN), a novel ad-\\nversarial multi-modal framework that generates complex\\nmusical samples conditioned on dance videos. Our pro-\\nposed framework takes dance video frames and human body\\nmotion as input, and learns to generate music samples that\\nplausibly accompany the corresponding input. Unlike most\\nexisting conditional music generation works that generate\\nspeci\\xef\\xac\\x81c types of mono-instrumental sounds using symbolic\\naudio representations (e.g., MIDI), and that heavily rely\\non pre-de\\xef\\xac\\x81ned musical synthesizers, in this work we gen-\\nerate dance music in complex styles (e.g., pop, breakdanc-\\ning, etc.) by employing a Vector Quantized (VQ) audio rep-\\nresentation, and leverage both its generality and the high\\nabstraction capacity of its symbolic and continuous coun-\\nterparts. By performing an extensive set of experiments on\\nmultiple datasets, and following a comprehensive evalua-\\ntion protocol, we assess the generative quality of our ap-\\nproach against several alternatives. The quantitative re-\\nsults, which measure the music consistency, beats corre-\\nspondence, and music diversity, clearly demonstrate the\\neffectiveness of our proposed method. Last but not least,\\nwe curate a challenging dance-music dataset of in-the-wild\\nTikTok videos, which we use to further demonstrate the ef-\\n\\xef\\xac\\x81cacy of our approach in real-world applications \\xe2\\x80\\x93 and\\nwhich we hope to serve as a starting point for relevant fu-\\nture research. The code is available at https://github.com/L-\\nYeZhu/D2M-GAN.\\n\\n1. Introduction\\n\\n\\xe2\\x80\\x9cWhen the music and dance create with accord, their\\nmagic captivates both the heart and the mind.\\xe2\\x80\\x9d 1 As a natu-\\nral form of expressive art, dance and music have enriched\\nour daily lives with a harmonious interplay of melodies,\\n\\n*This work was mainly done while the author was an intern at Snap\\n\\nInc.\\n\\n1Jean-Georges Noverre.\\n\\nFigure 1. Task illustration. We introduce a Vector Quantiza-\\ntion framework for music generation from dance videos, which\\ntakes human body motion and visual frames as input, and gener-\\nates suitable corresponding music. Our proposed model can gen-\\nerate complex and rich dance music - in contrast to most existing\\nconditional music generation works, which typically output mono-\\ninstrumental sounds.\\n\\nrhythms, and movements across the millennia. The grow-\\ning popularity of social media platforms for sharing dance\\nvideos, such as TikTok, has also demonstrated their signif-\\nicance as a source of entertainment in modern society. At\\nthe same time, new research is \\xef\\xac\\x82ourishing in the wake of\\nthis trend by exploring multi-modal generative tasks link-\\ning dance motion and music [1, 37\\xe2\\x80\\x9339].\\n\\nAlthough seemingly intuitive, music generation from\\ndance videos has been a challenging task compared to its\\ncounterpart in the inverse direction (i.e., dance generation\\nfrom music) for two primary reasons. First, typical au-\\ndio music signals are high-dimensional and require sophis-\\nticated temporal correlations for overall coherence [4, 28].\\nFor example, CD-quality audio has a typical sampling rate\\nof 44.1 kHz, resulting in over 2.5 million data points (\\xe2\\x80\\x9cdi-\\nmensions\\xe2\\x80\\x9d) for a one-minute musical piece [9].\\nIn con-\\ntrast, most dance generation works output the relatively\\nlow-dimensional motion data in the form of 2D or 3D skele-\\nton keypoints (e.g., displacements for dozens of joints) con-\\nditioned on the music [37, 39, 53, 56], which are then ren-\\ndered into dance sequences and videos. To tackle the chal-\\n\\n1\\n\\nQuantizedD2M-GAN\\xc2\\xb7\\xc2\\xb7\\xc2\\xb712kK+19N\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\x0clenge of the high dimensionality of audio data, research\\nstudies on music generation from visual input [16, 25, 57]\\noften rely on low-dimensional intermediate symbolic au-\\ndio representations (e.g., a 1D piano-roll or 2D MIDI). The\\nsymbolic representations provide existing learning frame-\\nworks with a more explicit audio-visual correlation map-\\nping and more stable training, as well as widely-established\\nstandard music synthesizers for decoding the intermediate\\nrepresentations. However, such symbolic-based works suf-\\nfer from serious limitations on the \\xef\\xac\\x82exibility of the gener-\\nated music. This brings us to the second challenge of dance\\nvideo conditioned music generation: a separately trained\\nmodel is usually required for each instrument, and the gen-\\nerated music is composed with acoustic sounds from a sin-\\ngle prede\\xef\\xac\\x81ned instrument [12, 16, 46]. Consequently, the\\nresulting music is typically simple, and lacking in harmony\\nand richness consistent with the accompanying real-world\\ndance videos (e.g., see the person dancing in a hip-hop style\\nwith piano-based generated samples in our supplementary\\nvideo). These facts make existing conditional music gen-\\neration works dif\\xef\\xac\\x81cult to generalize into complex musical\\nstyles and real-world scenarios.\\n\\nTo \\xef\\xac\\x81ll this gap, we propose a novel adversarial multi-\\nmodal framework that learns to generate complex musical\\nsamples from dance videos via Vector Quantized (VQ) au-\\ndio representations. Inspired by the recent success of VQ-\\nVAE [9,45,52] and VQ-GAN [14], we adopt quantized vec-\\ntors as our intermediate audio representation, and leverage\\nboth their increased abstraction ability compared to contin-\\nuous raw audio signals, as well as their \\xef\\xac\\x82exibility to bet-\\nter represent complex real-world music in comparison to\\nclassic symbolic representations. Speci\\xef\\xac\\x81cally, our frame-\\nwork takes the visual frames and dance motion as input\\n(Figure 1), which are encoded and fused to generate the\\ncorresponding audio VQ representations. After retrieving\\nthe generated VQ representations from a learned codebook,\\nthese entries are decoded back to the raw audio domain us-\\ning a \\xef\\xac\\x81ne-tuned JukeBox decoder [9]. Additionally, we de-\\nploy a convolution-based backbone and follow a hierarchi-\\ncal structure with two separate abstraction levels (i.e., dif-\\nferent hop-lengths) for the audio signals to demonstrate the\\nscalability of our framework. The higher-level model has a\\nlarger hop-length and fewer parameters, resulting in faster\\ninference. In contrast, the lower-level model has a lower ab-\\nstraction level with smaller hop-length, which enables the\\ngeneration of music with higher \\xef\\xac\\x81delity and better quality.\\n\\nLast but not least, we also procure a real-world paired\\ndance-music dataset collected from TikTok video compila-\\ntions. Our dataset contains in total 445 dance videos with 85\\nsongs and an average per-video duration of approximately\\n12.5 seconds. Unlike existing datasets (e.g., AIST [39,60]),\\nours is more challenging and better re\\xef\\xac\\x82ects the conditions of\\nreal-world scenarios, thus providing a new asset for relevant\\n\\nfuture research.\\n\\nEmploying such datasets, we conduct extensive experi-\\nments to demonstrate the effectiveness and robustness of the\\nproposed framework. Speci\\xef\\xac\\x81cally, we design and follow a\\nrich evaluation protocol to consider its generative quality\\nwith respect to the correspondence to the input dance mo-\\ntion in in terms of beats, genre and coherence. The general\\nquality of the generated music is also assessed. The attained\\nresults (both quantitative and qualitative) demonstrate that\\nour model can generate plausible dance music in terms of\\nvarious musical features, outperforming several competitive\\nconditional music generation methods.\\n\\nIn summary, our main contributions are:\\n\\n\\xe2\\x80\\xa2 We propose D2M-GAN, a novel adversarial multi-modal\\nframework that generates complex, free-form music from\\ndance videos via Vector Quantized (VQ) representations.\\n\\n\\xe2\\x80\\xa2 The proposed model, using a VQ generator and a multi-\\nscale discriminator, is able to effectively capture the tem-\\nporal correlations and rhythm for the musical sequence to\\ngenerate complex music.\\n\\n\\xe2\\x80\\xa2 To assess our model, we introduce a comprehensive eval-\\nuation protocol for music conditionally generated from\\nvideos, and demonstrate how the proposed D2M-GAN\\ngenerates more complex and plausibly corresponding mu-\\nsic compared to existing approaches.\\n\\n\\xe2\\x80\\xa2 Last but not least, we create a novel real-world dataset\\nwith dance videos captured in the wild \\xe2\\x80\\x93 and use it to\\nestablish a new, more challenging setup for conditioned\\nmusic generation, which further demonstrates the superi-\\nority of our framework.\\n\\n2. Related Work\\n\\n2.1. Audio, Vision, and Motion\\n\\nCombining data from audio, vision, and motion has been\\na popular research topic in recent years within the \\xef\\xac\\x81eld of\\nmulti-modal learning. Research focusing on general audio-\\nvisual learning typically assumes that the two modalities are\\nintrinsically correlated based on the natural synchronization\\nof the audio and visual signals [2, 3, 34, 47, 48, 67]. Such\\njointly learned audio-visual representations thus can be ap-\\nplied in multiple downstream tasks, like sound source sep-\\naration [17\\xe2\\x80\\x9320, 66], audio-visual captioning [51, 62], audio-\\nvisual action recognition [21, 31], and audio-visual event\\nlocalization and parsing [59, 63, 64, 67].\\n\\nOn the other hand, another branch of studies closely re-\\nlated to our work has investigated the correlations between\\nmotion and sounds [15,16,37\\xe2\\x80\\x9339,53,56,68]. A large portion\\nof this research aims to generate human motion based on\\naudio signals, either in the form of 2D poses [37, 53, 56] or\\ndirect 3D motion [29, 39, 58]. For the inverse direction that\\n\\n2\\n\\n\\x0cseeks to generate audio from motion, Zhao et al. [66] in-\\ntroduces an end-to-end model to generate sounds from mo-\\ntion trajectories using a curriculum learning scheme. Gan\\net al. [16] propose a graph-based transformer framework to\\ngenerate music from performance videos using raw move-\\nment as input. Di et al. [10] propose to generate video\\nbackground music conditioned on the motion and special\\ntiming/rhythmic features of the input videos. In contrast to\\nthese previous works, our work combines three modalities,\\nwhich takes the vision and motion data as input and gener-\\nates music accordingly.\\n\\n2.2. Music Generation\\n\\nRaw music generation is a challenging task due to the\\nhigh dimensionality of the audio data and its sophisti-\\ncated temporal correlations. Therefore, the existing mu-\\nsic generation approaches usually adopt an intermediate\\naudio representation for learning generative models to re-\\nduce the computational demand and simplify the learning\\nprocess [9, 12, 25, 35, 44]. Classical audio representations\\nmainly employ the symbolic and continuous approaches.\\nMusegan [12] introduces a multi-track GAN-based model\\nfor instrumental music generation via 1D piano-roll sym-\\nbolic representations. Music Transformer [25] aims to im-\\nprove the long-term coherence of generated musical pieces\\nusing 2D event-based MIDI-like audio representations [46].\\nMelgan [35] is a generative model for music in form of the\\naudio mel-spectrogram features. Recently, JukeBox [9] in-\\ntroduces a generic music generation model based on the\\nnovel Vector Quantized (VQ) representations. Our pro-\\nposed framework adopts this VQ representation for music\\ngeneration.\\n\\n2.3. Vector Quantized Generative Models\\n\\nVQ-VAEs [45, 52] are \\xef\\xac\\x81rstly proposed as a variant of the\\nVariational Auto-Encoder (VAE) [32] with discrete codes\\nand learned priors. Following works have demonstrated\\nthe potential of VQ-based framework in multiple genera-\\ntive tasks such as image and audio synthesis [9, 14, 26].\\nSpeci\\xef\\xac\\x81cally, the VQ-VAE [45] is initially tested for gen-\\nerating images, videos, and speech. An improved version\\nof VQ-VAE [52] is proposed with a multi-scale hierarchi-\\ncal organization. Esser et al. [14] apply the VQ represen-\\ntations in the GAN-based framework for generating high-\\nresolution images. Dhariwal et al. [9] introduce the Juke-\\nBox as a large-scale generative model for music synthe-\\nsis based on VQ-VAE. Compared to the classic symbolic\\nand continuous audio representations, the VQ representa-\\ntions leverage the bene\\xef\\xac\\x81ts of \\xef\\xac\\x82exibility (i.e., the ability to\\nrepresent complex music genres with a uni\\xef\\xac\\x81ed codebook in\\ncontrast to symbolic representations) and high compression\\nlevels (i.e., the learned codebooks largely reduce the data\\ndimensionality compared to raw waveform or spectrogram).\\n\\nOur proposed framework combines both the GAN [23] and\\nVAE [32], which uses the GAN-based learning to generate\\nVQ representations from the dance videos, and adopts the\\nVAE-based decoder for synthesizing music.\\n\\n3. Method\\n\\nAn overview of the architecture of the proposed D2M-\\nGAN is shown in Figure 2. Our approach employs a hier-\\narchical structure with two levels of generative models that\\nare independently trained with a similar pipeline for \\xef\\xac\\x82exible\\nscalability. In each level, the model consists of four compo-\\nnents: the motion module, the visual module, the VQ mod-\\nule consisting of a VQ generator with multi-scale discrim-\\ninators, and the music synthesizer. Our hierarchical struc-\\nture provides the \\xef\\xac\\x82exibility to balance the generated music\\nquality and computational costs given practical application\\nconsiderations.\\n\\n3.1. Data Representations\\n\\nDuring inference, the input to our proposed D2M-GAN\\ncomes from two domains: the visual frames of the dance\\nvideos and the inferred human body motion of the dancers.\\nThe ground-truth audio is also used as the supervision for\\nthe discriminators during the training stage. For the human\\nbody motion, several different data representations, such as\\nthe 3D Skinned Multi-Person Linear model (SMPL) [41]\\nor 2D body keypoints [5, 6] can be employed in our frame-\\nwork. We use SMPL and 2D body keypoints for different\\ndatasets in our experiments. To encode the visual frames,\\nwe extract I3D features [7] using a model pre-trained on\\nKinectics [30]. For the musical data, we adopt quantized\\nvectors as the intermediate audio representation.\\nIn or-\\nder to leverage the strong representation ability of code-\\nbooks trained on a large-scale musical dataset, we use the\\ncodebooks from a pre-trained JukeBox [9] model, which is\\ntrained on a dataset of 1.2 million songs.\\n\\n3.2. Generator\\n\\nThe generator G = {Gm, Gv, Gvq} includes the motion\\nmodule Gm, the visual module Gv, and the principal VQ\\ngenerator Gvq in the VQ module, which takes the fused\\nmotion-visual data as input and outputs the desired VQ au-\\ndio representations.\\n\\nfvq = Gvq(Gm(xm), Gv(xv)) = G(xm, xv),\\n\\n(1)\\n\\nwhere xm and xv represent the motion and visual input\\ndata, respectively. fvq is the output VQ representations. All\\nthese modules are implemented as convolution-based feed-\\nforward networks. For the principal VQ generator, we use\\nleaky recti\\xef\\xac\\x81ed activation functions [65] for its hidden lay-\\ners and a tanh activation for its last layer before output to\\npromote the stability of GAN-based training [50].\\n\\n3\\n\\n\\x0cFigure 2. Overview of the proposed architecture of the D2M-GAN. Our model takes the motion and visual data from the dance videos as\\ninput and process them with the motion and visual modules, respectively. It then forwards the fused representation containing information\\nfrom both modalities to ground the generation of audio VQ-based representations with the VQ module. The resulting features are calibrated\\nby a multi-scale GAN-based discriminator and are used to perform a lookup in the pre-learned codebook. Last, the retrieved codebook\\nentries are decoded to raw musical samples via by a pre-trained and \\xef\\xac\\x81ne-tuned decoder, responsible for synthesizing music.\\n\\nIt is also worth noting that we \\xef\\xac\\x81nd that using batch nor-\\nmalization and the aforementioned activation function de-\\nsigns [42, 50, 55] is crucial for a stable GAN training in our\\nframework. However, the application of the tanh activa-\\ntion will also restrict the output VQ representations within\\nthe data range between \\xe2\\x88\\x921 and +1. We choose to scale ac-\\ntivation after the last tanh activation by multiplying by a\\nfactor \\xcf\\x83. The hyper-parameter \\xcf\\x83 enlarges the data range\\nof VQ output and makes it possible to perform the lookup\\nof pre-learned large-scale codebooks LookUp(f \\xe2\\x80\\xb2\\nvq) with\\nf \\xe2\\x80\\xb2\\nvq = \\xcf\\x83fvq. Another signi\\xef\\xac\\x81cant observation regarding the\\ngenerator\\xe2\\x80\\x99s design is using a wide receptive \\xef\\xac\\x81eld. Music has\\nlong temporal dependencies and correlations compared to\\nimages, therefore, the principal VQ generator with a larger\\nreceptive \\xef\\xac\\x81eld is bene\\xef\\xac\\x81cial for generating music samples\\nwith better quality, which is consistent with the \\xef\\xac\\x81ndings\\nfrom previous works [11, 35]. To this end, we design our\\ngenerator with relatively large kernel sizes in the convolu-\\ntional layers, and we also add residual blocks with dilations\\nafter the convolutional layers. All previously described sub-\\nmodules within our generator G are jointly optimized.\\n\\n3.3. Multi-Scale Discriminator\\n\\nSimilar\\n\\nthe discriminator\\n\\nto the generator,\\n\\nin the\\nD2M-GAN is also expected to capture the long-term\\ndependencies of musical sig-\\nnals encoded in the gener-\\nated sequence of VQ fea-\\ntures. However, unlike the\\ngenerator design, which fo-\\ncuses on increasing the re-\\nceptive \\xef\\xac\\x81elds of\\nthe neu-\\nral networks, we address\\nthis problem in the discrim-\\ninator design by using a\\nmulti-scale architecture. The\\nmulti-scale discriminator de-\\nsign has been studied in pre-\\nvious works within the \\xef\\xac\\x81eld\\nof audio synthesis and gener-\\nation [33, 35, 61].\\n\\nIllustration of the\\nFigure 3.\\nimportant\\nreshape operation\\nand the window-based dis-\\ncriminator for our D2M-GAN.\\n\\nThe discriminator D = {D1, D2, D3} in the VQ module\\nof our D2M-GAN is composed of 3 discriminators that op-\\nerate on the sequence of generated VQ representations and\\n\\n4\\n\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7Motion InputVisual InputMotion EncoderI3D modelsfused motion-visualfeaturesLow-LevelVQ GeneratorHigh-LevelVQ GeneratorGeneratedVQD1D2D3Low-LevelMulti-scaleDiscriminatorGround TruthVQFine-tuned JukeBoxEncoderFine-tuned JukeBoxDecoderCodebook Lookup79k12kK+19N\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7GT AudioGenerated AudioOmitted due tosimilar pipeline with low level\\xc2\\xb7\\xc2\\xb7\\xc2\\xb71. Motion Module2. Visual Module3. VQ Module4. Synthesis ModulereshapetimeD1 with window-based objective\\x0cits downsampled features by a factor of 2 and 4, respec-\\ntively. Speci\\xef\\xac\\x81cally, unlike the multi-scale discriminators\\nproposed in previous works that directly take the raw audio\\nas input, we reshape the VQ representations f \\xe2\\x80\\xb2\\nvq along the\\ntemporal dimension before feeding them into the discrim-\\ninators, which is also important for D2M-GAN to reach a\\nstable adversarial training, as music is a temporal audio se-\\nquence. Finally, we use the window-based objectives [35]\\n(Markovian window-based discriminator analog to image\\nInstead of learning to distinguish the\\npatches in [27]).\\ndistributions between two entire sequences, window-based\\nobjective learns to classify between distributions of small\\nchunks of VQ sequences to further enhance the overall co-\\nherence as illustrated in Figure 3.\\n\\n3.4. Lookup and Synthesis\\n\\nAfter generating the VQ representations, we perform a\\ncodebook lookup operation similar to other VQ-based gen-\\nerative models [9, 14, 45, 52] to retrieve the closest corre-\\nsponding entries.\\n\\nFinally, we \\xef\\xac\\x81ne-tune the decoder from the JukeBox [9]\\nwithout modifying the codebook entries as the music syn-\\nthesizer for our learned VQ representations. Speci\\xef\\xac\\x81cally,\\nwe also adopt the GAN-based technique for \\xef\\xac\\x81ne-tuning the\\nmusic synthesizer, where the generator is replaced by the\\ndecoder of JukeBox and the discriminator follows the simi-\\nlar architecture as described in the previous subsection.\\n\\n3.5. Training Objectives\\n\\nGAN Loss. We use the hinge loss version of GAN objec-\\ntive [40, 43] adopted for our music generation task to train\\nthe proposed D2M-GAN.\\n\\nLadv.(D; G) = \\xe2\\x88\\x91\\nk\\n\\nLadv.(Dk; G)\\n\\n(E\\xcf\\x86(xa)[min(0, 1 \\xe2\\x88\\x92 Dk(\\xcf\\x86(xa)))]\\n\\n= \\xe2\\x88\\x91\\nk\\n\\n+ E(xm,xv)[min(0, 1 + Dk(G(xm, xv)))]),\\n\\xe2\\x88\\x92Dk(G(xm, xv))],\\n\\nLadv.(G; D) = Exm,xv [\\xe2\\x88\\x91\\n\\n(2)\\n\\n(3)\\n\\nk\\n\\nwhere xa is the original music in a waveform, \\xcf\\x86 represents\\nthe \\xef\\xac\\x81ne-tuned encoder from JukeBox [9]. k indicates the\\nnumber of multi-scale discriminators, which is empirically\\nchosen to be 3 in our case.\\nFeature Matching Loss. To encourage the construction\\nof subtle details in audio signals, we also include a feature\\nmatching loss [36] in the overall training objective. Similar\\nto the audio generation works [33,35], the feature matching\\nloss is de\\xef\\xac\\x81ned as the L1 distance between the discriminator\\nfeature maps of the real and generated VQ features.\\n\\nLF M (G; D) =\\n1\\nNi\\n\\nE(xm,xv)[\\n\\nT\\n\\xe2\\x88\\x91\\ni=1\\n\\n\\xe2\\x88\\xa5Di\\n\\n(\\xcf\\x86(xa)) \\xe2\\x88\\x92 Di\\n\\n(G(xm, xv))\\xe2\\x88\\xa51].\\n\\n(4)\\n\\nCodebook Commitment Loss. The codebook commit-\\nment loss [45, 52] is de\\xef\\xac\\x81ned as the L1 distance between\\nthe generated VQ features and the corresponding codebook\\nentries of the ground truth VQ features after the codebook\\nlookup process.\\n\\nLcode(G) = E(xm,xv)[\\xe2\\x88\\xa5LookU p(\\xcf\\x86(xa) \\xe2\\x88\\x92 G(xm, xv)\\xe2\\x88\\xa51].(5)\\n\\nAudio Perceptual Losses. To further improve the percep-\\ntual auditory quality, we consider the perception losses of\\nthe raw audio signals from both time and frequency do-\\nmains. Speci\\xef\\xac\\x81cally, the perceptual losses are calculated as\\nthe L1 distance between the original audio and the gener-\\nated audio samples:\\n\\nLwav(G) = E(xm,xv)[\\xe2\\x88\\xa5xa \\xe2\\x88\\x92 G(xm, xv)\\xe2\\x88\\xa51].\\nLM el(G) = E(xm,xv)[\\xe2\\x88\\xa5\\xce\\xb8(xa) \\xe2\\x88\\x92 \\xce\\xb8(G(xm, xv))\\xe2\\x88\\xa51].\\nwhere \\xce\\xb8 is the function to compute the mel-spectrogram fea-\\ntures for the audio signal waveforms.\\nFinal Loss. The \\xef\\xac\\x81nal training objective for the entire gen-\\nerator module is de\\xef\\xac\\x81ned as follows:\\n\\n(6)\\n\\n(7)\\n\\nLG = Ladv.(G; D) + \\xce\\xbbf mLF M (G; D)\\n+ \\xce\\xbbcLcode + \\xce\\xbbwLwav + \\xce\\xbbmLmel,\\n\\n(8)\\n\\nwhere the \\xce\\xbbf m, \\xce\\xbbc, \\xce\\xbba, and \\xce\\xbbmel are set to be 3, 15, 40 and\\n15, respectively during our experiments for both levels.\\n\\n4. Experiments\\n\\n4.1. Experimental Setup\\n\\nDatasets. We validate the effectiveness of our method by\\nconducting experiments on two datasets with paired dance\\nvideo and music: the AIST++ [39] and our proposed Tik-\\nTok dance-music dataset. The AIST++ dataset [39] is a\\nsubset of AIST dataset [60] with 3D motion annotations.\\nWe adopt the of\\xef\\xac\\x81cial cross-modality data splits for training,\\nvalidation, and testing, where the videos are divided with-\\nout overlapping musical pieces between the training and the\\nvalidation/testing sets. The number of videos in each split is\\n980, 20, and 20, respectively. The videos from this dataset\\nare \\xef\\xac\\x81lmed in professional studios with clean backgrounds.\\nThere are in total 10 different dance genres and correspond-\\ning music styles, which include breakdancing, pop, lock,\\netc. The number of total songs is 60, with 6 songs for each\\ntype of music. We use this dataset for the main experiments\\nand evaluations.\\n\\nWe also collect and annotate a TikTok dance-music\\ndataset which contains 445 dance videos, with an aver-\\nage length of 12.5 seconds. This dataset contains 85 dif-\\nferent songs, with the majority of videos having a single\\ndance performer, and a maximum of \\xef\\xac\\x81ve performers. The\\ntraining-testing splits contain 392 and 53 videos, respec-\\ntively, without overlapping songs. Figure 4 shows example\\n\\n5\\n\\n\\x0cthis method is also monotonic in terms of the musical instru-\\nment. Controllable Music Transformer (CMT) [10]: CMT\\nis a Transformer-based model proposed for video back-\\nground music generation using MIDI representation. In ad-\\ndition to the above cross-modality models that are closely\\nrelated to our work, we also consider Ground Truth: GT\\nsamples are the original music from dance videos. Juke-\\nBox [9]: music samples generated or reconstructed via the\\nJukeBox model.\\n\\n4.2. Music Evaluations\\n\\nWe design a comprehensive evaluation protocol that in-\\ncorporates objective (i.e., metrics that can be automati-\\ncally calculated) and subjective (i.e., scores given by hu-\\nman testers) metrics to evaluate the generated music from\\nvarious perspectives. Speci\\xef\\xac\\x81cally, the evaluations are di-\\nvided into two categories: the \\xef\\xac\\x81rst category, which is also\\nthe focus of our work, measures correlations between the\\ngenerated music and the input dance videos, for which we\\ncompare our proposed model with other cross-modality mu-\\nsic generation works [1, 10, 16] and a random baseline from\\nJukeBox [9]. The second category focuses on the quality\\nof the music in general, for which we use the reconstructed\\nsamples using JukeBox [9] given the original audio as input\\nand GT samples for comparisons.\\nRhythm. Musical rhythm accounts for an important char-\\nacteristic of the generated music samples, especially given\\nthe dance video as input. To evaluate the correspondence\\nbetween the dance beats and generated musical rhythm, we\\nadopt two objective scores as evaluation metrics, which are\\nthe Beats Coverage Scores and the Beats Hit Scores sim-\\nilar to [8, 37]. Previous works [8, 37] have demonstrated\\nthe kinematic dance and musical beats (i.e., rhythm) are\\ngenerally aligned, we can therefore reasonably evaluate the\\nmusical rhythm by comparing the beats from the generated\\nmusic and those from the GT music samples as shown in\\nFig. 5. We detect the musical beats by the second-level on-\\nset strength [13], which can be considered as the start of an\\nacoustic event. We de\\xef\\xac\\x81ne the number of detected beats from\\nthe generated music samples as Bg, the total beats from the\\noriginal music as Bt, and the number of aligned beats from\\nthe generative samples as Ba. The Beats Coverage Scores\\nBg/Bt measure the ratio of overall generated beats to the\\ntotal musical beats. The Beats Hit Scores Ba/Bt measure\\nthe ratio of aligned beats to the total musical beats. The\\nquantitative results are presented in Table 1. We observe\\nthat both levels of our proposed D2M-GAN achieve better\\nscores compared to competing methods.\\nGenre and Diversity. Dance and music are both diverse\\nin terms of genres. The generated music samples are ex-\\npected to be diverse and harmonious with the given dance\\nstyle (e.g., breakdancing with strong beats paired with mu-\\nsic in fast rhythm). Therefore, we calculate the genre ac-\\n\\nFigure 4. Examples of dance videos from our TikTok dance-\\nmusic dataset. Unlike the AIST dataset [60] where dancing is\\nperformed by professional dancers in a studio environment, our\\ndataset consists of real-world videos collected \\xe2\\x80\\x9cin the wild\\xe2\\x80\\x9d.\\n\\nframes of the dance videos and makes apparent the key dif-\\nferences compared to the professional studio \\xef\\xac\\x81lmed dance\\nvideo from AIST [60]. Our videos have wildly different\\nbackgrounds, and often contain incomplete human body\\nskeleton data, which signi\\xef\\xac\\x81cantly increases the challenge\\nof learning from this dataset. For the TikTok music dataset,\\nwe use 2D human skeleton data as the underlying motion\\nrepresentation.\\nImplementation Details. For the presented experiments,\\nwe adopt a sampling rate of 22.5 kHz for all audio sig-\\nnals. We use the video and audio segments in the length\\nof 2 seconds for training and standard testing in the main\\nexperiments. The generation of longer sequences is also in-\\nvestigated in Section 4.3. The hop lengths for the high and\\nlow level are 128 and 32, respectively. During the GAN\\ntraining, we adopt the Adam optimizer with a learning rate\\nof 1e-4 with \\xce\\xb21 = 0.5 and \\xce\\xb22 = 0.9 for the generators and\\ndiscriminators. We de\\xef\\xac\\x81ne the scaling factor \\xcf\\x83 = 100 for the\\nVQ generators. The number of discriminators k is 3 for the\\nmulti-scale structure. The batch size is set to be 16 for all\\nexperiments. During the \\xef\\xac\\x81ne-tuning of the JukeBox synthe-\\nsizer, we use the Adam optimizer with a learning rate of 1e-\\n5 with \\xce\\xb21 = 0.5 and \\xce\\xb22 = 0.9 for the synthesizer and multi-\\nscale discriminators. We perform a denoising process [54]\\non the generated raw music data for better audio quality.\\nComparisons. We compare our proposed method with sev-\\neral baselines. Foley Music [16]: Foley Music model gener-\\nates MIDI musical representations based on keypoints mo-\\ntion data and then converts the MIDI back to a raw wave-\\nform using a pre-de\\xef\\xac\\x81ned MIDI synthesizer. Speci\\xef\\xac\\x81cally,\\nthe MIDI audio representation is unique for each musical\\ninstrument, and therefore the Foley music model can only\\ngenerate musical samples with mono-instrumental sound.\\nDance2Music [1]: Similar to [16], the generated music with\\n\\n6\\n\\n\\x0cCategory\\n\\nFeatures\\n\\nType\\n\\nMetric\\n\\nDance-Music\\n\\nRhythm\\n\\nObj.\\n\\nBeats Coverage\\n&\\nBeats Hit\\n\\nDance-Music Genre&Diversity\\n\\nObj.\\n\\nGenre Accuracy\\n(Retrieval-based)\\n\\nDance-Music\\n\\nCoherence\\n\\nSubj. Mean Opinion Scores\\n\\nMusic\\n\\nOverall quality\\n\\nSubj. Mean Opinion Scores\\n\\nMethods\\nDance2Music [1]\\nFoley Music [16]\\nCMT [10]\\nOurs High-level\\nOurs Low-level\\n\\nDance2Music [1]\\nFoley Music [16]\\nCMT [10]\\nOurs High-level\\nOurs Low-level\\n\\nRandom JukeBox [9]\\nDance2Music [1]\\nFoley Music [16]\\nCMT [10]\\nOurs High-level\\nOurs Low-level\\nGT\\n\\nJukeBox [9]\\nOurs High-level\\nOurs Low-level\\nGT\\n\\nScores\\n83.5 & 82.4\\n74.1 & 69.4\\n85.5 & 83.5\\n88.2 & 84.7\\n92.3 & 91.7\\n\\n7.0\\n8.1\\n11.6\\n24.4\\n26.7\\n\\n2.0\\n2.8\\n2.8\\n3.0\\n3.5\\n3.3\\n4.6\\n\\n3.5\\n3.5\\n3.7\\n4.8\\n\\nTable 1. Evaluation protocol and the corresponding results for the experiments on the AIST++ dataset [39]. Obj. stands for Objective,\\nwhich means the scores are automatically calculated. Subj. stands for Subjective, which means the scores are given by human evaluators.\\n\\nthe retrieved musical sample has the same genre as the given\\ndance style, we consider the segment to be genre accurate.\\nThe genre accuracy is then calculated by Sc/St, where Sc\\ncounts the number of genre accurate segments and St is the\\ntotal number of segments from the testing split.\\n\\nWe observe in Table 1 that the genre accuracy scores\\nof our D2M-GAN are considerably higher compared to the\\ncompeting methods. This is due to the reason that the\\ncompeting methods rely on MIDI events as audio repre-\\nsentations, which require a speci\\xef\\xac\\x81c synthesizer for each in-\\nstrument, and thus can only generate music samples with\\nmono-instrumental sound.\\nIn contrast, our generated VQ\\naudio representations can represent complex dance mu-\\nsic similar to the input music types, which helps to in-\\ncrease the diversity of the generated music samples. It also\\nmakes the generated samples to be more harmonious with\\nthe dance videos compared to acoustic instrumental sounds\\nfrom [1,10,16], as shown in the next evaluation protocol for\\nthe coherence test.\\nCoherence. Since we generate music samples conditioned\\non the dance videos, the dance video input and the output\\nare expected to be harmonious and coherent when com-\\nbined together. Speci\\xef\\xac\\x81cally, a given dance sequence could\\nbe accompanied by multiple appropriate songs. However,\\nthe evaluation of the dance-music coherence is very subjec-\\ntive, therefore we conduct the Mean Opinion Scores (MOS)\\nhuman test for assessing the coherence feature. During the\\nevaluation process, the human testers are asked to give a\\n\\nFigure 5. Qualitative example of rhythm evaluations and beat\\ncorrespondence. The lower-abstraction level model (D2M-Low)\\nappears to align better than its high-counterpart (D2M-High) with\\nthe ground-truth (GT), which is consistent with the quantitative\\nscores from the Table 1.\\n\\ncuracy for evaluating whether the generated music samples\\nhave a consistent genre with the dance style. The calcu-\\nlation of this objective metric requires the annotations of\\ndance and music genres, therefore, we use the retrieved mu-\\nsical samples from the AIST++ [39] for this evaluation set-\\nting. Speci\\xef\\xac\\x81cally, we retrieve the musical samples with the\\nhighest similarity scores from the segment-level database\\nformed by original audio samples with the same sequence\\nlength. The similarities scores are de\\xef\\xac\\x81ned as the euclidean\\ndistance between the audio features extracted via a VGG-\\nlike network [24] pre-trained on AudioSet [22]. In case that\\n\\n7\\n\\nTimeGTD2M LowD2M High\\x0cBeats Coverage Beats Hit\\n\\nBeats Coverage Beats Hit Genre Acc.\\n\\nModels\\nHigh w/o M\\nHigh w/o V\\nHigh (full)\\nLow w/o M\\nLow w/o V\\nLow (full)\\n\\n85.5\\n86.3\\n88.4\\n83.8\\n85.2\\n87.1\\n\\n72.4\\n81.7\\n82.3\\n74.6\\n81.7\\n83.9\\n\\nTable 2. Evaluations for the experiments on the TikTok dataset.\\n\\nscore between 1 and 5 to evaluate the coherence between\\nthe dance moves and the music given a video with audio\\nsounds. The higher scores indicate the fact the tester feels\\nthe given dance and music are more coherent. We prepare\\nthe videos with original visual frames and fused generated\\nmusic samples for testing.\\nIn addition to the previously\\ncross-modality generation methods [1, 10, 16], we also in-\\nclude the GT samples and the randomly generated music\\nfrom JukeBox [9] for comparison. Our D2M-GAN achieves\\nbetter scores compared to other baselines, which validates\\nthe fact that our proposed framework is able to catch the\\ncorrelations with the given dance video and generates rather\\ncomplex music that well matches the input.\\nOverall Quality. Although our main research focus is to\\nlearn the dance-music correlations in this work, we also\\nlook at the general sound quality of the generated samples.\\nWe conduct the subjective MOS tests similar to the coher-\\nence evaluation, where the human testers are asked to give\\na score between 1 to 5 for the general quality of the music\\nsamples. During this test, only audio signals are played to\\nthe testers. The JukeBox samples are obtained by directly\\nfeeding the GT samples as input. The MOS tests show that\\nour D2M-GAN is able to generate music sample with plau-\\nsible sound quality comparable to the JukeBox. JukeBox\\nhas multiple variants with different hop lengths, we com-\\npare with samples obtained from the model with same au-\\ndio hop length for fairness (i.e., the hop lengths for our high\\nand low levels are 128 and 32, respectively.). It is worth\\nnoting that synthesizing high quality audio itself has been\\na vary challenging and computational demanding research\\ntopic, for example, it takes 3 hrs to sample a 20-seconds\\nhigh-quality music sample with a hop length of 8 [9].\\nResults on the TikTok Dance-Music Dataset. Compared\\nto the AIST++ [39], our TikTok dance-music dataset is a\\nmore challenging dataset with \\xe2\\x80\\x9cin the wild\\xe2\\x80\\x9d video settings\\nthat contains various occlusions and noisy backgrounds. Ta-\\nble 2 shows the quantitative evaluation results for the ex-\\nperiments on the TikTok dataset, which demonstrates the\\noverall robustness of the proposed D2M-GAN.\\n\\n4.3. Ablation Studies\\n\\nSequence Length. In the main experiments, we use the 2-\\nsecond length sequence for experiments with reference to\\n\\nLength\\nHigh - 2s\\nHigh - 3s\\nHigh - 4s\\nLow - 2s\\nLow - 3s\\nLow - 4s\\n\\nModels\\nHigh w/o M\\nHigh w/o V\\nHigh (full)\\nLow w/o M\\nLow w/o V\\nLow (full)\\n\\n88.2\\n88.2\\n87.1\\n92.3\\n90.1\\n88.2\\n\\n83.5\\n87.1\\n88.2\\n89.4\\n90.6\\n92.3\\n\\n84.7\\n85.3\\n83.0\\n91.7\\n88.2\\n84.7\\n\\n82.9\\n88.2\\n84.7\\n87.6\\n90.0\\n91.7\\n\\n24.4\\n25.6\\n23.3\\n26.7\\n25.6\\n23.3\\n\\n15.1\\n16.3\\n24.4\\n15.1\\n17.4\\n26.7\\n\\nTable 3. Results for ablation studies in terms of sequence length.\\n\\nBeats Coverage Beats Hit Genre Acc.\\n\\nTable 4. Results for ablation studies in terms of input modalities\\non the AIST++ dataset. M means the motion data, and V means\\nthe visual data.\\n\\nBeats Coverage Beats Hit Genre Acc.\\n\\nModels\\nHigh 1-layer D.\\nHigh 2-layer D.\\nHigh w/o scaling\\nHigh w/o reshape\\nHigh w/o \\xef\\xac\\x81ne-tune\\nHigh (full)\\nLow 1-layer D.\\nLow 2-layer D.\\nLow w/o scaling\\nLow w/o reshape\\nLow w/o \\xef\\xac\\x81ne-tune\\nLow (full)\\n\\n75.3\\n85.3\\n72.9\\n73.5\\n87.0\\n88.2\\n73.5\\n87.0\\n72.4\\n73.5\\n92.3\\n92.3\\n\\n72.9\\n82.9\\n71.8\\n70.1\\n84.7\\n84.7\\n71.8\\n85.9\\n70.1\\n71.8\\n91.2\\n91.7\\n\\n9.3\\n21.0\\n14.0\\n11.6\\n24.4\\n24.4\\n8.1\\n22.1\\n12.8\\n12.8\\n26.7\\n26.7\\n\\nTable 5. Results for ablation studies in terms of model architec-\\ntures on the AIST++ dataset. D. means discriminators.\\n\\nother similar cross-modality generation tasks [39]. How-\\never, our model can also be effectively trained and tested\\nwith a longer sequence length as shown in Table 3 via a\\nrelatively larger network with more parameters.\\nData Modality. We perform ablation studies in terms of the\\ninput data modalities, by removing either the dance motion\\nor the visual frame from the input data. Table 4 lists the\\ncorresponding experimental results. We observe that both\\nmotion and visual data contribute to our conditioned mu-\\nsic generation task. Speci\\xef\\xac\\x81cally, the motion data impose\\na larger impact on the musical rhythm, which is consistent\\nwith our expectations since the musical rhythm is closely\\ncorrelated with the dance motion.\\nModel Architecture. We also test various variants of our\\nD2M-GAN in terms of the model architecture and proposed\\nmodel design techniques. The corresponding results are\\nrepresented in Tabel 5. The experimental results show that\\nthe multi-scale layer for the discriminators, the scaling op-\\neration in the generator, as well as the reshape techniques\\nfor discriminators are crucial.\\n\\n8\\n\\n\\x0cBeats Coverage Beats Hit Genre Acc.\\n\\nReferences\\n\\nLosses\\nHigh w/o LF M\\nHigh w/o Lwav\\nHigh w/o Lmel\\nHigh (full)\\nLow w/o LF M\\nLow w/o Lwav\\nLow w/o Lmel\\nLow (full)\\n\\n85.3\\n85.9\\n77.6\\n88.2\\n91.7\\n89.4\\n78.8\\n92.3\\n\\n84.7\\n84.7\\n76.5\\n84.7\\n90.1\\n88.8\\n77.1\\n91.7\\n\\n23.3\\n23.3\\n18.6\\n24.4\\n24.4\\n23.3\\n17.4\\n26.7\\n\\nTable 6. Results for ablation studies in terms of losses on the\\nAIST++ dataset.\\n\\nLoss function. We analyze the impact of different losses\\nincluded in the overall training objective. The results from\\nTable 6 show the contributions of each loss term. Specif-\\nically, we observe the audio perceptual loss from the fre-\\nquency domain Lmel helps with the generation of musical\\nrhythm, it is reasonable due to the fact that mel-spectrogram\\nfeatures help to capture the high frequencies from the audio\\nsignals, which is closely related to the dance beats.\\n\\n5. Conclusion and Discussion\\n\\nWe propose D2M-GAN framework for complex music\\ngeneration from dance videos via the VQ audio representa-\\ntions. Extensive experiments on multiple datasets, and com-\\nprehensive evaluations in terms of various musical charac-\\nteristics prove the effectiveness of our method. We also in-\\ntroduce a novel TikTok dance-music dataset in this work.\\n\\nAs for limitations, the proposed D2M-GAN is an end-to-\\nend framework, an interesting future direction is to explore\\nhow one can use controllable conditioning information to\\npromote an interactively editing/generating system.\\n\\n[1] Gunjan Aggarwal and Devi Parikh. Dance2music: Au-\\narXiv preprint\\n\\ntomatic dance-driven music generation.\\narXiv:2107.06252, 2021. 1, 6, 7, 8\\n\\n[2] Relja Arandjelovic and Andrew Zisserman. Look, listen and\\n\\nlearn. In ICCV, 2017. 2\\n\\n[3] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Sound-\\nnet: Learning sound representations from unlabeled video.\\nNeurIPS, 2016. 2\\n\\n[4] Jean-Pierre Briot, Ga\\xc2\\xa8etan Hadjeres, and Franc\\xc2\\xb8ois-David Pa-\\nchet. Deep learning techniques for music generation, vol-\\nume 1. Springer, 2020. 1\\n\\n[5] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.\\nSheikh. Openpose: Realtime multi-person 2d pose estima-\\ntion using part af\\xef\\xac\\x81nity \\xef\\xac\\x81elds. IEEE TPAMI, 2019. 3\\n\\n[6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\\nRealtime multi-person 2d pose estimation using part af\\xef\\xac\\x81nity\\n\\xef\\xac\\x81elds. In CVPR, 2017. 3\\n\\n[7] Joao Carreira and Andrew Zisserman. Quo vadis, action\\nrecognition? a new model and the kinetics dataset. In CVPR,\\n2017. 3\\n\\n[8] Abe Davis and Maneesh Agrawala. Visual rhythm and beat.\\n\\nIn ACM Transactions on Graphics (TOG), 2018. 6\\n\\n[9] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook\\nKim, Alec Radford, and Ilya Sutskever. Jukebox: A gen-\\nerative model for music. arXiv preprint arXiv:2005.00341,\\n2020. 1, 2, 3, 5, 6, 7, 8, 12\\n\\n[10] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan\\nZhu, Zexin He, Hongming Liu, and Shuicheng Yan. Video\\nbackground music generation with controllable music trans-\\nformer. In ACMMM, 2021. 3, 6, 7, 8\\n\\n[11] Chris Donahue, Julian McAuley, and Miller Puckette. Ad-\\n\\nversarial audio synthesis. In ICLR, 2019. 4\\n\\n[12] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan\\nYang. Musegan: Multi-track sequential generative adversar-\\nial networks for symbolic music generation and accompani-\\nment. In AAAI, 2018. 2, 3\\n\\n[13] Daniel PW Ellis. Beat tracking by dynamic programming.\\n\\nJournal of New Music Research, 2007. 6\\n\\n[14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\\ntransformers for high-resolution image synthesis. In CVPR,\\n2021. 2, 3, 5\\n\\n[15] Joao P Ferreira, Thiago M Coutinho, Thiago L Gomes,\\nJos\\xc2\\xb4e F Neto, Rafael Azevedo, Renato Martins, and Erick-\\nson R Nascimento. Learning to dance: A graph convolu-\\ntional adversarial network to generate realistic dance mo-\\ntions from audio. Computers & Graphics, 2021. 2\\n\\n[16] Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenen-\\nbaum, and Antonio Torralba. Foley music: Learning to gen-\\nerate music from videos. In ECCV, 2020. 2, 3, 6, 7, 8\\n[17] Chuang Gan, Deng Huang, Hang Zhao, Joshua B Tenen-\\nbaum, and Antonio Torralba. Music gesture for visual sound\\nseparation. In CVPR, 2020. 2\\n\\n[18] Ruohan Gao, Rogerio Feris, and Kristen Grauman. Learning\\nto separate object sounds by watching unlabeled video. In\\nECCV, 2018. 2\\n\\n9\\n\\n\\x0c[19] Ruohan Gao and Kristen Grauman. 2.5 d visual sound. In\\n\\nCVPR, 2019. 2\\n\\n[20] Ruohan Gao and Kristen Grauman. Co-separating sounds of\\n\\nvisual objects. In ICCV, 2019. 2\\n\\n[21] Ruohan Gao, Tae-Hyun Oh, Kristen Grauman, and Lorenzo\\nTorresani. Listen to look: Action recognition by previewing\\naudio. In CVPR, 2020. 2\\n\\n[22] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren\\nJansen, Wade Lawrence, R Channing Moore, Manoj Plakal,\\nand Marvin Ritter. Audio set: An ontology and human-\\nIn ICASSP. IEEE, 2017.\\nlabeled dataset for audio events.\\n7\\n\\n[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\nYoshua Bengio. Generative adversarial nets. In NIPS, 2014.\\n3\\n\\n[24] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F\\nGemmeke, Aren Jansen, R Channing Moore, Manoj Plakal,\\nDevin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn ar-\\nchitectures for large-scale audio classi\\xef\\xac\\x81cation. In ICASSP.\\nIEEE, 2017. 7\\n\\n[25] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit,\\nNoam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M\\nDai, Matthew D Hoffman, Monica Dinculescu, and Douglas\\nEck. Music transformer: Generating music with long-term\\nstructure. In ICLR, 2019. 2, 3\\n\\n[26] Vladimir Iashin and Esa Rahtu. Taming visually guided\\nIn British Machine Vision Conference\\n\\nsound generation.\\n(BMVC), 2021. 3\\n\\n[27] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\\nImage-to-image translation with conditional adver-\\n\\nEfros.\\nsarial networks. In CVPR, 2017. 5\\n\\n[28] Shulei Ji, Jing Luo, and Xinyu Yang. A comprehensive sur-\\nvey on deep music generation: Multi-level representations,\\nalgorithms, evaluations, and future directions. arXiv preprint\\narXiv:2011.06801, 2020. 1\\n\\n[29] Hsuan-Kai Kao and Li Su. Temporally guided music-to-\\n\\nbody-movement generation. In ACMMM, 2020. 2\\n\\n[30] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\\nman action video dataset. arXiv preprint arXiv:1705.06950,\\n2017. 3\\n\\n[31] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and\\nDima Damen. Epic-fusion: Audio-visual temporal binding\\nfor egocentric action recognition. In ICCV, 2019. 2\\n\\n[32] Diederik P Kingma and Max Welling. Auto-encoding varia-\\n\\ntional bayes. In ICLR, 2014. 3\\n\\n[33] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hi\\xef\\xac\\x81-gan:\\nGenerative adversarial networks for ef\\xef\\xac\\x81cient and high \\xef\\xac\\x81-\\ndelity speech synthesis. In NIPS, 2020. 4, 5\\n\\n[34] Bruno Korbar, Du Tran, and Lorenzo Torresani. Coopera-\\ntive learning of audio and video models from self-supervised\\nsynchronization. In NeurIPS, 2018. 2\\n\\n[35] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lu-\\ncas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de\\n\\nBr\\xc2\\xb4ebisson, Yoshua Bengio, and Aaron C Courville. Mel-\\ngan: Generative adversarial networks for conditional wave-\\nform synthesis. In NIPS, 2019. 3, 4, 5\\n\\n[36] Anders Boesen Lindbo Larsen, S\\xc3\\xb8ren Kaae S\\xc3\\xb8nderby, Hugo\\nLarochelle, and Ole Winther. Autoencoding beyond pixels\\nusing a learned similarity metric. In International conference\\non machine learning. PMLR, 2016. 5\\n\\n[37] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun\\nWang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz.\\nDancing to music. In NIPS, 2019. 1, 2, 6, 13\\n\\n[38] Buyu Li, Yongchi Zhao, and Lu Sheng. Dancenet3d: Music\\nbased dance generation with parametric motion transformer.\\narXiv preprint arXiv:2103.10206, 2021. 1, 2\\n\\n[39] Ruilong Li, Shan Yang, David A. Ross, and Angjoo\\nKanazawa. Ai choreographer: Music conditioned 3d dance\\ngeneration with aist++. In ICCV, 2021. 1, 2, 5, 7, 8, 13\\n[40] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv\\n\\npreprint arXiv:1705.02894, 2017. 5\\n\\n[41] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\\nmulti-person linear model. ACM Trans. Graphics (Proc.\\nSIGGRAPH Asia), 34, 2015. 3\\n\\n[42] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain\\nGelly, and Olivier Bousquet. Are gans created equal? a\\nlarge-scale study. In NeurIPS, 2018. 4\\n\\n[43] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and\\nYuichi Yoshida. Spectral normalization for generative ad-\\nversarial networks. arXiv preprint:1802.05957, 2018. 5\\n[44] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen\\nSimonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,\\nAndrew Senior, and Koray Kavukcuoglu. Wavenet: A gen-\\nerative model for raw audio. In ICLR, 2016. 3\\n[45] Aaron van den Oord, Oriol Vinyals,\\n\\nand Koray\\nNeural discrete representation learning.\\n\\nKavukcuoglu.\\nIn NIPS, 2017. 2, 3, 5, 12\\n\\n[46] Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck,\\nand Karen Simonyan. This time with feeling: Learning ex-\\npressive musical performance. Neural Computing and Ap-\\nplications, pages 955\\xe2\\x80\\x93967, 2020. 2, 3\\n\\n[47] Andrew Owens and Alexei A Efros. Audio-visual scene\\nanalysis with self-supervised multisensory features.\\nIn\\nECCV, 2018. 2\\n\\n[48] Andrew Owens, Jiajun Wu, Josh H McDermott, William T\\nFreeman, and Antonio Torralba. Ambient sound provides\\nsupervision for visual learning. In ECCV, 2016. 2\\n\\n[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-\\nperative style, high-performance deep learning library. In H.\\nWallach, H. Larochelle, A. Beygelzimer, F. d \\xc2\\xb4Alch\\xc2\\xb4e-Buc, E.\\nFox, and R. Garnett, editors, Advances in Neural Informa-\\ntion Processing Systems 32, pages 8024\\xe2\\x80\\x938035. Curran Asso-\\nciates, Inc., 2019. 12\\n\\n10\\n\\n\\x0c[67] Ye Zhu, Yu Wu, Hugo Latapie, Yi Yang, and Yan Yan. Learn-\\ning audio-visual correlations from variational cross-modal\\ngeneration. In ICCASP, 2021. 2\\n\\n[68] Wenlin Zhuang, Congyi Wang, Siyu Xia, Jinxiang Chai, and\\nYangang Wang. Music2dance: Dancenet for music-driven\\ndance generation. arXiv preprint arXiv:2002.03761, 2020. 2\\n\\n[50] Alec Radford, Luke Metz, and Soumith Chintala. Un-\\nsupervised representation learning with deep convolu-\\narXiv preprint\\ntional generative adversarial networks.\\narXiv:1511.06434, 2015. 3, 4\\n\\n[51] Tanzila Rahman, Bicheng Xu, and Leonid Sigal. Watch,\\nlisten and tell: Multi-modal weakly supervised dense event\\ncaptioning. In ICCV, 2019. 2\\n\\n[52] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Gen-\\nerating diverse high-\\xef\\xac\\x81delity images with vq-vae-2. In NIPS,\\n2019. 2, 3, 5\\n\\n[53] Xuanchi Ren, Haoran Li, Zijian Huang, and Qifeng Chen.\\nSelf-supervised dance video synthesis conditioned on music.\\nIn ACM MM, 2020. 1, 2\\n\\n[54] Tim Sainburg, Marvin Thielk, and Timothy Q Gentner. Find-\\ning, visualizing, and quantifying latent structure across di-\\nverse animal vocal repertoires. PLoS computational biology,\\n16(10):e1008228, 2020. 6\\n\\n[55] Tim Salimans and Durk P Kingma. Weight normalization:\\nA simple reparameterization to accelerate training of deep\\nneural networks. In NeurIPS, 2016. 4\\n\\n[56] Eli Shlizerman, Lucio Dery, Hayden Schoen, and Ira\\nKemelmacher-Shlizerman. Audio to body dynamics.\\nIn\\nCVPR, 2018. 1, 2\\n\\n[57] Kun Su, Xiulong Liu, and Eli Shlizerman. Audeo: Audio\\ngeneration for a silent performance video. In NeurIPS, 2020.\\n2\\n\\n[58] Taoran Tang, Jia Jia, and Hanyang Mao. Dance with melody:\\nAn lstm-autoencoder approach to music-oriented dance syn-\\nthesis. In ACMMM, 2018. 2\\n\\n[59] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chen-\\nliang Xu. Audio-visual event localization in unconstrained\\nvideos. In ECCV, 2018. 2\\n\\n[60] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki,\\nand Masataka Goto. Aist dance video database: Multi-genre,\\nmulti-dancer, and multi-camera database for dance informa-\\ntion processing. In Proceedings of the 20th International So-\\nciety for Music Information Retrieval Conference, (ISMIR),\\n2019. 2, 5, 6, 13\\n\\n[61] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,\\nJan Kautz, and Bryan Catanzaro. High-resolution image syn-\\nthesis and semantic manipulation with conditional gans. In\\nCVPR, 2018. 4\\n\\n[62] Xin Wang, Yuan-Fang Wang, and William Yang Wang.\\nWatch, listen, and describe: Globally and locally aligned\\nIn NAACL,\\ncross-modal attentions for video captioning.\\n2018. 2\\n\\n[63] Yu Wu and Yi Yang. Exploring heterogeneous clues for\\nIn CVPR,\\n\\nweakly-supervised audio-visual video parsing.\\n2021. 2\\n\\n[64] Yu Wu, Linchao Zhu, Yan Yan, and Yi Yang. Dual attention\\nmatching for audio-visual event localization. In ICCV, 2019.\\n2\\n\\n[65] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical\\nevaluation of recti\\xef\\xac\\x81ed activations in convolutional network.\\narXiv preprint arXiv:1505.00853, 2015. 3\\n\\n[66] Hang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Tor-\\n\\nralba. The sound of motions. In ICCV, 2019. 2, 3\\n\\n11\\n\\n\\x0cA. Network Architecture\\n\\nA.1. Generator\\n\\nThe following Table 7, Table 8, Table 9 and Table 10\\nshow the detailed model architectures of the motion en-\\ncoder, high-level VQ generator, low-level VQ generator and\\nthe residual block, respectively.\\n\\n6 \\xc3\\x97 1, stride=1, Conv 256, LeakyReLU\\nResidual Stack 256\\n3 \\xc3\\x97 1, stride=1, Conv 512, LeakyReLU\\nResidual Stack 512\\n3 \\xc3\\x97 1, stride=1, Conv 1024, LeakyReLU\\nResidual Stack 1024\\n3 \\xc3\\x97 1 , stride=1, Conv 1024, LeakyReLU\\n4 \\xc3\\x97 1, stride=1, Conv 1\\n\\nTable 7. Architecture for the motion encoder.\\n\\n6 \\xc3\\x97 1, stride=2, Conv 32, LeaklyReLU\\nResidual Stack 32\\n41 \\xc3\\x97 1, stride=2, Conv 64, LeakyReLU\\nResidual Stack 64\\n41 \\xc3\\x97 1, stride=1, Conv 128, LeakyReLU\\nResidual Stack 128\\n41 \\xc3\\x97 1, stride=1, Conv 256, LeaklyReLU\\nResidual Stack 256\\n41 \\xc3\\x97 1, stride=1, Conv 512, LeakyReLU\\nResidual Stack 512\\n40 \\xc3\\x97 1, stride=1, Conv 64\\nTanh()\\n\\nTable 8. Architecture for the high-level VQ generator.\\n\\nA.2. Discriminator\\n\\nWe adopt the multi-scale discriminator design for the\\nproposed D2M-GAN, where is formed by a stack of 3\\ndiscriminator blocks that operates on the original VQ\\nsequence, and its downsampled features based on the\\nwindow-based objective functions as introduced in the main\\npaper. The architecture of each discriminator block is\\nshown below in Table 11.\\n\\nB. Experimental Details\\n\\nWe implement\\n\\nthe entire framework using the Py-\\nTorch [49] framework for automatic differentiation and\\nGPU-accelerated training and inference.\\nPre-learned Codebook. We adopt two independently pre-\\ntrained codebooks for two levels in our D2M-GAN. Speci\\xef\\xac\\x81-\\ncally, the original JukeBox [9] contains three levels of VQ-\\n\\n12\\n\\n6 \\xc3\\x97 1, stride=2, Conv 32, LeakyReLU\\nResidual Stack 32\\n4 \\xc3\\x97 1, stride=1, Conv 64, LeakyReLU\\nResidual Stack 64\\n40 \\xc3\\x97 1, stride=2, Conv 128, LeakyReLU\\nResidual Stack 128\\n40 \\xc3\\x97 1, stride=1, Conv 256, LeakyReLU\\nResidual Stack 256\\n40 \\xc3\\x97 1, stride=1, Conv 512, LeakyReLU\\nResidual Stack 512\\n40 \\xc3\\x97 1, stride=1, Conv 1024, LeakyReLU\\nResidual Stack 1024\\n40 \\xc3\\x97 1, stride=1, Conv 1024, LeakyReLU\\n40 \\xc3\\x97 1, stride=1, Conv 64, LeakyReLU\\nTanh()\\n\\nTable 9. Architecture for the low-level VQ generator.\\n\\nLeakyReLU, dilation=1, Conv\\nLeakyReLU, dilation=1. Conv\\nShortcut Path\\nLeakyReLU, dilation=3, Conv\\nLeakyReLU, dilation=1, Conv\\nShortcut Path\\nLeakyReLU, dilation=9, Conv\\nLeakyReLU, dilation=1, Conv\\nShortcut Path\\n\\nTable 10. Architecture for the residual stack.\\n\\n15 \\xc3\\x97 1, stride=1, Conv 16, LeakyReLU\\n41 \\xc3\\x97 1, stride=4, Groups=4, Conv 64, LeakyReLU\\n41 \\xc3\\x97 1, stride=4, Groups=16, Conv 256, LeakyReLU\\n41 \\xc3\\x97 1, stride=4, Groups=64, Conv 1024, LeakyReLU\\n41 \\xc3\\x97 1, stride=4, Groups=256, Conv 1024, LeakyReLU\\n5 \\xc3\\x97 1, stride=1, Conv 1024, LeakyReLU\\n3 \\xc3\\x97 1, stride=1, Conv 1\\n\\nTable 11. Architecture for the discriminator block.\\n\\nVAE [45] based models, which are de\\xef\\xac\\x81ned as top, middle\\nand bottom levels with hop lengths of 128, 32, and 8, re-\\nspectively. We adopt the top level codebook for the high-\\nlevel D2M-GAN, and the middle level codebook for the\\nlow-level D2M-GAN. Therefore, for a two-second audio se-\\nquence with a sampling rate of 22050 Hz, the generated VQ\\nsequences from the high-level and low-level VQ generators\\nare in dimension of 64 \\xc3\\x97 344 and 64 \\xc3\\x97 1378, respectively,\\nwhere 64 is the dimension of the codebook entry, 344 and\\n1378 are the sequence lengths.\\nTraining Losses. Since our proposed D2M-GAN includes\\n\\n\\x0cFigure 6. Training losses for the proposed D2M-GAN. Adv. stands for adversarial.\\n\\nmultiple loss terms in the overall training objective, we\\nshow the change of each loss term during the training pro-\\ncess in Figure 6. It is worth noting the model architectures\\nand techniques described in our main paper are crucial for\\nD2M-GAN to maintain a stable training. Notably, the code-\\nbook commitement loss, audio waveform loss and audio\\nmel-spectrogram loss can reach the comparable levels with\\nthe GT audio samples after convergence.\\n\\nC. TikTok Dance-Music Dataset\\n\\nThe current version of our TikTok dance-music dataset\\ncontains in total 445 videos, which we annotate from 15\\nTikTok dance video compilations. There are 85 different\\nsongs, with majority of videos having a single dance per-\\nformer, and a maximum of \\xef\\xac\\x81ve performers. The average\\nlength of each video is approximately 12.5s. We split the\\ntraining and testing set based on the music IDs, and ensure\\nthat there are no overlapping songs for two splits.\\n\\nCompared to the existing music and dance datasets such\\nas AIST++ [39, 60], our dataset is closer to the real-world\\nscenario with various background, which is also our initial\\nmotivation to introduce this dataset. Additionally, majority\\nof the current datasets available are not initially proposed\\nfor the dance to music generation task, AIST [60] is de-\\nsigned for dance music processing, AIST++ [39] provides\\nthe extra annoations for the subset of AIST for generat-\\ning dance motion conditioned on music, some other sim-\\nilar datasets for motion generation have also been intro-\\nduced [37]. Therefore, we hope that our proposed TikTok\\ndance-music dataset can serve as a starting point for rele-\\nvant future researches.\\n\\nD. Subjective Evaluations\\n\\nWe conduct the Mean Opinion Scores (MOS) test for the\\nsubjective evaluations. In total, 26 subjects participated our\\nMOS tests, among which 9 of them are females, the rest are\\nmales.\\n\\nTwo of our music evaluation protocols are based on the\\nhuman subjective evaluations, which are the dance-music\\ncoherence test and the music overall quality test. For the\\ndance-music coherence test, each evaluator is asked to rate\\n15 dance videos that are post-processed by fusing the orig-\\ninal visual frames and generated music samples from dif-\\nferent models. Speci\\xef\\xac\\x81cally, the evaluators are asked to rate\\nfrom the coherence aspect of the dance video (i.e., whether\\nthey feels the music is coherent with the dance moves)\\nwith reference to the GT videos and original music. For\\nthe overall quality test, 15 audio samples (without video\\nframes) are played during the test for each evaluator, af-\\nter which the evaluator is asked to rate the sound quality\\nfrom the score range of 1 to 5. It is worth noting that for\\nthe overall quality test, we do not compare with the music\\nsamples obtained from the symbolic MIDI representation\\nbased methods. This is due to the reason that the symbolic\\nrepresentations and pre-de\\xef\\xac\\x81ned music synthesizers in na-\\nture do not introduce audio noises to the generated signals,\\nwhich makes the music samples sound rather \\xe2\\x80\\x9cclean and\\nhigh-quality\\xe2\\x80\\x9d, while the continuous or VQ audio represen-\\ntations can hardly achieve the similar effects with a learned\\nmusic synthesizer (samples included in our demo video).\\nTherefore, we do not include the MIDI-based methods as\\nour baselines for fairness considerations.\\n\\n13\\n\\n(a) Adv. loss for generator.(b) Adv. loss for discriminator.(c) Feature matching loss.(d) Codebook commitment loss.(e) Audio waveform loss.(f) Audio mel-spectrogram loss.\\x0c']\n"
     ]
    }
   ],
   "source": [
    "#Encoding all the pdf text in UTF-8\n",
    "\n",
    "elements=[]\n",
    "for i in array_pdf_text:\n",
    "    elements.append(i.encode(\"utf-8\"))\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a4129a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottou',\n",
       "  'David_Lopez_Paz',\n",
       "  'Kyunghyun_Cho',\n",
       "  'Yoshua_Bengio',\n",
       "  'Food_Xe',\n",
       "  'David_Fleet_Tomas_Pajdla_Bernt_Schiele',\n",
       "  'Bottou_Frank',\n",
       "  'Jorge_Nocedal',\n",
       "  'Nxavier_Bouthillier_Christos',\n",
       "  'Corneau',\n",
       "  'Thomas_Schweizer',\n",
       "  'Lin_Dong_Npierre',\n",
       "  'Michael_Nnoukhovitch',\n",
       "  'Chao_Xue',\n",
       "  'Satya_Ortiz_Gagn',\n",
       "  'Peter_Henderson',\n",
       "  'Christopher_Beckham',\n",
       "  'Joulin_Matthijs_Douze',\n",
       "  'Joulin_Nunsupervised',\n",
       "  'Ntianlong_Chen_Sijia_Liu_Shiyu_Chang_Yu',\n",
       "  'Cheng',\n",
       "  'Lisa_Amini',\n",
       "  'Zhangyang_Wang',\n",
       "  'Simon_Kornblith_Mohammad_Norouzi_Geoffrey_Hinton',\n",
       "  'Hal_Daum',\n",
       "  'Jul_Nxinlei',\n",
       "  'Chen_Kaiming',\n",
       "  'Cimpoi_Subhransu',\n",
       "  'Sammy_Mohamed_Andrea_Vedaldi',\n",
       "  'Elisa_Ricci',\n",
       "  'Rob_Romijn',\n",
       "  'Dustin_Tran_Mario_Lucic',\n",
       "  'Njosip_Djolonga',\n",
       "  'Jessica_Yung_Michael',\n",
       "  'Alexander_Nkolesnikov',\n",
       "  'Minderer_Alexander',\n",
       "  'Dan_Moldovan_Syl',\n",
       "  'Neil_Houlsby',\n",
       "  'Mario_Lucic',\n",
       "  'Nli_Fei_Fei_Rob_Fergus_Pietro_Perona',\n",
       "  'Greg_Wayne_Ivo_Danihelka',\n",
       "  'Njean_Bastien_Grill',\n",
       "  'Corentin_Tallec',\n",
       "  'Carl_Doersch',\n",
       "  'Remi_Munos_Michal_Valko',\n",
       "  'Lin',\n",
       "  'Harris',\n",
       "  'Jarrod_Millman_St',\n",
       "  'Walt_Ralf',\n",
       "  'David_Ncournapeau',\n",
       "  'Eric_Wieser',\n",
       "  'Sebastian_Berg_Nathaniel',\n",
       "  'Robert_Kern',\n",
       "  'Kerkwijk_Matthew_Brett',\n",
       "  'Allan_Haldane',\n",
       "  'Mark_Wiebe_Pearu_Peterson',\n",
       "  'Kevin_Sheppard_Tyler',\n",
       "  'Abbasi_Christoph_Gohlke_Travis',\n",
       "  'Jian_Sun',\n",
       "  'Wu_Saining_Xie_Ross_Girshick',\n",
       "  'Norman_Mu_Saurav',\n",
       "  'Kadavath_Frank',\n",
       "  'Zhu_Samyak',\n",
       "  'Mike_Guo_Dawn_Song_Jacob_Steinhardt',\n",
       "  'Nr_Devon',\n",
       "  'Alex_Fedorov_Samuel_Lavoie_Marchildon',\n",
       "  'Phil_Bachman',\n",
       "  'Adam_Ntrischler',\n",
       "  'Francis_Bach',\n",
       "  'Proceedings',\n",
       "  'Xe_Jul',\n",
       "  'Nprannay_Khosla_Piotr',\n",
       "  'Chen_Wang',\n",
       "  'Dilip_Krishnan',\n",
       "  'Nalexander_Kolesnikov_Xiaohua',\n",
       "  'Michael_Stark',\n",
       "  'Jia_Deng_Li_Fei_Fei',\n",
       "  'Xef_Xac_Xne',\n",
       "  'Nkuang_Huei',\n",
       "  'John_Canny_Ian_Fischer',\n",
       "  'Nmaria_Elena_Nilsback_Andrew_Zisserman',\n",
       "  'Andrea_Vedaldi_Andrew_Zisserman',\n",
       "  'Jawahar',\n",
       "  'Sam_Gross_Francisco_Massa',\n",
       "  'James_Bradbury',\n",
       "  'Killeen_Zeming',\n",
       "  'Edward_Yang',\n",
       "  'Devito_Martin',\n",
       "  'Steiner_Lu',\n",
       "  'Fang_Junjie_Bai_Soumith',\n",
       "  'Alch',\n",
       "  'Garnett',\n",
       "  'Rebecca_Roelofs',\n",
       "  'Naaqib_Saeed',\n",
       "  'David_Grangier_Neil_Zeghidour',\n",
       "  'Audio_Nrepresentations',\n",
       "  'Vaart_Jon',\n",
       "  'Wellner',\n",
       "  'Parmar_Jakob',\n",
       "  'Jones_Aidan',\n",
       "  'Gomez_Lukasz_Nkaiser_Illia_Polosukhin',\n",
       "  'Wang_Zhongqi_Yue_Jianqiang_Huang_Qianru',\n",
       "  'Sun_Hanwang_Zhang',\n",
       "  'Self_Supervised_Nlearning',\n",
       "  'Njianxiong_Xiao',\n",
       "  'James_Hays',\n",
       "  'Antonio_Torralba',\n",
       "  'Abbey_Zoo',\n",
       "  'Ieee_Nasano_Ym_Rupprecht_Vedaldi',\n",
       "  'Hyx_Jybfpr_Nyuning',\n",
       "  'Yongduo_Sui_Ting_Chen_Zhangyang',\n",
       "  'Wang_Yang_Shen',\n",
       "  'Graph',\n",
       "  'Deny',\n",
       "  'Self',\n",
       "  'Proof_Theorem',\n",
       "  'De_Xef',\n",
       "  'Kl',\n",
       "  'Xczk_Kl_Xe_Kj',\n",
       "  'Nthen_De_Xef',\n",
       "  'Xy',\n",
       "  'Kl_Xe',\n",
       "  'Xczk_Kl_Nwe',\n",
       "  'Xe_Qj_Ik',\n",
       "  'Xe_Ck',\n",
       "  'Xce_Xbk',\n",
       "  'Eqt_Xcf',\n",
       "  'Xe_Supq',\n",
       "  'Xe_Xqi_Not',\n",
       "  'Xcf_Xcf',\n",
       "  'Ck',\n",
       "  'Xce_Xb_Probability_Least',\n",
       "  'Xe_Ner',\n",
       "  'Xa_Xad',\n",
       "  'Xbk_Xe',\n",
       "  'Xef_Xa_Xb',\n",
       "  'Xef_Xa',\n",
       "  'Xce_Xb_Nn',\n",
       "  'Ck_Pr',\n",
       "  'Pr_Xe',\n",
       "  'Xe_Xik_Nk',\n",
       "  'Npr_Xe',\n",
       "  'Xef_Xac',\n",
       "  'Xef_Xa_Xab',\n",
       "  'Xef_Xa_Xaderk',\n",
       "  'Xe_Xef_Xa',\n",
       "  'Xe_Xik_Nsubstituting',\n",
       "  'Xe_Ik',\n",
       "  'Ck_Xe_Nk',\n",
       "  'Xe_Xik',\n",
       "  'Xef_Xa_Xb_Xef_Xa_Xb_Xcby',\n",
       "  'Vaart_Wellner',\n",
       "  'Xe_Xa_Nk',\n",
       "  'Xe_Xa',\n",
       "  'Xe_Xce',\n",
       "  'Xab',\n",
       "  'Nbase',\n",
       "  'Ik_Xef_Xa_Xad',\n",
       "  'Xbk_Xe_Nn',\n",
       "  'Xe_Ck_Xe',\n",
       "  'Ik_Ik',\n",
       "  'Ni_Xe_Xik',\n",
       "  'Xe_Xa_Sup',\n",
       "  'Xa_Xcf_Xcf',\n",
       "  'Lipschitz',\n",
       "  'Xcf_Xcf_Xcf',\n",
       "  'Nv',\n",
       "  'Xe_Xcf_Xcf',\n",
       "  'Xe_Xqi',\n",
       "  'Xcf_Xe_Xcf_Xcf_Cid',\n",
       "  'Xe_Xzk_Nl',\n",
       "  'Xe_Xa_Nl',\n",
       "  'Xcf_Nbase',\n",
       "  'Ik_Xef',\n",
       "  'Lemma_Lemma',\n",
       "  'Xe_Xa_Nn_Nn',\n",
       "  'Lemma_Lemma_Nwe',\n",
       "  'Xe_Xe_Xcf',\n",
       "  'Xe_Xcf',\n",
       "  'Recall_De_Xef_Xac',\n",
       "  'Xcf_Xcf_Neqj',\n",
       "  'Xef_Xac_Xrst',\n",
       "  'Xe_Xe',\n",
       "  'Qv',\n",
       "  'Xce_Xbi',\n",
       "  'Xce_Xbi_Ni',\n",
       "  'Xe_Xce_Xb',\n",
       "  'Eq_Xcf_Xef_Xa_Xad',\n",
       "  'Xce_Xbi_Xcf',\n",
       "  'Xef_Xa_Xab_Nv',\n",
       "  'Xcf_Xef_Xa_Xad',\n",
       "  'Xb_Xcf_Xcf_Neq',\n",
       "  'Xce_Xbj_Xcf',\n",
       "  'Xe_Xce_Xbi_Xcf_Xce_Xbj_Xcf',\n",
       "  'Xce_Xbj',\n",
       "  'Xe_Xij_Xcf_Xcf',\n",
       "  'Xe_Xij',\n",
       "  'Nusing',\n",
       "  'Xe_Xcf_Xcf_Xe_Xcf_Xcf_Cid',\n",
       "  'Xe_Xce_Xbi_Xcf',\n",
       "  'Xcf_Xcf_Xe_Xcf_Xcf',\n",
       "  'Xce_Xb',\n",
       "  'Xcf',\n",
       "  'Xcf_Nlemma',\n",
       "  'Xcf_Xe_Xcf',\n",
       "  'Xcf_Xe_Xcf_Nsem',\n",
       "  'Xe_Lim',\n",
       "  'Xe_Xa_Xe_Nn_Nproof',\n",
       "  'Lemma_Xcf_Xcf_Nsem',\n",
       "  'Xe_Xa_Xe',\n",
       "  'Nv_Nv',\n",
       "  'Nrecall_De_Xef_Xac',\n",
       "  'Xnition_Xcf_Nbase',\n",
       "  'Xe_Xcb',\n",
       "  'Xq',\n",
       "  'Xce_Xb_Xe',\n",
       "  'Xcf_Nsem',\n",
       "  'Xe_Xa_Xe_Xe',\n",
       "  'Xe_Xe_Xcwe_Combine_Lemma',\n",
       "  'Xcf_Xe_Xcf_Nbase',\n",
       "  'Xe_Xa_Xe_Xe_Xcf_Nn_Nproof',\n",
       "  'Nadamw_Byol_Sem',\n",
       "  'Nc_Experiment',\n",
       "  'Xe_Xa_Random',\n",
       "  'Gaussian_Blue',\n",
       "  'Noptimizer_Nepochs_Nbase',\n",
       "  'Chen',\n",
       "  'De_Xef_Xac',\n",
       "  'Al_Nc',\n",
       "  'Lee',\n",
       "  'Kolesnikov',\n",
       "  'Al_Chen',\n",
       "  'Xsh_Xef_Xac_Xowers',\n",
       "  'Xsh_Xef_Xac',\n",
       "  'Xat_Xef_Xac_Xsh_Ray_Shark',\n",
       "  'Xef_Xac_Xowers',\n",
       "  'Wolf',\n",
       "  'Sea_Ncamel',\n",
       "  'Nhamster_Mouse',\n",
       "  'Ntable_Set',\n",
       "  'Krizhevsky'],\n",
       " ['Neal_Madras',\n",
       "  'Combin',\n",
       "  'Brualdi_Eliseu_Fritscher',\n",
       "  'Hankel_Toeplitz',\n",
       "  'Kenneth_Barrese',\n",
       "  'Nicholas_Loehr',\n",
       "  'Xac',\n",
       "  'Remmel_Bruce',\n",
       "  'Sagan',\n",
       "  'Combin_Xe',\n",
       "  'Mansour_Alex_Postnikov_Simone_Severini',\n",
       "  'Elsevier_Sci',\n",
       "  'Amsterdam_Nkaren',\n",
       "  'Briggs_Je',\n",
       "  'Xef_Xac_Xrey',\n",
       "  'Roman_Glebov',\n",
       "  'Daniel_Kr',\n",
       "  'Xbal_Xe',\n",
       "  'Xe_Nhong_Liu',\n",
       "  'Combin_Xe_Nira',\n",
       "  'Gessel_Guoce_Xin',\n",
       "  'Herman_Attila_Kuba',\n",
       "  'Numerical_Har',\n",
       "  'Birkh',\n",
       "  'Carlos_Hoppen',\n",
       "  'Kohayakawa_Carlos_Gustavo_Moreira',\n",
       "  'Charles_Radin_Peter_Winkler',\n",
       "  'Montgomery_Robert,_Vaughan',\n",
       "  'Discrete_Math',\n",
       "  'Nalexander_Postnikov',\n",
       "  'Njohn_Riordan',\n",
       "  'Ngian_Carlo_Rota',\n",
       "  'Sergey_Fomin_Nrichard'],\n",
       " ['Adrien_Barde_Njean',\n",
       "  'Ponce_Yann_Lecun',\n",
       "  'David_Berthelot',\n",
       "  'Nicholas_Carlini',\n",
       "  'Ian_Goodfellow_Nicolas',\n",
       "  'Colin_Raffel',\n",
       "  'Luca_Bertinetto',\n",
       "  'Jack_Valmadre_Joao_Henriques_Andrea_Nvedaldi_Philip_Hs_Torr',\n",
       "  'Jane_Bromley',\n",
       "  'Yann_Lecun_Eduard_Ns',\n",
       "  'Roopak_Shah',\n",
       "  'Zhaowei_Cai_Avinash_Ravichandran_Subhransu_Maji_Char',\n",
       "  'Fowlkes_Zhuowen',\n",
       "  'Tu_Stefano_Soatto',\n",
       "  'Mathilde_Caron_Ishan_Misra',\n",
       "  'Priya_Goyal',\n",
       "  'Bojanowski_Armand_Joulin',\n",
       "  'Mathilde_Caron_Hugo_Touvron_Ishan_Misra_Herv',\n",
       "  'Njulien_Mairal',\n",
       "  'Emerg',\n",
       "  'Ting_Chen_Simon_Kornblith_Mohammad',\n",
       "  'Xinlei_Chen',\n",
       "  'Ross_Girshick_Kaiming',\n",
       "  'Xinlei_Chen_Kaiming',\n",
       "  'Xinlei_Chen_Saining_Xie',\n",
       "  'Ekin_Cubuk',\n",
       "  'Jonathon',\n",
       "  'Jia_Deng',\n",
       "  'Wei_Dong',\n",
       "  'Richard_Socher',\n",
       "  'Li_Jia_Li_Kai_Li',\n",
       "  'Li_Fei_Fei',\n",
       "  'Alexey_Dosovitskiy_Lucas',\n",
       "  'Alexander_Kolesnikov',\n",
       "  'Zhai_Thomas',\n",
       "  'Heigold_Syl',\n",
       "  'Gelly_Jakob',\n",
       "  'Neil_Houlsby',\n",
       "  'Jonathan_Tompson',\n",
       "  'Andrew_Zisserman',\n",
       "  'Mark_Everingham_Luc_Van_Gool_Christopher_Ki',\n",
       "  'Priya_Goyal_Piotr',\n",
       "  'Ross_Girshick',\n",
       "  'Pieter_Noord',\n",
       "  'Lukasz_Wesolowski',\n",
       "  'Jia_Kaiming',\n",
       "  'Nsgd_Training_Imagenet',\n",
       "  'Jean_Bastien_Grill',\n",
       "  'Corentin_Ntallec',\n",
       "  'Pierre_Richemond_Elena_Buchatskaya',\n",
       "  'Bernardo_Avila',\n",
       "  'Daniel_Guo_Moham',\n",
       "  'Kaiming_He',\n",
       "  'Wu_Saining',\n",
       "  'Ross_Ngirshick',\n",
       "  'Kaiming_He_Xiangyu_Zhang',\n",
       "  'Jian_Sun',\n",
       "  'Sergey_Ioffe_Christian_Szegedy',\n",
       "  'Zihang_Jiang',\n",
       "  'Qibin_Hou_Li',\n",
       "  'Zhou_Yujun_Shi_Nxiaojie',\n",
       "  'Jin_Anran',\n",
       "  'Wang_Jiashi_Feng',\n",
       "  'Alex_Krizhevsky',\n",
       "  'Zeming_Li',\n",
       "  'Songtao_Liu',\n",
       "  'Nsgdr_Nstochas',\n",
       "  'Ilya_Loshchilov',\n",
       "  'Frank_Hutter',\n",
       "  'Subhransu_Maji_Esa_Rahtu',\n",
       "  'Kannala_Matthew',\n",
       "  'Nblaschko_Andrea_Vedaldi',\n",
       "  'Ishan_Misra_Laurens',\n",
       "  'Maaten',\n",
       "  'Self',\n",
       "  'Aaron_Van_Den_Oord_Yazhe_Li',\n",
       "  'Repre',\n",
       "  'Yun_Sangdoo_Han_Dongyoon',\n",
       "  'Yoo_Youngjoon',\n",
       "  'Kihyuk_Sohn',\n",
       "  'Chun_Liang_Li_Zizhao_Nzhang',\n",
       "  'Kurakin_Han_Nzhang',\n",
       "  'Fixmatch_Simplify',\n",
       "  'Xef_Xac_Xdence',\n",
       "  'Ming_Yang',\n",
       "  'Xaurelio_Ranzato_Lior_Nwolf',\n",
       "  'Deepface_Closing',\n",
       "  'Harri_Valpola',\n",
       "  'Yuandong_Tian',\n",
       "  'Lantao_Yu',\n",
       "  'Tongzhou_Wang_Phillip_Isola',\n",
       "  'Xiao_Wang_Daisuke',\n",
       "  'Kihara_Jiebo_Luo_Guo_Jun_Nenaet_Self',\n",
       "  'Xiao_Wang_Guo',\n",
       "  'Jun_Qi',\n",
       "  'Yisen_Wang',\n",
       "  'Weiyang_Liu',\n",
       "  'Xingjun_Ma',\n",
       "  'James_Bailey_Nhongyuan',\n",
       "  'Zha_Le',\n",
       "  'Shu_Tao_Xia',\n",
       "  'Noisy_Label',\n",
       "  'Yuxin_Wu_Kaiming',\n",
       "  'Zhirong_Wu',\n",
       "  'Stella_Yu_Dahua_Lin_Nunsupervised',\n",
       "  'Haohang_Xu',\n",
       "  'Xiaopeng_Zhang',\n",
       "  'Hao_Li_Lingxi_Xie',\n",
       "  'Chun_Hsiao_Yeh',\n",
       "  'Cheng_Yao_Hong_Yen_Chi_Hsu_Tyng',\n",
       "  'Chen_Yann_Lecun',\n",
       "  'Yang_You_Igor_Gitman',\n",
       "  'Boris_Ginsburg',\n",
       "  'Deny',\n",
       "  'Hongyi_Zhang',\n",
       "  'Cisse_Yann',\n",
       "  'Ndavid_Lopez_Paz',\n",
       "  'Mixup_Beyond',\n",
       "  'Bolei_Zhou_Agata_Lapedriza_Jianxiong_Xiao',\n",
       "  'Antonio_Tor',\n",
       "  'Aude_Oliva',\n",
       "  'Pan_Zhou_Caiming_Xiong_Xiao'],\n",
       " ['Aldrich_Autonomy',\n",
       "  'Angrist',\n",
       "  'Rubin_Identi',\n",
       "  'Bottou',\n",
       "  'Completeness',\n",
       "  'Pearl_Causal',\n",
       "  'Peters',\n",
       "  'Ben_David',\n",
       "  'Lu',\n",
       "  'Vincent_Representation',\n",
       "  'Xef_Xac_Xeld',\n",
       "  'Lothringer',\n",
       "  'Gao',\n",
       "  'Greene_Nc',\n",
       "  'Dragomir',\n",
       "  'Fortney',\n",
       "  'Fraine_Water',\n",
       "  'Forr',\n",
       "  'Skolnick_Weisberg',\n",
       "  'Perona_Causal',\n",
       "  'Zien_Ed',\n",
       "  'Semi',\n",
       "  'Charig',\n",
       "  'Wickham_Comparison',\n",
       "  'Br_Med_Clin',\n",
       "  'Res_Ed',\n",
       "  'Chen',\n",
       "  'Chickere_Learning',\n",
       "  'Xe_Springer',\n",
       "  'Chickere_Optimal',\n",
       "  'Cooper',\n",
       "  'Xsis',\n",
       "  'Proceedings',\n",
       "  'Dawid_Causal',\n",
       "  'Dax',\n",
       "  'Buonanno',\n",
       "  'Xef_Xac',\n",
       "  'Meng',\n",
       "  'Sheehan_Assumptions_Iv',\n",
       "  'Oliver_Boyd_Edinburgh_London',\n",
       "  'Foreman_Mackey',\n",
       "  'Hogg',\n",
       "  'Morton',\n",
       "  'Kernel',\n",
       "  'Geiger',\n",
       "  'Heckerman_Learning',\n",
       "  'Pearl_Logical',\n",
       "  'Intelligence_Xe',\n",
       "  'Liu',\n",
       "  'Tao',\n",
       "  'Tao_Causal',\n",
       "  'Pouget_Abadie',\n",
       "  'Xu',\n",
       "  'Warde_Farley',\n",
       "  'Sodhani',\n",
       "  'Gresele',\n",
       "  'Von_Xc_Xaugelgen',\n",
       "  'Gretton',\n",
       "  'Hilbert_Schmidt',\n",
       "  'Xe_Springer_Verlag',\n",
       "  'Gutmann',\n",
       "  'Heckerman',\n",
       "  'Meek',\n",
       "  'Hern',\n",
       "  'Xban',\n",
       "  'Keiding_The_Simpson_Xe',\n",
       "  'Glorot',\n",
       "  'Lerchner_Beta',\n",
       "  'Ramsey',\n",
       "  'Sanchez_Romero',\n",
       "  'Causal',\n",
       "  'Mach',\n",
       "  'Valtorta_Pearl',\n",
       "  'Hyv_Xc_Xaarinen',\n",
       "  'Hyvarinen',\n",
       "  'Rubin_Causal',\n",
       "  'Xe_Nno,_Xe',\n",
       "  'Intelligence_Xe_Xe',\n",
       "  'Markov',\n",
       "  'Jin',\n",
       "  'Karimi',\n",
       "  'Rojas_Carulla',\n",
       "  'Hardt',\n",
       "  'Kingma',\n",
       "  'Klein',\n",
       "  'Betrachtungen',\n",
       "  'Xauber',\n",
       "  'Verlag_Von_Andreas_Deichert_Erlangen',\n",
       "  'Friedman_Probabilistic',\n",
       "  'Nproceedings',\n",
       "  'Russell',\n",
       "  'Lauritzen_Graphical',\n",
       "  'Bengio',\n",
       "  'Leibniz_Discours_De_Xc_Xbetaphysique',\n",
       "  'Chaitin',\n",
       "  'Gelly',\n",
       "  'Lopez_Paz',\n",
       "  'Bottou_Discovering',\n",
       "  'Loschmidt',\n",
       "  'Zustand_De_Xc',\n",
       "  'Systems_Von',\n",
       "  'Schwerkraft_Nakademie',\n",
       "  'Wissenschaften_Wien',\n",
       "  'Wu',\n",
       "  'Lobato',\n",
       "  'Verlag_New_York',\n",
       "  'Meek_Causal',\n",
       "  'Messerli_Chocolate',\n",
       "  'Johnson',\n",
       "  'Bowler',\n",
       "  'Bieryla_Na',\n",
       "  'Mann_Stellar',\n",
       "  'Janzing',\n",
       "  'Learn',\n",
       "  'Muandet',\n",
       "  'Pearl_Bayesian',\n",
       "  'Biometrika',\n",
       "  'Proceedings_Nof',\n",
       "  'Meinshausen_Causal',\n",
       "  'Xef_Xac_Xdence_Ninterval',\n",
       "  'Identi_Xef',\n",
       "  'Peters_Kernel',\n",
       "  'Robins',\n",
       "  'Xe_Nbiometrika',\n",
       "  'Robinson_Counting',\n",
       "  'Peters_Invariant',\n",
       "  'Rubin',\n",
       "  'Rubenstein',\n",
       "  'Grosse_Wentrup',\n",
       "  'Rubin_Estimating',\n",
       "  'Causality',\n",
       "  'Halpern_Ngeffner',\n",
       "  'Pearl',\n",
       "  'Herbrich',\n",
       "  'Helmbold',\n",
       "  'Williamson',\n",
       "  'Simon_Gabriel',\n",
       "  'Paz_Causal',\n",
       "  'Kalchbrenner',\n",
       "  'Proceedings_Ieee',\n",
       "  'Peters_Computing',\n",
       "  'Hilbert',\n",
       "  'Smola_Learning',\n",
       "  'Zhou',\n",
       "  'Schwarz',\n",
       "  'Shah',\n",
       "  'Kerminen',\n",
       "  'Shpitser',\n",
       "  'Pearl_Identi',\n",
       "  'Markovian_Causal_Model',\n",
       "  'Simpson',\n",
       "  'Spohn_Grundlagen',\n",
       "  'Entscheidungstheorie',\n",
       "  'Verlag',\n",
       "  'Christmann_Support',\n",
       "  'Bethge',\n",
       "  'Nunsupervised',\n",
       "  'Xe_Morgan_Kaufmann_Publishers_Inc',\n",
       "  'Yurchenko_Water',\n",
       "  'Nature_Astronomy',\n",
       "  'Vapnik_Statistical',\n",
       "  'Simpson_Xe_X',\n",
       "  'Covid',\n",
       "  'Brendel',\n",
       "  'Locatello_Self',\n",
       "  'Towards',\n",
       "  'Mey',\n",
       "  'Woodward_Causation',\n",
       "  'Hyv_Xc',\n",
       "  'Wang_Domain'],\n",
       " ['Lmel',\n",
       "  'Mel',\n",
       "  'Gunjan_Aggarwal_Devi_Parikh',\n",
       "  'Relja_Arandjelovic',\n",
       "  'Andrew_Zisserman',\n",
       "  'Carl_Vondrick',\n",
       "  'Antonio_Torralba',\n",
       "  'Jean_Pierre_Briot_Ga',\n",
       "  'Hadjeres_Franc',\n",
       "  'Hidalgo_Martinez',\n",
       "  'Xef_Xac_Xeld',\n",
       "  'Ieee_Tpami',\n",
       "  'Zhe_Cao_Tomas',\n",
       "  'Simon_Shih_En_Wei_Yaser',\n",
       "  'Joao_Carreira_Andrew_Zisserman',\n",
       "  'Quo',\n",
       "  'Abe_Davis_Maneesh_Agrawala',\n",
       "  'Jong_Wook',\n",
       "  'Ilya_Sutskever',\n",
       "  'Jukebox_Gen',\n",
       "  'Di_Zeren',\n",
       "  'Jiang_Si_Liu_Zhaokai_Wang_Leyan_Nzhu',\n",
       "  'Zexin_He_Hongme',\n",
       "  'Liu_Shuicheng_Yan',\n",
       "  'Chris_Donahue',\n",
       "  'Julian_Mcauley_Miller_Puckette',\n",
       "  'Hao_Wen_Dong_Wen_Yi_Hsiao_Li_Chia_Yang_Yi_Hsuan_Nyang',\n",
       "  'Daniel_Pw_Ellis',\n",
       "  'Patrick_Esser',\n",
       "  'Robin_Rombach',\n",
       "  'Joao_Ferreira_Thiago_Coutinho_Thiago',\n",
       "  'Neto_Rafael_Azevedo_Renato_Martins_Erick',\n",
       "  'Chuang_Gan_Deng',\n",
       "  'Huang_Peihao',\n",
       "  'Chen_Joshua_Tenen',\n",
       "  'Foley',\n",
       "  'Learning',\n",
       "  'Huang_Hang_Zhao_Joshua_Tenen',\n",
       "  'Ruohan_Gao',\n",
       "  'Rogerio_Feris_Kristen_Grauman',\n",
       "  'Ruohan_Gao_Kristen_Grauman',\n",
       "  'Ruohan_Gao_Tae_Hyun_Oh_Kristen_Grauman',\n",
       "  'Lorenzo_Ntorresani',\n",
       "  'Jort_Gemmeke',\n",
       "  'Dylan_Freedman_Aren_Njansen',\n",
       "  'Lawrence_Channing_Moore',\n",
       "  'Marvin_Ritter',\n",
       "  'Audio',\n",
       "  'Ian_Goodfellow',\n",
       "  'Abadie_Mehdi_Mirza_Bing_Nxu',\n",
       "  'David_Warde',\n",
       "  'Aaron_Courville',\n",
       "  'Nyoshua_Bengio',\n",
       "  'Ndevin_Platt',\n",
       "  'Bryan_Seybold',\n",
       "  'Xef_Xac',\n",
       "  'Cheng_Zhi',\n",
       "  'Anna_Huang',\n",
       "  'Matthew_Hoffman',\n",
       "  'Monica_Dinculescu_Douglas',\n",
       "  'Vladimir_Iashin_Esa_Rahtu',\n",
       "  'Phillip_Isola',\n",
       "  'Jun_Yan_Zhu_Tinghui',\n",
       "  'Zhou_Alexei_Nimage',\n",
       "  'Shulei_Ji',\n",
       "  'Luo_Xinyu_Yang',\n",
       "  'Hsuan_Kai_Kao_Li_Su',\n",
       "  'Kay_Joao_Carreira',\n",
       "  'Brian_Zhang',\n",
       "  'Hillier_Sudheendra_Vijayanarasimhan_Fabio_Viola',\n",
       "  'Hu',\n",
       "  'Evangelos_Kazakos_Arsha_Nagrani',\n",
       "  'Diederik_Kingma',\n",
       "  'Max_Welling',\n",
       "  'Jungil_Kong_Jaehyeon_Kim_Jaekyoung_Bae',\n",
       "  'Bruno_Korbar',\n",
       "  'Tran_Lorenzo_Torresani',\n",
       "  'Coopera',\n",
       "  'Kundan_Kumar_Rithesh',\n",
       "  'Boissiere_Lu',\n",
       "  'Gestin_Wei_Zhen',\n",
       "  'Jose_Sotelo_Alexandre',\n",
       "  'Yoshua_Bengio',\n",
       "  'Xbren_Kaae',\n",
       "  'Hugo_Nlarochelle_Ole_Winther',\n",
       "  'Hsin_Ying_Lee_Xiaodong_Yang',\n",
       "  'Ming_Yu',\n",
       "  'Liu_Ting_Chun_Nwang_Yu',\n",
       "  'Ding_Lu',\n",
       "  'Hsuan_Yang',\n",
       "  'Kautz_Ndancing',\n",
       "  'Buyu_Li',\n",
       "  'Zhao_Lu_Sheng',\n",
       "  'Ruilong_Li_Shan_Yang_David',\n",
       "  'Ross_Angjoo_Nkanazawa',\n",
       "  'Ai',\n",
       "  'Jae_Hyun_Lim_Jong_Chul_Ye',\n",
       "  'Geometric',\n",
       "  'Matthew_Loper_Naureen_Mahmood',\n",
       "  'Pons_Moll_Michael',\n",
       "  'Masanori_Koyama',\n",
       "  'Aaron_Van_Den_Oord_Sander_Dieleman',\n",
       "  'Koray_Kavukcuoglu',\n",
       "  'Aaron_Van_Den_Oord_Oriol_Vinyals',\n",
       "  'Sageev_Oore',\n",
       "  'Ian_Simon_Sander',\n",
       "  'Douglas_Eck',\n",
       "  'Karen_Simonyan',\n",
       "  'Neural_Computing_Ap',\n",
       "  'Andrew_Owens_Alexei_Efros',\n",
       "  'Nin_Neccv',\n",
       "  'Andrew_Owens',\n",
       "  'Jiajun_Wu_Josh_Mcdermott',\n",
       "  'William_Nfreeman',\n",
       "  'Adam_Paszke',\n",
       "  'Sam_Gross_Francisco_Massa',\n",
       "  'Edward_Yang',\n",
       "  'Devito_Martin_Rai',\n",
       "  'Junjie_Bai_Soumith_Chintala',\n",
       "  'Buc_Nfox',\n",
       "  'Garnett',\n",
       "  'Advances_Neural',\n",
       "  'Curran_Asso',\n",
       "  'Ye_Zhu_Yu',\n",
       "  'Wu_Hugo_Latapie',\n",
       "  'Yi_Yang_Yan_Yan',\n",
       "  'Wenlin_Zhuang',\n",
       "  'Congyi_Wang_Siyu',\n",
       "  'Xia_Jinxiang_Chai_Nyangang_Wang',\n",
       "  'Musicdance_Dancenet',\n",
       "  'Alec_Radford_Luke_Metz_Soumith_Chintala',\n",
       "  'Tanzila_Rahman',\n",
       "  'Bicheng_Xu',\n",
       "  'Leonid_Sigal',\n",
       "  'Ali_Razavi_Aaron_Van_Den_Oord_Oriol_Vinyals',\n",
       "  'Gen',\n",
       "  'Li_Zijian',\n",
       "  'Huang_Qifeng_Chen_Nself',\n",
       "  'Tim_Sainburg',\n",
       "  'Timothy_Gentner',\n",
       "  'Tim_Salimans_Durk_Kingma',\n",
       "  'Lucio_Dery',\n",
       "  'Hayden_Schoen_Ira_Nkemelmacher_Shlizerman',\n",
       "  'Kun_Su_Xiulong',\n",
       "  'Tang_Jia',\n",
       "  'Jia_Hanyang_Mao',\n",
       "  'Yapeng_Tian',\n",
       "  'Shi_Bochen_Li_Zhiyao',\n",
       "  'Duan_Chen',\n",
       "  'Xu',\n",
       "  'Shuhei_Tsuchida',\n",
       "  'Fukayama_Masahiro_Hamasaki',\n",
       "  'Masataka_Goto',\n",
       "  'Ting_Chun_Wang_Ming_Yu',\n",
       "  'Jun_Yan_Zhu_Andrew_Tao_Njan_Kautz_Bryan_Catanzaro',\n",
       "  'Xin_Wang_Yuan',\n",
       "  'Fang_Wang_William_Yang_Wang_Nwatch',\n",
       "  'Yu_Wu_Yi_Yang',\n",
       "  'Yu_Wu',\n",
       "  'Linchao_Zhu_Yan_Yan',\n",
       "  'Yi_Yang',\n",
       "  'Iccv_Bing_Xu_Naiyan',\n",
       "  'Chen_Mu_Li',\n",
       "  'Hang_Zhao_Chuang',\n",
       "  'Gan_Wei_Chiu_Ma',\n",
       "  'Antonio_Tor',\n",
       "  'Codebook',\n",
       "  'Groups_Conv',\n",
       "  'Adv',\n",
       "  'Xef_Xac_Xve_Performer',\n",
       "  'Audio_Mel']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All function in one\n",
    "\n",
    "entities=[]\n",
    "def main():\n",
    "    for text in elements:\n",
    "        temp=after_references(str(text))\n",
    "        temp=preprocess_text(temp)\n",
    "        temp=extract(temp)\n",
    "        entities.append(temp)\n",
    "    return entities\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897bff1",
   "metadata": {},
   "source": [
    "## Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "164621c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready import *\n",
    "\n",
    "onto_path.append(\"owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8470daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Creating new ontology onto <http://test.org/onto.owl>.\n"
     ]
    }
   ],
   "source": [
    "onto = Ontology(\"http://test.org/onto.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5aad8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class References(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class quoted_by(Property):\n",
    "    ontolgy = onto\n",
    "    domain = [References]\n",
    "    range = [Author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd4f26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.sax.saxutils import escape\n",
    "\n",
    "def invalid_xml_remove(char):\n",
    "    \"\"\"Tracks illegal unicode characters\"\"\"\n",
    "    #http://stackoverflow.com/questions/1707890\n",
    "    # /fast-way-to-filter-illegal-xml-unicode-chars-in-python\n",
    "    illegal_unichrs = [ (0x00, 0x08), (0x0B, 0x1F), (0x7F, 0x84), (0x86, 0x9F),\n",
    "                    (0xD800, 0xDFFF), (0xFDD0, 0xFDDF), (0xFFFE, 0xFFFF),\n",
    "                    (0x1FFFE, 0x1FFFF), (0x2FFFE, 0x2FFFF), (0x3FFFE, 0x3FFFF),\n",
    "                    (0x4FFFE, 0x4FFFF), (0x5FFFE, 0x5FFFF), (0x6FFFE, 0x6FFFF),\n",
    "                    (0x7FFFE, 0x7FFFF), (0x8FFFE, 0x8FFFF), (0x9FFFE, 0x9FFFF),\n",
    "                    (0xAFFFE, 0xAFFFF), (0xBFFFE, 0xBFFFF), (0xCFFFE, 0xCFFFF),\n",
    "                    (0xDFFFE, 0xDFFFF), (0xEFFFE, 0xEFFFF), (0xFFFFE, 0xFFFFF),\n",
    "                    (0x10FFFE, 0x10FFFF) ]\n",
    "\n",
    "    illegal_ranges = [f\"{chr(low)}-{chr(high)}\"\n",
    "                  for (low, high) in illegal_unichrs\n",
    "                  if low < sys.maxunicode]\n",
    "\n",
    "    illegal_xml_re = re.compile(f'[{\"\".join(illegal_ranges)}]')\n",
    "    if illegal_xml_re.search(char) is not None:\n",
    "        #Replace with space\n",
    "        return ''\n",
    "    else:\n",
    "        return char\n",
    "\n",
    "def clean_char(char):\n",
    "    \"\"\"\n",
    "    Function for remove invalid XML characters from\n",
    "    incoming data.\n",
    "    \"\"\"\n",
    "    #Get rid of the ctrl characters first.\n",
    "    #http://stackoverflow.com/questions/1833873/python-regex-escape-characters\n",
    "    char = re.sub('\\x1b[^m]*m', '', char)\n",
    "    #Clean up invalid xml\n",
    "    char = invalid_xml_remove(char)\n",
    "    replacements = [\n",
    "        ('\\u201c', '\\\"'),\n",
    "        ('\\u0141', '\\\"'),\n",
    "        ('\\u201d', '\\\"'),\n",
    "        (\"\\u001B\", ''), #http://www.fileformat.info/info/unicode/char/1b/index.htm\n",
    "        (\"\\u0019\", ''), #http://www.fileformat.info/info/unicode/char/19/index.htm\n",
    "        (\"\\u0016\", ''), #http://www.fileformat.info/info/unicode/char/16/index.htm\n",
    "        (\"\\u001C\", ''), #http://www.fileformat.info/info/unicode/char/1c/index.htm\n",
    "        (\"\\u0003\", ''), #http://www.utf8-chartable.de/unicode-utf8-table.pl?utf8=0x\n",
    "        (\"\\u000C\", ''),\n",
    "        (\"\\u03b1\", ''),\n",
    "        (\"u\\u039C\", ''),\n",
    "        (\"\\u03C3\", ''),\n",
    "        (\"\\u0141\", ''),\n",
    "        (\"\\u0308\", ''),\n",
    "        (\"\\u2032\", ''),\n",
    "        (\"\\u03b8\", '')\n",
    "\n",
    "    \n",
    "    ]\n",
    "    for rep, new_char in replacements:\n",
    "        if char == rep:\n",
    "            return new_char\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4794f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_value(text: str):\n",
    "        \"\"\"Escape the illegal characters for an ontology property\"\"\"\n",
    "        if text is None:\n",
    "            return None\n",
    "        # function to escape XML character data\n",
    "        text = escape(text)\n",
    "        text = text.replace('\\n', '')\n",
    "        text = text.replace('\\r', '')\n",
    "        text = text.replace('\\f', '')\n",
    "        text = text.replace('\\b', '')\n",
    "        text = text.replace('\"', '')\n",
    "        text = text.replace('[', '')\n",
    "        text = text.replace(']', '')\n",
    "        text = text.replace('{', '')\n",
    "        text = text.replace('}', '')\n",
    "        text = text.replace('#', '')\n",
    "        text = text.replace('|', '')\n",
    "        text = text.replace(' ', '_')\n",
    "        text = clean_char(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2126e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Samuel.Lavoie]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Christos.Tsirigotis]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Max.Schwarzer]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Kenji.Kawaguchi]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Ankit.Vani]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Aaron.Courville]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Pakawut.Jiradilok]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Xiao.Wang]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Haoqi.Fan]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Yuandong.Tian]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Daisuke.Kihara]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Xinlei.Chen]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Bernhard.Sch.lkopf]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Julius.von.K.gelgen]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Ye.Zhu]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Kyle.Olszewski]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Yu.Wu]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Panos.Achlioptas]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n",
      "[onto.Menglei.Chai]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Yan.Yan]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n",
      "[onto.Sergey.Tulyakov]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(array_authors)):\n",
    "    for j in range(len(array_authors[i])):\n",
    "        aut_num = Author(escape_value(array_authors[i][j]))\n",
    "        for z in range(len(entities[i])):\n",
    "            ref_num = References(escape_value(entities[i][z]))\n",
    "            ref_num.quoted_by.append(aut_num)\n",
    "            print(ref_num.quoted_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e27e8710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Saving ontology onto to owl\\onto.owl...\n"
     ]
    }
   ],
   "source": [
    "onto.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
