{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "126a7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdftotext in c:\\users\\admin\\anaconda3\\envs\\projetfilrouge\\lib\\site-packages (2.2.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install arxiv\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install nameparser\n",
    "# !pip install pdfminer\n",
    "# !pip install refextract\n",
    "# !pip install pdfx\n",
    "# !pip install -U textblob\n",
    "# !pip install owlready\n",
    "# !pip install boto3\n",
    "# Install a conda package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !conda install -c conda-forge poppler\n",
    "# !pip install pdftotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2ac646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import of librairies\n",
    "\n",
    "#ARVIX\n",
    "import arxiv\n",
    "\n",
    "#PDFTEXT\n",
    "import urllib.request\n",
    "import pdftotext\n",
    "\n",
    "# Prevent future/deprecation warnings from showing in output\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#Lol=cal\n",
    "import os\n",
    "import requests as r\n",
    "\n",
    "#AI\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk.tag.stanford as st\n",
    "from nltk.tag.stanford import StanfordNERTagger \n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "import boto3\n",
    "\n",
    "from owlready import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf6f98",
   "metadata": {},
   "source": [
    "## Download pdf file from Arvix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63be17ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203.11194v1\n",
      "2203.11192v1\n",
      "2203.11191v1\n",
      "2203.11190v1\n",
      "2203.11187v1\n",
      "2203.11184v1\n",
      "2203.11183v1\n",
      "2203.11181v1\n",
      "2203.11178v1\n",
      "2203.10916v1\n"
     ]
    }
   ],
   "source": [
    "#You can choose here how many pdf you want. \n",
    "max_results=10\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = \"Computer Science & AI\",\n",
    "    max_results = max_results,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "for result in search.results():\n",
    "    print(result.pdf_url[21:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b3ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://arxiv.org/pdf/2203.11194v1', 'http://arxiv.org/pdf/2203.11192v1', 'http://arxiv.org/pdf/2203.11191v1', 'http://arxiv.org/pdf/2203.11190v1', 'http://arxiv.org/pdf/2203.11187v1', 'http://arxiv.org/pdf/2203.11184v1', 'http://arxiv.org/pdf/2203.11183v1', 'http://arxiv.org/pdf/2203.11181v1', 'http://arxiv.org/pdf/2203.11178v1', 'http://arxiv.org/pdf/2203.10916v1']\n",
      "[['Mihir.Prabhudesai', 'Anirudh.Goyal', 'Deepak.Pathak', 'Katerina.Fragkiadaki'], ['Christoph.Mayer', 'Martin.Danelljan', 'Goutam.Bhat', 'Matthieu.Paul', 'Danda.Pani.Paudel', 'Fisher.Yu', 'Luc.Van.Gool'], ['Matthieu.Paul', 'Martin.Danelljan', 'Christoph.Mayer', 'Luc.Van.Gool'], ['Nima.Anari', 'Callum.Burgess', 'Kevin.Tian', 'Thuy.Duong.Vuong'], ['Chen.Zheng', 'Parisa.Kordjamshidi'], ['R.mi.Abgrall', 'Pratik.Rai', 'Florent.Renac'], ['Haotian.Liu', 'Mu.Cai', 'Yong.Jae.Lee'], ['Giulio.Falcioni', 'Franz.Herzog'], ['Qinqin.Yang', 'Zi.Wang', 'Kunyuan.Guo', 'Congbo.Cai', 'Xiaobo.Qu'], ['Christos.Karras', 'Aristeidis.Karras']]\n",
      "['Generating Fast and Slow: Scene Decomposition via Reconstruction', 'Transforming Model Prediction for Tracking', 'Robust Visual Tracking by Segmentation', 'Improved Sampling-to-Counting Reductions in High-Dimensional Expanders and Faster Parallel Determinantal Sampling', 'Relevant CommonSense Subgraphs for \"What if...\" Procedural Reasoning', 'A discontinuous Galerkin spectral element method for a nonconservative compressible multicomponent flow model', 'Masked Discrimination for Self-Supervised Learning on Point Clouds', 'Renormalization of gluonic leading-twist Operators in covariant Gauges', 'Physics-driven Synthetic Data Learning for Biomedical Magnetic Resonance', 'DBSOP: An Efficient Heuristic for Speedy MCMC Sampling on Polytopes']\n"
     ]
    }
   ],
   "source": [
    "#Store in array all informations on pdf (title, authors and links)\n",
    "\n",
    "array_link=[]\n",
    "array_authors=[]\n",
    "array_title=[]\n",
    "\n",
    "for result in search.results():\n",
    "    array_link.append(result.pdf_url)\n",
    "    array_title.append(result.title)\n",
    "    temp=result.authors\n",
    "    array_authors.append([re.sub(\"[^A-Za-z0-9]\",\".\",str(i)) for i in temp])\n",
    "    #Download pdf as link.pdf\n",
    "    result.download_pdf(dirpath=\".\\Download\", filename=f'{result.pdf_url[21:]}.pdf')\n",
    "    \n",
    "print(\"Link: \",array_link)\n",
    "print(\"Authors: \"array_authors)\n",
    "print(\"Title: \"array_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf0bf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pdf link to a pdf text to work on it\n",
    "\n",
    "def load_pdf(value):\n",
    "    with open(f'Download/{value}.pdf', \"rb\") as f:\n",
    "        text = pdftotext.PDF(f)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c06a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                   Generating Fast and Slow:\\r\\n             Scene Decomposition via Reconstruction\\r\\n\\r\\n\\r\\n  Mihir Prabhudesai1      Anirudh Goyal2                  Deepak Pathak1         Katerina Fragkiadaki1\\r\\n               1                                           2\\r\\n                 Carnegie Mellon University                  Mila, University of Montreal\\r\\n           {mprabhud, dpathak, katef}@cs.cmu.edu                  anirudhgoyal9119@gmail.com\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 1: Point-cloud and image decompositions with Generating Fast and Slow Networks (GFS-Nets).\\r\\nGFS-Nets parse completely novel scenes into familiar entities via slow inference, i.e., gradient descent on the\\r\\nreconstruction error of the scene example under consideration. Left: GFS-Nets outperform a state-of-the-art\\r\\n3D-DETR detector by 50% in segmentation accuracy in out-of-distribution 3D point clouds, when trained on\\r\\nthe same training data. Right: GFS-Nets outperform state-of-the-art unsupervised generative models of Slot\\r\\nAttention [42] and uORF [67] in RGB image decomposition.\\r\\n\\r\\n                                                 Abstract\\r\\n         We consider the problem of segmenting scenes into constituent entities, i.e. under-\\r\\n         lying objects and their parts. Current supervised visual detectors though impressive\\r\\n         within their training distribution, often fail to segment out-of-distribution scenes\\r\\n         into their constituent entities. Recent slot-centric generative models break such\\r\\n         dependence on supervision, by attempting to segment scenes into entities unsuper-\\r\\n         vised, by reconstructing pixels. However, they have been restricted thus far to toy\\r\\n         scenes as they suffer from a reconstruction-segmentation trade-off: as the entity\\r\\n         bottleneck gets wider, reconstruction improves but then the segmentation collapses.\\r\\n         We propose GFS-Nets (Generating Fast and Slow Networks) that alleviate this issue\\r\\n         with two ingredients: i) curriculum training in the form of primitives, often missing\\r\\n         from current generative models and, ii) test-time adaptation per scene through gra-\\r\\n         dient descent on the reconstruction objective, what we call slow inference, missing\\r\\n         from current feed-forward detectors. We show the proposed curriculum suffices\\r\\n         to break the reconstruction-segmentation trade-off, and slow inference greatly\\r\\n         improves segmentation in out-of-distribution scenes. We evaluate GFS-Nets in 3D\\r\\n         and 2D scene segmentation benchmarks of PartNet, CLEVR, Room Diverse++,\\r\\n         and show large (∼ 50%) performance improvements against SOTA supervised\\r\\n         feed-forward detectors and unsupervised object discovery methods.\\r\\n\\r\\nPreprint. Under review.\\r\\n\\x0c\\n\\n1    Introduction\\r\\n\\r\\nScenes are composed of objects and objects are composed of parts, and this compositional organization\\r\\nof the perceptual representations is considered a critical component for the level of combinatorial\\r\\ngeneralization that humans are capable of, that extends far beyond their direct experiences [25].\\r\\nEven when encountered with a truly novel image, humans try to parse it in terms of known entity\\r\\ncomponents [26]. Such an entity-centric understanding or reasoning of scenes allows humans to\\r\\ngeneralize known concepts to out-of-distribution examples [25, 39, 44].\\r\\nAn expensive way to build this entity-centric understanding is to supervise the part parsing using\\r\\nhuman labels for training object and parts detection or segmentation in images and 3D point clouds [56,\\r\\n69, 5, 71]. However, these part detectors are category specific and often fail outside the training\\r\\ndistribution [29, 19, 2]. Consider the abstract shape shown in Figure 1 (last row on the left). We can\\r\\nintuitively figure out the meaningful entities that this shape could be broken into. Yet, a state-of-the-art\\r\\ntransformer based 3D-DETR detector [5], when trained supervised for segmenting chairs, fails to\\r\\ndecompose these abstract shapes into entities, even though these shapes contain familiar (chair) parts.\\r\\nAn underlying issue is that these models do not receive any top-down feedback [3] from entities, i.e.,\\r\\nthey do not have any entity-centric reasoning rather, they are purely feed-forward in nature, i.e., the\\r\\ninformation is routed from input to output layers in a sequential manner. How do we enable top-down\\r\\nfeedback between the scene and its constituent parts?\\r\\nThere has been interest recently in building models that segment scenes into entities in an unsupervised\\r\\nway by optimizing a reconstruction objective [42, 17, 24, 63, 57, 35, 16, 4, 23, 68, 55, 13, 22, 73, 34].\\r\\nThese methods differ in details but share the notion of having a fixed set of entities, also known as\\r\\nslots or object files. Each slot extracts information about a single entity during encoding, and it is\\r\\n“rendered\" back to the input during decoding, thus, exhibiting top down feedback. Yet, it has been\\r\\nhard to scale the results of these models beyond toy datasets.\\r\\nWe argue this is because these models answer two questions at once: understanding what the entity is\\r\\nand where it is present in the input. This creates a chicken-and-egg problem because unless we have\\r\\na representation for an entity, we can’t find where it is in the scene, and unless we localize the entity,\\r\\nwe can’t learn a representation of what it is. Hence, a trivial solution often encountered is to assign\\r\\none slot to the input and other slots to nothing, as noted in Engelcke et al. [15] and also confirmed by\\r\\nour experiments in Section 4.1.1 and 4.2.\\r\\nWe propose Generating Fast and Slow Networks (GFS-Nets), a slot-centric autoencoder that segments\\r\\nscenes while optimizing a reconstruction objective. Our model builds on top of Slot Attention[42],\\r\\nwith two key insights: First, it uses curriculum to learn the what question (what entities are) before\\r\\ngoing to the where question (where they are) it does this by training the model to autoencode primitive\\r\\nentities using a single slot bottleneck. This collection of primitive entities can either be collected\\r\\nby design [60] or can be taken from a collection of part segmentations [49]. Second, inference in\\r\\nGFS-Nets is performed in two ways: i) Fast inference where a visual scene is simply encoded and\\r\\ndecoded and no model weights are updated. ii) Slow inference where the weights of the model are\\r\\nadapted by gradient descent while optimizing the reconstruction objective. In this way, GFS-Nets\\r\\nadapts to a truly novel scene by projecting it onto a learned distribution of entities, while also\\r\\nslowly adapting the entity distribution. Slow inference in GFS-Nets successfully parses completely\\r\\nunfamiliar scenes into familiar entities, implementing a form of search for explanations of the scene\\r\\nat inference, as we show in Figure 3.\\r\\nOur slow inference is related to test-time adaptation or optimization [72, 54, 59, 58, 74, 65]. However,\\r\\ntest-time adaptation has not been used so far in entity-centric models to improve scene decomposition.\\r\\nIn fact, it has been recently noted that reconstruction loss can make entity segmentation worse, to\\r\\nquote Engelcke et al. [15], “If the bottleneck is too narrow, segmentation and reconstruction degrade.\\r\\nIf it is too wide, reconstruction is reasonable but the model collapses to a single component and no\\r\\nuseful segmentation is learned.\" GFS-Nets alleviate from this issue by coupling curriculum training\\r\\nwith slow inference for entity-centric learning.\\r\\n\\r\\n    Project page: https://mihirp1998.github.io/project_pages/gfsnets/\\r\\n\\r\\n\\r\\n                                                     2\\r\\n\\x0c\\n\\nWe test GFS-Nets on the following datasets: PartNet [49], ShapeNet[7], CLEVR [30] and a difficult\\r\\nversion of Room Diverse [67]. We evaluate GFS-Nets’s ability to parse out-of-distribution scenes\\r\\nand compare it against state of the art entity-centric generative models [42, 67], program synthesis\\r\\nmodels [60], 3D unsupervised part discovery models [21, 64] and state of the art supervised visual\\r\\ndetectors [5, 43] trained with labeled data to segment entities. We show improvements across all\\r\\nbaselines in our ability to segment novel scenes. Additionally, we ablate different design choices of\\r\\nGFS-Nets. We will make our code and datasets publicly available to the community.\\r\\n\\r\\n2   Related Work\\r\\nEntity-centric generative models for scene and structure decomposition Entity-centric mod-\\r\\nels attempt to segment a scene into objects and parts while autoencoding images. MONet[4],\\r\\nGENESIS[16] and IODINE[23] perform multiple steps between their encoder and the decoder to\\r\\noutput a set of independent latent vectors that describe objects in the image. This iterative encode-\\r\\ndecode inference helps them in receiving top-down feedback. However, this also results in making\\r\\nthe system computationally inefficient and inflexible. Slot Attention [42] on the other hand receives\\r\\ntop-down feedback within a single encoding step. The idea of having a single-encode step makes\\r\\nthe process of extracting out symbols modular and computationally efficient. However, all methods\\r\\nabove fail on non-toy scenes. We attribute this failure to the what or where problem, a.k.a. symmetry\\r\\nproblem, pointed out in the ‘Instance Slots’ Section in Greff et al.[25]: “bottom-up information is\\r\\nunable to break the symmetry amongst slots and thus ends up assigning the same content to each\\r\\none.\" In GFS-Nets, we propose to circumvent this problem using curriculum training.\\r\\n\\r\\nShape program synthesis and analysis-by-synthesis GFS-Nets is also related to works in\\r\\nanalysis-by-synthesis [37], program synthesis for shape prediction [60, 14, 41], as well as ear-\\r\\nlier works on Computer Vision, such as Marr’s 3D sketch [45] which involves representing a scene\\r\\nin terms of generalized cylinders and their syntactic relations to each other. In place of data-driven\\r\\nMarkov Chain Monte Carlo search of analysis-by-synthesis methods that require good initialization,\\r\\nour slow inference searches in the space of primitives by gradient descent. In contrast to program\\r\\nsynthesis methods, it does not require a predefined domain-specific language (DSL) or program\\r\\nannotations for visual structures [41], rather, it discovers compositions over primitives via its slow\\r\\ninference.\\r\\n\\r\\nUnsupervised 3D Part Discovery There are numerous methods that attempt the decomposition of\\r\\ncomplex 3D shapes into primitive parts without primitive supervision[33, 21, 51, 18, 12, 62, 11, 9].\\r\\nTraditional primitives include cuboids [62, 50], superquadrics [53, 51], and convexes [11, 8]. [20]\\r\\nproposes a 3D representation that decomposes space into a structured set of implicit functions [21].\\r\\nNeural Parts [52] represents arbitrarily complex genus-zero shapes and thus yields comparatively\\r\\nexpressive parts. However the resulting parts of these methods[20, 52] are still not semantically\\r\\nmeaningful and the decomposition is highly dependent on the number of parts initialized. The work\\r\\nof [66] does 3D part reconstruction directly from a 2D image input without access to any ground-truth\\r\\n3D shapes for training. However, both [66] and [52] take as input the number of parts, and different\\r\\ndecompositions are predicted with varying part numbers. There is no clear way to select the right\\r\\nnumber of parts. In our case, parts can be quite complex: pairs of parallel surfaces, quadruplets of\\r\\nlegs, as we use implicit functions to represent them. Moreover GFS-Nets’s dynamic attention-based\\r\\nrouting allows it to infer different number of parts for each input scene.\\r\\n\\r\\n3   Generating Fast and Slow (GFS-Nets)\\r\\nThe goal of GFS-Nets is to decompose an unfamiliar scene into familiar entities. The model encodes\\r\\nthe scene, which can be a 3D point cloud or an RGB image, into a set of slot vectors and decodes\\r\\nback the scene using a decoder shared across slots. The model uses the slot attention feature-to-slot\\r\\nmapping of Locatello et al.[42], where visual features are softly partitioned across slots through\\r\\nattention, and update the slot vectors.\\r\\nGFS-Nets is trained with a curriculum. First, it is first trained to autoencode individual entities using\\r\\na single slot in their bottleneck, as shown in the left of Figure 2 and described in Section 3.2. Next,\\r\\nGFS-Nets is trained to autoencode complex scenes using multiple slots in their bottleneck, as shown\\r\\n\\r\\n\\r\\n                                                   3\\r\\n\\x0c\\n\\nStage 1: Autoencoding primitive entities                                                                        Stage 2: Autoencoding scenes\\r\\n\\r\\n3D Object Decomposition                                                                                                                                       Competition\\r\\n                                                                                                                                                             Amongst Slots\\r\\n                                                                                                                                                                         sampling N slots\\r\\n                                                       sampling 1 slot\\r\\n                    Point                                                                                                             Point                                                   slot\\r\\n                                                                                                                                                                                               slot\\r\\n                                                                           slot                                                                                                                 slot         Max-Pool\\r\\n                 Transformer                                                                                                       Transformer                                              decoder\\r\\n                                                                                                                                                                                                  slot\\r\\n                                                                                                                                                                                             decoder\\r\\n                                                                         decoder                                                                                                              decoder\\r\\n                                                                                                                                                                                                    slot\\r\\n                   Encoder                                                                                                           Encoder                                                   decoder\\r\\n                                                                                                                                                                                                 decoder\\r\\n\\r\\n\\r\\n\\r\\n                                      reconstruction loss                                                                                                          reconstruction loss\\r\\n\\r\\n 2D Scene Decomposition\\r\\n                                Competition                                                                                                             Competition\\r\\n                               Amongst Slots Foreground                                                                                                Amongst Slots Foreground\\r\\n                                         sampling 1 slot                                                                                                            sampling N slots\\r\\n                                                             slot                                                                                                                        slot\\r\\n                                                                                                                                                                                          slot             Weighted\\r\\n                                                               slot                                                                                                                        slot\\r\\n                                                           decoder                 Weighted                                                                                            decoder\\r\\n                                                                                                                                                                                            slot\\r\\n                                                                                                                                                                                       decoder             Average\\r\\n           CNN                                              decoder                                                                    CNN                                              decoder\\r\\n                                                                                                                                                                                              slot\\r\\n                                                                                   Average                                                                                               decoder\\r\\n                                                                                                                                                                                           decoder\\r\\n\\r\\n                                        sampling 1 slots                                                                                                        sampling 1 slots\\r\\n                                                     Background                                                                                                                 Background\\r\\n\\r\\n                                      reconstruction loss                                                                                                             reconstruction loss\\r\\n\\r\\n\\r\\nFigure 2: Stages of training in GFS-Nets. Left: Autoencoding primitive entities. Primitive entities are\\r\\npoint clouds of individual parts in 3D, and single object RGB images in 2D. GFS-Nets use one and two slots\\r\\nrespectively, for primitive entity autoencoding in 3D point clouds and RGB images. Right: Autoencoding scenes.\\r\\nScenes can be 3D points clouds of whole object or multi-object RGB images. In this stage, GFS-Nets use more\\r\\nslots in their bottleneck. During inference, some slots get mapped to nothing in the input scene. In this way,\\r\\nGFS-Nets is able to infer decompositions with a varying number of entities, in an unsupervised manner.\\r\\n\\r\\n\\r\\n\\r\\non the right of Figure 2 and described in Section 3.3. We first describe slot attention of [42] in Section\\r\\n3.1, as well as encoders and decoders for 3D point clouds [70, 40] and RGB images [47, 67] which\\r\\nour method builds upon.\\r\\n\\r\\n3.1     Background\\r\\n\\r\\n3.1.1     Routing from input to slots with iterative attention\\r\\n\\r\\nMany approaches instantiate slots (a.k.a. query vectors) from 2D visual feature maps or 3D point\\r\\nfeature clouds [5, 48]. Most works use standard cross attention operations [61, 22, 42] or iterative\\r\\ncross (features to slots) and self-attention (slot-to-slots) operations [5] to map a set of N input feature\\r\\nvectors to a set of K slot vectors. Competition amongst slots and iterative routing introduced in\\r\\n[22, 42] encourages the slots not to encode information in a redundant manner.\\r\\nGiven a visual scene encoded as a set of feature vectors M ∈ RN ×C and K randomly initialized slots\\r\\nsampled from a multivariate Gaussian distribution with a diagonal covariance s ∼ N (µ, Diag(σ 2 )) ∈\\r\\nRK×D , where µ, σ ∈ RC are learnable parameters of the Gaussian, Slot Attention[42] computes an\\r\\nattention map a between the feature map M and the slots s :\\r\\n\\r\\n                                                         a = \\\\mathrm {Softmax}(\\\\key (\\\\MM ) \\\\cdot \\\\query (s)^T, \\\\textrm {axis=1}) \\\\in \\\\mathbb {R}^{N \\\\times K}. \\\\label {eq:1}                                            (1)\\r\\n\\r\\nk, q, and v are learnable linear transformations that map inputs and slots to a common dimension\\r\\nD. The softmax normalization over slots ensures competition amongst them to attend to a specific\\r\\nfeature vector in M. We then generate the update vector for each slot given the computed attention\\r\\nand input feature maps:\\r\\n\\r\\n                                                    updates = a^T v(M) \\\\in \\\\mathbb {R}^{K \\\\times C}, \\\\textrm {where} \\\\ a_{i,1} = \\\\dfrac {a_{i,1}}{\\\\sum _{i=0}^{N}a_{i,1}}                                               (2)\\r\\n\\r\\n\\r\\nwhich we then use to do a gated update on each slot feature using a GRU[10]: slots = GRU(state =\\r\\nslots, input = updates). We iterate over the steps of calculating attention and updating the slots for\\r\\n3 timesteps.\\r\\nSince the slot vectors are sampled from a shared Gaussian distribution, slot attention[42] can in-\\r\\nstantiate a variable number of slots in different scenes, in contrast to DETR encoder [5] where a\\r\\nconstant number of (deterministic) slot vectors are learned and used in each scene. For a more\\r\\ndetailed description, we refer the reader to [42].\\r\\nSlot attention is one of many ways of mapping inputs to slots. GFS-Nets is agnostic to the method of\\r\\nmapping visual features to slot vectors and we ablate different encoders in our experiment section\\r\\n4.1.2.\\r\\n\\r\\n\\r\\n                                                                                                                4\\r\\n\\x0c\\n\\n3.1.2    Encoding and decoding 3D point clouds\\r\\n\\r\\nGFS-Nets featurize a 3D point cloud using a 3D point transformer [70] which maps the 3D input\\r\\npoints to a set of M feature vectors of C dimensions each. We set M to 128 and C to 64 in our\\r\\nexperiments.\\r\\nGFS-Nets decodes 3D point clouds from each slot using implicit functions [46]. Specifically, each\\r\\ndecoder takes in as input the slot vector si and an (X, Y, Z) location and returns the corresponding\\r\\noccupancy score ox,y,z = Dec(si , (x, y, z)) , where Dec is a multi-block ResNet MLP similar to that\\r\\nof Lal et al. [40].\\r\\n\\r\\n3.1.3    Encoding and decoding RGB images\\r\\n\\r\\nGFS-Nets featurize an RGB image concatenated with its pixel coordinates using a convolutional\\r\\nneural network (CNN) and produce a feature map of H × W × C where H, W are the spatial\\r\\ndimensions and C is the feature dimension. We reshape the output feature map to a set of vectors\\r\\nM ∈ R(H×W )×C . We set C as 64.\\r\\nGFS-Nets decodes RGB pixels from each slot using conditional NeRFs [47] that predict the color\\r\\nc and volume density σ at each 3D location of the latent 3D volume. Thus given slot vector\\r\\nsi , spatial location x and viewing direction d, the MLP based decoder network Dec predicts\\r\\n(c, σ) = Dec(si , (x, d)). The color c and density σ of the points are composed together across\\r\\n                                                               PK             PK\\r\\nslots using density σ weighted averaging. Specifically, σ̂ = i=0 wi σi , ĉ = i=0 wi ci where\\r\\n            PK\\r\\nwi = σi / i=0 σi . We then use differentiable volume rendering of the composed σ̂ and ĉ using\\r\\ncamera ray casting to generate the final RGB prediction similar to [67].\\r\\n\\r\\n\\r\\n3.2     Autoencoding primitive entities\\r\\n\\r\\nIn this stage, we train GFS-Nets using a set of primitive entities. Primitive entities are generic or\\r\\nsemantic parts of objects in 3D point clouds or single object multi-view images, as shown on the left\\r\\nof Figure 2. We experiment with different primitive sets in the experimental section.\\r\\nFor encoding 3D point cloud primitive entities, we sample a single slot in the entity bottleneck of\\r\\nGFS-Nets, that is, s ∈ R1×D . For encoding a 2D RGB image, we sample two slots, one from a\\r\\nforeground learnable Gaussian distribution and one from a background learnable Gaussian distribution,\\r\\nthat is s ∈ R2×D . In a forward pass of the encoder, we then update slots s as described in Section 3.1.\\r\\n\\r\\n\\r\\n3.3     Autoencoding scenes\\r\\n\\r\\nIn this stage, we initialize the parameters of the model\\r\\nwith the parameters learnt in the previous section. The\\r\\nonly difference is that instead of instantiating one or two\\r\\nslots we now instantiate K slot vectors, where\\r\\nK is the maximum number of entities any example in the\\r\\ndataset could possibly have. For RGB input, we sample K\\r\\nslots from the foreground Gaussian and one slot from the\\r\\nbackground Gaussian since there can be many foreground\\r\\n                                                                Input & Target   Slow Inference\\r\\nobjects but only a single background.\\r\\nFinally we pass each of our slot vectors through the slot de-   Figure 3: Slow Inference in GFS-Nets:\\r\\ncoder outk = Dec(sk ), whose weights are shared across          First columns shows the input point cloud.\\r\\nslots. For decoding RGB images, GFS-Nets uses separate          In 2nd to 4th columns, we show the segmen-\\r\\ndecoders for foreground and background. For 3D point            tation results throughout training iterations\\r\\ncloud input, outk represents the probability of sampled         of the autoencoding process. Segmentation\\r\\n                                                                improves over time with reconstruction qual-\\r\\npoints being occupied by a given slot sk ; we aggregate         ity, despite that the autoencoding loss only\\r\\nthem by max pooling across the slot dimension. We then          penalizes the reconstruction error.\\r\\nuse binary cross entropy loss between the ground truth\\r\\noccupancies and the predictions.\\r\\n\\r\\n\\r\\n                                                    5\\r\\n\\x0c\\n\\nFor RGB input, outk represents the color and density of\\r\\npoints being occupied by a given slot sk ; we aggregate across slots as described in Section 3.1 and a\\r\\nuse pixel mean squared error loss.\\r\\nFast and Slow Inference Fast inference refers to a single forward pass through the trained net-\\r\\nwork. Slow inference refers to test-time adaptation of all the learnable parameters of the model for\\r\\nautoencoding a single scene using the method described in Section 3.3. Our experiment show that\\r\\nGFS-Nets with slow inference always achieves much better scene decomposition than GFS-Nets with\\r\\nfast inference, especially in out-of-distribution scenes. In contrast, slow inference on entity-centric\\r\\ngenerative models without curricula, such as [42], results in improved reconstruction accuracy but\\r\\nworse scene decomposition performance, due to the reconstruction-segmentation trade off noted in\\r\\n[15].\\r\\n\\r\\n4     Experiments\\r\\nWe test GFS-Nets in the tasks of segmenting object parts in 3D point clouds and segmenting objects\\r\\nand predicting views in 2D multiview images under varying amounts of label supervision. We compare\\r\\nGFS-Nets against previous state-of-the-art models in each task. We further quantify contribution of\\r\\nvarious design choices, namely, slow inference through reconstruction feedback, curriculum learning,\\r\\nand encoder architecture. We evaluate segmentation performance on scenes within the training\\r\\ndistribution as well as out-of-distribution scenes.\\r\\nIn each setup, our experiments aim to answer the following questions:\\r\\n\\r\\n         • How does GFS-Nets compare against state-of-the-art 2D and 3D scene decomposition\\r\\n           models ([43, 67, 5]) under varying amounts of label supervision?\\r\\n         • How much, if any, slow inference through reconstruction feedback improves segmentation\\r\\n           accuracy in GFS-Nets and its variants?\\r\\n         • How much curriculum or supervision during training contributes to segmentation perfor-\\r\\n           mance?\\r\\n\\r\\n4.1     Segmenting parts in 3D object point clouds\\r\\n\\r\\nIn this task, the input is a complete 3D point cloud of an object, and the goal is to segment the 3D\\r\\npoint cloud into different semantic part instances. We consider two setups with different segmentation\\r\\nsupervision:\\r\\n\\r\\n        1. Segmenting without any part segmentation labels.\\r\\n        2. Segmenting with part segmentation labels from a related object category.\\r\\n\\r\\n4.1.1    Segmenting 3D pointclouds without segmentation supervision\\r\\nIn this setup, we test our model and baselines in segmenting the test objects of the Chair and Table\\r\\ncategories in the PartNet benchmark [49] without access to any ground-truth 3D segmentations at\\r\\ntraining time. For training, we provide our model and baselines access to unsegmented objects of the\\r\\nChair category in the trainset of the PartNet benchmark. We evaluate the segmentation performance\\r\\nof our model and baselines with the Adjusted Random Index (ARI) [28] using the implementation of\\r\\nKabra et al. [32], which calculates the similarity between two-point clusters while being invariant to\\r\\nthe ordering of the cluster centers. Neither our model nor the baselines have access to ground-truth\\r\\n3D segmentations during training; as a result, they may output 3D parts of coarser or finer resolution.\\r\\nPartNet contains three different levels of ground-truth segmentation labels with progressively finer\\r\\nsegmentation granularity. We evaluate the ARI score across all three levels of PartNet and report the\\r\\nbest of three for all the models.\\r\\nOur model and two amongst three of our baselines, specifically (PQ-Nets[64] of Wu et al. and\\r\\nShape2Prog [60] of Tian et al.) expect access to a set of 3D primitive parts during training. In this\\r\\nsetup, we consider the primitive part dataset introduced by Shape2Prog [60]. The dataset consists of\\r\\ndifferently sized cubes, cuboids, and discs; we call it the generic primitive set since it is not related\\r\\nto any semantic object category but rather consists of generic 3D parts that remind of generalized\\r\\n\\r\\n\\r\\n                                                   6\\r\\n\\x0c\\n\\ncylinders of Marr et al.[45]. Please refer to the supplementary file for visualizations of the generic\\r\\nprimitive set.\\r\\nBaselines: We compare our model against the following baselines: (i) PQ-Nets of Wu et al.[64]\\r\\nassume access to a set of primitive 3D parts, similar to our model, and learn a primitive part\\r\\nautoencoder. Whole object autoencoding uses a sequential model that encodes the 3D point cloud\\r\\ninto a 1D latent vector and sequentially decodes parts using the part decoder. We use the publicly\\r\\navailable code to train the model to autoencode the generic primitive set and the unlabelled 3D point\\r\\nclouds of the instances of the Chair category in PartNet. (ii) Shape2Prog of Tian et al.[60] is a shape\\r\\nprogram synthesis method that is trained supervised to predict shape programs from object 3D point\\r\\nclouds. The program represents the part category, location, and the symmetry relations among the\\r\\nparts (if any). Shape2Prog introduced two synthetically generated datasets that helped the model\\r\\nparse 3D pointclouds from ShapeNet [6] into shape programs without any supervision: i) the generic\\r\\nprimitive set we discussed earlier which they use to train their part decoders, and ii) a synthetic whole\\r\\nshape dataset of chairs and tables generated programmatically alongside its respective ground-truth\\r\\nprograms. Their model requires supervised pre-training on the dataset of synthetic whole shapes\\r\\npaired with programs. We therefore use their publicly available model weights trained on synthetic\\r\\nwhole shapes to further train on PartNet Chairs. Note that no other baseline nor GFS-Nets assumes\\r\\naccess to the synthetic whole shape dataset. (iii) StructImplicit of Genova et al.[21] encodes the\\r\\nobject point cloud into a 1D latent and then uses an MLP to map it to a fixed number of part vectors,\\r\\neach represented with a 3D implicit function. We use the publicly available code to train the model\\r\\non the unlabelled Chair PartNet dataset. We set the number of part vectors based on the maximum\\r\\nnumber of parts any example in the dataset can have.\\r\\n\\r\\n                                                   in-dist (Chairs)                  out-of-dist (Tables)\\r\\n        Method\\r\\n                                             Fast Infer.         Slow Infer.      Fast Infer.       Slow Infer.\\r\\n        Shape2Prog [60]                         0.28                0.53             0.23                 0.40\\r\\n        PQ-Nets [64]                            0.20                0.31             0.17                 0.21\\r\\n        StructImplicit [21]                     0.25                0.23             0.17                 0.17\\r\\n        GFS-Nets w/o curriculum                 0.41                0.35             0.47                 0.38\\r\\n        GFS-Nets-PrimitiveOnly                  0.42                0.48             0.49                 0.57\\r\\n        GFS-Nets                                0.57                0.62             0.60                 0.69\\r\\n\\r\\nTable 1: ARI Segmentation accuracy (higher the better) in the test set of Chair (in-distribution) and Table\\r\\ncategory of PartNet(out-of-distribution). GFS-Nets significantly outperform all of the baselines.\\r\\n\\r\\nWe consider the following ablative versions of GFS-Nets: (i) GFS-Nets w/o curriculum is trained\\r\\nwithout the primitive learning stage. Instead, it is trained to directly autoencode the aggregated dataset\\r\\nof both the generic primitive set and 3D point clouds of training chairs (as described in Section 3.3).\\r\\nThis version of our model coincides precisely with the previous work of Slot Attention of Locatello\\r\\net al. [42]. (ii) GFS-Nets-PrimitiveOnly is trained only on the generic primitive set (as described in\\r\\nSection 3.2) and is not trained to autoencode and 3D point clouds of training chairs.\\r\\n                                                                                      Input     3D-DETR   Learning to Group   Ours\\r\\n\\r\\nAny model that has a decoder that reconstructs the input\\r\\ncan be additionally trained with slow inference with re-\\r\\nconstruction feedback at each test scene independently.\\r\\nGFS-Nets, PQ-Nets [64], Shape2Prog [60], and StructIm-\\r\\nplicit [21] are all equipped with such decoders. We thus   Input  Shape2Prog [62] PQ-Nets [66] Structured Implicit [22] Ours\\r\\n\\r\\ntest both our model and the baselines in both modes of\\r\\nfast (regular) and slow inference; we use the notation Figure 4: Unsupervised 3D segmentations of\\r\\n[modelname]-Slow and [modelname]-Fast to denote the out-of-distribution scenes for GFS-Nets and\\r\\n                                                         baselines.\\r\\nmodel with the corresponding inference type.\\r\\nWe show quantitative and qualitative results of our model\\r\\nand the baselines in Table 4 and Figure 4 respectively. From the Table 4 we draw the following\\r\\nconclusions:\\r\\nSegregation into slot like entities using attention helps. GFS-Nets significantly outperform PQ-\\r\\nNets [64] and StructImplicit [21] that map the input object 3D pointcloud into a 1D latent vec-\\r\\n\\r\\n\\r\\n                                                             7\\r\\n\\x0c\\n\\ntor instead of maintaining segregated representations into multiple slot latent vectors. Moreover,\\r\\nGFS-Nets’s ability to dynamically route information per example via attention allows it to automati-\\r\\ncally estimate the number of parts required per input 3D pointcloud due to which, different instances\\r\\ncan have a different number of active slots, while StructImplicit segregates all object instances into a\\r\\nconstant number of parts, which is a hyperparameter of the model.\\r\\nCurriculum training helps. GFS-Nets-Fast outperform by a large margin GFS-Nets w/o curriculum-\\r\\nFast, the version of GFS-Nets that uses exactly the same training data and inference method, but is\\r\\ntrained without curriculum.\\r\\nSlow inference through reconstruction feedback helps in the presence of curriculum and hurts\\r\\nin the absence of it. GFS-Nets, GFS-Nets-PrimitiveOnly, PQ-Nets and Shape2Prog all have cur-\\r\\nriculum through a primitive learning stage, while GFS-Nets w/o curriculum and StructImplicit\\r\\ndo not. GFS-Nets-Slow outperform GFS-Nets-Fast and GFS-Nets-PrimitiveOnly-Slow outperform\\r\\nGFS-Nets-PrimitiveOnly-Fast, PQ-Nets-Slow outperform PQ-Nets-Fast and ShapeProg-Slow outper-\\r\\nform ShapeProg-Fast. In contrast, GFS-Nets w/o curriculum-Slow performs worse than GFS-Nets\\r\\nw/o curriculum-Fast and StructImplicit-Slow does worse than StructImplicit-Fast. Such trade-off\\r\\nbetween reconstruction and segmentation in generative model for scene decomposition has been\\r\\npointed out by Engelcke et al. [15] and our results verify their findings.\\r\\nUnsupervised autoencoding of complex (non primitive) 3D pointclouds helps. GFS-Nets outper-\\r\\nforms GFS-Nets-PrimitiveOnly.\\r\\nPlease see the supplementary file for more experimental details. We visualize intermediate iterations\\r\\nduring slow inference in our supplementary video.\\r\\n\\r\\n4.1.2    Segmenting 3D pointclouds with weak segmentation supervision\\r\\nIn this setup, we test our model and baselines in segmenting the test objects of the Chair and Table\\r\\ncategories in the PartNet benchmark [49], with access to ground-truth pointcloud segmentation\\r\\nlabels of level-3 in the training 3D pointclouds of the Chair category in PartNet. We use the ARI\\r\\nsegmentation metric to evaluate the level 3 ground-truth labels of PartNet for our model and baselines.\\r\\nGFS-Nets assume access to a set of 3D primitive parts for primitive entity learning. In this setup, we\\r\\nconsider each 3D part from each training chair 3D pointcloud example as a separate 3D primitive. We\\r\\naugment these primitives by random rotations and translations; we call it the semantic primitive set.\\r\\n\\r\\n                                                            in-dist (Chair)                   out-dist (Table)\\r\\n        Method\\r\\n                                                 Fast Infer.            Slow Infer.       Fast Infer.   Slow Infer.\\r\\n        3D-DETR [5]                                  0.67                   n/a              0.41           n/a\\r\\n        Learning2Group [43]                          0.62                   n/a              0.46           n/a\\r\\n        GFS-Nets w/o supervision                     0.40                  0.44              0.31          0.48\\r\\n        GFS-Nets w/o curriculum                      0.61                  0.66              0.48          0.60\\r\\n        GFS-Nets w/o Gaussian                        0.65                  0.62              0.38          0.58\\r\\n        GFS-Nets w/o SlotAttention                   0.58                  0.55              0.31          0.44\\r\\n        GFS-Nets                                     0.64                  0.67              0.51          0.63\\r\\n\\r\\nTable 2: ARI Segmentation scores (higher the better) on the test set of the Chair category (in-distribution)\\r\\nand on the test set of the Table category (out-of-distribution). All the models are trained using the training set of\\r\\nthe Chair category in PartNet.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n               Input    3D-DETR [5]   Learning2Group [45]    Ours       Cupboard   Lamp      Display    Laptop\\r\\n\\r\\n\\r\\nFigure 5: Out-of-distribution 3D segmentation results for GFS-Nets and baselines. Left: 3D segmentation\\r\\nresults on out-of-distribution shapes for GFS-Nets and baselines. Right: 3D segmentation results for out-of-\\r\\ndistribution categories of PartNet for GFS-Nets trained semi-supervised only on the Chair category.\\r\\n\\r\\n\\r\\n\\r\\n                                                                    8\\r\\n\\x0c\\n\\nBaselines We compare our model against the following baselines: (i) Learning2Group of Luo\\r\\net al. [43] progressively groups points into segments by learning pairwise grouping decisions pa-\\r\\nrameterized by features of the two point clusters. Given the intermediate grouping decisions are not\\r\\nsupervised, and the non-differentiability of their grouping functions they use reinforcement learning\\r\\ngradients for training using supervision from the final segmentation. We used the publicly available\\r\\ncode and trained the model in the training set of the Chair category. (ii) 3D-DETR is the equivalent of\\r\\n2D-DETR of Carion et al. [5] for 3D point-cloud segmentation. A point transformer [70] encodes the\\r\\ninput 3D point cloud into a set of point features which are mapped to a learnable set of 16 parametric\\r\\nqueries using 6 layers of transformer encoder (self-attention) and decoder (cross-attention) blocks.\\r\\nThen, each query decodes a 3D point segmentation map by computing multi-head attention with the\\r\\nencoded point features. The attention for each query is then upsampled using a point transformer\\r\\ndecoder. The predicted 3D part segments are then matched against ground-truth segments using\\r\\nHungarian matching [36], and error is computed for each pair of matched ground-truth part and\\r\\npaired prediction through binary cross-entropy loss.\\r\\nWe train GFS-Nets in a semi-supervised way combining an (unsupervised) autoencoding loss and a\\r\\nsupervised part segmentation loss. Specifically alongside the autoencoding loss of the training 3D\\r\\npoint clouds, we apply a part segmentation loss via Hungarian matching between the decoded 3D\\r\\npart shapes from the slots and the ground-truth part 3D segments in the training set, similar to our\\r\\n3D-DETR baseline model. For this case we did not consider the models of PQ-Nets [64], Shape2Prog\\r\\n[60] or [21] as it is not clear on how to train them in a semi-supervised manner using segmentation\\r\\nsupervision.\\r\\nWe consider the following ablative versions of GFS-Nets: (i) GFS-Nets w/o supervision is trained\\r\\nwithout the supervised part segmentation loss, so the model only receives gradients from autoencoding\\r\\nthe primitives and whole chairs. (ii) GFS-Nets w/o Gaussian does not initialize the slot vectors by\\r\\nsampling from a learnable Gaussian but instead uses separate learnable query vectors for each slot,\\r\\nsimilar to DETR. (iii) GFS-Nets w/o SlotAttention does not use Slot Attention for mapping point\\r\\nfeatures to slots. Instead it maps 3D point features to slots via iterative layers of cross (query to point)\\r\\nand self (query-to-query) attention layers on learnable query vectors similar to 3D-DETR and DETR\\r\\n[5]. Please note that slots and queries represent the same thing, but we use the terminology of DETR\\r\\n[5] in this case.\\r\\nWe report the fast and slow inference results for all ablative versions of our model. Our baselines in\\r\\nthis case, 3D-DETR and Learning2Group [43] are feedforward in nature, they are not equipped with\\r\\ndecoders, and thus cannot be evaluated with slow inference.\\r\\nWe show quantitative results in Table 2 and qualitative results in Figure 5. GFS-Nets significantly\\r\\noutperform the baselines, and GFS-Nets-Slow results in a significant boost in performance (∼ 50%)\\r\\nover the feedforward inference in our model, GFS-Nets-Fast. We draw the following conclusions\\r\\nfrom Table 2:\\r\\nSupervision can be used as a replacement to curriculum training. GFS-Nets only slightly out-\\r\\nperforms GFS-Nets w/o curriculum in this setup.\\r\\nGeneric primitives generalize a lot better than category-specific primitives or supervision.\\r\\nGFS-Nets in Table 4 (that uses the generic primitive set) outperforms GFS-Nets and GFS-Nets-\\r\\nw/o supervision in Table 2 (that use the semantic primitive set) in OOD generalization on the Table\\r\\ncategory.\\r\\nCompetition amongst slots helps. GFS-Nets outperforms GFS-Nets w/o SlotAttention, thus show-\\r\\ning competition amongst slot vectors during encoding helps generalization.\\r\\nSlow inference through reconstruction feedback helps OOD generalization of GFS-Nets. Our\\r\\nbaselines 3D-DETR and Learning2Group [43] are feed-forward in nature, they lack any form of\\r\\nreconstruction feedback, and thus cannot adapt as our model through such feedback.\\r\\n\\r\\n4.2   RGB image segmentation and view prediction without supervision\\r\\n\\r\\nIn this task, the input to the model is a single RGB image that contains multiple objects and the goal\\r\\nis to segment the image into objects and predict images from alternative scene viewpoints. We test\\r\\nour model in the following two benchmarks: ((i) CLEVR [31]: this is a commonly used benchmark\\r\\nfor unsupervised image segmentation. The dataset contains scenes that consist of 5 to 7 spheres,\\r\\ncylinders and cubes, in various colors and materials. We use the multi-view version of this dataset\\r\\nintroduced by uORF [67] as the training set.\\r\\n\\r\\n\\r\\n                                                     9\\r\\n\\x0c\\n\\n    Input   Slot Attention [44]   uORF [69]   GFS-Nets-Fast   GFS-Nets-Slow      Input          Predicted Novel Views by GFS-Nets\\r\\n\\r\\n\\r\\nFigure 6: RGB image decomposition and view prediction for GFS-Nets and baselines. Left: From 2nd to 5th\\r\\ncolumn we show reconstructed RGB image superimposed with the predicted segmentation masks for our model\\r\\nand baselines. Right: From 2nd to 5th column we show RGB renderings across multiple novel viewpoints for\\r\\nGFS-Nets.\\r\\n\\r\\n\\r\\n(ii) Room Diverse++: Room Diverse++ builds upon Room Diverse, which is a multi-view RGB\\r\\ndataset introduced by Yu et al.[67] where they placed multiple ShapeNet Chairs with different solid\\r\\ncolors (Red, Blue etc) onto a textured background. We make it more challenging by applying real-\\r\\nworld ShapeNet textures to the object meshes instead of solid colors. We also add more categories\\r\\nsuch as Beds, Benches, Tables, Cupboards, and Sofas to the Chair category. We sample about 6 to 10\\r\\nobjects in each scene.\\r\\n\\r\\nBaselines We consider the following baselines: (i) Slot attention Locatello et al. [42] autoencodes\\r\\nsingle view RGB images, and is described in Section 3.1. We use the publicly available code and\\r\\ntrain the model on the training set for single image autoencoding.\\r\\n(ii) uORF Yu et al. [67] is a recent work that\\r\\nextends Slot Attention of [42] to multiview RGB                                Method                   CLEVR               Room Diverse++\\r\\nimages.                                                                        uORF [67]                 0.86                    0.65\\r\\nDuring primitive training for our model, we as-                                Slot Attention [42]       0.04                    0.28\\r\\nsume access to posed RGB images of single                                      GFS-Nets-Fast             0.86                    0.74\\r\\nobject scenes as our primitive dataset, as shown                               GFS-Nets-Slow             0.92                    0.81\\r\\nin Figure 2 bottom row left. Our implementa-\\r\\ntion builds upon [67] with the difference that we                             Table 3: ARI Segmentation accuracy (higher the bet-\\r\\nconsider curriculum and slow inference at test                                ter) on CLEVR and RoomDiverse++ Dataset for RGB\\r\\n                                                                              input. As can be seen GFS-Nets-Slow outperform all\\r\\ntime. We ablate slow and fast versions of our                                 other baselines in segmentation accuracy.\\r\\nmodel.\\r\\nWe show quantitative results of GFS-Nets and\\r\\nthe baselines in Table 3 and qualitative view prediction and segmentation results in Figure 6.\\r\\nGFS-Nets-slow outperforms the baselines significantly. This suggests that curriculum and slow\\r\\ninference are key to out-of-distribution generalization. Slot Attention [42] achieves very low accuracy\\r\\non CLEVR, as it is unable to disentangle background from foreground, as also reported in Yu et\\r\\nal.[67]. We further visualize GFS-Nets’s ability to render novel viewpoints in the supplementary\\r\\nvideo.\\r\\n\\r\\nLimitations and Future Work: GFS-Nets has limitations that offer several promising directions\\r\\nfor future work. First, GFS-Nets does not model pairwise interactions between slots. Incorporating\\r\\nsuch interactions is a direct avenue for future work. Second, entity decomposition is inherently\\r\\nhierarchical: objects - parts - subparts while GFS-Nets currently models a flat non-hierarchical list of\\r\\nentities. Third, amortizing the slow inference in feedforward predictions could be another interesting\\r\\nextension of the current framework. Finally, although GFS-Nets is tested on far more realistic data\\r\\nthan considered in prior works, scaling the model to more realistic datasets used by mainstream\\r\\nsupervised visual detectors, is a direct avenue for future work.\\r\\n\\r\\n5     Conclusion\\r\\nWe presented GFS-Nets, a model that segments scenes into entities while autoencoding the input\\r\\nscene through slot-based encoders and shared slot decoders. We showed that curriculum training\\r\\nin the form of primitive entities, single object scenes, or labelled segmentations is necessary to\\r\\nbreak the trade-off between segmentation and reconstruction observed in previous works [15]. This\\r\\npermits GFS-Nets to use slow inference through reconstruction feedback and drastically improve the\\r\\ndecompositions of out-of-distribution scenes. We believe that a compositional approach to AI, in\\r\\nterms of grounded symbol-like representations [44, 38] is of fundamental importance for realizing\\r\\nhuman-level generalization [1], and we hope that GFS-Nets may contribute towards that goal.\\r\\n\\r\\n\\r\\n                                                                          10\\r\\n\\x0c\\n\\nReferences\\r\\n [1] Bahdanau, D., Murty, S., Noukhovitch, M., Nguyen, T.H., de Vries, H., Courville, A.: System-\\r\\n     atic generalization: what is required and can it be learned? arXiv preprint arXiv:1811.12889\\r\\n     (2018)\\r\\n [2] Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., Katz, B.:\\r\\n     Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\\r\\n     models. Advances in neural information processing systems 32 (2019)\\r\\n [3] van Bergen, R.S., Kriegeskorte, N.: Going in circles is the way forward: the role of re-\\r\\n     currence in visual inference. Current Opinion in Neurobiology 65, 176–193 (Dec 2020).\\r\\n     https://doi.org/10.1016/j.conb.2020.11.009, http://dx.doi.org/10.1016/j.conb.2020.\\r\\n     11.009\\r\\n [4] Burgess, C.P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., Lerchner, A.:\\r\\n     Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390\\r\\n     (2019)\\r\\n [5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end\\r\\n     object detection with transformers. In: European Conference on Computer Vision. pp. 213–229.\\r\\n     Springer (2020)\\r\\n [6] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\\r\\n     M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet: An Information-Rich 3D Model\\r\\n     Repository. Tech. Rep. arXiv:1512.03012 [cs.GR], Stanford University — Princeton University\\r\\n     — Toyota Technological Institute at Chicago (2015)\\r\\n [7] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\\r\\n     M., Song, S., Su, H., et al.: Shapenet: An information-rich 3d model repository. arXiv preprint\\r\\n     arXiv:1512.03012 (2015)\\r\\n [8] Chen, Z., Tagliasacchi, A., Zhang, H.: Bsp-net: Generating compact meshes via binary space\\r\\n     partitioning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition. pp. 45–54 (2020)\\r\\n [9] Chen, Z., Yin, K., Fisher, M., Chaudhuri, S., Zhang, H.: Bae-net: branched autoencoder\\r\\n     for shape co-segmentation. In: Proceedings of the IEEE/CVF International Conference on\\r\\n     Computer Vision. pp. 8490–8499 (2019)\\r\\n[10] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio,\\r\\n     Y.: Learning phrase representations using rnn encoder-decoder for statistical machine translation.\\r\\n     arXiv preprint arXiv:1406.1078 (2014)\\r\\n[11] Deng, B., Genova, K., Yazdani, S., Bouaziz, S., Hinton, G., Tagliasacchi, A.: Cvxnet: Learnable\\r\\n     convex decomposition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\\r\\n     Pattern Recognition. pp. 31–44 (2020)\\r\\n[12] Deprelle, T., Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: Learning elementary\\r\\n     structures for 3d shape generation and matching. arXiv preprint arXiv:1908.04725 (2019)\\r\\n[13] Du, Y., Smith, K., Ulman, T., Tenenbaum, J., Wu, J.: Unsupervised discovery of 3d physical\\r\\n     objects from video. arXiv preprint arXiv:2007.12348 (2020)\\r\\n[14] Ellis, K., Wong, C., Nye, M., Sable-Meyer, M., Cary, L., Morales, L., Hewitt, L., Solar-Lezama,\\r\\n     A., Tenenbaum, J.B.: Dreamcoder: Growing generalizable, interpretable knowledge with\\r\\n     wake-sleep bayesian program learning. arXiv preprint arXiv:2006.08381 (2020)\\r\\n[15] Engelcke, M., Jones, O.P., Posner, I.: Reconstruction bottlenecks in object-centric generative\\r\\n     models. arXiv preprint arXiv:2007.06245 (2020)\\r\\n[16] Engelcke, M., Kosiorek, A.R., Jones, O.P., Posner, I.: Genesis: Generative scene inference and\\r\\n     sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052 (2019)\\r\\n\\r\\n\\r\\n                                                 11\\r\\n\\x0c\\n\\n[17] Eslami, S., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., Hinton,\\r\\n     G.E.: Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint\\r\\n     arXiv:1603.08575 (2016)\\r\\n[18] Gao, L., Yang, J., Wu, T., Yuan, Y.J., Fu, H., Lai, Y.K., Zhang, H.: Sdm-net: Deep generative\\r\\n     network for structured deformable mesh. ACM Transactions on Graphics (TOG) 38(6), 1–15\\r\\n     (2019)\\r\\n[19] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.: Imagenet-\\r\\n     trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness.\\r\\n     arXiv preprint arXiv:1811.12231 (2018)\\r\\n[20] Genova, K., Cole, F., Sud, A., Sarna, A., Funkhouser, T.: Local deep implicit functions for\\r\\n     3d shape. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition. pp. 4857–4866 (2020)\\r\\n[21] Genova, K., Cole, F., Vlasic, D., Sarna, A., Freeman, W.T., Funkhouser, T.: Learning shape\\r\\n     templates with structured implicit functions. In: Proceedings of the IEEE/CVF International\\r\\n     Conference on Computer Vision. pp. 7154–7164 (2019)\\r\\n[22] Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio, Y., Schölkopf, B.: Recurrent\\r\\n     independent mechanisms. arXiv preprint arXiv:1909.10893 (2019)\\r\\n[23] Greff, K., Kaufman, R.L., Kabra, R., Watters, N., Burgess, C., Zoran, D., Matthey, L., Botvinick,\\r\\n     M., Lerchner, A.: Multi-object representation learning with iterative variational inference. In:\\r\\n     International Conference on Machine Learning. pp. 2424–2433. PMLR (2019)\\r\\n[24] Greff, K., Rasmus, A., Berglund, M., Hao, T.H., Schmidhuber, J., Valpola, H.: Tagger: Deep\\r\\n     unsupervised perceptual grouping. arXiv preprint arXiv:1606.06724 (2016)\\r\\n[25] Greff, K., van Steenkiste, S., Schmidhuber, J.: On the binding problem in artificial neural\\r\\n     networks. CoRR abs/2012.05208 (2020), https://arxiv.org/abs/2012.05208\\r\\n[26] Guerin, F.: Projection: A mechanism for human-like reasoning in artificial intelligence. CoRR\\r\\n     abs/2103.13512 (2021), https://arxiv.org/abs/2103.13512\\r\\n[27] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro-\\r\\n     ceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770–778\\r\\n     (2016)\\r\\n[28] Hubert, L., Arabie, P.: Comparing partitions. Journal of classification 2(1), 193–218 (1985)\\r\\n[29] Jo, J., Bengio, Y.: Measuring the tendency of cnns to learn surface statistical regularities. arXiv\\r\\n     preprint arXiv:1711.11561 (2017)\\r\\n[30] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.:\\r\\n     Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In:\\r\\n     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2901–2910\\r\\n     (2017)\\r\\n[31] Johnson, J., Krishna, R., Stark, M., Li, L.J., Shamma, D., Bernstein, M., Fei-Fei, L.: Image\\r\\n     retrieval using scene graphs. In: Proceedings of the IEEE Conference on Computer Vision and\\r\\n     Pattern Recognition (CVPR) (June 2015)\\r\\n[32] Kabra, R., Burgess, C., Matthey, L., Kaufman, R.L., Greff, K., Reynolds, M., Lerchner, A.:\\r\\n     Multi-object datasets. https://github.com/deepmind/multi-object-datasets/ (2019)\\r\\n[33] Kato, H., Harada, T.: Learning view priors for single-view 3d reconstruction. In: Proceedings of\\r\\n     the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9778–9787 (2019)\\r\\n[34] Kipf, T., Elsayed, G.F., Mahendran, A., Stone, A., Sabour, S., Heigold, G., Jonschkowski,\\r\\n     R., Dosovitskiy, A., Greff, K.: Conditional object-centric learning from video. arXiv preprint\\r\\n     arXiv:2111.12594 (2021)\\r\\n\\r\\n\\r\\n                                                  12\\r\\n\\x0c\\n\\n[35] Kosiorek, A., Kim, H., Teh, Y.W., Posner, I.: Sequential attend, infer, repeat: Generative\\r\\n     modelling of moving objects. Advances in Neural Information Processing Systems 31, 8606–\\r\\n     8616 (2018)\\r\\n\\r\\n[36] Kuhn, H.W.: The hungarian method for the assignment problem. Naval research logistics\\r\\n     quarterly 2(1-2), 83–97 (1955)\\r\\n\\r\\n[37] Kulkarni, T.D., Kohli, P., Tenenbaum, J.B., Mansinghka, V.: Picture: A probabilistic program-\\r\\n     ming language for scene perception. In: Proceedings of the ieee conference on computer vision\\r\\n     and pattern recognition. pp. 4390–4399 (2015)\\r\\n\\r\\n[38] Lake, B.M., Salakhutdinov, R., Tenenbaum, J.B.: Human-level concept learning through\\r\\n     probabilistic program induction. Science 350(6266), 1332–1338 (2015)\\r\\n\\r\\n[39] Lake, B.M., Ullman, T.D., Tenenbaum, J.B., Gershman, S.J.: Building machines that learn and\\r\\n     think like people. Behavioral and brain sciences 40 (2017)\\r\\n\\r\\n[40] Lal, S., Prabhudesai, M., Mediratta, I., Harley, A.W., Fragkiadaki, K.: Coconets: Continuous\\r\\n     contrastive 3d scene representations (2021)\\r\\n\\r\\n[41] Li, Y., Mao, J., Zhang, X., Freeman, W.T., Tenenbaum, J.B., Snavely, N., Wu, J.: Multi-plane\\r\\n     program induction with 3d box priors. arXiv preprint arXiv:2011.10007 (2020)\\r\\n\\r\\n[42] Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J.,\\r\\n     Dosovitskiy, A., Kipf, T.: Object-centric learning with slot attention (2020)\\r\\n\\r\\n[43] Luo, T., Mo, K., Huang, Z., Xu, J., Hu, S., Wang, L., Su, H.: Learning to group: A bottom-up\\r\\n     framework for 3d part discovery in unseen categories. arXiv preprint arXiv:2002.06478 (2020)\\r\\n\\r\\n[44] Marcus, G.F.: The algebraic mind: Integrating connectionism and cognitive science. MIT press\\r\\n     (2003)\\r\\n\\r\\n[45] Marr, D.: Vision: A Computational Investigation into the Human Representation and Processing\\r\\n     of Visual Information. W.H. Freeman and Co., New York, NY (1982)\\r\\n\\r\\n[46] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\\r\\n     Learning 3d reconstruction in function space. In: Proceedings of the IEEE/CVF Conference on\\r\\n     Computer Vision and Pattern Recognition. pp. 4460–4470 (2019)\\r\\n\\r\\n[47] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf:\\r\\n     Representing scenes as neural radiance fields for view synthesis. In: European conference on\\r\\n     computer vision. pp. 405–421. Springer (2020)\\r\\n\\r\\n[48] Misra, I., Girdhar, R., Joulin, A.: An end-to-end transformer model for 3d object detection. In:\\r\\n     Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2906–2917\\r\\n     (2021)\\r\\n\\r\\n[49] Mo, K., Zhu, S., Chang, A.X., Yi, L., Tripathi, S., Guibas, L.J., Su, H.: Partnet: A large-scale\\r\\n     benchmark for fine-grained and hierarchical part-level 3d object understanding. In: Proceedings\\r\\n     of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 909–918 (2019)\\r\\n\\r\\n[50] Niu, C., Li, J., Xu, K.: Im2struct: Recovering 3d shape structure from a single rgb image. In:\\r\\n     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4521–4529\\r\\n     (2018)\\r\\n\\r\\n[51] Paschalidou, D., Gool, L.V., Geiger, A.: Learning unsupervised hierarchical part decomposition\\r\\n     of 3d objects from a single rgb image. In: Proceedings of the IEEE/CVF Conference on\\r\\n     Computer Vision and Pattern Recognition. pp. 1060–1070 (2020)\\r\\n\\r\\n[52] Paschalidou, D., Katharopoulos, A., Geiger, A., Fidler, S.: Neural parts: Learning expressive 3d\\r\\n     shape abstractions with invertible neural networks. In: Proceedings of the IEEE/CVF Conference\\r\\n     on Computer Vision and Pattern Recognition. pp. 3204–3215 (2021)\\r\\n\\r\\n\\r\\n                                                 13\\r\\n\\x0c\\n\\n[53] Paschalidou, D., Ulusoy, A.O., Geiger, A.: Superquadrics revisited: Learning 3d shape parsing\\r\\n     beyond cuboids. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition. pp. 10344–10353 (2019)\\r\\n[54] Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by self-supervised\\r\\n     prediction. In: International conference on machine learning. pp. 2778–2787. PMLR (2017)\\r\\n[55] Rahaman, N., Goyal, A., Gondal, M.W., Wuthrich, M., Bauer, S., Sharma, Y., Bengio, Y.,\\r\\n     Schölkopf, B.: S2rms: Spatially structured recurrent modules. arXiv preprint arXiv:2007.06533\\r\\n     (2020)\\r\\n[56] Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object detection with\\r\\n     region proposal networks. CoRR abs/1506.01497 (2015), http://arxiv.org/abs/1506.\\r\\n     01497\\r\\n[57] Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. arXiv preprint\\r\\n     arXiv:1710.09829 (2017)\\r\\n[58] Sitzmann, V., Chan, E., Tucker, R., Snavely, N., Wetzstein, G.: Metasdf: Meta-learning signed\\r\\n     distance functions. Advances in Neural Information Processing Systems 33, 10136–10147\\r\\n     (2020)\\r\\n[59] Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., Hardt, M.: Test-time training with self-\\r\\n     supervision for generalization under distribution shifts. In: International Conference on Machine\\r\\n     Learning. pp. 9229–9248. PMLR (2020)\\r\\n[60] Tian, Y., Luo, A., Sun, X., Ellis, K., Freeman, W.T., Tenenbaum, J.B., Wu, J.: Learning to infer\\r\\n     and execute 3d shape programs. arXiv preprint arXiv:1901.02875 (2019)\\r\\n[61] Tsai, Y.H.H., Srivastava, N., Goh, H., Salakhutdinov, R.: Capsules with inverted dot-product\\r\\n     attention routing. arXiv preprint arXiv:2002.04764 (2020)\\r\\n[62] Tulsiani, S., Su, H., Guibas, L.J., Efros, A.A., Malik, J.: Learning shape abstractions by\\r\\n     assembling volumetric primitives. In: Proceedings of the IEEE Conference on Computer Vision\\r\\n     and Pattern Recognition. pp. 2635–2643 (2017)\\r\\n[63] Van Steenkiste, S., Chang, M., Greff, K., Schmidhuber, J.: Relational neural expectation\\r\\n     maximization: Unsupervised discovery of objects and their interactions. arXiv preprint\\r\\n     arXiv:1802.10353 (2018)\\r\\n[64] Wu, R., Zhuang, Y., Xu, K., Zhang, H., Chen, B.: Pq-net: A generative part seq2seq network\\r\\n     for 3d shapes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition (CVPR) (June 2020)\\r\\n[65] Xia, W., Zhang, Y., Yang, Y., Xue, J.H., Zhou, B., Yang, M.H.: Gan inversion: A survey. arXiv\\r\\n     preprint arXiv:2101.05278 (2021)\\r\\n[66] Yao, C.H., Hung, W.C., Jampani, V., Yang, M.H.: Discovering 3d parts from image collections.\\r\\n     In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12981–\\r\\n     12990 (2021)\\r\\n[67] Yu, H.X., Guibas, L.J., Wu, J.: Unsupervised discovery of object radiance fields. arXiv preprint\\r\\n     arXiv:2107.07905 (2021)\\r\\n[68] Zablotskaia, P., Dominici, E.A., Sigal, L., Lehrmann, A.M.: Unsupervised video decomposition\\r\\n     using spatio-temporal iterative inference. arXiv preprint arXiv:2006.14727 (2020)\\r\\n[69] Zhang, N., Donahue, J., Girshick, R., Darrell, T.: Part-based r-cnns for fine-grained category\\r\\n     detection. In: European conference on computer vision. pp. 834–849. Springer (2014)\\r\\n[70] Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: Proceedings of the\\r\\n     IEEE/CVF International Conference on Computer Vision. pp. 16259–16268 (2021)\\r\\n[71] Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty-thousand classes\\r\\n     using image-level supervision. arXiv preprint arXiv:2201.02605 (2022)\\r\\n\\r\\n\\r\\n                                                  14\\r\\n\\x0c\\n\\n[72] Zhu, J.Y., Krähenbühl, P., Shechtman, E., Efros, A.A.: Generative visual manipulation on the\\r\\n     natural image manifold. In: European conference on computer vision. pp. 597–613. Springer\\r\\n     (2016)\\r\\n[73] Zoran, D., Kabra, R., Lerchner, A., Rezende, D.J.: Parts: Unsupervised segmentation with\\r\\n     slots, attention and independence maximization. In: Proceedings of the IEEE/CVF International\\r\\n     Conference on Computer Vision. pp. 10439–10447 (2021)\\r\\n[74] Zuffi, S., Kanazawa, A., Berger-Wolf, T., Black, M.J.: Three-d safari: Learning to estimate\\r\\n     zebra pose, shape, and texture from images\" in the wild\". In: Proceedings of the IEEE/CVF\\r\\n     International Conference on Computer Vision. pp. 5359–5368 (2019)\\r\\n\\r\\n\\r\\nAppendix\\r\\nThe structure of this appendix is as follows: In Section 6 we cover the details on the datasets. In\\r\\nSection 7 we specify further implementation details. In Section 8 we provide additional qualitative\\r\\nand quantitative results for the experiments in Section 4 of our main paper.\\r\\n\\r\\n6     Datasets\\r\\n6.1   Point Cloud\\r\\n\\r\\nFor all the tasks we subsample the input point clouds to a standard size of 2048 points.\\r\\n\\r\\nGeneric primitive part dataset. We use the primitive dataset of [60] for learning the primitive\\r\\ndistribution in Section 4.1.1. The dataset consists of 200K primitive instances sampled from the\\r\\nprimitive templates that are visualized in Figure 7. Instances are sampled from the templates by\\r\\nchanging their sizes and placing them in random locations, similar to [60].\\r\\n\\r\\nCategory-specific primitives from PartNet dataset. We seperate out individual segments from\\r\\nthe level-3 instance segmentation masks from the official train-split of Chair category of the PartNet\\r\\ndataset. We additionally do rotation and translation based augmentations on individual segments. We\\r\\nvisualize some of these primitive examples in Figure 8. We use this dataset for learning the primitive\\r\\ndistribution in Section 4.1.2. The dataset in total consists of about 100K primitive examples.\\r\\n\\r\\nSynthetic whole shape dataset. This dataset was generated by Shape2Prog[60]. The segmentation\\r\\nlabels from this dataset are used by them as a supervision signal for later generalizing to PartNet\\r\\nshapes. The dataset consists of about 120K synthetically generated Chairs and Tables, we visualize\\r\\nsome of these synthetically generated tables and chairs in Figure 9. Note that no other baseline or\\r\\nGFS-Nets has access to this dataset.\\r\\n\\r\\nPartNet dataset. We use the official level-3 train-test split of PartNet[49]. We use the train split\\r\\nof Chair category as our training set, we consider test split of Table category in PartNet our test\\r\\ncategories. We use this as the train-test split in Section 4.1 of the main paper. We further compare our\\r\\nmodel on the other PartNet categories in supplementary section 8. We set the value of number of\\r\\nslots K as 16 for this dataset.\\r\\n\\r\\n6.2   RGB\\r\\n\\r\\nCLEVR dataset. We use the train-test split of CLEVR dataset opensourced by [67]. Their dataset\\r\\nis built on top of the original CLEVR [30] dataset, where they add additional multi-view rendering.\\r\\nTheir train/test dataset consists of 5,6,7 objects from random geometric primitives(cylinder, cube and\\r\\nsphere) and random colors(red, blue, purple, gray, cyan, yellow, green, brown) placed in random\\r\\nlocations in the scene. The multi-view setting consists of randomly sampling from 360 degree\\r\\nazimuth angles with a fixed elevation angle. Our train data consists of 1000 single object scenes and\\r\\n1000 multi-object scenes. The only difference between their train-test data are the novel locations\\r\\nat which the objects would be placed in the scene. We set the value of number of slots K as 8. We\\r\\nvisualize some of the primitive examples (multi-view single-object scenes) of the CLEVR dataset in\\r\\nFigure 10.\\r\\n\\r\\n\\r\\n                                                  15\\r\\n\\x0c\\n\\nFigure 7: We visualize the generic primitive templates of [60], as you can see they mainly consists of Cubes,\\r\\nCuboids and Discs.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n   Figure 8: We visualize the category-specific primitives extracted from level-3 Chair category of PartNet.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      16\\r\\n\\x0c\\n\\nFigure 9: We visualize the synthetic whole shape dataset of [60]. Shape2Prog supervises their model using the\\r\\nannotations from this dataset.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 10: We visualize some examples of the primitives used in the CLEVR dataset. Primitives are represented\\r\\nas multi-view single object RGB scenes as shown in the figure.\\r\\n\\r\\n\\r\\nRoomDiverse++ dataset. We build RoomDiverse++ on top of the RoomDiverse dataset of [67].\\r\\nIn which instead of using the solid CLEVR colors as textures of meshes we keep the original textures\\r\\nof the ShapeNet meshes, additionaly along with Chairs we also add other ShapeNet categories such\\r\\nas Benches, Cabinets, Beds, Tables and Sofas in the dataset. In total we use 20 object meshes and\\r\\n3 background textures to generate the dataset. We use the same multi-view rendering settings as\\r\\nCLEVR. Our train data consists of 1000 single object scenes and 2000 multi-object scenes. We use\\r\\n6,7,8 objects placed at random locations for generating our multi-object train dataset. We use 500 of\\r\\n7,8,9 object scenes as our test data. We set the value of number of slots K as 10. We visualize some\\r\\nof the primitive examples in the dataset in Figure 11.\\r\\n\\r\\n\\r\\n7     Implementation details\\r\\n\\r\\n7.1   Proposed Model\\r\\n\\r\\nCode, training details and computational complexity. Proposed model is implemented in\\r\\nPython/PyTorch. We use a batch size of 8 for point cloud input and a batch size of 2 on RGB\\r\\ninput. We set our learning rate as 10−4 . We use the Adam optimizer with β1 = 0.9, β2 = 0.999. Our\\r\\nprimitive learning model takes 24 hours (approximately 200k iterations) to converge. Similarly our\\r\\n\\r\\n\\r\\n                                                     17\\r\\n\\x0c\\n\\nFigure 11: We visualize some examples of the primitives used in the Room Diverse++ dataset. Primitives are\\r\\nrepresented as multi-view single object RGB scenes as shown in the figure.\\r\\n\\r\\n\\r\\nprimitive composition learning model takes about 12hr (approximately 100k iterations) to converge.\\r\\nOur slow inference per example takes about 1 min (500 iterations). A forward pass through the\\r\\nproposed model takes about 0.15 secs. We use a single V100 GPU for training and inference.\\r\\n\\r\\nInputs. For RGB input we randomly select 2 views which include the RGB and ground-truth\\r\\negomotion. We use a single RGB image for testing. For RGB, our input is a 64 × 64 × 3 tensor, for\\r\\npoint cloud our input is a 2048 × 3\\r\\n\\r\\nPoint Cloud Encoder. We adopt the point transformer[70] architecture as our encoder. Point\\r\\ntransformer encoder is essentially layers of self attention blocks. Specifically a self attention block\\r\\nincludes sampling of query points and updating them using their N most neighbouring points as\\r\\nkey/value vectors. In the architecture we specifically apply 5 layers of self attention which look as\\r\\nfollows: 2048-16-64, 2048-16-64, 512-16-64, 512-16-64, 128-16-64, 128-16-64. We use the notation\\r\\nof S-N -C, where S is the number of subsampled query points from the point cloud, N is the number\\r\\nof neighbouring points and C is the feature dimension. We thus get an output feature map of size\\r\\n128 × 64.\\r\\n\\r\\nRGB Encoder. Our RGB encoder is similar to that of [67], which is basically a U-net. We\\r\\nconcatenate the RGB values with the pixel coordinates that are normalized in the range of -1 to 1. We\\r\\nthen pass the concatenated 2D image through a convolutional feature extractor. The architecture of\\r\\nthe CNN is as follows: 3-2-64, 3-2-64, 3-2-64, 3-1-64, 3-1-64, 3-1-64. We use the notation of k-s-c,\\r\\nwhere k is the kernel size, s is the number of strides and c is the feature channel. Our final output\\r\\nsize is 64 × 64 × 64.\\r\\n\\r\\nPoint Cloud Decoder. We obtain point occupancies by querying the slot feature vector slotk at\\r\\ndiscrete locations (x, y, z) specifically ox,y,z = Dec(slotk , (x, y, z)). The architecture of Dec is\\r\\nsimilar to that of [40]. Given slotk , which is one of the slot feature vector. We encode the coordinate\\r\\n(x, y, z) into a 64-D feature vector using a linear layer. We denote this vector as z. The inputs slotk\\r\\nand z are then processed as follows:\\r\\n\\r\\n   out_k = RB_i( RB_{i-1}( \\\\cdots RB_1( z + FC_1(slot_k)) \\\\\\\\ \\\\cdots ] + FC_{i-1}(slot_k)) + FC_{i}(slot_k)).\\r\\n                                                                                                               (3)\\r\\nWe set i = 3. F Ci is a linear layer that outputs a 64 dimensional vector. RNi is a 2 layer ResNet\\r\\nMLP block [27]. The architecture of ResNet block is: ReLU, 64-64, ReLU, 64-64. Here, i − o\\r\\nrepresents a linear layer, where i and o are the input and output dimension. Finally outk is then\\r\\npassed through a ReLU activation function followed by a linear layer to generate a single value for\\r\\noccupancy.\\r\\n\\r\\nRGB Decoder. Our decoder follows the architecture of [67]. The decoder is a 8 layer MLP,\\r\\nspecifically ReLU, 64-64, ReLU, 64-64 .... 64-4. The MLP takes in as input slotk feature and the\\r\\nlocation x, y, z and predicts the RGB and density values at that location.\\r\\n\\r\\n\\r\\n                                                       18\\r\\n\\x0c\\n\\nSlot Extractor. Following is the pseudo-code of Slot Attention, which we use for learning primitive\\r\\ncompositions.\\r\\n\\r\\nAlgorithm 1 Slot Attention module. Input is N vectors of dimension Dinputs which is mapped to a\\r\\nset of K slots of dimension Dslots . We sample the learnable multivariate gaussian to get the initial\\r\\nslot features, here the learnable parameters are: µ ∈ RDslots and σ ∈ RDslots .For our experiments we\\r\\nset T = 3.\\r\\n1: Input: inputs ∈ RN ×Dinputs , slots ∼ N (µ, diag(σ)) ∈ RK×Dslots\\r\\n2: Layer params: k, q, v: linear projections; GRU; MLP; LayerNorm (x3)\\r\\n3: inputs = LayerNorm (inputs)\\r\\n4: for t = 0 . . . T\\r\\n5:     slots_prev = slots\\r\\n6:     slots = LayerNorm (slots)\\r\\n7:     attn = Softmax ( √1 k(inputs) · q(slots)T , axis=‘slots’)\\r\\n                             D\\r\\n8:     updates = Avg (weights=attn + ϵ, values=v(inputs))\\r\\n9:     slots = GRU (state=slots_prev, inputs=updates)\\r\\n10:      slots += MLP (LayerNorm (slots))\\r\\n11: return slots\\r\\n\\r\\n\\r\\nFor primitive learning stage on PointCloud we set the value of number of slots K as 1. Additionally\\r\\nwe replace the Softmax () activation in step 7 with a Sigmoid () activation. For primitive learning\\r\\nstage on RGB scenes we set the value of number of slots K as 2. This is due to RGB images having\\r\\na background componenet in them.\\r\\n\\r\\n7.2   Baselines                               Training\\r\\n\\r\\n\\r\\n               Cross Attention                                                                              P\\r\\n                 With Heads                               Attention\\r\\n                                                                                  C                         O\\r\\n                                                            Map\\r\\n                                                                                  o                         I\\r\\n                                                                                  n                         N\\r\\n                                                                                  c                         T\\r\\n                                                          Attention               a FPN-Style\\r\\n            Encoder\\r\\n                                                            Map                   t Transformer             A\\r\\n                                                                                  e                         R\\r\\n                                                                                  n                         G\\r\\n                                                                                  a                         M\\r\\n                                                                                  t                         A\\r\\n                                                                                  e                         X\\r\\n                                                          Attention\\r\\n                                                            Map\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                 Head features Head features        Spatial Attention\\r\\n                                        T=1           T=2                Maps                     Hungarian Matching\\r\\n                                                                                                              +\\r\\n                                                                                                  Binary CrossEntropy Loss\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 12: 3D-DETR model architecture. Our baseline model is very similar to 2D-DETR[5], where we replace\\r\\ntheir base 2D encoder and 2D FPN-style CNN with a 3D PointTransformer[70]. Given point clouds as input\\r\\nour encoder backbone featurizes the points into N feature vectors, we then do iterative self and cross attention\\r\\nusing K learnable query heads, following the implementation of Carion et al.[5]. We then compute attention of\\r\\nthe updated queries wrt to the encoded feature vectors. We then concatenate these attentions along the batch\\r\\ndimension and pass them through a FPN-style transformer that increases their resolution and outputs each query\\r\\nmask logits. We then do hungarian matching and binary cross entropy loss\\r\\n\\r\\n\\r\\n3D-DETR. 3D-DETR is a SOTA segmentation architecture, we build this architecture on top\\r\\nof 2D-DETR[5], where we replace their 2D encoders with a 3D PointTransformer[70]. For fair\\r\\ncomparision we make sure the number of parameters in 3D-DETR is the same as GFS-Nets.\\r\\nThe base encoder is a 5 layer architecture of 1024-16-64, 1024-16-64, 1024-16-64, 1024-16-64,\\r\\n512-16-64, following the notation of S,N ,C where S is the number of sampled query points, N is\\r\\nthe number of selected neigbouring points and C is the feature dimension. We have 16 learnable\\r\\n\\r\\n\\r\\n                                                               19\\r\\n\\x0c\\n\\nqueries with six encoder-decoder layers of transformer attention, each layer consists of 8 heads. We\\r\\nthen compute multi-head cross-attention between each query vector and the encoded 3d points. This\\r\\ngives us a map of size M × 512 × 16, where M is the number of heads in the multi-head attention.\\r\\nEach attention map is individually upsampled through a PointTransformer decoder of the architecture\\r\\n512-16-64, 1024-16-64, 1024-16-64, which gives us the final instance segmentation mask per query,\\r\\nsimilar to DETR[5]. We then use Hungarian matching to match the predicted masks against the\\r\\nground truth masks and then apply binary cross entropy loss for each match. We aggregate the losses\\r\\nfrom each query and backpropagate. Figure 12 visualizes the architecture of 3D-DETR. Note that\\r\\nwe do not follow the 2 stage training of [5], rather we train their model end-to-end for instance\\r\\nsegmentation, similar to our model. We have found this trick to save compute and still not harm the\\r\\nend results.\\r\\n\\r\\nLearning2Group.[43] We use their open-sourced architecture and code for comparision with our\\r\\nmode. We train their model using our datasets from scratch.\\r\\n\\r\\nShape2Prog[60] We use their open-sourced architecture, code and pretrained checkpoints for\\r\\ncomparision with our model. We change the value of number of blocks similar to the number of slots\\r\\nin our model for each dataset.\\r\\n\\r\\nPQ-Nets.[64] We use their open-sourced architecture and code for comparision with our model.\\r\\nWe train their model using our datasets from scratch. We change the value of number of slots in their\\r\\nmodel based on the maximum number of parts in the dataset.\\r\\n\\r\\nStruct Implicit.[21] We use their open-sourced architecture and code for comparision with our\\r\\nmode. We train their model using our datasets from scratch. We change the value of number of slots\\r\\nin their model based on the maximum number of parts in the dataset.\\r\\n\\r\\nuORF.[67] We use their open-sourced architecture, code and pretrained checkpoints for compari-\\r\\nsion with our model on the CLEVR dataset. For RoomDiverse++ dataset, we train their model on\\r\\nour dataset using the hyperparmeters they use for training their model on RoomDiverse dataset. We\\r\\nchange the value of number of slots in their model similar to our model for each dataset.\\r\\n\\r\\nSlot Attention.[42] We use their open-sourced architecture and code for comparision with our\\r\\nmodel on the CLEVR and RoomDiverse++ dataset. We train their model using our datasets from\\r\\nscratch. We change the value of number of slots in their model similar to our model for each dataset.\\r\\n\\r\\n8   More Results\\r\\nSegmenting 3D pointclouds without segmentation supervision. In this section we show ad-\\r\\nditional qualitative and quantitative results for Section 4.1.1 of the main paper. We follow the\\r\\nexperimental setup of Section 4.1.1, where we use the generic part dataset (visualized in Fig 7) for\\r\\nprimitive learning. In Table 4, we further extend Table 1 of the main paper to include different levels\\r\\nin Chair and Table category. Here (X/Y /Z) refers to (level 1/level 2/level 3) scores respectively. We\\r\\npick the best performing level in fast inference and specifically report it’s slow inference results. We\\r\\nfurther qualitatively compare our model against Shape2Prog (best performing baseline) in Figure 13\\r\\n\\r\\n                                     in-dist (Chairs)                  out-of-dist (Tables)\\r\\n      Method\\r\\n                              Fast Infer.         Slow Infer.        Fast Infer.      Slow Infer.\\r\\n      Shape2Prog [60]         0.28/0.21/0.26          0.53        0.21/0.23/0.23          0.40\\r\\n      PQ-Nets [64]            0.20/0.18/0.16          0.31        0.17/0.14/0.16          0.21\\r\\n      StructImplicit [21]     0.18/0.22/0.25          0.23        0.14/0.15/0.17          0.17\\r\\n      GFS-Nets                0.51/0.48/0.57          0.62        0.51/0.55/0.60          0.69\\r\\n\\r\\nTable 4: ARI Segmentation accuracy (higher the better) in the test set of Chair (in-distribution) and Table\\r\\ncategory of PartNet(out-of-distribution). GFS-Nets significantly outperform all of the baselines.\\r\\n\\r\\n\\r\\n\\r\\n                                                    20\\r\\n\\x0c\\n\\nRGB image segmentation and view prediction without supervision In Figure 15, we show more\\r\\nqualitative results for Section 4.3 of the main paper, where we qualitatively compare our model for\\r\\nsegmentation against uORF[67] and Slot Attention [42]. We see that slow inference prevents our\\r\\nmodel from missing to detect occluded objects in the scene, where as the baselines have a lot of False\\r\\nPositives i.e missed detections.\\r\\n\\r\\n                        Input          Shape2Prog          GFS-Nets-Fast   GFS-Nets-Slow\\r\\n                                       (Supervised)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n        Figure 13: Additional shape decomposition results using generic primitives. (Section 4.1.1)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      21\\r\\n\\x0c\\n\\n                                    Instance Segmentation\\r\\n\\r\\n\\r\\n                  3D-DETR          GFS-Nets-Slow        3D-DETR        GFS-Nets-Slow\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 14: Additional shape decomposition results using category-specific primitives. (Section 4.1.2)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                   22\\r\\n\\x0c\\n\\n          Ground Truth   Slot Attention   uORF   GFS-Nets-Fast GFS-Nets-Slow\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 15: Additional RGB segmentation results on RoomDiverse++. (Section 4.3)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          23\\r\\n\\x0c',\n",
       " '                                                                     Transforming Model Prediction for Tracking\\r\\n\\r\\n                                              Christoph Mayer         Martin Danelljan Goutam Bhat Matthieu Paul Danda Pani Paudel\\r\\n                                                                                   Fisher Yu Luc Van Gool\\r\\n                                                                     Computer Vision Lab, D-ITET, ETH Zürich, Switzerland\\r\\narXiv:2203.11192v1 [cs.CV] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 Abstract                                                                                              ToMP-50\\r\\n                                                                                                                   66.9                                                      ToMP-101\\r\\n                                                                                                                   66.7\\r\\n                                            Optimization based tracking methods have been widely                   66.4            TrDiMP                 KeepTrack\\r\\n                                                                                                                   66.2\\r\\n                                                                                                                                              TransT    STARK-ST101\\r\\n                                         successful by integrating a target model prediction mod-\\r\\n                                                                                                                   63.9\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                             NFS\\r\\n                                         ule, providing effective global reasoning by minimizing an                                                     STARK-ST50\\r\\n                                         objective function. While this inductive bias integrates                  65.2\\r\\n\\r\\n                                         valuable domain knowledge, it limits the expressivity of                  64.7                   ToMP-50+\\r\\n                                                                                                                          SuperDiMP         IoUNet\\r\\n                                         the tracking network. In this work, we therefore pro-\\r\\n                                                                                                                   63.9                            SiamRCNN\\r\\n                                         pose a tracker architecture employing a Transformer-based\\r\\n                                         model prediction module. Transformers capture global                               63.1   63.9     64.7              66.4   67.1       68.5\\r\\n                                         relations with little inductive bias, allowing it to learn                                                 LaSOT\\r\\n                                         the prediction of more powerful target models. We fur-\\r\\n                                                                                                             Figure 1. Performance improvements when transforming the\\r\\n                                         ther extend the model predictor to estimate a second set\\r\\n                                                                                                             model optimizer based tracker SuperDiMP [12] ( ) step-by-step.\\r\\n                                         of weights that are applied for accurate bounding box               First, we replace the model optimizer by a Transformer based\\r\\n                                         regression. The resulting tracker relies on training and            model predictor ( ). Secondly, we replace the probabilistic\\r\\n                                         on test frame information in order to predict all weights           IoUNet by a new regressor and predict its weights with the same\\r\\n                                         transductively. We train the proposed tracker end-to-end            model predictor ( ). The performance (success AUC) is reported\\r\\n                                         and validate its performance by conducting comprehensive            on NFS [23] and LaSOT [20] and compared with recent track-\\r\\n                                         experiments on multiple tracking datasets. Our tracker              ers ( ). ToMP-50 and ToMP-101 refer to the different employed\\r\\n                                         sets a new state of the art on three benchmarks, achiev-            backbones ResNet-50 [28] and ResNet-101 [28].\\r\\n                                         ing an AUC of 68.5% on the challenging LaSOT [20]\\r\\n                                         dataset. The code and trained models are available at               frames, providing effective global reasoning when learning\\r\\n                                         https://github.com/visionml/pytracking                              the model. However, it also imposes severe inductive bias\\r\\n                                                                                                             on the predicted target model. Since the target model is ob-\\r\\n                                                                                                             tained by solely minimizing an objective over the previous\\r\\n                                                                                                             frames, the model predictor has limited flexibility. For in-\\r\\n                                         1. Introduction\\r\\n                                                                                                             stance, it cannot integrate any learned priors in the predicted\\r\\n                                            Generic visual object tracking is one of the fundamental         target model. On the other hand, Transformers have also\\r\\n                                         problems in computer vision. The task involves estimating           been shown to provide strong global reasoning across mul-\\r\\n                                         the state of the target object in every frame of a video se-        tiple frames, thanks to the use of self and cross attention.\\r\\n                                         quence, given only the initial target location. One of the          Consequently, Transformers have been applied to generic\\r\\n                                         key problems in object tracking is learning to robustly de-         object tracking [7, 59, 63, 67] with considerable success.\\r\\n                                         tect the target object, given the scarce annotation. Among              In this work, we propose a novel tracking framework that\\r\\n                                         exiting methods, Discriminative Correlation Filters (DCF)           aims at bridging the gap between DCF and Transformer\\r\\n                                         [1, 5, 13, 14, 24, 29, 46, 54] have achieved much success.          based trackers. Our approach employs a compact target\\r\\n                                         These approaches learn a target model to localize the tar-          model for localizing the target, as in DCF. The weights of\\r\\n                                         get in each frame, by minimizing a discriminative objective         this model are however obtained using a Transformer-based\\r\\n                                         function. The target model, often set to a convolutional ker-       model predictor, allowing us to learn more powerful target\\r\\n                                         nel, provides a compact and generalizable representation of         models, compared to DCFs. This is achieved by introduc-\\r\\n                                         the tracked object, leading to the popularity of DCFs.              ing novel encodings of the target state, allowing the Trans-\\r\\n                                            The objective function in DCF integrates both fore-              former to effectively utilize this information. We further ex-\\r\\n                                         ground and background knowledge over the previous                   tend our model predictor to generate weights for a bounding\\r\\n\\r\\n\\r\\n                                                                                                         1\\r\\n\\x0c\\n\\nbox regressor network, in order to condition its predictions           trast, TransT [7] employs a feature fusion network that con-\\r\\non the current target. Our proposed approach ToMP obtains              sists of multiple self and cross attention modules. The fused\\r\\nsignificant improvement in tracking performance compared               output features are fed into a target classifier and a bound-\\r\\nto state-of-the-art DCF-based methods, while also outper-              ing box regressor. TrDiMP [59] adopts the DiMP [1] model\\r\\nforming recent Transformer based trackers (see Fig. 1).                predictor to produce the model weights given the output fea-\\r\\nContributions: In summary, our main contributions are the              tures of the Transformer Encoder as training samples. Af-\\r\\nfollowing: i) We propose a novel Transformer-based model               terwards, the target model computes the target score map\\r\\nprediction module in order to replace traditional optimiza-            by applying the predicted weights on the output features\\r\\ntion based model predictors. ii) We extend the model pre-              produced by the Transformer Decoder. TrDiMP adopts\\r\\ndictor to estimate a second set of weights that are applied            the probabilistic IoUNet [16] for bounding box regression.\\r\\nfor bounding box regression. iii) We develop two novel                 Similar to our tracker, TrDiMP encodes target state infor-\\r\\nencodings that incorporate target location and target extent           mation but integrates it via two different cross attention\\r\\nallowing the Transformer-based model predictor to utilize              modules in the Decoder instead of using two encoding mod-\\r\\nthis information. iv) We propose a parallel two stage track-           ules in front of the Transformer.\\r\\ning procedure at test time to decouple target localization and             In contrast to the aforementioned Transformer based\\r\\nbounding box regression in order to achieve robust and ac-             trackers, STARK [63] adopts the Transformer architecture\\r\\ncurate target detection. v) We perform a comprehensive set             from DETR [6]. Instead of fusing the training and test\\r\\nof ablation experiments to assess the contribution of each             features in the Transformer Decoder they are stacked and\\r\\nbuilding block of our tracking pipeline and evaluate it on             processed jointly by the full Transformer. A single object-\\r\\nseven tracking benchmarks. The proposed tracker ToMP                   query then produces the Decoder output that is fused with\\r\\nsets a new state of the art on three including LaSOT [20]              the Transformer Encoder features. These features are then\\r\\nwhere it achieves an AUC of 68.5% (see Fig. 1). In addition            further processed to directly predict the bounding box of\\r\\nwe show that our tracker ToMP outperforms other Trans-                 the target. In contrast, our tracker employs the same Trans-\\r\\nformer based trackers for every attribute of LaSOT [20].               former architecture from DETR [6] but to replace the model\\r\\n                                                                       optimizer. In the end, our resulting Transformer-based\\r\\n                                                                       model predictor estimates the weights of two separate mod-\\r\\n2. Related Work                                                        els: the target classifier and the bounding box regressor.\\r\\nDiscriminative Model Prediction: DCF based approaches\\r\\nlearn a target model to distinguish the target from back-              3. Method\\r\\nground by minimizing an objective. For long Fourier-                      In this work, we propose a Transformer-based target\\r\\ntransform based solvers were predominant for DCF based                 model prediction network for tracking called ToMP. We first\\r\\ntrackers [5, 15, 29, 46]. Danelljan et al. [13] employed               revisit existing optimization based model predictors and\\r\\na two layer Perceptron as target model and use Conju-                  discuss their limitations in Sec. 3.1. Next, we describe our\\r\\ngate Gradient to solve the optimization problem. Recently,             Transformer-based model prediction approach in Sec. 3.2.\\r\\nmultiple methods have been introduced that enable end-to-              We extend this approach to perform joint target classifi-\\r\\nend training by casting the tracking problem into a meta-              cation and bounding box regression in Sec. 3.3. Finally,\\r\\nlearning problem [1, 58, 72]. These methods are based on               we detail our offline training procedure and online tracking\\r\\nthe idea of unrolling the iterative optimization algorithm for         pipeline in Sec. 3.4 and Sec. 3.5, respectively.\\r\\na fixed number of iterations and to integrate it in the tracking\\r\\npipeline to allow end-to-end training. Bhat et al. [1] learn a         3.1. Background\\r\\ndiscriminative feature space and predict the weights of the               One of the popular paradigms for visual object tracking\\r\\ntarget model based on the target state in the initial frame and        is discriminative model prediction based tracking. These\\r\\nrefine the weights with an optimization algorithm.                     approaches, visualized in Fig. 2a, use a target model to lo-\\r\\nTransformers for Tracking: Recently, several trackers                  calize the target object in the test frame. The weights (pa-\\r\\nhave been introduced that use Transformers [7, 59, 63, 67].            rameters) of this target model are obtained from the model\\r\\nTransformers are typically employed to predict discrim-                optimizer, using the training frames and their annotation.\\r\\ninative features to localize the target object and regress             While a variety of target models are used in the litera-\\r\\nits bounding box. The training features are processed by               ture [1, 13, 33, 46, 54, 58, 72], discriminative trackers share\\r\\nthe Transformer Encoder whereas the Transformer Decoder                a common base formulation to produce the target model\\r\\nfuses training and test features using cross attention layers          weights. This involves solving an optimization problem\\r\\nto compute discriminative features [7, 59, 67].                        such that the target model produces the desired target states\\r\\n    DTT [67] feeds these features to two networks that pre-            yi ∈ Y for the training samples Strain ∈ {(xi , yi )}m     i=1 .\\r\\ndict the location and the bounding box of the target. In con-          Here, xi ∈ X refers to a deep feature map of frame i and m\\r\\n\\r\\n                                                                   2\\r\\n\\x0c\\n\\n                        Test\\r\\n                      Test    Frame\\r\\n                           Frame                                                                    Test Test\\r\\n                                                                                                         FrameFrame\\r\\n                                                                                                                                                   Model\\r\\n                                                                                                                                                 Model    Predictor\\r\\n                                                                                                                                                       Predictor\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                 Backbone\\r\\n                                      Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                      Backbone\\r\\n                                                                                                                                                                                   TargetTarget\\r\\n                                                                                                                                                                                          ScoresScores\\r\\n                                                     Target\\r\\n                                                  Target    Model\\r\\n                                                         Model                                                                              Test Test\\r\\n                                                                                                                                                 FrameFrame\\r\\n                                                                                                                                                  Encoding\\r\\n                                                                                                                                             Encoding\\r\\n\\r\\n                                                          Model\\r\\n                                                              ModelTarget Scores\\r\\n                                                                       Target Scores\\r\\n                                                          Weights                                                                                               Transformer\\r\\n                                                                                                                                                                     Transformer\\r\\n                                                             Weights\\r\\n            Training Frames\\r\\n               Training Frames                                                             Training Frames\\r\\n                                                                                               Training Frames                                                    Encoder\\r\\n                                                                                                                                                                       Encoder\\r\\n                                                                                                                                                                                          ModelModel\\r\\n                                                                                                                                                                                   TargetTarget\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                      Backbone\\r\\n                                      Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                 Backbone\\r\\n                                                                                                                                            Target StateState\\r\\n                                                                                                                                                Target          Transformer\\r\\n                                                                                                                                                                     Transformer\\r\\n                                                 Model Optimizer\\r\\n                                                   Model Optimizer\\r\\n                                                                                                                                             Encoding\\r\\n                                                                                                                                                  Encoding        Decoder\\r\\n                                                                                                                                                                       Decoder\\r\\n                                                                                                                                                                               ModelModel\\r\\n                                                                                                                                                                               Weights\\r\\n                                                                                                                                                                                   Weights\\r\\n\\r\\n\\r\\n         (a) Tracker with optimization based model prediction.                                   (b) Proposed tracker with Transformer based model prediction.\\r\\n\\r\\nFigure 2. Comparison between trackers that employ optimization based model prediction and our Transformer-based model prediction.\\r\\nThe model optimizer [\\x04] in Fig. 2a is replaced by the model predictor in Fig. 2b that consists of the proposed modules [\\x04,\\x04,\\x04,\\x04].\\r\\n\\r\\n\\r\\ndenotes the total number of training frames. The optimiza-                                  predictor based on Transformers (see Fig. 2b). Instead of\\r\\ntion problem reads as follows,                                                              explicitly minimizing an objective as stated in (1), our ap-\\r\\n                     X                                                                      proach learns to directly predict the target model purely\\r\\n    w = arg min              f (h(w̃; x), y) + λg(w̃). (1)                                  from data by end-to-end training. This allows the model\\r\\n             w̃\\r\\n                     (x,y)∈Strain                                                           predictor to integrate target specific priors in the predicted\\r\\nHere, the objective consists of the residual function f which                               model so that it can focus on characteristic features of the\\r\\ncomputes an error between the target model output h(w̃; x)                                  target, in addition to the features that allow to differenti-\\r\\nand the ground truth label y. g(w̃) denotes the regulariza-                                 ate the target from the seen background. Furthermore, our\\r\\ntion term weighted by a scalar λ, while w represents the                                    model predictor also utilizes the current test frame features,\\r\\noptimal weights of the target model. Note that the training                                 in addition to the previous training features, to predict the\\r\\nset Strain contains the annotated first frame, as well as the                               target model in a transductive manner. As a result, the\\r\\nprevious tracked frames with the tracker’s predictions being                                model predictor can utilize the current frame information\\r\\nused as pseudo-labels.                                                                      to predict a more suitable target model. Finally, instead of\\r\\n    Learning the target model by explicitly minimizing the                                  applying the target model on a fixed feature space, defined\\r\\nobjective of (1) provides a robust target model that can dis-                               by the pre-trained feature extractor, our approach can uti-\\r\\ntinguish the target from the previously seen background.                                    lize the target information to dynamically construct a more\\r\\nHowever, such a strategy suffers from notable limita-                                       discriminative feature space for every frame.\\r\\ntions. The optimization based methods compute the tar-                                         An overview of the proposed tracker employing the\\r\\nget model using only limited information available in previ-                                Transformer-based model prediction is shown in Fig. 2b.\\r\\nously tracked frames. That is, they cannot integrate learned                                Similar to the optimization based trackers, it consists of a\\r\\npriors in the target model prediction so as to minimize future                              test and training branch. We first encode the target state in-\\r\\nfailures. Similarly, these methods typically lack the possi-                                formation in the training frames and fuse it with the deep\\r\\nbility to utilize the current test frame in a transductive man-                             image features [\\x04]. Similarly, we also add an encoding\\r\\nner when computing the model weights to improve tracking                                    to the test frame in order to mark it as test frame [\\x04].\\r\\nperformance. The optimization based methods also require                                    The features from both the training and test branches are\\r\\nsetting multiple optimizer hyper-parameters, and can over-                                  then jointly processed in the Transformer Encoder [\\x04] that\\r\\nfit/underfit on the training samples. Another limitation of                                 produces enhanced features by reasoning globally across\\r\\noptimization based trackers is their procedure that produces                                frames. Next, the Transformer Decoder [\\x04] predicts the tar-\\r\\nthe discriminative features. Usually, the features provided                                 get model weights [ ] using the output of the Transformer\\r\\nto the target model are simply the extracted test features.                                 Encoder. Finally, the predicted target model is applied on\\r\\nInstead of reinforced features by using the target state in-                                the enhanced test frame features to localize the target. Next,\\r\\nformation contained in the training frames. Extracting such                                 we describe the main components in our tracking pipeline.\\r\\nenhanced features would allow reliable differentiation be-\\r\\ntween the target and background regions in the test frame.                                  Target Location Encoding: We propose a target location\\r\\n                                                                                            encoding that allows the model predictor to incorporate the\\r\\n3.2. Transformer-based Target Model Prediction\\r\\n                                                                                            target state information from the training frames, when pre-\\r\\n   In order to overcome the aforementioned limitations of                                   dicting the target model. In particular, we use the embed-\\r\\noptimization based target localization approaches, we pro-                                  ding efg ∈ R1×C that represents foreground. Together with\\r\\npose to replace the model optimizer by a novel target model                                 a Gaussian yi ∈ RH×W ×1 centered at the target location,\\r\\n\\r\\n\\r\\n                                                                                       3\\r\\n\\x0c\\n\\n                    Backbone\\r\\n  Test                                                                                                                              Target Classi cation\\r\\n                                                 +\\r\\n Frame\\r\\n                                  xtest                                                                                                     wcls\\r\\n                   etest            µ(etest )\\r\\n                                                                                                                                    h(wcls ; ztest )\\r\\nTraining Frames\\r\\n                                                                                                                                                                ŷtest\\r\\n                                                             v1            v2         vtest\\r\\n                    Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             +   +\\r\\n\\r\\n                                     xi\\r\\n                                                             Tenc ([v1 , v2 , vtest ])\\r\\n   d1       d2                                                                                ztest\\r\\n                                                               z1          z2         ztest\\r\\n                     (di )                                                                                                      ⇤\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                           +\\r\\n                                                                                                                wcls\\r\\n                                                                                              w                                                        CNN          dˆtest\\r\\n                                                     efg   Tdec ([z1 , z2 , ztest ], efg )          Linear\\r\\n                                                                                                                  wbbreg\\r\\n                               (yi , efg )\\r\\n\\r\\n   y1       y2           efg\\r\\n\\r\\n\\r\\n Feature Extraction and Target Encoding                    Transformer-based Model Prediction                                   Bounding Box Regression\\r\\nFigure 3. Overview of the entire ToMP tracking pipeline for joint model prediction. First, the training [\\x04] and test [\\x04] features are\\r\\nextracted using a backbone. Then the target location [\\x04] and bounding box [\\x04] encodings are added to the training features. For the test\\r\\nframe the test embedding is encoded [\\x04] and added to the test features. The features are then concatenated and jointly processed by the\\r\\nTransformer-based model predictor that produces the weights used for target classification [\\x04] and bounding box regression [\\x04].\\r\\n    fi\\r\\nwe define the target encoding function                                                   features, which serve as the input to the target model when\\r\\n                                                                                         localizing the target.\\r\\n                      ψ(yi , efg ) = yi · efg ,                          (2)                 Given multiple encoded training features vi ∈\\r\\n                                                                                         RH×W ×C and an encoded test feature vtest ∈ RH×W ×C ,\\r\\nwhere ”·” denotes point-wise multiplication with broadcast-                              we reshape the features to R(H·W )×C and concatenate all m\\r\\ning. Note, that Him = s · H and Wim = s · W correspond to                                training features vi and the test feature vtest along the first\\r\\nthe spatial dimension of the image patch and s to the stride                             dimension. These concatenated features are then processed\\r\\nof the backbone network used to extract the deep features                                jointly in a Transformer Encoder\\r\\nx ∈ RH×W ×C . Next, we combine the target encoding and\\r\\nthe deep image features x as follows                                                              [z1 , . . . , zm , ztest ] = Tenc ([v1 , . . . , vm , vtest ]).        (5)\\r\\n                                                                                         The Transformer Encoder consists of multi-headed self-\\r\\n                     vi = xi + ψ(yi , efg ).                             (3)             attention modules [56] that enable it to reason globally\\r\\n                                                                                         across a full frame and even across multiple training and\\r\\nThis provides us the training frame features vi ∈ RH×W ×C                                test frames. In addition, the encoded target state identifies\\r\\nwhich contain encoded target state information. Similarly,                               foreground and background regions and enables the Trans-\\r\\nwe also add a test encoding to identify the features corre-                              former to differentiate between both regions.\\r\\nsponding to the test frame as,\\r\\n                                                                                         Transformer Decoder: The outputs of the Transformer\\r\\n                   vtest = xtest + µ(etest ),                            (4)             Encoder (zi and ztest ) are used as inputs for the Transformer\\r\\n                                                                                         Decoder [6, 56] to predict the target model weights\\r\\nwhere µ(·) repeats the token etest for each patch of xtest .                                              w = Tdec ([z1 , . . . , zm , ztest ], efg ).                   (6)\\r\\nTransformer Encoder: We aim to predict our target\\r\\n                                                                                         Note that the inputs zi and ztest are obtained by jointly rea-\\r\\nmodel using the foreground and background information\\r\\n                                                                                         soning over the whole training and test samples, allowing us\\r\\nfrom both the training, as well as the test frames. To achieve\\r\\n                                                                                         to predict a discriminative target model. We use the same\\r\\nthis, we use a Transformer Encoder [6, 56] module to first\\r\\n                                                                                         learned foreground embedding efg as used for target state\\r\\njointly process the features from the training frames and the\\r\\n                                                                                         encoding as input query of the Transformer Decoder such\\r\\ntest frame. The Transformer Encoder serves two purposes\\r\\n                                                                                         that the Decoder predicts the target model weights.\\r\\nin our approach. First, as described later, it computes the\\r\\n                                                                                         Target Model: We use the DCF target model to obtain the\\r\\nfeatures used by the Transformer Decoder module to pre-\\r\\n                                                                                         target classification scores\\r\\ndict the target model. Secondly, inspired by STARK [63],\\r\\nour Transformer Encoder also outputs enhanced test frame                                                         h(w, ztest ) = w ∗ ztest .                              (7)\\r\\n\\r\\n\\r\\n                                                                                 4\\r\\n\\x0c\\n\\nHere, the weights of the convolution filter w ∈ R1×C are                         gression. Concretely, we pass the output w of the Trans-\\r\\npredicted by the Transformer Decoder. Note that the tar-                         former Decoder through a linear layer to obtain the weights\\r\\nget model is applied on the output test features ztest of                        for bounding box regression wbbreg and target classification\\r\\nthe Transformer Encoder. These features are obtained after                       wcls . The weights wcls are then directly used within the tar-\\r\\njoint processing of training and test frames, and thus sup-                      get model h(wcls ; ztest ) as before. The weights wbbreg , on\\r\\nport the target model to reliably localize the target.                           the other hand, are used to condition the output test features\\r\\n                                                                                 ztest of the Transformer Encoder with target information for\\r\\n3.3. Joint Localization and Box Regression                                       bounding box regression, as explained next.\\r\\n    In the previous section, we presented our Transformer                        Bounding Box Regression: To make the encoder output\\r\\nbased architecture for predicting the target model. Although                     features ztest target aware, we follow Yan et al. [63] and\\r\\nthe target model can localize the object center in each frame,                   first compute an attention map wbbreg ∗ ztest using the pre-\\r\\na tracker needs to also estimate an accurate bounding box of                     dicted weights wbbreg . The attention weights are then mul-\\r\\nthe target. DCF based trackers typically employ a dedicated                      tiplied point-wise with the test features ztest before feeding\\r\\nbounding box regression network [13] for this task. While it                     them into a Convolutional Neural Network (CNN). The last\\r\\nis possible to follow a similar strategy, we decide to predict                   layer of the CNN uses an exponential activation function\\r\\nboth models jointly since target localization and bounding                       to produce the normalized bounding box prediction in the\\r\\nbox regression are related tasks that can benefit from one                       same ltrb representation as described in Eq. (8). In order to\\r\\nanother. In order to achieve this, we extend our model as                        obtain the final bounding box estimation, we first extract the\\r\\nfollows. First, instead of only using the target center loca-                    center location by applying the argmax(·) function on the\\r\\ntion when generating the target state encoding, we also en-                      target score map ŷtest predicted by the target model. Next,\\r\\ncode target size information to provide a richer input to our                    we query the dense bounding box prediction dˆtest at the cen-\\r\\nmodel predictor. Secondly, we extend our model predictor                         ter location of the target object to obtain the bounding box.\\r\\nto estimate weights for a bounding box regression network,                       We use two dedicated networks for target localization and\\r\\nin addition to the target model weights. The resulting track-                    bounding box regression in contrast to Yan et al. [63] that\\r\\ning architecture is visualized in Fig. 3. Next, we describe                      uses one network trying to predict both. This allows us as\\r\\neach of these changes in detail.                                                 explained in Sec. 3.5 to decouple target localization from\\r\\nTarget Extent Encoding: In addition to the extracted                             bounding box regression during tracking.\\r\\ndeep image features xi and the target location encoding\\r\\n                                                                                 3.4. Offline Training\\r\\nψ(yi , efg ), we add another encoding to incorporate infor-\\r\\nmation about the bounding box of the target. In order to                            In this section, we describe the protocol to train the pro-\\r\\nencode the bounding box bi = {bxi , byi , bw                 h\\r\\n                                                       i , bi } encompass-       posed tracker ToMP. Similar to recent end-to-end trained\\r\\ning the target object in the training frame i, we adopt the ltrb                 discriminative trackers [1, 16], we sample multiple training\\r\\nrepresentation [22, 55, 62, 67]. First, we map each location                     and test frames from a video sequence to form training sub-\\r\\n(j x , j y ) on the feature map xi back to the image domain                      sequences. In particular, we use two training frames and one\\r\\nusing (k x , k y ) = (b 2s c + s · j x , b 2s c + s · j y ). Then, we com-       test frame. In contrast to recent Transformer based track-\\r\\npute the normalized distance of each remapped location to                        ers [7, 63, 67] but similar to DCF based trackers [1, 13, 16],\\r\\nthe four sides of the bounding box bi as follows,                                we keep the same spatial resolution for training and test\\r\\n                                                                                 frames. We pair each image Ii with the corresponding\\r\\n   li = (k x − bxi )/Wim ,       ri = (k x − bxi − bw\\r\\n                                                    i )/Wim ,                    bounding box bi . We use the target state of the training\\r\\n                                                                      (8)\\r\\n  ti = (k y − byi )/Him ,        bi = (k y − byi − bhi )/Him ,                   frames to encode target information and use the bounding\\r\\n                                                                                 box of the test frame only to supervise training by comput-\\r\\nwhere Wim = s · W and Him = s · H. These four sides are                          ing two losses based on the predicted bounding boxes and\\r\\nused to produce the dense bounding box representation d =                        the derived center location of the target in the test frame.\\r\\n(l, t, r, b), where d ∈ RH×W ×4 . In this representation, we                        We employ the target classification loss from DiMP [1]\\r\\nencode the bounding box using a Multi-Layer Perceptron                           that consists of different losses for background and fore-\\r\\n(MLP) φ and thereby increase the number of dimensions                            ground regions. Further, we employ the generalized Inter-\\r\\nfrom 4 to C before adding the obtained encoding to Eq. (3)                       section over Union loss [52] using the ltrb bounding box\\r\\nsuch that                                                                        representation [55] to supervise bounding box regression\\r\\n                 vi = xi + ψ(yi , efg ) + φ(di ).         (9)\\r\\n                                                                                                                                  ˆ d),\\r\\n                                                                                          Ltot = λcls Lcls (ŷ, y) + λgiou Lgiou (d,      (10)\\r\\nHere, vi is the resulting feature map which is used as input\\r\\nto the Transformer Encoder, see Fig. 3.                                          where λcls and λgiou are scalars weighting the contribution\\r\\nModel Prediction: We extend our architecture to predict                          of each loss. Note that in contrast to FCOS [55] and related\\r\\nweights for the target model, as well as bounding box re-                        trackers [22] we omit an additional centerness loss since\\r\\n\\r\\n                                                                             5\\r\\n\\x0c\\n\\nit would be redundant in addition to our classification loss          and ToMP-50 achieve 19.6 and 24.8 FPS and use a ResNet-\\r\\nthat serves the same purpose. A detailed study examining              101 [28] and ResNet-50 [28] as backbone respectively.\\r\\nthe impact of centerness is available in the supplementary.\\r\\nTraining Details: We train our tracker on the training splits         4.1. Ablation Study\\r\\nof the LaSOT [20], GOT10k [31], Trackingnet [50] and                      We perform a comprehensive analysis of the proposed\\r\\nMS-COCO [43] datasets. We sample 40k sub-sequences                    tracker. First, we analyze the contribution of the different\\r\\nand train for 300 epochs on two Nvidia Titan RTX GPUs.                proposed target state encodings and then examine the effect\\r\\nWe use ADAMW [45] with a learning rate of 0.0001 that we              of different inference settings. Finally, we report the perfor-\\r\\ndecay by a factor of 0.2 after 150 and 250 epochs and weight          mance achieved when replacing the target classifier or the\\r\\ndecay of 0.0001. We set λcls = 100 and λgiou = 1. We con-             bounding box regressor of SuperDiMP with ours. All abla-\\r\\nstruct a training sub-sequence by randomly sampling two               tion experiments in this part use a ResNet-50 as backbone.\\r\\ntraining frames and a test frame from a 200 frame window\\r\\n                                                                      Target State Encoding: In order to analyze the effect of\\r\\nwithin a video sequence. We then extract the image patches\\r\\n                                                                      the different target state encodings we train different vari-\\r\\nafter randomly translating and scaling the image relative to\\r\\n                                                                      ants of our network and evaluate them on multiple datasets.\\r\\nthe target bounding box. Moreover, we use random image\\r\\n                                                                      The first five rows of Tab. 1 correspond to versions with dif-\\r\\nflipping and color jittering for data augmentation. We set\\r\\n                                                                      ferent target location encodings. All other settings are kept\\r\\nthe spatial resolution of the target scores to 18 × 18 and set\\r\\n                                                                      the same. In addition to the foreground and test embedding,\\r\\nthe search area scale factor to 5.0. Further training and ar-\\r\\n                                                                      we include a learned background embedding (instead of set-\\r\\nchitecture details are provided in the supplementary Sec. A.\\r\\n                                                                      ting ebg = 0) to our analysis as follows: ψ(yi , efg , ebg ) =\\r\\n3.5. Online Tracking                                                  yi · efg + (1 − yi ) · ebg . However, Tab. 1 shows (4th vs. 5th\\r\\n                                                                      row) that adding such a learned background embedding de-\\r\\n   During tracking, we use the annotated first frame, as              creases the tracking performance. We further observe that\\r\\nwell as previously tracked frames as our training set Strain .        setting the foreground embedding efg = 0 (1st row) and\\r\\nWhile we always keep the initial frame and its annotation,            only relying on the target extent encoding φ(·) still achieves\\r\\nwe include one previously tracked frame and replace it with           high tracking performance but clearly lacks behind all other\\r\\nthe most recent frame that achieves a target classifier confi-        versions that include the foreground embedding. We con-\\r\\ndence higher than a threshold. Hence, the training set Strain         clude that using only the foreground encoding efg and the\\r\\ncontains at most two frames.                                          test encoding etest leads to the best performance (4th row).\\r\\n   We observed that incorporating previous tracking re-\\r\\n                                                                          In the second part of Tab. 1 we choose the best settings\\r\\nsults in Strain improves the target localization considerably..\\r\\n                                                                      for the target location encoding and remove either the target\\r\\nHowever, including predicted bounding box estimations de-\\r\\n                                                                      extent encoding φ(·) or decouple the Transformer Decoder\\r\\ngrades the bounding box regression performance due to in-\\r\\n                                                                      query from the foreground embedding efg . We observe that\\r\\naccurate predictions, see Sec. 4.1. Hence, we run the model\\r\\n                                                                      using a separate query (6th row) decreases the overall per-\\r\\npredictor twice. First, we include intermediate predictions\\r\\n                                                                      formance. Similarly, we notice that incorporating target ex-\\r\\nin Strain to obtain the classifier weights. In the second pass,\\r\\n                                                                      tent information via the proposed encoding is crucial. Oth-\\r\\nwe only use the annotated initial frame to predict the bound-\\r\\n                                                                      erwise, the performance drops significantly (7th row).\\r\\ning box. Note that for efficiency both steps can be per-\\r\\nformed in parallel in a single forward pass. In particular,           Model Predictor: Since our model predictor estimates two\\r\\nwe reshape the feature map corresponding to two training              different model weights, it seems natural to use two differ-\\r\\nand one test frame to a sequence and duplicate it. Then, we           ent Transformer queries: one to produce the target model\\r\\nstack both in the batch dimension to process them jointly                   efg   ebg   etest   φ(·)   qdec = efg   LaSOT   NFS    OTB\\r\\nwith the model predictor. To only allow attention between              1    7     7      7      X         n.a.      66.0    64.8   68.2\\r\\nthe initial frame with ground truth annotation and the test            2    X     7      7      X          X        67.1    66.6   70.0\\r\\nframe when predicting the model for bounding box regres-               3    X     X      7      X          X        67.1    66.3   69.4\\r\\n                                                                       4    X     7      X      X          X        67.6    66.9   70.1\\r\\nsion, we make use of the so-called key padding mask that\\r\\n                                                                       5    X     X      X      X          X        67.4    66.0   69.5\\r\\nallows to ignore certain keys when computing attention.\\r\\n                                                                       6    X     7      X      X          7        66.0    66.2   69.9\\r\\n                                                                       7    X     7      X      7          X        63.1    64.2   64.0\\r\\n4. Experiments\\r\\n                                                                      Table 1. For efg , ebg and etest learning the embedding is denoted\\r\\n   We evaluate our proposed tracking architecture ToMP                by X whereas 7 means setting it to zero. Using the encoding φ(·)\\r\\non seven benchmarks. Our approach is based on PyTorch                 is denoted by X whereas 7 refers to omitting it. For qdec = efg the\\r\\n1.7 and is developed within the PyTracking [12] frame                 symbol X means sharing the learned embedding efg for encoding\\r\\nwork. PyTracking is available under the GNU GPL 3.0 li-               and querying the Decoder wheres 7 means learning two separate\\r\\ncense. On a single Nvidia RTX 2080Ti GPU, ToMP-101                    embeddings for both tasks. (Our final model is in the 4th row).\\r\\n\\r\\n\\r\\n                                                                  6\\r\\n\\x0c\\n\\n   Number of                        Decoder                                       4.2. Comparison to the State of the Art\\r\\n Decoder queries    Linear Layer   query qdec       LaSOT NFS         OTB\\r\\n        1                X         qdec = efg        67.6     66.9    70.1           We compare our tracker ToMP on seven tracking bench-\\r\\n        2                7         qdec 6= efg       63.7     62.8    67.9        marks. The same settings and parameters are used for all\\r\\n                                                                                  datasets. We recompute the metrics of all trackers using the\\r\\nTable 2. Analysis of different model predictor architectures and its\\r\\n                                                                                  raw predictions if available or otherwise report the results\\r\\nimpact on the tracking performance in terms of success AUC.\\r\\n                                                                                  given in the respective paper.\\r\\n   Two Stage               Previous                                               LaSOT [20]: First, we compare ToMP on the large-scale\\r\\n Model Prediction      Tracking Results          LaSOT      NFS      OTB          LaSOT dataset (280 test sequences with 2500 frames on\\r\\n        n.a.                   7                 65.7       65.3     67.8         average). The success plot in Fig. 5a shows the overlap\\r\\n         X                     X                 67.6       66.9     70.1         precision OPT as a function of the threshold T . Trackers\\r\\n         7                     X                 62.0       64.8     62.8\\r\\n                                                                                  are ranked w.r.t. their area-under-the-curve (AUC) score,\\r\\nTable 3. Analysis of different inference settings an of their impact              shown in the legend. Tab. 5 shows more results including\\r\\non the tracking performance in terms of success AUC.                              precision and normalized precision for each tracker. Both\\r\\n                                                                                  versions of ToMP with different backbones outperform the\\r\\n  Model         Bounding Box                                       LaSOT\\r\\n Predictor        Regressor         LaSOT         NFS    UAV       ExtSub         recent trackers STARK [63], TransT [7], TrDiMP [59] and\\r\\n DiMP [1]      Prob. IoUNet [16]      63.1        64.8   67.7        43.7\\r\\n                                                                                  DTT [67] in AUC and sets a new state-of-the-art result.\\r\\n  ToMP         Prob. IoUNet [16]      64.7        65.2   65.0        45.2         Note that even ToMP with ResNet-50 outperforms STARK-\\r\\n  ToMP               ToMP             67.6        66.9   69.0        45.4         ST101 with ResNet-101 (67.6 vs 67.1). Fig. 4 shows the\\r\\nTable 4. Impact of replacing DiMP [1] and the probabilistic                       success AUC gain of ToMP compared to recent Trans-\\r\\nIoUNet [16] with ToMP for localization and box regression.                        former based trackers for different attributes annotated in\\r\\n                                                                                  LaSOT [20]. We want to highlight that ToMP outper-\\r\\nweights and the other to obtain the bounding box regressor                        forms TransT [7] and TrDiMP [59] on each attribute by\\r\\nweights. However, this involves decoupling the query from                         more than one percent point. Similarly, ToMP achieves\\r\\nthe foreground embedding efg and the experiments in Tab. 2                        higher performance than STARK-ST101 for every attribute.\\r\\nshow a significant performance drop for this case.                                It achieves the highest gain over STARK for Background\\r\\nInference Settings: During online tracking, we use the                            Clutter, showing the disadvantage of using small templates\\r\\ninitial frame and its annotation as training frames. In addi-                     instead of training frames with a large field of view that\\r\\ntion, we include the most recent frame and its target predic-                     allow not only to leverage target, but also background in-\\r\\ntion if the classifier confidence is above a certain threshold.                   formation.\\r\\nTab. 3 shows that including previous tracking results leads                       LaSOTExtSub [19]: This dataset is an extension of\\r\\nto higher tracking performance than using only the initial                        LaSOT. It only contains test sequences assigned to 15\\r\\nframe. Disabling the described two stage model prediction                         new classes with 10 videos each. The sequences con-\\r\\napproach and predicting the weights of the target model and                       tain 2500 frames on average showing challenging track-\\r\\nbounding box regressor at once decreases the tracking per-                        ing scenarios of small, fast moving objects with distrac-\\r\\nformance drastically (-5.6 AUC on LaSOT). The reason is\\r\\nthe sensitivity of the bounding box predictor to inaccurate                                              ToMP ToMP STARK Keep STARK Alpha         Siam   Tr Super      STM        Pr\\r\\n                                                                                                          101  50 ST101 Track ST50 Refine TransT R-CNN DiMP DiMP SAOT Track DTT DiMP\\r\\npredicted boxes that are encoded and used for training.                                                              [63] [48] [63]  [64]   [7]    [57] [59] [12] [73] [22] [67] [16]\\r\\n                                                                                       Precision     73.5      72.2   72.2   70.2   71.2   68.0   69.0   68.4   66.3 65.3    -   63.3 - 60.8\\r\\nTransforming Model Prediction Step-by-Step:                Our                         Norm. Prec    79.2      78.0   76.9   77.2   76.3   73.2   73.8   72.2   73.0 72.2   70.8 69.3 - 68.8\\r\\n                                                                                       Success (AUC) 68.5      67.6   67.1   67.1   66.4   65.3   64.9   64.8   63.9 63.1   61.6 60.6 60.1 59.8\\r\\nmodel predictor can estimate model weights for the target\\r\\nmodel and bounding box regressor. In this part, we will                           Table 5. Comparison on the LaSOT [20] test set ordered by AUC.\\r\\ntransform an optimization based tracker step-by-step to as-\\r\\n                                                                                                    7   ToMP-101 vs TransT [4.38]\\r\\nsess the impact of each transformation step. Tab. 4 shows                                               ToMP-101 vs TrDiMP [4.04]\\r\\n                                                                                 Success AUC Gain\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                    6\\r\\nthat replacing the model optimizer in SuperDiMP (1st row)                                           5   ToMP-101 vs STARK-ST101 [1.58]\\r\\nwith our proposed model predictor to only predict the tar-                                          4\\r\\n                                                                                                    3\\r\\nget model (2nd row) outperforms SuperDiMP on three out                                              2\\r\\nof four datasets. Our tracker ToMP that jointly predicts                                            1\\r\\n                                                                                                    0\\r\\nmodel weights for target localization and bounding box re-\\r\\n                                                                                                                             n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                            Ou ion\\r\\n\\r\\n                                                                                                                            w\\r\\n                                                                                                         tio Blur\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                             r\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                             e\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                             e\\r\\n                                                                                                         Fu ation\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                           tio tion\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                               for n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                             n\\r\\n                                                                                                               nd n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                            n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                           oin tion\\r\\n                                                                                                        rtia lutte\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                       Sc hang\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                        ng\\r\\n                                                                                                       Ca clusio\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                           o\\r\\n                                                                                                                         io\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                       tio\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                           o\\r\\n                                                                                                       Lo f-Vie\\r\\n                                                                                                                          i\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                         i\\r\\n                                                                                                                    tat\\r\\n                                                                                                                        t\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                    lut\\r\\n                                                                                                    ckg clus\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                   ha\\r\\n                                                                                                                   Mo\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                   ria\\r\\n\\r\\n                                                                                                                   ma\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                      o\\r\\n                                                                                                  mi otion\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ngression (3rd row) achieves the best performance on all four\\r\\n                                                                                                                  ari\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                   Vie st M\\r\\n                                                                                                                   C\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                eso\\r\\n                                                                                                                 Ro\\r\\n\\r\\n                                                                                                                t-o\\r\\n                                                                                                               nC\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                tC\\r\\n                                                                                                                Va\\r\\n                                                                                                                 c\\r\\n                                                                                                                 c\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                 pe era\\r\\n                                                                                                             nV\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                             lO\\r\\n                                                                                                            ll O\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                          wR\\r\\n\\r\\n                                                                                                            Fa\\r\\n                                                                                                          ale\\r\\n                                                                                                            M\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                          De\\r\\n                                                                                                         rou\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                          m\\r\\n                                                                                                       Ra\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                       wp\\r\\n                                                                                                      na\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                     Pa\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ndatasets. We conclude that predicting the weights of the tar-\\r\\n                                                                                                    ct\\r\\n                                                                                                 Ba\\r\\n                                                                                              Illu\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                             As\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nget model improves the performance and likewise predict-\\r\\ning the weights of the bounding box regressor. Note that we                       Figure 4. Per attribute analysis on LaSOT [20] between ToMP and\\r\\nreport the average over five runs for all trackers based on                       recent Transformer based trackers. The bar heights correspond to\\r\\nthe probabilistic IoUNet due to its stochasticity.                                the gain of our tracker and the legend shows the average gain.\\r\\n\\r\\n\\r\\n                                                                             7\\r\\n\\x0c\\n\\n                        90\\r\\n                                            Success plot                                                  70\\r\\n                                                                                                                                    Success plot                                         ToMP ToMP STARK Super STARK\\r\\n                        80                                                                                                                                                                101  50   ST50 DiMP ST101 DPMT TRAT UPDT DiMP ATOM\\r\\n                                                                                                          60                                                                                         [63] [12, 35] [63] [35] [35] [3, 35] [1, 35] [13, 35]\\r\\n                        70\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                  Overlap Precision [%]\\r\\n                                                                                                          50                                                                   Accuracy 0.453 0.453     0.478.   0.477   0.481   0.492 0.464 0.465   0.457   0.462\\r\\n                        60    ToMP 101 [68.5]\\r\\n                              ToMP 50 [67.6]                                                              40\\r\\n                                                                                                                                                                               Robustness 0.814 0.789   0.799    0.728   0.775   0.745 0.744 0.755   0.734   0.734\\r\\n                        50    KeepTrack [67.1]\\r\\n                              STARK-ST101 [67.1]                                                                      KeepTrack [48.2]                                         EAO        0.309 0.297   0.308    0.305   0.303   0.303 0.280 0.278   0.274   0.271\\r\\n                        40    STARK-ST50 [66.4]                                                           30          ToMP 101 [45.9]\\r\\n                              AlphaRefine [65.9]                                                                      ToMP 50 [45.4]                                           Table 8. Comparison to the state of the art of bounding box only\\r\\n                        30    TransT [64.9]                                                                           Super DiMP [43.7]\\r\\n                              Siam R-CNN [64.8]                                                           20          LTMU [41.4]\\r\\n                        20    TrDiMP [63.9]\\r\\n                              Super DiMP [63.1]\\r\\n                                                                                                                      DiMP [39.2]\\r\\n                                                                                                                      ATOM [37.6]\\r\\n                                                                                                                                                                               methods on VOT2020ST [35] in terms of EAO score.\\r\\n                        10    STMTrack [60.6]                                                             10          DaSiamRPN [35.6]\\r\\n                              PrDiMP50 [59.8]                                                                         SiamRPN++ [34.0]\\r\\n                         00                                                                                00\\r\\n                                 0.2         0.4     0.6\\r\\n                                         Overlap threshold\\r\\n                                                                0.8          1                                           0.2        0.4       0.6\\r\\n                                                                                                                                  Overlap threshold\\r\\n                                                                                                                                                           0.8         1       and challenging sequences with distractors. Both versions\\r\\n                                   (a) LaSOT [20]                                                                     (b) LaSOTExtSub [19]\\r\\n                                                                                                                                                                               of ToMP exceed the performance of the current best method\\r\\n                                                                                                                                                                               KeepTrack [48] by +0.5% and +0.3%, see Tab. 7.\\r\\n  Figure 5. Success plots, showing OPT , on LaSOT [20] and LaSO-                                                                                                               VOT2020 [35]: Finally, we evaluate on the 2020 edition of\\r\\n  TExtSub [19] and AUC is reported in the legend.                                                                                                                              the Visual Object Tracking short-term challenge. We com-\\r\\n                                   ToMP ToMP STARK      STARK Siam Alpha STM          Tr Keep Super Pr Siam                                                                    pare with the top methods in the challenge [35], as well\\r\\n                                    101  50 ST101 TransT ST50 R-CNN Refine Track DTT DiMP Track DiMP DiMP FC++\\r\\n                                               [63] [7]   [63]  [57] [64] [22] [67] [59] [48] [12] [16] [62]                                                                   as more recent methods. The dataset contains 60 videos\\r\\n           Precision\\r\\n           Norm. Prec\\r\\n                         78.9\\r\\n                         86.4\\r\\n                                             78.6\\r\\n                                             86.2\\r\\n                                                     -\\r\\n                                                    86.9\\r\\n                                                             80.3\\r\\n                                                             86.7\\r\\n                                                                       -\\r\\n                                                                      86.1\\r\\n                                                                                 80.0\\r\\n                                                                                 85.4\\r\\n                                                                                                               78.3\\r\\n                                                                                                               85.6\\r\\n                                                                                                                       76.7 78.9 73.1 73.8 73.3 70.4 70.5\\r\\n                                                                                                                       85.1 85.0 83.3 83.5 83.5 81.6 80.0\\r\\n                                                                                                                                                                               annotated with segmentation masks. Since ToMP produces\\r\\n           Success (AUC) 81.5                81.2   82.0     81.4     81.3       81.2                          80.5    80.3 79.6 78.4 78.1 78.1 75.8 75.4\\r\\n                                                                                                                                                                               bounding boxes we only compare with trackers that produce\\r\\n                              Table 6. Comparison on the TrackingNet [50] test set.                                                                                            the bounding boxes as well. The trackers are evaluated fol-\\r\\n                              ToMP ToMP Keep\\r\\n                               101\\r\\n                                                   STARK              STARK Super Pr STM Siam Siam\\r\\n                                    50 Track CRACT ST101 TrDiMP TransT ST50 DiMP DiMP Track AttN R-CNN KYS DiMP\\r\\n                                                                                                                                                                               lowing the multi-start protocol and are ranked according to\\r\\n                                        [48] [21]    [63]  [59]   [7]   [63] [12] [16] [22] [68] [57] [2] [1]                                                                  the EAO metric that is based on tracking accuracy and ro-\\r\\n          UAV123 66.9                69.0    69.7   66.4     68.2     67.5       69.1                          69.1    67.7 68.0 64.7 65.0          64.9    – 65.3\\r\\n          OTB-100 70.1               70.1    70.9   72.6     68.1     71.1       69.4                          68.5    70.1 69.6 71.9 71.2          70.1   69.5 68.4           bustness, defined using IoU overlap and failure rate respec-\\r\\n          NFS     66.7               66.9    66.4   62.5     66.2     66.2       65.7                          65.2    64.8 63.5  –    –            63.9   63.5 62.0\\r\\n                                                                                                                                                                               tively. The results in Tab. 8 show that ToMP-101 achieves\\r\\n  Table 7. Comparison with the state of the art on the OTB-100 [61],\\r\\n                                                                                                                                                                               the best overall performance, with the highest robustness\\r\\n  NFS [23] and UAV123 [49] datasets in terms of AUC score.\\r\\n                                                                                                                                                                               and competitive accuracy compared to previous methods.\\r\\n  tors present. Fig. 5b shows the success plot where the re-\\r\\n  sults of most trackers are obtained from [19], e.g., DaSi-\\r\\n                                                                                                                                                                               4.3. Limitations\\r\\n  amRPN [74], SiamRPN++ [39], ATOM [13], DiMP [1] and                                                                                                                             Transformer Encoders consist of self-attention layers\\r\\n  LTMU [11]. ToMP exceeds the performance of all track-                                                                                                                        that compute similarity matrices between multiple training\\r\\n  ers except KeepTrack [48] that employs explicit distractor                                                                                                                   and test frame features and thus lead to a large memory foot-\\r\\n  matching between frames. In particular, we outperform Su-                                                                                                                    print that impacts training and inference run-time. Thus, in\\r\\n  perDiMP [12] that uses a model optimizer (+2.2%).                                                                                                                            future work this limitation should be addressed by evalu-\\r\\n  TrackingNet [50]: We evaluate ToMP on the large-scale                                                                                                                        ating alternatives such as [32, 34, 53] aiming at decreasing\\r\\n  TrackingNet dataset that contains 511 test sequences with-                                                                                                                   the memory burden. Another limiting factor of ToMP arises\\r\\n  out publicly available ground-truth. An online evaluation                                                                                                                    from challenging tracking sequences. In particular, distrac-\\r\\n  server is used to obtain the tracking metrics shown in Tab. 6                                                                                                                tors present while the target is occluded is a typical failure\\r\\n  by submitting the raw tracking results. Both versions of                                                                                                                     scenario of ToMP, since it is lacking explicit distractor han-\\r\\n  ToMP achieve competitive results close to the current state                                                                                                                  dling as in KeepTrack [48].\\r\\n  of the art. In particular, ToMP-101 achieves the second\\r\\n  best performance in terms of AUC behind STARK [63],                                                                                                                          5. Conclusion\\r\\n  outperforming other Transformer based trackers such as\\r\\n                                                                                                                                                                                  We propose a novel tracking architecture employing a\\r\\n  TransT [7] and TrDiMP [59].\\r\\n                                                                                                                                                                               Transformer-based model predictor. The model predictor\\r\\n  UAV123 [49]: The UAV dataset consists of 123 test videos                                                                                                                     estimates the weights of the compact DCF target model to\\r\\n  that contains small objects, target occlusion, and distractors.                                                                                                              localize the target in the test frame. In addition, the pre-\\r\\n  Tab. 7 shows the achieved results in terms of success AUC.                                                                                                                   dictor produces a second set of weights used for precise\\r\\n  Again, ToMP achieves competitive results compared to the                                                                                                                     bounding box regression. To achieve this, we develop two\\r\\n  current state of the art achieved by KeepTrack [48].                                                                                                                         new modules that encode target location and its bounding\\r\\n  OTB-100 [61]: We also report results on the OTB-100                                                                                                                          box in the training features. We conduct comprehensive ex-\\r\\n  dataset that contains 100 short sequences. Multiple trackers                                                                                                                 perimental validation and analysis of ToMP on several chal-\\r\\n  achieve results above 70% AUC. Among them are both ver-                                                                                                                      lenging datasets, and set a new state of the art on three.\\r\\n  sions of ToMP, see Tab. 7. ToMP achieve the same perfor-                                                                                                                     Acknowledgments: This work was partly supported by the\\r\\n  mance as SuperDiMP [12] but slightly higher results than                                                                                                                     ETH Zürich Fund (OK), Siemens Smart Infrastructure, the\\r\\n  TransT [7] and slightly lower than TrDiMP [59].                                                                                                                              ETH Future Computing Laboratory (EFCL) financed by a\\r\\n  NFS [23]: We compete on the NFS dataset (30FPS ver-                                                                                                                          gift from Huawei Technologies, an Amazon AWS grant,\\r\\n  sion) containing 100 test videos. It contains fast motions                                                                                                                   and an Nvidia hardware grant.\\r\\n\\r\\n\\r\\n                                                                                                                                                                           8\\r\\n\\x0c\\n\\nReferences                                                              [13] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\\r\\n                                                                             Michael Felsberg. ATOM: Accurate tracking by overlap\\r\\n [1] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu                   maximization. In Proceedings of the IEEE/CVF Conference\\r\\n     Timofte. Learning discriminative model prediction for track-            on Computer Vision and Pattern Recognition (CVPR), June\\r\\n     ing. In Proceedings of the IEEE/CVF International Confer-               2019. 1, 2, 5, 8, 16\\r\\n     ence on Computer Vision (ICCV), October 2019. 1, 2, 5, 7,          [14] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\\r\\n     8, 13, 15, 16                                                           Michael Felsberg. ECO: efficient convolution operators for\\r\\n [2] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu                   tracking. In Proceedings of the IEEE Conference on Com-\\r\\n     Timofte. Know your surroundings: Exploiting scene infor-                puter Vision and Pattern Recognition (CVPR), June 2017. 1,\\r\\n     mation for object tracking. In Proceedings of the European              16\\r\\n     Conference on Computer Vision (ECCV), August 2020. 8,              [15] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,\\r\\n     16                                                                      and Michael Felsberg. Beyond correlation filters: Learning\\r\\n [3] Goutam Bhat, Joakim Johnander, Martin Danelljan, Fa-                    continuous convolution operators for visual tracking. In Pro-\\r\\n     had Shahbaz Khan, and Michael Felsberg. Unveiling the                   ceedings of the European Conference on Computer Vision\\r\\n     power of deep tracking. In Proceedings of the European                  (ECCV), October 2016. 2, 16\\r\\n     Conference on Computer Vision (ECCV), September 2018.              [16] Martin Danelljan, Luc Van Gool, and Radu Timofte. Prob-\\r\\n     8, 16                                                                   abilistic regression for visual tracking. In Proceedings of\\r\\n [4] Goutam Bhat, Felix Järemo Lawin, Martin Danelljan, An-                 the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     dreas Robinson, Michael Felsberg, Luc Van Gool, and Radu                Recognition (CVPR), June 2020. 2, 5, 7, 8, 16, 17\\r\\n     Timofte. Learning what to learn for video object segmenta-         [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\r\\n     tion. In Proceedings of the European Conference on Com-                 and Li Fei-Fei. Imagenet: A large-scale hierarchical image\\r\\n     puter Vision ECCV, August 2020. 15                                      database. In Proceedings of the IEEE/CVF Conference on\\r\\n [5] David S. Bolme, J. Ross Beveridge, Bruce A. Draper, and                 Computer Vision and Pattern Recognition (CVPR), 2009. 13\\r\\n     Yui Man Lui. Visual object tracking using adaptive correla-        [18] Xingping Dong, Jianbing Shen, Ling Shao, and Fatih Porikli.\\r\\n     tion filters. In CVPR, 2010. 1, 2                                       Clnet: A compact latent network for fast adjusting siamese\\r\\n [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas              trackers. In Proceedings of the European Conference on\\r\\n     Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-                 Computer Vision (ECCV), August 2020. 16\\r\\n     to-end object detection with transformers. In Proceedings          [19] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge\\r\\n     of the European Conference on Computer Vision (ECCV),                   Deng, Sijia Yu, Mingzhen Huang, Juehuan Liu, Yong Xu,\\r\\n     pages 213–229, August 2020. 2, 4, 13                                    et al. Lasot: A high-quality large-scale single object track-\\r\\n                                                                             ing benchmark. International Journal of Computer Vision\\r\\n [7] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang,\\r\\n                                                                             (IJCV), 129(2):439–461, 2021. 7, 8, 17, 18\\r\\n     and Huchuan Lu. Transformer tracking. In Proceedings of\\r\\n     the IEEE/CVF Conference on Computer Vision and Pattern             [20] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\\r\\n     Recognition (CVPR), June 2021. 1, 2, 5, 7, 8, 16, 17                    Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\\r\\n                                                                             Lasot: A high-quality benchmark for large-scale single ob-\\r\\n [8] Yiwei Chen, Jingtao Xu, Jiaqian Yu, Qiang Wang, Byungin\\r\\n                                                                             ject tracking. In Proceedings of the IEEE/CVF Conference\\r\\n     Yoo, and Jae Joon Han. AFOD: Adaptive focused discrimi-\\r\\n                                                                             on Computer Vision and Pattern Recognition (CVPR), June\\r\\n     native segmentation tracker. In Proceedings of the European\\r\\n                                                                             2019. 1, 2, 6, 7, 8, 13, 14, 15, 16, 17, 18\\r\\n     Conference on Computer Vision Workshops (ECCVW), Au-\\r\\n                                                                        [21] Heng Fan and Haibin Ling. Cract: Cascaded regression-\\r\\n     gust 2020. 15\\r\\n                                                                             align-classification for robust visual tracking. arXiv preprint\\r\\n [9] Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang,                   arXiv:2011.12483, 2020. 8, 16\\r\\n     and Rongrong Ji. Siamese box adaptive network for vi-              [22] Zhihong Fu, Qingjie Liu, Zehua Fu, and Yunhong Wang.\\r\\n     sual tracking. In Proceedings of the IEEE/CVF Conference                Stmtrack: Template-free visual tracking with space-time\\r\\n     on Computer Vision and Pattern Recognition (CVPR), June                 memory networks. In Proceedings of the IEEE/CVF Confer-\\r\\n     2020. 16                                                                ence on Computer Vision and Pattern Recognition (CVPR),\\r\\n[10] Janghoon Choi, Junseok Kwon, and Kyoung Mu Lee. Visual                  June 2021. 5, 7, 8, 16\\r\\n     tracking by tridentalign and context embedding. In Proceed-        [23] Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva\\r\\n     ings of the Asian Conference on Computer Vision (ACCV),                 Ramanan, and Simon Lucey. Need for speed: A benchmark\\r\\n     November 2020. 16                                                       for higher frame rate object tracking. In ICCV, 2017. 1, 8,\\r\\n[11] Kenan Dai, Yunhua Zhang, Dong Wang, Jianhua Li,                         13, 16, 17\\r\\n     Huchuan Lu, and Xiaoyun Yang. High-performance long-               [24] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.\\r\\n     term tracking with meta-updater. In Proceedings of the                  Learning background-aware correlation filters for visual\\r\\n     IEEE/CVF Conference on Computer Vision and Pattern                      tracking. In Proceedings of the IEEE/CVF International\\r\\n     Recognition (CVPR), June 2020. 8, 16                                    Conference on Computer Vision (ICCV), October 2017. 1\\r\\n[12] Martin Danelljan and Goutam Bhat. PyTracking: Vi-                  [25] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. Graph\\r\\n     sual tracking library based on PyTorch. https : / /                     convolutional tracking. In Proceedings of the IEEE/CVF\\r\\n     github.com/visionml/pytracking, 2019. Ac-                               Conference on Computer Vision and Pattern Recognition\\r\\n     cessed: 1/05/2021. 1, 6, 7, 8, 13, 14, 15, 16, 18                       (CVPR), June 2019. 16\\r\\n\\r\\n\\r\\n                                                                    9\\r\\n\\x0c\\n\\n[26] Dongyan Guo, Yanyan Shao, Ying Cui, Zhenhua Wang,                          Abdelrahman Eldesokey, Jani Käpylä, Gustavo Fernández,\\r\\n     Liyan Zhang, and Chunhua Shen. Graph attention tracking.                   and et al. The seventh visual object tracking vot2019 chal-\\r\\n     In Proceedings of the IEEE/CVF Conference on Computer                      lenge results. In Proceedings of the IEEE/CVF International\\r\\n     Vision and Pattern Recognition (CVPR), June 2021. 16                       Conference on Computer Vision Workshop (ICCVW), Octo-\\r\\n[27] Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, and                         ber 2019. 16\\r\\n     Shengyong Chen. Siamcar: Siamese fully convolutional                  [38] Matej Kristan, Jiřı́ Matas, Aleš Leonardis, Michael Felsberg,\\r\\n     classification and regression for visual tracking. In Proceed-             Roman Pflugfelder, Joni-Kristian Kämäräinen, Hyung Jin\\r\\n     ings of the IEEE/CVF Conference on Computer Vision and                     Chang, Martin Danelljan, Luka Cehovin, Alan Lukežič, On-\\r\\n     Pattern Recognition (CVPR), June 2020. 16                                  drej Drbohlav, Jani Käpylä, Gustav Häger, Song Yan, Jinyu\\r\\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.                     Yang, Zhongqun Zhang, and Gustavo Fernández. The ninth\\r\\n     Deep residual learning for image recognition. In Proceed-                  visual object tracking vot2021 challenge results. In Proceed-\\r\\n     ings of the IEEE/CVF Conference on Computer Vision and                     ings of the IEEE/CVF International Conference on Com-\\r\\n     Pattern Recognition (CVPR), June 2016. 1, 6, 15                            puter Vision (ICCV) Workshops, pages 2711–2738, October\\r\\n[29] João F. Henriques, Rui Caseiro, Pedro Martins, and Jorge                  2021. 16\\r\\n     Batista. High-speed tracking with kernelized correlation fil-         [39] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing,\\r\\n     ters. IEEE Transactions on Pattern Analysis and Machine                    and Junjie Yan. Siamrpn++: Evolution of siamese vi-\\r\\n     Intelligence (TPAMI), 37(3):583–596, 2015. 1, 2                            sual tracking with very deep networks. In Proceedings of\\r\\n[30] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Globaltrack:                    the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     A simple and strong baseline for long-term tracking. In Pro-               Recognition (CVPR), June 2019. 8, 16\\r\\n     ceedings of the Conference on Artificial Intelligence (AAAI),         [40] Siyuan Li, Zhi Zhang, Ziyu Liu, Anna Wang, Linglong Qiu,\\r\\n     February 2020. 16                                                          and Feng Du. Tlpg-tracker: Joint learning of target local-\\r\\n[31] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A                      ization and proposal generation for visual tracking. In Pro-\\r\\n     large high-diversity benchmark for generic object tracking                 ceedings of the International Joint Conference on Artificial\\r\\n     in the wild. IEEE Transactions on Pattern Analysis and Ma-                 Intelligence, IJCAI, July 2020. 16\\r\\n     chine Intelligence (TPAMI), 43(5):1562–1577, 2021. 6                  [41] Yiming Li, Changhong Fu, Fangqiang Ding, Ziyuan Huang,\\r\\n[32] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and                   and Geng Lu. Autotrack: Towards high-performance visual\\r\\n     François Fleuret. Transformers are RNNs: Fast autoregres-                 tracking for uav with automatic spatio-temporal regulariza-\\r\\n     sive transformers with linear attention. In Proceedings of                 tion. In Proceedings of the IEEE/CVF Conference on Com-\\r\\n     the International Conference on Machine Learning (ICML),                   puter Vision and Pattern Recognition (CVPR), June 2020. 16\\r\\n     pages 5156–5165, July 2020. 8                                         [42] Bingyan Liao, Chenye Wang, Yayun Wang, Yaonong Wang,\\r\\n[33] Dai Kenan, Wang Dong, Lu Huchuan, Sun Chong, and Li                        and Jun Yin. Pg-net: Pixel to global matching network for\\r\\n     Jianhua. Visual tracking via adaptive spatially-regularized                visual tracking. In Proceedings of the European Conference\\r\\n     correlation filters. In Proceedings of the IEEE/CVF Confer-                on Computer Vision (ECCV), August 2020. 16\\r\\n     ence on Computer Vision and Pattern Recognition (CVPR),               [43] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.\\r\\n     2019. 2                                                                    Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva\\r\\n[34] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-                     Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft\\r\\n     former: The efficient transformer. In Proceedings of the In-               COCO: common objects in context. In Proceedings of the\\r\\n     ternational Conference on Learning Representations (ICLR),                 European Conference on Computer Vision (ECCV), 2014. 6\\r\\n     2020. 8                                                               [44] Yuan Liu, Ruoteng Li, Yu Cheng, Robby T. Tan, and Xi-\\r\\n[35] Matej Kristan, Aleš Leonardis, Jiřı́ Matas, Michael Fels-                ubao Sui. Object tracking using spatio-temporal networks\\r\\n     berg, Roman Pflugfelder, Joni-Kristian Kämäräinen, Martin               for future prediction location. In Proceedings of the Euro-\\r\\n     Danelljan, Luka Čehovin Zajc, Alan Lukežič, Ondrej Dr-                  pean Conference on Computer Vision (ECCV), August 2020.\\r\\n     bohlav, Linbo He, Yushan Zhang, Song Yan, Jinyu Yang,                      16\\r\\n     Gustavo Fernández, and et al. The eighth visual object track-        [45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\r\\n     ing vot2020 challenge results. In Proceedings of the Euro-                 regularization. In Proceedings of the International Confer-\\r\\n     pean Conference on Computer Vision Workshops (ECCVW),                      ence on Learning Representations (ICLR), 2019. 6\\r\\n     August 2020. 8, 15, 16                                                [46] Alan Lukezic, Tomás Vojı́r, Luka Cehovin Zajc, Jiri Matas,\\r\\n[36] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-                   and Matej Kristan. Discriminative correlation filter tracker\\r\\n     berg, Roman Pfugfelder, Luka Cehovin Zajc, Tomas Vojir,                    with channel and spatial reliability. International Journal of\\r\\n     Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey, Gus-                     Computer Vision (IJCV), 126(7):671–688, 2018. 1, 2\\r\\n     tavo Fernandez, and et al. The sixth visual object track-             [47] Ziang Ma, Linyuan Wang, Haitao Zhang, Wei Lu, and Jun\\r\\n     ing vot2018 challenge results. In Proceedings of the Euro-                 Yin. RPT: learning point set representation for siamese vi-\\r\\n     pean Conference on Computer Vision Workshops (ECCVW),                      sual tracking. In Proceedings of the European Conference on\\r\\n     September 2018. 16                                                         Computer Vision Workshops (ECCVW), August 2020. 15, 16\\r\\n[37] Matej Kristan, Jirı́ Matas, Aleš Leonardis, Michael Felsberg,        [48] Christoph Mayer, Martin Danelljan, Danda Pani Paudel, and\\r\\n     Roman Pflugfelder, Joni-Kristian Kämäräinen, Luka Ce-                   Luc Van Gool. Learning target candidate association to keep\\r\\n     hovin Zajc, Ondrej Drbohlav, Alan Lukezic, Amanda Berg,                    track of what not to track. In Proceedings of the IEEE/CVF\\r\\n\\r\\n\\r\\n                                                                      10\\r\\n\\x0c\\n\\n     International Conference on Computer Vision (ICCV), pages             [61] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-\\r\\n     13444–13454, October 2021. 7, 8, 16, 17, 18                                ing benchmark. IEEE Transactions on Pattern Analysis and\\r\\n[49] Matthias Mueller, Neil Smith, and Bernard Ghanem. A                        Machine Intelligence (TPAMI), 37(9):1834–1848, 2015. 8,\\r\\n     benchmark and simulator for uav tracking. In Proceedings                   13, 16, 17\\r\\n     of the European Conference on Computer Vision (ECCV),                 [62] Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and Gang Yu.\\r\\n     October 2016. 8, 16, 17                                                    Siamfc++: Towards robust and accurate visual tracking with\\r\\n[50] Matthias Müller, Adel Bibi, Silvio Giancola, Salman Al-                   target estimation guidelines. In Proceedings of the Confer-\\r\\n     Subaihi, and Bernard Ghanem. Trackingnet: A large-scale                    ence on Artificial Intelligence (AAAI), February 2020. 5, 8,\\r\\n     dataset and benchmark for object tracking in the wild. In                  16\\r\\n     Proceedings of the European Conference on Computer Vi-                [63] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and\\r\\n     sion (ECCV), 2018. 6, 8                                                    Huchuan Lu. Learning spatio-temporal transformer for\\r\\n[51] Hyeonseob Nam and Bohyung Han. Learning multi-domain                       visual tracking. In Proceedings of the IEEE/CVF Inter-\\r\\n     convolutional neural networks for visual tracking. In Pro-                 national Conference on Computer Vision (ICCV), pages\\r\\n     ceedings of the IEEE/CVF Conference on Computer Vision                     10448–10457, October 2021. 1, 2, 4, 5, 7, 8, 14, 15, 16,\\r\\n     and Pattern Recognition (CVPR), June 2016. 16                              18\\r\\n[52] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir                   [64] Bin Yan, Xinyu Zhang, Dong Wang, Huchuan Lu, and Xi-\\r\\n     Sadeghian, Ian Reid, and Silvio Savarese. Generalized in-                  aoyun Yang. Alpha-refine: Boosting tracking performance\\r\\n     tersection over union: A metric and a loss for bounding box                by precise bounding box estimation. In Proceedings of\\r\\n     regression. In Proceedings of the IEEE/CVF Conference                      the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     on Computer Vision and Pattern Recognition (CVPR), June                    Recognition (CVPR), June 2021. 7, 8, 15, 16, 18\\r\\n     2019. 5                                                               [65] Bin Yan, Haojie Zhao, Dong Wang, Huchuan Lu, and Xi-\\r\\n[53] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and                    aoyun Yang. ’skimming-perusal’ tracking: A framework for\\r\\n     Hongsheng Li. Efficient attention: Attention with linear                   real-time and robust long-term tracking. In Proceedings of\\r\\n     complexities. In Proceedings of the IEEE/CVF Winter Con-                   the IEEE/CVF International Conference on Computer Vision\\r\\n     ference on Applications of Computer Vision (WACV), pages                   (ICCV), October 2019. 16\\r\\n     3531–3539, January 2021. 8\\r\\n                                                                           [66] Tianyu Yang, Pengfei Xu, Runbo Hu, Hua Chai, and An-\\r\\n[54] Chong Sun, Dong Wang, Huchuan Lu, and Ming-Hsuan\\r\\n                                                                                toni B. Chan. Roam: Recurrently optimizing tracking model.\\r\\n     Yang. Correlation tracking via joint discrimination and relia-\\r\\n                                                                                In Proceedings of the IEEE/CVF Conference on Computer\\r\\n     bility learning. In Proceedings of the IEEE/CVF Conference\\r\\n                                                                                Vision and Pattern Recognition (CVPR), June 2020. 16\\r\\n     on Computer Vision and Pattern Recognition (CVPR), 2018.\\r\\n                                                                           [67] Bin Yu, Ming Tang, Linyu Zheng, Guibo Zhu, Jinqiao Wang,\\r\\n     1, 2\\r\\n                                                                                Hao Feng, Xuetao Feng, and Hanqing Lu. High-performance\\r\\n[55] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\r\\n                                                                                discriminative tracking with transformers. In Proceedings of\\r\\n     Fully convolutional one-stage object detection. In Proceed-\\r\\n                                                                                the IEEE/CVF International Conference on Computer Vision\\r\\n     ings of the IEEE/CVF International Conference on Com-\\r\\n                                                                                (ICCV), pages 9856–9865, October 2021. 1, 2, 5, 7, 8, 16\\r\\n     puter Vision (ICCV), October 2019. 5, 14\\r\\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-               [68] Yuechen Yu, Yilei Xiong, Weilin Huang, and Matthew R.\\r\\n     reit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia                Scott. Deformable siamese attention networks for visual ob-\\r\\n     Polosukhin. Attention is all you need. In Advances in Neural               ject tracking. In Proceedings of the IEEE/CVF Conference\\r\\n     Information Processing Systems (NeurIPS), 2017. 4                          on Computer Vision and Pattern Recognition (CVPR), June\\r\\n[57] Paul Voigtlaender, Jonathon Luiten, Philip H.S. Torr, and                  2020. 8, 16\\r\\n     Bastian Leibe.       Siam R-CNN: Visual tracking by re-               [69] Zhipeng Zhang, Yihao Liu, Xiao Wang, Bing Li, and Weim-\\r\\n     detection. In IEEE/CVF Conference on Computer Vision and                   ing Hu. Learn to match: Automatic matching network de-\\r\\n     Pattern Recognition (CVPR), June 2020. 7, 8, 16                            sign for visual tracking. In Proceedings of the IEEE/CVF\\r\\n[58] Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong,                      International Conference on Computer Vision (ICCV), pages\\r\\n     and Wenjun Zeng. Tracking by instance detection: A meta-                   13339–13348, October 2021. 16\\r\\n     learning approach. In Proceedings of the IEEE/CVF Confer-             [70] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and\\r\\n     ence on Computer Vision and Pattern Recognition (CVPR),                    Weiming Hu. Ocean: Object-aware anchor-free tracking. In\\r\\n     June 2020. 2, 16                                                           Proceedings of the European Conference on Computer Vi-\\r\\n[59] Ning Wang, Wengang Zhou, Jie Wang, and Houqiang Li.                        sion (ECCV), August 2020. 16\\r\\n     Transformer meets tracker: Exploiting temporal context for            [71] Zikai Zhang, Bineng Zhong, Shengping Zhang, Zhenjun\\r\\n     robust visual tracking. In Proceedings of the IEEE/CVF                     Tang, Xin Liu, and Zhaoxiang Zhang. Distractor-aware fast\\r\\n     Conference on Computer Vision and Pattern Recognition                      tracking via dynamic convolutions and mot philosophy. In\\r\\n     (CVPR), June 2021. 1, 2, 7, 8, 16, 18                                      Proceedings of the IEEE/CVF Conference on Computer Vi-\\r\\n[60] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and                     sion and Pattern Recognition (CVPR), June 2021. 16\\r\\n     Philip H.S. Torr. Fast online object tracking and segmenta-           [72] Linyu Zheng, Ming Tang, Yingying Chen, Jinqiao Wang, and\\r\\n     tion: A unifying approach. In Proceedings of the IEEE/CVF                  Hanqing Lu. Learning feature embeddings for discriminant\\r\\n     Conference on Computer Vision and Pattern Recognition                      model based tracking. In Proceedings of the European Con-\\r\\n     (CVPR), June 2019. 16                                                      ference on Computer Vision (ECCV), August 2020. 2, 16\\r\\n\\r\\n\\r\\n                                                                      11\\r\\n\\x0c\\n\\n[73] Zikun Zhou, Wenjie Pei, Xin Li, Hongpeng Wang, Feng\\r\\n     Zheng, and Zhenyu He. Saliency-associated object track-\\r\\n     ing. In Proceedings of the IEEE/CVF International Confer-\\r\\n     ence on Computer Vision (ICCV), pages 9866–9875, October\\r\\n     2021. 7, 16\\r\\n[74] Zheng Zhu, Qiang Wang, Li Bo, Wei Wu, Junjie Yan, and\\r\\n     Weiming Hu. Distractor-aware siamese networks for visual\\r\\n     object tracking. In Proceedings of the European Conference\\r\\n     on Computer Vision (ECCV), September 2018. 8, 16\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                  12\\r\\n\\x0c\\n\\nAppendices                                                                  the model predictor we extract features with a stride of 16\\r\\n                                                                            from the third block of the ResNet that we use as back-\\r\\n                                                                            bone. We initialize the backbone with the official weights\\r\\nIn this supplementary material, we first provide details\\r\\n                                                                            obtained by training the backbone on ImageNet [17] and\\r\\nabout training, model architecture and inference in Sec. A.\\r\\n                                                                            freeze the batch norm statistics during training. Since we\\r\\nFurther, we report visual results such as a comparison to\\r\\n                                                                            use a channel dimension of 256 for the Transformer and\\r\\nstate-of-the-art trackers, a comparison of different model\\r\\n                                                                            the ResNet features have 1024 channels we employ an sin-\\r\\npredictors and failure cases of our tracker in Sec. B. After-\\r\\n                                                                            gle convolutional layer to decrease the number of chan-\\r\\nwards, we provide more detailed results of the experiments\\r\\n                                                                            nels before feeding the features into the Transformer En-\\r\\nshown in the main paper in Sec. C.\\r\\n                                                                            coder. The Transformer Encoder consists of layers contain-\\r\\n                                                                            ing multi-headed self attention and a feed-forward network.\\r\\nA. Training, Architecture and Inference                                     We use eight heads and a hidden dimension of 2048 for\\r\\n   First, we provide additional details about the training                  the feed-forward network. Furthermore, we use Dropout\\r\\nfollowed by a detailed description of the architectures em-                 with probability 0.1 and layer normalization. The Trans-\\r\\nployed and finally we provide further inference details.                    former settings are adopted from DETR [6]. The predicted\\r\\n                                                                            target model weights for classification and bounding box\\r\\nA.1. Training and Architecture Details                                      regression consist of a single 1 × 1 filter with 256 chan-\\r\\n   For training we produce the target states y by using a                   nels. The bounding box regression CNN consists of four\\r\\nGaussian with standard deviation 1/4 relative to the base                   convolution-instance-normalization-ReLU layers and a fi-\\r\\ntarget size and by settting τ = 0.05 to differentiate be-                   nal convolution layer, followed by an exponential activa-\\r\\ntween foreground and background regions in the corre-                       tion. The MLP for target extent encoding φ consists of three\\r\\nsponding classification loss lcls adopted from DiMP [1]. For                layers (4 → 64 → 256 → 256) where each layer consists\\r\\n                                                                            of a linear projection, batch normalization and ReLU activa-\\r\\n                                                                            tion except the last that only consist of a linear projection.\\r\\n     Video Frame               SuperDiMP           ToMP-101                 The region-encoding tokens efg and etest are 256 dimen-\\r\\n                     #0037                                                  sional learnable embeddings.\\r\\n\\r\\n                                                                            A.2. Inference Details\\r\\n                                                                               In order to decide whether a previous tracking result\\r\\n                                                                            should be used for training of not we use the maximal value\\r\\n                                                                            of the target score map produced by the target model. In\\r\\n                     #0051\\r\\n                                                                            particular, we select the sample if its confidence value is\\r\\n                                                                            above a certain threshold η. Tab. 9 shows that the chosen\\r\\n                                                                            threshold of 0.9 leads to high performance on LaSOT [20],\\r\\n                                                                            NFS [23] and OTB-100 [61]. Furthermore, we follow Su-\\r\\n                                                                            perDiMP [12] and enter in the target not found state if the\\r\\n                                                                            maximal value of the target score map is bellow 0.25. More-\\r\\n                     #0162\\r\\n\\r\\n\\r\\n\\r\\n                                                                             training frames              NFS    OTB       UAV LaSOT LaSOTExtSub          Speed [FPS]\\r\\n                                                                             1 initial                    65.3   67.8      68.7   65.7          43.7          26.2\\r\\n                                                                             1 initial + 1 recent         66.9   70.1      69.0   67.6          45.4          24.8\\r\\n                                                                             2 initial + 1 recent         67.6   70.5      67.2   68.0          45.4          20.5\\r\\n                                                                             1 initial + 2 recent         66.7   70.8      69.4   67.6          44.4          21.8\\r\\n                                                                             1 initial + 3 recent         66.8   70.5      69.2   67.6          44.2          17.6\\r\\nFigure 6. Visual comparison of the target score maps resulting               1 initial + 4 recent         67.2   70.1      68.2   67.3          44.7          13.2\\r\\n                                                                             1 initial + 5 recent         66.8   70.1      69.1   67.2          43.9          11.3\\r\\nfrom different model predictors.\\r\\n                                                                            Table 10. Comparison of different number of training samples in\\r\\n   Two Stage          Previous     Confidence                               success AUC.\\r\\n Model Prediction Tracking Results Threshold η   LaSOT NFS OTB\\r\\n       X                X             0.85       67.3   66.9 70.3                                            Lcenterness   NFS    OTB    UAV      LaSOT   LaSOTExtSub\\r\\n       X                X             0.90       67.6   66.9 70.1           Classification                       7         66.9   70.1   69.0      67.6      45.4\\r\\n       X                X             0.95       67.4   66.0 69.8           Classification                       X         65.8   69.2   67.3      67.9      45.5\\r\\n                                                                            Centerness                           X         62.7   66.3   67.4      64.4      41.3\\r\\n                                                                            Classification · Centerness          X         63.7   67.8   68.7      65.8      45.3\\r\\nTable 9. Analysis of different inference settings an of their impact\\r\\non the tracking performance in terms of AUC of the success curve.           Table 11. Impact of centerness scores on training and inference.\\r\\n\\r\\n\\r\\n                                                                       13\\r\\n\\x0c\\n\\n                #0200                #0784                #1660                       #0002                 #0412                       #0746\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                #0090                #1136                #1407                       #0006                 #0362                       #1343\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                #0001                #1522                #4919                       #0010                 #1263                       #1853\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                #0030                #0686                #1194                       #0003                 #0616                       #1223\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      Annotation                             SuperDiMP                           STARK-ST101                                ToMP-101\\r\\n\\r\\nFigure 7. Visual comparison of different trackers (ToMP-101, SuperDiMP [12] and STARK-ST101 [63]) on different LaSOT [20] se-\\r\\nquences.\\r\\n\\r\\n\\r\\nover, we use the same spatial resolution of the target scores          therefore needed to identify the center location of the ob-\\r\\nof 18 × 18 and the same search area scale factor of 5.0 dur-           ject, used to select the bounding box offsets. In contrast,\\r\\ning inference and training.                                            our classification branch is directly trained to accurately lo-\\r\\n    Furthermore, we study the effect of using more than two            cate the object’s center. The additional centerness branch is\\r\\ntraining frames stored in the sample memory. Instead of                therefore redundant. Nonetheless, we train our best model\\r\\nusing only one initial and one recent training frame to pre-           with a centerness head and Lcenterness and report the results\\r\\ndict the network weights we test the impact of increasing              in Tab. 11 (2nd -4th rows). The 1st row shows the perfor-\\r\\nthe number of recent training frames and of using multiple             mance when omitting centerness for training. We achieve\\r\\ninitial training frames. We increase the number of initial             comparable results when using the model trained with cen-\\r\\ntraining frames with ground truth bounding box annotations             terness but applying only the classification scores to local-\\r\\nusing an augmentation (vertical flipping and random trans-             ize the target (2nd row). Using only the centerness scores\\r\\nlation). Tab. 10 shows the results for different combinations\\r\\nof multiple initial and recent training frames. Note, that we\\r\\n                                                                                Video Frame          Centerness Scores   Classi cation Scores\\r\\nuse the same network weights for all experiments trained\\r\\nwith one initial and one recent recent frame in all cases.\\r\\nWe observer that using more training frames can improve\\r\\nthe tracking performance but decreases the run-time. Fur-\\r\\nthermore, we observe that the tracker greatly benefits from\\r\\nincluding at least one recent frame for training.\\r\\n\\r\\nA.3. Centerness\\r\\n   Our proposed bounding box regression component is in-\\r\\nspired by FCOS [55] but in contrast to FCOS we omit\\r\\nan auxiliary centerness branch. The classification head of\\r\\nFCOS is trained to predict a high score for almost every               Figure 8. Visual Comparison between centerness and classifica-\\r\\nregion inside the bounding box. The centerness branch is               tion scores.\\r\\n\\r\\n\\r\\n                                                                  14\\r\\n\\x0c\\n\\n              ToMP ToMP                STARK  STARK Ocean Alpha                    Fast\\r\\n             101+AR 50 +AR     RPT ST50+AR ST101+AR Plus Refine AFOD LWTL Ocean\\r\\n                                                                                                      notation of the target object and the predictions of three dif-\\r\\n                              [35, 47]   [63]   [63] [8, 35] [35, 64] [35] [4, 35] [35]               ferent trackers: SuperDiMP [12], STARK-ST101 [63] and\\r\\nEAO           0.497   0.496     0.530   0.505   0.497     0.491   0.482   0.472   0.463 0.461\\r\\nAccuracy      0.750   0.754     0.700   0.759   0.763     0.685   0.754   0.713   0.719 0.693         ToMP-101. We observe that our tracker produces in most\\r\\nRobustness    0.798   0.793     0.869   0.817   0.789     0.842   0.777   0.795   0.798 0.803\\r\\n                                                                                                      sequences more robust and in some more accurate bound-\\r\\nTable 12. Comparison to the state of the art of segmentation only\\r\\n                                                                                                      ing box predictions than the related methods. In particular\\r\\nmethods on VOT2020ST [35] in terms of EAO score.                                                      it achieves solid robustness for scenarios where distractors\\r\\n                                                                                                      are present but the target object is at least partially visible\\r\\n                                                                                                      and not undergoing a full occlusion.\\r\\ndecreases the performance (3rd row) because centerness of-\\r\\nten fails to identify the target among distractors (see Fig. 8).                                      B.2. Target Model Prediction\\r\\nFinally, we follow FCOS and multiply the classification and                                               Fig. 6 shows the target score maps produced by the tar-\\r\\ncenterness scores point-wise to retrieve the target object (4th                                       get model when using two different model predictors for\\r\\nrow). We conclude that omitting the centerness branch for                                             three different sequences. In detail we compare the tar-\\r\\ntraining and during inference to localize the target achieves                                         get score map produced by SuperDiMP [12] that adopts\\r\\nthe best tracking performance.                                                                        the DiMP [1] model predictor with optimized settings. In\\r\\n                                                                                                      particular it uses a slightly smaller search area factor of 6\\r\\nB. Visual Results                                                                                     instead of 5 and a target score resolution of 22 instead of\\r\\n                                                                                                      18. Note, that our tracker uses 5 and 18 similar to DiMP [1]\\r\\n   In this part we provide visual results of our tracker. First,\\r\\n                                                                                                      as stated Sec. A.2. We observe that our model predictor\\r\\nwe show three frames of different sequences where our\\r\\n                                                                                                      leads to much cleaner and unambiguous target localization\\r\\ntracker outperforms the state of the art. Secondly, we com-\\r\\n                                                                                                      than DiMP. While the former often produces multiple local\\r\\npare the produced target score map of our tracker with score\\r\\n                                                                                                      maxima for distractors, our methods is able to almost fully\\r\\nmaps obtained by optimization based model prediction. Fi-\\r\\n                                                                                                      suppress these. An important design choice that enables this\\r\\nnally, we show some failure cases of our tracker.\\r\\n                                                                                                      is the transductive model weight and test feature prediction\\r\\nB.1. Visual Comparison to the State of the Art                                                        produced by our Transformer based model predictor. How-\\r\\n                                                                                                      ever, the cleaner score maps come with the risk, that once\\r\\n   Fig. 7 shows three frames of eight different LaSOT [20]                                            the target is lost and a distractor is tracked instead recov-\\r\\nsequences where each frame contains the ground truth an-                                              ering is less likely since our tracker effectively suppresses\\r\\n                                                                                                      distractors. Similarly, our method learns to produce a score\\r\\n                         #0012                          #0586                            #0776\\r\\n                                                                                                      map containing a Gaussian such that overall the maximum\\r\\n                                                                                                      score values are higher than by SuperDiMP. Thus, we chose\\r\\n                                                                                                      a relatively high threshold to decide whether to use a previ-\\r\\n                                                                                                      ous prediction as training sample or not.\\r\\n\\r\\n                                                                                                      B.3. Failure Cases\\r\\n                         #0016                          #0146                            #0926\\r\\n\\r\\n                                                                                                         Fig. 9 shows failure cases of our tracker. In particular, it\\r\\n                                                                                                      shows three frames of four different LaSOT [20] sequences\\r\\n                                                                                                      containing the ground truth annotations and the predicted\\r\\n                                                                                                      bounding boxes of our tracker using a ResNet-101 [28] as\\r\\n                        #0003                           #0781                           #1877\\r\\n                                                                                                      backbone. To summarize, our tracker typically fails if ob-\\r\\n                                                                                                      ject similar to the targets so called distractors are present.\\r\\n                                                                                                      While the sole presence of distractors typically does not\\r\\n                                                                                                      lead to tracking failure, our tracker shows difficulties in\\r\\n                                                                                                      sequences where the target is occluded and distractors are\\r\\n                        #1218                           #1512                           #2126\\r\\n                                                                                                      present (1st and 3rd row). Instead of detecting that the target\\r\\n                                                                                                      is occluded the tracker starts to track a distractor instead.\\r\\n                                                                                                      Another challenging scenario are sequences where the tar-\\r\\n                                                                                                      get and a distractor approach each other (2nd row in Fig. 9)\\r\\n                                                                                                      or one occludes the other (4th row in Fig. 9). The model\\r\\n                                                                                                      then detects only a single object instead of two in both sce-\\r\\n                      Annotation                        ToMP-101                                      narios. Once they diverge again and the tracker detects two\\r\\n                                                                                                      objects it typically fails to reliably differentiate between the\\r\\n       Figure 9. Visualization of failure cases of our tracker.                                       target and the distractor.\\r\\n\\r\\n\\r\\n                                                                                                 15\\r\\n\\x0c\\n\\n                        100\\r\\n                                                    Success plot                                                 100\\r\\n                                                                                                                                             Success plot                                                    100\\r\\n                                                                                                                                                                                                                                          Success plot\\r\\n\\r\\n                         80                                                                                       80                                                                                          80\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                         Overlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                     Overlap Precision [%]\\r\\n                                KeepTrack [69.7]\\r\\n                                TransT [69.1]\\r\\n                         60     ToMP 50 [69.0]                                                                    60     KeepTrack [70.9]                                                                     60\\r\\n                                PrDiMP50 [68.0]                                                                          TrDiMP [70.7]                                                                                ToMP 50 [66.9]\\r\\n                                Super DiMP [67.7]                                                                        UPDT [70.4]                                                                                  ToMP 101 [66.7]\\r\\n                                TrDiMP [67.5]                                                                            Super DiMP [70.1]                                                                            KeepTrack [66.4]\\r\\n                         40     ToMP 101 [66.9]                                                                   40     ToMP 50 [70.1]                                                                       40      TrDiMP [66.2]\\r\\n                                STMTrack [65.7]                                                                          ToMP 101 [70.1]                                                                              TransT [65.3]\\r\\n                                DiMP50 [65.3]                                                                            SiamRPN++ [69.6]                                                                             Super DiMP [64.8]\\r\\n                                SiamRPN++ [65.2]                                                                         PrDiMP50 [69.6]                                                                              PrDiMP50 [63.5]\\r\\n                                ATOM [64.2]                                                                              ECO [69.1]                                                                                   DiMP50 [61.9]\\r\\n                         20     DaSiamRPN [57.7]                                                                  20     DiMP50 [68.4]                                                                        20      ATOM [58.4]\\r\\n                                UPDT [54.5]                                                                              CCOT [68.2]                                                                                  UPDT [53.6]\\r\\n                                ECO [53.2]                                                                               ATOM [66.3]                                                                                  CCOT [48.8]\\r\\n                                CCOT [51.3]                                                                              DaSiamRPN [65.8]                                                                             ECO [46.6]\\r\\n                          00        0.2             0.4          0.6      0.8        1                             00        0.2             0.4       0.6          0.8          1                             00         0.2             0.4          0.6          0.8          1\\r\\n                                               Overlap threshold                                                                        Overlap threshold                                                                            Overlap threshold\\r\\n                                          (a) UAV123 [49]                                                                          (b) OTB-100 [61]                                                                                (c) NFS [23]\\r\\n\\r\\n  Figure 10. Success plots on the UAV123 [49], OTB-100 [61] and NFS [23] datasets in terms of overall AUC score, reported in the legend.\\r\\n\\r\\n                                ToMP ToMP Keep STARK Tr                STARK                                                        Super            Pr  Siam STM             Siam           Retina FCOS\\r\\n                                 101  50 Track ST101 DiMP TransT SAOT ST50                                                          DiMP           DiMP R-CNN Track DiMP KYS RPN++ ATOM UPDT MAML MAML\\r\\n                                          [48]   [63] [59]  [7]   [73]   [63]                                                        [12]           [16]  [57] [22]  [1]  [2]  [39] [13] [3]  [58]   [58]\\r\\n           UAV123 66.9                      69.0          69.7     68.2    67.5    69.1                          69.1     –           67.7          68.0      64.9        64.7   65.3                           –       61.3        64.2        54.5          –            –\\r\\n           OTB-100 70.1                     70.1          70.9     68.1    71.1    69.4                          68.5    71.4         70.1          69.6      70.1        71.9   68.4                          69.5     69.6        66.9        70.2         71.2         70.4\\r\\n           NFS     66.7                     66.9          66.4     66.2    66.2    65.7                          65.2    65.6         64.8          63.5      63.9         –     62.0                          63.5      –          58.4        53.7          –            –\\r\\n\\r\\n                                           Auto Auto                      Siam     Siam                                                                                   Siam                                          Siam       Siam             DaSiam\\r\\n                                Ocean STN Match Track                     BAN      CAR                       ECO DCFST PG-NET CRACT                          GCT          GAT CLNet TLPG                                AttN       FC++ MDNet CCOT RPN\\r\\n                                 [70] [44] [69]  [41]                      [9]      [27]                     [14] [72]   [42]  [21]                          [25]          [26] [18] [40]                                [68]       [62] [51]  [15]  [74]\\r\\n           UAV123   –                       64.9           –       67.1    63.1    61.4                          53.2     –            –            66.4      50.8        64.6   63.3                           –       65.0         –           –           51.3         57.7\\r\\n           OTB-100 68.4                     69.3          71.4      –      69.6     –                            69.1    70.9         69.1          72.6      64.8        71.0    –                            69.8     71.2        68.3        67.8         68.2         65.8\\r\\n           NFS      –                        –             –        –      59.4     –                            46.6    64.1          –            62.5       –           –     54.3                           –        –           –          41.9         48.8          –\\r\\n\\r\\n\\r\\n                   Table 13. Comparison with state-of-the-art on the OTB-100 [61], NFS [23] and UAV123 [49] datasets in terms of overall AUC score.\\r\\n\\r\\n                               ToMP ToMP STARK Keep STARK Alpha          Siam   Tr  Super                                                                          STM         Pr                                      DM         Auto\\r\\n                                101  50  ST101 Track ST50 Refine TransT R-CNN DiMP Dimp                                                                      SAOT Track DTT DiMP                                      Track       Match TLPG                 TACT         LTMU\\r\\n                                           [63] [48]  [63] [64]    [7]    [57] [59]  [12]                                                                     [73]  [22] [67] [16]                                     [71]        [69]  [40]                 [10]         [11]\\r\\n           LaSOT 68.5                     67.6            67.1     67.1     66.4     65.3                         64.9      64.8        63.9        63.1     61.6         60.6 60.1                          59.8      58.4        58.3         58.1         57.5         57.2\\r\\n\\r\\n                                                      Siam             Siam         Siam                          PG   FCOS Global      DaSiam Siam Siam      Siam Retina Siam\\r\\n                               DiMP Ocean             AttN       CRACT FC++         GAT                           NET MAML Track ATOM RPN BAN CAR CLNet RPN++ MAML Mask ROAM++ SPLT\\r\\n                                [1]  [70]              [68]       [21]  [62]         [26]                         [42]  [58] [30]  [13]  [74]†  [9] [27] [18] [39]† [58] [60]† [66] [65]\\r\\n           LaSOT 56.9                     56.0            56.0     54.9     54.4     53.9                         53.1      52.3        52.1        51.5     51.5         51.4 50.7                          49.9      49.6        48.0         46.7         44.7         42.6\\r\\n\\r\\n\\r\\n  Table 14. Comparison with state-of-the-art on the LaSOT [20] test set in terms of overall AUC score. The symbol † marks results that were\\r\\n  produced by Fan et al. [20] otherwise they are obtained directly from the official paper.\\r\\n\\r\\n\\r\\n  C. Experiments                                                                                                                                     the target in each frame. In the main paper we compare\\r\\n                                                                                                                                                     our method with methods that produce bounding boxes.\\r\\n     We provide more detailed experiments to complement                                                                                              Thus, in addition, we compare our method on the VOT2020\\r\\n  the comparison to the state of-the art performed in the main                                                                                       short-term challenge to methods that produce a segmen-\\r\\n  paper. And provide results for the VOT2020ST [35] chal-                                                                                            tation mask in each frame. Since our method produces\\r\\n  lenge when using AlphaRefine [64] on top of our method in                                                                                          only a bounding box, we use AlphaRefine [64] that is able\\r\\n  order to compare with methods that produce a segmentation                                                                                          to produce a segmentation mask give the bounding box.\\r\\n  mask as output.                                                                                                                                    Tab. 12 shows that our method achieves competitive results.\\r\\n                                                                                                                                                     In particular ToMP-101 achieves the same EAO (for more\\r\\n  C.1. VOT2020 with AlphaRefine                                                                                                                      details on EAO we refer the reader to [35]) as STARK-\\r\\n     In contrast to previous years where the sequences in                                                                                            ST101+AR [63] that employs AlphaRefine too. Nonethe-\\r\\n  the VOT short-term challenge were annotated with bound-                                                                                            less, RPT [47] achieves higher EAO than our tracker. In\\r\\n  ing boxes [36, 37] the sequences of the more recent chal-                                                                                          particular it scores a higher robustness but a lower accuracy\\r\\n  lenges contain segmentation mask annotations [35, 38] of                                                                                           than our trackers.\\r\\n\\r\\n\\r\\n                                                                                                                                               16\\r\\n\\x0c\\n\\n                        90\\r\\n                                                   Success plot                                           90\\r\\n                                                                                                                  Normalized Precision plot\\r\\n                        80                                                                                80\\r\\n                        70                                                                                70\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                 Distance Precision [%]\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                              ToMP 101 [68.5]                                                                                                      ToMP 101 [79.2]\\r\\n                        60    ToMP 50 [67.6]                                                              60                                       ToMP 50 [78.0]\\r\\n                              KeepTrack [67.1]                                                                                                     KeepTrack [77.2]\\r\\n                        50    STARK-ST101 [67.1]                                                          50                                       STARK-ST101 [76.9]\\r\\n                              STARK-ST50 [66.4]                                                                                                    STARK-ST50 [76.3]\\r\\n                              AlphaRefine [65.9]                                                                                                   AlphaRefine [73.8]\\r\\n                        40    TransT [64.9]\\r\\n                              Siam R-CNN [64.8]\\r\\n                                                                                                          40                                       TransT [73.8]\\r\\n                                                                                                                                                   TrDiMP [73.0]\\r\\n                              TrDiMP [63.9]                                                                                                        Siam R-CNN [72.2]\\r\\n                        30    Super DiMP [63.1]                                                           30                                       Super DiMP [72.2]\\r\\n                              STMTrack [60.6]                                                                                                      STMTrack [69.3]\\r\\n                              PrDiMP50 [59.8]                                                                                                      PrDiMP50 [68.8]\\r\\n                        20    DMTrack [58.4]                                                              20                                       DMTrack [66.9]\\r\\n                              TACT [57.5]                                                                                                          LTMU [66.5]\\r\\n                        10    LTMU [57.2]                                                                 10                                       TACT [66.0]\\r\\n                              DiMP50 [56.9]                                                                                                        Ocean [65.1]\\r\\n                              Ocean [56.0]                                                                                                         DiMP50 [65.0]\\r\\n                         00         0.2            0.4           0.6   0.8   1                             00   0.1         0.2         0.3           0.4           0.5\\r\\n                                               Overlap threshold                                                      Location error threshold\\r\\n                                                   (a) Success                                                        (b) Normalized Precision\\r\\n\\r\\nFigure 11. Success and normalized precision plots on LaSOT [20]. Our approach outperforms all other methods by a large margin in AUC,\\r\\nreported in the legend.\\r\\n                        70\\r\\n                                                   Success plot                                           70\\r\\n                                                                                                                  Normalized Precision plot\\r\\n\\r\\n                        60                                                                                60\\r\\n                                                                                 Distance Precision [%]\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                        50                                                                                50\\r\\n\\r\\n                        40                                                                                40\\r\\n                                KeepTrack [48.2]                                                                                              KeepTrack [61.7]\\r\\n                        30      ToMP 101 [45.9]                                                           30                                  ToMP 101 [58.1]\\r\\n                                ToMP 50 [45.4]                                                                                                ToMP 50 [57.6]\\r\\n                                Super DiMP [43.7]                                                                                           Super DiMP [56.3]\\r\\n                        20      LTMU [41.4]                                                               20                                LTMU [53.6]\\r\\n                                DiMP [39.2]                                                                                                 DiMP [51.2]\\r\\n                        10      ATOM [37.6]                                                               10                                ATOM [49.6]\\r\\n                                DaSiamRPN [35.6]                                                                                            DaSiamRPN [48.0]\\r\\n                                SiamRPN++ [34.0]                                                                                            SiamRPN++ [45.3]\\r\\n                         00        0.2        0.4                0.6   0.8   1                             00   0.1         0.2         0.3         0.4       0.5\\r\\n                                               Overlap threshold                                                      Location error threshold\\r\\n                                                   (a) Success                                                        (b) Normalized Precision\\r\\n\\r\\nFigure 12. Success and normalized precision plots on LaSOTExtSub [19]. Our approach outperforms all other methods by a large margin\\r\\nin AUC, reported in the legend.\\r\\n\\r\\n\\r\\nC.2. UAV123, OTB-100 and NFS                                                                      Fig. 10c shows that our tracker is almost as robust as Keep-\\r\\n                                                                                                  Track [48] but achieves superior accuracy leading to a new\\r\\n   To complement the results detailed in the paper, we                                            state of the art. While we reported only the methods with\\r\\nprovide the success plots for the UAV123 [49] dataset in                                          the highest performances on these datasets in the main pa-\\r\\nFig. 10a, the OTB-100 [61] dataset in Fig. 10b and the                                            per, we compare our method in Tab. 13 with additional re-\\r\\nNFS [23] dataset in Fig. 10c. Fig. 10a shows that Keep-                                           lated methods.\\r\\nTrack [48] and PrDiMP50 [16] achieve higher robustness\\r\\n                                                                                                  C.3. LaSOT and LaSOTExtSub\\r\\nthan our tracker (T < 0.6) but that our trackers together\\r\\nwith TransT [7] reaches the highest accuracy among all                                               In addition to the success plots, we provide the normal-\\r\\ntrackers (T > 0.7) compensating for the lower robustness.                                         ized precision plots on the LaSOT [20] test set in Fig. 11\\r\\nFig. 10b reveals similar conclusions on OTB-100. For NFS                                          the LaSOTExtSub [19] test set in Fig. 12. The normalized\\r\\n\\r\\n\\r\\n                                                                                 17\\r\\n\\x0c\\n\\n               Illumination Partial             Motion Camera        Background Viewpoint Scale     Full     Fast                Low          Aspect\\r\\n                 Variation Occlusion Deformation Blur Motion Rotation Clutter    Change Variation Occlusion Motion Out-of-View Resolution Ration Change   Total\\r\\n LTMU             56.5      54.0       57.2     55.8    61.6   55.1      49.9        56.7    57.1     49.9    44.0      52.7      51.4        55.1        57.2\\r\\n PrDiMP50         63.7      56.9       60.8     57.9    64.2   58.1      54.3        59.2    59.4     51.3    48.4      55.3      53.5        58.6        59.8\\r\\n STMTrack         65.2      57.1       64.0     55.3    63.3   60.1      54.1        58.2    60.6     47.8    42.4      51.9      50.3        58.8        60.6\\r\\n SuperDiMP        67.8      59.7       63.4     62.0    68.0   61.4      57.3        63.4    62.9     54.1    50.7      59.0      56.4        61.6        63.1\\r\\n TrDiMP           67.5      61.1       64.4     62.4    68.1   62.4      58.9        62.8    63.4     56.4    53.0      60.7      58.1        62.3        63.9\\r\\n Siam R-CNN       64.6      62.2       65.2     63.1    68.2   64.1      54.2        65.3    64.5     55.3    51.5      62.2      57.1        63.4        64.8\\r\\n TransT           65.2      62.0       67.0     63.0    67.2   64.3      57.9        61.7    64.6     55.3    51.0      58.2      56.4        63.2        64.9\\r\\n AlphaRefine      69.4      62.3       66.3     65.2    70.0   63.9      58.8        63.1    65.4     57.4    53.6      61.1      58.6        64.1        65.3\\r\\n STARK-ST50       66.8      64.3       66.9     62.9    69.0   66.1      57.3        67.8    66.1     58.7    53.8      62.1      59.4        64.9        66.4\\r\\n STARK-ST101      67.5      65.1       68.3     64.5    69.5   66.6      57.4        68.8    66.8     58.9    54.2      63.3      59.6        65.6        67.1\\r\\n KeepTrack        69.7      64.1       67.0     66.7    71.0   65.3      61.2        66.9    66.8     60.1    57.7      64.1      62.0        65.9        67.1\\r\\n ToMP-50          66.8      64.9       68.5     64.6    70.2   67.3      59.1        67.2    67.5     59.3    56.1      63.7      61.1        66.5        67.6\\r\\n ToMP-101         69.0      65.3       69.4     65.2    71.7   67.8      61.5        69.2    68.4     59.1    57.9      64.1      62.5        67.2        68.5\\r\\n\\r\\n\\r\\nTable 15. LaSOT [20] attribute-based analysis. Each column corresponds to the results computed on all sequences in the dataset with the\\r\\ncorresponding attribute.\\r\\n\\r\\n\\r\\nprecision score NPrD measures the percentage of frames\\r\\nwhere the normalized distance (relative to the target size)\\r\\nbetween the predicted and ground-truth target center loca-\\r\\ntion is less than a threshold D ∈ [0, 0.5]. The ranking is de-\\r\\ntermined by computing the AUC of each tracker. The AUC\\r\\nis reported in the legend of Figs. 11b and 12b. We compare\\r\\nour tracker on LaSOT with the state of the art in Tab. 14 and\\r\\nshow their performance if available in Fig. 11. In Fig. 12 we\\r\\nshow results of methods produced by Fan et al. [19] except\\r\\nKeepTrack [48] and SuperDiMP [12] that we obtained from\\r\\nMayer et al. [48].\\r\\n\\r\\nC.3.1   Attributes\\r\\nTo support the attribute based analysis in the main paper,\\r\\nwhere we compared the performance of our tracker with\\r\\nother Transformer based trackers, we provide the detailed\\r\\nanalysis for multiple trackers and ToMP in Tab. 15. ToMP-\\r\\n101 achieves the best performance on all but three. It\\r\\nachieves the second best results for Motion Blur behind\\r\\nKeepTrack [48] and similar to AlphaRefine [64]. Further\\r\\nToMP-101 achieves the third best for Full Occlusion be-\\r\\nhind KeepTrack [48] and ToMP-50. Similarly it scores\\r\\nthird for Illumination Variation behind KeepTrack [48] and\\r\\nAlphaRefine [64]. We further observe, that discrimina-\\r\\ntive model prediction based methods such as TrDiMP [59],\\r\\nSuperDiMP [12], AlphaRefine [64], KeepTrack [48] and\\r\\nToMP all outperform STARK [63] on the attribute Back-\\r\\nground Clutter showing the advantage of using full training\\r\\nsamples during tracking instead of cropped templates that\\r\\nmainly cover the centered target.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                18\\r\\n\\x0c',\n",
       " '                                               Robust Visual Tracking by Segmentation\\r\\n\\r\\n                                             Matthieu Paul, Martin Danelljan, Christoph Mayer, and Luc Van Gool\\r\\n\\r\\n                                                         Computer Vision Lab, ETH Zürich, Switzerland\\r\\n                                                   {paulma, damartin, chmayer, vangool}@vision.ee.ethz.ch\\r\\narXiv:2203.11191v1 [cs.CV] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                               Abstract. Estimating the target extent poses a fundamental challenge\\r\\n                                               in visual object tracking. Typically, trackers are box-centric and fully rely\\r\\n                                               on a bounding box to define the target in the scene. In practice, objects\\r\\n                                               often have complex shapes and are not aligned with the image axis. In\\r\\n                                               these cases, bounding boxes do not provide an accurate description of\\r\\n                                               the target and often contain a majority of background pixels.\\r\\n                                               We propose a segmentation-centric tracking pipeline that not only pro-\\r\\n                                               duces a highly accurate segmentation mask, but also works internally\\r\\n                                               with segmentation masks instead of bounding boxes. Thus, our tracker\\r\\n                                               is able to better learn a target representation that clearly differentiates\\r\\n                                               the target in the scene from background content. In order to achieve\\r\\n                                               the necessary robustness for the challenging tracking scenario, we pro-\\r\\n                                               pose a separate instance localization component that is used to condition\\r\\n                                               the segmentation decoder when producing the output mask. We infer a\\r\\n                                               bounding box from the segmentation mask and validate our tracker on\\r\\n                                               challenging tracking datasets and achieve the new state of the art on\\r\\n                                               LaSOT [16] with a success AUC score of 69.7%. Since fully evaluating\\r\\n                                               the predicted masks on tracking datasets is not possible due to the miss-\\r\\n                                               ing mask annotations, we further validate our segmentation quality on\\r\\n                                               two popular video object segmentation datasets. The code and trained\\r\\n                                               models are available at https://github.com/visionml/pytracking.\\r\\n\\r\\n\\r\\n                                         1   Introduction\\r\\n                                         Visual object tracking is the task of estimating the state of a target object for\\r\\n                                         each frame in a video sequence. The target is solely characterized by its initial\\r\\n                                         state in the video. Current approaches predominately characterize the state itself\\r\\n                                         with a bounding box. However, this only gives a very coarse representation of\\r\\n                                         the target’s state in the image. In practice, objects often have complex shapes,\\r\\n                                         undergo substantial deformations, can be highly elongated, or simply not align\\r\\n                                         well with the image axes. In such cases, the majority of the image content inside\\r\\n                                         the target’s bounding box often consists of background regions, thus providing\\r\\n                                         very limited information about the object itself. In contrast, a segmentation mask\\r\\n                                         gives a precise characterization of the object’s extent in the image (see Fig. 1\\r\\n                                         frames #1600 and #3200). Such information is vital in a variety of applications,\\r\\n                                         including video analysis, video editing, and robotics. In this work, we therefore\\r\\n                                         aim to develop an approach capable of accurately and robustly segmenting the\\r\\n                                         target object, even in the highly challenging tracking datasets [16,35].\\r\\n\\x0c\\n\\n2       Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n        # 0000    # 1600     # 3200           # 0000           # 0700           # 1400\\r\\n\\r\\n\\r\\n\\r\\nSTARK\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      STARK\\r\\n        # 0000    # 1600     # 3200           # 0000           # 0700           # 1400\\r\\nLWL\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      LWL\\r\\n        # 0000    # 1600     # 3200           # 0000           # 0700           # 1400\\r\\nRTS\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      RTS\\r\\nFig. 1. Comparison between the VOT method Stark [54], the VOS method LWL [5]\\r\\nand our proposed method on two tracking sequences from the LaSOT [16] dataset.\\r\\nThe ground-truth annotation (\\x97) is shown in each frame for reference. Our approach\\r\\nis more robust and predicts a more accurate target representation.\\r\\n\\r\\n    While severely limiting the information achieved about the target’s state\\r\\nin the video, the aforementioned issues with the bounding box representation\\r\\ncan itself lead to inaccurate bounding box predictions, or even tracking failure.\\r\\nFig. 1 shows two typical tracking sequences. The tracking method STARK [54]\\r\\n(first row) fails to regress bounding boxes that contain the entire object (#1600,\\r\\n#1400) or even starts tracking the wrong object (#0700). Conversely, segmenta-\\r\\ntion masks are a better fit to define the target object in a scene because only pix-\\r\\nels corresponding to the target are marked as foreground. Thus, a segmentation-\\r\\ncentric tracking architecture that is designed to work internally with a segmen-\\r\\ntation mask of the target instead of a bounding box has the potential to learn\\r\\nbetter target representations because it can clearly differentiate background from\\r\\nforeground regions in the scene.\\r\\n    A few recent tracking methods [46,53] have recognized the advantage of pro-\\r\\nducing a segmentation mask instead of a bounding box as tracking output. How-\\r\\never, these trackers are typically bounding box-centric and the final segmentation\\r\\nmask is obtained by a separate box-to-mask post-processing network. Thus, these\\r\\nmethods miss the opportunity to leverage the accurate target definition of seg-\\r\\nmentation masks to learn a more accurate and robust internal representation\\r\\nof the target. In contrast, most video object segmentation method [37,5] fol-\\r\\nlow a segmentation-centric paradigm. However, these methods are not designed\\r\\nfor the challenging tracking scenarios. Typical VOS sequences consist only of a\\r\\nfew hundred frames [40] whereas multiple tracking sequence of more than ten\\r\\nthousand frames exist in tracking datasets [16]. Due to this setup, VOS meth-\\r\\nods focus on producing highly accurate segmentation masks but are sensitive to\\r\\ndistractors, substantial deformations and occlusions of the target object. Fig. 1\\r\\nshows two typical tracking sequences where the VOS method LWL [5] (second\\r\\nrow) produces a fine-grained segmentation mask of the wrong object (#3200) or\\r\\nis unable to detect only the target within a crowd (#0700, #1400).\\r\\n    We propose RTS, a unified tracking architecture capable of predicting accu-\\r\\nrate segmentation masks. To design a segmentation-centric approach, we take\\r\\ninspiration from the VOS method LWL [5]. However, to achieve robust and ac-\\r\\ncurate segmentation on tracking datasets, we propose several new components.\\r\\n\\x0c\\n\\n                                   Robust Visual Tracking by Segmentation        3\\r\\n\\r\\nIn particular, we propose an instance localization branch that is trained to pre-\\r\\ndict a target appearance model, which allows the detection of occlusions and to\\r\\nidentify the correct target even in cluttered scenes. The output of the instance\\r\\nlocalization branch is further used to condition the high dimensional mask en-\\r\\ncoding. This enables the segmentation decoder to focus on the localized target,\\r\\nleading to a more robust mask prediction. Since, our proposed method contains\\r\\na segmentation and instance memory that needs to be updated with previous\\r\\ntracking results, we design a memory management module. This module first\\r\\nassesses the prediction quality, decides whether the sample should enter into the\\r\\nmemory and triggers the tracking model if it should be updated.\\r\\nContributions Our contributions are the following: (i) We propose a unified\\r\\ntracking architecture capable of predicting robust classification scores and ac-\\r\\ncurate segmentation masks. We design separate feature spaces and memories to\\r\\nensure optimal receptive fields and update-rates for segmentation and instance\\r\\nlocalization. (ii) To produce a segmentation mask which also agrees with the\\r\\ninstance prediction, we design a fusion mechanism that further conditions the\\r\\nsegmentation decoder on the instance localization output and leads to more ro-\\r\\nbust tracking performance. (iii) We introduce an effective inference procedure\\r\\ncapable of fusing the instance localization output and mask encoding to ensure\\r\\nboth robust and accurate tracking. (iv) We perform comprehensive evaluation\\r\\nand ablation studies of the proposed tracking pipeline on multiple popular track-\\r\\ning benchmarks. Our approach achieves the new state of the art on LaSOT with\\r\\nan area-under-the-curve (AUC) score of 69.7%.\\r\\n\\r\\n\\r\\n2   Related Work\\r\\nIn this section we will revisit recent tracking and video object segmentation\\r\\nmethods and particularly focus on existing tracking methods that produce a\\r\\nsegmentation mask as output.\\r\\nVisual Object Tracking Over the years many new challenging tracking bench-\\r\\nmarks such as LaSOT [16], GOT-10k [24], and TrackingNet [36] have been pro-\\r\\nposed that accelerated the tracking research. In particular, Discriminative Corre-\\r\\nlation Filter (DCF) based, Siamese and more recently transformer based trackers\\r\\nare the most dominant paradigms in visual object tracking.\\r\\n    Visual trackers based on DCFs [6,22,15,32,12,47,60,3,14] are very popular.\\r\\nThese methods essentially solve an optimization problem to estimate the weights\\r\\nof the DCF that allow to distinguish foreground from background regions. The\\r\\nDCF is often referred to as the target appearance model and allows to lo-\\r\\ncalize the target in the video frame. More recent DCF approaches [3,14] en-\\r\\nable end-to-end training by unrolling a fixed number of the optimization itera-\\r\\ntions during offline training. Siamese tracking methods have become more and\\r\\nmore popular because they are typically end-to-end trainable, simple and fast\\r\\n[43,2,42,61,20,49,21,29,28]. These trackers aim at learning a similarity metric us-\\r\\ning only the initial video frame and its annotation that allows to clearly identify\\r\\nthe target offline. Since no online learning component is involved, these trackers\\r\\n\\x0c\\n\\n4      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\nachieve high frame rates at the cost of limited online adaptability to changes\\r\\nof the target’s appearance. Nonetheless, several methods have been proposed\\r\\nto overcome these issues [43,2,29,28]. The very recently proposed transformer\\r\\nbased trackers achieve state of the art performance on many datasets often out-\\r\\nperforming DCF or Siamese based trackers. This group of trackers typically uses\\r\\na Transformer component in order to fuse information extracted from train-\\r\\ning and test frames to produce discriminative features that allow to accurately\\r\\nlocalize and estimate the target in the scene [8,55,54,48].\\r\\nVideo Object Segmentation Semi-supervised VOS is the task of classifying\\r\\nall pixels belonging to the target in each video frame, given only the segmen-\\r\\ntation mask of the target in the initial frame. The cost of annotating accu-\\r\\nrate segmentation masks is limiting the sequence length and number of videos\\r\\ncontained in available VOS datasets. Despite the relatively small size of VOS\\r\\ndatasets compared to other computer vision problems, new benchmarks such as\\r\\nYoutube-VOS [52] and DAVIS [40] accelerated the research progress in the last\\r\\nyears. A group of methods relies on a learnt target detector [7,45,33] whereas an-\\r\\nother learns how to propagate the segmentation mask across frames [51,39,30,25].\\r\\nOther methods use feature matching techniques across one or multiple frames\\r\\nwith or without using an explicit spatio-temporal memory [9,23,44,37]. Another\\r\\nmethod employs meta-learning to tackle VOS [5]. Specifically, Bhat et al. [5] in-\\r\\ntroduce an end-to-end trainable VOS architecture employing a few-shot learner\\r\\nthat predicts a learnable labels encoding. In particular, the few-shot learner gen-\\r\\nerates and updates online the parameters of a segmentation target model that\\r\\nproduces the mask encoding used to generate the final segmentation mask.\\r\\nJoint Visual Tracking and Segmentation A group of tracking methods have\\r\\nalready identified the advantages of predicting a segmentation mask instead of a\\r\\nbounding box [53,59,46,31,50,41]. Siam-RCNN is a box-centric tracker that used\\r\\na pretrained box2seg network to predict the segmentation mask given a bound-\\r\\ning box prediction. In contrast, AlphaRefine represents a novel box2seg method\\r\\nthat has been evaluated with many recent trackers such as SuperDiMP [14]\\r\\nor SiamRPN++ [28]. Further, Zhao et al. [59] focus on generating segmentation\\r\\nmasks from bounding box annotations in videos using a spatio-temporal aggrega-\\r\\ntion module to mine consistencies of the scene across multiple frames. Conversely,\\r\\nSiamMask [50] and D3S [31] are segmentation-centric trackers that produce di-\\r\\nrectly a segmentation output mask without employing a box2seg module. In\\r\\nparticular, SiamMask [50] consists of a fully-convolutional Siamese network that\\r\\nemploys a separate branch to predict binary segmentation masks supervised via\\r\\na segmentation loss. From a high-level view the single-shot segmentation tracker\\r\\nD3S [31] is most related to our proposed method. Both methods employ two\\r\\ndedicated modules or branches; one for localization and one for segmentation.\\r\\nWhereas D3S adopts the target classification component of ATOM [12] that re-\\r\\nquires to optimize online the weights of a two-layer CNN, we simply learn online\\r\\nthe weights of a DCF similar to DiMP [3]. For segmentation, they propose a\\r\\nfeature matching technique that matches test frame features with background\\r\\nand foreground features corresponding to the initial frame. In contrast, we adopt\\r\\n\\x0c\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Robust Visual Tracking by Segmentation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5\\r\\n\\r\\n               Segmentation Memory                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Update\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Signal\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Label\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Encoder E✓\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      <latexit sha1_base64=\"PlAH+J4KN+KlAUk6ay2VBYkA7lQ=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgqiOCxgv2ANpTNdtIu3WzC7kQooX/CgxcVr/4dj/4bt20O2vpg4PHeDDPzgkQKg6777RTW1jc2t4rbpZ3dvf2D8uFRy8Sp5tDksYx1J2AGpFDQRIESOokGFgUS2sH4dua3n0AbEatHnCTgR2yoRCg4Qyt17vo9HAGyfrniVt056CrxclIhORr98ldvEPM0AoVcMmO6npugnzGNgkuYlnqpgYTxMRtC11LFIjB+Nr93Ss+sMqBhrG0ppHP190TGImMmUWA7I4Yjs+zNxP+8borhtZ8JlaQIii8WhamkGNPZ83QgNHCUE0sY18LeSvmIacbRRmQz8JY/XiWti6p3Wa091Cr1mzyNIjkhp+SceOSK1Mk9aZAm4USSZ/JK3pzEeXHenY9Fa8HJZ47JHzifP2Wcj9E=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSegmentation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Few-Shot\\r\\n   Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Learner A✓\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <latexit sha1_base64=\"SX+8urpQwwtaDfbK8LPe/AglH3Q=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHiqePFYwX5AG8pmO2mXbjZhdyKU0D/hwYuKV/+OR/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZeJUc2jyWMa6EzADUihookAJnUQDiwIJ7WB8N/PbT6CNiNUjThLwIzZUIhScoZU6t/0ejgBZv1xxq+4cdJV4OamQHI1++as3iHkagUIumTFdz03Qz5hGwSVMS73UQML4mA2ha6liERg/m987pWdWGdAw1rYU0rn6eyJjkTGTKLCdEcORWfZm4n9eN8Xw2s+ESlIExReLwlRSjOnseToQGjjKiSWMa2FvpXzENONoI7IZeMsfr5LWRdW7rNYeapX6TZ5GkZyQU3JOPHJF6uSeNEiTcCLJM3klb07ivDjvzseiteDkM8fkD5zPH190j80=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  Updater                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Weight\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Predictor W✓\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     <latexit sha1_base64=\"0ZbxCC53UdelsGaHgMYGUGmajTs=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69BIvgqSQiKp4KXjxWsB/QhrLZTtqlm03YnQil9E948KLi1b/j0X/jts1BWx8MPN6bYWZemEphyPO+ncLa+sbmVnG7tLO7t39QPjxqmiTTHBs8kYluh8ygFAobJEhiO9XI4lBiKxzdzfzWE2ojEvVI4xSDmA2UiARnZKV2q9elIRLrlSte1ZvDXSV+TiqQo94rf3X7Cc9iVMQlM6bjeykFE6ZJcInTUjczmDI+YgPsWKpYjCaYzO+dumdW6btRom0pcufq74kJi40Zx6HtjBkNzbI3E//zOhlFN8FEqDQjVHyxKMqkS4k7e97tC42c5NgSxrWwt7p8yDTjZCOyGfjLH6+S5kXVv6pePlxWard5GkU4gVM4Bx+uoQb3UIcGcJDwDK/w5qTOi/PufCxaC04+cwx/4Hz+AIFQj+M=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ds\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  <latexit sha1_base64=\"k6nH96m/cMqClDvpNU67v0HYhcg=\">AAAB/HicbVDLSsNAFJ3UV62v+Ni5CRbBVUmkqLgq6MJlBfuAJoTJdNoOnUzCzI1YQ/wWF25U3PofLv0bJ20W2npg4HDOvdwzJ4g5U2Db30ZpaXllda28XtnY3NreMXf32ipKJKEtEvFIdgOsKGeCtoABp91YUhwGnHaC8VXud+6pVCwSdzCJqRfioWADRjBoyTcP3BDDiGCeXme+C/QBUpX5ZtWu2VNYi8QpSBUVaPrml9uPSBJSAYRjpXqOHYOXYgmMcJpV3ETRGJMxHtKepgKHVHnpNH1mHWulbw0iqZ8Aa6r+3khxqNQkDPRknlXNe7n4n9dLYHDhpUzECVBBZocGCbcgsvIqrD6TlACfaIKJZDqrRUZYYgK6MN2BM//jRdI+rTlntfptvdq4LNooo0N0hE6Qg85RA92gJmohgh7RM3pFb8aT8WK8Gx+z0ZJR7OyjPzA+fwDXL5XB</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ⌧\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <latexit sha1_base64=\"CEjhwhrnOB4cECOwBNgXaMXzlK4=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1JMUvHisYD+gDWWz3bRLN5uwOxFK6F/w4EXFq3/Io//GTZuDtj4YeLw3w8y8IJHCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7nK/88S1EbF6xGnC/YiOlAgFo5hLfaTpoFpz6+4cZJV4BalBgeag+tUfxiyNuEImqTE9z03Qz6hGwSSfVfqp4QllEzriPUsVjbjxs/mtM3JmlSEJY21LIZmrvycyGhkzjQLbGVEcm2UvF//zeimGN34mVJIiV2yxKEwlwZjkj5Oh0JyhnFpCmRb2VsLGVFOGNh6bgbf88SppX9S9q/rlw2WtcVukUYYTOIVz8OAaGnAPTWgBgzE8wyu8OdJ5cd6dj0VrySlmjuEPnM8fntGOPQ==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               xs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      xm\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <latexit sha1_base64=\"iS2FGbLVU7HB5ez4hFdk3GURAhc=\">AAAB8HicbVDLSgNBEJyNrxhfUY9eBoPgKexKUPEU8OIxgnlgsoTZSScZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7glgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJ5lDnkYx0K2AGpFBQR4ESWrEGFgYSmsHoduY3n0AbEakHnMTgh2ygRF9whlZ6HHc7CGNMw2m3WHLL7hx0lXgZKZEMtW7xq9OLeBKCQi6ZMW3PjdFPmUbBJUwLncRAzPiIDaBtqWIhGD+dXzylZ1bp0X6kbSmkc/X3RMpCYyZhYDtDhkOz7M3E/7x2gv1rPxUqThAUXyzqJ5JiRGfv057QwFFOLGFcC3sr5UOmGUcbks3AW/54lTQuyt5luXJfKVVvsjTy5IScknPikStSJXekRuqEE0WeySt5c7Tz4rw7H4vWnJPNHJM/cD5/AKbIkSw=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                xf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ss\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Decoder D✓\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <latexit sha1_base64=\"qJZdQQ7DYCOLM7QkbS8IW2EfdxQ=\">AAAB8HicbVDLSgNBEJyNrxhfUY9eBoPgKexKUPEU8OIxgnlgsoTZSScZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7glgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJ5lDnkYx0K2AGpFBQR4ESWrEGFgYSmsHoduY3n0AbEakHnMTgh2ygRF9whlZ6HHc7CGNMzbRbLLlldw66SryMlEiGWrf41elFPAlBIZfMmLbnxuinTKPgEqaFTmIgZnzEBtC2VLEQjJ/OL57SM6v0aD/SthTSufp7ImWhMZMwsJ0hw6FZ9mbif147wf61nwoVJwiKLxb1E0kxorP3aU9o4CgnljCuhb2V8iHTjKMNyWbgLX+8ShoXZe+yXLmvlKo3WRp5ckJOyTnxyBWpkjtSI3XCiSLP5JW8Odp5cd6dj0VrzslmjskfOJ8/r+CRMg==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <latexit sha1_base64=\"DYVMID1Su0tImuST/hWLppZAgSo=\">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKexKUPEU8OIxgnlgEsLsZDYZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7/FgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJZrzOIhnplk8Nl0LxOgqUvBVrTkNf8qY/up35zSeujYjUA05i3g3pQIlAMIpWehz3OsjHmAbTXrHklt05yCrxMlKCDLVe8avTj1gScoVMUmPanhtjN6UaBZN8WugkhseUjeiAty1VNOSmm84vnpIzq/RJEGlbCslc/T2R0tCYSejbzpDi0Cx7M/E/r51gcN1NhYoT5IotFgWJJBiR2fukLzRnKCeWUKaFvZWwIdWUoQ3JZuAtf7xKGhdl77Jcua+UqjdZGnk4gVM4Bw+uoAp3UIM6MFDwDK/w5mjnxXl3PhatOSebOYY/cD5/AJwskSU=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <latexit sha1_base64=\"2LUbZ/Tung/IOwUZeZXeZHm3IvM=\">AAAB8HicbVBNS8NAEN3Ur1q/qh69LBbBU0lEVDwVvHisYD+wDWWznbRLN5uwOxFL6L/w4EXFq//Go//GbZuDtj4YeLw3w8y8IJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqOTR4LGPdDpgBKRQ0UKCEdqKBRYGEVjC6mfqtR9BGxOoexwn4ERsoEQrO0EoPptdFeMLMTHrlilt1Z6DLxMtJheSo98pf3X7M0wgUcsmM6Xhugn7GNAouYVLqpgYSxkdsAB1LFYvA+Nns4gk9sUqfhrG2pZDO1N8TGYuMGUeB7YwYDs2iNxX/8zophld+JlSSIig+XxSmkmJMp+/TvtDAUY4tYVwLeyvlQ6YZRxuSzcBb/HiZNM+q3kX1/O68UrvO0yiSI3JMTolHLkmN3JI6aRBOFHkmr+TN0c6L8+58zFsLTj5zSP7A+fwBqCSRLQ==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n Test Frame                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         F✓\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <latexit sha1_base64=\"IsOXO/arn2AU5sGT/2DQTPMaZ9w=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgqCOKxgv2ANpTNdtIu3WzC7kQooX/CgxcVr/4dj/4bt20O2vpg4PHeDDPzgkQKg6777RTW1jc2t4rbpZ3dvf2D8uFRy8Sp5tDksYx1J2AGpFDQRIESOokGFgUS2sH4dua3n0AbEatHnCTgR2yoRCg4Qyt17vo9HAGyfrniVt056CrxclIhORr98ldvEPM0AoVcMmO6npugnzGNgkuYlnqpgYTxMRtC11LFIjB+Nr93Ss+sMqBhrG0ppHP190TGImMmUWA7I4Yjs+zNxP+8borhtZ8JlaQIii8WhamkGNPZ83QgNHCUE0sY18LeSvmIacbRRmQz8JY/XiWti6p3Wa091Cr1mzyNIjkhp+SceOSK1Mk9aZAm4USSZ/JK3pzEeXHenY9Fa8HJZ47JHzifP2cmj9I=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      <latexit sha1_base64=\"UNy6gYkK56vySrLyPmHP5RT3wps=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgq6MFjBfsBbSib7aRdutmE3YlQQv+EBy8qXv07Hv03btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzaHJYxnrTsAMSKGgiQIldBINLAoktIPx7cxvP4E2IlaPOEnAj9hQiVBwhlbq3PV7OAJk/XLFrbpz0FXi5aRCcjT65a/eIOZpBAq5ZMZ0PTdBP2MaBZcwLfVSAwnjYzaErqWKRWD8bH7vlJ5ZZUDDWNtSSOfq74mMRcZMosB2RgxHZtmbif953RTDaz8TKkkRFF8sClNJMaaz5+lAaOAoJ5YwroW9lfIR04yjjchm4C1/vEpaF1Xvslp7qFXqN3kaRXJCTsk58cgVqZN70iBNwokkz+SVvDmJ8+K8Ox+L1oKTzxyTP3A+fwBkEo/Q</latexit>\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             xb                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Segmentation\\r\\n                Backbone B✓                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 +\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <latexit sha1_base64=\"9HMnH5MJiZFydCxhN8g/AxeA0Lo=\">AAAB8HicbVDLSgNBEJyNrxhfUY9eBoPgKexKUPEU8OIxgnlgsoTZSScZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7glgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJ5lDnkYx0K2AGpFBQR4ESWrEGFgYSmsHoduY3n0AbEakHnMTgh2ygRF9whlZ6HHc7CGNMg2m3WHLL7hx0lXgZKZEMtW7xq9OLeBKCQi6ZMW3PjdFPmUbBJUwLncRAzPiIDaBtqWIhGD+dXzylZ1bp0X6kbSmkc/X3RMpCYyZhYDtDhkOz7M3E/7x2gv1rPxUqThAUXyzqJ5JiRGfv057QwFFOLGFcC3sr5UOmGUcbks3AW/54lTQuyt5luXJfKVVvsjTy5IScknPikStSJXekRuqEE0WeySt5c7Tz4rw7H4vWnJPNHJM/cD5/AJYckSE=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Model T⌧\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <latexit sha1_base64=\"xKhTZkes5gD1IVF59I2pQ/hdWu0=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqHgqePFYoa2FNpTNdtOu3WzC7kQoof/BgxcVr/4fj/4bt20O2vpg4PHeDDPzgkQKg6777RTW1jc2t4rbpZ3dvf2D8uFR28SpZrzFYhnrTkANl0LxFgqUvJNoTqNA8odgfDvzH564NiJWTZwk3I/oUIlQMIpWajf7PaRpv1xxq+4cZJV4OalAjka//NUbxCyNuEImqTFdz03Qz6hGwSSflnqp4QllYzrkXUsVjbjxs/m1U3JmlQEJY21LIZmrvycyGhkziQLbGVEcmWVvJv7ndVMMr/1MqCRFrthiUZhKgjGZvU4GQnOGcmIJZVrYWwkbUU0Z2oBsBt7yx6ukfVH1Lqu1+1qlfpOnUYQTOIVz8OAK6nAHDWgBg0d4hld4c5Tz4rw7H4vWgpPPHMMfOJ8/+TmPAA==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Mask Output\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      H✓\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      <latexit sha1_base64=\"f0ZPzGZiJxncG2gTuzU4I2EYWEI=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqHgqeOmxgv2ANpTNdtMu3WzC7kQooX/CgxcVr/4dj/4bt20O2vpg4PHeDDPzgkQKg6777RQ2Nre2d4q7pb39g8Oj8vFJ28SpZrzFYhnrbkANl0LxFgqUvJtoTqNA8k4wuZ/7nSeujYjVI04T7kd0pEQoGEUrdRuDPo450kG54lbdBcg68XJSgRzNQfmrP4xZGnGFTFJjep6boJ9RjYJJPiv1U8MTyiZ0xHuWKhpx42eLe2fkwipDEsbalkKyUH9PZDQyZhoFtjOiODar3lz8z+ulGN76mVBJilyx5aIwlQRjMn+eDIXmDOXUEsq0sLcSNqaaMrQR2Qy81Y/XSfuq6l1Xaw+1Sv0uT6MIZ3AOl+DBDdShAU1oAQMJz/AKb07ivDjvzseyteDkM6fwB87nD2o6j9Q=</latexit>\\r\\n                 <latexit sha1_base64=\"waH8la2opelqfIRL3tJJyWmgWiU=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgqevFYwX5AG8pmO2mXbjZhdyKU0D/hwYuKV/+OR/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZeJUc2jyWMa6EzADUihookAJnUQDiwIJ7WB8N/PbT6CNiNUjThLwIzZUIhScoZU6t/0ejgBZv1xxq+4cdJV4OamQHI1++as3iHkagUIumTFdz03Qz5hGwSVMS73UQML4mA2ha6liERg/m987pWdWGdAw1rYU0rn6eyJjkTGTKLCdEcORWfZm4n9eN8Xw2s+ESlIExReLwlRSjOnseToQGjjKiSWMa2FvpXzENONoI7IZeMsfr5LWRdW7rNYeapX6TZ5GkZyQU3JOPHJF6uSeNEiTcCLJM3klb07ivDjvzseiteDkM8fkD5zPH2D+j84=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Generation\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          xc\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <latexit sha1_base64=\"ENutjdbeR27zYGiwwbUE1g4Iji0=\">AAAB8HicbVDLSgNBEJyNrxhfUY9eBoPgKexKUPEU8OIxgnlgsoTZSScZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7glgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJ5lDnkYx0K2AGpFBQR4ESWrEGFgYSmsHoduY3n0AbEakHnMTgh2ygRF9whlZ6HHc7CGNM+bRbLLlldw66SryMlEiGWrf41elFPAlBIZfMmLbnxuinTKPgEqaFTmIgZnzEBtC2VLEQjJ/OL57SM6v0aD/SthTSufp7ImWhMZMwsJ0hw6FZ9mbif147wf61nwoVJwiKLxb1E0kxorP3aU9o4CgnljCuhb2V8iHTjKMNyWbgLX+8ShoXZe+yXLmvlKo3WRp5ckJOyTnxyBWpkjtSI3XCiSLP5JW8Odp5cd6dj0VrzslmjskfOJ8/l6CRIg==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   G✓\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   <latexit sha1_base64=\"UaesNKwLEajqVI0tpttUbVk/IB4=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgqeNBjBfsBbSib7aRdutmE3YlQQv+EBy8qXv07Hv03btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzaHJYxnrTsAMSKGgiQIldBINLAoktIPx7cxvP4E2IlaPOEnAj9hQiVBwhlbq3PV7OAJk/XLFrbpz0FXi5aRCcjT65a/eIOZpBAq5ZMZ0PTdBP2MaBZcwLfVSAwnjYzaErqWKRWD8bH7vlJ5ZZUDDWNtSSOfq74mMRcZMosB2RgxHZtmbif953RTDaz8TKkkRFF8sClNJMaaz5+lAaOAoJ5YwroW9lfIR04yjjchm4C1/vEpaF1Xvslp7qFXqN3kaRXJCTsk58cgVqZN70iBNwokkz+SVvDmJ8+K8Ox+L1oKTzxyTP3A+fwBosI/T</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Instance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       sc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   <latexit sha1_base64=\"UybyA/02kICKOl+UyCujOjkaa4c=\">AAAB8HicbVBNS8NAEN3Ur1q/qh69LBbBU0lEVDwVvHisYD+wDWWznbRLN5uwOxFL6L/w4EXFq//Go//GbZuDtj4YeLw3w8y8IJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqOTR4LGPdDpgBKRQ0UKCEdqKBRYGEVjC6mfqtR9BGxOoexwn4ERsoEQrO0EoPptdFeMKMT3rlilt1Z6DLxMtJheSo98pf3X7M0wgUcsmM6Xhugn7GNAouYVLqpgYSxkdsAB1LFYvA+Nns4gk9sUqfhrG2pZDO1N8TGYuMGUeB7YwYDs2iNxX/8zophld+JlSSIig+XxSmkmJMp+/TvtDAUY4tYVwLeyvlQ6YZRxuSzcBb/HiZNM+q3kX1/O68UrvO0yiSI3JMTolHLkmN3JI6aRBOFHkmr+TN0c6L8+58zFsLTj5zSP7A+fwBj+SRHQ==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Model T\\uf8ff\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <latexit sha1_base64=\"sP2wB1f7zTzX/PAGw75Ko8vGgfc=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69BIvgqSRSVDwVvHis0C9oQ5lsN+3SzWbZ3Qgl9E948KLi1b/j0X/jts1BWx8MPN6bYWZeKDnTxvO+ncLG5tb2TnG3tLd/cHhUPj5p6yRVhLZIwhPVDVFTzgRtGWY47UpFMQ457YST+7nfeaJKs0Q0zVTSIMaRYBEjaKzUbQ76E5QSB+WKV/UWcNeJn5MK5GgMyl/9YULSmApDOGrd8z1pggyVYYTTWamfaiqRTHBEe5YKjKkOssW9M/fCKkM3SpQtYdyF+nsiw1jraRzazhjNWK96c/E/r5ea6DbImJCpoYIsF0Upd03izp93h0xRYvjUEiSK2VtdMkaFxNiIbAb+6sfrpH1V9a+rtcdapX6Xp1GEMziHS/DhBurwAA1oAQEOz/AKb450Xpx352PZWnDymVP4A+fzB27wj9c=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 sc\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 <latexit sha1_base64=\"UybyA/02kICKOl+UyCujOjkaa4c=\">AAAB8HicbVBNS8NAEN3Ur1q/qh69LBbBU0lEVDwVvHisYD+wDWWznbRLN5uwOxFL6L/w4EXFq//Go//GbZuDtj4YeLw3w8y8IJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqOTR4LGPdDpgBKRQ0UKCEdqKBRYGEVjC6mfqtR9BGxOoexwn4ERsoEQrO0EoPptdFeMKMT3rlilt1Z6DLxMtJheSo98pf3X7M0wgUcsmM6Xhugn7GNAouYVLqpgYSxkdsAB1LFYvA+Nns4gk9sUqfhrG2pZDO1N8TGYuMGUeB7YwYDs2iNxX/8zophld+JlSSIig+XxSmkmJMp+/TvtDAUY4tYVwLeyvlQ6YZRxuSzcBb/HiZNM+q3kX1/O68UrvO0yiSI3JMTolHLkmN3JI6aRBOFHkmr+TN0c6L8+58zFsLTj5zSP7A+fwBj+SRHQ==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Segmentation\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\uf8ff\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <latexit sha1_base64=\"KQXSi9ugrQS7nhOzZArzINSX990=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69BIvgqSRS1JMUvHisYD+gDWWy3bRrN5tldyOU0P/gwYuKV/+PR/+N2zYHbX0w8Hhvhpl5oeRMG8/7dgpr6xubW8Xt0s7u3v5B+fCopZNUEdokCU9UJ0RNORO0aZjhtCMVxTjktB2Ob2d++4kqzRLxYCaSBjEOBYsYQWOlVm+MUmK/XPGq3hzuKvFzUoEcjX75qzdISBpTYQhHrbu+J02QoTKMcDot9VJNJZIxDmnXUoEx1UE2v3bqnlll4EaJsiWMO1d/T2QYaz2JQ9sZoxnpZW8m/ud1UxNdBxkTMjVUkMWiKOWuSdzZ6+6AKUoMn1iCRDF7q0tGqJAYG5DNwF/+eJW0Lqr+ZbV2X6vUb/I0inACp3AOPlxBHe6gAU0g8AjP8ApvjnBenHfnY9FacPKZY/gD5/MHExqPFA==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Mask\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Dc\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <latexit sha1_base64=\"u1I1c6T52TInSm1j7l9IIKl8RZg=\">AAAB/HicbVDLSsNAFJ3UV62v+Ni5CRbBVUmkqLgq6MJlBfuAJoTJdNoOnUzCzI1YQ/wWF25U3PofLv0bJ20W2npg4HDOvdwzJ4g5U2Db30ZpaXllda28XtnY3NreMXf32ipKJKEtEvFIdgOsKGeCtoABp91YUhwGnHaC8VXud+6pVCwSdzCJqRfioWADRjBoyTcP3BDDiGCeXme+C/QBUpL5ZtWu2VNYi8QpSBUVaPrml9uPSBJSAYRjpXqOHYOXYgmMcJpV3ETRGJMxHtKepgKHVHnpNH1mHWulbw0iqZ8Aa6r+3khxqNQkDPRknlXNe7n4n9dLYHDhpUzECVBBZocGCbcgsvIqrD6TlACfaIKJZDqrRUZYYgK6MN2BM//jRdI+rTlntfptvdq4LNooo0N0hE6Qg85RA92gJmohgh7RM3pFb8aT8WK8Gx+z0ZJR7OyjPzA+fwC+75Wx</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Instance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Model\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Model\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Predictor P✓\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              <latexit sha1_base64=\"uAPAp4KTN4ZFlGAbt9hylC/kgXE=\">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKqHgKePEYwTwgWcLspDcZMvtgplcIIT/hwYuKV3/Ho3/jJNmDJhY0FFXddHcFqZKGXPfbKaytb2xuFbdLO7t7+wflw6OmSTItsCESleh2wA0qGWODJClspxp5FChsBaO7md96Qm1kEj/SOEU/4oNYhlJwslK73uvSEIn3yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzO+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhjT+RcZoRxmKxKMwUo4TNnmd9qVGQGlvChZb2ViaGXHNBNiKbgbf88SppXlS9q+rlw2WldpunUYQTOIVz8OAaanAPdWiAAAXP8ApvTuq8OO/Ox6K14OQzx/AHzucPdoqP3A==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Updater\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Update\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Instance Memory                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Signal\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2. Overview of our entire online tracking pipeline used for inference, see Sec 3.1.\\r\\n\\r\\n\\r\\nthe few-shot learning based model prediction proposed in LWL [5] to produce\\r\\naccurate segmentation masks. Furthermore, D3S proposes to simply concatenate\\r\\nthe outputs of both modules whereas we learn a localization encoding to con-\\r\\ndition the segmentation mask decoding based on the localization information.\\r\\nCompared to D3S we update not only the instance localization but also the seg-\\r\\nmentation models and memories. Hence, our method integrates specific memory\\r\\nmanagement components.\\r\\n\\r\\n\\r\\n3      Method\\r\\n\\r\\n3.1       Overview\\r\\n\\r\\nVideo object segmentation methods can produce high quality segmentation masks\\r\\nbut are typically not robust enough for video object tracking. Robustness be-\\r\\ncomes vital for medium and long sequences, which are most prevalent in track-\\r\\ning datasets [16,35]. In such scenarios, the target object frequently undergoes\\r\\nsubstantial appearance changes, also occlusions and similarly looking objects\\r\\nare common. Hence, we propose to adapt a typical VOS approach with track-\\r\\ning components to increase its robustness. In particular, we base our approach\\r\\non the Learning What to Learn (LWL) [5] method and design a novel and\\r\\nsegmentation-centric tracking pipeline that estimates accurate object masks in-\\r\\nstead of bounding boxes. During inference a segmentation mask is typically not\\r\\nprovided in visual object tracking. Hence, we use STA [59] to generate the re-\\r\\nquired initial segmentation mask from the provided initial bounding box. An\\r\\noverview of our tracking method RTS is shown in Fig. 2. Our pipeline consists\\r\\nof a backbone network, a segmentation branch, an instance localization branch\\r\\nand a segmentation decoder. For each video frame, the backbone first extracts a\\r\\nfeature map xb . These features are further processed into segmentation features\\r\\nxs and classification features xc to serve as input of their respective branch.\\r\\n\\x0c\\n\\n6      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\nThe segmentation branch is designed to capture the details of the object with a\\r\\nhigh dimensional mask encoding whereas the instance localization branch aims\\r\\nat providing a coarser but robust score map representing the target location.\\r\\nBoth branches contain online learned components that are trained on memories\\r\\n(Ds and Dc ) that store features and predictions of past frames. The instance\\r\\nlocalization branch has two purposes: it is used to control the updating of both\\r\\nmodels and memories and it is used to condition the segmentation mask decod-\\r\\ning based on a learned score encoding produced by Hθ in order to inject instance\\r\\nlocalization information. The obtained segmentation scores and the raw instance\\r\\nmodel score map are then used to generate the final segmentation mask output.\\r\\n\\r\\n\\r\\n3.2   Segmentation Branch\\r\\n\\r\\nThe architecture of the segmentation branch is adopted from LWL [5], and we\\r\\nbriefly review it here. It consists of a segmentation sample memory Ds , a label\\r\\ngenerator Eθ , a weight predictor Wθ , a few-shot learner Aθ and a segmentation\\r\\nmodel Tτ . The goal of the few-shot learner Aθ is producing the parameters τ of\\r\\nthe segmentation model Tτ such that the obtained mask encoding xm contains\\r\\nthe information needed to compute the final segmentation mask of the target\\r\\nobject. The label mask encodings used by the few-shot learner are predicted by\\r\\nthe label generator Eθ .\\r\\n    The few-shot learner is formulated through the following optimization prob-\\r\\nlem, which is unrolled through steepest descent iterations in the network\\r\\n\\r\\n                     1      X                                            \\x01 2       λs\\r\\n         Ls (τ ) =                      Wθ (ys ) · Tτ (xs ) − Eθ (ys )         +      kτ k2 ,   (1)\\r\\n                     2                                                             2\\r\\n                         (xs ,ys )∈Ds\\r\\n\\r\\nwhere Ds corresponds to the segmentation memory, xs denotes the segmentation\\r\\nfeatures, ys the segmentation masks and λs is a learnable scalar regularization\\r\\nparameter. The weight predictor Wθ produces sample confidence weights for\\r\\neach spatial location in each memory sample. Applying the optimized model\\r\\nparameters τ ∗ within the segmentation model produces the mask encoding xm =\\r\\nTτ ∗ (xs ) for the segmentation features xs .\\r\\n     LWL [5] feeds the mask encoding directly into the segmentation decoder to\\r\\nproduce the segmentation mask. For long and challenging tracking sequences,\\r\\nonly relying on the mask encoding may lead to an accurate segmentation mask,\\r\\nbut often for the wrong object in the scene (see Fig 1). Since LWL [5] is only\\r\\nable to identify the target to a certain degree in challenging tracking sequences,\\r\\nwe propose to condition the mask encoding based on an instance localization\\r\\nrepresentation, described next.\\r\\n\\r\\n\\r\\n3.3   Instance Localization Branch\\r\\n\\r\\nThe previously described segmentation branch can produce accurate segmenta-\\r\\ntion masks but typically lacks the necessary robustness for tracking in medium\\r\\n\\x0c\\n\\n                                       Robust Visual Tracking by Segmentation      7\\r\\n\\r\\nor long-term sequences. Especially challenging are sequences where objects simi-\\r\\nlar to the target appear, where the target object is occluded or vanishes from the\\r\\nscene for a short time. Therefore, we propose a dedicated branch for target in-\\r\\nstance localization, in order to robustly identify the target among distractors or\\r\\nto detect occlusions. A powerful tracking paradigm that learns a target specific\\r\\nappearance model on both foreground and background information are discrim-\\r\\ninative correlation filters (DCF) [6,22,13,3]. These methods learn the weights of\\r\\na filter that differentiates foreground from background pixels represented by a\\r\\nscore map, where the maximal value corresponds to the target’s center.\\r\\n    Similar to the segmentation branch, we propose an instance localization\\r\\nbranch that consists of a sample memory Dc and a model predictor Pθ . The\\r\\nlatter predicts the parameters κ of the instance model Tκ . The instance model\\r\\nis trained online to produce the target score map used to localize the target\\r\\nobject. To obtain the instance model parameters κ we minimize the following\\r\\nloss function\\r\\n\\r\\n                               X                            \\x01 2       λc\\r\\n                 Lc (κ) =                  R Tκ (xc ), yc         +      kκk2 ,   (2)\\r\\n                                                                      2\\r\\n                            (xc ,yc )∈Dc\\r\\n\\r\\n\\r\\nwhere Dc corresponds to the instance memory containing the classification fea-\\r\\ntures xc and the Gaussian labels yc . R denotes the robust hinge-like loss [3]\\r\\nand λc is a fixed regularization parameter. To solve the optimization problem\\r\\nwe apply the method from [3], which unrolls steepest descent iterations of the\\r\\nGauss-Newton approximation of (2) to obtain the final model parameters κ∗ .\\r\\nThe score map can then be obtained with sc = Tκ∗ (xc ) by evaluating the target\\r\\nmodel on the classification features xc .\\r\\n\\r\\n\\r\\n\\r\\n3.4   Instance-Conditional Segmentation Decoder\\r\\n\\r\\n\\r\\nIn video object segmentation the produced mask encoding is directly fed into\\r\\nthe segmentation decoder to generate the segmentation mask. However, solely\\r\\nrelying on the mask encoding is not robust enough for the challenging tracking\\r\\nscenario, see Fig 1. Thus, we propose to integrate the instance localization in-\\r\\nformation into the segmentation decoding procedure. In particular, we condition\\r\\nthe mask encoding on a learned encoding of the instance localization score map.\\r\\n    First, we encode the raw score maps using a multi-layer Convolutional Neural\\r\\nNetwork (CNN) to learn a suitable representation. Secondly, we simply condition\\r\\nthe mask encoding with the learned representation via an element-wise addition.\\r\\nThe entire conditioning procedure can be defined as xf = xm + Hθ (sc ), where\\r\\nHθ denotes the CNN encoding the scores sc , and xm the mask encoding. The\\r\\nresulting features are then fed into the segmentation decoder that produces the\\r\\nsegmentation scores of the target object.\\r\\n\\x0c\\n\\n8       Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n3.5   Jointly Learning Instance Localization and Segmentation\\r\\n\\r\\nIn this section, we describe our general training strategy and parameters. In\\r\\nparticular, we further detail the segmentation and classification losses that we\\r\\nuse for offline training.\\r\\nSegmentation Loss First, we randomly sample J frames from an annotated\\r\\nvideo sequence and sort them according to their frame ids in increasing order to\\r\\nconstruct the training sequence V = {(xjb , ysj , ycj )}J−1          j        j\\r\\n                                                        j=0 , where xb = Bθ (I ) are\\r\\n                                              j                             j\\r\\nthe extracted features of the video frame I using the backbone Bθ , ys is the\\r\\ncorresponding segmentation mask and ysj denotes the Gaussian label at the tar-\\r\\nget’s center location. We start with entry v0 ∈ V and store it in the segmentation\\r\\nDs and instance memory Dc and obtain parameters τ 0 and κ0 of the segmenta-\\r\\ntion and instance model. We use these parameters to compute the segmentation\\r\\nloss for v1 ∈ V. Using the predicted segmentation mask we update the segmen-\\r\\ntation model parameters to τ 1 but keep the instance model parameters fixed.\\r\\nWe use this procedure since the segmentation parameters typically need to be\\r\\nupdated frequently to enable accurate segmentation. Conversely, we train the\\r\\nmodel predictor to produce instance model parameters only on a single frame\\r\\nthat generalize to multiple unseen future frames in order to ensure robust target\\r\\nlocalization. The resulting segmentation loss for the entire sequence V can thus\\r\\nbe described as follows\\r\\n                         J−1     \\x12 \\x10                                    \\x13\\r\\n                         X                                        \\x01\\x11\\r\\n           Lseq\\r\\n            s (θ; V) =         Ls Dθ Tτ j−1 (xjs ) + Hθ Tκ0 (xjc ) , ysj ,      (3)\\r\\n                         j=1\\r\\n\\r\\n\\r\\nwhere xs = Fθ (xb ) and xc = Gθ (xb ) and Ls is the Lovasz segmentation loss [1].\\r\\nClassification Loss Instead of training our tracker only with the segmentation\\r\\nloss, we add an auxiliary loss to ensure that the instance module produces score\\r\\nmaps localizing the target via a Gaussian distribution. Generating such a score\\r\\nmap is important because we directly use it to update the segmentation and\\r\\ninstance memories and to generate the final output. As explained before, we\\r\\nuse only the first training v0 ∈ V to optimize the instance model parameters.\\r\\nInstead of using only the parameters corresponding to the final iteration Niter\\r\\nof the optimization method κ0(Niter ) explained in Sec. 3.3 we use all intermediate\\r\\nparameters κ0(i) to compute the loss to encourage fast convergence. The final\\r\\ntarget classification loss for the whole sequence V is defined as follows\\r\\n                               J−1        Niter\\r\\n                                                                      !\\r\\n                   seq\\r\\n                               X      1 X         \\x10\\r\\n                                                            j     j\\r\\n                                                                    \\x11\\r\\n                 Lc (θ; V) =                    Lc Tκ0(i) (xc ), yc     ,       (4)\\r\\n                               j=1\\r\\n                                    Niter i=0\\r\\n\\r\\nwhere Lc is the hinge loss defined in [3]. To train our tracker we combine the\\r\\nsegmentation and classification losses using the scalar weight η and minimize\\r\\nboth losses jointly\\r\\n\\r\\n                     Lseq           seq             seq\\r\\n                      tot (θ; V) = Ls (θ; V) + η · Lc (θ; V).                   (5)\\r\\n\\x0c\\n\\n                                    Robust Visual Tracking by Segmentation         9\\r\\n\\r\\nTraining Details We use the train sets of LaSOT [16], GOT-10k [24], Youtube-\\r\\nVOS [52] and DAVIS [40]. For VOT datasets that only provide annotated bound-\\r\\ning boxes, we use these boxes and STA [59] to generate segmentation masks and\\r\\ntreat them as ground truth annotations during training. STA [59] is trained sep-\\r\\narately on YouTube-VOS 2019 [52] and DAVIS 2017 [38]. For our model, we\\r\\nuse ResNet-50 with pre-trained MaskRCNN weights as our backbone and ini-\\r\\ntialize the segmentation model and decoder weights with the ones available from\\r\\nLWL [5]. We train for 200 epochs and sample 15’000 videos per epoch, which\\r\\ntakes 96 hours to train on a single Nvidia A100 GPU. We use the ADAM [26]\\r\\noptimizer with a learning rate decay of 0.2 at epochs 25, 115 and 160. We weight\\r\\nthe losses such that the segmentation loss is predominant but in the same range\\r\\nas the classification. We empirically choose η = 10. Further details about training\\r\\nand the network architecture are given in the appendix.\\r\\n\\r\\n\\r\\n3.6   Inference\\r\\n\\r\\nIn this section we describe our inference approach, used during tracking.\\r\\nMemory Management and Model Updating Our tracker consists of two\\r\\ndifferent memories. The segmentation memory stores segmentation features and\\r\\npredicted segmentation masks of previous frames. In contrast, the instance mem-\\r\\nory contains classification features and Gaussian labels marking the center lo-\\r\\ncation of the target in the predicted segmentation mask of the previous video\\r\\nframe. The quality of the predicted labels directly influences the localization\\r\\nand segmentation quality in future video frames. Hence, it is crucial to avoid\\r\\ncontaminating the memories with wrong predictions not corresponding to the\\r\\nactual target. We propose the following strategy to keep the memory as clean\\r\\nas possible. (a) If the instance model is able to clearly localize the target (maxi-\\r\\nmum value in the score map larger than tsc = 0.3) and the segmentation model\\r\\nconstructs a valid segmentation mask (at least one pixel above tss = 0.5) we up-\\r\\ndate both memories with the current predictions and features. (b) If either the\\r\\ninstance localization or segmentation fail to identify the target we omit updating\\r\\nthe segmentation memory. (c) If only the segmentation mask fails to represent\\r\\nthe target but the instance model can localize it, we update the instance memory\\r\\nonly. (d) If instance localization fails we omit updating either memory. Further,\\r\\nwe trigger the few-shot learner and model predictor after 20 frames have passed\\r\\nbut only if the corresponding memory is updated.\\r\\nFinal Mask Output Generation We achieve the final segmentation mask by\\r\\nsimply thresholding the segmentation decoder output. To obtain the bounding\\r\\nbox required for standard tracking benchmarks, we simply report the smallest\\r\\naxis-aligned box that contains the entire estimated object mask.\\r\\nInference Details We set the input image resolution such that the segmenta-\\r\\ntion learner features have a resolution of 52 × 30 (stride 16), while the instance\\r\\nlearner operates of features of size 26 × 15 (stride 32). The learning rate is set to\\r\\n0.1 and 0.01 for the segmentation and instance learner respectively. We use a size\\r\\n\\x0c\\n\\n10      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\nof 32 frames for the segmentation memory and 50 frames for the instance mem-\\r\\nory. We keep the samples corresponding to the initial frame in both memories\\r\\nand replace the oldest entries if the memory is full. We update both memories\\r\\nfor the first 100 video frames and afterwards only after every 20th frame. We\\r\\nrandomly augment the sample corresponding to the initial frame with vertical\\r\\nflip, random translation and blurring.\\r\\n\\r\\n\\r\\n4     Evaluation\\r\\n\\r\\nOur approach is developed within the PyTracking [11] framework. The imple-\\r\\nmentation is done with PyTorch 1.9 with CUDA 11.1. Our model is evaluated\\r\\non a single Nvidia GTX 2080Ti GPU. Our method runs on average at 30 FPS\\r\\non LaSOT [16]. The code will be made available upon publication. Each number\\r\\ncorresponds to the average of five runs with different random seeds.\\r\\n\\r\\n\\r\\n4.1   Branch Ablation Study\\r\\n\\r\\nFor the ablation study, we analyze the impact of the instance branch on three\\r\\ndatasets and present the results in Tab. 1. First, we report the performance of\\r\\nLWL [5] since we build upon it to design our final tracking pipeline. We use\\r\\nthe network weights provided by Bhat et al. [5] and the corresponding infer-\\r\\nence settings. We input the same segmentation masks obtained from the initial\\r\\nbounding box for LWL as used for our method. We observe that LWL is not\\r\\nrobust enough for challenging tracking scenarios. The second row in Tab. 1 cor-\\r\\nresponds to our method but we omit the proposed instance branch. Hence, we\\r\\nuse the proposed inference components and settings and train the tracker as ex-\\r\\nplained in Sec. 3.5 but remove the conditioning. We observe that even without\\r\\nthe instance localization branch our tracker can achieve competitive performance\\r\\non all three datasets (e.g. +5.6% on LaSOT) but fully integrating the instance\\r\\nlocalization branch increases the performance even more (e.g. +4.4 on LaSOT).\\r\\nThus, we conclude that adapting the baseline method to the tracking domain\\r\\nimproves the tracking performance but to boost the performance and achieve\\r\\nstate-of-the-art results an additional component able to increase the tracking\\r\\nrobustness is required.\\r\\n\\r\\n\\r\\n                         Seg.  Inst. Branch   LaSOT [16]       NFS [19]        UAV123 [35]\\r\\n                        Branch Conditioning AUC P     NP    AUC P     NP     AUC P     NP\\r\\nLWL [5]                   3        -       59.7 60.6 63.3   61.5 75.1 76.9   59.7 78.8 71.4\\r\\nRTS (No Inst. Branch)     3        7       65.3 68.5 71.5   65.8 84.0 85.0   65.2 85.6 78.8\\r\\nRTS                       3        3       69.7 73.7 76.2   65.4 82.8 84.0   67.6 89.4 81.6\\r\\n\\r\\nTable 1. Comparison between our segmentation network baseline LWL and our\\r\\npipeline, with and without Instance conditioning on different VOT datasets.\\r\\n\\x0c\\n\\n                                       Robust Visual Tracking by Segmentation              11\\r\\n\\r\\nInst. Branch                  LaSOT [16]             NFS [19]              UAV123 [35]\\r\\n  Fallback      tsc    AUC       P     NP     AUC     P       NP    AUC       P     NP\\r\\n      7         0.30   69.3     73.1   75.9   65.3    82.7   84.0   66.3     87.2   80.4\\r\\n      3         0.30   69.7     73.7   76.2   65.4    82.8   84.0   67.6     89.4   81.6\\r\\n\\r\\n      3         0.20   68.6     72.3   75.0   65.3    82.7   83.9   67.0     88.7   80.7\\r\\n      3         0.30   69.7     73.7   76.2   65.4    82.8   84.0   67.6     89.4   81.6\\r\\n      3         0.40   69.1     72.7   75.6   63.3    79.7   81.7   67.1     89.1   80.7\\r\\n\\r\\nTable 2. Ablation on inference strategies. The first column analyzes the effect of using\\r\\nthe instance branch as fallback for target localization if the segmentation branch is\\r\\nunable to detect the target (max(ss ) < tss ). The second column shows the impact of\\r\\ndifferent confidence thresholds tsc .\\r\\n\\r\\n\\r\\n4.2       Inference Parameters\\r\\n\\r\\nIn this part we ablate two key aspects of our inference strategy. First, we study\\r\\nthe effect of relying on the instance branch if the segmentation decoder is unable\\r\\nto localize the target (max(ss ) < tss ). Secondly, we study different values for tsc\\r\\nthat determines whether the target is detected by the instance model, see Tab. 2.\\r\\n    We observe, that using the instance branch if the segmentation branch cannot\\r\\nidentify the target improves the tracking performance on all datasets (e.g. +1.3%\\r\\non UAV123). Furthermore, Tab. 2 shows that our tracking pipeline achieves the\\r\\nbest performance when setting tsc = 0.3 whereas smaller or larger values for\\r\\ntsc decrease the tracking accuracy. Hence, it is important to find a suitable\\r\\ntrade-off between frequently updating the model and memory to quickly adapt\\r\\nto appearance changes and updating only rarely to avoid contaminating the\\r\\nmemory and model based on wrong predictions.\\r\\n\\r\\n\\r\\n4.3       Comparison to the state of the art\\r\\n\\r\\nWe compare our approach on six VOT benchmarks and validate the segmen-\\r\\ntation masks quality on two VOS datasets because assessing the segmentation\\r\\naccuracy on tracking datasets is not possible since only bounding box annota-\\r\\ntions are provided.\\r\\nLaSOT [16] We evaluate our method on the test set of the LaSOT dataset\\r\\nwhich consists of 280 sequences with 2500 frames on average. Thus, the bench-\\r\\nmark challenges the long term adaptability and robustness of trackers. Fig. 3\\r\\nshows the success plot reporting the overlap precision OP with respect to the\\r\\noverlap threshold T . Trackers are ranked by AUC score. In addition, Tab. 3 re-\\r\\nports the precision and normalized precision for all compared methods. Our\\r\\nmethod outperforms the state-of-the-art trackers KeepTrack [34] and Stark-\\r\\nST101 [54] by a large margin (+2.6% AUC). Our method is not only as robust\\r\\nas KeepTrack (see the success plot for T < 0.2) but also estimates far more\\r\\naccurate bounding boxes than any tracker (0.8 < T < 1.0).\\r\\nGOT-10k [24] The large-scale GOT-10k dataset contains over 10.000 shorter\\r\\nsequences. Since we train our method on several datasets instead of only on\\r\\n\\x0c\\n\\n12           Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n                                     90\\r\\n                                                                 Success plot                                                                90\\r\\n                                                                                                                                                                  Precision plot\\r\\n                                     80                                                                                                      80\\r\\n                                     70                                                                                                      70\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                    Distance Precision [%]\\r\\n             Overlap Precision [%]\\r\\n                                     60            RTS [69.7]                                                                                60                                   RTS [73.7]\\r\\n                                                   KeepTrack [67.1]                                                                                                              STARK-ST101 [72.2]\\r\\n                                     50            STARK-ST101 [67.1]                                                                        50                                  STARK-ST50 [71.2]\\r\\n                                                   STARK-ST50 [66.4]                                                                                                             KeepTrack [70.2]\\r\\n                                     40            AlphaRefine [65.9]                                                                        40                                  TransT [69.0]\\r\\n                                                   TransT [64.9]                                                                                                                 AlphaRefine [68.8]\\r\\n                                     30            Siam R-CNN [64.8]                                                                         30                                  Siam R-CNN [68.4]\\r\\n                                                   TrDiMP [63.9]                                                                                                                 TrDiMP [66.3]\\r\\n                                     20            Super DiMP [63.1]                                                                         20                                  Super DiMP [65.3]\\r\\n                                                   STMTrack [60.6]                                                                                                               STMTrack [63.3]\\r\\n                                     10            PrDiMP50 [59.8]                                                                           10                                  PrDiMP50 [60.8]\\r\\n                                                   LWL [59.7]                                                                                                                    LWL [60.6]\\r\\n                                      00              0.2         0.4       0.6          0.8        1                                         00         10         20        30         40         50\\r\\n                                                                Overlap threshold                                                                         Location error threshold [pixels]\\r\\n\\r\\n\\r\\nFig. 3. Success (left) and Precision (right) plots on LaSOT [16] with other state-of-\\r\\nthe-art methods. The AUCs for all methods are ordered and reported in the legend.\\r\\nOur method outperforms all existing approaches, both in Overlap Precision (left) and\\r\\nDistance Precision (right).\\r\\n\\r\\n                                          Keep STARK Alpha            Siam   Tr Super STM Pr         DM\\r\\n                                      RTS Track ST-101 Refine TransT R-CNN DiMP DiMP Track DiMP LWL Track LTMU DiMP Ocean D3S\\r\\n                                           [34]  [54]   [53]    [8]    [46] [48] [11] [18] [14]  [5] [58] [10]  [3]  [57] [31]\\r\\nPrecision     73.7 70.2                                         72.2      68.8      69.0          68.4     66.3    65.3                           63.3   60.8     60.6     59.7     57.2       56.7      56.6   49.4\\r\\nNorm. Prec    76.2 77.2                                         76.9      73.8      73.8          72.2     73.0    72.2                           69.3   68.8     63.3     66.9     66.2       65.0      65.1   53.9\\r\\nSuccess (AUC) 69.7 67.1                                         67.1      65.9      64.9          64.8     63.9    63.1                           60.6   59.8     59.7     58.4     57.2       56.9      56.0   49.2\\r\\n∆ AUC to Ours                              -       ↑2.6         ↑2.6     ↑3.8       ↑4.8         ↑4.9      ↑5.8 ↑6.6 ↑9.1 ↑9.9 ↑10.0 ↑11.3 ↑12.5 ↑12.8 ↑13.7 ↑20.5\\r\\n\\r\\nTable 3. Comparison to the state of the art on the LaSOT [16] test set in terms of\\r\\nAUC score. The methods are ordered by AUC score.\\r\\n\\r\\n                                               RTS                   STA                 LWL                PrDiMP-50                                         DiMP-50                  SiamRPN++\\r\\n                                                                     [59]                 [5]                  [14]                                             [3]                        [28]\\r\\nSR0.50 (%)                                     94.5                 95.1                  92.4                    89.6                                           88.7                          82.8\\r\\nSR0.75 (%)                                     82.6                 85.2                  82.2                    72.8                                           68.8                           -\\r\\nAO(%)                                          85.2                 86.7                  84.6                    77.8                                           75.3                          73.0\\r\\n\\r\\nTable 4. Results on the GOT-10k validation set [24] in terms of Average Overlap\\r\\n(AO) and success rates (SR) for overlap thresholds of 0.5 and 0.75.\\r\\n\\r\\n                                               Keep STARK STARK                 Siam Alpha STM         Tr Super Pr\\r\\n                                           RTS Track ST101 ST50 STA LWL TransT R-CNN Refine Track DTT DiMP DiMP DiMP D3S\\r\\n                                                [34]  [54]  [54] [59] [5] [8]    [46] [53] [18] [55] [48] [11] [14] [31]\\r\\nPrecision     79.4 73.8                                             -              -           79.1 78.4    80.3                        80.0             78.3     76.7 78.9 73.1               73.3      70.4 66.4\\r\\nNorm. Prec    86.0 83.5                                            86.9           86.1         84.7 84.4    86.7                        85.4             85.6     85.1 85.0 83.3               83.5      81.6 76.8\\r\\nSuccess (AUC) 81.6 78.1                                            82.0           81.3         81.2 80.7    81.4                        81.2             80.5     80.3 79.6 78.4               78.1      75.8 72.8\\r\\n∆ AUC to Ours                                  -      ↑3.5         ↓0.4           ↑0.3 ↑0.4 ↑0.9 ↑0.2                                   ↑0.4             ↑1.1 ↑1.3 ↑2.0 ↑3.2 ↑3.5 ↑5.8 ↑8.8\\r\\n\\r\\nTable 5. Comparison to the state of the art on the TrackingNet [36] test set in terms\\r\\nof AUC scores, Precision and Normalized precision.\\r\\n\\r\\nGOT-10k train, we evaluate our approach on the val set only, which consists of\\r\\n180 short videos. We compile the results in Tab. 4. Our method ranks second\\r\\nfor all metrics, falling between two segmentation oriented methods, +0.6% over\\r\\nLWL [5] and −1.5% behind STA [59]. Note, that our tracker outperforms other\\r\\ntracking methods by a large margin.\\r\\nTrackingNet [36] We compare our approach on the test set of the Track-\\r\\ningNet dataset, consisting of 511 sequences. Tab. 5 shows the results obtained\\r\\nfrom the online evaluation server. Our method outperforms most of the existing\\r\\n\\x0c\\n\\n                                                 Robust Visual Tracking by Segmentation                                      13\\r\\n\\r\\n        RTS Keep        STARK              STARK Super Pr STM Siam Siam\\r\\n            Track CRACT ST101 TrDiMP TransT ST50 DiMP DiMP Track AttN R-CNN KYS DiMP LWL\\r\\n             [34]  [17]   [54]  [48]   [8]   [54] [11] [14] [18] [56]  [46]  [4] [3]  [5]\\r\\nUAV123 67.6 69.7     66.4     68.2     67.5      69.1     69.1      67.7   68.0     64.7   65.0    64.9     –     65.3    59.7\\r\\nNFS    65.4 66.4     62.5     66.2     66.2      65.7     65.2      64.8   63.5      –      –      63.9    63.5   62.0    61.5\\r\\nTable 6. Comparison with state-of-the-art on the UAV123 [35] and NFS [19] datasets\\r\\nin terms of AUC score.\\r\\n                    STARK STARK-\\r\\n                     ST-50 ST-101-                         Ocean       Fast   Alpha\\r\\n                RTS +AR     +AR          LWL       STA      Plus      Ocean   Refine       RPT     AFOD      D3S         STM\\r\\n                      [54]   [54]         [27]     [59]     [27]       [27]    [27]        [27]     [27]     [27]         [27]\\r\\nRobustness      0.845 0.817    0.789     0.798    0.824     0.842     0.803       0.777    0.869   0.795     0.769       0.574\\r\\nAccuracy        0.710 0.759    0.763     0.719    0.732     0.685     0.693       0.754    0.700   0.713     0.699       0.751\\r\\nEAO             0.506 0.505    0.497     0.463    0.510     0.491     0.461       0.482    0.530   0.472     0.439       0.308\\r\\n∆ EAO to Ours    -   ↑0.001 ↑0.009 ↑0.043 ↓0.004 ↑0.015 ↑0.045 ↑0.024 ↓0.024 ↑0.034 ↑0.067 ↑0.198\\r\\n\\r\\nTable 7. Results on the VOT2020-ST [27] challenge in terms of Expected Average\\r\\nOverlap (EAO), Accuracy and Robustness.\\r\\napproaches and ranks second in terms of AUC, close behind STARK-ST101 [54]\\r\\nwhich is based on a ResNet-101 backbone. Note, that we outperform STARK-\\r\\nST50 [54] that uses a ResNet-50 as backbone. We want to highlight that we\\r\\nachieve a higher precision score than other methods that produce a segmenta-\\r\\ntion mask output such as LWL [5], STA [59], Alpha-Refine [53] and D3S [31].\\r\\nUAV123 [35] The UAV dataset consists of 123 test videos that contain small\\r\\nobjects, target occlusion, and distractors. Small objects are particularly chal-\\r\\nlenging in a segmentation setup. Tab. 6 shows the achieved results in terms of\\r\\nsuccess AUC. Our method achieves competitive results on UAV123 similar to\\r\\nTrDiMP [48] or SuperDiMP [11]. Note, that it outperforms LWL [5] by a large\\r\\nmargin.\\r\\nNFS [19] The NFS dataset (30FPS version) contains 100 test videos with fast\\r\\nmotions and challenging sequences with distractors. Our method achieves an\\r\\nAUC score that is only 1% below the current best method KeepTrack [34] while\\r\\noutperforming numerous other trackers, including STARK-ST50 [54] (+0.2) Su-\\r\\nperDiMP [3] (+0.6) and PrDiMP [14] (+1.9).\\r\\nVOT 2020 [27] Finally, we evaluate our method on the VOT2020 short-term\\r\\nchallenge. It consists of 60 videos and provides segmentation mask annotations.\\r\\nFor the challenge, the multi-start protocol is used and the tracking performance\\r\\nis assessed based on accuracy and robustness. We compare with the top meth-\\r\\nods on the leader board and include more recent methods in Tab. 7. In this\\r\\nsituation, our method ranks 2nd in Robustness, thus outperforming most of the\\r\\nother methods. In particular we achieve a higher EAO score than STARK [54],\\r\\nLWL [5], AlphaRefine [53] and D3S [31].\\r\\nYouTube-VOS 2019 [52] We use the validation set of Youtube-VOS 2019 [52]\\r\\nwhich consist of 507 sequences. They contain 91 object categories out of which\\r\\n26 are unseen in the training set. The results presented in Tab. 8 were generated\\r\\nby an online server after uploading the raw results. On this benchmark we are\\r\\nnot aiming at achieving the new state of the art but rather validate the quality\\r\\nof the produced segmentation masks.\\r\\n\\x0c\\n\\n14       Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n                            YouTube-VOS 2019 [52]                DAVIS 2017 [40]\\r\\nMethod              G     Jseen  Junseen Fseen       Funseen   J &F     J        F\\r\\nRTS                79.7   77.9     75.4     82.0       83.3    80.2     77.9   82.6\\r\\nLWL [5]            81.0   79.6     76.4     83.8       84.2    81.6     79.1   84.1\\r\\nSTA [59]           80.6    -        -        -          -       -        -      -\\r\\nSTM [37]           79.2   79.6     73.0     83.6       80.6    81.8     79.2   84.3\\r\\n\\r\\nRTS (Box)          70.8   71.1     65.2     74.0       72.8    72.6     69.4   75.8\\r\\nLWL (Box) [5]       -      -        -        -          -      70.6     67.9   73.3\\r\\nSiam-RCNN [46]     67.3   68.1     61.5     70.8       68.8    70.6     66.1   75.0\\r\\nD3S [50]            -      -        -        -          -      60.8     57.8   63.8\\r\\nSiamMask [31]      52.8   60.2     45.1     58.2       47.7    56.4     54.3   58.5\\r\\nTable 8. Results on the Youtube-VOS 2019 [52] and DAVIS 2017 [40] datasets. The\\r\\ntable is split in two parts to separate methods using bounding box initialization or\\r\\nsegmentation masks initialization, in order to enable a fair comparison.\\r\\n\\r\\n\\r\\n    Hence, we use the same model weight as for VOT without further fine tuning.\\r\\nWhen using the provided segmentation masks for initialization, we observe that\\r\\nour method performs slighly worse than LWL [5] and STA [59] (-1.3 G, -0.9 G)\\r\\nbut still outperforms the VOS method STM [37] (+0.5 G). We conclude that our\\r\\nmethod can generate accurate segmentation masks. When using bounding boxes\\r\\nto predict the initialization and predict the segmentation masks, we outperform\\r\\nall other methods by a large margin. This confirms that even with our bounding-\\r\\nbox initialization strategy our method can produce accurate segmentation masks.\\r\\nDAVIS 2017 [40] Similarly, we compare our method on the validation set\\r\\nof DAVIS 2017 which contains 30 sequences without fine-tuning the model for\\r\\nthis benchmark. The results are shown in Tab. 8 and confirm the the obser-\\r\\nvation made above that our method is able to generate accurate segmentation\\r\\nmasks. Our method is competitive for the mask-initialization setup. In the box-\\r\\ninitialization however, our approach outperforms all other methods in J &F, in\\r\\nparticular the segmentation trackers like SiamMask [50] (+16.2) and D3S [31]\\r\\n(+11.8).\\r\\n\\r\\n\\r\\n5    Conclusion\\r\\nWe introduced RTS, a robust, end-to-end trainable, segmentation-driven track-\\r\\ning method that is able to generate accurate segmentation masks. Compared to\\r\\nthe traditional bounding box outputs of classical visual object trackers, segmen-\\r\\ntation masks enable a more accurate representation of the target’s shape and\\r\\nextent. The proposed instance localization branch helps increasing the robust-\\r\\nness of our tracker to enable robust tracking even for long sequences consisting\\r\\nof thousands of frames. Our method outperforms by a large margin previous\\r\\nsegmentation-driven tracking methods, and it is competitive on several VOT\\r\\nbenchmarks. In particular, we set a new state of the art for the challenging La-\\r\\nSOT [16] dataset with a success AUC of 69.7%, outperforming the previous best\\r\\nmethods by 2.6%. Competitive results on two VOS datasets confirm the high\\r\\nquality of the generated segmentation masks.\\r\\n\\x0c\\n\\n                                     Robust Visual Tracking by Segmentation           15\\r\\n\\r\\nReferences\\r\\n 1. Berman, M., Triki, A.R., Blaschko, M.B.: The lovász-softmax loss: A tractable\\r\\n    surrogate for the optimization of the intersection-over-union measure in neural\\r\\n    networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\\r\\n    Recognition (CVPR) (June 2018) 8\\r\\n 2. Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H.: Fully-\\r\\n    convolutional siamese networks for object tracking. In: Proceedings of the Eu-\\r\\n    ropean Conference on Computer Vision Workshops (ECCVW) (October 2016) 3,\\r\\n    4\\r\\n 3. Bhat, G., Danelljan, M., Gool, L.V., Timofte, R.: Learning discrimina-\\r\\n    tive model prediction for tracking. In: IEEE/CVF International Confer-\\r\\n    ence on Computer Vision, ICCV, Seoul, South Korea. pp. 6181–6190\\r\\n    (2019). https://doi.org/10.1109/ICCV.2019.00628, https://doi.org/10.1109/\\r\\n    ICCV.2019.00628 3, 4, 7, 8, 12, 13, 19, 23\\r\\n 4. Bhat, G., Danelljan, M., Van Gool, L., Timofte, R.: Know your surroundings:\\r\\n    Exploiting scene information for object tracking. In: Proceedings of the European\\r\\n    Conference on Computer Vision (ECCV) (August 2020) 13\\r\\n 5. Bhat, G., Lawin, F.J., Danelljan, M., Robinson, A., Felsberg, M., Gool, L.V.,\\r\\n    Timofte, R.: Learning what to learn for video object segmentation. In: European\\r\\n    Conference on Computer Vision ECCV (2020), https://arxiv.org/abs/2003.\\r\\n    11540 2, 4, 5, 6, 9, 10, 12, 13, 14, 19, 21, 22, 23\\r\\n 6. Bolme, D.S., Beveridge, J.R., Draper, B.A., Lui, Y.M.: Visual object tracking using\\r\\n    adaptive correlation filters. In: CVPR (2010) 3, 7\\r\\n 7. Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taixé, L., Cremers, D., Van Gool,\\r\\n    L.: One-shot video object segmentation. In: Proceedings of the IEEE Conference\\r\\n    on Computer Vision and Pattern Recognition. pp. 221–230 (2017) 4\\r\\n 8. Chen, X., Yan, B., Zhu, J., Wang, D., Yang, X., Lu, H.: Transformer tracking.\\r\\n    In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n    Recognition (CVPR) (June 2021) 4, 12, 13, 23\\r\\n 9. Chen, Y., Pont-Tuset, J., Montes, A., Van Gool, L.: Blazingly fast video object seg-\\r\\n    mentation with pixel-wise metric learning. In: Proceedings of the IEEE Conference\\r\\n    on Computer Vision and Pattern Recognition. pp. 1189–1198 (2018) 4\\r\\n10. Dai, K., Zhang, Y., Wang, D., Li, J., Lu, H., Yang, X.: High-performance long-\\r\\n    term tracking with meta-updater. In: Proceedings of the IEEE/CVF Conference\\r\\n    on Computer Vision and Pattern Recognition (CVPR) (June 2020) 12, 23\\r\\n11. Danelljan, M., Bhat, G.: PyTracking: Visual tracking library based on PyTorch.\\r\\n    https://github.com/visionml/pytracking (2019), accessed: 16/09/2019 10, 12,\\r\\n    13\\r\\n12. Danelljan, M., Bhat, G., Khan, F.S., Felsberg, M.: ATOM: accurate track-\\r\\n    ing by overlap maximization. In: IEEE Conference on Computer Vision and\\r\\n    Pattern Recognition, CVPR, Long Beach, CA, USA. pp. 4660–4669 (2019).\\r\\n    https://doi.org/10.1109/CVPR.2019.00479 3, 4\\r\\n13. Danelljan, M., Bhat, G., Shahbaz Khan, F., Felsberg, M.: ECO: efficient convolu-\\r\\n    tion operators for tracking. In: Proceedings of the IEEE Conference on Computer\\r\\n    Vision and Pattern Recognition (CVPR) (June 2017) 7\\r\\n14. Danelljan, M., Gool, L.V., Timofte, R.: Probabilistic regression for visual tracking.\\r\\n    In: CVPR (2020) 3, 4, 12, 13, 23\\r\\n15. Danelljan, M., Robinson, A., Shahbaz Khan, F., Felsberg, M.: Beyond correlation\\r\\n    filters: Learning continuous convolution operators for visual tracking. In: Proceed-\\r\\n\\x0c\\n\\n16      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n    ings of the European Conference on Computer Vision (ECCV) (October 2016)\\r\\n    3\\r\\n16. Fan, H., Lin, L., Yang, F., Chu, P., Deng, G., Yu, S., Bai, H., Xu, Y., Liao, C.,\\r\\n    Ling, H.: Lasot: A high-quality benchmark for large-scale single object tracking.\\r\\n    CoRR abs/1809.07845 (2018), http://arxiv.org/abs/1809.07845 1, 2, 3, 5, 9,\\r\\n    10, 11, 12, 14, 19, 21, 22, 23, 24\\r\\n17. Fan, H., Ling, H.: Cract: Cascaded regression-align-classification for robust visual\\r\\n    tracking. arXiv preprint arXiv:2011.12483 (2020) 13\\r\\n18. Fu, Z., Liu, Q., Fu, Z., Wang, Y.: Stmtrack: Template-free visual tracking with\\r\\n    space-time memory networks. In: Proceedings of the IEEE/CVF Conference on\\r\\n    Computer Vision and Pattern Recognition (CVPR) (June 2021) 12, 13, 23\\r\\n19. Galoogahi, H.K., Fagg, A., Huang, C., Ramanan, D., Lucey, S.: Need for speed: A\\r\\n    benchmark for higher frame rate object tracking. In: ICCV (2017) 10, 11, 13, 21,\\r\\n    22, 23\\r\\n20. Guo, Q., Feng, W., Zhou, C., Huang, R., Wan, L., Wang, S.: Learning dynamic\\r\\n    siamese network for visual object tracking. In: ICCV (2017) 3\\r\\n21. He, A., Luo, C., Tian, X., Zeng, W.: Towards a better match in siamese network\\r\\n    based visual object tracker. In: ECCV workshop (2018) 3\\r\\n22. Henriques, J.F., Caseiro, R., Martins, P., Batista, J.: High-speed tracking with\\r\\n    kernelized correlation filters. IEEE Transactions on Pattern Analysis and Machine\\r\\n    Intelligence (TPAMI) 37(3), 583–596 (2015) 3, 7\\r\\n23. Hu, Y.T., Huang, J.B., Schwing, A.G.: Videomatch: Matching based video object\\r\\n    segmentation. In: Proceedings of the European Conference on Computer Vision\\r\\n    (ECCV). pp. 54–70 (2018) 4\\r\\n24. Huang, L., Zhao, X., Huang, K.: GOT-10k: A large high-diversity benchmark for\\r\\n    generic object tracking in the wild. IEEE Transactions on Pattern Analysis and\\r\\n    Machine Intelligence (2019). https://doi.org/10.1109/tpami.2019.2957464, http:\\r\\n    //dx.doi.org/10.1109/TPAMI.2019.2957464 3, 9, 11, 12, 21\\r\\n25. Khoreva, A., Benenson, R., Ilg, E., Brox, T., Schiele, B.: Lucid data dreaming for\\r\\n    object tracking. In: The DAVIS Challenge on Video Object Segmentation (2017)\\r\\n    4\\r\\n26. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio,\\r\\n    Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations,\\r\\n    ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings\\r\\n    (2015), http://arxiv.org/abs/1412.6980 9\\r\\n27. Kristan, M., Leonardis, A., Matas, J., Felsberg, M., Pflugfelder, R., Kämäräinen,\\r\\n    J.K., Danelljan, M., Zajc, L.Č., Lukežič, A., Drbohlav, O., He, L., Zhang, Y.,\\r\\n    Yan, S., Yang, J., Fernández, G., et al: The eighth visual object tracking vot2020\\r\\n    challenge results. In: Proceedings of the European Conference on Computer Vision\\r\\n    Workshops (ECCVW) (August 2020) 13\\r\\n28. Li, B., Wu, W., Wang, Q., Zhang, F., Xing, J., Yan, J.: Siamrpn++: Evolution of\\r\\n    siamese visual tracking with very deep networks. In: Proceedings of the IEEE/CVF\\r\\n    Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019) 3,\\r\\n    4, 12\\r\\n29. Li, B., Yan, J., Wu, W., Zhu, Z., Hu, X.: High performance visual tracking with\\r\\n    siamese region proposal network. In: Proceedings of the IEEE/CVF Conference on\\r\\n    Computer Vision and Pattern Recognition (CVPR) (June 2018) 3, 4\\r\\n30. Li, X., Change Loy, C.: Video object segmentation with joint re-identification and\\r\\n    attention-aware mask propagation. In: Proceedings of the European Conference on\\r\\n    Computer Vision (ECCV). pp. 90–105 (2018) 4\\r\\n\\x0c\\n\\n                                      Robust Visual Tracking by Segmentation            17\\r\\n\\r\\n31. Lukezic, A., Matas, J., Kristan, M.: D3s - a discriminative single shot segmentation\\r\\n    tracker. In: CVPR (2020) 4, 12, 13, 14\\r\\n32. Lukezic, A., Vojı́r, T., Zajc, L.C., Matas, J., Kristan, M.: Discriminative correlation\\r\\n    filter tracker with channel and spatial reliability. International Journal of Computer\\r\\n    Vision (IJCV) 126(7), 671–688 (2018) 3\\r\\n33. Maninis, K.K., Caelles, S., Chen, Y., Pont-Tuset, J., Leal-Taixé, L., Cremers, D.,\\r\\n    Van Gool, L.: Video object segmentation without temporal information. IEEE\\r\\n    Transactions on Pattern Analysis and Machine Intelligence 41(6), 1515–1530\\r\\n    (2018) 4\\r\\n34. Mayer, C., Danelljan, M., Paudel, D.P., Gool, L.V.: Learning target candidate asso-\\r\\n    ciation to keep track of what not to track. In: IEEE/CVF International Conference\\r\\n    on Computer Vision, ICCV (2021), https://arxiv.org/abs/2103.16556 11, 12,\\r\\n    13, 23, 24\\r\\n35. Mueller, M., Smith, N., Ghanem, B.: A benchmark and simulator for uav tracking.\\r\\n    In: Proceedings of the European Conference on Computer Vision (ECCV) (October\\r\\n    2016) 1, 5, 10, 11, 13, 21, 22, 23\\r\\n36. Müller, M., Bibi, A., Giancola, S., Al-Subaihi, S., Ghanem, B.: Trackingnet: A\\r\\n    large-scale dataset and benchmark for object tracking in the wild. In: ECCV (2018)\\r\\n    3, 12, 21\\r\\n37. Oh, S.W., Lee, J.Y., Xu, N., Kim, S.J.: Video object segmentation using space-time\\r\\n    memory networks. In: Proceedings of the IEEE/CVF International Conference on\\r\\n    Computer Vision (ICCV) (October 2019) 2, 4, 14, 21\\r\\n38. Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-\\r\\n    Hornung, A.: A benchmark dataset and evaluation methodology for video object\\r\\n    segmentation. In: Computer Vision and Pattern Recognition (2016) 9\\r\\n39. Perazzi, F., Khoreva, A., Benenson, R., Schiele, B., Sorkine-Hornung, A.: Learn-\\r\\n    ing video object segmentation from static images. In: Proceedings of the IEEE\\r\\n    Conference on Computer Vision and Pattern Recognition. pp. 2663–2672 (2017) 4\\r\\n40. Pont-Tuset, J., Perazzi, F., Caelles, S., Arbeláez, P., Sorkine-Hornung, A., Van\\r\\n    Gool, L.: The 2017 davis challenge on video object segmentation. arXiv:1704.00675\\r\\n    (2017) 2, 4, 9, 14, 21, 22\\r\\n41. Son, J., Jung, I., Park, K., Han, B.: Tracking-by-segmentation with online gradient\\r\\n    boosting decision tree. In: 2015 IEEE International Conference on Computer Vision\\r\\n    (ICCV). pp. 3056–3064 (2015). https://doi.org/10.1109/ICCV.2015.350 4\\r\\n42. Tao, R., Gavves, E., Smeulders, A.W.M.: Siamese instance search for tracking. In:\\r\\n    CVPR (2016) 3\\r\\n43. Valmadre, J., Bertinetto, L., Henriques, J., Vedaldi, A., Torr, P.H.S.: End-to-end\\r\\n    representation learning for correlation filter based tracking. In: Proceedings of the\\r\\n    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\r\\n    (July 2017) 3, 4\\r\\n44. Voigtlaender, P., Chai, Y., Schroff, F., Adam, H., Leibe, B., Chen, L.C.: Feelvos:\\r\\n    Fast end-to-end embedding learning for video object segmentation. In: Proceedings\\r\\n    of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9481–\\r\\n    9490 (2019) 4\\r\\n45. Voigtlaender, P., Leibe, B.: Online adaptation of convolutional neural networks for\\r\\n    video object segmentation. In: BMVC (2017) 4\\r\\n46. Voigtlaender, P., Luiten, J., Torr, P.H., Leibe, B.: Siam R-CNN: Visual track-\\r\\n    ing by re-detection. In: IEEE/CVF Conference on Computer Vision and Pattern\\r\\n    Recognition (CVPR) (June 2020) 2, 4, 12, 13, 14, 23\\r\\n\\x0c\\n\\n18      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n47. Wang, G., Luo, C., Sun, X., Xiong, Z., Zeng, W.: Tracking by instance detec-\\r\\n    tion: A meta-learning approach. In: Proceedings of the IEEE/CVF Conference on\\r\\n    Computer Vision and Pattern Recognition (CVPR) (June 2020) 3\\r\\n48. Wang, N., Zhou, W., Wang, J., Li, H.: Transformer meets tracker: Exploiting\\r\\n    temporal context for robust visual tracking. In: Proceedings of the IEEE/CVF\\r\\n    Conference on Computer Vision and Pattern Recognition (CVPR) (June 2021) 4,\\r\\n    12, 13, 23\\r\\n49. Wang, Q., Teng, Z., Xing, J., Gao, J., Hu, W., Maybank, S.J.: Learning attentions:\\r\\n    Residual attentional siamese network for high performance online visual tracking.\\r\\n    In: CVPR (2018) 3\\r\\n50. Wang, Q., Zhang, L., Bertinetto, L., Hu, W., Torr, P.H.: Fast online object tracking\\r\\n    and segmentation: A unifying approach. In: Proceedings of the IEEE conference\\r\\n    on computer vision and pattern recognition (2019) 4, 14\\r\\n51. Wug Oh, S., Lee, J.Y., Sunkavalli, K., Joo Kim, S.: Fast video object segmentation\\r\\n    by reference-guided mask propagation. In: Proceedings of the IEEE Conference on\\r\\n    Computer Vision and Pattern Recognition. pp. 7376–7385 (2018) 4\\r\\n52. Xu, N., Yang, L., Fan, Y., Yue, D., Liang, Y., Yang, J., Huang, T.: Youtube-vos:\\r\\n    A large-scale video object segmentation benchmark (2018) 4, 9, 13, 14, 21, 22\\r\\n53. Yan, B., Wang, D., Lu, H., Yang, X.: Alpha-refine: Boosting tracking performance\\r\\n    by precise bounding box estimation. In: CVPR (2021) 2, 4, 12, 13, 23\\r\\n54. Yan, B., Peng, H., Fu, J., Wang, D., Lu, H.: Learning spatio-temporal transformer\\r\\n    for visual tracking. In: Proceedings of the IEEE/CVF International Conference on\\r\\n    Computer Vision (ICCV). pp. 10448–10457 (October 2021) 2, 4, 11, 12, 13, 23, 24\\r\\n55. Yu, B., Tang, M., Zheng, L., Zhu, G., Wang, J., Feng, H., Feng, X., Lu, H.:\\r\\n    High-performance discriminative tracking with transformers. In: Proceedings of\\r\\n    the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 9856–\\r\\n    9865 (October 2021) 4, 12\\r\\n56. Yu, Y., Xiong, Y., Huang, W., Scott, M.R.: Deformable siamese attention net-\\r\\n    works for visual object tracking. In: Proceedings of the IEEE/CVF Conference on\\r\\n    Computer Vision and Pattern Recognition (CVPR) (June 2020) 13\\r\\n57. Zhang, Z., Peng, H., Fu, J., Li, B., Hu, W.: Ocean: Object-aware anchor-free track-\\r\\n    ing. In: Proceedings of the European Conference on Computer Vision (ECCV)\\r\\n    (August 2020) 12\\r\\n58. Zhang, Z., Zhong, B., Zhang, S., Tang, Z., Liu, X., Zhang, Z.: Distractor-aware\\r\\n    fast tracking via dynamic convolutions and mot philosophy. In: Proceedings of\\r\\n    the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\r\\n    (June 2021) 12\\r\\n59. Zhao, B., Bhat, G., Danelljan, M., Gool, L.V., Timofte, R.: Generating masks from\\r\\n    boxes by mining spatio-temporal consistencies in videos. In: IEEE/CVF Interna-\\r\\n    tional Conference on Computer Vision, ICCV (2021), https://arxiv.org/abs/\\r\\n    2101.02196 4, 5, 9, 12, 13, 14, 21\\r\\n60. Zheng, L., Tang, M., Chen, Y., Wang, J., Lu, H.: Learning feature embeddings for\\r\\n    discriminant model based tracking. In: Proceedings of the European Conference\\r\\n    on Computer Vision (ECCV) (August 2020) 3\\r\\n61. Zhu, Z., Wang, Q., Bo, L., Wu, W., Yan, J., Hu, W.: Distractor-aware siamese\\r\\n    networks for visual object tracking. In: ECCV (2018) 3\\r\\n\\x0c\\n\\n                                  Robust Visual Tracking by Segmentation       19\\r\\n\\r\\nAppendix\\r\\nIn this Appendix, we provide further details on various aspects of our tracking\\r\\npipeline. First, we provide additional architectural and inference details in Sec-\\r\\ntions A and B. Second, we provide additional ablation studies, in particular on\\r\\nthe loss weighting parameter η on different benchmarks to show the importance\\r\\nof the auxiliary instance localization loss in Section C. Then, we provide success\\r\\nplots for different VOT benchmarks as well as a detailed analysis of our results\\r\\non LaSOT [16] by comparing our approach against the other state-of-the-art\\r\\nmethods for all the dataset attributes in Section D. Finally, we provide some\\r\\nadditional visual comparison to other trackers in Section E.\\r\\n\\r\\nA    Additional Architecture details\\r\\nClassification Scores Encoder Hθ First, we describe in Figure A1 the archi-\\r\\ntecture of the Classification Scores Encoder Hθ . It takes as input the H × W -\\r\\ndimensional scores predicted by the Instance Localization (Classification) branch\\r\\nand outputs a 16 channels deep representation of those scores. The score encoder\\r\\nconsists of a convolutional layer followed by a max-pool layer with stride one and\\r\\ntwo residual blocks. The output of the residual blocks has 64 channels. Thus, the\\r\\nfinal convolutional layer reduces the number of channels of the output to 16 to\\r\\nmatch the encoded scores with the mask encoding. All the convolutional layers\\r\\nuse (3 × 3) kernels with a stride of one to preserve the spatial size of the input\\r\\nclassification scores.\\r\\nSegmentation Decoder Dθ The segmentation decoder has the same struc-\\r\\nture has in LWL [5]. Together with the backbone it shows a U-Net structure and\\r\\nmainly consists of four decoder blocks. It takes as input the extracted ResNet-50\\r\\nbackbone features and the combined encoding xf from both the instance localiza-\\r\\ntion branch (Hθ (sc )) and the segmentation branch (xm ), with xf = xm + Hθ (sc ).\\r\\nSince the encoded instance localization scores have a lower spatial resolution\\r\\nthan the mask encoding xm , we upscale the encoded instance localization scores\\r\\nusing a bilinear interpolation before adding it with the mask encoding xm . We\\r\\nrefer the reader to [5] for more details about the decoder structure.\\r\\nSegmentation Branch We use the same architectures for the feature extractor\\r\\nFθ , the label encoder Eθ , the weight predictor Wθ , the few-shot learner Aθ and\\r\\nthe segmentation model Tτ as proposed in LWL [5]. Hence, we refer the reader\\r\\nto [5] for more details.\\r\\nInstance Localization Branch We use the same architectures for the feature\\r\\nextractor Gθ , the model predictor Pθ and the instance model Tκ as proposed in\\r\\nDiMP [3]. Hence, we refer the reader to [3] for more details.\\r\\n\\r\\nB    Additional Inference details\\r\\nSearch region selection The backbone does not extract features on the full\\r\\nimage. Instead, we sample a smaller image patch for extraction, which is centered\\r\\n\\x0c\\n\\n20      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n               Conv + ReLU\\r\\n         (Kernel Size = 3, Stride = 1)           ResBlock                         ReLU\\r\\n\\r\\n\\r\\n                  ResBlock\\r\\n                                                                                  +\\r\\n                                                                                 Conv\\r\\n                  ResBlock\\r\\n                                                                      (Kernel Size = 3, Stride = 1)\\r\\n\\r\\n                                                             Conv\\r\\n                MAX POOL                          (Kernel Size = 3, Stride = 1)\\r\\n         (Kernel Size = 3, Stride = 1)\\r\\n                                                                            Conv + ReLU\\r\\n               Conv + ReLU                                            (Kernel Size = 3, Stride = 1)\\r\\n         (Kernel Size = 3, Stride = 1)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                           Fig. A1. Classification Scores Encoder Hθ .\\r\\n\\r\\n\\r\\nat the current target location and 6 times larger than the current estimated\\r\\ntarget size, when it does not exceed the size of the image. The estimation of the\\r\\ntarget state (position and size) is therefore crucial to ensure an optimal crop. In\\r\\nmost situations, the segmentation output is used to determine the target state\\r\\nsince it has a high accuracy. The target center is computed as the center of\\r\\nmass of the predicted per-pixel segmentation probability scores. The target size\\r\\nis computed as the variance of the segmentation probability scores.\\r\\n    If the segmentation branch cannot find the target (as described in the main\\r\\npaper), but the instance branch still outputs a high enough confidence score, we\\r\\nuse it to update the target position. This is particularly important in sequences\\r\\nwhere the target is becoming too small for some time, but we still can track the\\r\\ntarget position.\\r\\n    When both branch cannot find the target, the internal state of the tracker is\\r\\nnot updated. We upscale the search area based on the previous 60 valid predicted\\r\\nscales. This is helpful in situations where the size of the object shrinks although\\r\\nits size does not change. This typically happens during occlusions or if the target\\r\\ngoes out of the frame partially on completely.\\r\\n\\r\\n\\r\\nC    Additional Ablations\\r\\n\\r\\nIn this section, we provide additional ablation studies related to our method, first\\r\\non the weighting of the segmentation and classification losses used for training,\\r\\n\\x0c\\n\\n                                          Robust Visual Tracking by Segmentation              21\\r\\n\\r\\n         LaSOT [16]        GOT-10k [24]     TrackingNet [36]     NFS [19]       UAV123 [35]\\r\\n η          AUC               AO                 AUC              AUC             AUC\\r\\n0.0         67.7               84.0               81.2                63.7         64.7\\r\\n0.4         69.8               84.0               81.4                66.2         67.4\\r\\n10          69.7               85.2               81.6                65.4         67.6\\r\\n\\r\\nTable A1. Ablation on the classification vs. segmentation loss weighting on different\\r\\ndatasets in terms of AUC (area-under-the-curve) and AO (average overlap)\\r\\n\\r\\n\\r\\n                                 YouTube-VOS 2019 [52]                     DAVIS 2017 [40]\\r\\nMethod                 G       Jseen  Junseen Fseen          Funseen     J &F     J        F\\r\\nRTS                   79.7     77.9       75.4    82.0         83.3      80.2      77.9   82.6\\r\\nRTS (YT-FT)           80.3     78.8       76.2    82.9         83.5      80.3      77.7   82.9\\r\\nLWL [5]               81.0     79.6       76.4    83.8         84.2      81.6      79.1   84.1\\r\\nSTA [59]              80.6      -          -       -            -         -         -      -\\r\\nSTM [37]              79.2     79.6       73.0    83.6         80.6      81.8      79.2   84.3\\r\\n\\r\\nTable A2. Results on the Youtube-VOS 2019 [52] and DAVIS 2017 [40] datasets with\\r\\na fined tuned model and inference parameters refered as RTS (YT-FT).\\r\\n\\r\\n\\r\\nsecond on the parameters that might make a difference specifically for VOS\\r\\nbenchmarks like Youtube-VOS [52].\\r\\nWeighting segmentation and classification losses For this ablation, we\\r\\nstudy the weighting of the segmentation loss Ls and the instance localization\\r\\nloss Lc in the total loss Ltot used to train our model and its influence on the\\r\\noverall performance during tracking. We recall that\\r\\n\\r\\n                                      Ltot = Ls + η · Lc .                                    (6)\\r\\nTable A1 shows the results when training the tracker with three different values\\r\\nof η on five VOT datasets. First we examine the case where we omit the aux-\\r\\niliary instance localization loss (η = 0.0), which means that the whole pipeline\\r\\nis trained for segmentation and the instance branch is not trained to produce\\r\\nspecifically accurate localization scores. We observe that for this setting leads to\\r\\nthe lowest performance on all tested datasets, often by a large margin. Secondly,\\r\\nwe test a dominant segmentation loss (η = 0.4) because the segmentation branch\\r\\nneeds to be trained for a more complex task than the instance branch. We see a\\r\\nperformance gain for almost all datasets. Thus, employing the auxiliary loss to\\r\\ntrain the instance localization branch helps to improve the tracking performance.\\r\\nWe observed that using the auxiliary loss leads to localization scores generated\\r\\nduring inference that are sharper, cleaner and localize the center of the target\\r\\nmore accurately. Finally, we put an even higher weight on the classification term\\r\\n(η = 10). This setup leads to an even more accurate localization and leads to\\r\\nthe on average best results. Thus, we set η = 10 to train our tracking pipeline.\\r\\nFinetuning on Youtube-VOS [16] In this section, we analyze whether we\\r\\ncan gear our pipeline towards VOS benchmarks. To do that, we take our model\\r\\nand inference parameters and modify them slightly. On the one hand, the model\\r\\n\\x0c\\n\\n22                              Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n                        90\\r\\n                                                    Success plot                                                                            90\\r\\n                                                                                                                                                         Normalized Precision plot\\r\\n                        80                                                                                                                  80\\r\\n                        70                                                                                                                  70\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                   Distance Precision [%]\\r\\nOverlap Precision [%]\\r\\n                               RTS [69.7]                                                                                                                                         KeepTrack [77.2]\\r\\n                        60     KeepTrack [67.1]                                                                                             60                                    STARK-ST101 [76.9]\\r\\n                               STARK-ST101 [67.1]                                                                                                                                 STARK-ST50 [76.3]\\r\\n                        50     STARK-ST50 [66.4]                                                                                            50                                    RTS [76.2]\\r\\n                               AlphaRefine [65.9]                                                                                                                                 AlphaRefine [73.8]\\r\\n                               TransT [64.9]                                                                                                                                      TransT [73.8]\\r\\n                        40     Siam R-CNN [64.8]\\r\\n                               TrDiMP [63.9]\\r\\n                                                                                                                                            40                                    TrDiMP [73.0]\\r\\n                                                                                                                                                                                  Siam R-CNN [72.2]\\r\\n                               Super DiMP [63.1]                                                                                                                                  Super DiMP [72.2]\\r\\n                        30     STMTrack [60.6]                                                                                              30                                    STMTrack [69.3]\\r\\n                               PrDiMP50 [59.8]                                                                                                                                    PrDiMP50 [68.8]\\r\\n                               LWL [59.7]                                                                                                                                         DMTrack [66.9]\\r\\n                        20     DMTrack [58.4]                                                                                               20                                    LTMU [66.5]\\r\\n                               TACT [57.5]                                                                                                                                        TACT [66.0]\\r\\n                        10     LTMU [57.2]                                                                                                  10                                    Ocean [65.1]\\r\\n                               DiMP50 [56.9]                                                                                                                                      DiMP50 [65.0]\\r\\n                               Ocean [56.0]                                                                                                                                       LWL [63.3]\\r\\n                         00          0.2            0.4                             0.6   0.8        1                                       00      0.1        0.2       0.3         0.4              0.5\\r\\n                                                Overlap threshold                                                                                          Location error threshold\\r\\n                                                                                   90\\r\\n                                                                                                  Precision plot\\r\\n                                                                                   80\\r\\n                                                                                   70\\r\\n                                                          Distance Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                    RTS [73.7]\\r\\n                                                                                   60                                               STARK-ST101 [72.2]\\r\\n                                                                                                                                    STARK-ST50 [71.2]\\r\\n                                                                                   50                                               KeepTrack [70.2]\\r\\n                                                                                                                                    TransT [69.0]\\r\\n                                                                                                                                    AlphaRefine [68.8]\\r\\n                                                                                   40                                               Siam R-CNN [68.4]\\r\\n                                                                                                                                    TrDiMP [66.3]\\r\\n                                                                                                                                    Super DiMP [65.3]\\r\\n                                                                                   30                                               STMTrack [63.3]\\r\\n                                                                                                                                    PrDiMP50 [60.8]\\r\\n                                                                                                                                    TACT [60.7]\\r\\n                                                                                   20                                               LWL [60.6]\\r\\n                                                                                                                                    DMTrack [59.7]\\r\\n                                                                                   10                                               LTMU [57.2]\\r\\n                                                                                                                                    DiMP50 [56.7]\\r\\n                                                                                                                                    Ocean [56.6]\\r\\n                                                                                    00    10        20        30                             40      50\\r\\n                                                                                          Location error threshold [pixels]\\r\\n\\r\\nFig. A2. Success, precision and normalized precision plots on LaSOT [16]. Our ap-\\r\\nproach outperforms all other methods by a large margin in AUC, reported in the\\r\\nlegend.\\r\\n\\r\\n\\r\\nis fined-tuned for 50 epochs using Youtube-VOS [52] only for both training and\\r\\nvalidation. We also increase the initialization phase from 100 to 200 frames, and\\r\\nremove the relative target scale change limit from one frame to the next (in our\\r\\nmodel, we limit that scale change to 20% for increased robustness).\\r\\n     The results are presented in Table A2 for Youtube-VOS [52] and Davis [40].\\r\\nWe observe that the performances between both of our models stay very close\\r\\nfor Davis but that the finetuned model is getting closer to the baseline LWL [5]\\r\\nfor Youtube-VOS. The more frequent updates seem to help and not restricting\\r\\nthe scale change of objects from one frame to the next seems to play a role, since\\r\\nwe get an improvement of 0.6 in G score.\\r\\n\\r\\n\\r\\nD                             Additional Evaluation results\\r\\n\\r\\nIn this section we provide additional plots of our approach on different bench-\\r\\nmarks and a attribute analysis on LaSOT [16].\\r\\nSuccess plots for LaSOT [16], NFS [19] and UAV123 [35] We provide\\r\\nin Figure A2 all the plots for the metrics we report for LaSOT [16] in the paper:\\r\\n\\x0c\\n\\n                                                                                   Robust Visual Tracking by Segmentation                                                                    23\\r\\n\\r\\n                        100\\r\\n                                                   Success plot                                                               100\\r\\n                                                                                                                                                               Success plot\\r\\n\\r\\n                         80                                                                                                    80\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                      Overlap Precision [%]\\r\\n                               KeepTrack [69.7]\\r\\n                         60    TransT [69.1]                                                                                   60\\r\\n                               PrDiMP50 [68.0]\\r\\n                               Super DiMP [67.7]                                                                                           KeepTrack [66.4]\\r\\n                               RTS [67.6]                                                                                                  TrDiMP [66.2]\\r\\n                         40    TrDiMP [67.5]                                                                                   40          RTS [65.4]\\r\\n                               STMTrack [65.7]                                                                                             TransT [65.3]\\r\\n                               DiMP50 [65.3]                                                                                               Super DiMP [64.8]\\r\\n                               SiamRPN++ [65.2]                                                                                            PrDiMP50 [63.5]\\r\\n                               ATOM [64.2]                                                                                                 DiMP50 [61.9]\\r\\n                         20    DaSiamRPN [57.7]                                                                                20          ATOM [58.4]\\r\\n                               UPDT [54.5]                                                                                                 UPDT [53.6]\\r\\n                               ECO [53.2]                                                                                                  CCOT [48.8]\\r\\n                               CCOT [51.3]                                                                                                 ECO [46.6]\\r\\n                          00       0.2             0.4      0.6          0.8         1                                          00             0.2             0.4          0.6     0.8       1\\r\\n                                              Overlap threshold                                                                                           Overlap threshold\\r\\n\\r\\nFig. A3. Success plots on the UAV123 [35] (left) and NFS [19] (right) datasets in terms\\r\\nof overall AUC score, reported in the legend.\\r\\n\\r\\n                                 Illumination Partial             Motion Camera         Background Viewpoint Scale     Full     Fast                 Low        Aspect\\r\\n                                   Variation Occlusion Deformation Blur Motion Rotation   Clutter   Change Variation Occlusion Motion Out-of-View Resolution Ratio Change Total\\r\\nLTMU [10]                            56.5          54.0   57.2    55.8    61.6    55.1      49.9       56.7                         57.1      49.9      44.0         52.7    51.4    55.1   57.2\\r\\nLWL [5]                              65.3          56.4   61.6    59.1    64.7    57.4      53.1       58.1                         59.3      48.7      46.5         51.5    48.7    57.9   59.7\\r\\nPrDiMP50 [14]                        63.7          56.9   60.8    57.9    64.2    58.1      54.3       59.2                         59.4      51.3      48.4         55.3    53.5    58.6   59.8\\r\\nSTMTrack [18]                        65.2          57.1   64.0    55.3    63.3    60.1      54.1       58.2                         60.6      47.8      42.4         51.9    50.3    58.8   60.6\\r\\nSuperDiMP [3]                        67.8          59.7   63.4    62.0    68.0    61.4      57.3       63.4                         62.9      54.1      50.7         59.0    56.4    61.6   63.1\\r\\nTrDiMP [48]                          67.5          61.1   64.4    62.4    68.1    62.4      58.9       62.8                         63.4      56.4      53.0         60.7    58.1    62.3   63.9\\r\\nSiam R-CNN [46]                      64.6          62.2   65.2    63.1    68.2    64.1      54.2       65.3                         64.5      55.3      51.5         62.2    57.1    63.4   64.8\\r\\nTransT [8]                           65.2          62.0   67.0    63.0    67.2    64.3      57.9       61.7                         64.6      55.3      51.0         58.2    56.4    63.2   64.9\\r\\nAlphaRefine [53]                     69.4          62.3   66.3    65.2    70.0    63.9      58.8       63.1                         65.4      57.4      53.6         61.1    58.6    64.1   65.3\\r\\nKeepTrack Fast [34]                  70.1          63.8   66.2    65.0    70.7    65.1      60.1       67.6                         66.6      59.2      57.1         63.4    62.0    65.6   66.8\\r\\nKeepTrack [34]                       69.7          64.1   67.0    66.7    71.0    65.3      61.2       66.9                         66.8      60.1      57.7         64.1    62.0    65.9   67.1\\r\\nSTARK-ST101 [54]                     67.5          65.1   68.3    64.5    69.5    66.6      57.4       68.8                         66.8      58.9      54.2         63.3    59.6    65.6   67.1\\r\\nRTS                                  68.7          66.9   71.6    67.7    74.4    67.9      61.4       69.7                         69.3      60.5      53.8         66.3    62.7    68.2   69.7\\r\\n\\r\\n\\r\\n\\r\\nTable A3. LaSOT [16] attribute-based analysis. Each column corresponds to the re-\\r\\nsults computed on all sequences in the dataset with the corresponding attribute. Our\\r\\nmethod outperforms all others in 12 out of 14 attributes.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSuccess, Normalized Precision and Precision plots. For completeness, we provide\\r\\nthe success plots for NFS [19] and UAV123 [35] in Figure A3.\\r\\nAttribute analysis on LaSOT [16] In this section, we focus on the dataset\\r\\nsequences attributes. We compare our approach to numerous other trackers, and\\r\\nprovide the detailed results in Table A3. Furthermore, we highlight the strength\\r\\nof our approach in Figure A4 by focusing the comparison only to the two current\\r\\nstate-of-the-art methods KeepTrack [34] and STARK-ST101 [54].\\r\\n    There are 14 attributes provided for LaSOT [16] sequences, representing dif-\\r\\nferent kind of challenges the tracker has to deal with in different situations. Com-\\r\\npared to existing trackers, our method achieves better AUC scores in 12 out of 14\\r\\nattributes. In particular, we outperform KeepTrack [34] and STARK-ST101 [54]\\r\\nby a large margin for the following attributes: Camera Motion (+3.4% and\\r\\n+3.9%), Background Clutter (+0.2% and +4.0%), Scale Variation (+2.5% and\\r\\n2.5%), Deformation (+4.6% and +3.3%) and Aspect Ratio Change (+2.3% and\\r\\n+2.6%). Our method is only outperformed on two attributes by KeepTrack [34]\\r\\nand KeepTrack Fast [34] for Fast Motion (-3.9% and -3.3%) and for Illumination\\r\\nVariation (-1.0% and -1.4%).\\r\\n\\x0c\\n\\n24      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n                                   Camera Motion\\r\\n                                   Motion Blur\\r\\n                          Rotation             Deformation\\r\\n                                         70\\r\\n                   Background                          Partial\\r\\n                     Clutter             60          Occlusion\\r\\n\\r\\n                  Viewpoint                 50              Illumination\\r\\n                   Change                                      Variation\\r\\n\\r\\n                     Scale                                  Aspect\\r\\n                    Variation                                Ratio\\r\\n                                                            Change\\r\\n                           Full                         Low\\r\\n                         Occlusion                   Resolution\\r\\n                                      Fast\\r\\n                                     Motion Out-of-View\\r\\n                       RTS       KeepTrack           STARK-ST101\\r\\n\\r\\n                  Fig. A4. Attributes comparison on LaSOT [16].\\r\\n\\r\\n\\r\\nE    Additional Content\\r\\n\\r\\nFigure A5 shows additional visual results compared to other state-of-the-art\\r\\ntrackers on 6 different sequences of LaSOT [16]. For more content, we refer the\\r\\nreader to: https://github.com/visionml/pytracking.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. A5. Qualitative results on LaSOT [16] of our approach compared to the previous\\r\\nstate-of-the-art methods KeepTrack [34] and STARK-ST101 [54]. As they do not pro-\\r\\nduce segmentation masks, we represent ours as a red overlay and print for all methods\\r\\nthe predicted bounding boxes with the following color code:\\r\\n\\x97 KeepTrack \\x97 STARK-ST101 \\x97 RTS\\r\\n\\x0c',\n",
       " '                                             Improved Sampling-to-Counting Reductions in\\r\\n                                             High-Dimensional Expanders and Faster Parallel\\r\\n                                                       Determinantal Sampling\\r\\narXiv:2203.11190v1 [cs.DS] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             Nima Anari1 , Callum Burgess1 , Kevin Tian1 , and Thuy-Duong Vuong1\\r\\n                                            1 Stanford University, {anari,callumb,kjtian,tdvuong}@stanford.edu\\r\\n\\r\\n\\r\\n\\r\\n                                                                                     Abstract\\r\\n                                             We study parallel sampling algorithms for classes of distributions defined via determi-\\r\\n                                         nants: symmetric, nonsymmetric, and partition-constrained determinantal point processes.\\r\\n                                         For these distributions, counting, a.k.a. computing the partition function, can be reduced to\\r\\n                                         a simple determinant computation which is highly parallelizable; Csanky proved it is in NC.\\r\\n                                         However, parallel counting does not automatically translate to parallel sampling, as the classic\\r\\n                                         reductions between sampling and counting are inherently sequential. Despite this, we show\\r\\n                                         that for all the aforementioned determinant-based distributions, a roughly quadratic parallel\\r\\n                                         speedup over sequential sampling can be achieved. If the distribution is supported on subsets\\r\\n                                         of size k of a ground set, we show how to approximately produce a sample in O    e (k 12 +c ) time\\r\\n                                         with polynomially many processors for any c > 0.   √ In the special case of symmetric determi-\\r\\n                                         nantal point processes, our bound improves to O e ( k) and we show how to sample exactly in\\r\\n                                         this case.\\r\\n                                             We obtain our results via a generic sampling-to-counting reduction that uses approximate\\r\\n                                         rejection sampling. As our main technical contribution, we show that whenever a distribution\\r\\n                                         satisfies a certain form of high-dimensional expansion called entropic independence, approx-\\r\\n                                         imate rejection sampling can achieve a roughly quadratic speedup in sampling via counting.\\r\\n                                         Various forms of high-dimensional expansion, including the notion of entropic independence\\r\\n                                         we use in this work, have been the source of major breakthroughs in sampling algorithms\\r\\n                                         in recent years; thus we expect our framework to prove useful in the future for distributions\\r\\n                                         beyond those defined by determinants.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                         1\\r\\n\\x0c\\n\\n1 Introduction\\r\\nSampling and counting are intimately connected computational problems. For many classes\\r\\nof distributions defined by weight functions µ : X → R ≥0 , where typically the space X is\\r\\nexponential-sized, the problems of approximately sampling x ∈ X with P [ x] ∝ µ( x) and ap-\\r\\nproximately computing the partition function ∑ x ∈ X µ( x) are polynomial-time reducible to each\\r\\nother [JVV86]. However, this equivalence appears to break down for complexity classes below\\r\\nP. For example, there is no known polylogarithmic-overhead parallel reduction between approx-\\r\\nimate counting and approximate sampling.\\r\\nMotivated by the mysterious relationship between sampling and counting in the parallel algo-\\r\\nrithms world, Anari, Hu, Saberi, and Schild [Ana+20], based on the earlier work of Teng [Ten95],\\r\\nraised the question of designing fast parallel sampling algorithms for several classes of distribu-\\r\\ntions where counting, even exactly, was possible in polylogarithmic time and polynomial work,\\r\\ni.e., in the class NC. The distributions in this challenge set all enjoy fast parallel counting algo-\\r\\nrithms because their partition functions can be written as determinants and determinants are\\r\\ncomputable in NC [Csa75]. Anari, Hu, Saberi, and Schild [Ana+20] solved one of these chal-\\r\\nlenges, and showed how to sample random arborescences in RNC, completing the earlier work\\r\\nof Teng [Ten95] on random spanning trees. However, the algorithms in these two works are\\r\\nhighly tailored to the random spanning tree and random arborescence distributions, and they do\\r\\nnot provide any general recipe for parallel sampling from other distributions.\\r\\nIn this work, we study a general framework to improve the parallel efficiency of sampling-to-\\r\\ncounting reductions. We build on the success of a recent trend in the analysis of random walks\\r\\nand sampling algorithms, where combinatorial distributions are analyzed through the lens of\\r\\nhigh-dimensional expanders [Ana+19; AL20; ALO20]. We show that under one notion of high-\\r\\ndimensional expansion, namely entropic independence [Ana+21b], the sampling-to-counting re-\\r\\nduction can be made roughly quadratically faster using parallelization. We formally define\\r\\nentropic independence in Section 3, see Definition 20.\\r\\nIn our setup, we consider combinatorial distributions defined on size k sets of a ground set of\\r\\nelements [n], which we denote by an unnormalized density\\r\\n                                         \\x12 \\x13\\r\\n                                           [n ]\\r\\n                                      µ:        → R ≥0 .\\r\\n                                            k\\r\\n\\r\\nWe remark that the choice of ([nk ]) is standard in the high-dimensional expanders literature, and\\r\\nmany other domains such as the popular product spaces, can be naturally transformed into ([nk ])\\r\\n[ALO20]. Our access to this distribution is through an oracle that can answer counting queries.\\r\\nGiven any1 set T ⊆ [n], the oracle returns\\r\\n                                     \\x1a          \\x12 \\x13         \\x1b\\r\\n                                                 [n ]\\r\\n                                 ∑ µ(S) S ∈ k , T ⊆ S .\\r\\nOur goal is to use the oracle and output a random set S that approximately follows P [S] ∝ µ(S).\\r\\nThe classical reduction from sampling to counting proceeds by picking the k elements of S one\\r\\nat a time. In each step, conditioned on all previously chosen elements, marginals PS∼µ [i ∈ S |\\r\\nprevious choices] of all remaining elements in the ground set are computed and a new element\\r\\n  1 Note that by querying sets T of size exactly k, a counting query can also return the value of µ on any desired set.\\r\\n\\r\\n\\r\\n\\r\\n                                                          2\\r\\n\\x0c\\n\\nis picked randomly with probability proportional to the conditional marginals. In each step,\\r\\nmarginals can be computed via parallel calls to the counting oracle. However, this procedure\\r\\nis inherently sequential as the choice of each element affects the conditional marginals in future\\r\\niterations. A parallel implementation of this reduction takes time Ω(k). The main question we\\r\\naddress is:\\r\\n      For which µ is there a faster parallel reduction from sampling to counting?\\r\\nOur main result establishes that for distributions µ which are good high-dimensional expanders,\\r\\nmeasured in terms of the notion of entropic independence [Ana+21b], the sampling-to-counting\\r\\nreduction can be sped up roughly quadratically. Throughout (e.g. in Theorem 1), we use O      e (·)\\r\\nto hide logarithmic factors in n and failure probabilities; these factors primarily come from the\\r\\nparallel complexity of linear algebra (e.g. evaluating determinants and partition functions).\\r\\n\\r\\nTheorem 1 (Main, informal, see Theorem 33). Let µ : ([nk ]) → R ≥0 be O(1)-entropically independent,\\r\\nand assume that we have access to a counting oracle for µ. For any constant c > 0 and any ǫ ∈ (0, 1),\\r\\n   \\x10√exists\\r\\nthere    \\x10 \\x11an  algorithm that can sample from a distribution within total variation distance ǫ of µ in\\r\\n             c\\x11\\r\\nOe    k·  k\\r\\n                 parallel time using (n/ǫ)O(1/c) machines in the PRAM model of computation.\\r\\n          ǫ\\r\\n\\r\\nWe remark that various notions of high-dimensional expansion and in particular entropic in-\\r\\ndependence have proven useful in the analysis of Markov chains and sequential sampling algo-\\r\\nrithms [Ana+21b; Ana+21c], but this is the first work to relate these notions to parallel algorithms.\\r\\nWe also note that entropic independence is not a binary property of a distribution, but rather ev-\\r\\nery distribution µ has some parameter of entropic independence ∈ [1, k] and the above result\\r\\napplies for all distributions whose entropic independence parameter is O(1). See Theorem 33 in\\r\\nSection 6 for details. This is also the exact condition that implies fast mixing of Markov chains\\r\\nand a polynomial runtime for sequential sampling algorithms [Ana+21b].\\r\\nRemark 2 (Beyond determinantal distributions). In this work, we explore the applications of The-\\r\\norem 1 to distributions defined via determinants defined in the next section; this is due to the\\r\\nfact that partition functions of such distributions can be computed in NC, which gives us a fast\\r\\nparallel counting oracle. However, we believe Theorem 1 may find applications beyond determi-\\r\\nnantal distributions in the future. As an example, for distributions whose partition functions do\\r\\nnot have roots in certain regions of the complex plane, Barvinok [Bar18] devised efficient deter-\\r\\nministic approximate counting algorithms, which have been refined by subsequent works [PR17].\\r\\nThese counting algorithms have the potential to be parallelized, as they involve enumerating a\\r\\nsmall number of combinatorial structures, unlike the more involved and inherently sequential\\r\\nMarkov Chain Monte Carlo methods. Recent works [Ali+21; CLV21] have shown that absence\\r\\nof roots in the complex plane implies forms of high-dimensional expansion, including entropic\\r\\nindependence, paving the way for the application of Theorem 1 to such distributions.\\r\\n\\r\\n1.1 Determinantal distributions\\r\\nIn this work, we consider applications of Theorem 1 to various distributions µ that are defined\\r\\nbased on determinants. Prior progress on designing parallel sampling algorithms for problems\\r\\nthat enjoy determinant-based counting has been very limited. Teng [Ten95] showed how to sim-\\r\\nulate random walks on a graph in parallel, which combined with the classic algorithm of Aldous\\r\\n[Ald90] and Broder [Bro89] yielded RNC algorithms for sampling spanning trees of a graph.\\r\\nAnari, Hu, Saberi, and Schild [Ana+20] extended this to sampling arborescences, a.k.a. directed\\r\\n\\r\\n\\r\\n\\r\\n                                                  3\\r\\n\\x0c\\n\\nspanning trees, of directed graphs. In this work we tackle a much larger class of problems that\\r\\nenjoy determinant-based counting, namely variants of determinantal point processes.\\r\\nDeterminantal point processes (DPPs) have found many applications, such as data summa-\\r\\nrization [Gon+14; LB12], recommender systems [GPK16; Wil+18], neural network compression\\r\\n[MS15], kernel approximation [LJS16], multi-modal output generation [Elf+19], and randomized\\r\\nnumerical linear algebra [DM21]. Formally, a DPP on a set of items [n] = {1, . . . , n} is a probabil-\\r\\nity distribution over subsets Y ⊆ [n] defined via an n × n matrix L where probabilities are given\\r\\n(proportionally) by principal minors: P[Y ] ∝ det( LY,Y ).\\r\\nNote that for the distribution to be well-defined, all principal minors of L have to be ≥ 0. For\\r\\nsymmetric L (L = L⊺ ), having nonnegative principle minors is equivalent to L being positive\\r\\nsemi-definite (PSD). Symmetric DPPs, where L = L⊺ is a PSD matrix, have received the most\\r\\nattention in the literature.\\r\\nDefinition 3 (Symmetric DPP). Given a symmetric n × n matrix L \\x17 0, the symmetric DPP\\r\\ndefined by L is the probability distribution over subsets Y ⊆ [n], where P [Y ] ∝ det( LY,Y ).\\r\\nBeyond the (symmetric) determinantal point proceses defined above, our work provides sam-\\r\\npling algorithms for a variety of discrete distributions related to determinants, which serve dif-\\r\\nferent roles in modeling applications. In the remainder of the section, we will outline each family\\r\\nof distributions.\\r\\nRecently, [Bru18; Gar+19; Gar+20] initiated the study of non-symmetric DPPs in applications\\r\\nand argued for their use because of their increased modeling power. Non-symmetric DPPs are\\r\\ncharacterized by a non-symmetric positive-definite matrix L, i.e., a matrix L where L + L⊺ \\x17\\r\\n0. Symmetric DPPs necessarily exhibit strong forms of negative dependence [BBL09], which\\r\\nare unrealistic in some applications; non-symmetric DPPs on the other hand, can have positive\\r\\ncorrelations. As an example application, a good recommender system for online shopping should\\r\\nmodel complementary items, such as tablets and tablet pens, as having positive correlation; non-\\r\\nsymmetric DPPs can model such positive interactions.\\r\\nDefinition 4. A matrix L ∈ R n×n is non-symmetric positive semidefinite (nPSD) if L + L⊺ \\x17 0.\\r\\nDefinition 5 (Non-symmetric DPP). Given an nPSD n × n matrix L, the non-symmetric DPP\\r\\ndefined by it is the probability distribution over subsets Y ⊆ [n] given by P [Y ] ∝ det( LY,Y ).\\r\\nA related and more commonly used model related to DPPs, is a k-DPP, where we constrain the\\r\\ncardinality of the sampled set Y to be exactly k. In many applications restricting to sets of a\\r\\npredetermined size is more desirable [KT12b].\\r\\nDefinition 6 (k-DPP). Given a PSD or nPSD matrix L, the k-DPP defined by it is the distribution\\r\\nof the corresponding determinantal point process restricted to only k-sized sets.\\r\\nA natural generalization of simple cardinality constraints on DPPs are DPPs under partition\\r\\nconstraints [Cel+16]. Partition constraints arise naturally when there is an inherent labeling or\\r\\ngrouping of the ground set items that is not captured by the DPP kernel itself. More concretely,\\r\\nsuppose the ground set [n] is partitioned into disjoint sets [n] = V1 ∪ V2 ∪ · · · ∪ Vr , and we want\\r\\nto produce a subset S with c1 items from V1 , c2 items from V2 and so on. We define Partition-DPP\\r\\nas the corresponding conditioning of the DPP under these constraints on S. Celis, Deshpande,\\r\\nKathuria, Straszak, and Vishnoi [Cel+16] established that efficiently sampling and counting from\\r\\nPartition-DPPs is possible when the number of constraints is O(1) and that counting is #P-hard\\r\\nwhen the number of constraints is unbounded – it includes as a special case the problem of\\r\\n\\r\\n                                                  4\\r\\n\\x0c\\n\\ncomputing mixed discriminants. In this paper, we will only study Partition-DPPs when the\\r\\nensemble matrix L is symmetric PSD and the number of constraints is O(1). Alimohammadi,\\r\\nAnari, Shiragur, and Vuong [Ali+21] showed that local Markov chains can be used to sample\\r\\nfrom these Partition-DPPs.\\r\\nDefinition 7 (Partition-DPP). Given a symmetric n × n matrix L \\x17 0 and a partitioning of [n] =\\r\\nV1 ∪ V2 ∪ · · · ∪ Vr into r = O(1) partitions together with c1 , . . . , cr ∈ Z ≥0 , the Partition-DPP is the\\r\\ndistribution of the DPP defined by L restricted to sets S that have |S ∩ Vi | = ci for all i.\\r\\nIn this work, we establish as corollaries of Theorem 1, a roughly quadratic parallel speedup in\\r\\nsampling from all of the aforementioned distributions. A crucial part of our algorithm relies on\\r\\nthe existence of highly parallel counting oracles for these models. For example, for unconstrained\\r\\nDPPs, the partition function can be written as\\r\\n\\r\\n                                            ∑ det( LS,S ) = det( L + I ),\\r\\n                                             S\\r\\n\\r\\nand this can be computed in NC [Csa75]. For k-DPPs and Partition-DPPs, the partition function\\r\\ncan be computed via polynomial interpolation [Cel+16], which is again highly parallelizable\\r\\n(by, e.g., solving linear systems of equations involving Vandermonde matrices). The entropic\\r\\nindependence of all the determinantal distributions discusses in this work was established by\\r\\nAlimohammadi, Anari, Shiragur, and Vuong [Ali+21] and Anari, Jain, Koehler, Pham, and Vuong\\r\\n[Ana+21b].\\r\\nTheorem 8 (Sampling from non-symmetric DPPs). Let L be a n × n non-symmetric PSD matrix,\\r\\nǫ ∈ (0, 1), and k ∈ [n].\\r\\n\\r\\n   1. Let µk : ([nk ]) → R ≥0 be the k-DPP defined by L. For any constant c > 0, there exists an algorithm\\r\\n                                                                                  \\x10√        \\x11\\r\\n      to approximately sample from within ǫ total variation distance of µk in Oe     k( k )c parallel time\\r\\n                                                                                                ǫ\\r\\n      using (n/ǫ)O(1/c) machines.\\r\\n   2. Let µ : 2[n] → R ≥0 be the DPP defined by L. For any constant c > 0,√there exists\\r\\n                                                                                    \\x01   an algorithm to\\r\\n                                                                          e\\r\\n      approximately sample from within ǫ total variation distance of µ in O n( nǫ )c parallel time using\\r\\n      (n/ǫ)O(1/c) machines.\\r\\nTheorem 9 (Sampling from partition-DPPs). Let L be a n × n symmetric PSD matrix. Let r = O(1),\\r\\nand let V1 ∪ · · · ∪ Vr = [n] be a partition of [n] together with integers t1 , . . . , tr . Let k = ∑i∈[r ] ti . Let\\r\\nµ L;V,t : 2[n] → R ≥0 be the DPP with partition constraints defined by\\r\\n                                                                 r\\r\\n                                    µ L;V,t (S) ∝ det( LS,S ) · ∏ 1 [|S ∩ Vi | = ti ].\\r\\n                                                                i=1\\r\\n\\r\\nFor any constant c > \\x10   0, there exists an algorithm to approximately sample from within ǫ total variation\\r\\n                          √ k \\x11\\r\\n                       e\\r\\ndistance of µ L;V,t in O     k( )c parallel time using (n/ǫ)O(1/c) machines.\\r\\n                                ǫ\\r\\n\\r\\nIn the case of symmetric DPPs√and symmetric k-DPPs, we are able to improve Theorem 1 to\\r\\n                             e ( k). Our algorithms below have a small chance δ of failure, but\\r\\nobtain a parallel runtime of O\\r\\nconditioned on success they sample exactly from the desired distribution; this is desirable, as we\\r\\ncan repeat the algorithm in the case of failure, to sample exactly from the desired distribution.\\r\\nTheorem 10 is proven in Section 4.\\r\\n\\r\\n\\r\\n                                                            5\\r\\n\\x0c\\n\\nTheorem 10 (Sampling from symmetric DPPs). Let L be a n × n symmetric PSD matrix, k ∈ [n], and\\r\\nδ ∈ (0, 1).\\r\\n\\r\\n   1. Let µk : ([nk ]) → R ≥0 be the k-DPP defined by L. There exists an algorithm that with probability\\r\\n                                             √\\r\\n                                          e ( k) parallel time using poly(n) · log k machines.\\r\\n      ≥ 1 − δ, exactly samples from µk in O                                        δ\\r\\n\\r\\n   2. Let µ : 2[n] → R ≥0 be the DPP defined\\r\\n                                           √ by L. There exists an algorithm, nthat with probability\\r\\n                                         e\\r\\n      ≥ 1 − δ, exactly samples from µ in O( n) parallel time using poly(n) · log δ machines.\\r\\nWe are also able to refine our results about DPPs so that the runtime is expressed in terms of\\r\\ntypical sizes of the sets S in the support, as measured by eigenvalues or traces of the matrix L.\\r\\nWe leave the details to Section 5, where we prove Theorem 28.\\r\\n\\r\\n1.2 Techniques and algorithms\\r\\nThroughout, we heavily use the fact that for all distributions µ that we study in this paper, the\\r\\nmarginals P S∼µ [i ∈ S] can be computed in NC, and that the distributions µ are self-reducible —\\r\\nby conditioning on element inclusion, we obtain another distribution in the same family of DPP\\r\\nvariants. These two properties alone are the basis of the most classical (inherently sequential)\\r\\nalgorithm for sampling from DPPs, which we describe below.\\r\\n\\r\\nfor i = 1, . . . , k do\\r\\n    Compute the marginals of µ conditioned on elements x1 , . . . , xi−1 .\\r\\n    Sample an element outside x1 , . . . , xi−1 with probability proportional to the computed\\r\\n     marginals. Call the sampled element xi .\\r\\nreturn { x1 , . . . , xk }.\\r\\n\\r\\nWe show that rejection sampling can be used to speed up this algorithm. Roughly speaking, we\\r\\ncompute marginals of µ, and sample a batch of elements x1 , . . . , xℓ i.i.d. from these marginals.\\r\\nWe then use rejection sampling to accept or reject the batch to make sure any set { x1 , . . . , xℓ }\\r\\nis selected with probability given by the ℓ-order marginals ∝ PS∼µ [{ x1 , . . . , xℓ } ⊆ S]. Once we\\r\\nhave a batch of elements successfully accepted, we continue sampling the next batch from the\\r\\ndistribution conditioned on including this batch. A high-level description of this algorithm can\\r\\nbe seen in Algorithm 1. Our innovation is to implement the batch sampling step highlighted via\\r\\n(*) by i.i.d. sampling from marginals and performing a correction based on rejection sampling.\\r\\n\\r\\nAlgorithm 1: Batched sampling\\r\\nInput: µ : ([nk ]) → R ≥0\\r\\nk0 ← k\\r\\nµ ( 0) ← µ             √\\r\\nfor i = 0, 1, . . . , 2 k do\\r\\n                                           √\\r\\n      (*): Sample Ti ∼ µ(i) with | Ti | = ⌈ ki ⌉\\r\\n      Update µ(i+1) ← µ(i) (· | Ti )\\r\\n      Update ki+1 = ki − | Ti |\\r\\n               S\\r\\nreturn T := i Ti .\\r\\n\\r\\nClearly, the sizes of the batches we sample dictate the parallel runtime\\r\\n                                                                  √      of this algorithm. Even\\r\\nfor symmetric DPPs, there is a natural barrier at batch size ℓ ≃ k. Consider L to be the Gram\\r\\n\\r\\n\\r\\n                                                   6\\r\\n\\x0c\\n\\nmatrix of vectors {e1 , e1 , . . . , ek , ek } (where every standard basis vector ei ∈ R n is repeated twice,\\r\\nso k = n2 ). The marginals of this DPP are uniform, but because of the Birthday Paradox, any\\r\\n              √\\r\\nsample of ≫ k elements contains a pair of identical vectors       √    with high probability, resulting in a\\r\\nDPP weight of 0. Hence, we must set the batch size ℓ . k to have a good acceptance probability.\\r\\nA significant difficulty that arises for DPP variants beyond symmetric DPPs is the lack of negative\\r\\ndependence. This not only poses an analysis challenge, but presents an algorithmic difficulty as\\r\\nwell. Roughly speaking, due to the lack of negative dependence, the acceptance probabilities\\r\\nused in symmetric DPPs for rejection sampling have to be scaled down in other cases by a factor\\r\\nof ≃ 2ℓ ; otherwise we would sometimes have to accept with probability > 1. We overcome\\r\\nthis challenge by replacing rejection sampling with approximate rejection sampling, where we\\r\\nallow the acceptance probabilities to go above 1 on a small subset of the event space, and if we\\r\\nsee any batch of this kind we declare the algorithm has failed. Our main insight is that such\\r\\nbad batches of elements must consist of large groups of highly correlated elements. On the other\\r\\nhand, we quantify limits on correlations in all of our models using the recently established notion\\r\\nof entropic independence [Ana+21b]. Intuitively, we prove that correlations in our model must\\r\\n                                                            1\\r\\nbe limited to small groups of elements and a batch of ≃ k 2 −ǫ elements will, with high probability\\r\\nnot contain more than one element from the same group of highly correlated elements. We\\r\\nformalize this approach to prove Theorems 8 and 9 in Section 6. Finally, we give an example\\r\\nshowing that this sub-polynomial overhead in the parallel depth is likely to be necessary for our\\r\\nbatched rejection sampling approach in Section 7.\\r\\n\\r\\n1.3 Further related work\\r\\nThe prior works of [Ten95; Ana+20] study the problems of sampling spanning trees and arbores-\\r\\ncences from graphs in parallel. Spanning trees are a special case of DPPs, but there are specialized\\r\\nalgorithms for sampling from spanning trees and arborescences [Ald90; Bro89] that were paral-\\r\\nlelized in prior works; as far as we know the random-walk-based algorithms of Aldous [Ald90]\\r\\nand Broder [Bro89] have no counterpart for general DPPs.\\r\\nBeyond determinant-based distributions, Feng, Hayes, and Yin [FHY21], and more recently Liu\\r\\nand Yin [LY21], showed how to efficiently parallelize a popular class of Metropolis Markov chains\\r\\nand obtain nearly optimal parallelism for several graphical models such as the hardcore and Ising\\r\\nmodels, as well as proper colorings. While their results are stated more generally for arbitrary\\r\\nMetropolis chains satisfying certain Lipschitz conditions on log-densities, they do not directly\\r\\napply to our models. While there are efficient Markov chains for sampling from DPPs [AOR16;\\r\\nHS19; Ali+21], the Metropolis versions of these Markov chains (where a rejection filter is applied)\\r\\ndo not have a nearly-linear mixing time; in fact, even for the simplest case of symmetric DPPs,\\r\\nthey have at least a quadratic mixing time, O(nk) in the case of symmetric DPPs (this is not to be\\r\\nconfused with non-Metropolis versions of these chains which do have linear mixing time). The\\r\\nresults of Feng, Hayes, and Yin [FHY21] can only achieve a linear speedup, i.e., a reduction by a\\r\\nfactor of n, for Metropolis chains, which is moot by the existence of Õ(k) parallel time sampling\\r\\nalgorithms for k-DPPs.\\r\\n\\r\\n1.4 Acknowledgements\\r\\nNima Anari and Thuy-Duong Vuong are supported by NSF CAREER Award CCF-2045354, a\\r\\nSloan Research Fellowship, and a Google Faculty Research Award. Callum Burgess was sup-\\r\\nported by a Stanford CURIS Fellowship. Kevin Tian was supported by a Google Ph.D. Fellowship\\r\\n\\r\\n                                                     7\\r\\n\\x0c\\n\\nand a Simons-Berkeley VMware Research Fellowship.\\r\\n\\r\\n\\r\\n2 Overview of approach\\r\\nIn this section, we give a technical outline of how we prove our main results. We also provide a\\r\\nroadmap of the rest of the paper. All preliminaries can be found in Section 3.\\r\\n\\r\\nSymmetric DPPs. In Section 4, we prove our most basic result, Theorem 10 (sampling symmet-\\r\\nric DPPs), as an introduction to our techniques and to demonstrate how we apply Algorithm 1.\\r\\nConveniently, symmetric DPPs exhibit strong negative dependence properties which the rest of\\r\\n                               √ Theorem 10, we directly bound the acceptance probability of\\r\\nour applications do not. To prove\\r\\nAlgorithm 1 for batch size ℓ ≃ k. Specifically, we show that for such a batch size, directly apply-\\r\\n                                                                        2\\r\\ning negative dependence bounds the acceptance probability by exp(− ℓk ). By a simple recursion\\r\\n                                                                    √\\r\\nargument to bound the overall parallel depth of our sampler by O( k), we obtain Theorem 10.\\r\\n\\r\\nBounded symmetric DPPs. In Section 5, we give improved sampling guarantees in terms of\\r\\nparallel depth for DPPs with kernel matrices exhibiting bounded spectral structure by proving\\r\\nTheorem 28. The two types of spectral bounds we consider are parameterized by the kernel\\r\\nmatrix trace and largest eigenvalue. To obtain the first result, we apply a concentration bound\\r\\nfrom [PP13] to show that the size of the sample concentrates tightly around its mean, which by\\r\\nlinearity of expectation is the trace. This implies by a strategy outlined in Remark 12 that with\\r\\nhigh probability, the parallel depth is bounded by a function of the trace, via our algorithm in\\r\\nTheorem 10.\\r\\nFor our other result, we appeal to an alternative analysis of our rejection sampling, which uses\\r\\nproperties of negatively-correlated distributions. We use Algorithm 4, a modification of Algo-\\r\\nrithm 1 based on directly manipulating the kernel matrix, as our base method. We show that\\r\\nif the kernel matrix is scaled down by a factor α so its largest eigenvalue is bounded by ≈ √1n ,\\r\\nwe can achieve polylogarithmic parallel depth for each run of Algorithm 4. We also prove a\\r\\ncharacterization of this scaling down procedure as randomly dropping elements from a sample\\r\\nfrom the original DPP, so that each element is kept with probability α. By setting α ≈ λ (1K)√n ,\\r\\n                                                                                        max\\r\\nstandard binomial concentration bounds the number of calls to this “scaled-down sampler” and\\r\\nyields the other half of Theorem 28.\\r\\n\\r\\nEntropic independence. In Section 6, we provide a meta-result (Theorem 33, a formal restate-\\r\\nment of Theorem 1) used to derive Theorems 8 and 9 as corollaries. Theorem 33 shows that for\\r\\nany constant-entropically independent distribution supported on subsets of size k, we can reduce\\r\\n                                                                  e (k 12 +c ) with high probability.\\r\\nsampling to marginal computations at a parallel depth overhead of O\\r\\nHere, c is any constant, which parameterizes the (polynomial) number of machines used.\\r\\nTo demonstrate Theorem 33, we use the assumed entropic independence property to derive\\r\\nconcentration bounds on the acceptance probability in Algorithm 3 applied to our distribution.\\r\\nAs a first step towards this goal, we use entropic independence to demonstrate that up to parallel\\r\\n               1\\r\\ndepth ℓ ≈ k 2 −c , the KL divergence between our target distribution (the ℓ-marginals) and our\\r\\nproposal distribution (the product distribution on 1-marginals) is bounded.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 8\\r\\n\\x0c\\n\\nThis KL divergence bound does not suffice for our overall scheme; intuitively, it provides an “av-\\r\\nerage case” bound on the log-acceptance probability of Algorithm 3, whereas√ we would like to\\r\\nargue a high probability bound, since we need to union bound over at least k stages of rejection\\r\\nsampling. To simplify our concentration argument, we begin by assuming without loss of gener-\\r\\nality that our distribution has roughly even 1-marginals, by using a subdivision process used in\\r\\n[AD20; Ana+21a]. We then use comparison inequalities between KL divergences and (exponen-\\r\\ntiated) Renyi divergences for nearly-uniform distributions to bound moments associated with\\r\\nour rejection sampler’s acceptance probabilities. Finally, we use these moment bounds to show\\r\\nthat over a high-probability set of outcomes (in the sense of Algorithm 3), the log-acceptance\\r\\nprobability is a submartingale, which yields concentration via Markov’s inequality.\\r\\n\\r\\nHard instance. It is natural to ask: can we improve the subpolynomial overhead in Theorem 33\\r\\n(and hence, Theorems 8 and 9) to a smaller overhead, e.g. polylogarithmic? In Section 7, we\\r\\ngive a hard example showing that the subpolynomial overhead may be inherent to rejection\\r\\nsampling strategies, at least in the full generality of entropically independent distributions. In\\r\\nparticular, our hard instance pairs indices in [n], randomly chooses 2k of these pairs, and then\\r\\nincludes both elements in each of the 2k selected pairs. This distribution is a k-nonsymmetric\\r\\n                                                                                      \\x14      \\x15\\r\\n                                                           n                            0 −1\\r\\nDPP defined by a block-diagonal matrix L composed of 2 2 × 2 blocks, all equal to              . To\\r\\n                                                                                        1 0\\r\\ndemonstrate hardness, we first argue that to succeed with good probability on a polynomially-\\r\\nbounded number of parallel machines, our rejection sampler must have the property that it is\\r\\nlikely only a constant number of “duplicates” are encountered when drawing samples from\\r\\nthe product distribution. We then show using a Birthday Paradox-like\\r\\n                                                                  √       analysis that under this\\r\\nrestriction, our batch size ℓ must be polynomially smaller than k, yielding our claim.\\r\\n\\r\\n\\r\\n3 Preliminaries\\r\\nIn this section, we provide preliminaries for the rest of the paper.\\r\\nWe use [n] to denote the set {1, . . . , n}. For a set S, (Sk) denotes the family of subsets of size k.\\r\\n\\r\\nFor a distribution µ : 2[n] → R ≥0 and T ⊆ [n], define µ(· | T ) to be the distribution on 2[n]\\\\ T\\r\\ndefined by µ( F | T ) ∝ µ( F ∪ T ). We will sometimes use the shorthand µ| T .\\r\\n\\r\\nFor density function µ : ([nk ]) → R ≥0 , the generating polynomial of µ is the multivariate k-\\r\\nhomogeneous polynomial defined as follows:\\r\\n\\r\\n                                    gµ ( z1 , . . . , z n ) =     ∑ µ( S ) ∏ zi .\\r\\n                                                                S ∈([nk ])   i∈S\\r\\n\\r\\n\\r\\n\\r\\n3.1 Determinantal point processes\\r\\nA DPP on n items defines a probability distribution over subsets Y ⊆ [n]. It is parameterized by\\r\\na matrix L ∈ R n×n : P L [Y ] ∝ det( LY ), where LY is the principal submatrix whose columns and\\r\\nrows are indexed by Y. We call L the ensemble matrix. We define the marginal kernel K of P L by\\r\\n\\r\\n                           K = L ( I + L ) −1 = I − ( I + L ) −1 = ( L −1 + I ) −1 .                      (1)\\r\\n\\r\\n\\r\\n\\r\\n                                                            9\\r\\n\\x0c\\n\\nThen, det(K A ) = P L [ A ⊆ Y ] (a proof can be found in [KT12a]). This also implies K \\x16 I for\\r\\nsymmetric K. Conversely,\\r\\n\\r\\n                           L = K ( I − K ) −1 = ( I − K ) −1 − I = ( K −1 − I ) −1 .                    (2)\\r\\n\\r\\nGiven a cardinality constraint k, the k-DPP parameterized by L is a distribution over subsets Y\\r\\n                                     det( L )\\r\\nof size k, defined by P kL [Y ] = ∑ ′ detY( L ′ ) . To ensure that P L defines a probability distribution,\\r\\n                                    |Y |= k   Y\\r\\nall principal minors of L must be non-negative: det( LS ) ≥ 0. Matrices that satisfy this property\\r\\nare called P0 -matrices [Fan89, Definition 1]. Any nonsymmetric (or symmetric) PSD matrix is\\r\\nautomatically a P0 -matrix [Gar+19, Lemma 1].\\r\\nConsider a matrix L ∈ R n×n , partition V1 ∪ · · · ∪ Vr = [n] of [n], and tuple {ci }ri=1 of integers. The\\r\\nDPP with partition constraint (Partition-DPP) µ L;V,c : 2[n] → R ≥0 is defined by\\r\\n\\r\\n                                µ L;V,c (S) ∝ 1[∀i : |S ∩ Vi | = ci ] det( LS,S )\\r\\n\\r\\nFor any Y ⊆ [n], if we condition the distribution P L (P kL resp.) on the event that items in Y are\\r\\nincluded in the sample, we still get a DPP ((k − |Y |)-DPP resp.); the new ensemble matrix is\\r\\n                                                 −1\\r\\ngiven by the Schur complement LY = LỸ − LỸ,Y LY,Y LY,Ỹ where Ỹ = [n] \\\\ Y.\\r\\nFor Partition-DPPs, a similar statement holds. Conditioning µ L;V,c on Y being included in the set\\r\\nresults in a Partition-DPP µ LY ;V ′ ,c′ with ensemble matrix LY and partition V1′ ∪ · · · ∪ Vr′ = [n] \\\\ Y\\r\\nwith Vi′ = Vi \\\\ Y, and c′i = ci − |Vi ∩ Y |.\\r\\nProposition 11. Suppose µ is one of the following distributions.\\r\\n   1. k-DPP: µ(S) ∝ 1[|Y | = k] det( LS ).\\r\\n   2. DPP: µ(S) ∝ det( LS ).\\r\\n   3. Partition-DPP: µV,c (S) ∝ 1[∀i ∈ [r] : |S ∩ Vi | = ci ] det( LS ) with r = O(1).\\r\\n                                                         e (1)-parallel time using poly(n) machines.\\r\\nThere are algorithms that perform the following tasks in O\\r\\n   1. Given S ⊆ T ⊆ [n], exactly computes the conditional probabilities µ( T | S).\\r\\n   2. Given S ⊆ [n] and integer t ∈ [n], exactly computes P T ∼µ [| T | = t].\\r\\n\\r\\nProof of Proposition 11. First, note that DPPs and k-DPPs correspond to Partition-DPPs with 0\\r\\nand 1 partition constraints respectively. Thus, we only need to show the claim for µ being\\r\\na Partition-DPP with O(1) constraints. Computing the marginals is equivalent to computing\\r\\nthe partition functions of µ and of µ conditioned on subsets. As shown in [Cel+17, Theorem\\r\\n1.1], computing the partition function is equivalent to computing the coefficients of a certain\\r\\nunivariate polynomial which can be evaluated efficiently given access to gµ . Evaluating gµ at\\r\\n                                                                                             e (1)-parallel\\r\\n(z1 , . . . , zn ) is equivalent to computing det( L + diag(zi )ni=1 ), which can be done in O\\r\\ntime [Ber84].\\r\\n\\r\\nRemark 12. To sample from a DPP µ : 2[n] → R ≥0 , we can first in constant parallel-time compute\\r\\nthe distribution H on [n] defined by P H [k] := P S∼µ [|S| = k] and sample the cardinality of the set\\r\\nk from H, and then sample S from µk using the results of this paper.\\r\\n\\r\\n\\r\\n\\r\\n                                                       10\\r\\n\\x0c\\n\\n3.2 Real-stable polynomials and negative correlation\\r\\nIn this section, we define various polynomial properties and their relationships.\\r\\n                                                                 \\x08\\r\\nDefinition 13. Consider an open half-plane Hθ = e−iθ z Im(z) > 0 ⊆ C. We say a polynomial\\r\\ng(z1 , · · · , zn ) ∈ C [z1 , · · · , zn ] real-stable if g has real coefficients and does not have any root in\\r\\nH0n . In particular, the zero polynomial is real-stable.\\r\\nWe say that distribution µ : 2[n] → R ≥0 is strongly Rayleigh if and only if its generating polyno-\\r\\nmial is real stable [see BBL09]. If µ is strongly Rayleigh then for any set F ⊆ [n], the conditional\\r\\ndistribution µ(· | F ) is also strongly Rayleigh. A useful property of strongly Rayleigh polynomi-\\r\\nals is that they satisfy negative correlation, defined in the following.\\r\\nLemma 14 (Negative correlation). Suppose µ : 2[n] → R ≥0 is strongly Rayleigh. For any set T\\r\\n                                      P S ∼ µ [ T ⊆ S ] ≤ ∏ P S ∼ µ [i ∈ S ].\\r\\n                                                           i∈ T\\r\\n\\r\\n\\r\\nLemma 15. Let L be a symmetric PSD matrix. The following distributions are strongly Rayleigh.\\r\\n\\r\\n   1. µk : ([nk ]) → R ≥0 , the k-DPP defined by L.\\r\\n\\r\\n   2. µ : 2[n] → R ≥0 , the DPP defined by L.\\r\\nCorollary 16. Let K ∈ R n×n satisfy 0 \\x16 K \\x16 I. For any set T ⊆ [n],\\r\\n                                                 det(K T ) ≤ ∏ Ki,i\\r\\n                                                               i∈ T\\r\\n\\r\\nProof. Consider the DPP µ with kernel matrix K. Apply Lemma 14 and note that det(K T ) =\\r\\nP S∼µ [ T ⊆ S] and Ki,i = PS∼µ [i ∈ S].\\r\\n\\r\\n\\r\\n                                                                                                          √\\r\\nLemma 17. Let µ : 2[n] → R ≥0 be a strongly Rayleigh distribution. Suppose E S∼µ [|S|] ≤                       n. For\\r\\nǫ ∈ (0, 14 ), there exists an absolute constant c > 0 such that\\r\\n                                              \"        r          #\\r\\n                                                                1\\r\\n                                         P S∼µ |S| ≥ c n log        ≤ǫ\\r\\n                                                                ǫ\\r\\n\\r\\nand                                         \\x14                             \\x15\\r\\n                                                                        1\\r\\n                                    P S∼µ       |S| ≥ c E S∼µ [|S|] log     ≤ǫ\\r\\n                                                                        ǫ\\r\\n\\r\\nProof. Let f (S) = |S| correspond to the 1-Lipschitz (with respect to the Hamming metric) func-\\r\\ntion of taking the magnitude of a sample S from µ, and let f indicate the value E S∼µ [ f ].\\r\\nBy applying [PP13, Theorem 3.2], it follows that\\r\\n                                                                      \\x12                 \\x13\\r\\n                                                                              − a2\\r\\n                                 P S∼µ [ f − f > a] ≤ 3 exp                               .\\r\\n                                                                          16( a + 2 f )\\r\\n                                      q                                                                   q\\r\\n                                                                          2\\r\\nFor the first statement, let a = 10       n log 1ǫ , and note exp(− 16( aa+2 f¯) ) ≤ 3ǫ and a + f¯ ≤ 11       n log 1ǫ .\\r\\n                                                       2\\r\\nFor the second, let a = f¯ log 1ǫ and note exp(− 16( aa+2 f¯) ) ≤ 3ǫ and a + f¯ ≤ 2 f¯ log 1ǫ .\\r\\n\\r\\n                                                          11\\r\\n\\x0c\\n\\n3.3 Fractional log-concavity and entropic independence\\r\\nWe recall the notions of fractional log-concavity [Ali+21] and entropic independence [Ana+21b].\\r\\n\\r\\nDefinition 18 ([Ali+21]). A probability distribution µ : ([nk ]) → R ≥0 is α-fractionally-log-concave if\\r\\ngµ (z1α , . . . , zαn ) is log-concave for z1 , . . . , zn ∈ R n≥0 . If α = 1, we say µ is log-concave.\\r\\nTo define entropic independence we need the definition of the “down” operator. In brief, Dk→ℓ\\r\\ntransitions from a set S of size k to a uniformly random subset of size ℓ.\\r\\n                                                                                           ([n ])×([nℓ ])\\r\\nDefinition 19 (Down operator). For ℓ ≤ k define the row-stochastic matrix Dk→ℓ ∈ R ≥k0                      by\\r\\n                                             (\\r\\n                                               0    if T 6⊆ S\\r\\n                               Dk→ℓ (S, T ) = 1\\r\\n                                                k   otherwise.\\r\\n                                                        ( ℓ)\\r\\n\\r\\nNote that for a distribution µ on size-k sets, µDk→ℓ will be a distribution on size-ℓ sets. In\\r\\nparticular, µDk→1 will be the vector of normalized marginals of µ: { 1k P [i ∈ S]}i∈[n] .\\r\\n\\r\\nDefinition 20. A probability distribution µ on ([nk ]) is said to be α1 -entropically independent, for\\r\\nα ∈ (0, 1], if for all probability distributions ν on ([nk ]),\\r\\n                                                                 1\\r\\n                                D KL (νDk→1 k µDk→1 ) ≤             D KL (ν k µ).\\r\\n                                                                 αk\\r\\n\\r\\nLemma 21 ([Ana+21b], Theorem 4). If µ is α-FLC then µ and all conditional distributions of µ, i.e.\\r\\nµ(· | S) for any S ⊆ [n], are 1α -entropically independent.\\r\\nLemma 22 ([Ali+21]). The following distributions are α-FLC for α = Ω(1) and k ∈ [n].\\r\\n   1. k-DPP and DPP defined by nonsymmetric PSD L ∈ R n×n .\\r\\n   2. Partition-DPP defined by symmetric PSD L ∈ R n×n and a partition {Vi }ri=1 with r = O(1).\\r\\n\\r\\n3.4 Rejection sampling\\r\\n                                                                                                     µ( x)\\r\\nConsider distributions µ, ν over the same domain, and parameter C such that maxx ∈supp(ν) ν( x ) ≤\\r\\nC. Assuming sample access to ν, we can also sample from µ via rejection sampling as follows.\\r\\n\\r\\nAlgorithm 2: Rejection sampling\\r\\n                                                      µ( x)\\r\\nInput: parameter C > 0 such that maxx ∈supp(ν) ν( x ) ≤ C.\\r\\nSample x ∼ ν.\\r\\n                                     µ( x)\\r\\nAccept and output x with probability Cν( x ) .\\r\\n\\r\\n\\r\\nWhen Algorithm 2 succeeds, its output distribution is exactly µ, since\\r\\n                                                               µ( x)\\r\\n                            P [Algorithm 2 outputs x] ∝               ν( x ) ∝ µ ( x ).\\r\\n                                                               Cν( x)\\r\\nAlgorithm 2 succeeds with probability\\r\\n                                                      µ( x)           1\\r\\n                                     P [accept] = ∑           ν( x ) = .\\r\\n                                                    x Cν( x )         C\\r\\n\\r\\n                                                      12\\r\\n\\x0c\\n\\nFor any δ ∈ (0, 1), by running C log δ−1 copies of the algorithm in parallel and taking the first\\r\\naccepted copy, we can boost the acceptance rate to 1 − δ, stated formally in the following.\\r\\nProposition 23. There is an algorithm that with probability 1 − δ, outputs a sample from µ in the same\\r\\nasymptotic parallel time as required to sample from ν, using O(Cpoly(n) log 1δ ) machines.\\r\\nWe consider the following modification of Algorithm 2 when we have a weaker assumption that\\r\\n                               µ( x)\\r\\nfor some Ω ⊆ supp(ν) : maxx ∈Ω ν( x ) ≤ C where ∑ x ∈Ω µ( x) ≥ 1 − ǫ for ǫ ∈ [0, 1).\\r\\n\\r\\nAlgorithm 3: Modified rejection sampling\\r\\n                                                                     µ( x)\\r\\nInput: Ω ⊆ supp(ν), parameter C > 0 such that max x ∈Ω ν( x ) ≤ C\\r\\nSample x ∼ ν.\\r\\n                                                µ( x)\\r\\nIf x ∈ Ω, accept and output x with probability Cµ( x ) .\\r\\n\\r\\n\\r\\nThe guarantees of Algorithm 3 follow immediately from Proposition 24 and the fact that the\\r\\nrestriction of µ to Ω has total variation at most ǫ from µ. In particular, we will use Proposition 24\\r\\nwith δ = ǫ and output an arbitrary sample when it fails to accept a sample.\\r\\nProposition 24. There is an algorithm that outputs a sample from µ̃ in the same asymptotic parallel time\\r\\nas required to sample from ν, using O(Cpoly(n) log 1ǫ ) machines, where dTV (µ̃, µ) = O(ǫ).\\r\\n\\r\\n3.5 Divergences\\r\\nLet q, p be distributions over the same finite ground set [n]. We define the KL divergence and,\\r\\nfor λ ≥ 1, the λ-divergence between q and p as follows:\\r\\n                                               \\x14     \\x15             \\x12 \\x13\\r\\n                                                   q                 qi\\r\\n                           DKL (qk p) := Eq log        = ∑ qi log        ,\\r\\n                                                   p     i∈[n ]\\r\\n                                                                     pi\\r\\n                                               \"\\x12 \\x13 #\\r\\n                                                 q λ\\r\\n                            Dλ ( qk p) : = E p          = ∑ qλi p1i −λ .\\r\\n                                                 p         i∈[n ]\\r\\n\\r\\nWe remark that our definition of Dλ is (up to a constant scalar multiplication) the exponential of\\r\\nthe standard Renyi divergence of order λ. These divergences exhibit the following useful bound.\\r\\nLemma 25. Let q, p be distributions over [n]. Suppose for some C ≥ 1 and S ⊆ [n]: pi ≤ Cn for all i ∈ [n]\\r\\n          1\\r\\nand pi ≥ Cn for all i ∈ S. Then, for any λ ≥ 1, if S = [n]\\r\\n                                        \\x10                                       \\x11\\r\\n                    D λ (q k p) ≤ C λ−1 1 + nλ−1 λ(λ − 1)(D KL (q k p) + log C ) .\\r\\n\\r\\nMore generally,\\r\\n                         \\x12        \\x13 λ −1          \\x10                                        \\x11\\r\\n                             qi\\r\\n                  ∑ qi                     ≤ C λ−1 1 + nλ−1 λ(λ − 1)(D KL (q k p) + log C ) .\\r\\n                  i∈S\\r\\n                             pi\\r\\n\\r\\nProof. We first prove the inequality for the case S = [n] and C = 1. Clearly,\\r\\n                                         \\x10                                  \\x11\\r\\n                         f ′ (r) = nλ−1 λ rλ−1 − (λ − 1)(1 + log r + log n)\\r\\n\\r\\n                                                           13\\r\\n\\x0c\\n\\nand                                                        \\x12         \\x13\\r\\n                                                                   1\\r\\n                                   f ′′ (r) = nλ−1 λ(λ − 1) rλ−2 −     ≤ 0.\\r\\n                                                                   r\\r\\nThus f is concave and                                                !        \\x12 \\x13\\r\\n                                  1 n                     1 n                  1    1\\r\\n                                  n i∑                    n i∑\\r\\n                                        f (qi ) ≤ f             qi        = f     =\\r\\n                                     =1                      =1\\r\\n                                                                               n    n\\r\\nwhich is equivalent to\\r\\n                              n                                                  n\\r\\n                             ∑ qλi nλ−1 ≤ 1 + nλ−1 λ(λ − 1) ∑ qi log(nqi ).\\r\\n                             i=1                                               i=1\\r\\n\\r\\nThe case C > 1 then follows from\\r\\n                                    n\\r\\n                   D λ (q k p) = ∑ qλi p1i −λ\\r\\n                                   i=1\\r\\n                                           n\\r\\n                              ≤ C λ−1 ∑ qλi nλ−1\\r\\n                                          i=1\\r\\n                                                                                          !\\r\\n                                                                           n\\r\\n                              ≤ C λ−1 1 + nλ−1 λ(λ − 1) ∑ qi log(nqi )\\r\\n                                                                           i=1\\r\\n                                                                                                     !!\\r\\n                                                                                 n\\r\\n                                                                                        qi\\r\\n                              ≤ C λ − 1 1 + n λ − 1 λ ( λ − 1)                 ∑ qi log pi + log C        .\\r\\n                                                                               i=1\\r\\n\\r\\nThe case S 6= [n] follows by noticing\\r\\n                                                \\x12        \\x13 λ −1       n\\r\\n                                                    qi\\r\\n                                         ∑ qi                     ≤ ∑ qλi p1i −λ .\\r\\n                                         i∈S\\r\\n                                                    pi               i=1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n4 Symmetric DPP\\r\\nHere, we prove our basic result, Theorem 10. We provide a strengthening for DPPs satisfying\\r\\nnontrivial spectral bounds, Theorem 28, in Section 5. We first state helper bounds used in the\\r\\nproof.\\r\\n\\r\\nLemma 26. Suppose µ : ([nk ]) → R ≥0 is negatively correlated. Let µt = µDk→t and pi = Pµ [i ∈ S].\\r\\nThen                                                     \\x12 2\\x13\\r\\n                                       µt ( T )           t\\r\\n                                                pi ≤ exp      .\\r\\n                                    t! ∏ i∈ T k            k\\r\\n\\r\\nProof. Note that\\r\\n                          \\x12 \\x13 −1\\r\\n                           k                                     t!\\r\\n               µt ( T ) =        P S∼µ [ T ⊆ S] =                               P S ∼ µ [ T ⊆ S ].\\r\\n                           t                      k ( k − 1) · · · ( k − t + 1)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                             14\\r\\n\\x0c\\n\\nThus, by negative correlation,\\r\\n\\r\\n                          µt ( T )                        kt                P S∼µ [ T ⊆ S]\\r\\n                                     pi =\\r\\n                        t! ∏ i∈ T k         k ( k − 1) · · · ( k − t + 1) ∏ i ∈ T P S ∼ µ [ i ∈ S ]\\r\\n                                                      kt\\r\\n                                       ≤\\r\\n                                        k ( k − 1) · · · ( k − t + 1)\\r\\n                                           t −1 \\x12         \\x13 ! −1       \\x12 2\\x13\\r\\n                                                        i               t\\r\\n                                       = ∏ 1−                    ≤ exp\\r\\n                                           i=1\\r\\n                                                       k                 k\\r\\n\\r\\n                                                                                 \\x01           \\x102      \\x11\\r\\n                                                  −1                                          t −t\\r\\nwhere we used the facts that 1 − x ≥ e−2x and ∏ ti= 1 exp\\r\\n                                                                            2i\\r\\n                                                                            k        = exp      k        .\\r\\n                                                                           √\\r\\nProposition 27. If step (*) takes O(τ )-parallel time, Algorithm 1 takes O( k · τ )-parallel time.\\r\\n\\r\\nProof. Note that\\r\\n                                                                     \\x13      \\x12p\\r\\n                                               p                  p1 2\\r\\n                         k i+1 = k i − ⌈ k i ⌉ ≤ k i − k i ≤  ki −     .\\r\\n                                                                   2\\r\\n         p        √                                  √       √       √\\r\\nand thus ki+1 ≤ ki − 12 . Hence, since t ≥ 2 k implies kt ≤ k0 − 2t ≤ 0, the algorithm\\r\\n                √                               √\\r\\nterminates in O( k) iterations, and takes O( k) parallel time.\\r\\n\\r\\nProof of Theorem 10. We first consider the case of sampling k-DPPs. By Lemma 15, when µ is a\\r\\nk-DPP defined by symmetric PSD ensemble matrix L, µ and the conditionals of µ are real-stable.\\r\\nIn particular, all µ(i) as defined in Algorithm 1 are real-stable and hence strongly Rayleigh.\\r\\nConsider some loop i, and let µ ≡ µ(i) . We use Lemma 26 to implement step (*). In particular, we\\r\\nfirst compute the marginals pi = P µ [i ∈ S] in O e (1)-parallel time. Next, let ν be the distribution\\r\\n                                            t\\r\\nover ordered tuples (i1 , . . . , it ) ∈ [n] with\\r\\n                                                                       t\\r\\n                                                                           pir\\r\\n                                              ν({i1 , . . . , it }) = ∏        .\\r\\n                                                                      r =1\\r\\n                                                                            k\\r\\n\\r\\n                                                                                                             µ ({i ,...,i })\\r\\nWe can identify µt with the distribution µ∗t over [n]t where µ∗t ({i1 , . . . , it }) = 1    t\\r\\n                                                                                               . Let\\r\\n                                             √                                            t!\\r\\n ′     δ\\r\\nδ = √ , where Algorithm 1 takes at most 2 k iterations by Proposition 27. We run the rejection\\r\\n      2 k\\r\\nsampling algorithm in Proposition 23, which succeeds with probability 1 − δ′ , to sample from µ∗t\\r\\n                                       2                    √\\r\\ngiven samples from ν, with C ≤ exp( tk ) = O(1) since t = ⌈ k⌉. Clearly obtaining a sample from\\r\\nµ∗t yields a sample from µt by forgetting the ordering on the elements.\\r\\n                                           e\\r\\n        √ iteration i of Algorithm 1 takes O(1)-parallel time. By Proposition 27, the\\r\\nHence, each                                                                           √ algorithm\\r\\ntakes O( k)-parallel time. By a union bound, the success probability is at least 1 − 2 kδ′ = 1 − δ.\\r\\nThe number of machines used is the same as in Proposition 23, that is, O(poly(n) log δk ).\\r\\nThe result for DPPs immediately follows from Remark 12.\\r\\n\\r\\n\\r\\n5 Refined guarantees for bounded symmetric DPPs\\r\\nFor symmetric PSD ensemble matrices L with non-trivial eigenvalue or trace bounds, we give the\\r\\nfollowing refined result improving upon Theorem 10 in various interesting parameter regimes.\\r\\n\\r\\n                                                             15\\r\\n\\x0c\\n\\nTheorem 28. Let L be a n × n symmetric PSD matrix and ǫ ∈ (0, 1). Let µ : 2[n] → R ≥0 be the DPP\\r\\ndefined by L. Let K = L( I + L)−1 \\x16 I be the kernel of L. There exists an algorithm to approximately\\r\\nsample from within ǫ total variation distance of µ in\\r\\n                                     \\x12     \\x1aq                       \\x1b\\x13\\r\\n                                                                 √\\r\\n                                   e\\r\\n                                   O min        Tr(K ), λmax (K ) n\\r\\n\\r\\nparallel time using poly(n)( 1ǫ )o(1) machines.\\r\\nWe will use the following “filtered” variant of Algorithm 1.\\r\\n\\r\\nAlgorithm 4: Filtering\\r\\n                  [n]\\r\\n         √ −µ1 : 2 → R ≥0 with kernel K, λmax (K ) ≤ λ.\\r\\nInput: DPP\\r\\nα ← (λ n)\\r\\nif α > 1 then\\r\\n    (1): Sample S ∼ µ and return S.\\r\\nS−1 , K (0) , L(0) ← ∅, K, L\\r\\nfor i = 0, 1, . . . , R do\\r\\n    (2): Sample Ti ∼ DPP with kernel K̃ (i) := αK (i)\\r\\n    Update Si ← Si−1 ∪ Ti\\r\\n    Update L(i+1) ← ((1 − α) L(i) ) Ti (where ( L) T is the ensemble matrix corresponding to the\\r\\n      DPP with ensemble matrix L, conditioned on including T; see Section 3.1)\\r\\n    Update K (i+1) ← I − ( I + L(i+1) )−1\\r\\nOutput SR\\r\\n\\r\\nWe prove Theorem 28 in this section. Our first step is to show that for R = Θ(α−1 log nǫ ), the\\r\\noutput distribution of Algorithm 4 is within ǫ of the target distribution µ.\\r\\nWe require the following helper claims. The first shows that randomly independently dropping\\r\\nelements of a sample from a DPP µ is equivalent to scaling the kernel matrix.\\r\\nProposition 29. Let µ be a DPP with kernel K. Let µ′ be the DPP with kernel K ′ := αK. Let ν be the\\r\\ndistribution obtained by first sampling U ∼ µ, then outputting S ⊆ U with probability α|S| (1 − α)|S| , i.e.\\r\\n\\r\\n                                   ν(S) = ∑ µ(U )α|S| (1 − α)|U |−|S| .\\r\\n                                            U ⊇S\\r\\n\\r\\nThen, µ′ and ν are identical.\\r\\n\\r\\nProof. Given a set A, we have P S∼µ′ [ A ⊆ S] = det((αK ) A ) = α| A| det(K A ) = α| A| ∑U ⊇ A µ(U ). On\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                    16\\r\\n\\x0c\\n\\nthe other hand, we have\\r\\n                            P S ∼ ν [ A ⊆ S ] = ∑ ν( S )\\r\\n                                                         S⊇ A\\r\\n\\r\\n                                                     =     ∑       µ(U )α|S| (1 − α)|U |−|S|\\r\\n                                                         U ⊇S⊇ A\\r\\n\\r\\n                                                     = α| A|       ∑       µ(U )α|S|−| A| (1 − α)|U |−|S|\\r\\n                                                                 U ⊇S⊇ A\\r\\n                                                          | A|                                   ′                       ′\\r\\n                                                     =α          ∑ µ (U ) ′ ∑               α|S | (1 − α)|U \\\\ A|−|S |\\r\\n                                                                 U⊇A             S ⊆U \\\\ A\\r\\n                                                          | A|\\r\\n                                                     =α          ∑ µ (U ).\\r\\n                                                                 U⊇A\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProposition 30. Consider the setup of Algorithm 4. Suppose α ≤ 1. Let P i denote the distribution of Si .\\r\\nFix ǫ > 0. For i = Ω(α−1 log nǫ ) for a sufficiently large constant,\\r\\n                                                                 dTV (P i , µ) ≤ ǫ.\\r\\n\\r\\nProof. Let µ(i) be the DPP with ensemble matrix L(i) (and kernel matrix K (i) ), and let ν(i) be the\\r\\nDPP with kernel matrix αK (i) . We will prove by induction that for all i,\\r\\n                          P i [Si ] = ∑ µ(0) (U )(1 − α)(i+1)(|U |−|Si|) (1 − (1 − α)i+1 )|Si | .\\r\\n                                          U ⊇ Si\\r\\n\\r\\nThe base case i = 0 follows from Proposition 29. Now, supposing the induction hypothesis holds\\r\\nfor some i − 1, we show that it also holds for i. In the following, let S0 be the set sampled in the\\r\\nfirst iteration of Algorithm 4, and let P i [Si | S0 ] denote the probability we observe Si conditioned\\r\\non the value of S0 . The induction hypothesis then yields the probability we observe Si \\\\ S0 in the\\r\\nnext i − 1 iterations, with the starting matrix L(1) ← ((1 − α) L)S0 as follows:\\r\\n                P i [Si | S0 ] = ∑ µ(1) (U \\\\ S0 )(1 − α)i(|U \\\\S0 |−|Si \\\\S0 |) (1 − (1 − α)i )|Si \\\\S0 | .\\r\\n                                        U ⊇ Si\\r\\n\\r\\nHence, we have\\r\\n        P i [Si ] =    ∑ P i [ S i | S0 ] P 0 [ S0 ]\\r\\n                      S0 ⊆ S i\\r\\n                                                                                                                                   !\\r\\n                                            ( 1)                           i(|U \\\\ S0 |−|Si \\\\ S0 |)                i | S i \\\\ S0 |\\r\\n                =      ∑             ∑ µ           (U \\\\ S0 )(1 − α)                                  (1 − (1 − α ) )                   P 0 [ S0 ]\\r\\n                      S0 ⊆ S i   U ⊇ Si\\r\\n\\r\\n                =         ∑          µ(1) (U \\\\ S0 )P0 [S0 ](1 − α)i(|U |−|Si |) (1 − (1 − α)i )|Si \\\\S0 |\\r\\n                      U ⊇ S i ⊇ S0\\r\\n\\r\\n                =         ∑          µ(0) (U )(1 − α)|U |−|S0 | α|S0 | (1 − α)i(|U |−|Si |) (1 − (1 − α)i )|Si \\\\S0 |\\r\\n                      U ⊇ S i ⊇ S0\\r\\n\\r\\n                = ∑ µ(0) (U )(1 − α)(i+1)(|U |−|Si|) ∑ (1 − α)|Si |−|S0 | (1 − (1 − α)i )|Si \\\\S0 | α|S0 |\\r\\n                      U ⊇ Si                                                 S0 ⊆ S i\\r\\n                                                    \\x10                           \\x11 | Si |\\r\\n                = ∑ µ(0) (U )(1 − α)(i+1)(|U |−|Si|) α + (1 − α)(1 − (1 − α)i )\\r\\n                      U ⊇ Si\\r\\n\\r\\n                = ∑ µ(0) (U )(1 − α)(i+1)(|U |−|Si|) (1 − (1 − α)i+1 )|Si |\\r\\n                      U ⊇ Si\\r\\n\\r\\n\\r\\n\\r\\n                                                                           17\\r\\n\\x0c\\n\\nwhere the third equality uses Proposition 29 and the definition of L1 = ((1 − α) L)S0 to derive\\r\\n\\r\\n            µ ( 1 ) ( U \\\\ S 0 ) P 0 [ S 0 ] = µ ( 1 ) ( U \\\\ S 0 ) ∑ ( 1 − α ) | V \\\\ S0 | α | S0 | µ ( 0 ) ( V )\\r\\n                                                                        V ⊇ S0\\r\\n\\r\\n                                                  ( 1 − α ) | U \\\\ S0 | µ ( 0 ) ( U )\\r\\n                                          =                       | V \\\\ S   |   ( 0) ( V ) ∑\\r\\n                                                                                                 ( 1 − α ) | V \\\\ S0 | α | S0 | µ ( 0 ) ( V )\\r\\n                                              ∑ V ⊇ S0 ( 1 −  α )         0   µ           V ⊇ S0\\r\\n\\r\\n                                          = ( 1 − α ) | U \\\\ S0 | α | S0 | µ ( 0 ) ( U ) .\\r\\n                                                                                                           log n\\r\\nThus the induction hypothesis holds for all i. By taking i = Ω( α ǫ ), and only considering the\\r\\nsummand corresponding to U = Si , we have\\r\\n                                                      \\x10     \\x10 ǫ \\x11\\x11n\\r\\n                                          i +1 | Si |\\r\\n            P i [Si ] ≥ µ(Si )(1 − (1 − α) ) ≥ µ(Si ) 1 − O         ≥ µ(Si )(1 − ǫ),\\r\\n                                                              n\\r\\nand hence,\\r\\n\\r\\n                  dTV (P i [·], µ) =               ∑                (µ(Si ) − P i [Si ]) ≤               ∑                 ǫµ(Si ) ≤ ǫ.\\r\\n                                          Si :P i [Si ]≤ µ ( Si )                                Si :P i [Si ]≤ µ ( Si )\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nNext, we show that each step of the for loop can be implemented in constant parallel time.\\r\\nLemma 31. Let µ : 2[n] → R ≥0 be a DPP with marginal kernel K. If λmax (K ) ≤ √1n then we can sample\\r\\nfrom a distribution ǫ-away in total variation distance from µ in O       e (1)-time using O(poly(n)( 1 )o(1) )\\r\\n                                                                                                      ǫ\\r\\nmachines.\\r\\n                   q\\r\\nProof. Let s = c n log ǫ1′ for c as in Lemma 17. Let Ω := {S ⊆ [n] | |S| ≤ s} . Let pi := Ki,i =\\r\\nP S∼µ [i ∈ S]. Let ν be the distribution obtained by independently sampling independent bi ∼\\r\\nBer( pi ) for all i ∈ [n], and outputting T = {i | bi = 1} . By Lemma 17, ∑S∈Ω µ(S) ≥ 1 − ǫ′ .\\r\\nMoreover, for fixed T ∈ Ω, we have\\r\\n                                                ! −1                                               ! −1\\r\\n    µ( T )       det L T\\r\\n              det( I + L) ∏\\r\\n            =                    pi ∏ (1 − pi )      = det( L T ) det( I − K ) ∏ Ki,i ∏(1 − Ki,i )      ,\\r\\n    ν( T )                  i∈ T    i6 ∈ T                                       i∈ T  i6 ∈ T\\r\\n\\r\\nwhere we use I + L = ( I − K )−1 . By applying Corollary 16 to I − K, we have\\r\\n\\r\\n                                       det( I − K ) ≤ ∏ (1 − Ki,i ) ≤ ∏(1 − Ki,i ),\\r\\n                                                                    i∈[n ]                  i6 ∈ T\\r\\n\\r\\nso it suffices to show\\r\\n                                             \\x12 \\x13 o ( 1)                       \\x12 \\x13 o ( 1)\\r\\n                                              1                      µ( T )    1\\r\\n                                det( L T ) ≤\\r\\n                                              ǫ         ∏    Ki,i =⇒\\r\\n                                                                     ν( T )\\r\\n                                                                            ≤\\r\\n                                                                               ǫ\\r\\n                                                                                         ,\\r\\n                                                        i∈ T\\r\\n\\r\\nat which point we can apply Proposition 24. Let K = UDU ⊺ where U ∈ R n×n is an orthonormal\\r\\nbasis of eigenvectors of K, and D = diag({λi }i∈[n] ), where λ1 ≥ · · · ≥ λn are the eigenvalues of\\r\\nK. By (2), we can write                                     !\\r\\n                                            \\x1a          \\x1b\\r\\n                                                 λi\\r\\n                                L = Udiag                     U⊺.\\r\\n                                               1 − λi i∈[n]\\r\\n\\r\\n                                                                             18\\r\\n\\x0c\\n\\nThus, by applying the Cauchy-Binet formula twice,\\r\\n                                                                 \\x1a              \\x1b             !               !\\r\\n                                                                         λi\\r\\n                 det( L T ) = det UT,[n]diag                                                      U[⊺n],T )\\r\\n                                                                       1 − λi        i∈[n ]\\r\\n                                                                                              !\\r\\n                                                                              λi\\r\\n                             =         ∑              det(Λ T,S )         ∏ 1 − λi                 det(Λ⊺S,T )\\r\\n                                 S ⊆[n ],|S |=| T |                       i∈S\\r\\n                                             r              !                                                     !\\r\\n                                                     1\\r\\n                             ≤ exp c             log                    ∑            det(Λ T,S )         ∏ λi         det(Λ⊺S,T )\\r\\n                                                     ǫ          S ⊆[n ],|S |=| T |                        i∈S\\r\\n                                             r              !\\r\\n                                                        1\\r\\n                             = exp c log                        det(K T )\\r\\n                                                        ǫ\\r\\n                                             r              !\\r\\n                                                     1\\r\\n                             ≤ exp c             log\\r\\n                                                     ǫ          ∏ Ki,i\\r\\n                                                                i∈ T\\r\\n\\r\\nwhere in the first inequality, we used\\r\\n                                         \\x12                  \\x13|S|        \\x12                 \\x13s                      r           !\\r\\n                                                 1                              1                                         1\\r\\n                   ∏ (1 − λ i ) ≥            1− √\\r\\n                                                  n\\r\\n                                                                   ≥        1− √\\r\\n                                                                                 n\\r\\n                                                                                                  ≥ exp −c            log\\r\\n                                                                                                                          ǫ\\r\\n                   i∈S\\r\\n\\r\\nand in the last inequality, we used Corollary 16. Thus by Proposition 24, we can sample from µ̃\\r\\nthat is ǫ-away from µ in O(1) parallel time using O(( 1ǫ )o(1) poly(n)) machines, by setting δ ← 2ǫ\\r\\nand adjusting the definition of ǫ in this proof by a constant.\\r\\n\\r\\nProposition 32. Consider the setup of Algorithm 4. Suppose α ≤ 1. We have λmax (K (i) ) ≤ λ for all i,\\r\\n                                                                                                  e (1) parallel\\r\\nso that λmax (K̃ (i) ) ≤ √1n . Consequently, each iteration of the for loop can be implemented in O\\r\\ntime using O(poly(n)( 1ǫ )o(1) ) machines, up to total variation distance ǫ.\\r\\n\\r\\nProof. We inductively show that λmax (K (i) ) ≤ λ for all i. The base case i = 0 directly follows\\r\\nfrom the input assumption. Now let us assume that λmax (K (i) ) ≤ λ for some i ≥ 0. We will show\\r\\nthat λmax (K (i+1) ) ≤ λ follows. Let S be the index set of Li and S̃ = S \\\\ Ti where Ti was sampled.\\r\\nThen,\\r\\n                      L(i+1) = ((1 − α) L(i) ) Ti = (1 − α) LS̃ − (1 − α) LS̃,Ti L−  1\\r\\n                                                                                  Ti ,Ti L Ti ,S̃ .\\r\\n                           \\x10      \\x11 −1\\r\\n                      ( i)   ( i)        ( i)                              ( i)        ( i)\\r\\nSince L(i) is PSD, LS̃,T L Ti ,Ti      L T ,S̃ \\x17 0. Thus L(i+1) \\x16 (1 − α) LS̃ \\x16 LS̃ .\\r\\n                         i                       i\\r\\n\\r\\n\\r\\nLet Λ be the set of the eigenvalues of K (i) . Due to (2), the eigenvalues of L(i) are given by\\r\\n{ 1−λ λ | λ ∈ Λ}. As 1−λ λ is strictly increasing in the range [0, 1], and the largest eigenvalue of\\r\\nL(i+1) is dominated by the largest eigenvalue of L(i) (since restrictions to index sets can only\\r\\ndecrease quadratic forms), we have the first desired conclusion. The second conclusion follows\\r\\nfrom Lemma 31 as the eigenvalue bound is satisfied.\\r\\n\\r\\nFinally, we are ready to prove Theorem 28.\\r\\n\\r\\nProof of Theorem 28. The bound involving Tr(K ) follows from a similar\\r\\n                                                                    \\x08 argument as in Remark 12.\\r\\nNote that Tr(K ) = E S∼µ [|S|], and that by Lemma 17, the set Ω := S ⊆ [n] |S| ≤ Tr(K ) log 2ǫ\\r\\nhas µ(Ω) ≥ 1 − 2ǫ . When drawing k from H (the distribution on cardinality values), if k ≤\\r\\n\\r\\n                                                                         19\\r\\n\\x0c\\n\\nTr(K ) log 2ǫ , we use Theorem 10 to approximately sample from within 2ǫ of µk , else we output an\\r\\narbitrary subset. By the triangle inequality, the output’s distribution is within 2ǫ + 2ǫ = ǫ of µ. The\\r\\nalgorithm runs in the stated parallel time depending on Tr(K ) using the number of machines as\\r\\nTheorem 10.\\r\\nNow we focus on the bound involving λmax (K ). If α > 1 then the conclusion follows            √ from\\r\\nLemma 31 applied to step (1). Else, suppose α ≤ 1. We run Algorithm 4 with R = O(λ n log nǫ )\\r\\nsuch that Proposition 30 guarantees that if we can run the algorithm correctly, the output has\\r\\ntotal variation 2ǫ . Let ǫ′ = Rǫ . Let ν(i) be the target distribution of Ti in ith step of the for loop.\\r\\nBy Proposition 32, we can modify step (2) to sample from ν̂(i) that is ǫ′ -away from ν(i) in TV-\\r\\ndistance in Oe (1) time using O(poly(n)( 1 )o(1) ) machines. Hence, by the triangle inequality, the\\r\\n                                             ǫ\\r\\noutput of the algorithm is 2ǫ away from the output if we were given exact sample access to each\\r\\nν(i) . Combining with the approximation error of Proposition 30 yields the conclusion.\\r\\n\\r\\n\\r\\n6 Entropic independence\\r\\nIn this section, we prove the following main result. We will use Theorem 33 to derive our\\r\\nsamplers for various entropically independent distributions, namely Theorems 8 and 9, which\\r\\nimmediately follow from combining Remark 12, Lemma 21, Lemma 22, and Theorem 33.\\r\\n\\r\\nTheorem 33. Let µ : ([nk ]) → R ≥0 be such that all its conditional distributions are α1 -entropically inde-\\r\\npendent with α = Ω(1). Suppose we can compute marginals P µ [i | S] for S ⊆ [n] and i 6∈ S in O        e ( 1)\\r\\nparallel time. For any constant c > 0 and any ǫ ∈ (0, 1), there exists an algorithm that uses O to sample\\r\\nfrom a distribution within total variation distance ǫ of µ in\\r\\n                                                \\x12√ \\x12 \\x13c \\x13\\r\\n                                              e          k\\r\\n                                             O      k·\\r\\n                                                         ǫ\\r\\n                                −1 )\\r\\nparallel time using ( nǫ )O(c          machines.\\r\\n\\r\\n6.1 Isotropic transformation\\r\\nWe first reduce to the case of near-isotropic distributions. Similarly to [AD20; Ana+21a], we say a\\r\\ndistribution µ : ([nk ]) → R ≥0 is isotropic if for all i ∈ [n], the marginal P S∼µ [i ∈ S] is nk . Prior work\\r\\n[AD20] introduced the following subdivision process transforming an arbitrary µ : ([nk ]) → R ≥0\\r\\nto a nearly-isotropic µ′ : (Uk ) → R ≥0 , while preserving entropic independence.\\r\\nDefinition 34. Let µ : (nk) → R ≥0 be an arbitrary probability distribution, and assume that we\\r\\nhave access to the marginals p1 , . . . , pn of the distribution with p1 + · · · + pn = k and pi =\\r\\n                                                                 n\\r\\nP S∼µ [i ∈ S] for all i. For a parameter β ∈ (0, 1), let ti := ⌈ βk pi ⌉. We create a new distribution out\\r\\nof µ as follows: for each i ∈ [n], create ti copies of element i and let the collection of all copies be\\r\\n                               S\\r\\nthe new ground set: U := ni=1 {i( j) } j∈[ti ] . Define the following distribution µiso : (Uk ) → R ≥0 :\\r\\n                                                  \\x10n                           o\\x11        µ({i1 , . . . , ik })\\r\\n                                            iso         (j )            (j )\\r\\n                                        µ              i1 1 , . . . , i k k         :=                         .\\r\\n                                                                                            t1 · · · t k\\r\\n\\r\\nWe call µiso the isotropic transformation of µ.\\r\\n\\r\\n\\r\\n\\r\\n                                                                               20\\r\\n\\x0c\\n\\nAnother way we can think of µiso is that to produce a sample from it, we can first generate\\r\\n                                                            (j )\\r\\na sample {i1 , . . . , ik } from µ, and then choose a copy imm for each element im in the sample,\\r\\nuniformly at random. We recall that subdivision preserves entropic independence.\\r\\nProposition 35 ([Ana+21a, Proposition 19]). If µ is α1 -entropically-independent, then so is µiso .\\r\\nThe following generalizes [Ana+21a, Proposition 24], which summarizes useful properties of µiso .\\r\\nProposition 36. Let µ : (nk) → R ≥0 , and let µiso : (Uk ) → R ≥0 be the subdivided distribution from\\r\\n                                     p\\r\\nDefinition 34 for some β. Let C = 1 + β. The following hold for µiso .\\r\\n   1. Marginal upper bound: For all i( j) ∈ U, the marginal P S∼µiso [i ( j) ∈ S] ≤ C |Uk | .\\r\\n                                                      √\\r\\n                                                        βk\\r\\n   2. Marginal lower bound: If pi := PS∼µ [i ∈ S] ≥ n , then for all j ∈ [ti ], P S∼µiso [i ( j) ∈ S] ≥ C |kU | .\\r\\n                               \\x1a             √             \\x1b\\r\\n                                   ( j )        βk\\r\\n      Furthermore, letting R := i        pi ≥ n , j ∈ [ti ] then for any ℓ ≤ k\\r\\n\\r\\n                                                                         p\\r\\n                                                  ∑ µiso\\r\\n                                                     ℓ (S) ≥ 1 −             β ℓ.\\r\\n                                                S ∈( Rℓ )\\r\\n\\r\\n\\r\\n\\r\\n   3. Bounded ground set size: nβ−1 ≤ |U | ≤ n(1 + β−1 ).\\r\\n\\r\\nProof. First, we check the cardinality of the new ground set U:\\r\\n                   n                   n       n \\x12       \\x13\\r\\n             −1        n                             n          n n\\r\\n          nβ = ∑\\r\\n                      kβ\\r\\n                         p i ≤ |U | = ∑ t i ≤ ∑ 1 +\\r\\n                                                    kβ\\r\\n                                                       pi = n +\\r\\n                                                                kβ ∑ pi = n (1 + β −1 ).\\r\\n                  i=1                 i=1     i=1                  i=1\\r\\n\\r\\n\\r\\nNext, we check that for any i ( j) , the marginal probabilities P S∼µiso [i ( j) ∈ S] are at most |Ck\\r\\n                                                                                                   U|\\r\\n                                                                                                      . In\\r\\nthe following calculation, we interpret the sampling from µiso as first sampling from µ, and then\\r\\nchoosing a copy j ∈ [ti ] for each element. This yields\\r\\n\\r\\n   P S∼µiso [i ( j) ∈ S] = ∑ P [we chose copy j | we sampled S from µ] · P [we sampled S from µ]\\r\\n                        S∋i\\r\\n                            1            1           1\\r\\n                      =∑      · µ ( S ) = ∑ µ ( S ) = · P S ∼ µ [i ∈ S ].\\r\\n                           t\\r\\n                        S∋i i\\r\\n                                         t i S∋i     t i\\r\\n\\r\\n\\r\\n          n            n\\r\\nSince 1 + βk pi ≥ ti ≥ βk pi , we obtain\\r\\n\\r\\n              k               pi                              βk    k ( β + 1)    k ( β + 1)   Ck\\r\\n                       =       n     ≤ P S∼µiso [i( j) ∈ S] ≤    =              ≤            ≤      .\\r\\n        kp− 1\\r\\n          i + nβ\\r\\n                 −1        1 + βk pi                          n             −\\r\\n                                                                   n (1 + β ) 1       |U |     |U |\\r\\n\\r\\nThe latter inequality shows√the marginal upper bound. Next, to show the marginal lower bound,\\r\\n                                 βk\\r\\nsuppose P µ [i ∈ S] = pi ≥      n . Then for all j ∈ [ti ],\\r\\n\\r\\n                                                        k                    k                     k\\r\\n                     P S∼µiso [i ( j) ∈ S] ≥                     ≥                  p        ≥          .\\r\\n                                               kp− 1\\r\\n                                                 i + nβ\\r\\n                                                        −1           nβ−1 (1 +          β)       C |U |\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            21\\r\\n\\x0c\\n\\n                              \\x1a              √ \\x1b\\r\\n                                              βk\\r\\nFinally, letting R̄ :=            i   pi ≥   n          ⊆ [ n ],\\r\\n\\r\\n                                                                                                                                            ℓ pi      p\\r\\n ∑ µiso\\r\\n    ℓ ( S ) = ∑ µℓ ( S̄ ) = 1 −                       ∑                   µℓ (S̄) ≥ 1 − ∑                ∑              µℓ (S̄) = 1 − ∑\\r\\n                                                                                                                                             k\\r\\n                                                                                                                                                 ≥ 1 − β ℓ.\\r\\nS ∈( Rℓ )       S̄ ∈( R̄ℓ )                  S̄ ⊆([nℓ ]): S̄6 ⊆ ( R̄ℓ )                      i6 ∈ R̄ S̄ ⊆([n ]) :i∈S̄\\r\\n                                                                                                           ℓ\\r\\n                                                                                                                                    i6 ∈ R̄\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThe following is a simple consequence of the data processing inequality.\\r\\nRemark 37. For any ℓ ∈ [k], suppose algorithm A can sample from within total variation distance\\r\\nǫ of µiso\\r\\n      ℓ . Then A can also be used to sample from within total variation distance ǫ of µℓ using\\r\\nthe same amount of (parallel) time and machines.\\r\\n\\r\\n6.2 KL divergence bound\\r\\nThroughout this section and Section 6.3, let ℓ ∈ [k]. We begin by proving a bound on the KL\\r\\ndivergence between conditional marginals from an observed set.\\r\\n\\r\\nLemma 38. Let S ∈ ([nt ]) for t ≤ 12 k. Let µt+1|S : [n] \\\\ S → R ≥0 be the marginal distribution of elements\\r\\nin S′ ∼ µt+1 conditioned on S ⊂ S′ , namely µt+1|S := µ(· | S) D(k−t)→1. Then,\\r\\n                               \\x12             \\x13             \\x12                 \\x13\\r\\n                                          1         2                1          2t\\r\\n                         DKL µt+1|S k p ≤              log                     + .                       (3)\\r\\n                                          k        αk        P T ∼µ [S ⊂ T ]    k\\r\\n\\r\\nProof. Throughout this proof, fix the set S ∈ ([nt ]), and let AS denote the left-hand side of (3). Let\\r\\nqi := P T ∼µ [i ∈ T | S ⊆ T ], and note that qi = 1 for all i ∈ S. Moreover, we have\\r\\n                                                        \\x12           \\x13\\r\\n                                              qi          qi     k\\r\\n                                  AS = ∑            log      ·\\r\\n                                       i6 ∈S\\r\\n                                             k−t          pi k − t\\r\\n                                                           \\x12 \\x13\\r\\n                                           k        qi       qi            k\\r\\n                                        k − t i∑\\r\\n                                     =                 log         + log                            (4)\\r\\n                                                6∈S\\r\\n                                                     k       p i         k − t\\r\\n                                                           \\x12 \\x13\\r\\n                                           k        qi       qi      2t\\r\\n                                     ≤         ∑\\r\\n                                        k − t i6 ∈S k\\r\\n                                                       log\\r\\n                                                             pi\\r\\n                                                                   + .\\r\\n                                                                      k\\r\\n                                                                                         q\\r\\nThe first equation used that by definition, µt+1|S = k−Sct where qSc restricts q to Sc := [n] \\\\ S; the\\r\\nonly inequality used log(1 + c) ≤ c for all c ≥ 0 and t ≤ 21 k. We note that for\\r\\n                                               \\x12 \\x13\\r\\n                                           qi    qi          1    1\\r\\n                               BS := ∑ log           + ∑ log ,\\r\\n                                     i6 ∈S\\r\\n                                           k     pi      i∈S\\r\\n                                                             k    pi\\r\\n\\r\\nwe have by t ≤ 21 k that                                                    \\x12        \\x13\\r\\n                                                      q                         qi           2t        2t\\r\\n                                         AS ≤ 2 ∑ i log                                  +      ≤ 2BS + ,                                             (5)\\r\\n                                                i6 ∈S\\r\\n                                                      k                         pi           k         k\\r\\n\\r\\nsince log p1i ≥ 0 for all i ∈ [n]. We next give an interpretation of the quantity BS . Let µS be the\\r\\ndistribution of T ∼ µ conditioned on S ⊂ T, so that\\r\\n                                                  (\\r\\n                                                    1\\r\\n                                                        i∈S\\r\\n                                       µS Dk→1 = qki            .\\r\\n                                                     k  i 6 ∈ S\\r\\n\\r\\n                                                                                22\\r\\n\\x0c\\n\\nNotice that BS is defined to be DKL (µS Dk→1 kµDk→1 ) (since µDk→1 = 1k p), which we can control\\r\\nby entropic independence of µ. In particular,\\r\\n\\r\\n                                                    1\\r\\n                BS = DKL (µS Dk→1 kµDk→1 ) ≤           DKL (µS kµ)\\r\\n                                                    αk\\r\\n                                                    1                      µS ( T )\\r\\n                                                  =\\r\\n                                                    αk  ∑     µS ( T ) log\\r\\n                                                                           µ( T )\\r\\n                                                         [n ]\\r\\n                                                            T ∈( k )\\r\\n                                                             S⊂ T\\r\\n                                                                                        \\x12                            \\x13\\r\\n                                                       1                                         µ( T )       1\\r\\n                                                  =\\r\\n                                                       αk      ∑         µS ( T ) log                      ·\\r\\n                                                                                            P T ∼µ [S ⊂ T ] µ( T )\\r\\n                                                            T ∈([nk ])\\r\\n                                                             S⊂ T\\r\\n                                                                    \\x12                        \\x13\\r\\n                                                    1                            1\\r\\n                                                  =    log                                       .\\r\\n                                                    αk                   P T ∼µ [S ⊂ T ]\\r\\n\\r\\nCombining the above display with (5) completes the proof.\\r\\n\\r\\nBy averaging Lemma 38 over µS (the conditional distribution of T ∼ µ on S ⊂ T), we immediately\\r\\nobtain the following corollary.\\r\\nCorollary 39. Let t ≤ 12 k. Then following the notation of Lemma 38,\\r\\n                                            \\x12                  \\x13     \\x12      \\x12 \\x13     \\x13\\r\\n                                                           1       2t 1      2n\\r\\n                    ∑      µD k→t ( S )D KL   µ t + 1| S k\\r\\n                                                           k\\r\\n                                                             p   ≤\\r\\n                                                                   k α\\r\\n                                                                        log\\r\\n                                                                              k\\r\\n                                                                                + 1   .\\r\\n                      [n ]\\r\\n                      S ∈( t )\\r\\n\\r\\n\\r\\nProof. It suffices to apply Lemma 38, and the calculation\\r\\n                                 \\x12                     \\x13                                                       !\\r\\n                                             1                                                         1\\r\\n          ∑ µDk→t (S) log            P T ∼µ [S ⊂ T ]\\r\\n                                                           =     ∑ µDk→t (S) log   µDk→t (S)(kt)\\r\\n        S ∈([nt ])                                             S ∈([nt ])\\r\\n                                                                                 \\x12              \\x13\\r\\n                                                                                       1                1\\r\\n                                                           = ∑ µDk→t (S) log                      + log k\\r\\n                                                                                   µD k→t ( S )        (t)\\r\\n                                                             S ∈([nt ])\\r\\n                                                                            \\x12    \\x13\\r\\n                                                                    ( n)      2n\\r\\n                                                           ≤ log kt ≤ t log        .\\r\\n                                                                    (t)        k\\r\\n\\r\\nThe first equality used P T ∼µ [S ⊂ T ] = µDk→t (S)(kt), and the last line used that the negative\\r\\nentropy of a distribution supported on N elements is bounded by log N.\\r\\n\\r\\nFinally, we use Corollary 39 to derive a KL divergence bound between the distributions µℓ and µ′ℓ ,\\r\\nrespectively the target and proposal distributions encountered in our rejection sampling scheme.\\r\\nLemma 40. Let µ′j be the distribution of the set formed by j independent draws from 1k p. Let ℓ ≤ 12 k.\\r\\nThen,                                             \\x12       \\x12 \\x13       \\x13\\r\\n                                         ′     ℓ2 1        2n\\r\\n                              DKL (µℓ kµℓ ) ≤         log        +1 .\\r\\n                                                k α         k\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                23\\r\\n\\x0c\\n\\nProof. For any j ∈ [ℓ], following the notation of Lemma 38,\\r\\n                                                                                                   \\x12               \\x13\\r\\n                                                                                                              1\\r\\n                DKL (µ j kµ′j ) − DKL (µ j−1 kµ′j−1 ) =      ∑ µDk→( j−1) (S)DKL                       µ j|S k p\\r\\n                                                                                                              k\\r\\n                                                          S ∈( j[−n ]1)\\r\\n                                                                          \\x12           \\x12        \\x13       \\x13\\r\\n                                                       2( j − 1)              1           2n\\r\\n                                                     ≤                          log                +1 .\\r\\n                                                            k                 α            k\\r\\n\\r\\nIn the first line, we used the chain rule of KL divergence, and the second line used Corollary 39.\\r\\nFinally, the conclusion follows by telescoping the above display for 1 ≤ j ≤ ℓ.\\r\\n\\r\\nLemma 40 bounds the KL divergence between µℓ and µ′ℓ , which can be thought of as an average\\r\\nlog-acceptance probability for our rejection sampling scheme. For constant α1 , this bound suggests\\r\\n                      √\\r\\nthat we can take ℓ ≈ k and obtain an efficient sampler for ℓ-marginals; however, it is only an\\r\\naverage bound. We make this intuition rigorous in Section 6.3, where we use the tools from this\\r\\nsection to give concentration bounds on the acceptance probability of rejection sampling.\\r\\n\\r\\n6.3 Concentration of acceptance probability\\r\\nIn this section, assume that we have already performed the transformation in Proposition 36\\r\\nparameterized by some β, and obtained a distribution νiso : (Uk ) → R ≥0 and a set R ⊆ U of\\r\\nelements with lower bounds on marginals as given by Proposition 36. Let ν := νiso and let ν′ be\\r\\ndefined analogously to Section 6.2. Our goal is to sample from within ǫ total variation of νℓ for\\r\\na suitably chosen ℓ, which also implies that we can sample from within ǫ of µℓ (see Remark 37).\\r\\nOur algorithm will be the modified rejection sampler of Algorithm 3.\\r\\nTo use Algorithm 3 with P = νDk→ℓ and Q the ℓ-wise product distribution drawing from 1k p, we\\r\\nfirst define a relevant high-probability set Ω on our state space X := (Uk ). Our set Ω will be a\\r\\nsubset of the following set, for some ε > 0 we will choose later:\\r\\n                                  \\x1a     \\x12 \\x13                               \\x1b\\r\\n                           Ωe ε := S ∈ U | ν| T | ( T ) ≥ ε| T | , ∀ T ⊆ S .\\r\\n                                          ℓ\\r\\n\\r\\nIn other words, Ω   e ε contains all sets S such that all subsets T ⊂ S are relatively well-represented\\r\\naccording to ν| T | ( T ). We begin with an observation lower bounding the measure of Ω    e ε.\\r\\n                             1\\r\\nLemma 41. For any 0 ≤ ε ≤ 2 |U |ℓ ,\\r\\n                                              ∑ νℓ (S) ≤ 2 |U |ℓε.\\r\\n                                                eε\\r\\n                                             S6∈Ω\\r\\n\\r\\n\\r\\nProof. Let C := ∪ℓt=1 {T ∈ (Ut ) | νt ( T ) ≤ εt }. For any S 6∈ Ωe ε , we say T ∈ C is a “certificate” of S\\r\\nif T ⊂ S; every S ∈ Ω e cε has at least one certificate, so there is a map M : Ω       e cε → C . Moreover, for\\r\\n                 e c                               e c\\r\\nsome T ∈ C , let Ωε ( T ) be the set of all S ∈ Ωε such that M(S) = T. Then since\\r\\n                                                                          \\x12 \\x13\\r\\n                                          1                                 ℓ\\r\\n                         νt ( T ) = ∑ ℓ νℓ (S) =⇒         ∑     νℓ (S) ≤       νt ( T ),\\r\\n                                    S⊇ T ( )              ec\\r\\n                                                                            t\\r\\n                                         t                   S∈Ωε ( T )\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                          24\\r\\n\\x0c\\n\\nsumming over all T ∈ C yields\\r\\n                                               \\x12 \\x13\\r\\n                                                 ℓ\\r\\n                               ∑ νℓ (S) ≤ ∑ t ν|T | (T )\\r\\n                                e cε\\r\\n                              S∈Ω         T ∈C\\r\\n                                                      \\x12 \\x13\\r\\n                                                       ℓ\\r\\n                                        = ∑        ∑ t νt (T )\\r\\n                                          1≤ t≤ℓ    U\\r\\n                                                        T ∈( t )\\r\\n                                                       νt ( T )≤ ε t\\r\\n                                                                         |U |ℓε\\r\\n                                          ≤     ∑ (|U |ℓε)t ≤ 1 − |U |ℓε ≤ 2 |U |ℓε.\\r\\n                                              1≤ t≤ℓ\\r\\n\\r\\n\\r\\nThe last line used the approximations (ℓt ) ≤ ℓt , (|Ut |) ≤ |U |t .\\r\\n\\r\\n                                                                                    e ε captures at least\\r\\nFor the remainder of the section we will specifically use ε = 32 |ǫU |ℓ , such that Ω\\r\\n      ǫ                          U                                                  e ε for simplicity.\\r\\na 1 − fraction of the mass of ( ) according to νℓ . We will also drop ε from Ω\\r\\n      16                              ℓ\\r\\n                                                        e have a polynomially bounded acceptance\\r\\nOur next goal is to show that almost all of the sets in Ω\\r\\nprobability when the proposal is given by independent draws from 1k p. Consider iteratively\\r\\nbuilding a set St for all 1 ≤ t ≤ ℓ, where St is a random variable formed by St−1 ∪ {it } for it ∼ 1k p.\\r\\nIn particular, we use it to denote the tth draw from 1k p in this process. For parameters τ, γ ≥ 0 to\\r\\nbe defined later, iteratively define the random variables:\\r\\n\\r\\n                      Yt+1 := Yt exp (∆t+1 ) ,\\r\\n                                \\uf8f1     \\x12              \\x13\\r\\n                                \\uf8f2γ log νt+1|St (it+1) − τ               νt (St ) ≥ εt and it+1 ∈ R\\r\\n                                          1\\r\\n                      ∆t +1 : =           k p i t +1\\r\\n                                \\uf8f3\\r\\n                                  −∞                                     otherwise\\r\\n              p\\r\\nwith C = 1 + β and R as defined in Proposition 36. Also by Proposition 36, pi ≤ |Ck\\r\\n                                                                                 U|\\r\\n                                                                                    for all\\r\\ni ∈ U. We use the convention exp(−∞) = 0.\\r\\nWe next prove that Yt+1 is a submartingale for appropriate parameter choices.\\r\\n                                                p        n                o\\r\\nLemma 42. Let St = T have νt ( T ) ≥ εt . Assume β ≤ min 3γ 1\\r\\n                                                              , αkt log 1ε . Then,\\r\\n                                          \\x12                 \\x13\\r\\n                          γ                   12t     1\\r\\n                 τ ≥ |U | γ ( 1 + γ ) ·           log             =⇒ Ei∼νt+1|St [Yt+1 | St = T ] ≤ Yt .\\r\\n                                              αk      ε\\r\\n\\r\\nProof. If Yt = 0 then Yt+1 = 0 by definition. In the following, assume Yt > 0. By the definition of\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 25\\r\\n\\x0c\\n\\nYt+1 = Yt exp(∆t+1 ), and since νt ( T ) ≥ εt , we have\\r\\n                          \\x14                \\x15\\r\\n                            Yt+1\\r\\n               E i∼νt+1|T         | St = T\\r\\n                             Yt\\r\\n                                         \"                       !γ             #\\r\\n                                                  νt+1| T (i )\\r\\n               = exp (−τ ) Ei∼νt+1|T 1 i∈ R            1\\r\\n                                                                      | St = T\\r\\n                                                       k p i\\r\\n                                                             !γ\\r\\n                                                νt+1| T (i )\\r\\n               = exp (−τ ) ∑ νt+1|T (i)            1\\r\\n                               i∈ R                k pi\\r\\n                                    \\x12                        \\x12        \\x12             \\x13         \\x13\\x13\\r\\n                                 γ           γ                                   1\\r\\n               ≤ exp (−τ ) C 1 + |U | γ(1 + γ) DKL νt+1|T k p + log C\\r\\n                                                                                 k\\r\\n                                             \\x12                             \\x12                  \\x13\\x13\\r\\n                                       p                   γ                 4t       1 p\\r\\n               ≤ exp (−τ ) (1 + 2 βγ) 1 + |U | γ(1 + γ) ·                        log + β         .\\r\\n                                                                             αk       ε\\r\\n                                                                             p\\r\\nThe second-to-last inequality used Lemma 25 with C = 1 + β, and the last inequality used\\r\\nLemma 38, which shows that since νt ( T ) ≥ εt ,\\r\\n                           \\x12            \\x13               \\x12                    \\x13\\r\\n                                      1        2                    1              2t\\r\\n                     DKL νt+1|T k p ≤             log                           +\\r\\n                                      k        αk          P S∼ν [ T ⊂ S]          k\\r\\n                                                                        !\\r\\n                                               2                 1           2t      4t    1\\r\\n                                           =      log                k\\r\\n                                                                           +     ≤      log ,\\r\\n                                               αk           νt ( T )( )       k     αk     ε\\r\\n                                                                            t\\r\\n                      p          p\\r\\nas well as log(1 +        β) ≤       β and\\r\\n\\r\\n                                             (1 + x)γ ≤ exγ ≤ 1 + 2xγ\\r\\n            p\\r\\nfor xγ :=       βγ ≤ 13 . The conclusion follows from\\r\\n                                      \\x12                         \\x12                   \\x13\\x13\\r\\n                             p                     γ               4t      1 p\\r\\n                       (1 + 2 βγ) 1 + |U | γ(1 + γ) ·                  log + β\\r\\n                                                                  αk       ε\\r\\n                                                             \\x12             \\x13\\r\\n                                p                               5t       1         p\\r\\n                        ≤ 1 + 2 βγ + |U |γ γ(1 + γ) ·               log      (1 + 2 βγ)\\r\\n                                                                αk       ε\\r\\n                                \\x12              \\x13                        \\x12          \\x13\\x12      \\x13\\r\\n                                    2t       1          γ                 5t     1       2\\r\\n                        ≤ 1+γ           log       + |U | γ ( 1 + γ ) ·       log      1+\\r\\n                                    αk       ε                            αk     ε       3\\r\\n                                                   \\x12            \\x13\\r\\n                                                     12t      1\\r\\n                        ≤ 1 + |U | γ γ ( 1 + γ ) ·       log       ≤ 1 + τ ≤ exp(τ ).\\r\\n                                                     αk       ε\\r\\n\\r\\n\\r\\n                                                              n         o\\r\\nNow, applying Lemma 42 with the definition of Ω   e ′ := Ωe ∩ S ∈ ( R) allows us to obtain a\\r\\n                                                                      ℓ\\r\\nhigh-probability bound on the acceptance probability of our rejection sampling scheme.\\r\\nLemma 43. Let B ≥ 1. For sufficiently small ǫ ∈ (0, 1), and ℓ ∈ [k] satisfying\\r\\n\\r\\n                                                      16\\r\\n                                                           \\x01 B3\\r\\n                                               12ℓ2    ǫ          log 1ε\\r\\n                                                                           ≤ 1,\\r\\n                                                      αk\\r\\n\\r\\n\\r\\n\\r\\n                                                             26\\r\\n\\x0c\\n\\nand supposing our choices of parameters satisfy\\r\\n                                                                  \\x1a                    \\x1b\\r\\n                                                 p                     1 1      1\\r\\n                                                     β ≤ min            ,   log            ,\\r\\n                                                                      3γ αk     ε\\r\\nwe have                                              \\x14                               \\x15\\r\\n                                                         νℓ (Sℓ )       B        e ′   ǫ\\r\\n                                          P Sℓ ∼νℓ        ′       ≥ |U | | S ℓ ∈ Ω ≤ .\\r\\n                                                         νℓ (Sℓ )                      8\\r\\n\\r\\nProof. Throughout this proof, we will assume\\r\\n                                                           2 log 16\\r\\n                                                                  ǫ        log 16\\r\\n                                                                                ǫ\\r\\n                                                  γ=                  , τ=        .\\r\\n                                                           B log |U |        ℓ\\r\\nWe first observe that our parameter choices indeed satisfy the condition on τ used in Lemma 42:\\r\\n                                  \\x12 \\x13 B3                          \\x12           \\x13\\r\\n             γ ( 1 + γ ) |U | γ    16                         γ     12ℓ     1       ℓ\\r\\n                      16\\r\\n                          \\x01     ≤        =⇒ γ ( 1 + γ ) | U |   ·       log     ·        ≤ 1.\\r\\n                 log ǫ              ǫ                               αk      ε     log 16\\r\\n                                                                                       ǫ\\r\\n\\r\\n\\r\\nIn the following we denote µ̂ j to be the joint distribution of {i1 , i2 , . . . , i j } where i1 ∼ ν1 , i2 ∼\\r\\nν2|S1 ={i1 } , and so on. In other words, if Sℓ is the unordered set of {i1 , i2 , . . . , iℓ }, we have νℓ (Sℓ ) =\\r\\nℓ! · µ̂ ({i1 , i2 , . . . , iℓ }). We similarly define µ̂′j so that νℓ′ (Sℓ ) = ℓ! · µ̂′ℓ ({i1 , i2 , . . . , iℓ }). For Sℓ ∈ (Uℓ ),\\r\\nand some realization {i1 , i2 , . . . , iℓ } whose unordered set is Sℓ , cancelling a factor of ℓ! yields\\r\\n\\r\\n                                    νℓ (Sℓ )  µ̂ ({i , i2 , . . . , iℓ })        νj|S j−1 ={i1 ,i2 ,...,i j−1} ({i j })\\r\\n                     L ( Sℓ ) : =    ′       = ′ℓ 1                         =∏                  1\\r\\n                                                                                                                                (6)\\r\\n                                    νℓ (Sℓ )  µ̂ℓ ({i1 , i2 , . . . , iℓ })  j∈ℓ                k pi j\\r\\n\\r\\nwhere νℓ′ is the distribution of the unordered set corresponding to ℓ draws from 1k p. Next, we\\r\\napply Lemma 42 which yields a submartingale property on Yℓ . Letting 1 S ∈Ω                      e ′ be the 0-1 valued\\r\\n                                                                                              ℓ\\r\\nindicator function of the event Sℓ ∈ Ω           e , we compute\\r\\n                                                   ′\\r\\n                                           h                  i\\r\\n           1 = Y0 ≥ E {i1 ,i2 ,...,iℓ }∼µ̂ℓ Yℓ · 1 Sℓ ∈Ω  e′\\r\\n                                                                       h                                  i\\r\\n                  = P Sℓ ∼νℓ [Sℓ ∈ Ω      e ′ ] · E {i ,i ,...,i }∼µ̂ exp (−ℓτ + γ log L(Sℓ )) | Sℓ ∈ Ω e′ .\\r\\n                                                      1 2       ℓ    ℓ\\r\\n\\r\\n\\r\\n\\r\\nIn the last two expressions, Sℓ denotes the unordered set {i1 , i2 , . . . , iℓ }. The first inequality used\\r\\nthe fact that Yℓ is always nonnegative, and whenever Sℓ ∈ Ω                      e ′ we can apply Lemma 42 to all\\r\\nsubsets in the stages of its construction. The second line follows since whenever Sℓ ∈ Ω                   e ′ , we are\\r\\nalways in the first case in the definition of ∆t+1 , and then we can apply (6). Hence, for any B ≥ 0,\\r\\n                                                                    h                           i\\r\\n                   P Sℓ ∼νℓ [Sℓ ∈ Ω                                                          e ′ ≤ exp (ℓτ )\\r\\n                                       e ′ ] · E {i ,i ,...,i }∼µ̂ exp (γ log L(Sℓ )) | Sℓ ∈ Ω\\r\\n                                                   1 2       ℓ    ℓ\\r\\n                                     h                                         i\\r\\n           =⇒ P {i1 ,i2 ,...,iℓ }∼µ̂ℓ log L(Sℓ ) ≥ B log |U | | Sℓ ∈ Ω      e ′ ≤ 2 exp (ℓτ − γB log |U |) ,\\r\\n\\r\\nwhere the last line used Markov’s inequality, and that Ω    e ′ captures at least half the mass of νℓ .\\r\\nHowever, every permutation giving rise to the unordered set Sℓ is equally likely under µ̂ℓ , so by\\r\\naggregating permutations, this can be rewritten as the desired\\r\\n                         h                          i\\r\\n                                                 e ′ ≤ 2 exp (ℓτ − γB log |U |) = ǫ .\\r\\n                 P Sℓ ∼νℓ L(Sℓ ) ≥ |U | B | Sℓ ∈ Ω\\r\\n                                                                                  8\\r\\n\\r\\n\\r\\n\\r\\n                                                                    27\\r\\n\\x0c\\n\\nFinally, we combine Lemma 41 and Lemma 43 to prove our main result.\\r\\nLemma 44. Let B ≥ 1, and for a sufficiently small constant below, suppose\\r\\n                                              \\x12              \\x13\\r\\n                                        2          αk      3\\r\\n                                       ℓ =O             · ǫB .\\r\\n                                                 log nǫ\\r\\n\\r\\nThere is a parallel algorithm using O((nk2 ǫ−2 ) B log 1ǫ ) machines which runs in O(1) time and returns a\\r\\ndraw from a distribution within total variation distance 2ǫ of µDk→ℓ .\\r\\n\\r\\nProof. Without loss of generality, we can assume n is at least a sufficiently large constant, else the\\r\\nstandard sequential sampler has parallel depth Oe (1). We set\\r\\n                                    \\x1a             \\x1b         (                     )\\r\\n                 p        ǫ            1     1 1                1      1 B log n\\r\\n                   β :=      ≤ min        log ,     = min         log ,             ,\\r\\n                         32k           αk    ε 3γ              αk      ε 6 log 16\\r\\n                                                                                ǫ\\r\\n\\r\\nwhich clearly satisfies the assumption of Lemma 42 for sufficiently large n. Set ε = 32 |ǫU |ℓ . Com-\\r\\nbining Proposition 36 and Lemma 41 and using a union bound, we have\\r\\n                                   \\x12       \\x12\\x1a    \\x12 \\x13\\x1b\\x13\\x13\\r\\n            ′                                     R                p                        ǫ\\r\\n                             e )) − 1 − νℓ\\r\\n          e ) ≥ 1 − (1 − νℓ (Ω\\r\\n      νℓ (Ω                                   S∈           ≥ 1 − 2 βk − 2 |U |ℓε ≥ 1 − .\\r\\n                                                  ℓ                                         8\\r\\nBy Proposition 36, |U | ≤ 2nβ−1 = O(nk2 ǫ−2 ) and\\r\\n                                        \\x12            \\x13    \\x10\\r\\n                                  1            |U |ℓ           n\\x11\\r\\n                              log = O log              = O log\\r\\n                                  ε              ǫ             ǫ\\r\\n\\r\\nThus, this setting of ℓ and β satisfies the assumption of Lemma 43. Hence, the subset Ω ⊂ Ω        e′\\r\\n                                                                             ǫ\\r\\nwhich satisfies the conclusion of Lemma 43 has measure at least 1 − 4 according to νℓ . Using\\r\\nAlgorithm 3 and Remark 37, we can sample from within total variation 2ǫ from νDk→ℓ and from\\r\\nµDk→ℓ in O e (1)-time using O(|U | B log 1 ) = O((nk2 ǫ−2 ) B log 1 ) machines. We note that to imple-\\r\\n                                         ε                        ǫ\\r\\nment our modified rejection sampling, it suffices to check that the likelihood ratio is bounded,\\r\\nwhich will certainly be the case for all elements in Ω e ′ , and if there are other sets with bounded\\r\\nlikelihood ratio this only improves the total variation distance guarantee.\\r\\n\\r\\n\\r\\n\\r\\n6.4 Proof of Theorem 33\\r\\nIn this section, we combine the isotropic transformation of Section 6.1, the parallel sampler of\\r\\nLemma 44, and the recursive strategy of Proposition 27 to prove Theorem 33.\\r\\n\\r\\nProof of Theorem 33. Since we can always sample in O      e (k) parallel time, the statement is nontrivial\\r\\n              1       ′   ǫ\\r\\nonly for c ≤ 2 . Set ǫ ← k . As in Proposition 27, it suffices to repeatedly sample from µDk→ℓ for\\r\\nsome choice of ℓ respecting the bound in Lemma 44, within total variation ǫ′ . We then condition\\r\\non this set, and then repeat. By the coupling characterization of total variation, the resulting\\r\\ndistribution will be at total variation ǫ from µ, since this process will clearly terminate within k\\r\\nrounds. It is straightforward to see this will terminate in\\r\\n                                      \\uf8eb                  \\uf8f6\\r\\n                                        v\\r\\n                                        u        k\\r\\n                                      \\uf8ecu                 \\uf8f7\\r\\n                                    O \\uf8edt               3 \\uf8f8 iterations\\r\\n                                              α       ′B\\r\\n                                            log n · ǫ\\r\\n                                                ǫ′\\r\\n\\r\\n\\r\\n\\r\\n                                                     28\\r\\n\\x0c\\n\\nby a variation of the proof of Proposition 27 and the maximum allowable ℓ in Lemma 44. Setting\\r\\nB = 3c gives the desired bound on the number of iterations.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n7 Hard instance for rejection sampling\\r\\nIn this section, we give a simple hard instance of a fractionally log-concave distribution, which\\r\\ndemonstrates that the dependence on k in Theorem 33 may be inherent to our rejection sampling\\r\\nstrategy. In particular, it is natural to hope that we can improve Theorem 33 to obtain a parallel\\r\\n          √                                  1\\r\\ndepth of k · polylog(k), as opposed to k 2 +c for a constant c. Here, we give an example which\\r\\nsuggests that new algorithmic techniques may be necessary to obtain this improvement.\\r\\n\\r\\nOur hard distribution µ : ([nk ]) → R ≥0 will be defined as follows. Let n and k be even, and\\r\\nconsider a partition of the ground set [n] into pairs Si := (2i − 1, 2i ) for all i ∈ [ n2 ]. Then, the\\r\\ndistribution µ is uniformly supported on sets of the form\\r\\n                                                         \\x12\\x02 n \\x03\\x13\\r\\n                                         [\\r\\n                                  S :=           Si , where S′ ∈   2\\r\\n                                                                   k\\r\\n                                                                       .                            (7)\\r\\n                                         i∈S ′                     2\\r\\n\\r\\nIn other words, µ randomly chooses 2k indices between 1 and n2 , and takes the k elements formed\\r\\nby including the pairs corresponding to those indices. It is known that µ is Ω(1)-FLC (see\\r\\n[Ana+21a]). To simplify notation, we will assume that k = o(n) and ℓ = o(k). We will also\\r\\nassume there is a constant B such that we have access to n B parallel machines. Following the\\r\\nguarantees of Algorithm 3 in Proposition 24, if we are willing to tolerate a total variation distance\\r\\nof δ from µℓ , we need to show that with probability at least 1 − δ, S ∼ µℓ satisfies\\r\\n\\r\\n                                                  µℓ (S)\\r\\n                                                          ≤ nB.                                     (8)\\r\\n                                                  µ′ℓ (S)\\r\\n\\r\\nHere and throughout the following discussion, µ′ℓ (S) = nℓ!ℓ is the probability S is formed by ℓ\\r\\nindependent draws from the uniform distribution on [n]. In particular, clearly the 1-marginal\\r\\ndistribution of µ is uniform, so this is the proposal distribution used by rejection sampling.\\r\\nOur argument on the tightness of our rejection sampling proceeds as follows. Say that a set\\r\\nS ∈ ([nℓ ]) has t “duplicates” if amongst the elements of S, there are exactly t pairs of elements\\r\\nbelonging to the same Si . For example, for ℓ = 4 we say the set {1, 2, 3, 5} contains 1 duplicate,\\r\\nthe pair (1, 2). We first show that for a set S to satisfy (8), it cannot contain more than t = O( B)\\r\\nduplicates. We then show that this limitation, along with attaining a failure probability δ inverse-\\r\\n                                            1\\r\\npolynomial in k, forces us to choose ℓ = k 2 −c for a constant c > 0 which may depend on B.\\r\\n\\r\\nHow many duplicates can we afford? Suppose S ∈ ([nℓ ]) contains t duplicates. Each permuta-\\r\\ntion of S is equally likely to be observed by either of the following processes starting from T0 = ∅\\r\\n(we use Ti to denote an ordered set, and Si to denote its unordered counterpart, for all i ∈ [ℓ]).\\r\\n   1. For i ∈ [ℓ], draw j ∈ [n] uniformly at random and add it to Ti−1 to form Ti .\\r\\n   2. For i ∈ [ℓ], draw j ∈ [n] according to the marginal distribution of µℓ conditioned on\\r\\n      including Ti−1 and add it to Ti−1 to form Ti .\\r\\n\\r\\n\\r\\n                                                       29\\r\\n\\x0c\\n\\n                  µ (S)\\r\\nHence, to bound µ′ℓ (S) as needed by (8), it suffices to fix a permutation Tℓ of Sℓ = S and bound\\r\\n                    ℓ\\r\\nthe ratios of the probabilities Tℓ is observed according to each of the above processes. Clearly it\\r\\nis observed with probability n−ℓ according to the first process above, so satisfying (8) means the\\r\\nprobability Tℓ is observed by the second must be at most n B−ℓ .\\r\\nIt is straightforward to see that the probability\\r\\n                                          \\x01       we observe each second element in a duplicate\\r\\n                                        1\\r\\npair in the relevant round i ∈ [ℓ] is Θ k . On the other hand, the probability of observing each\\r\\n                              \\x01\\r\\nsingleton in its round is Θ n1 . For k = o(n), this shows that to meet (8) we must have\\r\\n                                         \\x12 \\x12 \\x13\\x13ℓ−t \\x12 \\x12 \\x13\\x13t\\r\\n                                            1         1\\r\\n                                          Θ         Θ      ≤ n B−ℓ .\\r\\n                                            n         k\\r\\n\\r\\nThis shows that we must have t at most a constant (depending on B).\\r\\n\\r\\nProbability of t duplicates. Let t be a constant. Recall that the distribution µ is uniform over all\\r\\nsets of the form (7), and a sample from µℓ = Dk→ℓ µ is formed by sampling a set S ∼ µ and then\\r\\nrandomly selecting one of the (kℓ) subsets of S. Hence, it suffices to fix some S of the form (7),\\r\\nand bound the probability that this downsampling process results in a subset with t duplicates.\\r\\nBy symmetry of µ, we lose no generality by only considering the set S = ∪i∈[ k ] Si .\\r\\n                                                                                      2\\r\\n\\r\\nNow, for a constant t, the number of subsets S of size ℓ with exactly t duplicates is\\r\\n                                                \\x12k\\x13 \\x12 k     \\x13\\r\\n                                                 2 ·  2 −t    · 2ℓ−2t .\\r\\n                                                 t   ℓ − 2t\\r\\n\\r\\nThe first term corresponds to choosing which t sets Si will be fully included, the second corre-\\r\\nsponds to choosing which sets the remaining ℓ − 2t elements come from, and the third is because\\r\\nfor each of the non-duplicated sets we have two options. Hence, the probability a draw from µℓ\\r\\nhas exactly t duplicates for constant t scales as\\r\\n                  k       k\\r\\n                           2 − t ) · 2ℓ−2t\\r\\n                                              \\x12 \\x12 \\x13\\x13 −ℓ              \\x12 \\x12 \\x13\\x13ℓ−2t\\r\\n                ( 2t ) · (ℓ−  2t                 k                 t     k\\r\\n                                             = Θ        · (Θ (k)) · Θ             · 2ℓ−2t\\r\\n                              (kℓ)               ℓ                       ℓ\\r\\n                                              \\x12 \\x12 \\x13\\x132t                \\x12 \\x12 2 \\x13\\x13t\\r\\n                                                 ℓ               t        ℓ\\r\\n                                             = Θ       · (Θ ( k)) = Θ           .\\r\\n                                                 k                         k\\r\\n\\r\\nIn other words, to guarantee that a draw from µℓ contains less than t duplicates with probability\\r\\nat least 1 − δ, we need to ensure that\\r\\n                                     \\x12       \\x12 2 \\x13\\x13t              \\x10√ 1 \\x11\\r\\n                                              ℓ\\r\\n                                         Θ           ≤ δ =⇒ ℓ = O   kδ 2t .\\r\\n                                               k\\r\\n\\r\\nFor δ scaling inverse-polynomially in k (which is necessary to perform a union bound over the\\r\\n                                                                         1\\r\\npoly(k) iterations of rejection sampling), this shows we must take ℓ ≤ k 2 −c for some constant c\\r\\nwhich depends on our budget constant B from the earlier discussion.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                          30\\r\\n\\x0c\\n\\nReferences\\r\\n[AD20]    Nima Anari and Michał Dereziński. “Isotropy and Log-Concave Polynomials: Accel-\\r\\n          erated Sampling and High-Precision Counting of Matroid Bases”. In: Proceedings of\\r\\n          the 61st Annual Symposium on Foundations of Computer Science. 2020.\\r\\n[AL20]    Vedat Levi Alev and Lap Chi Lau. “Improved analysis of higher order random walks\\r\\n          and applications”. In: Proceedings of the 52nd Annual ACM SIGACT Symposium on\\r\\n          Theory of Computing. 2020, pp. 1198–1211.\\r\\n[Ald90]   David J Aldous. “The random walk construction of uniform spanning trees and\\r\\n          uniform labelled trees”. In: SIAM Journal on Discrete Mathematics 3.4 (1990), pp. 450–\\r\\n          465.\\r\\n[Ali+21]  Yeganeh Alimohammadi, Nima Anari, Kirankumar Shiragur, and Thuy-Duong Vuong.\\r\\n          “Fractionally Log-Concave and Sector-Stable Polynomials: Counting Planar Match-\\r\\n          ings and More”. In: arXiv preprint arXiv:2102.02708 (2021).\\r\\n[ALO20]   Nima Anari, Kuikui Liu, and Shayan Oveis Gharan. “Spectral Independence in High-\\r\\n          Dimensional Expanders and Applications to the Hardcore Model”. In: Proceedings of\\r\\n          the 61st IEEE Annual Symposium on Foundations of Computer Science. IEEE Computer\\r\\n          Society, 2020.\\r\\n[Ana+19]  Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant. “Log-concave\\r\\n          polynomials II: high-dimensional walks and an FPRAS for counting bases of a ma-\\r\\n          troid”. In: Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Com-\\r\\n          puting. 2019, pp. 1–12.\\r\\n[Ana+20]  Nima Anari, Nathan Hu, Amin Saberi, and Aaron Schild. “Sampling Arborescences\\r\\n          in Parallel”. In: arXiv preprint arXiv:2012.09502 (2020).\\r\\n[Ana+21a] Nima Anari, Michal Derezinski, Thuy-Duong Vuong, and Elizabeth Yang. “Domain\\r\\n          Sparsification of Discrete Distributions using Entropic Independence”. In: CoRR\\r\\n          abs/2109.06442 (2021).\\r\\n[Ana+21b] Nima Anari, Vishesh Jain, Frederic Koehler, Huy Tuan Pham, and Thuy-Duong\\r\\n          Vuong. “Entropic Independence I: Modified Log-Sobolev Inequalities for Fraction-\\r\\n          ally Log-Concave Distributions and High-Temperature Ising Models”. In: CoRR abs/2106.04105\\r\\n          (2021). arXiv: 2106.04105.\\r\\n[Ana+21c] Nima Anari, Vishesh Jain, Frederic Koehler, Huy Tuan Pham, and Thuy-Duong\\r\\n          Vuong. “Entropic Independence II: Optimal Sampling and Concentration via Re-\\r\\n          stricted Modified Log-Sobolev Inequalities”. In: arXiv preprint arXiv:2111.03247 (2021).\\r\\n[AOR16]   Nima Anari, Shayan Oveis Gharan, and Alireza Rezaei. “Monte Carlo Markov chain\\r\\n          algorithms for sampling strongly Rayleigh distributions and determinantal point\\r\\n          processes”. In: Conference on Learning Theory. PMLR. 2016, pp. 103–115.\\r\\n[Bar18]   Alexander Barvinok. “Approximating real-rooted and stable polynomials, with com-\\r\\n          binatorial applications”. In: arXiv preprint arXiv:1806.07404 (2018).\\r\\n[BBL09]   Julius Borcea, Petter Brändén, and Thomas Liggett. “Negative dependence and the\\r\\n          geometry of polynomials”. In: Journal of the American Mathematical Society 22.2 (2009),\\r\\n          pp. 521–567.\\r\\n[Ber84]   Stuart J. Berkowitz. “On computing the determinant in small parallel time using a\\r\\n          small number of processors”. In: Information Processing Letters 18.3 (1984), pp. 147–\\r\\n          150.\\r\\n[Bro89]   Andrei Z Broder. “Generating random spanning trees”. In: FOCS. Vol. 89. Citeseer.\\r\\n          1989, pp. 442–447.\\r\\n\\r\\n\\r\\n\\r\\n                                            31\\r\\n\\x0c\\n\\n[Bru18]    Victor-Emmanuel Brunel. “Learning Signed Determinantal Point Processes through\\r\\n           the Principal Minor Assignment Problem”. In: Advances in Neural Information Pro-\\r\\n           cessing Systems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-\\r\\n           Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc., 2018, pp. 7365–7374.\\r\\n[Cel+16]   L Elisa Celis, Amit Deshpande, Tarun Kathuria, Damian Straszak, and Nisheeth K\\r\\n           Vishnoi. “On the complexity of constrained determinantal point processes”. In: arXiv\\r\\n           preprint arXiv:1608.00554 (2016).\\r\\n[Cel+17]   L. Elisa Celis, Amit Deshpande, Tarun Kathuria, Damian Straszak, and Nisheeth\\r\\n           K. Vishnoi. “On the Complexity of Constrained Determinantal Point Processes”. In:\\r\\n           APPROX-RANDOM. 2017.\\r\\n[CLV21]    Zongchen Chen, Kuikui Liu, and Eric Vigoda. “Spectral independence via stability\\r\\n           and applications to holant-type problems”. In: arXiv preprint arXiv:2106.03366 (2021).\\r\\n[Csa75]    Laszlo Csanky. “Fast parallel matrix inversion algorithms”. In: 16th Annual Sympo-\\r\\n           sium on Foundations of Computer Science (sfcs 1975). IEEE. 1975, pp. 11–12.\\r\\n[DM21]     Michał Derezinski and Michael W Mahoney. “Determinantal point processes in ran-\\r\\n           domized numerical linear algebra”. In: Notices of the American Mathematical Society\\r\\n           68.1 (2021), pp. 34–45.\\r\\n[Elf+19]   Mohamed Elfeki, Camille Couprie, Morgane Riviere, and Mohamed Elhoseiny. “GDPP:\\r\\n           Learning diverse generations using determinantal point processes”. In: International\\r\\n           Conference on Machine Learning. PMLR. 2019, pp. 1774–1783.\\r\\n[Fan89]    Li Fang. “On the spectra of P- and P0-matrices”. In: Linear Algebra and its Applications\\r\\n           119 (1989), pp. 1–25. issn: 0024-3795. doi: https://doi.org/10.1016/0024-3795(89)90065-7.\\r\\n[FHY21]    Weiming Feng, Thomas P Hayes, and Yitong Yin. “Distributed metropolis sampler\\r\\n           with optimal parallelism”. In: Proceedings of the 2021 ACM-SIAM Symposium on Dis-\\r\\n           crete Algorithms (SODA). SIAM. 2021, pp. 2121–2140.\\r\\n[Gar+19]   Mike Gartrell, Victor-Emmanuel Brunel, Elvis Dohmatob, and Syrine Krichene. “Learn-\\r\\n           ing Nonsymmetric Determinantal Point Processes”. In: ArXiv abs/1905.12962 (2019).\\r\\n[Gar+20]   Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, and Victor-Emmanuel\\r\\n           Brunel. Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Pro-\\r\\n           cesses. 2020. arXiv: 2006.09862 [cs.LG].\\r\\n[Gon+14]   Boqing Gong, Wei-lun Chao, Kristen Grauman, and Fei Sha. Large-Margin Determi-\\r\\n           nantal Point Processes. 2014. arXiv: 1411.1537 [stat.ML].\\r\\n[GPK16]    Mike Gartrell, Ulrich Paquet, and Noam Koenigstein. “Bayesian Low-Rank Determi-\\r\\n           nantal Point Processes”. In: Proceedings of the 10th ACM Conference on Recommender\\r\\n           Systems. RecSys ’16. Boston, Massachusetts, USA: Association for Computing Ma-\\r\\n           chinery, 2016, pp. 349–356. isbn: 9781450340359. doi: 10.1145/2959100.2959178.\\r\\n[HS19]     Jonathan Hermon and Justin Salez. “Modified log-Sobolev inequalities for strong-\\r\\n           Rayleigh measures”. In: arXiv preprint arXiv:1902.02775 (2019).\\r\\n[JVV86]    Mark R Jerrum, Leslie G Valiant, and Vijay V Vazirani. “Random generation of com-\\r\\n           binatorial structures from a uniform distribution”. In: Theoretical computer science 43\\r\\n           (1986), pp. 169–188.\\r\\n[KT12a]    Alex Kulesza and Ben Taskar. “Determinantal Point Processes for Machine Learn-\\r\\n           ing”. In: Found. Trends Mach. Learn. 5.2-3 (2012), pp. 123–286.\\r\\n[KT12b]    Alex Kulesza and Ben Taskar. k-DPPs: Fixed-Size Determinantal Point Processes. 2012.\\r\\n[LB12]     Hui Lin and Jeff Bilmes. “Learning Mixtures of Submodular Shells with Application\\r\\n           to Document Summarization”. In: Uncertainty in Artificial Intelligence - Proceedings of\\r\\n           the 28th Conference, UAI 2012 (Oct. 2012).\\r\\n\\r\\n\\r\\n\\r\\n                                            32\\r\\n\\x0c\\n\\n[LJS16]    Chengtao Li, Stefanie Jegelka, and Suvrit Sra. “Fast DPP Sampling for Nyström with\\r\\n           Application to Kernel Methods”. In: CoRR abs/1603.06052 (2016). arXiv: 1603.06052.\\r\\n[LY21]     Hongyang Liu and Yitong Yin. “Simple Parallel Algorithms for Single-Site Dynam-\\r\\n           ics”. In: arXiv preprint arXiv:2111.04044 (2021).\\r\\n[MS15]     Zelda Mariet and Suvrit Sra. “Fixed-point algorithms for determinantal point pro-\\r\\n           cesses”. In: CoRR, abs/1508.00792 (2015).\\r\\n[PP13]     Robin Pemantle and Yuval Peres. Concentration of Lipschitz functionals of determinantal\\r\\n           and other strong Rayleigh measures. 2013. arXiv: 1108.0687 [math.PR].\\r\\n[PR17]     Viresh Patel and Guus Regts. “Deterministic polynomial-time approximation algo-\\r\\n           rithms for partition functions and graph polynomials”. In: SIAM Journal on Comput-\\r\\n           ing 46.6 (2017), pp. 1893–1919.\\r\\n[Ten95]    Shang-Hua Teng. “Independent sets versus perfect matchings”. In: Theoretical Com-\\r\\n           puter Science 145.1-2 (1995), pp. 381–390.\\r\\n[Wil+18]   Mark Wilhelm, Ajith Ramanathan, Alexander Bonomo, Sagar Jain, Ed H. Chi, and\\r\\n           Jennifer Gillenwater. “Practical Diversified Recommendations on YouTube with De-\\r\\n           terminantal Point Processes”. In: Proceedings of the 27th ACM International Conference\\r\\n           on Information and Knowledge Management. CIKM ’18. Torino, Italy: Association for\\r\\n           Computing Machinery, 2018, pp. 2165–2173. isbn: 9781450360142. doi: 10.1145/3269206.3272018.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                           33\\r\\n\\x0c',\n",
       " '                                                                                                                                                ACL’2022 findings\\r\\n\\r\\n\\r\\n                                         Relevant CommonSense Subgraphs for \"What if...\" Procedural Reasoning\\r\\n\\r\\n\\r\\n                                                              Chen Zheng                                   Parisa Kordjamshidi\\r\\n                                                         Michigan State University                       Michigan State University\\r\\n                                                          zhengc12@msu.edu                                kordjams@msu.edu\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                              Abstract                                   Procedural Text:\\r\\n                                                                                                         1. A plant produces a seed.\\r\\n                                                                                                         2. The seed falls to the ground.\\r\\n                                             We study the challenge of learning causal rea-              3. The seed is buried.\\r\\n                                             soning over procedural text to answer \"What                 4. The seed germinates.\\r\\n                                                                                                         5. A plant grows.\\r\\narXiv:2203.11187v1 [cs.CL] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             if...\" questions when external commonsense                  6. The plant produces flowers.\\r\\n                                                                                                         7. The flowers produce more seeds\\r\\n                                             knowledge is required. We propose a novel\\r\\n                                                                                                         Questions and Answers:\\r\\n                                             multi-hop graph reasoning model to 1) effi-                 1. suppose plants will produce more seeds\\r\\n                                             ciently extract a commonsense subgraph with                 happens, how will it affect less plants.\\r\\n                                                                                                         (A) More (B) Less (C) No effect\\r\\n                                             the most relevant information from a large                  2. suppose the soil is rich in nutrients happens,\\r\\n                                                                                                         how will it affect more seeds are produced.\\r\\n                                             knowledge graph; 2) predict the causal answer               (A) More (B) Less (C) No effect\\r\\n                                             by reasoning over the representations obtained              3. suppose The sun comes out happens, how\\r\\n                                                                                                         will it affect less plants.\\r\\n                                             from the commonsense subgraph and the con-                  (A) More (B) Less (C) No effect\\r\\n                                             textual interactions between the questions and\\r\\n                                             context. We evaluate our model on WIQA               Figure 1: WIQA contains procedural text, and different\\r\\n                                             benchmark and achieve state-of-the-art perfor-       types of questions. The bold choices are the answers.\\r\\n                                             mance compared to the recent models.\\r\\n                                                                                                  relatedto, soil) and (soil, relatedto, seed) derived\\r\\n                                         1   Introduction                                         from ConceptNet, we can build an explicit reason-\\r\\n                                         In recent years, large-scale pre-trained language        ing chain and choose an explainable answer.\\r\\n                                         models (LMs) have made a breakthrough progress              Two challenges exist in procedural text reason-\\r\\n                                         and demonstrate a high performance in many NLP           ing and using external KBs. The first challenge is\\r\\n                                         tasks, including procedural text reasoning (Tandon       effectively extracting the most relevant external in-\\r\\n                                         et al., 2019; Rajagopal et al., 2020). There is a        formation and reducing the noise from the KB. The\\r\\n                                         large amount of knowledge that is stored implicitly      second challenge is reasoning over the extracted\\r\\n                                         in language models that help in solving various          knowledge. Several works enhance the QA model\\r\\n                                         NLP tasks (Devlin et al., 2019b). When we rea-           with commonsense knowledge (Lin et al., 2019; Lv\\r\\n                                         son over text, sometimes, the knowledge contained        et al., 2020). However, the noisy knowledge from\\r\\n                                         in a given text is sufficient to predict the answer,     KG will seriously mislead the QA model in pre-\\r\\n                                         as it is shown in the question 1 of Figure 1. This       dicting the answer. Moreover, using KBs is often\\r\\n                                         knowledge is directly encoded and used by LMs            investigated in the tasks that perform QA directly\\r\\n                                         models (Tandon et al., 2019). However, there are         over KB itself, such as CommonsenseQA (Talmor\\r\\n                                         many cases in which the required knowledge is not        et al., 2019), etc. There are less sophisticated tech-\\r\\n                                         included in the procedural text itself. For example,     niques proposed for using external knowledge ex-\\r\\n                                         for the question 2 in Figure 1, the information about    plicitly (i.e. not through training LMs) in reading\\r\\n                                         the “nutrient” on the seeds does not exist in the pro-   comprehension for aiding QA over text. REM-\\r\\n                                         cedural text. Therefore, the external commonsense        Net (Huang et al., 2021) is the only work that uses\\r\\n                                         knowledge is required.                                   commonsense for WIQA and uses a memory net-\\r\\n                                            There are several existing resources that contain     work to extract the external triplets to solve the first\\r\\n                                         world knowledge and commonsense. Examples are            challenge. However, this work has no reasoning\\r\\n                                         knowledge graphs (KGs) like ConceptNet (Speer            process over the extracted knowledge and uses a\\r\\n                                         et al., 2017) and ATOMIC (Sap et al., 2019). Look-       simple multi-head attention operator to predict the\\r\\n                                         ing back at the question 2, we observe that through      answer. EIGEN (Madaan et al., 2020) constructs an\\r\\n                                         providing the external knowledge triplets (nutrient,     influence graph to find the chain of reasoning given\\r\\n\\x0c\\n\\n                               A. KG-attention Triplet Selection                                                      B. Multi-hop Reasoning\\r\\n\\r\\n                                  Triplet 1           (2)                        (2)                        (3)                           (4)               (4)\\r\\n          \\x02    \\x06\\x07 \\x0e               Triplet 2\\r\\n              \\x04\\x07\\x0e\\r\\n                         (1)\\r\\n                                          …\\r\\n                                                   KG Attention           Relevant Triplets                                       Multi-Hop Graph Answer Prediction\\r\\n                                   CLS n\\r\\n                                  Triplet                                                                                             Encoder\\r\\n                                                                                                                          \\x05\\x03\\x02\\x04 \\x01    Relational Graph\\r\\n               (1)                                                         [CLS ;Triplet i]                                    \\x08    Representation\\r\\n                               [CLS ;Triplet 1]                            [CLS ;Triplet j]\\r\\n      Open Information         [CLS ;Triplet 2]                                   …\\r\\n        Extraction                    …                                    [CLS ; Triplet x]     Highly Relevant Commonsense\\r\\n                               [CLS ; Triplet n]                                                                                          (4)\\r\\n                                                                                   concat        Subgraph Construction\\r\\n               (1)                                                                                                             Text Interaction Encoder\\r\\n                                     CLS                                                                                           Question + document\\r\\n        Question+                                  concat    Pretrain A            MLP                                             Contextual Interaction\\r\\n                         LM\\r\\n        Document                 Ques token rep                  (I)\\r\\n                         (2)\\r\\n                                 Doc token rep\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 2: MRRG Model is composed of Candidate Triplet Extraction, KG Attention, Commonsense Subgraph\\r\\nConstruction, Text encoder with contextual interaction, Graph Reasoning, and Answer prediction modules.\\r\\n\\r\\nprocedural text. However, EIGEN cannot deal with                                               sentences C = {s1 , . . . , sn }, and a large knowl-\\r\\nthe challenge when the required knowledge is not                                               edge graph KG.\\r\\nin the given document.                                                                            Figure 2 shows the proposed architecture. (1)\\x01\\r\\n   To solve these two challenges, we propose a                                                 We extract the entities from question and context\\r\\nMulti-hop Reasoning network over Relevant Com-                                                 in preprocessing step and use them to retrieve the\\r\\nmonSense SubGraphs (MRRG) for casual reason-                                                   set of candidate triples from the ConceptNet. (2)\\r\\ning over procedural Text. Our motivation is to                                                 We train the KG Attention module to extract the\\r\\neffectively and efficiently extract the most relevant                                          most relevant triplets given the procedure and ques-\\r\\ninformation from a large KG to help procedural                                                 tion and reduce the noisy concepts from candidate\\r\\nreasoning. First, we extract the entities, retrieve re-                                        triplets. (3) We augment the commonsense sub-\\r\\nlated external triplets from KG, and learn to extract                                          graph based on the relevant triplets. (4) We train\\r\\nthe most relevant triplets to a given the procedure                                            a model that uses two components, the common-\\r\\nand question input by a novel KG attention mecha-                                              sense subgraph as a relational graph network and\\r\\nnism. Then, we construct a commonsense subgraph                                                a text encoder including question and document to\\r\\nbased on the extracted KG triplets in a pipeline. We                                           do procedural reasoning. Below, we describe the\\r\\nuse the extracted subgraphs as a part of end-to-end                                            details of each module.\\r\\nQA model to help in filling the knowledge gaps\\r\\n                                                                                               2.2   Candidate Triplet Extraction from KG\\r\\nin the procedure and performing multi-hop reason-\\r\\ning. The final model predicts the causal answer                                                Given the input q and C, we extract the contextual\\r\\nby reasoning over the contextual interaction repre-                                            entities (concepts) by a open Information Extrac-\\r\\nsentations over the question and the document and                                              tion (OpenIE) model (Stanovsky et al., 2018). For\\r\\nlearning graph representations over the KB sub-                                                each extracted entity tin , we retrieve the relational\\r\\ngraphs. We evaluate our MRRG on the “what if”                                                  triplets t = (tin , r, tout ) from KG, where tout is\\r\\nWIQA benchmark. MRRG model achieves SOTA                                                       the concept taken from ConceptNet and r is a se-\\r\\nand brings significant improvements compared to                                                mantic relation type. We then apply a pre-trained\\r\\nthe existing baselines.                                                                        Language Model, RoBERTa, to obtain the represen-\\r\\n   The contributions of our work are: 1) We train a                                            tation of each triplet: E t = fLM ([tin , r, tout ]) ∈\\r\\nseparate module that extracts the relevant parts of                                            R3×d , where fLM denotes the language model op-\\r\\nthe KB given the procedure and question to avoid                                               eration and the triplets are given as a sequence of\\r\\nthe noisy and inefficient usage of the information in                                          concepts and relations to the LM.\\r\\nlarge KBs. 2) We design an end-to-end model that\\r\\n                                                                                               2.3   KG Attention\\r\\nuses the extracted QA-dependent KB as a subgraph\\r\\nto guide the reasoning over the procedural text                                                The KG attention module is shown in Figure 2-A\\r\\nto answer the questions. 3) Our MRRG achieves                                                  and Figure 3. We concatenate q and C to form Q =\\r\\nSOTA on the WIQA benchmark.                                                                    [[CLS]; q; [SEP ]; C], where [CLS] and [SEP] are\\r\\n                                                                                               special tokens in the LMs tokenizer process (Liu\\r\\n2     Model Description                                                                        et al., 2019). We use RoBERTa to obtain the list of\\r\\n                                                                                               token representations E[CLS] , Eq , and EC . E[CLS]\\r\\n2.1     Problem Formulation and Overview                                                       is the summary representation of the question and\\r\\nFormally, the problem is to predict an answer a                                                paragraph, Eq is the list of the question tokens\\r\\nfrom a set of pre-defined answers given input ques-                                            embeddings, and EC is the list of the paragraph\\r\\ntion q, a document C which is composed of several                                              tokens embeddings output of Roberta.\\r\\n\\x0c\\n\\n   Given triplet E t that is generated based                                                                        messages from its direct neighbors and relational\\r\\non the triplet extraction described in Section                                                                      semantic edges. The (l + 1)-th layer node represen-\\r\\n2.2, we build a context-triplet pair Ezt =                                                                                  (l+1)\\r\\n                                                                                                                    tation hi     is updated based on the neighborhood\\r\\n             t ; E t ; E t ], where E t is the represen-\\r\\n[E[CLS] ; Ein     r       out              in                                                                       node representations hlj from the l-layer multiplied\\r\\ntation of the head entity from text, Eout       t is the rep-                                                                                            (l)          (l)\\r\\n                                                                                                                    by the relational matrices Wr1 , . . . , Wr|R| . The\\r\\nresentation of the tail entity from KG, and Ert is                                                                                      (l+1)\\r\\nthe representation of the relation. Afterwards, we                                                                  representation hi  is computed as follows:\\r\\ncompute context-triplet pair attention and a soft-                                                                    (l+1)\\r\\n                                                                                                                                 X X 1\\r\\n                                                                                                                                                (l) (l)    (l) (l)\\r\\n                                                                                                                     hi     = σ(           r | Wr hj + W0 hi ),\\r\\nmax layer to output the Context-Triplet pairwise                                                                                      r\\r\\n                                                                                                                                        |N i\\r\\n                                                                                                                                   r∈R j∈Ni\\r\\nimportance Score CT S. The process is computed\\r\\n                                exp(M LP (E t ))                                                                    where σ denotes a non-linear activation function,\\r\\nas follows: CT St = Pm exp(M LPz(E t )) .\\r\\n                               j=1               z                                                                  Nir represents a set that includes neighbor indices\\r\\n   Then we choose the top-k relevant triplets                                                                       of node i under semantic relation r. Finally, we ob-\\r\\nwith the top CT S scores and then use the rele-                                                                     tain the EGs after several hops of message passing.\\r\\nvant triplets to construct the subgraph. For each                                                                   (II) Text Contextual Interaction Encoder: We\\r\\nselected triplet, we obtain the triplet represen-                                                                   have obtained the contextual token represen-\\r\\ntation E 0t = [Ein      0t , E t , E 0t ] ∈ R3×d , where\\r\\n                               r    out                                                                             tations E[CLS] , Eq , and EC in the KG\\r\\n  0t = f ([CT S · E t ; CT S · E t ]) and E 0t =\\r\\nEin       in          t      in          t    r        out                                                          attention module that described in Section\\r\\nfout ([CT St · Eoutt ; CT S · E t ]). Notice that f\\r\\n                                  t     r                  in                                                       2.3. Followed by Seo et al., we utilize Bi-\\r\\nand fout are MLP layers, [; ] is the concatenation,                                                                 DAF style contextual interaction module to\\r\\nand [·] is the scalar product.                                                                                      feed Eq and EC to Context-to-Question Atten-\\r\\n                                       KG Attention                                             Training Strategy   tion EC→q = sof tmax(sim(EqT , EC ))Eq and\\r\\n      \\x01\\x06\\x05\\x03\\x04\\x07\\x08\\r\\n                    (1)\\r\\n                              Triplet 1\\r\\n                              Triplet 2\\r\\n                                              KG Attention\\r\\n                                                  (2)\\r\\n                                                                  Relevant Triplets\\r\\n                                                                          (2)\\r\\n                                                                                            Answer Prediction       Question-to-Context Attention Eq→C to obtain the\\r\\n        \\x02\\x04\\x08                          …\\r\\n                               CLS\\r\\n                              Triplet n\\r\\n                                                                                                                    contextual interaction between question and con-\\r\\n        (1)                                                        [CLS ;Triplet i]\\r\\n\\r\\n Open Information\\r\\n   Extraction\\r\\n                          [CLS ;Triplet 1]\\r\\n                          [CLS ;Triplet 2]\\r\\n                                 …\\r\\n                                                                   [CLS ;Triplet j]\\r\\n                                                                          …\\r\\n                                                                   [CLS ; Triplet x]\\r\\n                                                                                       concat\\r\\n                                                                                                                    text. Then we use LSTM to obtain the hidden\\r\\n                          [CLS ; Triplet n]\\r\\n         (1)                                                                                                        state representations: Fq→C = LST M (Eq→C ),\\r\\n                                 CLS\\r\\n   Question+\\r\\n   Document\\r\\n                    LM\\r\\n                    (2)\\r\\n                            Ques token rep\\r\\n                            Doc token rep\\r\\n                                                         concat\\r\\n                                                                                                                    and FC→q = LST M (EC→q ).\\r\\n                                                                                                                    2.6    Answer Prediction\\r\\nFigure 3: The architecture of training the KG Attention\\r\\nmodule.                                                                                                             We concatenate E[CLS] , Fq→C , FC→q , and the com-\\r\\n                                                                                                                                                               0\\r\\n                                                                                                                    pact subgraph representation EGs obtained from\\r\\n2.4        Commonsense Subgraph Construction                                                                        attentive pooling, and use it as the final represen-\\r\\n                                                                                                                                                                     0\\r\\nWe construct the subgraph Gs based on the relevant                                                                  tation: F = [E[CLS] ; Fq→C ; FC→q ; EGs ]. Then we\\r\\n                                                                                                                                                \\x01\\r\\n\\r\\n\\r\\ntriplets from KG attention for each question and                                                                    utilize a classifier MLP (F ) to predict the answer.\\r\\nanswer pair. We add more edges to the subgraph                                                                      Our MRRG has two separate training modules used\\r\\nas follows: Two entities in the triplets will have an                                                               in a pipeline for triplet selection and procedural rea-\\r\\nedge if a relation r in the KG exists between them.                                                                 soning.\\r\\nThe assumption is that the augmented common-                                                                         (I) Training KG Attention for Triplet Selection:\\r\\nsense subgraph will contain the reasoning paths.                                                                    Figure 3 and the left block of Figure 2 show the\\r\\nWe use Ein 0t and E 0t for the KG subgraph initial                                                                  same triplet selection model. The architecture of\\r\\n                    out\\r\\nnode representation h(0) which is used in RGCN                                                                      Figure 2.B is taken and 3 extra MLP layers added\\r\\nformulation in Section 2.5.                                                                                         to it for training as shown in Figure 3. The MLP is\\r\\n                                                                                                                    applied on the concatenation of the concatenation\\r\\n2.5        Procedural Reasoning                                                                                     of [E[CLS] ; Eq ; EC ; E10t ; . . . ; Ek0t ] to predict the an-\\r\\nProcedural Reasoning composes of two parts:                                                                         swer. We use the cross-entropy as the loss function\\r\\nMulti-Hop Graph Reasoning and Text Contextual                                                                       to train the model.\\r\\nInteraction Encoder.                                                                                                  (II) Training End-to-End MRRG: After pre-\\r\\n(I) Multi-Hop Graph Reasoning: this is the Graph                                                                    training the KG attention, we keep the learned pa-\\r\\nReasoning part of Figure 2-B. Given the subgraph                                                                    rameters and extract the most relevant concepts\\r\\nGs , we use RGCN (Schlichtkrull et al., 2018) to                                                                    and construct the multi-relational commonsense\\r\\nlearn the representations of the relational graph.                                                                  subgraph Gs . We combine subgraph representa-\\r\\nRGCN learns graph representations by aggregating                                                                    tion and text interaction representation as input\\r\\n\\x0c\\n\\nto train the answer prediction module by cross-         base language model uses the rules as a regulariza-\\r\\nentropy loss.                                           tion term during training to impose the consistency\\r\\n                                                        between the answers of multiple questions.\\r\\n3     Experiments and Results                           RGN (Zheng and Kordjamshidi, 2021) is the re-\\r\\n                                                        cent SOTA baseline that utilizes a gating net-\\r\\nWe implemented our MRRG framework using Py-\\r\\n                                                        work (Zheng et al., 2020) to effectively filter out the\\r\\nTorch 1 . We use a pre-trained RoBERTa (Liu et al.,\\r\\n                                                        key entities and relationships in the given document\\r\\n2019) to encode the contextual information in the\\r\\n                                                        and learns the contextual representations to predict\\r\\ninput. The maximum number of triplets is 50 and\\r\\n                                                        the answer. RGN does not consider the external\\r\\nthe maximum number of nodes in the graph is 100.\\r\\n                                                        knowledge for procedural reasoning challenges.\\r\\nFurther details of hyper-parameters of the graph\\r\\n                                                        REM-Net (Huang et al., 2021) proposes a recur-\\r\\nare shown in Table 3. The maximum number of\\r\\n                                                        sive erasure memory network to find out the causal\\r\\nwords for the paragraph context is 256. For the\\r\\n                                                        evidence. Specifically, REM-Net refines the evi-\\r\\ngraph construction module, we utilize open Infor-\\r\\n                                                        dence by a recursive memory mechanism and then\\r\\nmation Extraction model (Stanovsky et al., 2018)\\r\\n                                                        uses a generative model to predict the causal an-\\r\\nfrom AllenNLP2 to extract the entities. The max-\\r\\n                                                        swer. REM-Net is the only work that uses external\\r\\nimum number of hops for the graph module is 3.\\r\\n                                                        knowledge for WIQA. REM-Net uses the external\\r\\nThe learning rate is 1e − 5. The model is optimized\\r\\n                                                        knowledge by training an attention mechanism that\\r\\nusing Adam optimizer (Kingma and Ba, 2015).\\r\\n                                                        considers the KG triplet representations for finding\\r\\n3.1    Datasets                                         the answer. It does not explicitly select the most\\r\\n                                                        relevant triplets as we do, and the graph reasoning\\r\\nWIQA is a large dataset for “what if” causal rea-       is not exploited for finding the chain of reasoning.\\r\\nsoning. WIQA contains three types of questions:\\r\\n                                                         Models                                     in-para   out-of-para   no-effect   Test V1 Acc\\r\\n1) the questions can be directly answered based on       Majority                                    45.46      49.47         55.0         30.66\\r\\n                                                         Polarity                                    76.31      53.59         27.0         39.43\\r\\nthe text, called in-paragraph questions. 2) the ques-    Adaboost (Freund and Schapire, 1995)\\r\\n                                                         emphDecomp-Attn (Parikh et al., 2016)\\r\\n                                                                                                     49.41\\r\\n                                                                                                     56.31\\r\\n                                                                                                                36.61\\r\\n                                                                                                                48.56\\r\\n                                                                                                                             48.42\\r\\n                                                                                                                             73.42\\r\\n                                                                                                                                           43.93\\r\\n                                                                                                                                           59.48\\r\\ntions require external knowledge to be answered,         BERT (no para) (Devlin et al., 2019a)\\r\\n                                                         BERT (Tandon et al., 2019)\\r\\n                                                                                                     60.32\\r\\n                                                                                                     79.68\\r\\n                                                                                                                43.74\\r\\n                                                                                                                56.13\\r\\n                                                                                                                             84.18\\r\\n                                                                                                                             89.38\\r\\n                                                                                                                                           62.41\\r\\n                                                                                                                                           73.80\\r\\n                                                         RoBERTa (Tandon et al., 2019)               74.55      61.29        89.47         74.77\\r\\ncalled out-of-paragraph questions, and 3) irrele-        EIGEN (Madaan et al., 2020)                 73.58      64.04        90.84         76.92\\r\\n                                                         REM-Net (Huang et al., 2021)                75.67      67.98        87.65         77.56\\r\\nvant causes and effects, called no-effect questions.     Logic-Guided (Asai and Hajishirzi, 2020)      -           -            -          78.50\\r\\n                                                         RoBERTa+KG-attention Triplet Selection      72.21      64.60        89.13         75.22\\r\\nWIQA contains 29808 training samples, 6894 de-           MRRG (RoBERTa-base)\\r\\n                                                         Human\\r\\n                                                                                                     79.85\\r\\n                                                                                                       -\\r\\n                                                                                                                69.93\\r\\n                                                                                                                   -\\r\\n                                                                                                                             91.02\\r\\n                                                                                                                                -\\r\\n                                                                                                                                           80.06\\r\\n                                                                                                                                           96.33\\r\\nvelopment samples, 3993 test samples (test V1),\\r\\nand 3003 test samples (test V2).                        Table 1: Model Comparisons on WIQA test V1 dataset.\\r\\n\\r\\n3.2    Baseline Description                             3.3      Results\\r\\nWe briefly describe the most recent baselines that      Table 1 and Table 2 show the performance of\\r\\nuse the Transformer-based language model as the         MRRG on the WIQA task compared to other base-\\r\\nbackbone. We separately fine-tune the BERT and          lines on two different test sets V1 and V2. First,\\r\\nRoBERTa as the first two baselines.                     Both tables show that our proposed KG Attention\\r\\nEIGEN (Madaan et al., 2020) is a baseline that          triplet selection model outperforms the RoBERTa\\r\\nbuilds an event influence graph based on a doc-         and has 3.3% improvement on the out-of-para cat-\\r\\nument and leverages LMs to create the chain of          egory. Second, our MRRG achieves SOTA results\\r\\nreasoning to predict the answer. However, EIGEN         compared to all baseline models. MRRG achieves\\r\\ndoes not use any external knowledge to solve the        the SOTA on both in-para, out-of-para, and no-\\r\\nproblem.                                                effect questions in WIQA V1 and V2.\\r\\nLogic-Guided (Asai and Hajishirzi, 2020) is a            Models                                     in-para   out-of-para   no-effect   Test v2 Acc\\r\\n                                                         Random                                      33.33      33.33        33.33         33.33\\r\\nbaseline that combines neural networks and logic         Majority                                    00.00      00.00        100.0         41.80\\r\\n                                                         BERT                                        70.57      58.54        91.08         74.26\\r\\nrules. Specifically, the Logic-Guided model uses         RoBERTa\\r\\n                                                         REM-Net\\r\\n                                                                                                     70.69\\r\\n                                                                                                     70.94\\r\\n                                                                                                                60.20\\r\\n                                                                                                                63.22\\r\\n                                                                                                                             91.11\\r\\n                                                                                                                             91.24\\r\\n                                                                                                                                           75.34\\r\\n                                                                                                                                           76.29\\r\\n                                                         REM-Net (RoBERTa-large)                     76.23      69.13        92.35         80.09\\r\\nlogic rules including symmetry and transitivity          QUARTET (RoBERTa-large)                     74.49      65.65        95.30         82.07\\r\\n                                                         (Rajagopal et al., 2020)\\r\\nrules to augment the training data. Moreover, the        RGN (Zheng and Kordjamshidi, 2021)          75.91       66.15        92.12        79.95\\r\\n                                                         RoBERTa+KG Attention Triplet Selection      70.02       62.30        91.23        75.86\\r\\n                                                         MRRG (RoBERTa-base)                         76.80       67.83        92.28        80.39\\r\\n  1                                                      MRRG (RoBERTa-large)                        78.82       71.10        93.53        82.95\\r\\n    Our code is available at https://github.com/         Human                                         -           -            -          96.30\\r\\nHLR/MRRG.\\r\\n  2\\r\\n    https://demo.allennlp.org/\\r\\nopen-information-extraction.                            Table 2: Model Comparisons on WIQA test V2 dataset.\\r\\n\\x0c\\n\\n                      Question and Document Content                            RoBERTa   +Interaction   Incorporating Triplets        +KG         +Graph\\r\\n                                                                                                                                      Attention\\r\\n Question: suppose more fruit is produced happens,\\r\\n           how will it affect MORE plants?\\r\\n Content: [“The seed germinates.”, “The plant grows.”, “The plant flowers.”,      X           √          (fruit, createdby, plant)       √          √      Model     # hop = 1   # hop = 2   # hop = 3\\r\\n           “Produces fruit.”, “The fruit releases seeds.”\\r\\n Gold Answer: More\\r\\n Question: suppose the soil is rich in nutrients happens,                                                                                                  BERT      71.6%       62.5%       59.5%\\r\\n           how will it affect more seeds are produced.                                                  (nutrient, relatedto, soil)\\r\\n Content: [“A plant produces a seed”, “The seed falls to the ground”, “The        X           X          (soil, relatedto, seed)         √          √\\r\\n           seed is buried”, “The seed germinates”, “A plant grows”, “The\\r\\n           plant produces flowers”, “The flowers produce more seeds.”]\\r\\n                                                                                                                                                           RoBERTa   73.5%       63.9%       61.1%\\r\\n Gold Answer: More\\r\\n Question: suppose more land available happens,                                                         (igneous rock, isa, rock)                          EIGEN     78.8%       63.5%       68.3%\\r\\n           how will it affect less igneous rock forming.                                                (land, relatedto, rock)\\r\\n Content: [“Different kinds of rocks melt into magma”, “Magma cools in            X           X         (land, relatedto, surface)       X          √\\r\\n the crust”, “Magma goes to the surface and becomes lava”, “Lava cools”,                                (surface, relatedto,\\r\\n           “Cooled magma and lava become igneous rock.”]                                                igneous rock)                                      MRRG      81.0%       72.3%       70.4%\\r\\n Gold Answer: Less\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 4: Left: Case study of the MRRG Framework. “+interaction” means adding the contextual interaction\\r\\nmodule. “KG ATTN” means adding the KG Attention Triplet Selection module. ’X’ indicates the model failed\\r\\nto predict the correct answer and “X” means the prediction was successful with the included module. Right:\\r\\nComparing the results over different number of hops.\\r\\n\\r\\n4      Analysis                                                                                              reasoning with multiple hops due to the relational\\r\\n                                                                                                             graph reasoning and the effectiveness of the ex-\\r\\n4.1       Effects of Using External Knowledge                                                                tracted commonsense subgraph. We study some\\r\\nIn the WIQA, all the baseline models achieve sig-                                                            cases to analyze the multi-hop reasoning and the\\r\\nnificantly lower accuracy in the out-of-para than                                                            reasoning chains. In the third case in Figure 4,\\r\\nin-para and no-effect categories. MRRG achieves                                                              the extracted relevant triplets (land, relatedto, sur-\\r\\nSOTA in the out-of-para category because of using                                                            face), (surface, relatedto, igneous rock) construct a\\r\\nthe highly relevant commonsense subgraphs and                                                                two-hop reasoning chain “land→surface→igneous\\r\\nthe combination of reasoning over text interaction                                                           rock” that helps MRRG to find the correct answer.\\r\\nand the graph reasoning modules. As is shown in\\r\\ntable 2, the advantage of the MRRG model is re-                                                              4.3          Ablation Study\\r\\nflected on out-of-para questions. MRRG improves                                                              Table 3 shows the ablation study results of MRRG\\r\\n4.61% over REM-Net. Notice that REM-Net is                                                                   using WIQA. Firstly, we remove the commonsense\\r\\nthe only model that utilizes external knowledge on                                                           subgraph and graph network. The accuracy de-\\r\\nWIQA. Figure 4 shows a case in which the “soil”                                                              creases 3.4% compared to MRRG. Second, we\\r\\nand “nutrient” only appear in the question and do                                                            remove the contextual interaction module and the\\r\\nnot exist in the text. The baseline models fail to                                                           accuracy decreases 1.3%. In an additional exper-\\r\\nanswer this out-of-para question due to missing                                                              iment, we use the KG attention triplet selection\\r\\nexternal knowledge. However, our model predicts                                                              module to directly predict the answer without the\\r\\nthe correct answer by explicitly incorporating the                                                           pipeline of constructing the subgraph and using the\\r\\n(nutrient, relatedto, soil), (soil, relatedto, seed) that                                                    graph reasoning module. We show the result as\\r\\nconnects the critical information between the ques-                                                          KG Attention Triplet Selection in Table 3. The re-\\r\\ntion and document.                                                                                           sult shows that removing the triplet selection mod-\\r\\n            Ablation                       Model                                Dev Acc                      ule decreases the accuracy by 1.8%. In the same\\r\\n            Text only                 RoBERTa-base                              75.51%                       table 3, we report results about the impact of in-\\r\\n            Text only             + contextual interaction                      76.85%\\r\\n            Text only           KG Attention Triplet Selection                  77.39%                       cluding the relation types in the RGCN graph and\\r\\n                                    - semantic relation                         78.31%\\r\\n                                       GNN dim=50                               79.18%                       the influence of changing the dimensionality of the\\r\\n          Text+Graph                  GNN dim=100                               80.30%\\r\\n                                      GNN dim=200                               79.88%\\r\\n                                                                                                             node representations in the model.\\r\\n\\r\\nTable 3: Ablation and hyper-para. choices on WIQA.                                                           5         Conclusion\\r\\n“GNN dim” is the dimension of graph representation.\\r\\n                                                                                                             We propose MRRG model for using external knowl-\\r\\n                                                                                                             edge graph in reasoning over procedural text. Our\\r\\n4.2       Relational Reasoning and Multi-Hops                                                                model extracts a relevant subgraph for each ques-\\r\\nBoth in-para and out-of-para question types require                                                          tion from the KG and uses that knowledge subgraph\\r\\nmultiple hops of reasoning to find the answer in                                                             for answering the question. The extracted subgraph\\r\\nthe WIQA. As shown in the right side of Figure 4,                                                            includes the reasoning path for answering the ques-\\r\\nthe MRRG model accuracy improved 2% for 1                                                                    tion and helps multi-hop reasoning to predict an\\r\\nhop, 8% for 2 hops, and 2% for 3 hops compared                                                               explainable answer. We evaluate MRRG on the\\r\\nto EIGEN. MRRG made a sharp improvement in                                                                   WIQA and achieve SOTA performance.\\r\\n\\x0c\\n\\nReferences                                                 Ankur Parikh, Oscar Täckström, Dipanjan Das, and\\r\\n                                                             Jakob Uszkoreit. 2016. A decomposable attention\\r\\nAkari Asai and Hannaneh Hajishirzi. 2020. Logic-             model for natural language inference. In Proceed-\\r\\n  guided data augmentation and regularization for con-       ings of the 2016 Conference on Empirical Methods\\r\\n  sistent question answering. In Proceedings of the          in Natural Language Processing, pages 2249–2255,\\r\\n  58th Annual Meeting of the Association for Compu-          Austin, Texas. Association for Computational Lin-\\r\\n  tational Linguistics, pages 5642–5650. Association         guistics.\\r\\n  for Computational Linguistics.\\r\\n                                                           Dheeraj Rajagopal, Niket Tandon, Peter Clark, Bha-\\r\\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina          vana Dalvi, and Eduard Hovy. 2020. What-if I ask\\r\\n   Toutanova. 2019a. Bert: Pre-training of deep bidi-        you to explain: Explaining the effects of perturba-\\r\\n   rectional transformers for language understanding.        tions in procedural text. In Findings of the Associ-\\r\\n   In NAACL-HLT.                                             ation for Computational Linguistics: EMNLP 2020,\\r\\n                                                             pages 3345–3355, Online. Association for Computa-\\r\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and                tional Linguistics.\\r\\n   Kristina Toutanova. 2019b. BERT: Pre-training of\\r\\n   deep bidirectional transformers for language under-     Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-\\r\\n   standing. In Proceedings of the 2019 Conference          dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\\r\\n   of the North American Chapter of the Association         Brendan Roof, Noah A Smith, and Yejin Choi. 2019.\\r\\n   for Computational Linguistics: Human Language            Atomic: An atlas of machine commonsense for if-\\r\\n  Technologies, Volume 1 (Long and Short Papers),           then reasoning. In Proceedings of the AAAI Con-\\r\\n   pages 4171–4186, Minneapolis, Minnesota. Associ-         ference on Artificial Intelligence, volume 33, pages\\r\\n   ation for Computational Linguistics.                     3027–3035.\\r\\nY. Freund and R. Schapire. 1995. A decision-theoretic      Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,\\r\\n   generalization of on-line learning and an application     Rianne Van Den Berg, Ivan Titov, and Max Welling.\\r\\n   to boosting. In EuroCOLT.                                 2018. Modeling relational data with graph convolu-\\r\\n                                                             tional networks. In European semantic web confer-\\r\\nYinya Huang, Meng Fang, Xunlin Zhan, Qingxing Cao,           ence, pages 593–607. Springer.\\r\\n  Xiaodan Liang, and Liang Lin. 2021. Rem-net: Re-\\r\\n  cursive erasure memory network for commonsense           Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\r\\n  evidence refinement. In AAAI.                              Hannaneh Hajishirzi. 2017. Bidirectional attention\\r\\n                                                             flow for machine comprehension. In ICLR.\\r\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\\r\\n  method for stochastic optimization. In ICLR.             Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.\\r\\n                                                             Conceptnet 5.5: An open multilingual graph of gen-\\r\\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xi-            eral knowledge. In Proceedings of the AAAI Confer-\\r\\n   ang Ren. 2019. KagNet: Knowledge-aware graph              ence on Artificial Intelligence, volume 31.\\r\\n   networks for commonsense reasoning. In Proceed-\\r\\n   ings of the 2019 Conference on Empirical Methods        Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer,\\r\\n   in Natural Language Processing and the 9th Inter-         and Ido Dagan. 2018. Supervised open information\\r\\n   national Joint Conference on Natural Language Pro-        extraction. In Proceedings of the 2018 Conference\\r\\n   cessing (EMNLP-IJCNLP), pages 2829–2839, Hong             of the North American Chapter of the Association\\r\\n   Kong, China. Association for Computational Lin-           for Computational Linguistics: Human Language\\r\\n   guistics.                                                 Technologies, Volume 1 (Long Papers), pages 885–\\r\\n                                                             895, New Orleans, Louisiana. Association for Com-\\r\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-          putational Linguistics.\\r\\n  dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\r\\n  Luke Zettlemoyer, and Veselin Stoyanov. 2019.            Alon Talmor, Jonathan Herzig, Nicholas Lourie, and\\r\\n  Roberta: A robustly optimized bert pretraining ap-         Jonathan Berant. 2019. CommonsenseQA: A ques-\\r\\n  proach. arXiv preprint arXiv:1907.11692.                   tion answering challenge targeting commonsense\\r\\n                                                             knowledge. In Proceedings of the 2019 Conference\\r\\nShangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang,               of the North American Chapter of the Association\\r\\n  Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang,             for Computational Linguistics: Human Language\\r\\n  Guihong Cao, and Songlin Hu. 2020. Graph-based             Technologies, Volume 1 (Long and Short Papers),\\r\\n  reasoning over heterogeneous external knowledge            pages 4149–4158, Minneapolis, Minnesota. Associ-\\r\\n  for commonsense question answering. In Proceed-            ation for Computational Linguistics.\\r\\n  ings of the AAAI Conference on Artificial Intelli-\\r\\n  gence, volume 34, pages 8449–8456.                       Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-\\r\\n                                                             ter Clark, and Antoine Bosselut. 2019. WIQA: A\\r\\nAman Madaan, Dheeraj Rajagopal, Yiming Yang, Ab-             dataset for “what if...” reasoning over procedural\\r\\n hilasha Ravichander, Eduard Hovy, and Shrimai               text. In Proceedings of the 2019 Conference on\\r\\n Prabhumoye. 2020. Eigen: Event influence gen-               Empirical Methods in Natural Language Processing\\r\\n eration using pre-trained language models. arXiv            and the 9th International Joint Conference on Natu-\\r\\n preprint arXiv:2010.11764.                                  ral Language Processing (EMNLP-IJCNLP), pages\\r\\n\\x0c\\n\\n  6076–6085, Hong Kong, China. Association for\\r\\n  Computational Linguistics.\\r\\nChen Zheng, Quan Guo, and Parisa Kordjamshidi.\\r\\n  2020. Cross-modality relevance for reasoning on\\r\\n  language and vision. In Proceedings of the 58th An-\\r\\n  nual Meeting of the Association for Computational\\r\\n  Linguistics, pages 7642–7651. Association for Com-\\r\\n  putational Linguistics.\\r\\nChen Zheng and Parisa Kordjamshidi. 2021. Rela-\\r\\n  tional gating for ”what if” reasoning. In Proceed-\\r\\n  ings of the Thirtieth International Joint Conference\\r\\n  on Artificial Intelligence, IJCAI-21, pages 4015–\\r\\n  4022. International Joint Conferences on Artificial\\r\\n  Intelligence Organization. Main Track.\\r\\n\\x0c',\n",
       " '                                                  A discontinuous Galerkin spectral element method for a nonconservative\\r\\n                                                                compressible multicomponent flow model\\r\\n\\r\\n                                                                                     Rémi Abgralla , Pratik Rai,b,c,∗, Florent Renacc,∗\\r\\n\\r\\n                                                                                     a Institute of Mathematics, University of Zürich, Switzerland\\r\\n\\r\\n                                                                           b CMAP, École Polytechnique, Route de Saclay, 91128 Palaiseau Cedex, France\\r\\n\\r\\n                                                                                c DAAA, ONERA, Université Paris Saclay F-92322 Châtillon, France\\r\\narXiv:2203.11184v1 [math.NA] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                              Abstract\\r\\n                                                      In this work, we propose an accurate, robust (the solution remains in the set of states), and stable discretization of a\\r\\n                                                  nonconservative model for the simulation of compressible multicomponent flows with shocks and material interfaces.\\r\\n                                                  We consider the gamma-based model by Shyue [J. Comput. Phys., 142 (1998), 208–242] where each component\\r\\n                                                  follows a stiffened gas equation of state (EOS). We here extend the framework proposed in Renac [J. Comput. Phys.,\\r\\n                                                  382 (2019), 1–26] and Coquel et al. [J. Comput. Phys. 431 (2021), 110135] for the discretization of hyperbolic\\r\\n                                                  systems, with both fluxes and nonconservative products, to unstructured meshes with curved elements in multiple\\r\\n                                                  space dimensions. The framework relies on a high-order discontinuous Galerkin spectral element method (DGSEM)\\r\\n                                                  using collocation of quadrature and interpolation points as proposed by Gassner [SIAM J. Sci. Comput., 35 (2013)]\\r\\n                                                  in the case of hyperbolic conservation laws. We modify the integrals over discretization elements where we replace\\r\\n                                                  the physical fluxes and nonconservative products by two-point numerical fluctuations. The contributions of this\\r\\n                                                  work are threefold. First, we analyze the semi-discrete DGSEM discretization of general hyperbolic systems with\\r\\n                                                  conservative and nonconservative terms and derive the conditions to obtain a scheme that is high-order accurate, free-\\r\\n                                                  stream preserving, and entropy stable when excluding material interfaces. Second, we design a three-point scheme\\r\\n                                                  with a HLLC solver for the gamma-based model that does not require a root-finding algorithm for the approximation\\r\\n                                                  of the nonconservative products. The scheme is proved to be robust and entropy stable for convex entropies, to\\r\\n                                                  preserve uniform profiles of pressure and velocity across material interfaces (material interface preservation), and to\\r\\n                                                  satisfy a discrete minimum principle on the specific entropy and maximum principles on the parameters of the EOS.\\r\\n                                                  Third, the HLLC solver is applied at interfaces in the DGSEM scheme, while we consider two kinds of fluctuations in\\r\\n                                                  the integrals over discretization elements: the former is entropy conservative (EC), while the latter preserves material\\r\\n                                                  interfaces (CP). Time integration is performed using high-order strong-stability preserving Runge-Kutta schemes.\\r\\n                                                  The fully discrete scheme is shown to preserve material interfaces with CP fluctuations. Under a given condition on\\r\\n                                                  the time step, both EC and CP fluctuations ensure that the cell-averaged solution remains in the set of states; satisfy\\r\\n                                                  a minimum principle on any convex entropy and maximum principles on the EOS parameters. These results allow\\r\\n                                                  to use existing limiters in order to restore positivity, and discrete maximum principles of degrees-of-freedom within\\r\\n                                                  elements. Numerical experiments in one and two space dimensions on flows with discontinuous solutions support\\r\\n                                                  the conclusions of our analysis and highlight stability, robustness and accuracy of the DGSEM scheme with either\\r\\n                                                  CP, or EC fluctuations, while the scheme with CP fluctuations is shown to offer better resolution capabilities.\\r\\n\\r\\n                                              Keywords. Compressible multicomponent flows, Nonconservative hyperbolic systems, Discontinuous Galerkin method,\\r\\n                                           Summation-by-parts, Material interface capturing, Entropy stability, High-order accuracy\\r\\n\\r\\n                                               AMS subject classifications. 65M12, 65M70, 76T10\\r\\n\\r\\n\\r\\n                                           1. Introduction\\r\\n\\r\\n                                               The discussion in this paper focuses on the approximation in multiple space dimensions of a compressible multicomponent\\r\\n                                           flow model in nonconservative form. We consider a gamma-based model [53] for a mixture with a stiffened gas equation of state\\r\\n                                           (EOS) approximating components including both gas and compressible liquids (hereafter referred to as the SG-gamma model).\\r\\n                                           The model is written in quasi-conservative form [1] to preserve velocity and pressure profiles across material interfaces separating\\r\\n\\r\\n\\r\\n                                              ∗ Corresponding author’s email addresses: pratik.rai@polytechnique.edu (Pratik Rai), florent.renac@onera.fr (Florent Renac)\\r\\n\\x0c\\n\\ndifferent components. The model approximates mixture quantities and presents the main advantage of being independent of the\\r\\nnumber of components. We are here interested in high-order, robust (i.e., preserving the solution in the set of admissible states),\\r\\nand entropy stable simulations of flows with shocks, material interfaces, and complex interactions triggering small scale flow\\r\\nphenomena. Numerical approximation of multicomponent and multiphase flows based on interface capturing methods has been\\r\\nthe subject of numerous works (see, e.g. [36, 51] and references therein).\\r\\n    We discretize the SG-gamma model using the discontinuous Galerkin spectral element method (DGSEM) with Gauss-Lobatto\\r\\nquadrature rules [39]. The DGSEM uses diagonal norm summation-by-parts (SBP) operators [24] and, in the case of hyperbolic\\r\\nconservation laws, falls into the general framework of conservative elementwise flux differencing schemes [21]. In this framework,\\r\\nusing entropy conservative (EC) numerical fluxes from Tadmor [55], semi-discrete EC finite-difference and spectral collocation\\r\\nschemes have been derived in [21, 24]. The framework has been extended to nonconservative hyperbolic systems on Cartesian\\r\\nmeshes in [47, 14] by using EC numerical fluctuations from [9]. In both frameworks, a semi-discrete entropy inequality may be\\r\\nobtained by replacing physical fluxes and nonconservative products with EC numerical fluxes or fluctuations within discretization\\r\\nelements, while using entropy stable ones at interfaces between elements. The design of the latter relies either on adding upwind-\\r\\ntype dissipation [33] to EC numerical fluxes and fluctuations [7, 22, 17, 25, 47, 14], or on designing approximate Riemann solvers\\r\\n[18, 49, 46]. Note that the SBP operators take into account the numerical quadrature when approximating integrals compared to\\r\\nother schemes that require their exact evaluation to achieve entropy stability [34, 30, 31].\\r\\n     Here, we extend the framework proposed in [47] to multidimensional unstructured meshes with curved elements by using\\r\\ntensor multiplication of quadrature rules and function basis [39] that satisfy geometric conservation laws (the so-called metric\\r\\nidentities [38]) at the discrete level. This framework has been recently applied to the approximation with a well-balanced DGSEM\\r\\nof balance laws with geometric source terms on multidimensional high-order meshes in [63]. We here rather focus on specific\\r\\nproperties of discretizations of nonconservative multicomponent flows: preservation of material interfaces, discrete conservation\\r\\nof physical fluxes, maximum principles on purely transported quantities such as the EOS parameters, and entropy stability. The\\r\\nlatter property presents some difficulties due to the form of the entropy associated to SG-gamma model. First, as a model for the\\r\\nmixture, the properties of the individual components, such as mass and void fractions, are not known in general which prevents the\\r\\nevaluation of the entropy variables necessary for the derivation of EC fluctuations from the Castro et al. condition [9]. Then, the\\r\\nentropy is not convex as is often the case in phase transition models [29] which will restrict the entropy stability across shocks not\\r\\ninteracting with material fronts. We here circumvent these difficulties by considering a specific entropy that allows the evaluation\\r\\nof the entropy variables. This entropy satisfies a Gibbs relation and thus defines a complete EOS [42] and is concave with respect\\r\\nto two thermodynamic intensive properties of the mixture, so entropy stability can be ensured when excluding material interfaces.\\r\\nLet us stress that material interfaces do not require the scheme to be entropy stable, but rather a consistent approximation of the\\r\\nenergy equations to ensure pressure equilibrium [1]. We thus also design material interface preserving (CP) fluctuations to preserve\\r\\nat the discrete level pressure and velocity fields across such interfaces.\\r\\n     We then design a HLLC solver [58, 60] for the SG-gamma model that does not require a root-finding algorithm to evaluate\\r\\nthe nonconservative product in contrast to other schemes [19, 11, 56]. We analyze the properties of a three-point scheme using\\r\\nthe HLLC solver and prove that the scheme is robust and entropy stable for convex entropies defining a complete EOS, preserves\\r\\nuniform profiles of pressure and velocity across material interfaces, and satisfies a discrete minimum principle on the specific\\r\\nentropy and maximum principles on the parameters of the EOS. We then apply the HLLC solver at mesh interfaces in the DGSEM\\r\\nscheme and analyze the properties of the fully discrete scheme with an explicit first-order Euler time integration. We derive\\r\\nconditions on the time step so that the cell-averaged solution is a convex combination of degrees-of-freedom (DOFs) and updates\\r\\nof three-point schemes, as a result the scheme inherits the properties of the three-point scheme. In particular, the DGSEM scheme\\r\\nsatisfies a minimum principle on the entropy irrespective of the fluctuations (EC or CP) that are used in the discretization elements.\\r\\nAs a consequence, the DGSEM with CP fluctuations within elements and the HLLC solver at interfaces is able to handle shocks and\\r\\nto preserve material interfaces. Time integration is performed using high-order strong-stability preserving Runge-Kutta schemes\\r\\nthat are convex combinations of explicit Euler schemes, while linear scaling limiters [65, 66] are applied at the end of each stage\\r\\nto impose positivity and maximum principles at all DOFs within discretization elements.\\r\\n    This paper is organized as follows. The SG-gamma model and the entropy pair are described in section 2. We then introduce\\r\\nthe semi-discrete DGSEM scheme on multidimensional and high-order unstructured meshes in section 3. In section 4, we derive\\r\\nCP and EC numerical fluxes for the SG-gamma model, and in section 5, we propose the HLLC approximate Riemann solver\\r\\nand analyze its properties. We recall the main properties of the fully-discrete DGSEM scheme in section 6. A posteriori limiters\\r\\nare described in section 7. The results are assessed by numerical experiments in one and two space dimensions in section 8 and\\r\\nconcluding remarks about this work are given in section 9.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                  2\\r\\n\\x0c\\n\\n2. The SG-gamma model\\r\\n\\r\\n2.1. Governing equations and thermodynamic model\\r\\n     In this work, we consider the gamma-based compressible multicomponent flow model where each component is assumed to\\r\\nbe a stiffened gas [53] and refer to it as the SG-gamma model. The main interest in this model is that the number of unknowns is\\r\\nindependent of the number of components. We are here interested in high-order approximations of the associated Cauchy problem\\r\\nin d space dimensions for flows with n s components:\\r\\n\\r\\n\\r\\n                                                     ∂t u + ∇x · f(u) + c(u)∇x u = 0,                                x ∈ Rd , t > 0,                                 (1a)\\r\\n                                                                                      u(x, 0) = u0 (x), x ∈ Rd ,                                                     (1b)\\r\\n                                                                \\x10                      \\x11\\r\\nwhere x = (x1 , . . . , xd ) are the spatial coordinates, f(u) = f1 (u), . . . , fd (u) , c(u)∇x u = di=1 ci (u)∂ xi u, and\\r\\n                                                                                                    P\\r\\n\\r\\n\\r\\n                                                  \\uf8ec\\uf8ec\\uf8ec ρ \\uf8f7\\uf8f7\\uf8f7                   \\uf8ec\\uf8ec\\uf8ec ρv>\\r\\n                                                  \\uf8eb \\uf8f6                         \\uf8eb                    \\uf8f6                       \\uf8eb                          \\uf8f6\\r\\n                                                                                                   \\uf8f7\\uf8f7\\uf8f7                     \\uf8ec\\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                      \\uf8ec\\uf8ec\\uf8ec ρv \\uf8f7\\uf8f7\\uf8f7                  \\uf8ec\\uf8ec\\uf8ec ρvv + pI \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\r\\n                                                                                            >\\r\\n                                                    \\uf8ec\\uf8ec\\uf8ec \\uf8f7\\uf8f7\\uf8f7                     \\uf8ec\\uf8ec\\uf8ec                  \\uf8f7                       \\uf8ec\\uf8ec\\uf8ec                      \\uf8f7\\r\\n                                                                                                                               \\uf8ec\\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\r\\n                                           u = \\uf8ec\\uf8ec\\uf8ec\\uf8ecρE \\uf8f7\\uf8f7\\uf8f7\\uf8f7 ,          f(u) = \\uf8ec\\uf8ec(ρE + p)v \\uf8f7\\uf8f7\\uf8f7\\uf8f7 ,>\\uf8f7\\r\\n                                                                                                               c(u)∇x u = \\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7\\uf8f7 ,                                 (2)\\r\\n                                                        \\uf8ec \\uf8f7                         \\uf8ec\\uf8ec\\uf8ec                                          \\uf8ec\\uf8ec\\uf8ec                  \\uf8f7\\r\\n                                                          \\uf8ec\\uf8ec\\uf8ec Γ \\uf8f7\\uf8f7\\uf8f7                          0                                       \\uf8ec\\uf8ec\\uf8ec v · ∇x Γ \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\r\\n                                                        \\uf8ec\\uf8ec\\uf8ec \\uf8f7\\uf8f7\\uf8f7                       \\uf8ec\\uf8ec\\uf8ec            \\uf8f7\\uf8f7\\uf8f7                           \\uf8ec\\uf8ec\\uf8ec                \\uf8f7\\r\\n                                                                                        \\uf8ec\\uf8ec\\uf8ec            \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                            \\uf8edΠ\\uf8f8                           \\uf8ed  0           \\uf8f8                             \\uf8edv · ∇ Π\\uf8f8\\r\\n                                                                                                                                              x\\r\\n\\r\\n\\r\\nrepresent the vector of state variables, the physical fluxes and the nonconservative products1 , respectively. The mixture density,\\r\\nmomentum, total energy, and internal energy are defined as\\r\\n\\r\\n                                         ns                             ns                                                                        ns\\r\\n                                         X                              X                               1                                         X\\r\\n                                   ρ=          αi ρi ,          ρv =           αi ρi vi ,      ρE = ρe + ρv · v,                      ρe =              αi ρi ei ,\\r\\n                                         i=1                             i=1\\r\\n                                                                                                        2                                         i=1\\r\\n\\r\\nwhere ρi , vi , and ei represent the density, the velocity vector and the specific internal energy of the ith component. The model\\r\\nassumes immiscible phases and thus imposes a saturation condition on the void fractions αi :\\r\\n                                                                                        ns\\r\\n                                                                                        X\\r\\n                                                                                               αi = 1.                                                                (3)\\r\\n                                                                                         i=1\\r\\n\\r\\n\\r\\n     The partial pressures are related to partial densities and specific internal energies through the stiffened gas EOS:\\r\\n\\r\\n                                                                                                                         p∞i\\r\\n                                      pi (ρi , ei ) = (γi − 1)ρi ei − γi p∞i ,                    ei = Cvi T i +             ,        i = 1, . . . , n s ,            (4)\\r\\n                                                                                                                          ρi\\r\\nwhere γi = Cp i /Cvi > 1 is the ratio of specific heats, T i is the temperature of the species, and p∞i > 0 is a pressure-like constant.\\r\\nObserve that when p∞i = 0 in (4) we recover the polytropic EOS. Assuming thermal equilibrium of the species, T i (ρi , ei ) = T (ρ, e),\\r\\n1 6 i 6 n s , the EOS for the mixture is indeed conveniently defined by [53]\\r\\n\\r\\n                                                                                                                             ns\\r\\n                                                    p + γp∞                                                                  X\\r\\n                                                            = pΓ + Π = ρe,                           ρe = ρCv T +                   αi p∞i ,                          (5)\\r\\n                                                    (γ − 1)                                                                   i=1\\r\\n\\r\\nwhere Cv = ni=1   Yi Cvi denotes the specific heat at constant volume of the mixture, the Yi = αiρρi are the mass fractions of the\\r\\n              Ps\\r\\nspecies, and the EOS parameters Γ and Π are defined by\\r\\n\\r\\n                                                                   ns                                               ns\\r\\n                                                             1    X      αi                                  γp∞   X   αi γi p∞i\\r\\n                                                 Γ=             =            ,                    Π=             =               .                                    (6)\\r\\n                                                            γ−1   i=1\\r\\n                                                                      γ i −1                                 γ−1   i=1\\r\\n                                                                                                                       γi − 1\\r\\n\\r\\n\\r\\n\\r\\n                                                                           i jk = ck (u)i j with the ck (u) in R\\r\\n   1 The components of the third-order tensor in (1a) read c(u)                                                 neq ×neq , so the ith component of c(u)∇u reads\\r\\nPneq Pd\\r\\n j=1 k=1 c(u)i jk ∂ xk u j . Likewise, for n = (n1 , . . . , nd ) , we have c(u)n = i=1 ni ci (u).\\r\\n                                                                 >                    Pd\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                3\\r\\n\\x0c\\n\\n    Hyperbolicity of the SG-gamma model requires that the solutions to (1) belong to the set of states\\r\\n                                          n                                                 o\\r\\n                                     ΩGM = u ∈ Rneq : ρ > 0, v ∈ Rd , ρe > p∞ , Γ > 0, Π > 0 ,                                         (7)\\r\\n                                                          \\x10                   \\x11\\r\\nwith neq = d + 4. The matrix-valued function         i=1 ni fi (u) + ci (u)\\r\\n                                                             0\\r\\n                                                 Pd\\r\\n                                                                                  in Rneq ×neq admits real eigenvalues\\r\\n\\r\\n                               λ1 (u) = v · n − c,    λ2 (u) = · · · = λneq −1 (u) = v · n,          λneq (u) = v · n + c,             (8)\\r\\n\\r\\nfor all unit vector n = (n1 , . . . , nd ), where c = γ(γ − 1)(ρe − p∞ )/ρ is the speed of sound of the mixture. Here, {λi }26i6neq −1 are\\r\\n                                                     p\\r\\n\\r\\nassociated to linearly degenerate (LD) fields, while λ1 and λneq are associated to genuinely nonlinear (GNL) fields. Observe that\\r\\n(1a) is not strictly hyperbolic as the eigenvalues associated to the LD fields are not distinct.\\r\\n\\r\\n\\r\\n2.2. Entropy pair\\r\\n    Solutions to (1) may develop discontinuities and (1a) has to be understood in the sense of distributions where we look for weak\\r\\nsolutions. Weak solutions are not necessarily unique and (1) must be supplemented with further admissibility conditions to select\\r\\nthe physical solution. We here focus on entropy inequalities\\r\\n\\r\\n                                               ∂t η(u) + ∇x · q(u) 6 0,                 x ∈ Rd , t > 0,                                (9)\\r\\n\\r\\nfor some convex entropy – entropy flux pair (η(u), q(u)). One common way to derive such pair consists in considering partial\\r\\nentropies of the species si (ρi , θ) = −Cvi ln θ + (γi − 1) ln ρi , where θ = 1/T is the inverse of the temperature, that satisfies a Gibbs\\r\\n                                                                 \\x01\\r\\nrelation\\r\\n\\r\\n                                                                                   pi\\r\\n                                                              T dsi = dei −            dρi ,                                          (10)\\r\\n                                                                                   ρ2i\\r\\n\\r\\nso the mixture entropy reads\\r\\n\\r\\n                                               ns\\r\\n                                               X                          ns\\r\\n                                                                          X\\r\\n                                                     Yi si = −Cv ln θ −            Yi (γi − 1)Cvi ln ρi .\\r\\n                                               i=1                        i=1\\r\\n\\r\\n    Unfortunately, we cannot use this entropy to derive EC fluxes in section 4 because they would require to evaluate the Yi which\\r\\nis not possible from u in (2). Hence, here we consider an alternative pair\\r\\n\\r\\n                                                         η(u) = −ρs,          q(u) = −ρsv,                                            (11)\\r\\n\\r\\nwhere, upon introducing τ = ρ1 the covolume of the mixture, the specific entropy reads\\r\\n\\r\\n                                                          p + p∞ (6) \\x12 \\x12       Π \\x13 1\\r\\n                                                                !                                  \\x13\\r\\n                               s(τ, e, Cv , Γ, Π) = Cv ln         = Cv ln e −     τ +   ln τ − ln Γ  .                                (12)\\r\\n                                                            ργ                Γ+1     Γ\\r\\n   The rationale for considering this entropy is as follows. First, we will see in section 4.2 that the EC fluctuations can be explicitly\\r\\ncomputed without knowledge of the individual mass fractions. Then, for smooth solutions of (1a), we have\\r\\n\\r\\n                             ∂t Cv + v · ∇x Cv = 0,     ∂t τ + v · ∇x τ = τ∇x · v,             ∂t e + v · ∇x e = −τp∇x · v,\\r\\n\\r\\nhence\\r\\n\\r\\n                                                                                                                          !\\r\\n                            \\x10               \\x11      \\x10               \\x11                                    1   p∞       p      (5)\\r\\n      ∂t s + v · ∇x s = ∂τ s ∂t τ + v · ∇x τ + ∂e s ∂t e + v · ∇x e = τ∇x · v(∂τ s − p∂e s) = Cv ∇x · v   −      −          = 0,\\r\\n                                                                                                        Γ ρe − p∞ ρe − p∞\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                          4\\r\\n\\x0c\\n\\nso the mixture entropy is conserved, i.e., (9) is an equality. Moreover, (τ, e) 7→ s(τ, e, Cv , Γ, Π) is obviously strictly concave in\\r\\nΩGM and from the first equality in the above relation, we conclude that the inequality in (9) makes sense even if η(u) is not strictly\\r\\nconvex which is often the case in phase transition models [29]. Finally, differentiating (12) while fixing Cv , Γ and Π, gives\\r\\n\\r\\n                                                                                        ρCv\\r\\n                                                        !                         !                    !\\r\\n                                             dp      dρ (5)    (γ − 1)d(ρe)    dρ                  p\\r\\n                               ds = Cv            −γ      = Cv              −γ      =         de − 2 dρ ,                        (13)\\r\\n                                           p + p∞     ρ           p + p∞        ρ     ρe − p∞     ρ\\r\\n\\r\\nso the entropy satisfies a Gibbs relation similar to (10) but with a different temperature T̃ = ρC1 v (ρe − p∞ ) instead of T in (5).\\r\\nThe entropy hence defines a complete EOS with both pressure and temperature [42]. This latter observation will be important in\\r\\nsection 5 to prove entropy stability of the HLLC solver through the existence of local minimum entropy principles (see proof of\\r\\nLemma 5.1).\\r\\n    We end this section by deriving the entropy variables associated to the entropy η(u) in (11) when considering pure phases. For\\r\\npure phases, we have dγ = dΠ = 0 so we obtain\\r\\n\\r\\n                                 (12)        d(p + p∞ )                (5)              dρe\\r\\n            dη = −ρds − sdρ = −ρCv                      + γCv dρ − sdρ = −ρCv (γ − 1)        + γCv dρ − sdρ\\r\\n                                              p + p∞                                  p + p∞\\r\\n                                                                            (γ − 1)Cv \\x12                 v·v \\x13\\r\\n                                                                       = −ρ             dρE − v · dρv +     dρ + γCv dρ − sdρ\\r\\n                                                                             p + p∞                      2\\r\\n                                                                       (15)\\r\\n                                                                                              \\x12            v · v\\x13\\r\\n                                                                        = −ζdρE + ζv · dρv + γCv − s − ζ          dρ,\\r\\n                                                                                                            2\\r\\n\\r\\nand we thus obtain the following expression of the entropy variables\\r\\n\\r\\n                                                            \\uf8ec\\uf8ec\\uf8ecγCv − s − ζ v·v\\r\\n                                                            \\uf8eb                  \\uf8f6\\r\\n                                                                               \\uf8f7\\uf8f7\\r\\n                                                                             2 \\uf8f7\\r\\n                                                                          ζv\\r\\n                                                              \\uf8ec\\uf8ec\\uf8ec               \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                  ∂\\r\\n                                                                \\uf8ec\\uf8ec\\uf8ec               \\uf8f7\\uf8f7\\uf8f7\\r\\n                                         ϑ(u) =     η(u) = \\uf8ec\\uf8ec             −ζ                       ∀u ∈ ΩGM ,                    (14)\\r\\n                                                                  \\uf8ec\\uf8ec\\uf8ec               \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                 ∂u\\r\\n                                                                                      \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                                          0\\r\\n                                                                    \\uf8ec\\uf8ec\\uf8ec                 \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                                      \\uf8ec\\uf8ec\\uf8ec                 \\uf8f7\\uf8f7\\uf8f8\\r\\n                                                                        \\uf8ed 0\\r\\n\\r\\nwhere\\r\\n                                                                                (γ − 1)Cv ρ\\r\\n                                                                           ζ=               .                                    (15)\\r\\n                                                                                  p + p∞\\r\\n\\r\\n\\r\\n3. The discontinuous Galerkin spectral element method (DGSEM)\\r\\n\\r\\n     In this section, we recall the DGSEM framework [14, 39, 47] which is used to discretize the Cauchy problem (1). Here, the\\r\\nspace domain Ω = Rd is discretized using a mesh Ωh consisting of nonoverlapping and nonempty cells κ (quadrangles for d = 2\\r\\nand hexahedra for d = 3) forming a partition of Ω. By Eh we denote the set of interfaces in Ωh . For the sake of clarity, we introduce\\r\\nthe DGSEM in two space dimensions d = 2, as the extension to d = 3 is straightforward while its derivation for d = 1 can be found\\r\\nin [14, 48].\\r\\n\\r\\n\\r\\n3.1. Numerical approximation and function space\\r\\n     Let us consider the reference element I2 = [−1, 1]2 with coordinates ξ = (ξ, η) and the reference edge I = [−1, 1], the functions\\r\\nxκ (ξ) and xe (ξ) map reference to physical element and edge, respectively. The approximate solution of (1) is sought in the function\\r\\nspace of piecewise polynomials\\r\\n                                                      n                                               o\\r\\n                                                 Vhp = φ ∈ L2 (Ωh ) : φκ ◦ xκ (ξ) ∈ Q p (I2 ), ∀κ ∈ Ωh ,\\r\\n\\r\\nwhere Q p (I2 ) denotes the space of polynomials over the reference element I2 formed by the tensor product of polynomials of degree\\r\\nat most p in each direction. The approximate solution reads\\r\\n\\r\\n                                                             p\\r\\n                                                             X\\r\\n                                              uh (x, t) :=            φiκj (x)Uiκj (t) ∀x ∈ κ,   κ ∈ Ωh ,   t > 0,               (16)\\r\\n                                                             i, j=0\\r\\n\\r\\n\\r\\n                                                                                   5\\r\\n\\x0c\\n\\n                                    •               •\\r\\n                                                                                •             e\\r\\n                                                                                              •\\r\\n                                        •         xiκj •                                            u+h\\r\\n                                                                                •               •\\r\\n                                                                                          u−h\\r\\n                                                               κ=κ          −                                                   κe+\\r\\n\\r\\n                                            •              •                         •     xke •\\r\\n                                                                                                                  nke\\r\\n                                                    •          •                      •           •\\r\\n\\r\\n                                                                                                                                            ij\\r\\nFigure 1: Notations for the mesh in two space dimensions: cell κ ∈ Ωh with quadrature points xκ (bullets •), edge e ∈ ∂κ with quadrature point xke ,\\r\\nassociated unit outward normal nke , and traces of the approximate solution u±h on e; adjacent cell κe+ sharing edge e.\\r\\n\\r\\n\\r\\n\\r\\nwhere {φiκj }06i, j6p constitutes a basis of Vhp restricted onto κ, with dimension (p + 1)2 , and {Uiκj }06i, j6p are the DOFs. Let `06k6p\\r\\ndenote the Lagrange interpolation polynomials associated to the Gauss-Lobatto nodes over I: −1 = ξ0 < ξ1 < · · · < ξ p = 1. We\\r\\ndefine the basis functions as the tensor products of these polynomials:\\r\\n\\r\\n                                                        φiκj (x) = φi j (xκ (ξ)) = `i (ξ)` j (η),                       0 6 i, j 6 p,                          (17)\\r\\n\\r\\nwhich satisfy the following cardinality relation\\r\\n\\r\\n                                            0 0\\r\\n                                   φiκj (xiκ j ) = φiκj (xκ (ξi0 j0 )) = `i (ξi0 )` j (η j0 ) = δii0 δ j j0 ,                         0 6 i, i0 , j, j0 6 p,   (18)\\r\\n\\r\\nwhere δii0 is the Kronecker delta. The DOFs are therefore point values of the solution: Uiκj (t) = uh (xiκj , t). Likewise, the elements\\r\\nκ are interpolated on the same grid of quadrature points as the numerical solution, i.e., xκ (ξ) = 06i, j6p `i (ξ)` j (η)xiκj and xe (ξ) =\\r\\n                                                                                                   P\\r\\n\\r\\n  k=0 `i (ξ)xe (see Figure 1).\\r\\nPp           k\\r\\n\\r\\n\\r\\n    The integrals over elements and interfaces are approximated by using Gauss-Lobatto quadrature rules where the quadrature\\r\\nand interpolation points are collocated:\\r\\n\\r\\n                                        Z                          p\\r\\n                                                                   X                                      Z                           p\\r\\n                                                                                                                                      X\\r\\n                                                f (x)dV ≈                   ωi ω j Jκi j f (xiκj ),               f (x)dS ≈                 ωk Jek f (xke ),   (19)\\r\\n                                            κ                      i, j=0                                     e                       k=0\\r\\n\\r\\n\\r\\nwhere ωi , ω j > 0 are the quadrature weights and Jκi j = Jκ (xiκj ) = |∇ξ xκ (ξi j )|, and Jek = Je (xke ) = |∇ξ xe (ξk )|. The cell-averaged solution\\r\\nthus reads\\r\\n\\r\\n                                                                            p\\r\\n                                                                                              Jκi j i j\\r\\n                                                                                                                        Z\\r\\n                                                                            X                               1\\r\\n                                                        huiκ (t) :=                  ωi ω j        U (t) ≈                      uh (x, t)dV,                   (20)\\r\\n                                                                            i, j=0\\r\\n                                                                                              |κ| κ        |κ|              κ\\r\\n\\r\\n\\r\\nwhere |κ| =     i, j=0 ωi ω j Jκ is the volume of the cell κ.\\r\\n              Pp               ij\\r\\n\\r\\n\\r\\n    We also introduce the discrete difference matrix with entries\\r\\n\\r\\n                                                                            Dik = `k0 (ξi ),            0 6 i, k 6 p,                                          (21)\\r\\n\\r\\n                         l=0 `l ≡ 1 implies\\r\\n                      Pp\\r\\nwhere the property\\r\\n\\r\\n                                                                             p\\r\\n                                                                             X\\r\\n                                                                                     Dkl = 0          ∀0 6 k 6 p.                                              (22)\\r\\n                                                                             l=0\\r\\n\\r\\n    The discrete difference matrix is known to satisfy the SBP property [39]:\\r\\n\\r\\n                                                        ωk Dkl + ωl Dlk = δkp δlp − δk0 δl0                             ∀0 6 k, l 6 p.                         (23)\\r\\n\\r\\n                                                                                                    6\\r\\n\\x0c\\n\\n     Finally, the discretization is assumed to satisfy the following metric identities [38]\\r\\n\\r\\n                                                        p\\r\\n                                                        X\\r\\n                                                              Dik Jκk j ∇x ξ(xkκ j ) + D jk Jκik ∇x η(xikκ ) = 0   ∀0 6 i, j 6 p,                                      (24)\\r\\n                                                        k=0\\r\\n\\r\\nand volume and edge metric terms are related by\\r\\n\\r\\n\\r\\n  Jκp j ∇ξ(xκp j ) = Je (xκp j )ne (xκp j ),       Jκ0 j ∇ξ(x0κ j ) = −Je (x0κ j )ne (x0κ j ),              κ ) = Je (xκ )ne (xκ ),\\r\\n                                                                                                   Jκip ∇η(xip         ip      ip\\r\\n                                                                                                                                               κ ) = −Je (xκ )ne (xκ ), (25)\\r\\n                                                                                                                                      Jκi0 ∇η(xi0          i0      i0\\r\\n\\r\\n\\r\\n\\r\\nfor 0 6 i, j 6 p, where ne denotes the unit normal to e in ∂κ pointing outward from κ (see Figure 1).\\r\\n\\r\\n\\r\\n3.2. Semi-discrete form\\r\\n   Following [23, 14, 47], we multiply (1a) with a test function vh ∈ Vhp and perform double integration-by-parts to get the\\r\\nsemi-discrete weak form of (1a): find uh in (Vhp )neq such that\\r\\n\\r\\n               XZ                                                    XZ\\r\\n                                                                        v−h D− (u−h , u+h , ne ) + v+h D+ (u−h , u+h , ne )dS = 0 ∀vh ∈ Vhp ,\\r\\n                             \\x10                                 \\x11\\r\\n                           vh ∂t uh + ∇x · f(uh ) + c(uh )∇x uh dV +\\r\\n               κ∈Ωh    κ                                                             e∈Eh    e\\r\\n\\r\\n\\r\\nwhere u±h (x, t) = limε↓0 uh (x ± εne (x), t) are the traces of uh at x on a given cell interface e ∈ Eh (see Figure 1) and D± (·, ·, ·) are the\\r\\nnumerical fluctuations that are applied at the interfaces and are considered under the form\\r\\n\\r\\n\\r\\n                                                         D− (u− , u+ , n) = h(u− , u+ , n) − f(u− ) · n + d− (u− , u+ , n),                                           (26a)\\r\\n                                                         D+ (u− , u+ , n) = f(u+ ) · n − h(u− , u+ , n) + d+ (u− , u+ , n),                                           (26b)\\r\\n\\r\\nto allow proper discretizations of each term in (1a), they satisfy the consistency relations\\r\\n\\r\\n                                                              h(u, u, n) = f(u) · n,             d± (u, u, n) = 0 ∀u ∈ ΩGM ,                                           (27)\\r\\n\\r\\nand will be introduced in section 5.\\r\\n     We then substitute vh for the Lagrange interpolation polynomials (17) and consider the quadrature rules (19). Using the discrete\\r\\ndifference matrix (21), the semi-discrete problem reads: find uh in (Vhp )neq such that\\r\\n\\r\\n                                                             p \\x12\\r\\n                                       d ij                 X       \\x10                         \\x11                  \\x10                       \\x11           \\x13\\r\\n                        ωi ω j Jκi j      Uκ + ωi ω j Jκi j      Dik f(Ukκ j ) + c(Uiκj )Ukκ j ∇x ξ(ξi j ) + D jk f(Uikκ ) + c(Uiκj )Uikκ ∇x η(ξi j )\\r\\n                                       dt                   k=0\\r\\n                                                    p\\r\\n                                                                                                                                                                       (28)\\r\\n                                                   XX\\r\\n                                                              φiκj (xke )ωk Jek D− Uiκj , u+h (xke , t), nke ∀t > 0,\\r\\n                                                                                  \\x10                         \\x11\\r\\n                                               +                                                                          κ ∈ Ωh ,    0 6 i, j 6 p,\\r\\n                                                   e∈∂κ k=0\\r\\n\\r\\n\\r\\nwhere by (18) φiκj (xke ) = 1 if xiκj = xke and φiκj (xke ) = 0 else.\\r\\n     The initial condition (1b) is projected onto the function space:\\r\\n\\r\\n                                                                  Uiκj (0) = u0 (xκi j ) ∀κ ∈ Ωh ,          0 6 i, j 6 p.                                              (29)\\r\\n\\r\\n    The integral over elements κ ∈ Ωh in (28) should be modified where we replace the physical fluxes and nonconservative\\r\\nproducts with numerical fluctuations of the form [9, 10, 14, 47]\\r\\n\\r\\n\\r\\n                                                        D−X (u− , u+ , n) = hX (u− , u+ , n) − f(u− ) · n + d−X (u− , u+ , n),                                        (30a)\\r\\n                                                        D+X (u− , u+ , n) = f(u+ ) · n − hX (u− , u+ , n) + d+X (u− , u+ , n),                                        (30b)\\r\\n\\r\\n\\r\\n                                                                                                  7\\r\\n\\x0c\\n\\nwhere the subscript X will refer to either ec or cp to denote either entropy conservative or contact preserving numerical fluctuations,\\r\\nrespectively, that will be introduced in section 4. The modified scheme now reads\\r\\n\\r\\n                                                    p\\r\\n                                    d ij           X   \\x10                                                                         \\x11\\r\\n                     ωi ω j Jκi j      Uκ + ωi ω j       Dik D̃X (Uiκj , Ukκ j , n(i,k) j ) + D jk D̃X (Uiκj , Uikκ , ni( j,k) )\\r\\n                                    dt             k=0\\r\\n                                                 p\\r\\n                                                                                                                                                                             (31)\\r\\n                                                XX\\r\\n                                                               φiκj (xke )ωk Jek D− Uiκj , u+h (xke , t), nke\\r\\n                                                                                    \\x10                           \\x11\\r\\n                                            +                                                                         ∀t > 0,         κ ∈ Ωh ,   0 6 i, j 6 p,\\r\\n                                                e∈∂κ k=0\\r\\n\\r\\nwhere\\r\\n\\r\\n\\r\\n                                 D̃X (u− , u+ , n) := D−X (u− , u+ , n) − D+X (u+ , u− , n),                                                                                (32a)\\r\\n                                                        (30)\\r\\n                                                        = hX (u− , u+ , n) + hX (u+ , u− , n) + d−X (u− , u+ , n) − d+X (u+ , u− , n),                                      (32b)\\r\\n                                                        (22)\\r\\n\\r\\n\\r\\nand\\r\\n\\r\\n                                               1 \\x10 ij                               \\x11                               1 \\x10 ij                              \\x11\\r\\n                                  n(i,k) j =      Jκ ∇x ξ(ξi j ) + Jκk j ∇x ξ(ξk j ) ,             ni( j,k) =          Jκ ∇x η(ξi j ) + Jκik ∇x η(ξik )                      (33)\\r\\n                                               2                                                                    2\\r\\nmust be introduced to keep conservation of the physical fluxes [64] and preserve uniform states.\\r\\n      The numerical flux and fluctuations in (30) satisfy the consistency conditions\\r\\n\\r\\n                                                     hX (u, u, n) = f(u) · n,                d±X (u, u, n) = 0            ∀u ∈ ΩGM ,                                         (34)\\r\\n\\r\\nand entropy conservative (EC) fluctuations D±ec (·, ·, ·) satisfy [9]\\r\\n\\r\\n                                      ϑ(u− )> D−ec (u− , u+ , n) + ϑ(u+ )> D+ec (u− , u+ , n) = ~q(u)\\x7f · n                              ∀u± ∈ ΩGM ,                          (35)\\r\\n\\r\\nwhere ~vh \\x7f = v+h − v−h and ϑ(u) := ∇u η(u) denotes the entropy variables. Additionally, the interface fluctuations (26) in (28) are\\r\\nassumed to be entropy stable:\\r\\n\\r\\n                                       ϑ(u− )> D− (u− , u+ , n) + ϑ(u+ )> D+ (u− , u+ , n) > ~q(u)\\x7f · n                                ∀u± ∈ ΩGM .                           (36)\\r\\n\\r\\n     The theorem below summarizes the main properties of the semi-discrete scheme (31) for the discretization of general systems\\r\\nof the form (1a).\\r\\nTheorem 3.1. Let D±X (·, ·, ·) in (30) be consistent fluctuations with d±X (·, ·, ·) in (32) satisfying\\r\\n                                                                       d±X (u− , u+ , n) = C± (u− , u+ , n)~u\\x7f,                                                             (37a)\\r\\n                                                                                    +             +             +                       +\\r\\n                                                               C(u , u , n) := C (u , u , n) + C (u , u , n),\\r\\n                                                                              −                        −                     −    −\\r\\n                                                                                                                                                                            (37b)\\r\\n                                                           +\\r\\n                                                C(u , u , n) + C(u+ , u− , n) = c(u− ) + c(u+ ) n,\\r\\n                                                                               \\x10               \\x11\\r\\n                                                    −\\r\\n                                                                                                                                                                            (37c)\\r\\n                                                                            C(u, u, n) = c(u)n,                                                                             (37d)\\r\\n\\r\\nwhere n = (n1 , . . . , nd )> , c(u)n = i=1 ni ci (u), and ~u\\x7f = u+ − u− , and let D− (·, ·, ·) in (28) be consistent (27) fluctuations. Then,\\r\\n                                               Pd\\r\\nthe semi-discrete scheme (31) has the following properties:\\r\\n\\r\\n   (i) it is a high-order accurate approximation of smooth enough solutions to (1);\\r\\n  (ii) it preserves uniform states (free-stream preservation);\\r\\n  (iii) the cell-averaged solution (20) satisfies the following cell-averaged semi-discrete scheme\\r\\n                                              p\\r\\n                         d                X             \\x10                                                        \\x11\\r\\n                   |κ|      huh iκ (t) +          ωi ω j Dik c(Uiκj )n(i,k) j Ukκ j + D jk c(Uiκj )ni( j,k) Uikκ\\r\\n                         dt              i, j,k=0\\r\\n                                             p\\r\\n                                                                                                                                                                             (38)\\r\\n                                            XX                    \\x12 \\x10                                                                         \\x11\\x13\\r\\n                                                                   h u−h (xke , t), u+h (xke , t), nke + d− u−h (xke , t), u+h (xke , t), nke\\r\\n                                                                                                      \\x11    \\x10\\r\\n                                        +               ωk Jek                                                                                         ∀t > 0,   κ ∈ Ωh ,\\r\\n                                            e∈∂κ k=0\\r\\n\\r\\n                                                                                              8\\r\\n\\x0c\\n\\n        which ensures a discretely conservative approximation of the physical fluxes in (1a);\\r\\n  (iv) if the fluctuations DX (·, ·, ·) in the volume integral are further assumed to be EC (35) and the fluctuations at interfaces\\r\\n       D(·, ·, ·) are entropy stable (36) for a convex entropy pair (η(u), q(u)), then the following semi-discrete entropy inequality\\r\\n       holds:\\r\\n                                                        p\\r\\n                                     d            XX\\r\\n                                                           ωk Jek Q u−h (xke , t), u+h (xke , t), nke 6 0 ∀t > 0, κ ∈ Ωh ,\\r\\n                                                                   \\x10                                 \\x11\\r\\n                                 |κ| hη(uh )iκ +                                                                                (39)\\r\\n                                    dt            e∈∂κ k=0\\r\\n\\r\\n        with the consistent and conservative entropy flux\\r\\n\\r\\n                                                                   1\\x10 −                  1                          1\\r\\n                                         Q(u− , u+ , n) =            q(u ) + q(u+ ) · n + ϑ(u− )> D− (u− , u+ , n) − ϑ(u+ )> D+ (u− , u+ , n).\\r\\n                                                                                   \\x11\\r\\n                                                                                                                                                                                                     (40)\\r\\n                                                                   2                     2                          2\\r\\n\\r\\nProof. Preliminary: By the metric identities (24) and (22), the metric terms in (33) satisfy\\r\\n\\r\\n                            p                                             p\\r\\n                            X                                          1 X \\x10 ij                                 \\x11      \\x10                                 \\x11\\r\\n                                  Dik n(i,k) j + D jk ni( j,k) =             Dik Jκ ∇ξ(xiκj ) + Jκk j ∇ξ(xkκ j ) + D jk Jκi j ∇η(xiκj ) + Jκk j ∇η(xikκ ) = 0.                                       (41)\\r\\n                            k=0\\r\\n                                                                       2 k=0\\r\\n\\r\\n     High-order accuracy: It is sufficient to prove that the volume integral in (31) is a high-order approximation of ∇ · f(u) + c(u)∇u\\r\\nat (xiκj , t) for smooth enough solutions. High-order accuracy of the conservative term ∇ · f(u) has been proved in [12, 45] and the\\r\\nhigh-order accuracy of c(u)∇u in one space dimension has been proved in [47, Th. 3.2]. Since we are using tensor products of\\r\\none-dimensional operators, the proof of accuracy follows by considering each space dimension independently.\\r\\n    Free-stream preservation: Let us assume that Uiκj = U in the space residuals for all κ ∈ Ωh and 0 6 i, j 6 p, then by\\r\\nconsistency: D− (U, U, nke ) = 0 and D̃X (u− , u+ , n) = 2f(U) · n. Further using (41), (31) becomes\\r\\n\\r\\n                                                                                   p\\r\\n                                                                   d ij           X                                                      d\\r\\n                                                0 = ωi ω j Jκi j      Uκ + ωi ω j     2f(U)(Dik n(i,k) j + D jk ni( j,k) ) = ωi ω j Jκi j Uiκj .\\r\\n                                                                   dt             k=0\\r\\n                                                                                                                                         dt\\r\\n\\r\\n    Cell-averaged semi-discrete scheme: Summing up (31) over 0 6 i, j 6 p gives\\r\\n\\r\\n                                                                p          p                p\\r\\n                                               d               X          X           XX\\r\\n                                                                                               ωk Jek D− u−h (xke , t), u+h (xke , t), nke = 0,\\r\\n                                                                                                        \\x10                                 \\x11\\r\\n                                         |κ|      huh iκ (t) +     ωjAj +     ωi Bi +\\r\\n                                               dt              j=0        i=0         e∈∂κ k=0\\r\\n\\r\\nwhere from (32), we have\\r\\n\\r\\n\\r\\n        p\\r\\n        X                                                   p\\r\\n                                                            X\\r\\n                                                                    ωi Dik hX (Uiκj , Ukκ j , n(i,k) j ) + hX (Ukκ j , Uiκj , n(i,k) j ) + d− (Uiκj , Ukκ j , n(i,k) j ) − d+ (Ukκ j , Uiκj , n(i,k) j )\\r\\n                                                                          \\x10                                                                                                                              \\x11\\r\\n Aj =           ωi Dik D̃X (Uiκj , Ukκ j , n(i,k) j ) =\\r\\n        i,k=0                                               i,k=0\\r\\n        p\\r\\n        X                                                   p\\r\\n                                                            X\\r\\n                                                                    ω j D jk hX (Uiκj , Uikκ , ni( j,k) ) + hX (Uikκ , Uiκj , ni( j,k) ) + d− (Uiκj , Uikκ , ni( j,k) ) − d+ (Uikκ , Uiκj , ni( j,k) )\\r\\n                                                                            \\x10                                                                                                                          \\x11\\r\\n Bi =           ω j D jk D̃X (Uiκj , Uikκ , ni( j,k) ) =\\r\\n        j,k=0                                               j,k=0\\r\\n\\r\\n\\r\\n    Let us consider the first term. Using (25), we have\\r\\n\\r\\n\\r\\n                  (23)\\r\\n           A j = Je (xκp j )f(Uκp j ) · ne (xκp j ) + Je (x0κ j )f(U0κ j ) · ne (x0κ j )\\r\\n                  (34)\\r\\n                      p\\r\\n                      X\\r\\n                  +           ωi Dik hX (Uiκj , Ukκ j , n(i,k) j ) − ωk Dki hX (Ukκ j , Uiκj , n(i,k) j ) + ωi Dik d− (Uiκj , Ukκ j , n(i,k) j ) + ωk Dki d+ (Ukκ j , Uiκj , n(i,k) j )\\r\\n                      i,k=0\\r\\n                                                                                                            p\\r\\n                                                                                                            X\\r\\n                    i↔k\\r\\n                      = Je (xκp j )f(Uκp j ) · ne (xκp j ) + Je (x0κ j )f(U0κ j ) · ne (x0κ j ) +                   ωi Dik C(Uiκj , Ukκ j , n(i,k) j )(Ukκ j − Uiκj )\\r\\n                  (37a,b)\\r\\n                                                                                                            i,k=0\\r\\n                   (23)              \\x10                                                         \\x11                \\x10                                                    \\x11\\r\\n                    =     Je (xκp j ) f(Uκp j ) · ne (xκp j ) − c(Uκp j )ne (xκp j )Uκp j          + Je (x0κ j ) f(U0κ j ) · ne (x0κ j ) − c(U0κ j )ne (x0κ j )U0κ j\\r\\n                  (37d)\\r\\n                      p\\r\\n                      X\\r\\n                  +           ωi Dik C(Uiκj , Ukκ j , n(i,k) j )Ukκ j + ωk Dki C(Uiκj , Ukκ j , n(i,k) j )Uiκj\\r\\n                      i,k=0\\r\\n\\r\\n                                                                                                        9\\r\\n\\x0c\\n\\n                   i↔k\\r\\n                                \\x10                                                   \\x11             \\x10                                                    \\x11\\r\\n                   = Je (xκp j ) f(Uκp j ) · ne (xκp j ) − c(Uκp j )ne (xκp j )Uκp j + Je (x0κ j ) f(U0κ j ) · ne (x0κ j ) − c(U0κ j )ne (x0κ j )U0κ j\\r\\n                  (37c)\\r\\n                      p\\r\\n                      X             \\x10                    \\x11\\r\\n                  +           ωi Dik c(Uiκj ) + c(Ukκ j ) n(i,k) j Ukκ j\\r\\n                      i,k=0\\r\\n                                                                                                    p\\r\\n                                                                                                    X\\r\\n                   (23)\\r\\n                    = Je (xκp j )f(Uκp j ) · ne (xκp j ) + Je (x0κ j )f(U0κ j ) · ne (x0κ j ) +             ωi Dik c(Uiκj )n(i,k) j (Ukκ j − Uiκj ),\\r\\n                  (37d)\\r\\n                                                                                                    i,k=0\\r\\n\\r\\n\\r\\nwhere i ↔ k indicates an inversion of indices i and k in some of the terms. Likewise, we have\\r\\n                                                                                                                    p\\r\\n                                                                                                                    X\\r\\n                                               κ )f(Uκ ) · ne (xκ ) + Je (xκ )f(Uκ ) · ne (xκ ) +\\r\\n                                     Bi = Je (xip    ip         ip         i0    i0         i0\\r\\n                                                                                                                            ω j D jk c(Uiκj )ni( j,k) (Uikκ − Uiκj ).\\r\\n                                                                                                                    j,k=0\\r\\n\\r\\n\\r\\n    From (41) we deduce\\r\\n                      p\\r\\n                      X                     p\\r\\n                                            X                p\\r\\n                                                            XX                                      p\\r\\n                                                                                                    X\\r\\n                                                                               \\x10             \\x11              \\x10                                                       \\x11\\r\\n                             ωjAj +               ωi Bi =              ωk Jek h u−h (xke , t) nke +   ωi ω j Dik c(Uiκj )n(i,k) j Ukκ j + D jk c(Uiκj )ni( j,k) Uikκ ,\\r\\n                      j=0                   i=0             e∈∂κ k=0                                i, j,k=0\\r\\n\\r\\n\\r\\nand we get (38).\\r\\n   Entropy stability: Let us now consider EC fluxes in the volume integral in the semi-discrete DGSEM scheme (31). Left\\r\\nmultiply (31) by ϑiκj = ϑ(uiκj ) and sum up over 0 6 i, j 6 p to get\\r\\n\\r\\n                                                   p            p                 p\\r\\n                                    d             X            X            XX\\r\\n                                                                                     ωk Jek ϑ u−h (xke , t) · D− u−h (xke , t), u+h (xke , t), nke = 0,\\r\\n                                                                                             \\x10             \\x11    \\x10                                 \\x11\\r\\n                              |κ|      hηiκ (t) +     ω jC j +     ωi E i +\\r\\n                                    dt            j=0          i=0          e∈∂κ k=0\\r\\n\\r\\nwhere\\r\\n\\r\\n\\r\\n        p\\r\\n        X                                                                                                       p\\r\\n                                                                                                                X\\r\\n                ωi Dik ϑiκj · D−ec (Uiκj , Ukκ j , n(i,k) j ) − D+ec (Ukκ j , Uiκj , n(i,k) j ) ,                       ω j D jk ϑiκj · D−ec (Uiκj , Uikκ , ni( j,k) ) − D+ec (Uikκ , Uiκj , ni( j,k) ) .\\r\\n                             \\x10                                                                 \\x11                                       \\x10                                                               \\x11\\r\\n Cj =                                                                                                 Ei =\\r\\n        i,k=0                                                                                                   j,k=0\\r\\n\\r\\n\\r\\n    Using (35), we have\\r\\n\\r\\n\\r\\n                          p\\r\\n                          X             \\x12                                                                                                                   \\x13\\r\\n                                                                                                                           \\x10                    \\x11\\r\\n                Cj =              ωi Dik ϑiκj · D−ec (Uiκj , Ukκ j , n(i,k) j ) + ϑkκ j · D−ec (Ukκ j , Uiκj , n(i,k) j ) − q(Uiκj ) − q(Ukκ j ) · n(i,k) j\\r\\n                          i,k=0\\r\\n                      (23)\\r\\n                      = Je (xκp j )q(Uκp j ) · ne (xκp j ) + Je (x0κ j )q(U0κ j ) · ne (x0κ j )\\r\\n                      (34)\\r\\n                          p\\r\\n                          X                                                                                                       \\x10                                  \\x11\\r\\n                      +           ωi Dik ϑiκj · D−ec (Uiκj , Ukκ j , n(i,k) j ) − ωk Dki ϑkκ j · D−ec (Ukκ j , Uiκj , n(i,k) j ) − ωi Dik q(Uiκj ) + ωk Dki q(Ukκ j ) · n(i,k) j\\r\\n                          i,k=0\\r\\n                                                                                                       p\\r\\n                                                                                                       X\\r\\n                      i↔k\\r\\n                      = Je (xκp j )q(Uκp j ) · ne (xκp j ) + Je (x0κ j )q(U0κ j ) · ne (x0κ j ) −              ωi Dik Jκk j q(Uiκj )∇ξ(xkκ j ).\\r\\n                      (22)\\r\\n                                                                                                       i,k=0\\r\\n\\r\\n\\r\\n    Likewise\\r\\n\\r\\n                                                                                                                           p\\r\\n                                                                                                                           X\\r\\n                                                   κ )q(Uκ ) · ne (xκ ) + Je (xκ )q(Uκ ) · ne (xκ ) −\\r\\n                                         Ei = Je (xip    ip         ip         i0    i0         i0\\r\\n                                                                                                                                   ω j D jk Jκik q(Uiκj )∇η(xikκ ),\\r\\n                                                                                                                           j,k=0\\r\\n\\r\\nand again using the metric identities (24), we get\\r\\n\\r\\n                                                                p\\r\\n                                            d             XX             \\x12                                                                      \\x13\\r\\n                                                                   ωk Jek ϑ(xke ) · D− u−h (xke , t), u+h (xke , t), nke + q u−h (xke , t) · nke = 0.\\r\\n                                                                                      \\x10                                 \\x11   \\x10             \\x11\\r\\n                                      |κ|      hηiκ (t) +\\r\\n                                            dt            e∈∂κ k=0\\r\\n\\r\\n                                                                                                 10\\r\\n\\x0c\\n\\n    Then, using (40) we have\\r\\n\\r\\n                                                                      1                1                       1\\r\\n                     ϑ− · D− (u− , u+ , n) + q− · n = Q(u− , u+ , n) + (q− − q+ ) · n + ϑ− · D− (u− , u+ , n) + ϑ+ · D+ (u− , u+ , n)\\r\\n                                                                      2                2                       2\\r\\nand since the sum of the three last terms are non-negative by (36), we obtain the desired entropy inequality (39).\\r\\n\\r\\nRemark 3.1. Fluctuations with the following expressions fall into the category (37a):\\r\\n\\r\\n                                                                                1\\x10\\r\\n                                                       C± (u− , u+ , n) =\\r\\n                                                                                                         \\x11\\r\\n                                                                                  αc(u± ) + (1 − α)c(u∓ ) n,            0 6 α 6 1,\\r\\n                                                                                2\\r\\nwhich belong to the Volpert path family of schemes [61] and correspond to the skew-symmetric splitting αc∇u + (1 − α)(∇ · (cu) −\\r\\n(∇ · c)u) of the nonconservative product. Relations (37b) and (37d) indeed correspond to the consistency condition of the Volpert\\r\\npath family of schemes [61], while (37c) is only necessary to get (38) which will be useful to prove Theorem 6.1.\\r\\n\\r\\n   In the next two sections, we describe the fluctuations we use in the DGSEM scheme (31) for the discretization of the SG-\\r\\ngamma model (1) and (2). In section 4, we first focus on designing CP and EC fluctuations that are applied in the volume integral.\\r\\nWe then design in section 5 a HLLC approximate Riemann solver for (1a) that is applied at interfaces.\\r\\n\\r\\n\\r\\n4. Numerical fluctuations for the volume integrals\\r\\n\\r\\n    In this section we focus on numerical fluctuations (30) for the scheme (31) that will be applied in the volume integrals. We\\r\\nwill make use of the Leibniz identities, which we recall here: let a+ , a− , b+ , b− , c+ , c− in R and have finite values, then we have\\r\\n                                                                                                       \\x10           \\x11\\r\\n                                                       ~ab\\x7f = a~b\\x7f + b~a\\x7f,                    ~abc\\x7f = a b~c\\x7f + c~b\\x7f + bc~a\\x7f,                                                         (42)\\r\\n\\r\\n               a+ + a−\\r\\nwhere a =              is the arithmetic mean and ~a\\x7f = a+ − a− the jump.\\r\\n                  2\\r\\n\\r\\n4.1. Contact preserving numerical fluxes\\r\\n     Here, we focus on deriving conditions that will ensure that the numerical fluxes maintain uniform pressure and velocity\\r\\nprofiles across an isolated material interface. To this purpose, we introduce CP fluctuations in (30), with index cp, where\\r\\nhcp (u− , u+ , n) = (hρcp , hρv>   ρE\\r\\n                             cp , hcp , 0, 0) is the vector of numerical fluxes for the conservative equations of mass, momentum and\\r\\n                                             >\\r\\n                         +\\r\\nenergy, and dcp (u , u , n) = (0, 0, 0, dΓ± , dΠ± )> is the vector for the fluctuations for the nonconservative products in (1a).\\r\\n                 ±  −\\r\\n\\r\\n\\r\\n     For the sake of brevity, we here focus on volume fluctuations only, though the same relations may be derived for the interface\\r\\nfluctuations by following the same lines (see section 5.4.2). Instead of using the symmetrizer in (32), we assume that the numerical\\r\\nfluxes are symmetric: hX (u− , u+ , n) = hcp (u+ , u− , n) without loss of generality (they will be, see proposition 4.1). Let us represent\\r\\nthe conserved and nonconserved quantities in (1a) using A ∈ {ρ, ρv, ρE} and B ∈ {Γ, Π}, respectively. The DGSEM scheme for (1a)\\r\\nnow reads\\r\\n\\r\\n\\r\\n                                    p                                                                                    p\\r\\n                   d ij            X                                                                            \\x11 XX\\r\\n                                                                                                                            φiκj (xke )ωk Jek D−A Uiκj , u+h (xke , t), nke\\r\\n                                       \\x10                                                                                                         \\x10                          \\x11\\r\\n    ωi ω j Jκi j      Aκ + 2ωi ω j            A\\r\\n                                         Dik hcp (Uiκj , Ukκ j , n(i,k) j ) + D jk hcp\\r\\n                                                                                    A\\r\\n                                                                                       (Uiκj , Uikκ , ni(k, j) ) +                                                                  (43a)\\r\\n                   dt              k=0                                                                             e∈∂κ k=0\\r\\n                                   p \\x12\\r\\n                   d ij           X       \\x10                                                                 \\x11      \\x10                                                                \\x11\\x13\\r\\n    ωi ω j Jκi j      Bκ + ωi ω j           B,−\\r\\n                                       Dik dcp  (Uiκj , Ukκ j , n(i,k) j ) − dcp\\r\\n                                                                              B,+\\r\\n                                                                                  (Ukκ j , Uiκj , n(i,k) j ) + D jk dcp\\r\\n                                                                                                                     B,−\\r\\n                                                                                                                         (Uiκj , Uikκ , ni(k, j) ) − dcp\\r\\n                                                                                                                                                      B,+\\r\\n                                                                                                                                                          (Uikκ , Uiκj , ni(k, j) )\\r\\n                   dt             k=0\\r\\n                                p\\r\\n                               XX\\r\\n                                          φiκj (xke )ωk Jek D−B Uiκj , u+h (xke , t), nke\\r\\n                                                               \\x10                          \\x11\\r\\n                           +                                                                                                                                                       (43b)\\r\\n                               e∈∂κ k=0\\r\\n\\r\\n\\r\\n    Now let us suppose that the initial condition consists of a material interface with uniform velocity, v = (u, v)> , and pressure, p,\\r\\nand states ρL , ΓL and πL in ΩL and ρR , ΓR and πR in ΩR with ΩL ∪ ΩR = Ω, then so do the DOFs. We now derive conditions for the\\r\\nnumerical fluxes (30) to preserve the uniform states in time.\\r\\n\\r\\n                                                                                                 11\\r\\n\\x0c\\n\\n     We, first, focus on the velocity state and impose the semi-discrete scheme (43) to satisfy a discrete counterpart to the differential\\r\\nrelation ρdv = dρv − vdρ = 0. Ignoring the interface fluxes, ωi ω j Jκi j ρiκj dt viκj = 0 requires\\r\\n\\r\\n                      p\\r\\n                      X\\r\\n                         (hρv                           ρ                                ρv                           ρ\\r\\n                        \\x10                                                                                                                         \\x11\\r\\n             ωi ω j        cp (Uκ , Uκ , n(i,k) j ) − vhcp (Uκ , Uκ , n(i,k) j ))Dik + (hcp (Uκ , Uκ , ni(k, j) ) − vhcp (Uκ , Uκ , ni(k, j ))D jk = 0,\\r\\n                                ij   kj                      ij   kj                          ij   ik                      ij   ik\\r\\n\\r\\n                      k=0\\r\\n\\r\\nand a sufficient condition reads\\r\\n\\r\\n                                               hρv       +               + ρ         +               +\\r\\n                                                cp (u , u , n) = ṽ(u , u )hcp (u , u , n) + p̃(u , u , n)\\r\\n                                                     −               −           −               −\\r\\n                                                                                                                                             ∀u± ∈ ΩGM ,           (44)\\r\\n\\r\\nwhere ṽ and p̃ are any consistent discretizations of the velocity vector and pressure. Similarly, a semi-discrete equation for the\\r\\npressure (5) can be obtained by using Γdp = dρE − ( 21 v · v)dρ − pdΓ − dΠ from (6), and again ignoring surface contributions in\\r\\n(43), ωi ω j Jκi j Γiκj dt p(Uiκj ) = 0 requires\\r\\n\\r\\n                            p\\r\\n                            X\\r\\n                                  Dik 2hρE                               ρ                                                        +\\r\\n                                                                                                     \\x10                                                    \\x11\\r\\n                  ωi ω j                cp (Uκ , Uκ , n(i,k) j ) − v · vhcp (Uκ , Uκ , n(i,k) j ) − p dΓ (Uκ , Uκ , n(i,k) j ) − dΓ (Uκ , Uκ , n(i,k) j )\\r\\n                                             ij   kj                          ij   kj                  −   ij   kj                    kj   ij\\r\\n\\r\\n                            k=0\\r\\n                                                                                                      !\\r\\n                                  − dΠ− (Uiκj , Ukκ j , n(i,k) j ) + dΠ+ (Ukκ j , Uiκj , n(i,k) j )\\r\\n\\r\\n                             +D jk 2hρE                                      ρ                                                 +\\r\\n                                                                                                   \\x10                                                  \\x11\\r\\n                                     cp (Uκ , Uκ , ni(k, j) ) − v · vhcp (Uκ , Uκ , ni( j,k) ) − p dΓ (Uκ , Uκ , ni(k, j) ) − dΓ (Uκ , Uκ , ni(k, j) )\\r\\n                                             ij     ik                               ij      ik      −  ij   ik                    ik   ij\\r\\n\\r\\n                                                                                             !\\r\\n                              − dΠ− (Uiκj , Uikκ , ni(k, j) ) + dΠ+ (Uikκ , Uiκj , ni(k, j) ) = 0,\\r\\n\\r\\n                                                                P p \\x10 ρE i j   v·v ρ\\r\\n                                                                                        \\x11\\r\\nand subtracting the trivial quantity 2ωi ω j                     k=0 f (Uκ ) − 2 f (Uκ ) · (Dik n(i,k) j + D jk ni( j,k) ) = 0, from (41), a sufficient condition\\r\\n                                                                                     ij\\r\\n\\r\\nreads\\r\\n\\r\\n\\r\\n                                                       v · v\\x10 ρ − +\\r\\n   hρE       +\\r\\n                                                                                        \\x11\\r\\n    cp (u , u , n) − (ρE + p)v · n =                         hcp (u , u , n) − ρ− v · n\\r\\n         −              −\\r\\n                                                        2\\r\\n                                                                            p\\x10 − − +                             \\x11 1\\x10\\r\\n                                                                               dΓ (u , u , n) − dΓ+ (u+ , u− , n) + dΠ− (u− , u+ , n) − dΠ+ (u+ , u− , n) .\\r\\n                                                                                                                                                         \\x11\\r\\n                                                                         +                                                                                         (45)\\r\\n                                                                            2                                      2\\r\\n\\r\\n     We can now propose CP fluxes for the volume integral.\\r\\n\\r\\nProposition 4.1. Numerical fluxes of the form (30) where\\r\\n\\r\\n                                                     \\uf8ec\\uf8ec\\uf8ec ρ v · n \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                     \\uf8eb                        \\uf8f6                                                           \\uf8eb                \\uf8f6\\r\\n                                                                                                                                          \\uf8ec\\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                         \\uf8ec\\uf8ec\\uf8ecρ v v · n + pn\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\r\\n                                                       \\uf8ec\\uf8ec\\uf8ec                    \\uf8f7                                                             \\uf8ec\\uf8ec\\uf8ec            \\uf8f7\\r\\n                                                                                                                                              \\uf8ec\\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\r\\n                                                                                                                               1\\r\\n                              hcp (u− , u+ , n) = \\uf8ec\\uf8ec\\uf8ec\\uf8ec(ρE + p)v · n\\uf8f7\\uf8f7\\uf8f7\\uf8f7 ,                                  d±cp (u− , u+ , n) = v± · n \\uf8ec\\uf8ec\\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7\\uf8f7 ,               (46)\\r\\n                                                           \\uf8ec                  \\uf8f7                                                                 \\uf8ec          \\uf8f7\\r\\n                                                           \\uf8ec\\uf8ec\\uf8ec                \\uf8f7\\uf8f7\\uf8f7                                              2                \\uf8ec\\uf8ec\\uf8ec        \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                             \\uf8ec\\uf8ec\\uf8ec\\uf8ed  0            \\uf8f7\\uf8f7\\uf8f7\\uf8f8                                                              \\uf8ec\\uf8ec\\uf8ec\\uf8ed ~Γ\\x7f   \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                                   0                                                                                   ~Π\\x7f\\uf8f8\\r\\n\\r\\npreserve the uniform pressure and velocity fields across contact discontinuities and material interfaces for the SG-gamma model\\r\\n(1a) and (2), with the mixture EOS (5).\\r\\n\\r\\nProof. Checking that condition (44) holds for (46) is direct, wile using the mixture EOS (5), we get\\r\\n\\r\\n\\r\\n     p\\x10 − − +                            \\x11 1\\x10                                    \\x11 v · n\\x10          \\x11 (42) v · n \\x12        v·v    \\x13\\r\\n       dΓ (u , u , n) − dΓ+ (u+ , u− , n) + dΠ− (u− , u+ , n) − dΠ+ (u+ , u− , n) =      p~Γ\\x7f + ~Π\\x7f =             ~ρE\\x7f −     ~ρ\\x7f ,                                 (47)\\r\\n     2                                     2                                         2                      2             2\\r\\n\\r\\nso (45) holds as well.\\r\\n\\r\\nRemark 4.1. The CP numerical fluxes (46) are similar to the one proposed in [37]. Here we have modified the contributions\\r\\ntowards the energy equation to satisfy (45).\\r\\n\\r\\n                                                                                                      12\\r\\n\\x0c\\n\\n4.2. Entropy conservative numerical fluxes\\r\\n    We, now, propose EC fluxes for the SG-gamma model that are applied to the modified volume integral in (31) and, according\\r\\nto Theorem 3.1, these numerical fluxes will contribute to the entropy stability of the numerical scheme.\\r\\n\\r\\nProposition 4.2. Consider the entropy pair (11) with (12), then fluctuations of the form (30) with\\r\\n\\r\\n                                                         \\uf8ec\\uf8ec\\uf8ecρ̂v · n\\uf8f7\\uf8f7\\uf8f7\\r\\n                                                         \\uf8eb                 \\uf8f6                                    \\uf8eb                \\uf8f6\\r\\n                                                                                                                \\uf8ec\\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                           \\uf8ec\\uf8ec\\uf8ec\\uf8ec hρv \\uf8f7\\uf8f7\\uf8f7\\uf8f7                                          \\uf8ec\\uf8ec\\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\r\\n                                                              \\uf8ec\\uf8ec\\uf8ec ec \\uf8f7\\uf8f7\\uf8f7\\r\\n                                  hec (u− , u+ , n) = \\uf8ec\\uf8ec\\uf8ec\\uf8ec hρE             \\uf8f7\\uf8f7 , d± (u− , u+ , n) = 1 v± · n \\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7 ,\\r\\n                                                                                                                     \\uf8ec           \\uf8f7\\r\\n                                                                                                                                                                        (48)\\r\\n                                                                \\uf8ec\\uf8ec\\uf8ec ec \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7     ec\\r\\n                                                                                                   2                 \\uf8ec\\uf8ec\\uf8ec ~Γ\\x7f \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\r\\n                                                                                                                     \\uf8ec\\r\\n                                                                                                                     \\uf8ec\\r\\n                                                                  \\uf8ec\\uf8ec\\uf8ec 0 \\uf8f7\\uf8f7\\uf8f7                                            \\uf8ec\\uf8ec\\uf8ed       \\uf8f7\\uf8f7\\r\\n                                                                    \\uf8ed 0 \\uf8f8                                                  ~Π\\x7f\\uf8f8\\r\\n\\r\\nwhere\\r\\n                                             \\uf8eb                          \\uf8f6                              \\uf8ebV                     \\uf8f6             \\uf8eb           !\\uf8f6\\r\\n                                                                                                                  v− · v+ \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\r\\n                                                              !                                        \\uf8ec\\uf8ec C !\\r\\n                                             \\uf8ec\\uf8ec            Cv           \\uf8f7\\uf8f7\\uf8f7                                                                 \\uf8ec\\uf8ec\\uf8ec       Cv \\uf8f7\\uf8f7\\uf8f7\\uf8f7\\r\\n        hρv       +\\r\\n                                                                                hρE       +                v\\r\\n         ec (u , u , n) = ρ̂(v · n)v + \\uf8ec\\uf8ec\\uf8edρ(γ − 1)                               ec (u , u , n) = \\uf8ec             +             \\uf8f7 ρ̂v · n + \\uf8ec\\uf8ec\\uf8edρ(γ − 1)\\r\\n              −                                                                       −           \\uf8ec\\uf8ec\\uf8ec\\r\\n                                                                − p∞ \\uf8f7\\uf8f7\\uf8f7\\uf8f8 n,                                                                                \\uf8f7\\uf8f7 v · n,\\r\\n                                       \\uf8ec\\uf8ec                                                                                                     \\uf8ec\\r\\n                                                           ζ                                        \\uf8ec\\uf8ed     ζ         2 \\uf8f7\\uf8f8                             ζ \\uf8f8\\r\\n\\r\\n                   Π\\r\\nand Cζv = Γρ (p + Γ+1 ) from (15), are EC in the sense (35) when excluding material interfaces, i.e., ~Γ\\x7f ≡ ~Π\\x7f ≡ 0.\\r\\n\\r\\nProof. As we consider pure phases, the system (1a) is conservative and (35) reduces to the Tadmor condition [55]\\r\\n\\r\\n                                                     hec (u− , u+ , n) · ~ϑ\\x7f − ~ψ(u)\\x7f · n = 0 ∀u± ∈ ΩGM ,\\r\\n\\r\\nwhere the entropy variables ϑ are defined in (14), and ψ ≡ f> ϑ − q is the entropy potential and reads\\r\\n               (14)\\r\\n                                                    \\x12            v · v\\x13\\r\\n          ψ(u) = −ζ(ρE + p)v + ζv(ρv · v + p) + γCv − s − ζ             ρv + ρsv = (γCv − ζe)ρv = (γ − 1)Cv ρ − p∞ ζ v,\\r\\n                                                                                                                    \\x01\\r\\n                                                                  2\\r\\nso\\r\\n                                                                    (42) \\x10                    \\x11\\r\\n                                                         ~ψ(u)\\x7f · n = (γ − 1)Cv ~ρv\\x7f − p∞ ~ζv\\x7f · n.                                                                     (49)\\r\\n\\r\\n     Using the definition of ζ in (15), the entropy of the mixture (12) may be reformulated as\\r\\n\\r\\n                                             p + p∞\\r\\n                                                    !                                    \\x10         \\x11\\r\\n                                  s = Cv ln           = −Cv ln ζ − (γ − 1)Cv ln ρ + Cv ln (γ − 1)Cv ,                                                                   (50)\\r\\n                                                ρ γ\\r\\n\\r\\n\\r\\n\\r\\nthen we have\\r\\n                                                                                                                            \\uf8eb\\uf8eb                                          \\uf8f6\\r\\n                                                                                                             \\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec Cv v− · v+ \\uf8f7\\uf8f7\\r\\n                                                                   \\uf8eb                                   \\uf8f6                                        \\uf8f6\\r\\n                                        v·v                          (γ − 1)Cv ρ                                               \\uf8f7\\uf8f7\\uf8f7 ρ̂ + (γ − 1)Cv ρ\\uf8f7\\uf8f7\\uf8f7\\uf8f7 v · n\\r\\n                                                                                                                                                      \\uf8f7\\r\\nh>ec (u− , u+ , n)~ϑ(u)\\x7f = ~γCv − s − ζ     \\x7fρ̂v · n + \\uf8ec\\uf8ec\\uf8ec\\uf8edv · vρ̂ +                                                 +\\r\\n                                                          \\uf8ec\\uf8ec                             \\uf8f7\\uf8f7\\r\\n                                                                                 − p∞ \\uf8f7\\uf8f7\\uf8f7\\uf8f8 ~ζv\\x7f · n − ~ζ\\x7f \\uf8ec\\uf8ec\\uf8ec\\uf8ed\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\r\\n                                                   2                                     ζ                                       ζ̂       2                  ζ\\r\\n                                                                                                                                 \\uf8f8                    \\uf8f7\\uf8f8\\r\\n                                 \\uf8eb                           \\uf8f6                       \\uf8eb                              \\uf8f6\\r\\n                           (50) \\uf8ec  ~ζ\\x7f               ~ρ\\x7f \\uf8f7\\uf8f7\\uf8f7                                    (γ − 1)Cv ρ\\r\\n                            = \\uf8ec\\uf8ec\\uf8ec\\uf8ed         + (γ − 1)         \\uf8f7\\uf8f8 Cv ρ̂v · n + ~ζv\\x7f \\uf8ec\\uf8ec\\uf8ec\\uf8edv · vρ̂ +\\r\\n                                 \\uf8ec                           \\uf8f7                       \\uf8ec\\r\\n                                                                                     \\uf8ec                              \\uf8f7\\uf8f7\\r\\n                                                                                                            − p∞ \\uf8f7\\uf8f7\\uf8f7\\uf8f8 · n\\r\\n                           (42)     ζ̂                ρ̂                                             ζ\\r\\n                                    \\uf8eb\\uf8eb                                          \\uf8f6\\r\\n                                    \\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec Cv v− · v+ \\uf8f7\\uf8f7\\r\\n                                                         \\uf8f6\\r\\n                                                         \\uf8f7\\uf8f7\\uf8f7 ρ̂ + (γ − 1)Cv ρ\\uf8f7\\uf8f7\\uf8f7\\uf8f7 v · n\\r\\n                                                                                \\uf8f7\\r\\n                           − ~ζ\\x7f \\uf8ec\\uf8ec\\uf8ed\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\uf8ec     +\\r\\n                                          ζ̂       2 \\uf8f8                   ζ      \\uf8f8\\uf8f7\\r\\n                              \\x10                                      \\x11\\r\\n                           = (γ − 1)Cv ~ρv\\x7f − p∞ ~ζv\\x7f · n,\\r\\n\\r\\nwhich cancels out with (49), so the proof is complete.\\r\\n\\r\\n\\r\\n5. An HLLC Riemann solver for the cell interfaces\\r\\n\\r\\n    We now look for two-point numerical fluctuations at interfaces and design an HLLC approximate Riemann solver for the SG-\\r\\ngamma model (1a) and analyze its properties. For the sake of generality, we assume that the entropy η(u) in (9) is convex, which\\r\\nexcludes material interfaces.\\r\\n\\r\\n                                                                                     13\\r\\n\\x0c\\n\\n5.1. One-dimensional Riemann problem\\r\\n    We are interested in approximating solutions to the following Riemann problem in a given unit direction n in Rd :\\r\\n\\r\\n                                                                          ∂t u + ∂ x f(u) · n + cn (u)∂ x u = 0,                                     (51a)\\r\\n\\r\\nwhere x = x · n and cn (u) = c(u)n =\\r\\n                                                           Pd\\r\\n                                                             i=1 ni ci (u), together with initial data\\r\\n\\r\\n                                                                                       \\uf8f2 uL ,     x < 0,\\r\\n                                                                                       \\uf8f1\\r\\n                                                                              u0 (x) = \\uf8f4\\r\\n                                                                                       \\uf8f4\\r\\n                                                                                       \\uf8f3 uR ,                                                        (51b)\\r\\n                                                                                                  x > 0.\\r\\n\\r\\n    By W( xt , uL , uR , n) we denote the exact entropy weak solution to (51) for t > 0. Following [28], we integrate (51a) over the\\r\\ncontrol volume [− 2h , 2h ] × [0, ∆t] with h > 0 and ∆t > 0 the space and time steps, respectively. Using (51b), we obtain\\r\\n                         Z h                \\x12 x                                                          Z ∆t Z h\\r\\n                                 2\\r\\n                                                             \\x13    h                                             2\\r\\n                                                ; uL , uR , n dx − (uL + uR ) + ∆t f(uR ) − f(uL ) · n +          cn (u)∂ x udxdt = 0.\\r\\n                                                                                                  \\x01\\r\\n                                      W                                                                                                               (52)\\r\\n                               − h2          ∆t                   2                                       0     h\\r\\n                                                                                                               −2\\r\\n\\r\\n\\r\\n     Note that both Γ and Π are continuous across shocks and discontinuous across the intermediate contact wave. Let us introduce\\r\\nthe two last components eΓ = (0, 0, 0, 1, 0)> and eΠ = (0, 0, 0, 0, 1)> of the canonical basis in Rneq , and define u := v · n. By (52)\\r\\nand (2) we have\\r\\n         Z ∆t Z h                                                                                  Z h\\r\\n                    2                                   h                                              2\\r\\n                                                                                                                \\x12 x                \\x13\\r\\n                        cn (u)∂ x udxdt =                 (uL + uR ) − ∆t f(uR ) − f(uL ) · n −                       ; uL , uR , n dx\\r\\n                                                                                         \\x01\\r\\n                                                                                                            W\\r\\n          0     − h2                                    2                                            − h2        ∆t\\r\\n                                                                                                            !                   !\\r\\n                                                      h               h                  h                                    h\\r\\n                                                    =   (ΓL + ΓR )eΓ + (ΠL + ΠR )eΠ −      − u? ∆t (ΓR eΓ + ΠR eΠ ) − u? ∆t +     (ΓL eΓ + ΠL eΠ )\\r\\n                                                      2               2                  2                                    2\\r\\n                                                    = ∆tu? (ΓR − ΓL )eΓ + (ΠR − ΠL )eΠ ,\\r\\n                                                           \\x10                          \\x11\\r\\n\\r\\n\\r\\nwhere uL , u? , and uR are the normal velocity components in the left state, the star region, and the right states, respectively. The\\r\\nintegral form for (1a) thus reads\\r\\n                    Z h\\r\\n               1          2\\r\\n                                       \\x12 x                 \\x13      h\\r\\n                                                                     (uL + uR ) + f(uR ) − f(uL ) · n + u? (ΓR − ΓL )eΓ + (ΠR − ΠL )eΠ = 0.\\r\\n                                                                                 \\x10               \\x11        \\x10                           \\x11\\r\\n                                W             ; uL , uR , n dx −                                                                                      (53)\\r\\n               ∆t       − h2               ∆t                    2∆t\\r\\n\\r\\n    Likewise integrating (9) in the direction n over the control volume [− h2 , 2h ] × [0, ∆t] gives\\r\\n                                           Z h                              \\x13!\\r\\n                                      1        2\\r\\n                                                           \\x12 x                       h \\x10               \\x11\\r\\n                                                    η W        ; uL , uR , n dxdt −     η(uL ) + η(uR ) + q(uR ) · n − q(uL ) · n 6 0.                (54)\\r\\n                                      ∆t     − h2           ∆t                      2∆t\\r\\n\\r\\n\\r\\n5.2. Three-point schemes and the Godunov method\\r\\n     It will be convenient for the analysis of the HLLC solver to consider one-dimensional three-point numerical schemes in\\r\\nfluctuation form [43]\\r\\n                                                  ∆t \\x10 − n n\\r\\n                                                      D (U j , U j+1 , n) + D+ (Unj−1 , Unj , n) = 0,\\r\\n                                                                                                \\x11\\r\\n                                     Un+1\\r\\n                                       j  − Unj +                                                                      (55)\\r\\n                                                   h\\r\\nwhere D± (·, ·, ·) are assumed to be consistent (27). Here, Unj approximates the cell-averaged solution in the jth cell of size h at time\\r\\nt(n) = n∆t. In the Godunov method, the fluctuations are defined by solving exact Riemann problems (51) centered at every interface\\r\\n j + 12 of coordinate x j+ 1 , between cells j and j + 1 (see Figure 2), with Unj and Unj+1 as the left and right initial data, respectively.\\r\\n                             2\\r\\nThe solution at time tn+1 in the jth cell is defined as the cell-average of the exact solution at t(n+1) = t(n) + ∆t:\\r\\n                         \\uf8eb                                                                                 \\uf8f6\\r\\n                         \\uf8ec\\uf8ec\\uf8ecZ x j                                  Z x 1                            \\x13 \\uf8f7\\uf8f7\\uf8f7\\r\\n                      1                     \\x12 x              \\x13          j+ 2\\r\\n                                                                               \\x12 x\\r\\n             Un+1  = \\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec                      ; Un , Un , n dx +                 ; Unj , Unj+1 , n dx\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7 ,\\r\\n                           \\uf8ec                                                                               \\uf8f7\\r\\n                                        W                                    W                                                                       (56)\\r\\n               j\\r\\n                      h\\uf8ed x 1                 ∆t j−1 j                xj         ∆t                         \\uf8f8\\r\\n                               j− 2\\r\\n                                    \\uf8eb                                                                                                            \\uf8f6\\r\\n                               ∆t\\r\\n                                    \\uf8ec\\uf8ec\\uf8ec               Z xj                                                     Z x 1                      \\x13 \\uf8f7\\uf8f7\\uf8f7\\r\\n                                          h         1           \\x12 x              \\x13          h          1          j+ 2\\r\\n                                                                                                                         \\x12 x\\r\\n                   = Unj −                   Un −                   ; Un , Un , n dx +          Un −                         ; Un , Un , n dx\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7 ,\\r\\n                                      \\uf8ec                                                                                                          \\uf8f7\\r\\n                                      \\uf8ec\\uf8ec\\uf8ec                    W                                                         W\\r\\n                                h \\uf8ec\\uf8ec\\uf8ed 2∆t j ∆t x 1               ∆t j−1 j                  2∆t j ∆t x j                   ∆t j j+1               \\uf8f8\\r\\n                                                                   j− 2\\r\\n\\r\\n\\r\\n                                                                                           14\\r\\n\\x0c\\n\\nwith x j = x j− 1 + h2 = x j+ 1 − h2 (see Figure 2), and takes the form (55) with the fluctuations defined as\\r\\n               2               2\\r\\n\\r\\n                                                         Z 0\\r\\n                                            h − 1                \\x12 x              \\x13\\r\\n                       D− (u− , u+ , n) =      u −           W       ; u− , u+ , n dx                                                                (57a)\\r\\n                                          2∆t        ∆t − h2      ∆t\\r\\n                                        = f W(0; u− , u+ , n) · n − f(u− ) · n + min(u? , 0) (ΓR − ΓL )eΓ + (ΠR − ΠL )eΠ ,\\r\\n                                           \\x10                 \\x11                              \\x10                           \\x11\\r\\n\\r\\n                                                         Z h\\r\\n                                            h + 1          2\\r\\n                                                                 \\x12 x              \\x13\\r\\n                     D+ (u− , u+ , n) : =      u −           W       ; u− , u+ , n dx                                                                (57b)\\r\\n                                          2∆t        ∆t 0         ∆t\\r\\n                                             +\\r\\n                                        = f(u ) · n − f W(0; u , u+ , n) · n + max(u? , 0) (ΓR − ΓL )eΓ + (ΠR − ΠL )eΠ .\\r\\n                                                       \\x10                 \\x11                  \\x10                            \\x11\\r\\n                                                                −\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                            h\\r\\n                                                                  x j− 12   xj    x j+ 21\\r\\n                                                                                                             x\\r\\n                                                           j−1               j              j+1\\r\\n\\r\\n                                           Figure 2: Notations for the mesh used for the three-point scheme (55).\\r\\n\\r\\n\\r\\n\\r\\n    Note that D± (·, ·, ·) in (57) satisfy the following path-conservation property [43]\\r\\n\\r\\n                            D− (u− , u+ , n) + D+ (u− , u+ , n) = f(u+ ) · n − f(u− ) · n + u? (ΓR − ΓL )eΓ + u? (ΓR − ΓL )eΠ ,                       (58)\\r\\n\\r\\nfor a path φ : [0, 1] × ΩGM × ΩGM → ΩGM such that\\r\\n                          Z 1 \\x10                                                                    Z 1 \\x10\\r\\n          u? (ΓR − ΓL ) =                                                        u? (ΠR − ΠL ) =\\r\\n                                              \\x11                                                                       \\x11\\r\\n                              u φ(s; uL , uR ) ∂ s φ(s; uL , uR ) · eΓ ds,                            u φ(s; uL , uR ) ∂ s φ(s; uL , uR ) · eΠ ds.\\r\\n                               0                                                                     0\\r\\n\\r\\n\\r\\n    The interface Riemann problems are assumed to be noninteracting through the definition of a half CFL condition:\\r\\n                                                            ∆t                               1\\r\\n                                                               max |λ|max (Unj , Unj+1 , n) 6 ,                                                       (59)\\r\\n                                                            h j∈Z                            2\\r\\n\\r\\nwhere |λ|max (uL , uR , n) is an upper bound of the absolute value of the signal speeds in the Riemann problem (51).\\r\\n    Finally, invoking a Jensen’s inequality in (56) for any convex entropy function in (11), we obtain the following discrete entropy\\r\\ninequality [28] consistent with (9):\\r\\n                                                                    ∆t \\x10                                        \\x11\\r\\n                                                 j ) 6 η(U j ) −\\r\\n                                              η(Un+1                     Q(Unj , Unj+1 , n) − Q(Unj−1 , Unj , n) ,                                    (60)\\r\\n                                                                    h\\r\\n\\r\\nwith the consistent entropy flux Q(u− , u+ , n) = q W(0; u− , u+ , n) · n.\\r\\n                                                                     \\x01\\r\\n\\r\\n\\r\\n5.3. HLLC Riemann solver\\r\\n    The HLLC solver [60, 2] is a simple solver [8] with four uniform states separated by simple discontinuities, see Figure 3:\\r\\n                                                                 \\uf8f1\\r\\n                                                                 \\uf8f4\\r\\n                                                                 \\uf8f4\\r\\n                                                                 \\uf8f4 uL , xt < sL ,\\r\\n                                                                 \\uf8f2 u?L , sL < xt < s? ,\\r\\n                                                \\x12x           \\x13 \\uf8f4 \\uf8f4\\r\\n                                                                 \\uf8f4\\r\\n                                         W HLLC ; uL , uR , n = \\uf8f4\\r\\n                                                                 \\uf8f4\\r\\n                                                                                                                               (61)\\r\\n                                                 t               \\uf8f4\\r\\n                                                                 \\uf8f4\\r\\n                                                                 \\uf8f4 u?R , s? < xt < sR ,\\r\\n                                                                 \\uf8f3 u R , sR < t ,\\r\\n                                                                 \\uf8f4\\r\\n                                                                 \\uf8f4\\r\\n                                                                 \\uf8f4             x\\r\\n\\r\\n\\r\\n\\r\\nwhere the wave s? approximates the speed of the intermediate contact wave. In contrast to [28], we here require an approximate\\r\\nconsistency of the HLLC solver (61) with the integral form (53). Integrating (61) over [− h2 , 2h ] at time ∆t gives\\r\\n                   Z h               \\x12 x                             !                                                   !\\r\\n                       2\\r\\n                                                      \\x13     h                                                          h\\r\\n                            W HLLC       ; uL , uR , n dx =   − sR ∆t uR + ∆t(sR − s? )u?R + ∆t(s? − sL )u?L + sL ∆t +     uL ,                       (62)\\r\\n                     − h2             ∆t                    2                                                          2\\r\\n                                                                             15\\r\\n\\x0c\\n\\n                                                                        t\\r\\n\\r\\n                                  sL                               ∆t                     s∗                  sR\\r\\n\\r\\n\\r\\n                                                            u∗L                            u∗R\\r\\n\\r\\n                                        uL                                                               uR\\r\\n\\r\\n\\r\\n\\r\\n                                                 ΓL , ΠL                                       ΓR , ΠR\\r\\n\\r\\n                                                                                                                                x\\r\\n                  − h2        sL ∆t                                     0                                      sR ∆t        h\\r\\n                                                                                                                            2\\r\\n\\r\\n                                                Figure 3: Wave pattern of the HLLC solver (61).\\r\\n\\r\\n\\r\\n\\r\\nand using (53) with W HLLC and s? in place of W and u? , we obtain\\r\\n\\r\\n              sR (u?R − uR ) + s? (u?L − u?R ) + sL (uL − u?L ) + f(uR ) · n − f(uL ) · n + s? (ΓR − ΓL )eΓ + (ΠR − ΠL )eΠ = 0.\\r\\n                                                                                              \\x10                           \\x11\\r\\n                                                                                                                                    (63)\\r\\n\\r\\n    The component on mass conservation in the above relation gives\\r\\n\\r\\n                                         ρR (sR − uR ) + ρL (uL − sL ) = ρ?R (sR − s? ) + ρ?L (s? − sL ),\\r\\n\\r\\nwhich is satisfied by further requiring the half-consistency conditions [3] which give\\r\\n\\r\\n                            QL = ρL (uL − sL ) = ρ?L (s? − sL ) > 0,             QR = ρR (sR − uR ) = ρ?R (sR − s? ) > 0,           (64)\\r\\n\\r\\nand will be referred to as the mass fluxes [59]. Note that (64) will be shown to be important for the proof of entropy stability\\r\\nin section 5.4.1 are also usually invoked through the satisfaction of some jump relations across the sL and sR waves [60, 2] (see\\r\\nbelow).\\r\\n    From the approximate global consistency relation (63) we also deduce\\r\\n\\r\\n                                         QR (u?R − uR ) + QL (u?L − uL ) = pL − pR ,\\r\\n                                      QR (v⊥R − v?⊥              ?⊥\\r\\n                                                 R ) + QL (vL − vL ) = 0,\\r\\n                                                            ⊥\\r\\n\\r\\n                                       QR (ER? − ER ) + QL (E L? − E L ) = pL uL − pR uR ,\\r\\n                                          sR (ΓR − Γ?R ) + sL (Γ?L − ΓL ) = s? (ΓR − Γ?R ) + s? (Γ?L − ΓL ),\\r\\n                                        sR (ΠR − Π?R ) + sL (Π?L − ΠL ) = s? (ΠR − Π?R ) + s? (Π?L − ΠL ),\\r\\n\\r\\n\\r\\n\\r\\nwhere v⊥ = v − un denotes the velocity component perpendicular to n. The second and two latter conditions impose\\r\\n\\r\\n                              v?⊥\\r\\n                               L = vL ,\\r\\n                                    ⊥\\r\\n                                               v?⊥\\r\\n                                                R = vR ,\\r\\n                                                     ⊥\\r\\n                                                              Γ?L = ΓL ,         Γ?R = ΓR ,      Π?L = ΠL ,   Π?R = ΠR ,            (65)\\r\\n\\r\\nmeaning that v⊥ , Γ, and Π are continuous across shocks and may be discontinuous across s? in agreement with the fact that v⊥ , Γ,\\r\\nand Π are associated to LD fields. The remaining unknowns can be computed by imposing the Rankine-Hugoniot relations across\\r\\nsL and sR\\r\\n\\r\\n                                       f?L − f(uL ) · n = sL (u?L − uL ),        f?R − f(uR ) · n = sR (u?R − uR ),                 (66)\\r\\n\\r\\n\\r\\n                                                                            16\\r\\n\\x0c\\n\\n        ?\\r\\nwhere fX=L,R means that we are considering p?X as an unknown instead of evaluating the pressure via the EOS (5) and u?X . This leads\\r\\nto the following definition of the velocity in the star regions:\\r\\n\\r\\n                                                                p?L − pL                            p?R − pR\\r\\n                                                   u?L = uL −            ,        u?R = uR +                 ,                                 (67)\\r\\n                                                                   QL                                  QR\\r\\nand to the following jump relation across the s? wave:\\r\\n\\r\\n                                            fR? − fL? = s? u?R − u?L − (ΓR − ΓL )eΓ − (ΠR − ΠL )eΠ .\\r\\n                                                          \\x10                                       \\x11\\r\\n\\r\\n    Further imposing continuity of the velocity and the pressure across the intermediate wave, u?L = u?R = s? and p?L = p?R = p? ,\\r\\ngives the expression for the pressure and velocity in the star region:\\r\\n\\r\\n                                         QL pR + QR pL + QR QL (uL − uR )                          QL uL + QR uR + pL − pR\\r\\n                                  p? =                                    ,               s? =                             .                   (68)\\r\\n                                                    Q L + QR                                              Q L + QR\\r\\n    The other states are then directly obtained:\\r\\n\\r\\n\\r\\n                                                      s? − u L\\r\\n                                                                    !                                                               !\\r\\n                uL − sL                                          pL                           1                                  pL\\r\\n        ρ?L =            ρL ,   e?L = eL + (s? − uL )          −      ,           E L? = e?L + v?L · v?L = E L + (s? − uL ) s? −      ,       (69a)\\r\\n                s? − s L                                 2       QL                           2                                  QL\\r\\n                                                      s? − uR\\r\\n                                                                    !                                                               !\\r\\n                sR − u R                                         pR                           1                                  pR\\r\\n        ρ?R =            ρR ,   e?R = eR + (s? − uR )          +       ,          ER? = e?R + v?R · v?R = ER + (s? − uR ) s? +         .      (69b)\\r\\n                sR − s?                                  2       QR                           2                                  QR\\r\\n\\r\\n    As for the Godunov method in (57), we now define the fluctuations through\\r\\n\\r\\n                                                                              Z 0\\r\\n                                                               h       1                           \\x12 x                \\x13\\r\\n                                         D− (uL , uR , n) =       uL −                    W HLLC         ; uL , uR , n dx,                    (70a)\\r\\n                                                              2∆t      ∆t         − h2              ∆t\\r\\n                                                                              Z h\\r\\n                                                               h       1              2\\r\\n                                                                                                   \\x12 x                \\x13\\r\\n                                         D+ (uL , uR , n) =       uR −                    W HLLC         ; uL , uR , n dx,                    (70b)\\r\\n                                                              2∆t      ∆t         0                 ∆t\\r\\n\\r\\nand plugging (61) into (70) gives\\r\\n                                          \\uf8f1\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4 0,                                                                               0 < sL ,\\r\\n                                          \\uf8f2 sL (u?L − uL ),                                                                  s L < 0 < s? ,\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                       D (uL , uR , n) = \\uf8f4\\r\\n                        −\\r\\n                                          \\uf8f4\\r\\n                                                                                                                                              (71a)\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4 f?R − f(uL ) · n + s? (ΓR − ΓL )eΓ + s? (ΠR − ΠL )eΠ ,                           s? < 0 < sR ,\\r\\n                                          \\uf8f3 f(uR ) · n − f(uL ) · n + s? (ΓR − ΓL )eΓ + s? (ΠR − ΠL )eΠ ,                    sR < 0,\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f1\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4 f(uR ) · n − f(uL ) · n + s? (ΓR − ΓL )eΓ + s? (ΠR − ΠL )eΠ ,                    0 < sL ,\\r\\n                                          \\uf8f2 f(uR ) · n − f?R + s? (ΓR − ΓL )eΓ + s? (ΠR − ΠL )eΠ ,                           s L < 0 < s? ,\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                       D+ (uL , uR , n) = \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                                                                                                                                              (71b)\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4 sR (uR − u?R ),                                                                  s? < 0 < sR ,\\r\\n                                                                                                                             sR < 0.\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f3 0,\\r\\n                                          \\uf8f4\\r\\n                                          \\uf8f4\\r\\n\\r\\n    Note that by construction the numerical fluxes (71) satisfy the relation\\r\\n\\r\\n                          D− (uL , uR , n) + D+ (uL , uR , n) = f(uR ) · n − f(uL ) · n + s? (ΓR − ΓL )eΓ + s∗ (ΠR − ΠL )eΠ\\r\\n\\r\\nsimilar to the path-conservation property (58) [43]. Note that for (58) to hold, one needs s? = u? which would require a root-finding\\r\\nalgorithm to evaluate u? . We here follow another strategy where we approximate u? by s? in (68), this is justified by the fact that\\r\\nthe nonconservative product is here associated to a LD field.\\r\\n    Finally, we define the updated cell-averaged solution as\\r\\n                                  1 xj                                        1 x j+ 12\\r\\n                                    Z             \\x12 x                  \\x13       Z               \\x12 x                  \\x13\\r\\n                         U(n+1) =         W  HLLC\\r\\n                                                      ; U n\\r\\n                                                              , U n\\r\\n                                                                    , n  dx +           W HLLC     ; Unj , Unj+1 , n dx,                       (72)\\r\\n                           j\\r\\n                                  h x 1            ∆t     j−1     j\\r\\n                                                                              h xj              ∆t\\r\\n                                            j− 2\\r\\n\\r\\n\\r\\n\\r\\nso the HLLC solver may be recast into the three-point scheme form (55) with (71).\\r\\n\\r\\n                                                                             17\\r\\n\\x0c\\n\\n5.4. Properties of the HLLC solver\\r\\n     In this section, we analyse the properties of the numerical scheme (55) using fluxes (71), where the time step ∆t > 0 is assumed\\r\\nto satisfy the CFL condition\\r\\n                                           ∆t       \\x12                                         \\x13 1\\r\\n                                               max sR (Unj , Unj+1 , n) , sL (Unj , Unj+1 , n) 6 ,                                (73)\\r\\n                                            h j∈Z                                               2\\r\\nwhere the wave speeds sL and sR are defined in section 5.4.4.\\r\\n\\r\\n\\r\\n5.4.1. Discrete entropy inequality\\r\\n     We are here interested in the nonlinear stability of the scheme (55) and follow [3] to use the local entropy minimum principles\\r\\nand first prove in Theorem 5.1 the entropy inequality in integral form (60) for the HLLC solution (61). We then prove in Lemma 5.1\\r\\nthat the local entropy minimum principles hold for the intermediate states.\\r\\n\\r\\nTheorem 5.1. Suppose that condition (73) on the time step holds and that the intermediate states in the HLLC solver (61), satisfy\\r\\nu?L , u?R ∈ ΩGM together with the following local minimum entropy principles\\r\\n\\r\\n                                                           s(u?L ) > s(uL ),        s(u?R ) > s(uR ),                                              (74)\\r\\n\\r\\nfor the specific entropy (12). Then, the three-point scheme (55) satisfies an entropy inequality (60) with the consistent numerical\\r\\nflux                                                          Z xj\\r\\n                                                            1               \\x12 x                  \\x13!     h \\x10 n\\x11\\r\\n                         Q(Unj−1 , Unj , n) = q(Unj ) · n +        η W HLLC     ; Unj−1 , Unj , n dx −     η Uj .              (75)\\r\\n                                                            ∆t x 1           ∆t                        2∆t\\r\\n                                                                      j− 2\\r\\n\\r\\n\\r\\n\\r\\nProof. We first prove the entropy inequality in integral form (54) using (62), therefore, we have\\r\\n Z h                 \\x12 x              \\x13!                 !                                                                !\\r\\n     2                                                 h                                                        h\\r\\n          η W HLLC       ; uL , uR , n dx = sL ∆t +        η(uL ) + (s? − sL )∆tη(u?L ) + (sR − s? )∆tη(u?R ) +    − sR ∆t η(uR ),\\r\\n   − h2               ∆t                               2                                                        2\\r\\n                                          (11) h \\x10\\r\\n                                                  η(uL ) + η(uR ) − ∆t sL ρL s(uL ) + (s? − sL )ρ?L s(u?L ) − ∆t (sR − s? )ρ?R s(u?R ) − sR ρR s(uR ) ,\\r\\n                                                                  \\x11      \\x10                                 \\x11    \\x10                                    \\x11\\r\\n                                           =\\r\\n                                               2\\r\\n                                          (74) h \\x10\\r\\n                                                  η(uL ) + η(uR ) − ∆ts(uL ) sL ρL + (s? − sL )ρ?L − ∆t (sR − s? )ρ?R − sR ρR s(uR ),\\r\\n                                                                  \\x11           \\x10                     \\x11        \\x10                   \\x11\\r\\n                                           6\\r\\n                                               2\\r\\n                                              h\\x10                \\x11      \\x10                         \\x11\\r\\n                                          = η(uL ) + η(uR ) − ∆t ρR vR s(uR ) − ρL vL s(uL ) · n,\\r\\n                                              2\\r\\nwhere we have used the half-consistency conditions (64). As a consequence, setting uL = Unj and uR = Unj+1 , the numerical flux\\r\\n(75) satisfies\\r\\n                                                         1 x j+ 12                               \\x13!\\r\\n                                                          Z                 \\x12 x                         h\\r\\n                      Q(Unj , Unj+1 , n) 6 q(Unj ) · n −           η W HLLC     ; Unj , Unj+1 , n dx +     η(Unj ),        (76)\\r\\n                                                         h xj                ∆t                        2∆t\\r\\n\\r\\nand using (72) we have the following relation through Jensen’s inequality for the convex entropy function (11)\\r\\n\\r\\n                        1 xj                                     \\x13!         1 x j+ 21                             \\x13!\\r\\n                          Z                   \\x12 x                             Z                \\x12 x\\r\\n            η(U j ) 6\\r\\n               n+1\\r\\n                                  η W   HLLC\\r\\n                                                  ; U , U , n dx +\\r\\n                                                       n     n\\r\\n                                                                                      η W HLLC\\r\\n                                                                                                    ; U , U , n dx,\\r\\n                                                                                                         n   n\\r\\n                        h x 1                  ∆t j−1 j                     h xj                 ∆t j j+1\\r\\n                             j− 2\\r\\n\\r\\n                    (75) ∆t                                                         ∆t\\r\\n                                                                                !                                                      !\\r\\n                                                                    h                                                        h\\r\\n                     6        Q(Unj−1 , Unj , n) − q(Unj ) · n +      η(Unj )) +        −Q(Unj , Unj+1 , n) + q(Unj ) · n +     η(Unj ) ,\\r\\n                    (76) h                                        2∆t                h                                      2∆t\\r\\n                                  ∆t \\x10                                        \\x11\\r\\n                    = η(Unj ) −        Q(Unj , Unj+1 , n) − Q(Unj−1 , Unj , n) .\\r\\n                                  h\\r\\n\\r\\n\\r\\nLemma 5.1. There exist wave speed estimates sL and sR large enough that: (i) bound the minimum and maximum wave speeds in\\r\\nthe exact entropy weak solution of the Riemann problem (51), (ii) satisfy the interlacing condition sL < s? < sR , (iii) ensure that\\r\\nthe local minimum entropy principles (74) hold.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                               18\\r\\n\\x0c\\n\\nProof. The first result (i) is obvious under the assumption that the wave speeds are finite in the exact Riemann solution. Then,\\r\\n(ii) and (iii) are consequences of [3, Prop. 3.2] and we now show that the required assumptions on the half domains separated by\\r\\n x\\r\\n t\\r\\n   = s? hold, see [3, after Prop. 3.2]. This is justified here because the EOS (5) does not change accross the extreme waves sL and\\r\\nsR , hence xt = s? therefore separates domains with one unique equivalent “pure phase” and its associated physical entropy (12)\\r\\nwhich is strictly convex and satisfies the Gibbs principle (13). The required conditions across the extreme waves are: first, the\\r\\nhalf-consistency relations hold through (64); then, the following quantities must be invariant across the sL and sR waves [3, § 4.2]:\\r\\n                                            Q2X       Q2X               (p?X )2        p2\\r\\n                                    p?X +    ? = pX +     ,     e?X −           = eX − X2 ,       X = L, R.\\r\\n                                            ρX        ρX                2QX  2\\r\\n                                                                                      2QX\\r\\n\\r\\n    The first relation is a direct consequence of (67), with u?L = u?R = s? :\\r\\n                                            p?L = pL + QL (uL − s? ),        p?R = pR + QR (s? − uR ),\\r\\n\\r\\nand (64). For the second relation, we inject the above relations in the expression of e?X=L,R in (69) to get\\r\\n                                            pX − p?X \\uf8ec\\uf8ec\\uf8ec pX − p?X pX − \\uf8f7\\uf8f7\\uf8f7     (p? )2 − p2X\\r\\n                                                       \\uf8eb                 \\uf8f6\\r\\n                                e?X − pX =             \\uf8ec\\uf8ed        −       \\uf8f7\\uf8f8 = − X           , X = L, R,\\r\\n                                               QX         2QX      QX             2QX\\r\\n\\r\\nwhich concludes the proof.\\r\\n\\r\\n    We finally link the discrete entropy inequality (54) to the entropy stable character of the numerical fluctuations [9], so that\\r\\nthe HLLC fluxes can be used at the interfaces in the DGSEM scheme to prove the semi-discrete entropy inequality established in\\r\\nTheorem 3.1.\\r\\nCorollary 5.1. Let a three-point scheme of the form (55) to discretize (1a). Then, the entropy inequality (60) implies the entropy\\r\\nstability of the numerical fluxes in the sense of (36).\\r\\n\\r\\nProof. The proof relies on similar arguments as the ones used in [5, Lemma 2.8] in the conservative setting. Let Unj−1 = Unj = u−\\r\\nand Unj+1 = u+ , then from (55), we obtain Un+1\\r\\n                                            j   = u− − ∆th D− (u− , u+ , n) and (60) gives\\r\\n                                         ∆t                              ∆t \\x10\\r\\n                                                          !\\r\\n                                 η u− − D− (u− , u+ , n) 6 η(u− ) −           Q(u− , u+ , n) − q(u− ) · n .\\r\\n                                                                                                         \\x11\\r\\n                                         h                                h\\r\\nLikewise, using Unj−1 = u− and Unj = Unj+1 = u+ , we get Un+1\\r\\n                                                           j  = u+ − D+ (u− , u+ , n) and\\r\\n                                        ∆t                           ∆t \\x10 +\\r\\n                                                         !\\r\\n                               η u+ − D+ (u− , u+ , n) 6 η(u+ ) −        q(u ) · n − Q(u− , u+ , n) .\\r\\n                                                                                                   \\x11\\r\\n                                         h                           h\\r\\nSumming both equations and letting ∆t → 0+ with fixed h, we have up to O(∆t):\\r\\n                   ∆t                                     ∆t                                             ∆t \\x10 +\\r\\n                      ϑ(u− ) · D− (u− , u+ , n) + η(u+ ) − ϑ(u+ ) · D+ (u− , u+ , n) 6 η(u− ) + η(u+ ) −\\r\\n                                                                                                                           \\x11\\r\\n          η(u− ) −                                                                                           q(u ) − q(u− ) · n,\\r\\n                   h                                      h                                              h\\r\\nand simplifying terms gives (36).\\r\\n\\r\\n    Finally, as an immediate consequence of the local minimum entropy principles (74) and the definition of the updated solution\\r\\n(72), the HLLC solver also satisfies a discrete minimum principle on the specific physical entropy s(u):\\r\\n                                                             \\x10                             \\x11\\r\\n                                                    j ) > min s(U j−1 ), s(U j ), s(U j+1 ) .\\r\\n                                                 s(Un+1          n          n        n\\r\\n                                                                                                                                   (77)\\r\\n\\r\\n5.4.2. Preservation of material interfaces and pure phases\\r\\n    We, first, prove that the three-point scheme (55), with numerical fluxes (71), preserves material interfaces [1] by following the\\r\\nsame analysis as in section 4.1. Let us assume that the left and right states satisfy vL = vR = v and pL = pR = p, then (68) gives\\r\\np? = p and s? = u, respectively. As a result, the discrete requirements dvρ = vdρ and dρE = ( 12 v · v)dρ + pdΓ + dΠ applied to the\\r\\nthree-point scheme (55) impose\\r\\n                                                                   v·v ±\\r\\n                                             D±ρu = uD±ρ , D±ρE =       D + pD±Γ + D±Π ,                                         (78)\\r\\n                                                                    2 ρ\\r\\nand are obviously satisfied. Now assume that ΓL = ΓR and ΠL = ΠR , then D±Γ = 0 and D±Π = 0 and the fluxes reduce to the\\r\\nconservative HLLC solver for the Euler equations so pure phases are preserved.\\r\\n\\r\\n                                                                        19\\r\\n\\x0c\\n\\n5.4.3. Positivity of the solution\\r\\n    Since the updated solution (72) is the cell-average of the superposition of approximate Riemann solutions from the HLLC\\r\\nsolver (61), and assuming that the left and right states are positive, it is sufficient to prove positivity of the intermediate states in the\\r\\n                                                                                                                         ?\\r\\nRiemann solution [20]. Note that the intermediate states should be evaluated from the intermediate fluxes [2], fX=L,R          in (66) since\\r\\nwe are imposing the pressure via (68), and not evaluating it from the EOS (5), see section 5.3.\\r\\n     The proof for positivity of density in the star region can be directly stated following its definition in (69) since sL < uL , uR < sR\\r\\nfrom the wave estimates in section 5.4.4, and the interlacing property sL < s? < sR in Lemma 5.1. According to (7) hyperbolicity\\r\\nof (1a) and positivity of the solution in the star region require satisfying ρ?X e?X > p∞X , X = L, R, which gives for the left intermediate\\r\\nstate:\\r\\n                  \\uf8ec\\uf8ec ? (s? )2 \\uf8f7\\uf8f7\\uf8f7                                                                      (s? )2 \\uf8f7\\uf8f7\\uf8f7\\r\\n                  \\uf8eb             \\uf8f6                 \\uf8eb                                                             \\uf8f6\\r\\n               ?\\uf8ec                         (69) ? \\uf8ec        ?  ?             ?             pL\\r\\n              ρL \\uf8ed\\uf8ecE L −                  ⇔ ρL \\uf8ed\\uf8ecE L + s (s − uL ) − (s − uL )\\r\\n                                \\uf8f8\\uf8f7 > p∞L (64)                                                        −          \\uf8f8\\uf8f7 > p∞L\\r\\n                                                  \\uf8ec\\r\\n                                                  \\uf8ec\\r\\n                           2                                                       ρL (sL − uL )         2\\r\\n                                                        (uL − s? )2\\r\\n                                                  \\uf8eb                                             \\uf8f6\\r\\n                                                                                       pL\\r\\n                                           ⇔ρ?L \\uf8ec\\uf8ec\\uf8edeL +              − (s? − uL )                 \\uf8f7\\uf8f8 > p∞L\\r\\n                                                  \\uf8ec\\uf8ec                                            \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                             2                    ρL (uL − sL )\\r\\n                                                            pL + γL p∞L       uL − sL ρL (uL − s? )2               s? − uL\\r\\n                                                        !                !             !                                   !\\r\\n                                           (5) uL − sL\\r\\n                                          ⇔ ?                              + ?                             − ?               pL − p∞L > 0\\r\\n                                          (64) s − sL         γL − 1          s − sL               2               s − sL\\r\\n                                       s? >sL             pL + γL p∞L                 ρL\\r\\n                                                                       !\\r\\n                                        ⇔ (uL − sL )                     + (uL − sL ) σ2 + σpL − (s? − sL )p∞L > 0\\r\\n                                                             γL − 1                    2\\r\\n                                                        ρL 2                                   pL + p∞L\\r\\n                                                                                                            !\\r\\n                                           ⇔(uL − sL ) σ + (pL + p∞L )σ + (uL − sL )                              > 0,\\r\\n                                                         2                                          γL − 1\\r\\n\\r\\nwhere σ = uL − s? . The sign for this inequality holds for all σ ∈ R if the discriminant D of the above quadratic equation is\\r\\nnegative:\\r\\n                                                                                 pL + p∞L\\r\\n                                                                                           !\\r\\n                                             D = (pL + p∞L )2 − 2ρL (uL − sL )2\\r\\n                                                                                   γL − 1\\r\\n\\r\\n    Using γL (pL + p∞L ) = ρL c2L , D < 0 implies sL < uL − (γL − 1)/2γL cL which is satisfied by the wave speed estimates in\\r\\n                                                                  p\\r\\n\\r\\nsection 5.4.4 since γ2γ\\r\\n                     L −1\\r\\n                        L\\r\\n                          < 1. A similar result holds for the right intermediate state. Note that the above relations are the same as in\\r\\n[2] with pX + p∞X instead of pX .\\r\\n     Finally, let prove positivity of Γ and Π through a discrete maximum principle. The discrete equation for X ∈ {Γ, Π} in (55) with\\r\\nfluctuations (71) reads\\r\\n                                  ∆t \\x12                 \\x10             \\x11                   \\x10             \\x11\\x13\\r\\n                    X n+1\\r\\n                      j   = X n\\r\\n                              j −      min(s ∗\\r\\n                                                1\\r\\n                                             j+ 2\\r\\n                                                  , 0)   X n\\r\\n                                                           j+1 − X n\\r\\n                                                                   j   + max(s ∗\\r\\n                                                                                  1\\r\\n                                                                               j− 2\\r\\n                                                                                    , 0)   X n\\r\\n                                                                                             j−1 − X n\\r\\n                                                                                                     j\\r\\n                                   h\\r\\n                                  ∆t \\x12                                   \\x13!       ∆t                           ∆t\\r\\n                          = 1−         max(s∗j− 1 , 0) − min(s∗j+ 1 , 0) X nj −        min(s∗j+ 1 , 0)X nj+1 +    max(s∗j− 1 , 0)X nj−1 (79)\\r\\n                                  h             2                   2              h             2             h           2\\r\\n\\r\\n\\r\\n\\r\\nwhich shows that X n+1\\r\\n                   j\\r\\n                                                       n\\r\\n                       is a convex combination of the Xi= j±1, j under the CFL condition (73).\\r\\n\\r\\n\\r\\n\\r\\n5.4.4. Wave speed estimates\\r\\n     The fan of waves of the HLLC solver must contain the fan of waves of the exact Riemann problem (51). This is in particular\\r\\nrequired to ensure the local entropy minimum principles (74), see [3, Prop. 3.2]. Direct wave speed estimates have been proposed\\r\\nthat do not require to solve the exact Riemann problem [57, 5]. Note that the usual estimate S R = −S L = max(|uL | + cL , |uR | + cR )\\r\\nmay be wrong due to the Lax entropy condition across a shock. On the other hand, the time steps can be affected by overestimating\\r\\nwave speed estimates through (73). We propose the following wave speeds estimates\\r\\n\\r\\n                                                        sL = uL − c̃L ,        sR = uR + c̃R ,                                              (80)\\r\\n\\r\\nwhere uX = vX · n, X = L, R, and\\r\\n                       \\uf8f1                      \\x12                      \\x13                   \\uf8f1                     \\x12                      \\x13\\r\\n                                      γ+1                                                              γ+1\\r\\n                              =     +           pR −pL\\r\\n                                                       +          ,     ,                      =     +           pL −pR\\r\\n                                                                                                                        +          ,    ,\\r\\n                       \\uf8f4                                                                 \\uf8f4\\r\\n                         c̃     c         max            u   − u    0                      c̃    c         max            u   − u    0\\r\\n                       \\uf8f4\\r\\n                       \\uf8f4                                                                 \\uf8f4\\r\\n                                                                                         \\uf8f4\\r\\n                            L     L                        L    R                            R     R                        L    R\\r\\n                                              \\x12 ρR cR                                                          \\x12 ρL cL\\r\\n                       \\uf8f4\\r\\n                       \\uf8f4               2\\r\\n                                                                                         \\uf8f4\\r\\n                                                                                         \\uf8f4              2\\r\\n          if pR > pL : \\uf8f4                                                          else : \\uf8f4                                                  (81)\\r\\n                       \\uf8f2                                              \\x13                  \\uf8f2                                            \\x13\\r\\n                                      γ+1                                                              γ+1\\r\\n                       \\uf8f3 c̃R = cR + 2 max ρL c̃L + uL − uR , 0 ,                         \\uf8f3 c̃L = cL + 2 max ρR c̃R + uL − uR , 0 ,\\r\\n                       \\uf8f4\\r\\n                       \\uf8f4\\r\\n                       \\uf8f4                        p  −p\\r\\n                                                  L R                                    \\uf8f4\\r\\n                                                                                         \\uf8f4\\r\\n                                                                                         \\uf8f4                       p  −p\\r\\n                                                                                                                   R L\\r\\n                       \\uf8f4                                                                 \\uf8f4\\r\\n\\r\\n                                                                          20\\r\\n\\x0c\\n\\nand γ = max(γL , γR ), cX = γ(pX + p∞X )/ρX for X = L, R. These estimates will bound the wave speeds in the exact Riemann\\r\\n                               p\\r\\n\\r\\nsolution in the case of pure phases [5]. Then, the above definition of γ will allow to bound the signal speeds in the case of polytropic\\r\\ngases, p∞L = p∞R = 0 [41]. Though, it is difficult to guarantee such properties in the general case when ΓL , ΓR and ΠL , Πr ,\\r\\nthese estimates proved to be robust in the present numerical experiments.\\r\\n\\r\\n\\r\\n6. Properties of the DGSEM scheme\\r\\n\\r\\n    We recall here the main properties of the DGSEM scheme proposed in this work for the discretization of the SG-gamma model\\r\\n(1) with a stiffened gas EOS (5).\\r\\n\\r\\n\\r\\n6.1. Semi-discrete scheme\\r\\n    The semi-discrete scheme (31) with the EC fluxes (48) in the volume integral and the HLLC flux (71) at interfaces satisfy the\\r\\nsemi-discrete entropy inequality, see Theorem 3.1. In contrast, the scheme with the CP fluxes (46) in the volume integral, along\\r\\nwith the HLLC solver at the interfaces, preserves uniform pressure and velocity profiles across material interfaces.\\r\\n\\r\\n\\r\\n6.2. Fully discrete scheme\\r\\n     We restrict ourselves to the use of a one-step first-order explicit time discretization for both CP and EC numerical fluctuations\\r\\nin the volume integral. High-order time integration will be done by using a strong-stability preserving explicit Runge-Kutta method\\r\\nfrom [52] that is a convex combination of forward Euler steps and thus keeps the properties of the first-order in time scheme under\\r\\nsome condition on the time step. The fully discrete DGSEM scheme reads\\r\\n\\r\\n                                                  Uiκj,n+1 − Uiκj,n\\r\\n                                   ωi ω j Jκi j                     + Riκj (uh(n) ) = 0   ∀κ ∈ Ωh , 0 6 i, j 6 p, n > 0,                                (82)\\r\\n                                                        ∆t(n)\\r\\n\\r\\n                                                                      h = uh (·, t ), and the vector of space residuals Rκ (·) is defined from\\r\\nwhere ∆t(n) = t(n+1) − t(n) is the time step, Uiκj,n = Uiκj (t(n) ), u(n)         (n)                                    ij\\r\\n\\r\\n(31). The following theorem summarizes the properties of the above scheme with fluctuations (30) and (26). These properties are\\r\\nindependent of the EC or CP character of the volume fluctuations and therefore hold for both EC and CP fluxes for the SG-gamma\\r\\nmodel (2), though the minimum entropy principle holds when excluding material interfaces. Likewise, any other interface flux that\\r\\nsatisfy properties (i) to (iv) below can be used in place of the HLLC solver.\\r\\n     The derivation of the CFL condition will rely on the work in [6, Lemma 3.4] that proves that there exist pseudo-equilibrium\\r\\nstates u?,n                                        ?,n\\r\\n        κ in ΩGM and finite wave speed estimates λκ > 0 such that\\r\\n\\r\\n\\r\\n                       p                                                                                                   \\x12\\r\\n                      XX                                                                                                                          \\x11\\x13\\r\\n                                 ωk Jek hRus u?,n                                  λ?,n                              |λ|max u?,n\\r\\n                                            \\x10                      \\x11                                                \\x10\\r\\n                                              κ , uh (xe , t ), ne = 0,                                                      κ , uh (xe , t ), ne ,\\r\\n                                                   − k (n)       k                                                                − k (n)       k\\r\\n                                                                                    κ >              max                                                (83)\\r\\n                                                                                                  06k6p, e∈∂κ\\r\\n                      e∈∂κ k=0\\r\\n\\r\\n                                                    λ?,n\\r\\nwhere hRus (u− , u+ , n) = 21 f(u− ) + f(u+ ) · n − κ2 (u+ − u− ) is the Rusanov flux and λ?,n\\r\\n                             \\x10               \\x11\\r\\n                                                                                                    κ   bounds the fans of waves in the exact\\r\\nRiemann problems (51) with n = nke and left and right states u?,n   κ   and u−h (xke , t(n) ), see (84). Note that other conditions adapted to\\r\\nunstructured meshes may be used [49].\\r\\n\\r\\nTheorem 6.1. Let us consider the DGSEM scheme (82) with consistent fluctuations (30) of the form (37) in the volume integrals\\r\\n                                                                                                             j∈Z ∈ ΩGM ; (ii) satisfies\\r\\nand consistent interface fluctuations (26) such that the associated three-point scheme (55): (i) is robust: Un>0\\r\\na discrete entropy inequality (60) with consistent numerical flux; (iii) satisfies a discrete minimum principle (77) on the specific\\r\\nphysical entropy s(u); (iv) satisfies discrete maximum principles (79) on Γ and Π. Then, the updated cell-averaged solution hu(n+1)\\r\\n                                                                                                                                h    iκ\\r\\nis a convex combination of DOFs at time t(n) and updates of three-point schemes under the following conditions on the time step:\\r\\n\\r\\n\\r\\n                                                                ωk Jek                                                                    1\\r\\n                                                                                                                   , λ?,n , λ?,n\\r\\n                                                                                 \\x10                                               \\x11\\r\\n                                 ∆t(n) max max                                max snL              , snR              κ−     κ+\\r\\n                                                                                                                                   6            ,      (84a)\\r\\n                                        e∈Eh 06k6p ω̃k min Jκ± (xke ))                    j+ 21            j− 12                       p(p + 1)\\r\\n                                                    p\\r\\n                                                   X     ωk \\x10                                                      \\x11\\r\\n                            ∆t(n) max max                        ω j Dki vkκ j,n · n(i,k) j + ωi Dk j vik,n\\r\\n                                                                                                       κ · ni( j,k) 6 1,                               (84b)\\r\\n                                                   k=0 ω̃i j Jκ\\r\\n                                     κ∈Ωh 06i, j6p            ij\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                  21\\r\\n\\x0c\\n\\n                                                                                               ωω                         ω0\\r\\nwhere λ?,n\\r\\n        κ is defined in (83), ω̃i j = ωi ω j when 0 < i, j < p, ω̃i j = 2 else, and ω̃k = ωk when 0 < k < p, ω̃0 = ω̃ p = 2 = p(p+1) .\\r\\n                                                                        i j                                                     1\\r\\n\\r\\nAssuming that the DOFs Uκ are in ΩGM for all κ ∈ Ωh and 0 6 i, j 6 p, the DGSEM scheme (82) guarantees positivity of the\\r\\n                           i j,n\\r\\n\\r\\ncell-averaged solution:\\r\\n                                                           h iκ ∈ ΩGM\\r\\n                                                        hun+1             ∀κ ∈ Ωh ,\\r\\n\\r\\ntogether with a minimum principle on the specific entropy when excluding material interfaces\\r\\n\\r\\n                                 iκ ) > min s(Uiκj,n ) : 0 6 i, j 6 p ∪ s u+h (xke , t(n) ) : e ∈ ∂κ, 0 6 k 6 p ,\\r\\n                                           n                         o n \\x10                 \\x11                   o\\r\\n                       s(hu(n+1)\\r\\n                           h\\r\\n\\r\\n\\r\\nand maximum principles on the EOS parameters Y in {Γ, Π}:\\r\\n\\r\\n                                                                    Sκ (Yh(n) ) = {Yκi j,n : 0 6 i, j 6 p} ∪ Yh+ (xke , t(n) ) : e ∈ ∂κ, 0 6 k 6 p ∀κ ∈ Ωh .\\r\\n                                                                                                            n                                     o\\r\\n     min Sκ (Yh(n) ) 6 hYh(n+1) iκ 6 max Sκ (Yh(n) ),\\r\\n\\r\\nProof. From (38) and (82), the cell-averaged discrete scheme for the d + 2 first components of (1a), Y in {ρ, ρv, ρE}, reads\\r\\n                                                                                  p\\r\\n                                                                       ∆t(n) X X\\r\\n                                                                                                   u−h (xke , t(n) ), u+h (xke , t(n) ), nke ,\\r\\n                                                                                                  \\x10                                         \\x11\\r\\n                                          hYh(n+1) iκ = hYh(n) iκ −                  ωk Jek hHLLC\\r\\n                                                                                             Y\\r\\n                                                                        |κ| e∈∂κ k=0\\r\\n\\r\\n                 +                +\\r\\n        Y (u , u , n) = DY (u , u , n) + fY (u ) · n is the numerical flux in conservation form associated to the HLLC fluctuations\\r\\n             −             − −                 −\\r\\nwhere hHLLC\\r\\n(71). Following [6, Sec. 4.2], we add ∆t|κ| times the flux balance in (83) to the above relation and decompose hYh(n) iκ in (20) to get\\r\\n                                         (n)\\r\\n\\r\\n\\r\\n\\r\\n                                                p−1                             p\\r\\n                                                X             Jκi j i j,n X X             Jκ (xke ) − k (n)\\r\\n                                hYh(n+1) iκ =         ωi ω j       Yκ +            ω0 ω̃k             uh (xe , t )\\r\\n                                                i, j=1\\r\\n                                                              |κ|         e∈∂κ k=0\\r\\n                                                                                            |κ|\\r\\n                                                       ∆t(n) ωk Jek \\x12 HLLC \\x10 − k (n) + k (n) k \\x11                    Rus ?,n\\r\\n                                                                                                                       \\x10                      \\x11\\x13!\\r\\n                                                −                       h   u  (x  , t ), u  (x   , t   ), n    − h     uκ  , u− k (n)\\r\\n                                                                                                                                (x , t ), n k\\r\\n                                                                                                                                                 .\\r\\n                                                     ω0 ω̃k Jκ (xke ) Y       h e          h e              e       Y          h e          e\\r\\n\\r\\n\\r\\n\\r\\n    As a consequence, the first d + 2 components of hu(n+1)      h   iκ are convex combination of quantities in ΩGM under (84a) which\\r\\nconfirms positivity of hρ(n+1)\\r\\n                         h     i κ and ρe(hu(n+1)\\r\\n                                            h     i κ ) by concavity of ρe(u) = ρE − ρv·ρv\\r\\n                                                                                      2ρ\\r\\n                                                                                           . Excluding material interfaces, we also get the\\r\\nminimum entropy principle.\\r\\n    Likewise, the two last components Y in {Γ, Π} satisfy\\r\\n                                     \\uf8eb p                                                                               p                                                          \\uf8f6\\r\\n                            ∆t(n) \\uf8ec\\uf8ec\\uf8ec\\uf8ec X                                                                      \\x11 XX\\r\\n                                                                                                                          ωk Jek D−Y u−h (xke , t(n) ), u+h (xke , t(n) ), nke \\uf8f7\\uf8f7\\uf8f7\\uf8f8.\\r\\n                                                \\x10                                                                                   \\x10                                         \\x11\\uf8f7\\uf8f7\\r\\n   hYh(n+1) iκ =hYh(n) iκ −          \\uf8ec\\uf8ed   ωi ω j Dik viκj,n · n(i,k) j Yκk j,n + D jk viκj,n · ni( j,k) Yκik,n +\\r\\n                             |κ| i, j,k=0                                                                        e∈∂κ k=0\\r\\n\\r\\n\\r\\n     Inverting indices i and k, then j and k in the volume integral, then using consistency of the interface fluctuations (27) to add\\r\\nthe trivial quantity\\r\\n                                                     p\\r\\n                                          ∆t(n) X X\\r\\n                                                        ωk Jek D+Y u−h (xke , t(n) ), u−h (xke , t(n) ), nke = 0,\\r\\n                                                                  \\x10                                         \\x11\\r\\n                                        −\\r\\n                                           |κ| e∈∂κ k=0\\r\\n                                                                                          ij\\r\\n                                                                                           Jκ                            Jκ (xke ) − k (n)\\r\\nand finally using the convex decomposition hYh(n) iκ =                        i, j=0 ω̃i j |κ| Yκ +           k=0 ω̃0 ω̃k |κ| Yh (xe , t ) from (20) we get\\r\\n                                                                           Pp                   i j,n P      Pp\\r\\n                                                                                                        e∈∂κ\\r\\n\\r\\n\\r\\n\\r\\n                   p\\r\\n                                    \\uf8eb                   p\\r\\n                                                                                                                              \\uf8f6\\r\\n                   X       Jκi j \\uf8ec\\uf8ec\\uf8ec\\uf8ec        ∆t(n) X ωk ω j                                   ωi ωk                           \\uf8f7\\uf8f7\\uf8f7\\r\\n   hYh(n+1) iκ =        ω̃i j       \\uf8ec\\uf8ec\\uf8ed1 − i j                       Dki vkκ j,n · n(i,k) j +          Dk j vik,n\\r\\n                                                                                                              κ   · n i( j,k) \\uf8f7\\r\\n                                                                                                                                \\uf8f7\\uf8f7 Y i j,n\\r\\n                                                                                                                                 \\uf8f8 κ\\r\\n                  i, j=0\\r\\n                           |κ|                J κ      k=0\\r\\n                                                             ω̃  i j                           ω̃ i j\\r\\n\\r\\n                         p\\r\\n                                                                           ∆t(n) ωk Jek \\x12 − \\x10 − k (n) + k (n) k \\x11\\r\\n                                                     \\uf8eb                                                                                                                            \\uf8f6\\r\\n                  XX                     Jκ (xke ) \\uf8ec\\uf8ec\\uf8ec + k (n)                                                                                    + − k (n)\\r\\n                                                                                                                                                    \\x10                           \\x11\\x13\\uf8f7\\r\\n                +          ω̃0 ω̃k                   \\uf8ed\\uf8ecYh (xe , t ) −                        D     u   (x  , t   ), u   (x        , t    ), n  + D   u  (x , t ), u    ,\\r\\n                                                                                                                                                                   − k (n)\\r\\n                                                                                                                                                                    (x   t ), nk \\uf8f7\\uf8f7\\uf8f8 ,\\r\\n                  e∈∂κ k=0\\r\\n                                           |κ|                           ω̃ 0  ω̃ k Jκ (xk)\\r\\n                                                                                         e\\r\\n                                                                                               Y      h e            h e                     e    Y    h e         h e         e \\uf8f7\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nand invoking the metric identities in (41), hYh(n+1) iκ is indeed a convex combination of the DOFs associated to Yh(n) and updates of\\r\\nthree-point scheme (55) under (84). Positivity and the minimum and maximum principles follow directly.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                          22\\r\\n\\x0c\\n\\n7. A posteriori limiters\\r\\n\\r\\n    Properties of the discrete DGSEM scheme in Theorem 6.1 hold for the cell-averaged solution and a posteriori limiters are\\r\\napplied at the end of each Runge-Kutta stage to extend these properties to all DOFs within elements. Here we describe these\\r\\nlimiters for (82) that are similar to the ones proposed in [66, 65, 14, 62]. The basic principle consists in limiting DOFs at time t(n+1)\\r\\nthrough a linear scaling around the cell-average (20):\\r\\n                                    i j,n+1\\r\\n                                                \\x10                     \\x11\\r\\n                                  Ũκ       = θκ Uiκj,n+1 − hu(n+1)\\r\\n                                                              h     iκ + hu(n+1)\\r\\n                                                                           h     iκ ∀0 6 i, j 6 p, κ ∈ Ωh ,\\r\\n\\r\\nwhere 0 6 θκ 6 1 is the limiter coefficient. We here apply successive limiters (θκρ , θκρe , θκΓ , θκΠ ) on:\\r\\n\\r\\n    • mixture density:\\r\\n                                                                                                          \\uf8eb                          \\uf8f6\\r\\n                                                                                                          \\uf8ec\\uf8ec hρ(n+1) iκ − \\x0f          \\uf8f7\\uf8f7\\uf8f7\\r\\n                        ρ̃iκj,n+1 = θκρ ρiκj,n+1 − hρ(n+1)                                  θκρ = min \\uf8ec\\uf8ec\\uf8ec\\uf8ed\\r\\n                                         \\x10                         \\x11\\r\\n                                                           iκ          + hρ(n+1) iκ ,                          h\\r\\n                                                                                                                               , 1\\uf8f7\\uf8f7\\uf8f7\\uf8f8 ,            ρmin = min ρiκj,n+1 ;       (85)\\r\\n                                                                                                         \\uf8ec\\r\\n                                                     h                     h                                                                         κ\\r\\n                                                                                                             hρ (n+1)\\r\\n                                                                                                                h     i −ρ\\r\\n                                                                                                                      κ\\r\\n                                                                                                                           min\\r\\n                                                                                                                              κ\\r\\n                                                                                                                                                            06i, j6p\\r\\n\\r\\n\\r\\n    • EOS parameters Γ and Π:\\r\\n                                                                                                   \\uf8eb (n+1)                                       \\uf8f6\\r\\n                                 \\x10                       \\x11                                         \\uf8ec\\uf8ec hY   iκ − mY            MY − hYh(n+1) iκ   \\uf8f7\\uf8f7\\uf8f7\\r\\n              Ỹκi j,n+1 = θκY    Yκi j,n+1 − hYh(n+1) iκ + hYh(n+1) iκ ,         θκY = min \\uf8ec\\uf8ec\\uf8ec\\uf8ed         h\\r\\n                                                                                                                      ,                    , 1\\uf8f7\\uf8f7\\uf8f7\\uf8f8 ,              Y ∈ {Γ, Π},   (86)\\r\\n                                                                                               \\uf8ec\\r\\n                                                                                                       (n+1)      min     max      (n+1)\\r\\n                                                                                                    hYh      i −Y\\r\\n                                                                                                               κ      κ Y     − hYκ      i      h       κ\\r\\n\\r\\n\\r\\n       where Yκmin = min06i, j6p Yκi j,n+1 , Yκmax = max06i, j6p Yκi j,n+1 , and\\r\\n\\r\\n                                                    1                                   1                                 γi p∞i                              γi p∞i\\r\\n                                 mΓ = min                ,       MΓ = max                      ,     mΠ = min                         ,       MΠ = max                 ;\\r\\n                                        16i6n s γi − 1                     16i6n s γi − 1                      16i6n s γi − 1                         16i6n s γi − 1\\r\\n\\r\\n\\r\\n    • mixture total internal energy:\\r\\n\\r\\n                                                  Ỹκi j,n+1 = θκρe Yκi j,n+1 − hYh(n+1) iκ + hYh(n+1) iκ ,\\r\\n                                                                   \\x10                       \\x11\\r\\n                                                                                                                          Y ∈ {ρ, ρv, ρE},                                      (87)\\r\\n\\r\\n       where ρe(u) = ρE − ρv·ρv\\r\\n                           2ρ\\r\\n                                and\\r\\n                                                                            \\uf8eb           \\uf8eb                                         \\uf8f6       \\uf8f6\\r\\n                                                                            \\uf8ec\\uf8ec\\uf8ec         \\uf8ec\\uf8ec\\uf8ec ρe(huh in+1 ) − p̃i∞j,n+1 − \\x0f \\uf8f7\\uf8f7\\uf8f7 \\uf8f7\\uf8f7\\uf8f7\\r\\n                                                                                                    κ\\r\\n                                                             θκρe = min \\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed min \\uf8ec\\uf8ec\\uf8ec\\uf8ed                            κ         \\uf8f7\\uf8f7\\uf8f7 , 1\\uf8f7\\uf8f7\\uf8f7 .                                        (88)\\r\\n                                                                               06i, j6p      ρe(huh iκn+1 ) − ρeiκj,n+1 \\uf8f8 \\uf8f8\\r\\n                                                                                                                                     \\uf8f7\\r\\n\\r\\n    Note that in (85) and (88), 0 < \\x0f \\x1c 1 is a small parameter, which we set as \\x0f = 10−8 in our numerical tests. The limiters (85)\\r\\nand (87) thus guarantee that ρ̃06i\\r\\n                               κ\\r\\n                                   j6p,n+1\\r\\n                                                    ˜ κ06i j6p,n+1 > p̃06i\\r\\n                                           > 0 and ρe                  ∞κ\\r\\n                                                                           j6p,n+1\\r\\n                                                                                   , respectively. Recalling (6), we have mY 6 Y 6 MY , with\\r\\nY in {Γ, Π}, formally which may be viewed as relaxed maximum principles. The limiters in (86) thus impose similar maximum\\r\\nprinciples:\\r\\n                                           mΓ ≤ Γ̃06k6p,n+1\\r\\n                                                  j            6 MΓ , mΠ 6 Π̃06k6p,n+1j        6 MΠ .\\r\\n\\r\\n\\r\\n8. Numerical experiments\\r\\n\\r\\n     We now perform numerical tests on the DGSEM scheme for the SG-gamma model (1)-(2) where the space discretization is\\r\\ndefined in (31) and where the three-stage third-order strong stability-preserving Runge-Kutta scheme by Shu and Osher [52] is\\r\\nused for the time discretization. The time step is evaluated from the CFL condition (84a) and the limiter, introduced in section 7, is\\r\\napplied at the end of each stage. We consider numerical tests from [13, 15, 14, 35, 16]. The scheme was implemented in the CFD\\r\\ncode Aghora developed at ONERA [50]. We recall that here we have proposed two DGSEM schemes for the SG-gamma model\\r\\nthat differ in the fluctuations (32) in the volume integral: a CP scheme with (46); a semi-discrete entropy stable scheme with EC\\r\\nfluxes (48). We will compare the performances of both schemes in this section. All numerical tests are performed at fourth-order\\r\\naccuracy, p = 3, in space unless stated otherwise.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                            23\\r\\n\\x0c\\n\\n8.1. Advection of a density wave\\r\\n    We begin by validating the high-order accuracy of the scheme (31) by advecting a density wave in a uniform flow in the domain\\r\\nΩ = [0, 1]2 , discretized with unstructured meshes with fourth-order curved elements (see Figure 4(a)), with periodic conditions\\r\\nand initial condition u0 (x):\\r\\n                               1 1                                    1\\r\\n                   α10 (x) =    + sin 4π(x + y),       ρ0 (x) = 1 +     sin 2π(x + y),   u0 (x) = v0 (x) = 1,   p0 (x) = 1,\\r\\n                               2 4                                    2\\r\\n\\r\\nalong with the EOS parameters Cv1 = Cv2 = 1, γ1 = 1.4, p∞1 = 0, γ2 = 3, p∞2 = 2. In this case, the density and EOS parameters\\r\\nare purely convected in uniform velocity and pressure fields. Norms on the mixture density error, eh = ρh − ρ, under h- and\\r\\np-refinements are displayed in Table 1 with either CP fluxes, or EC fluxes in the volume integral. We observe that as the mesh is\\r\\nrefined the expected p + 1 order of convergence is recovered by both schemes, but lower error levels are obtained with the CP flux.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                    (a)                                                          (b)\\r\\n\\r\\nFigure 4: Examples of meshes used for the numerical tests: (a) square mesh h = 1/4 with 16 fourth-order curved quadrangles with representation\\r\\nof the nodes; (b) example of unstructured mesh in the range 2.5 6 x 6 5 with 1580 elements and initial positions of the shock and bubble (see\\r\\nsection 8.3).\\r\\n\\r\\n\\r\\n\\r\\n8.2. Riemann problems\\r\\n    We first consider the advection of an isolated material discontinuity to assess the contact preservation property of the present\\r\\nscheme. The computational domain for this test is chosen to be Ωh = [−0.5, 0.5] with 100 elements. The Riemann initial data are\\r\\n\\r\\n                                                                                  x < 0,\\r\\n                                                           \\uf8f1\\r\\n                                                           \\uf8f2 (0.375, 2, 1, 1),\\r\\n                                          (α1 , ρ, u, p) = \\uf8f4\\r\\n                                                           \\uf8f4\\r\\n                                                           \\uf8f3 (0.146342, 1, 1, 1), x > 0,\\r\\n\\r\\nwith Cv1 = 1, Cv2 = 2, γ1 = 1.4, γ2 = 1.5, p∞1 = p∞2 = 0. Results at time t = 0.2 are displayed in Figure 5. As expected, compu-\\r\\ntations with CP fluxes preserve uniform profiles of pressure and velocity across the material interface, while spurious oscillations\\r\\noccur when using EC fluxes. In both cases, the interface is well captured with low amplitude oscillations in the ρ and Γ profiles.\\r\\n    We now consider a gas-gas shock-interface interaction problem which was originally proposed in [40]. Here a shock wave\\r\\nin helium gas travels at Mach 8.96 and interacts with an helium-air interface. The computational domain Ωh = [−1, 1] with 100\\r\\nelements and the initial condition is as follows:\\r\\n\\r\\n                                                       (0, 0.386, 26.59, 100), x < −0.8,\\r\\n                                                     \\uf8f1\\r\\n                                                     \\uf8f4\\r\\n                                                     \\uf8f4\\r\\n                                                     \\uf8f4\\r\\n                                    (α1 , ρ, u, p) = \\uf8f4                         −0.8 < x < −0.2,\\r\\n                                                     \\uf8f4\\r\\n                                                       (0, 0.1, −0.5, 1),\\r\\n                                                     \\uf8f2\\r\\n                                                                               x > −0.2,\\r\\n                                                     \\uf8f4\\r\\n                                                     \\uf8f4\\r\\n                                                     \\uf8f3 (1, 1.0, −0.5, 1),\\r\\n                                                     \\uf8f4\\r\\n\\r\\n\\r\\nwith Cv1 = 1, Cv2 = 2.5, γ1 = 1.4, γ2 = 5/3, p∞1 = p∞2 = 0. The results are displayed in Figure 6, where the solution shows two\\r\\nshocks, one traveling left and the other traveling right, with a right traveling material interface in between. As a result, the shocks\\r\\nand interface are well captured, while spurious oscillations of small amplitude occur in the pressure and velocity fields.\\r\\n\\r\\n                                                                      24\\r\\n\\x0c\\n\\n                                  p    h         keh kL1 (Ωh )    O1     keh kL2 (Ωh )    O2     keh kL∞ (Ωh )   O∞\\r\\n                                       1/8       3.76E-01          –     4.16E-01         –      6.25E-01          –\\r\\n                                       1/16      1.82E-01        1.05    2.04E-01        1.03    3.69E-01        0.76\\r\\n                                  1\\r\\n                                       1/32      5.47E-02        1.73    6.11E-02        1.74    1.08E-01        1.77\\r\\n                                       1/64      1.43E-02        1.93    1.59E-02        1.94    2.56E-02        2.08\\r\\n                                       1/8       1.17E-02          –     1.41E-02         –      3.32E-02          –\\r\\n                                       1/16      1.06E-03        3.46    1.23E-03        3.52    3.64E-03        3.19\\r\\n                          CP      2\\r\\n                                       1/32      9.64E-05        3.45    1.17E-04        3.40    4.82E-04        2.90\\r\\n                                       1/64      1.01E-05        3.25    1.30E-05        3.16    6.50E-05        2.90\\r\\n                                       1/8       3.51E-04          –     4.68E-04         –      3.18E-03          –\\r\\n                                       1/16      1.73E-05        4.35    2.65E-05        4.14    1.98E-04        4.01\\r\\n                                  3\\r\\n                                       1/32      1.05E-06        4.04    1.66E-06        4.00    1.20E-05        4.04\\r\\n                                       1/64      6.73E-08        3.96    1.05E-07        3.98    7.54E-07        3.99\\r\\n                                       1/8       3.72E-01          –     4.11E-01         –      6.24E-01          –\\r\\n                                       1/16      1.78E-01        1.06    2.05E-01        1.01    4.29E-01        0.54\\r\\n                                  1\\r\\n                                       1/32      5.62E-02        1.66    7.14E-02        1.52    2.12E-01        1.02\\r\\n                                       1/64      1.52E-02        1.89    1.99E-02        1.84    6.81E-02        1.64\\r\\n                                       1/8       2.78E-02          –     3.48E-02         –      9.99E-02          –\\r\\n                                       1/16      3.95E-03        2.81    5.23E-03        2.73    1.76E-02        2.50\\r\\n                          EC      2\\r\\n                                       1/32      3.56E-04        3.47    4.55E-04        3.52    1.56E-03        3.50\\r\\n                                       1/64      2.91E-05        3.61    3.77E-05        3.59    1.41E-04        3.46\\r\\n                                       1/8       4.58E-03          –     5.78E-03         –      1.62E-02          –\\r\\n                                       1/16      2.35E-04        4.28    3.10E-04        4.22    1.10E-03        3.88\\r\\n                                  3\\r\\n                                       1/32      8.53E-06        4.78    1.21E-05        4.68    6.62E-05        4.06\\r\\n                                       1/64      2.91E-07        4.87    4.63E-07        4.71    4.01E-06        4.05\\r\\n\\r\\nTable 1: Advection of a density wave: results using either CP (top) numerical fluxes (46), or EC (bottom) numerical fluxes (48) in the volume\\r\\nintegral. Norms of the error on density under p- and h-refinements and associated orders of convergence at time t = 2.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 5: Advection of an isolated material interface: fourth-order accurate simulations obtained on a mesh with 100 elements, and using either\\r\\nCP (top), or EC (bottom) fluxes in the volume integral. Approximate results (symbols) are shown at t = 0.2 and are compared to the exact solution\\r\\n(lines).\\r\\n\\r\\n\\r\\n                                                                        25\\r\\n\\x0c\\n\\nFigure 6: Shock-material interface interaction: fourth-order accurate, p = 3, simulations obtained on a mesh with 100 elements, and using either\\r\\nCP (top), or EC (bottom) fluxes in the volume integral. Approximate results (symbols) are shown at t = 0.07 and are compared to the exact solution\\r\\n(lines).\\r\\n\\r\\n\\r\\n\\r\\n     The last test case concerns a gas-water shock-interface interaction problem and simulates an underwater explosion, where the\\r\\ninitial condition consists of a material interface separating highly compressed air to the left and water at atmospheric pressure to\\r\\nthe right. The computational domain is Ωh = [−5, 5] with 100 elements, and the initial data are given as\\r\\n\\r\\n                                                                                         x < 0,\\r\\n                                                         \\uf8f1\\r\\n                                                         \\uf8f2 (1, 1.241, 0, 2.753),\\r\\n                                        (α1 , ρ, u, p) = \\uf8f4\\r\\n                                                         \\uf8f4\\r\\n                                                         \\uf8f3 (0, 0.991, 0, 3.059 × 10−4 ), x > 0,\\r\\n\\r\\nwith Cv1 = 1.2, Cv2 = 0.073037, γ1 = 1.4, γ2 = 5.5, p∞1 = 0, and p∞2 = 1.505. The results, in Figure 7, show a right traveling\\r\\nshock, a right traveling contact wave, a right advected material interface and a left rarefaction wave. We observe small oscillations\\r\\non the velocity and pressure profiles even though the shock is of large amplitude, and the shock and material interface are well\\r\\ncaptured.\\r\\n\\r\\n\\r\\n8.3. Shock wave-helium bubble interaction\\r\\n    We now consider the interaction of a shock with a helium bubble, which was experimentally investigated in [27] and used to\\r\\nassess numerical schemes for multiphase and multicomponent flows [14, 26, 32, 35, 36, 49, 44]. The test involves a stationary\\r\\nhelium bubble (γ1 = 1.648 and Cv1 = 6.0598) which is surrounded by air (γ2 = 1.4 and Cv2 = 1.7857) and interacts with a left\\r\\nmoving Mach 1.22 shock. The computational domain Ωh = [0.0, 6.5] × [0, 1.78] is discretized with an unstructured mesh with\\r\\n433016 elements (see Figure 4(b)). The helium bubble of unit diameter is centered at x = 3.5 and y = 0.89 and the left traveling\\r\\nshock is located at x = 4. Periodic boundary conditions are imposed on the top and bottom boundaries, while non-reflective\\r\\nconditions are applied at the left and right boundaries. The initial data are made nondimensional with the initial bubble diameter,\\r\\ndensity, temperature and sound speed of air in the pre-shock region.\\r\\n     We first test the ability of both schemes to preserve material interfaces and consider the advection of the bubble only on a mesh\\r\\nwith 64 × 64 fourth-order curved elements (see Figure 4(a)). We thus remove the shock wave, impose a uniform velocity field\\r\\nv0 (x) = (1, 0)> and reduce the size of the bubble which is now initially centered at (0.5, 0.5). Figure 8 shows the bubble at time\\r\\nt = 76.19µs corresponding to a transport of the bubble over a unit distance. The scheme with the CP fluxes captures the interface\\r\\nsharply and preserves the uniform velocity and pressure profiles across the interface, while spurious oscillations are observed with\\r\\nthe EC flux.\\r\\n     We now consider the shock-bubble interaction. Figure 9 shows the deformation of the bubble at several physical times as\\r\\nthe left traveling shock passes through it and represents contours of void fraction of the helium bubble, α1 , and mixture pressure,\\r\\n\\r\\n                                                                       26\\r\\n\\x0c\\n\\nFigure 7: Gas-water shock-interface interaction problem: fourth-order accurate, p = 3, simulations obtained on a mesh with 100 elements, and\\r\\nusing either CP (top), or EC (bottom) fluxes in the volume integral. Approximate results (symbols) are shown at t = 1 and are compared to the\\r\\nexact solution (lines).\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                    27\\r\\n\\x0c\\n\\n                                                  (a) t = 0                      (b) t = 76.19µs, y = 0.5\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            (c) t = 76.19µs, CP                    (d) t = 76.19µs, EC\\r\\n\\r\\nFigure 8: Advection of a helium bubble in air: (a) initial condition, and results obtained with fourth-order accuracy (p = 3) in space with either\\r\\nCP fluxes, or EC fluxes: (b) absolute error levels (logarithmic scale) on the pressure and velocity distributions along y = 0.5 obtained with either\\r\\nCP (black symbols), or EC (red symbols) fluctuations; (c,d) numerical Schlieren φ = exp(|∇ρ|/|∇ρ|max ).\\r\\n\\r\\n\\r\\n\\r\\np, together with numerical Schlieren. We observe that the scheme with CP fluxes allows a better and sharper resolution of the\\r\\nbubble interface for all physical times and is able to accurately capture the shock and bubble dynamics. The bubble interface\\r\\ndevelops vortices after interacting with shock due to a Kelvin-Helmholtz instability. Once again, results with the EC fluxes show\\r\\nsome spurious oscillations at the material interface before and after the interaction in contrast to CP fluxes, while both CP and EC\\r\\nschemes show good resolutions of the shock.\\r\\n\\r\\n\\r\\n8.4. Strong shock wave-hydrogen bubble interaction\\r\\n     We finally consider an interaction problem of a strong M = 2 shock in air with a hydrogen bubble that has been numerically\\r\\ninvestigated in [4, 54]. Compared to section 8.3, these conditions result in faster shock and bubble dynamics. The computational\\r\\ndomain for this test is Ωh = [0, 22.5] × [0, 7.5] and is discretized with an unstructured mesh with 154622 elements. The hydrogen\\r\\nbubble (γ1 = 1.41 and Cv1 = 7.424) is initially centered at x = 4 and y = 0, and the right traveling shock located at x = 7 in air\\r\\n(γ2 = 1.353 and Cv2 = 0.523). The initial data is made nondimensional with the pre-shock density, velocity and temperature of the\\r\\nair and a length scale of 1mm. We impose symmetry conditions at the top and bottom boundaries, along with supersonic inflow\\r\\ncondition at the left boundary and nonreflecting conditions at the right boundary.\\r\\n    Figure 10 shows the deformation of the bubble as the shock passes through it, where we plot contours of the void fraction\\r\\nof the hydrogen bubble, α1 , mixture pressure pressure, p, and the numerical Schlieren. Here, we once again observe that the\\r\\nnumerical scheme is able to resolve the bubble interface well along with the shock. The oscillations at the interface are due to the\\r\\nKelvin-Helmholtz instability and they were also observed in [4]. Using CP fluxes maintains sharp resolution of the interface while\\r\\nalso proving to well capture shocks.\\r\\n\\r\\n\\r\\n9. Concluding remarks\\r\\n\\r\\n   In this work, we propose a high-order, robust and entropy stable discretization of the nonconservative multicomponent SG-\\r\\ngamma model [53]. The space discretization of this system relies on the DGSEM framework [47, 14] based on the modification of\\r\\n\\r\\n                                                                        28\\r\\n\\x0c\\n\\nt = 32µs\\r\\nt = 102µs\\r\\nt = 427µs\\r\\nt = 674µs\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                      CP fluxes                            EC fluxes                           CP fluxes                            EC fluxes\\r\\n\\r\\n        Figure 9: Interaction of a M = 1.22 shock in air with a helium bubble: fourth order accurate in space, p = 3, numerical simulations obtained at\\r\\n        different times using either CP, or EC fluxes in the volume integral, on an unstructured mesh with 433016 elements. For each snapshot the left\\r\\n        image shows the helium void fraction contours (color levels) and the pressure contours (lines), while the right image shows the numerical Schlieren\\r\\n        of the density φ = exp |∇ρ|/|∇ρmax |.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                               29\\r\\n\\x0c\\n\\nt = 1.5µs\\r\\nt = 2.5µs\\r\\nt = 4.0µs\\r\\nt = 8.8µs\\r\\nt = 13.6µs\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                30\\r\\n                       CP fluxes                           EC fluxes                            CP fluxes                            EC fluxes\\r\\n\\r\\n         Figure 10: Interaction of a M = 2 shock in air with a Hydrogen bubble: fourth-order accurate in space, p = 3, numerical solutions obtained at\\r\\n         different times using either CP, or EC numerical fluxes in the volume integral, on an unstructured mesh with 154622 elements. For each snapshot\\r\\n         the left image shows the helium void fraction contours (color levels) and the pressure contours (lines), while the right image shows the numerical\\r\\n         Schlieren of the density φ = exp |∇ρ|/|∇ρmax |.\\r\\n\\x0c\\n\\nthe integral over discretization elements where we replace the physical fluxes and nonconservative products by two-point numerical\\r\\nfluctuations. We first extend this framework to multidimensional unstructured grids with curved elements and derive conditions\\r\\nunder which the semi-discrete scheme is high-order accurate, free-stream preserving and entropy stable. We then design a robust,\\r\\nentropy stable, material interface preserving, and maximum principles preserving HLLC solver for the SG-gamma model that does\\r\\nnot require a root-finding algorithm to evaluate the nonconservative product. The HLLC solver is then used as interface fluctuations\\r\\nin the DGSEM scheme, while we consider either EC, or CP fluctuations in the integrals over discretization elements. The DGSEM\\r\\nscheme is shown to either satisfy a semi-discrete entropy inequality with EC fluctuations (when excluding material interfaces), or\\r\\nto preserve material interfaces with CP fluctuations.\\r\\n    We then analyze the fully discrete DGSEM scheme with a forward Euler time discretization. We derive conditions on the\\r\\ntime step to guaranty that the cell-averaged solution remains in the set of states and satisfies minimum principles on entropy\\r\\nand maximum principles on EOS parameters with either EC, or CP fluctuations. We use a posteriori scaling limiters [65, 66] to\\r\\nextend these properties to all DOFs within elements, while a strong-stability preserving Runge-Kutta scheme [52] is used for the\\r\\nhigh-order time integration and keeps properties of the first-order in time scheme.\\r\\n    High-order accurate numerical simulations of flows in one and two space dimensions with discontinuous solutions and complex\\r\\nwave interactions confirm robustness, stability and accuracy of the present scheme with either EC, or CP fluctuations, and the\\r\\nscheme with CP fluxes present better resolution capabilities.\\r\\n\\r\\n\\r\\nReferences\\r\\n [1] R. Abgrall, How to prevent pressure oscillations in multicomponent flow calculations: a quasi conservative approach, J. Comput. Phys., 125\\r\\n     (1996), pp. 150–160.\\r\\n [2] P. Batten, N. Clarke, C. Lambert, and D. M. Causon, On the choice of wavespeeds for the HLLC Riemann solver, J. Sci. Comput., 18 (1997),\\r\\n     pp. 1553–1570.\\r\\n [3] C. Berthon, B. Dubroca, and A. Sangam, A local entropy minimum principle for deriving entropy preserving schemes, SIAM J. Numer.\\r\\n     Anal., 50 (2012), pp. 468–491.\\r\\n [4] G. Billet, V. Giovangigli, and G. De Gassowski, Impact of volume viscosity on a shock–hydrogen-bubble interaction, Combust. Theory\\r\\n     Model., 12 (2008), pp. 221–248.\\r\\n [5] F. Bouchut, Nonlinear stability of finite Volume Methods for hyperbolic conservation laws: And Well-Balanced schemes for sources, Springer\\r\\n     Science & Business Media, 2004.\\r\\n [6] V. Carlier and F. Renac, Invariant domain preserving high-order spectral discontinuous approximations of hyperbolic systems,\\r\\n     arXiv:2203.05452 [math.NA], (2022).\\r\\n [7] M. H. Carpenter, T. C. Fisher, E. J. Nielsen, and S. H. Frankel, Entropy stable spectral collocation schemes for the Navier–Stokes equations:\\r\\n     Discontinuous interfaces, SIAM J. Sci. Comput., 36 (2014), pp. B835–B867.\\r\\n [8] M. J. Castro, T. M. de Luna, and C. Parés, Well-balanced schemes and path-conservative numerical methods, in Handbook of Numer. Anal.,\\r\\n     vol. 18, Elsevier, 2017, pp. 131–175.\\r\\n [9] M. J. Castro, U. S. Fjordholm, S. Mishra, and C. Parés, Entropy conservative and entropy stable schemes for nonconservative hyperbolic\\r\\n     systems, SIAM J. Numer. Anal., 51 (2013), pp. 1371–1391.\\r\\n[10] M. J. Castro, J. Gallardo, and C. Parés, High order finite volume schemes based on reconstruction of states for solving hyperbolic systems\\r\\n     with nonconservative products. applications to shallow-water systems, Math. Comput., 75 (2006), pp. 1103–1134.\\r\\n[11] Castro Dı́az, Manuel Jesús, Fernández-Nieto, Enrique Domingo, Morales de Luna, Tomás, Narbona-Reina, Gladys, and Parés, Carlos, A\\r\\n     hllc scheme for nonconservative hyperbolic problems. application to turbidity currents with sediment transport, ESAIM: M2AN, 47 (2013),\\r\\n     pp. 1–32.\\r\\n[12] T. Chen and C.-W. Shu, Entropy stable high order discontinuous Galerkin methods with suitable quadrature rules for hyperbolic conservation\\r\\n     laws, J. Comput. Phys., 345 (2017), pp. 427–461.\\r\\n[13] J. Cheng, F. Zhang, and T. Liu, A discontinuous Galerkin method for the simulation of compressible gas-gas and gas-water two-medium\\r\\n     flows, J. Computat. Phys., 403 (2020), p. 109059.\\r\\n[14] F. Coquel, C. Marmignon, P. Rai, and F. Renac, An entropy stable high-order discontinuous Galerkin spectral element method for the Baer-\\r\\n     Nunziato two-phase flow model, J. Comput. Phys., 431 (2021), p. 110135.\\r\\n[15] V. Coralic and T. Colonius, Finite-volume WENO scheme for viscous compressible multicomponent flows, J. Computat. Phys., 274 (2014),\\r\\n     pp. 95–121.\\r\\n[16] M. T. H. de Frahan, S. Varadan, and E. Johnsen, A new limiting procedure for discontinuous Galerkin methods applied to compressible\\r\\n     multiphase flows with shocks and interfaces, J. Comput. Phys., 280 (2015), pp. 489–509.\\r\\n[17] D. Derigs, A. R. Winters, G. J. Gassner, and S. Walch, A novel averaging technique for discrete entropy-stable dissipation operators for\\r\\n     ideal MHD, J. Comput. Phys., 330 (2017), pp. 624–632.\\r\\n[18] B. Despres, Entropy inequality for high order discontinuous Galerkin approximation of Euler equations, in Hyperbolic Problems: Theory,\\r\\n     Numerics, Applications, M. Fey and R. Jeltsch, eds., Basel, 1999, Birkhäuser Basel, pp. 225–231.\\r\\n[19] M. Dumbser and D. S. Balsara, A new efficient formulation of the hllem riemann solver for general conservative and non-conservative\\r\\n     hyperbolic systems, J. Comput. Phys., 304 (2016), pp. 275–319.\\r\\n[20] B. Einfeldt, C.-D. Munz, P. L. Roe, and B. Sjögreen, On godunov-type methods near low densities, J. Comput. Phys., 92 (1991), pp. 273–295.\\r\\n[21] T. C. Fisher and M. H. Carpenter, High-order entropy stable finite difference schemes for nonlinear conservation laws: Finite domains, J.\\r\\n     Comput. Phys., 252 (2013), pp. 518–557.\\r\\n\\r\\n                                                                       31\\r\\n\\x0c\\n\\n[22] U. S. Fjordholm, S. Mishra, and E. Tadmor, Arbitrarily high-order accurate entropy stable essentially nonoscillatory schemes for systems of\\r\\n     conservation laws, SIAM J. Numer. Anal., 50 (2012), pp. 544–573.\\r\\n[23] E. Franquet and V. Perrier, Runge–Kutta discontinuous Galerkin method for the approximation of Baer and Nunziato type multiphase\\r\\n     models, J. Comput. Phys., 231 (2012), pp. 4096–4141.\\r\\n[24] G. J. Gassner, A skew-symmetric discontinuous Galerkin spectral element discretization and its relation to SBP-SAT finite difference methods,\\r\\n     SIAM J. Sci. Comput., 35 (2013), pp. A1233–A1253.\\r\\n[25] G. J. Gassner, A. R. Winters, and D. A. Kopriva, Split form nodal discontinuous Galerkin schemes with summation-by-parts property for\\r\\n     the compressible Euler equations, J. Comput. Phys., 327 (2016), pp. 39–66.\\r\\n[26] J. Giordano and Y. Burtschell, Richtmyer-Meshkov instability induced by shock-bubble interaction: Numerical and analytical studies with\\r\\n     experimental validation, Phys. Fluids, 18 (2006), p. 036102.\\r\\n[27] J. F. Haas and B. Sturtevant, Interaction of weak shock waves with cylindrical and spherical gas inhomogeneities, J. Fluid Mech., 181\\r\\n     (1987), pp. 41–76.\\r\\n[28] A. Harten, P. D. Lax, and B. V. Leer, On upstream differencing and Godunov-type schemes for hyperbolic conservation laws, SIAM review,\\r\\n     25 (1983), pp. 35–61.\\r\\n[29] Helluy, Philippe and Seguin, Nicolas, Relaxation models of phase transition flows, ESAIM: M2AN, 40 (2006), pp. 331–352.\\r\\n[30] A. Hiltebrand and S. Mishra, Entropy stable shock capturing space–time discontinuous Galerkin schemes for systems of conservation laws,\\r\\n     Numer. Math., 126 (2014), pp. 103–151.\\r\\n[31] A. Hiltebrand, S. Mishra, and C. Parés, Entropy-stable space–time DG schemes for non-conservative hyperbolic systems, ESAIM: M2AN,\\r\\n     52 (2018), pp. 995–1022.\\r\\n[32] X. Y. Hu, B. Khoo, N. A. Adams, and F. L. Huang, A conservative interface method for compressible flows, J. Comput. Phys., 219 (2006),\\r\\n     pp. 553–578.\\r\\n[33] F. Ismail and P. L. Roe, Affordable, entropy-consistent Euler flux functions ii: Entropy production at shocks, J. Comput. Phys., 228 (2009),\\r\\n     pp. 5410–5436.\\r\\n[34] G. S. Jiang and C.-W. Shu, On a cell entropy inequality for discontinuous Galerkin methods, Math. Comput., 62 (1994), pp. 531–538.\\r\\n[35] E. Johnsen and T. Colonius, Implementation of WENO schemes in compressible multicomponent flow problems, J. Comput. Phys., 219 (2006),\\r\\n     pp. 715–732.\\r\\n[36] S. Kawai and H. Terashima, A high-resolution scheme for compressible multicomponent flows with shock waves, Int. J. Numer. Methods.\\r\\n     Fluids, 66 (2011), pp. 1207–1225.\\r\\n[37] C. A. Kennedy and A. Gruber, Reduced aliasing formulations of the convective terms within the navier–stokes equations for a compressible\\r\\n     fluid, J. Comput. Phys., 227 (2008), pp. 1676–1700.\\r\\n[38] D. A. Kopriva, Metric identities and the discontinuous spectral element method on curvilinear meshes., J. Sci. Comput., 26 (2006), pp. 302–\\r\\n     327.\\r\\n[39] D. A. Kopriva and G. Gassner, On the quadrature and weak form choices in collocation type discontinuous Galerkin spectral element\\r\\n     methods, J. Sci. Comput., 44 (2010), pp. 136–155.\\r\\n[40] T. G. Liu, B. C. Khoo, and K. S. Yeo, Ghost fluid method for strong shock impacting on material interface, J. Comput. Phys., 190 (2003),\\r\\n     pp. 651–681.\\r\\n[41] C. Marmignon, F. Naddei, and F. Renac, Energy relaxation approximation for the compressible multicomponent flows in thermal nonequilib-\\r\\n     rium, arXiv:2103.03731 [math.NA], (2021).\\r\\n[42] R. Menikoff and B. J. Plohr, The riemann problem for fluid flow of real materials, Rev. Mod. Phys., 61 (1989), pp. 75–130.\\r\\n[43] C. Parés, Numerical methods for nonconservative hyperbolic systems: a theoretical framework., SIAM J. Numer. Anal., 44 (2006), pp. 300–\\r\\n     321.\\r\\n[44] J. J. Quirk and S. Karni, On the dynamics of a shock–bubble interaction, J. Fluid Mech., 318 (1996), pp. 129–163.\\r\\n[45] H. Ranocha, Comparison of some entropy conservative numerical fluxes for the Euler equations, J. Sci. Comput., 76 (2018), pp. 216–242.\\r\\n[46] F. Renac, A robust high-order discontinuous Galerkin method with large time steps for the compressible Euler equations, Commun. Math.\\r\\n     Sci., 15 (2017), pp. 813–837.\\r\\n[47]          , Entropy stable DGSEM for nonlinear hyperbolic systems in nonconservative form with application to two-phase flows, J. Comput.\\r\\n     Phys., 382 (2019), pp. 1–26.\\r\\n[48]          , Entropy stable, positive DGSEM with sharp resolution of material interfaces for a 4 × 4 two-phase flow system: a legacy from\\r\\n     three-point schemes, arXiv preprint arXiv:2001.05710, (2019).\\r\\n[49]          , Entropy stable, robust and high-order DGSEM for the compressible multicomponent Euler equations, J. Comput. Phys., (2021),\\r\\n     p. 110584.\\r\\n[50] F. Renac, M. de la Llave Plata, E. Martin, J. B. Chapelier, and V. Couaillier, Aghora: A High-Order DG Solver for Turbulent Flow\\r\\n     Simulations, Springer International Publishing, Cham, 2015, pp. 315–335.\\r\\n[51] R. Saurel and C. Pantano, Diffuse-interface capturing methods for compressible two-phase flows, Annu. Rev. Fluid Mech., 50 (2018),\\r\\n     pp. 105–130.\\r\\n[52] C.-W. Shu and S. Osher, Efficient implementation of essentially non-oscillatory shock-capturing schemes, J. Comput. Phys., 77 (1988),\\r\\n     pp. 439–471.\\r\\n[53] K.-M. Shyue, An efficient shock-capturing algorithm for compressible multicomponent problems, J. Comput. Phys., 142 (1998), pp. 208–242.\\r\\n[54] B. Sjögreen and H. C. Yee, Grid convergence of high order methods for multiscale complex unsteady viscous compressible flows, J. Comput.\\r\\n     Phys., 185 (2003), pp. 1–26.\\r\\n[55] E. Tadmor, The numerical viscosity of entropy stable schemes for systems of conservation laws. I, Math. Comput., 49 (1987), pp. 91–103.\\r\\n[56] S. A. Tokareva and E. F. Toro, HLLC-type Riemann solver for the Baer–Nunziato equations of compressible two-phase flow, J. Comput.\\r\\n     Phys., 229 (2010), pp. 3573–3604.\\r\\n[57] E. Toro, L. Müller, and A. Siviglia, Bounds for wave speeds in the Riemann problem: Direct theoretical estimates, Comput. Fluids, 209\\r\\n     (2020), p. 104640.\\r\\n\\r\\n                                                                       32\\r\\n\\x0c\\n\\n[58] E. F. Toro, Riemann-problem-based techniques for computing reactive two-phased flows, in Numer. Combustion, Springer, 1989, pp. 472–\\r\\n     481.\\r\\n[59]        , Riemann solvers and numerical methods for fluid dynamics: a practical introduction, Springer Science & Business Media, 2013.\\r\\n[60] E. F. Toro, M. Spruce, and W. Speares, Restoration of the contact surface in the hll-riemann solver, Shock waves, 4 (1994), pp. 25–34.\\r\\n[61] A. Volpert, The spaces BV and quasilinear equations, Math. USSR Sbornik, 115 (1967), pp. 255–302.\\r\\n[62] C. Wang, X. Zhang, C.-W. Shu, and J. Ning, Robust high order discontinuous Galerkin schemes for two-dimensional gaseous detonations, J.\\r\\n     Comput. Phys., 231 (2012), pp. 653–665.\\r\\n[63] M. Waruszewski, J. E. Kozdon, L. C. Wilcox, T. H. Gibson, and F. X. Giraldo, Entropy stable discontinuous galerkin methods for balance\\r\\n     laws in non-conservative form: Applications to euler with gravity, 2021.\\r\\n[64] N. Wintermeyer, A. R. Winters, G. J. Gassner, and D. A. Kopriva, An entropy stable nodal discontinuous Galerkin method for the two\\r\\n     dimensional shallow water equations on unstructured curvilinear meshes with discontinuous bathymetry, J. Comput. Phys., 340 (2017),\\r\\n     pp. 200–242.\\r\\n[65] X. Zhang and C. Shu, On positivity-preserving high order discontinuous Galerkin schemes for compressible Euler equations on rectangular\\r\\n     meshes, J. Comput. Phys., 229 (2010), pp. 8918–8934.\\r\\n[66] X. Zhang and C.-W. Shu, On maximum-principle-satisfying high order schemes for scalar conservation laws, J. Comput. Phys., 229 (2010),\\r\\n     pp. 3091–3120.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                    33\\r\\n\\x0c',\n",
       " '              Masked Discrimination for\\r\\n      Self-Supervised Learning on Point Clouds\\r\\n\\r\\n                       Haotian Liu      Mu Cai      Yong Jae Lee\\r\\n\\r\\n                           University of Wisconsin–Madison\\r\\n                        {lht,mucai,yongjaelee}@cs.wisc.edu\\r\\n\\r\\n        Abstract. Masked autoencoding has achieved great success for self-\\r\\n        supervised learning in the image and language domains. However, mask\\r\\n        based pretraining has yet to show benefits for point cloud understanding,\\r\\n        likely due to standard backbones like PointNet being unable to properly\\r\\n        handle the training versus testing distribution mismatch introduced by\\r\\n        masking during training. In this paper, we bridge this gap by proposing\\r\\n        a discriminative mask pretraining Transformer framework, MaskPoint,\\r\\n        for point clouds. Our key idea is to represent the point cloud as discrete\\r\\n        occupancy values (1 if part of the point cloud; 0 if not), and perform sim-\\r\\n        ple binary classification between masked object points and sampled noise\\r\\n        points as the proxy task. In this way, our approach is robust to the point\\r\\n        sampling variance in point clouds, and facilitates learning rich represen-\\r\\n        tations. We evaluate our pretrained models across several downstream\\r\\n        tasks, including 3D shape classification, segmentation, and real-word ob-\\r\\n        ject detection, and demonstrate state-of-the-art results while achieving\\r\\n        a significant pretraining speedup (e.g., 4.1× on ScanNet) compared to\\r\\n        the prior state-of-the-art Transformer baseline.1\\r\\n\\r\\n\\r\\n1     Introduction\\r\\nLearning rich feature representations without human supervision, also known\\r\\nas self-supervised learning, has made tremendous strides in recent years. We\\r\\nnow have methods in NLP [41,12,40] and computer vision [21,5,20,8,2] that can\\r\\nproduce stronger features than those learned on labeled datasets.\\r\\n    In particular, masked autoencoding, whose task is to reconstruct the masked\\r\\ndata from the unmasked input (e.g., predicting the masked word in a sentence\\r\\nor masked patch in an image, based on surrounding unmasked context) is the\\r\\ndominant self-supervised learning approach for text understanding [12,26,27,63]\\r\\nand has recently shown great promise in image understanding [2,20] as well.\\r\\nCuriously, for point cloud data, masked autoencoding has not yet been able\\r\\nto produce convincing results [53,65,62]. Self-supervised learning would be ex-\\r\\ntremely beneficial for point cloud data, as obtaining high-quality annotations\\r\\nis both hard and expensive, especially for real-world scans. At the same time,\\r\\nmasked autoencoding should also be a good fit for point cloud data, since each\\r\\npoint (or group of points) can easily be masked or unmasked.\\r\\n    We hypothesize that the primary reason why masked autoencoding has thus\\r\\nfar not worked well for point cloud data is because standard point cloud back-\\r\\n1\\r\\n    Code will be publicly available at https://github.com/haotian-liu/MaskPoint.\\r\\n\\x0c\\n\\n2       Liu et al.\\r\\n\\r\\n                            Unmasked Points                  real   fake\\r\\n                                              Transformer\\r\\n                                                encoder     Transformer\\r\\n                                                              decoder\\r\\n\\r\\n\\r\\n\\r\\n                                                                    noise\\r\\n                             Masked Points\\r\\n\\r\\nFig. 1: Main Idea. We randomly partition the point cloud into masked and\\r\\nunmasked sets. We only feed the visible portion of the point cloud into the\\r\\nencoder. Then, a set of real query points are sampled from the masked points,\\r\\nand a set of fake query points are randomly sampled from 3D space. We train\\r\\nthe decoder so that it distinguishes between the real and fake points. After pre-\\r\\ntraining, we discard the decoder and use the encoder for downstream tasks.\\r\\n\\r\\n\\r\\nbones are unable to properly handle the distribution mismatch between train-\\r\\ning and testing data introduced by masking. Specifically, PointNet type back-\\r\\nbones [39,37,38] leverage local aggregation layers that operate over local neigh-\\r\\nborhoods (e.g., k-nearest neighbors) of each point. The extent of the local neigh-\\r\\nborhoods can change drastically with the introduction of masking, creating a\\r\\ndiscrepancy between the distribution of local neighborhoods seen on masked\\r\\ntraining scenes versus unmasked test scenes.\\r\\n    Transformers [52], on the other hand, can perform self-attention (a form of\\r\\naggregation) on either all or selective portions of the input data. This means\\r\\nthat it has the ability to only process the unmasked portions of the scene in\\r\\nthe training data, without being impacted by the masked scene portions. This\\r\\nproperty suggests that Transformers could be an ideal backbone choice for self-\\r\\nsupervised masked autoencoding for point clouds.\\r\\n    For image understanding, the state-of-the-art masked autoencoding Trans-\\r\\nformer approach MAE [20] masks out a large random subset of image patches,\\r\\napplies the Transformer encoder to the unmasked patches, and trains a small\\r\\nTransformer decoder that takes in the positional encodings of the masked patches\\r\\nto reconstruct their original pixel values. However, this approach cannot be di-\\r\\nrectly applied to point cloud data, because the raw representation of each 3D\\r\\npoint is its spatial xyz location. Thus, training the decoder to predict the xyz co-\\r\\nordinates of a masked point would be trivial, since its positional encoding would\\r\\nleak the correct answer. In this case, the network would simply take a shortcut\\r\\nand not learn meaningful features.\\r\\n    To address this, we propose a simple binary point classification objective as a\\r\\nnew pretext task for point cloud masked autoencoding. We first group points into\\r\\nlocal neighborhoods, and then mask out a large random subset of those groups.\\r\\nThe Transformer encoder takes in the unmasked point groups, and encodes each\\r\\ngroup through self-attention with the other groups. The Transformer decoder\\r\\ntakes in a set of real query points and fake query points, where the real queries\\r\\nare sampled from the masked points, while the fake queries are randomly sampled\\r\\nfrom the full 3D space. We then perform cross attention between the decoder\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds       3\\r\\n\\r\\nqueries and encoder outputs. Finally, we apply a binary classification head to\\r\\nthe decoder’s outputs and require it to distinguish between the real and fake\\r\\nqueries. We find this design to be simple yet effective, as it creates a difficult\\r\\nand meaningful pretext task that requires the network to deduce the shape of\\r\\nthe object from only a small amount of visible point groups.\\r\\n    Importantly, we find that a much higher masking ratio (e.g., 90%) is required\\r\\nfor point cloud data compared to the image domain (75% in [20]). We identify\\r\\ntwo possible reasons. First, the additional depth dimension in point clouds can\\r\\nhelp disambiguate object instances (i.e., make the task easier) – with the same\\r\\nmasking ratio, a 3D patch in point cloud data is less likely to overlap multiple\\r\\nobjects than a 2D patch in images. Second, local point groups often already\\r\\ncontain unique categorical information, e.g. a wheel-shaped local point patch can\\r\\nyield a high probability of a car-shaped object. These two observations demand\\r\\nstronger masking ratios in order for the pretext task to be hard enough to make\\r\\nthe model learn useful feature representations.\\r\\n    Among existing self-supervised point cloud approaches, Point-BERT [65] is\\r\\nthe most related. It trains a discrete Variational AutoEncoder (dVAE) [43] to\\r\\nencode the input point cloud into discrete point token representations, and per-\\r\\nforms BERT-style pretraining over them. To aid training, it uses point patch\\r\\nmixing augmentation, together with an auxiliary MoCo [21] loss. However, the\\r\\ndependency on a pretrained dVAE together with other auxiliary techniques, cre-\\r\\nates a significant computational overhead in pretraining – our experiments show\\r\\nthat its pre-training is significantly slower (e.g., 4.1× on ScanNet [9]) than ours\\r\\neven without taking into account the training time of the dVAE module. The\\r\\nlarge speedup is also due to our design of having a high masking rate and the\\r\\nTransformer encoder processing only the unmasked points.\\r\\n    In sum, our main contributions are: (1) A novel masked point classification\\r\\nTransformer, MaskPoint, for self-supervised learning on point clouds. (2) Our\\r\\napproach is simple and effective, achieving state-of-the-art performance on a va-\\r\\nriety of downstream tasks, including object classification on ModelNet40 [58] /\\r\\nScanObjectNN [51], part segmentation on ShapeNetPart [64], object detection\\r\\non ScanNet [9], and few-shot object classification on ModelNet40 [58]. (3) No-\\r\\ntably, for the first time, we show that a standard Transformer architecture can\\r\\noutperform sophisticatedly designed point cloud backbones.\\r\\n\\r\\n2   Related Work\\r\\nTransformers. Transformers were first proposed to model long-term dependen-\\r\\ncies in sequential data [52], and have achieved great success in natural language\\r\\nprocessing [52,12,41]. More recently, they have also shown promising performance\\r\\non various computer vision tasks, including image classification [14,50,46], ob-\\r\\nject detection [3], semantic segmentation [54], image generation [25], and multi-\\r\\nmodal learning [40]. There have also been attempts to adopt transformers to 3D\\r\\npoint cloud data. PCT [19] and Point Transformer [71] propose new attention\\r\\nmechanisms for point cloud feature aggregation. 3DETR [34] uses Transfomer\\r\\nblocks and the parallel decoding strategy from DETR [3] for 3D object detec-\\r\\n\\x0c\\n\\n4      Liu et al.\\r\\n\\r\\ntion. However, it is still hard to get promising performance using the standard\\r\\nTransformer. For example, in 3D object detection, there is a large performance\\r\\ngap between 3DETR [34] and state-of-the-art point based [69] and convolution\\r\\nbased [10] methods. In this paper, we propose a novel masked autoencoding\\r\\nTransformer for self-supervised learning on point clouds.\\r\\nSelf-supervised Learning. Self-supervised learning (SSL) aims to learn mean-\\r\\ningful representations from the data itself, to better serve downstream tasks. Tra-\\r\\nditional methods typically rely on pretext tasks, such as image rotation predic-\\r\\ntion [18], image colorization [67], and solving jigsaw puzzles [35]. Recent methods\\r\\nbased on contrastive learning (e.g., MoCo [21], SimCLR [5], SimSiam [8]) have\\r\\nachieved great success in the image domain, sometimes producing even better\\r\\ndownstream performance compared to supervised pretraining on ImageNet [11].\\r\\n    Self-supervised learning has also begun to be explored for point cloud data.\\r\\nPretext methods include deformation reconstruction [1], geometric structure pre-\\r\\ndiction [47], and orientation estimation [36]. Contrastive learning approaches in-\\r\\nclude PointContrast [60], which learns corresponding points from different cam-\\r\\nera views, and DepthContrast [68], which learns representations by comparing\\r\\ntransformations of a 3D point cloud/voxel. OcCo [53] learns an autoencoder\\r\\nto reconstruct the scene from the occluded input. However, due to the sampling\\r\\nvariance of the underlying 3D shapes, explicitly reconstructing the original point\\r\\ncloud will inevitably capture such variance. In this paper, we explore a simple\\r\\nbut effective discriminative classification pretext task to learn representations\\r\\nthat are robust to the sampling variance.\\r\\nMask based Pretraining. Masking out content has been used in various ways\\r\\nto improve model robustness including as a regularizer [45,17], data augmenta-\\r\\ntion [13,44,72], and self-supervised learning [7,12,20]. For self-supervised learn-\\r\\ning, the key idea is to train the model to predict the masked content based on its\\r\\nsurrounding context. The most successful approaches are built upon the Trans-\\r\\nformer [52], due in part to its token-based representation and ability to model\\r\\nlong-range dependencies.\\r\\n    In masked language modeling, BERT [12] and its variants [26,27] achieve\\r\\nstate-of-the-art performance across nearly all NLP downstream tasks by predict-\\r\\ning masked tokens during pretraining. Masked image modeling works [2,20] adopt\\r\\na similar idea for image pretraining. BEiT [2] maps image patches into discrete\\r\\ntokens, then masks a small portion of patches, and feeds the remaining visible\\r\\npatches into the Transformer to reconstruct the tokens of the masked patches.\\r\\nInstead of reconstructing tokens, the recent Masked AutoEncoder (MAE) [20]\\r\\nreconstructs the masked patches at the pixel level, and with a much higher mask\\r\\nratio of ≥ 70%. Following works try to predict high-level visual features such as\\r\\nHoG [56], or improve the representation capability of the encoder by aligning\\r\\nthe feature from both visible patches and masked patches [7]. To our knowl-\\r\\nedge, the only self-supervised mask modeling Transformer approach for point\\r\\nclouds is Point-BERT [65], which adopts a similar idea as BEiT [2]. However, to\\r\\nobtain satisfactory performance, it requires a pretrained dVAE and other auxil-\\r\\niary techniques (e.g., a momentum encoder [21]), which slow down training. We\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds       5\\r\\n\\r\\npropose a simple and effective masked autoencoding Transformer approach for\\r\\npoint clouds, which largely accelerates training (4.1× faster than Point-BERT)\\r\\nwhile achieving state-of-the-art performance for various downstream tasks.\\r\\n\\r\\n3     Approach\\r\\nThe goal is to learn semantic feature representations without human supervision\\r\\nthat can perform well on downstream point cloud recognition tasks. We motivate\\r\\nour self-supervised learning design with a qualitative example. Fig. 1 “Unmasked\\r\\nPoints” shows a point cloud with a large portion (90%) of its points masked out.\\r\\nStill, based on our prior semantic understanding of the world, we as humans are\\r\\nable to say a number of things about it: (1) it might be an airplane; (2) if so, it\\r\\nshould consist of the head, body, tail, and wing; and even (3) roughly where these\\r\\nparts should be present. In other words, because we already know what airplanes\\r\\nare, we can recover the missing information from the small visible subset of the\\r\\npoint cloud. In a similar way, training a model to recover information about the\\r\\nmasked portion of the point cloud given the visible portion could force the model\\r\\nto learn object semantics.\\r\\n    However, even as humans, it can be difficult or impossible to precisely re-\\r\\nconstruct all missing points, since there are several ambiguous factors; e.g., the\\r\\nprecise thickness of the wings or the precise length of the airplane. If we are\\r\\ninstead given a sampled 3D point in space, and are asked to answer whether it\\r\\nlikely belongs to the object or not, we would be more confident in our answer.\\r\\nThis discriminative point classification task is much less ambiguous than the\\r\\nreconstruction task, yet still requires a deep understanding of object semantics\\r\\nin order to deduce the masked points from the small number of visible points.\\r\\n\\r\\n\\r\\n3.1   Masked Point Discrimination\\r\\n\\r\\nOur approach works as follows. We randomly partition each input point cloud\\r\\nP ∈ RN ×3 into two groups: masked M and unmasked U. We use the Transformer\\r\\nencoder to model the correlation between the sparsely-distributed unmasked\\r\\ntokens U via self-attention. Ideally, the resulting encoded latent representation\\r\\ntokens L should not only model the relationship between the unmasked points\\r\\nU, but also recover the latent distribution of masked points M, so as to perform\\r\\nwell on the pretraining task. We next sample a set of real query points Qreal\\r\\nand a set of fake query points Qf ake . The real query points are sampled from\\r\\nthe masked point set M, while the fake query points are randomly sampled from\\r\\nthe full 3D space. We then perform cross attention between each decoder query\\r\\nq ∈ {Qreal , Qf ake } and the encoder outputs: CA(q, L), to model the relationship\\r\\nbetween the masked query point and the unmasked points. Finally, we apply a\\r\\nbinary classification head to the decoder’s outputs and require it to distinguish\\r\\nbetween the real and fake queries.\\r\\n    We show in our experiments that our approach is both simple and effective,\\r\\nas it creates a pretext task that is difficult and meaningful enough for the model\\r\\nto learn rich semantic point cloud representations.\\r\\n\\x0c\\n\\n6       Liu et al.\\r\\n\\r\\n         Input Points         Grouping & Masking             unmasked\\r\\n                                                             masked\\r\\n                                                                          Point Queries          Discriminative Decoder\\r\\n                                                                                           xyz\\r\\n\\r\\n                            FPS                    Masking                                fake\\r\\n                                                                                          real\\r\\n                                                                                                        transformer\\r\\n                                                                                                          decoder\\r\\n\\r\\n                             Masked Encoder                                     …\\r\\n\\r\\n                                                              transformer                              Downstream Tasks\\r\\n                                  Point Patchification          encoder                           Classification   Few-Shot\\r\\n\\r\\n                        …              PointNet                                 …                 Segmentation     Detection\\r\\n         Visible Point Groups                                  Visible Tokens\\r\\n\\r\\n\\r\\n\\r\\nFig. 2: MaskPoint architecture. We first uniformly sample point groups from\\r\\nthe point cloud, and partition them to masked and unmasked. We patchify the\\r\\nvisible point groups to token embeddings with PointNet and feed these visible\\r\\ntokens into the encoder. Then, a set of real query points are sampled from\\r\\nthe masked points, and a set of fake query points are randomly sampled from\\r\\n3D space. We train the decoder so that it distinguishes between the real and\\r\\nfake points. After pre-training, we discard the decoder and use the encoder for\\r\\ndownstream tasks. See Sec. 3.1 for details.\\r\\n\\r\\n\\r\\nDiscarding Ambiguous Points. Since we sample fake query points uniformly\\r\\nat random over the entire space, there will be some points that fall close to the\\r\\nobject’s surface. Such points can cause training difficulties since their target label\\r\\nis ‘fake’ even though they are on the object. In preliminary experiments, we find\\r\\nthat such ambiguous points can lead to vanishing gradients in the early stages of\\r\\ntraining. Thus, to stabilize training, we simply remove all fake points p̂ ∈ Qf ake\\r\\nwhose euclidean distance is less than γ to any object (masked or unmasked) point\\r\\npi ∈ P: mini ||p̂−pi ||2 < γ. To address the size variance of the input point cloud,\\r\\nγ is dynamically selected per point cloud P: P̂ = FPS(P), γ = minj̸=i ||P̂i −P̂j ||2 .\\r\\n\\r\\n3D Point Patchification. Feeding every single point into the Transformer\\r\\nencoder can yield an unacceptable cost due to the quadratic complexity of self-\\r\\nattention operators. Following [65,14], we adopt a patch embedding strategy that\\r\\nconverts input point clouds into 3D point patches.\\r\\n    Given the input point cloud P ∈ RN ×3 , S points {pi }Si=1 are sampled as\\r\\npatch centers using farthest point sampling [39]. We then gather the k nearest\\r\\nneighbors for each patch center to generate a set of 3D point patches {gi }Si=1 . A\\r\\nPointNet [38] is then applied to encode each 3D point patch gi ∈ Rk×3 to a fea-\\r\\nture embedding fi ∈ Rd . In this way, we obtain S tokens and their corresponding\\r\\nfeatures {fi }Si=1 and center coordinates {pi }Si=1 .\\r\\n    Note that our patchification strategy can generate overlapping patches (e.g.,\\r\\nif two point centers are sampled to be close to each other). Although a prior\\r\\nstudy [59] showed that such overlapping patches can stabilize training in vision\\r\\ntransformers, in our case, they could undesirably leak information that would\\r\\nallow the network to take a shortcut solution; e.g., a portion of a masked patch\\r\\nbeing part of a neighboring unmasked patch. In practice, we set a very high\\r\\n\\x0c\\n\\n                                                                Masked Discrimination for Self-Supervised Learning on Point Clouds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                7\\r\\n\\r\\nmasking ratio (e.g., 90%), and consequently such overlap rarely happens and\\r\\ndoes not hinder training.\\r\\n\\r\\nTransformer Architecture. Our network architecture is shown in Fig. 2. We\\r\\nadopt the standard Transformer encoder [52] as the encoding backbone, where\\r\\neach Transformer encoder block consists of a multi-head self-attention (MSA)\\r\\nlayer and a feed forward network (FFN). As noted in Section 3.1, we construct\\r\\n                                                                            N ×3\\r\\npatch-wise features {fi }M         i=1 from the input point cloud P ∈ R          . Following [65],\\r\\n                                                             M\\r\\nwe apply the MLP positional embedding {posi }i=1 to the patch features {fi }M                 i=1 .\\r\\nThen, the class token E[s], which will be used for downstream classification\\r\\ntasks, is stacked to the top of the patch features {fi }M           i=1 ; i.e., the input to the\\r\\nTransformer encoder is I0 = {E[s], f1 + pos1 , f2 + pos2 , · · · , fM + posM }. Af-\\r\\nter n Transformer blocks, we get the feature embedding for each group In =\\r\\n                              n\\r\\n{En [s], f1n , f2n , · · · , fM }.\\r\\n    During the decoding stage, Nq real query points Qreal and Nq fake query\\r\\npoints Qf ake are sampled. We pass the encoder output In and its positional\\r\\n                                                                                           Q 2N\\r\\nembedding {posi }M           i=1 , Qreal , Qf ake and their positional embedding {posi }i=1\\r\\ninto a one-layer Transformer decoder. Cross attention is only performed between\\r\\nthe queries and encoder keys/values, but not between different queries. Finally,\\r\\nthe decoder output goes through an MLP classification head, which is trained\\r\\nwith the binary focal loss [29], since there can be a large imbalance between\\r\\npositive and negative samples.\\r\\n    For downstream tasks, the point patchification module and Transformer en-\\r\\ncoder will be used with their pretrained weights as initialization.\\r\\n\\r\\nAn Information Theoretic Perspective. Here, we provide an information\\r\\ntheoretic perspective to our self-supervised learning objective, using mutual in-\\r\\nformation. The mutual information between random variables X and Y , I(X; Y ),\\r\\nmeasures the amount of information that can be gained about random variable\\r\\nX from the knowledge about the other random variable Y .\\r\\n    Ideally, we would like the model to learn a rich feature representation of\\r\\nthe point cloud: the latent representation L from our encoder E should contain\\r\\nenough information to recover the original point cloud P, i.e., we would like to\\r\\nmaximize the mutual information I(P; L). However, directly estimating I(P; L)\\r\\nis hard since we need to know the exact probability distribution of P (P|L).\\r\\nFollowing [6], we instead use auxiliary distribution Q to approximate it:\\r\\n\\r\\n      \\\\begin {aligned} I(\\\\mathcal {P}; \\\\mathcal {L}) &=-H(\\\\mathcal {P} | \\\\mathcal {L}) + H(\\\\mathcal {P}) \\\\\\\\ &=\\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P} | \\\\mathcal {L})}[\\\\log P(p^{\\\\prime } | x)]]+H(\\\\mathcal {P}) \\\\\\\\ &=\\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\underbrace {D_{\\\\mathrm {KL}}(P(\\\\cdot | x) \\\\| Q(\\\\cdot | x))}_{\\\\geq 0} + \\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P}|\\\\mathcal {L})}[\\\\log Q(p^{\\\\prime } | x)]]+H(\\\\mathcal {P}) \\\\\\\\ & \\\\geq \\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P}|\\\\mathcal {L})}[\\\\log Q(p^{\\\\prime } | x)]]+H(\\\\mathcal {P}) \\\\end {aligned} \\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (1)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nLemma 3.1. For random variables X, Y and function f (x, y) under suitable\\r\\nregularity conditions: Ex∼X,y∼Y |x [f (x, y)] = Ex∼X,y∼Y |x,x′ ∼X|y [f (x′ , y)].\\r\\n\\x0c\\n\\n8                  Liu et al.\\r\\n\\r\\n    Therefore, we can define a variational lower bound, LI (Q, L), of the mutual\\r\\ninformation, I(P; L):\\r\\n                                        \\\\begin {aligned} L_{I}(Q, \\\\mathcal {L}) &=\\\\mathbb {E}_{p \\\\sim P(\\\\mathcal {P}), x \\\\sim \\\\mathcal {L}}[\\\\log Q(p|x)] + H(\\\\mathcal {P}) \\\\\\\\ &=\\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P}|\\\\mathcal {L})}[\\\\log Q(p^{\\\\prime } | x)]] + H(\\\\mathcal {P}) \\\\\\\\ & \\\\leq I(\\\\mathcal {P}; \\\\mathcal {L}) \\\\end {aligned} \\r\\n                                                                                                                                                                                                                                                                                                                                                                                    (2)\\r\\n\\r\\n\\r\\nTherefore, we have:\\r\\n      \\\\max I(\\\\mathcal {P}; \\\\mathcal {L}) \\\\iff \\\\max L_{I}(Q, \\\\mathcal {L}) \\\\iff \\\\max \\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P}|\\\\mathcal {L})}[\\\\log Q(p^{\\\\prime } | x)]]  (3)\\r\\n    Previous works use the Chamfer distance to approximate such auxiliary func-\\r\\ntion Q, but it has the disadvantage of being sensitive to point sampling variance\\r\\n(discussed in detail in Sec. 3.2). Thus, we instead represent the point cloud distri-\\r\\nbution with occupancy values within the tightest 3D bounding box of the point\\r\\ncloud: B ∈ {x, y, z, o}L , where (x, y, z) ∈ R3 , o ∈ {0, 1}, and L is the number\\r\\nof densely sampled points. We let the output of Q denote the continuous dis-\\r\\ntribution of the occupancy value ô, where Q(·) ∈ [0, 1]. In our implementation,\\r\\nas discussed in Sec. 3.1, we construct a set of real query points and fake query\\r\\npoints, assign them with the corresponding occupancy labels, and optimize the\\r\\nprobability outputs from the model with a binary classification objective.\\r\\n\\r\\n3.2           Why not reconstruction, as in MAE?\\r\\nIn this section, we delve into the details on why a reconstruction objective (i.e.,\\r\\nreconstructing the original point cloud from the unmasked points) as used in the\\r\\nrelated Masked AutoEncoder (MAE) [20] approach for images would not work\\r\\nfor our point cloud setting.\\r\\n    First, in MAE, the self-supervised learning task is to reconstruct the masked\\r\\npatches, based on the input image’s unmasked (visible) patches. Specifically,\\r\\ngiven the 2D spatial position for each masked image patch query, the objective is\\r\\nto generate its RGB pixel values. In our case, the analogue would be to generate\\r\\nthe spatial xyz values for a masked 3D point patch query – which would be\\r\\ntrivial for the model since the query already contains the corresponding spatial\\r\\ninformation. Such a trivial solution will result in perfect zero-loss, and prevent\\r\\nthe model from learning meaningful feature representations.\\r\\n    Another issue with the reconstruction objective for point clouds is that there\\r\\nwill be point sampling variance. Specifically, the true 3D shape of the object will\\r\\nbe a continuous surface, but a point cloud will a discrete sampling of it. Suppose\\r\\nwe sample two such point clouds, and denote the first set as the “ground truth”\\r\\ntarget, and the second set as the prediction of a model. Although both sets\\r\\nreflect the same geometric shape of the object, the Chamfer distance (which can\\r\\nbe used to measure the shape difference between the two point sets) between\\r\\nthem is non-zero (i.e., there would be a loss). Thus, minimizing the Chamfer\\r\\ndistance would force the model to generate predictions that exactly match the\\r\\nfirst set. And since the first set is just one sampling of the true underlying\\r\\ndistribution, this can be an unnecessarily difficult optimization problem that\\r\\nleads to suboptimal model performance.\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds      9\\r\\n\\r\\n4     Experiments\\r\\n\\r\\nWe evaluate the pre-trained representation learned by the proposed model on\\r\\na variety of point cloud downstream tasks, including object classification, part\\r\\nsegmentation, object detection, and few-shot object classification. We also visu-\\r\\nalize the reconstruction results from masked point clouds, to qualitatively study\\r\\nthe effect of our pretraining. Finally, we perform ablation studies on masking\\r\\nstrategies and decoder designs.\\r\\n\\r\\nPretraining Datasets. (1) ShapeNet [4] has 50,000 unique 3D models from\\r\\n55 common object categories, and is used as our pre-training dataset for object\\r\\nclassification, part segmentation, and few-shot classification. For ShapeNet pre-\\r\\ntraining, we sample 1024 points from each 3D model as the inputs. We follow\\r\\n[65] to sample 64 point groups, each containing 32 points.\\r\\n    (2) We also use single-view depth map videos from the popular ScanNet [9]\\r\\ndataset, which contains around 2.5 million RGBD scans. We do not use its RGB\\r\\ninformation in this paper. We adopt similar pre-processing steps as DepthCon-\\r\\ntrast [68], but we generate a smaller subset of the dataset than in [68], which we\\r\\ncall ‘ScanNet-Medium’, to accelerate pretraining. ScanNet-Medium is generated\\r\\nby sampling every 10-th frame from ScanNet, resulting in ∼25k samples. We use\\r\\nScanNet-Medium (only geometry information) as the pre-training dataset for 3D\\r\\nobject detection. For pretraining, we sample 20k points from each 3D scene scan\\r\\nas the input. We follow [34] to sample 2048 groups, each containing 64 points.\\r\\n\\r\\nTransformer Encoder. We construct a 12-layer standard Transformer en-\\r\\ncoder, named PointViT, for point cloud understanding. Following Point-BERT\\r\\n[65], we set the hidden dimension of each encoder block to 384, number of heads\\r\\nto 6, FFN expansion ratio to 4, and drop rate of stochastic depth [23] to 0.1.\\r\\n\\r\\nTransformer Decoder. We use a single-layer Transformer decoder for pre-\\r\\ntraining. The configuration of the attention block is identical to the encoder.\\r\\n\\r\\nTraining Details. Following [65], we pretrain with the AdamW [33] optimizer\\r\\nwith a weight decay of 0.05, and a learning rate of 5 × 10−4 decayed with the\\r\\ncosine schedule. The model is trained for 300 epochs with a batch size of 128,\\r\\nwith random scaling and translation data augmentation. Only for ModelNet40\\r\\nexperiments, we also pretrain with a MoCo loss [21], following [65]. However,\\r\\nwe do not use it for any other datasets, unlike [65], which always uses it. For\\r\\nfinetuning and additional training details, please see supp.\\r\\n\\r\\n\\r\\n4.1   3D Object Classification\\r\\n\\r\\nDatasets. We compare the performance of object classification on two datasets:\\r\\nthe synthetic ModelNet40 [58], and real-world ScanObjectNN [51]. ModelNet40\\r\\n[58] consists of 12,311 CAD models from 40 classes. We follow the official data\\r\\nsplitting scheme in [58]. We evaluate the overall accuracy (OA) over all test\\r\\nsamples. ScanObjectNN [51] is a more challenging point cloud benchmark that\\r\\n\\x0c\\n\\n10     Liu et al.\\r\\n\\r\\nMethod                 SSL #point OA                                 OA\\r\\n                                             Method\\r\\nPointNet [38]               1k    89.2                          OBJ BG PB\\r\\nPointNet++ [39]             1k    90.7       PointNet [38]      79.2 73.3 68.0\\r\\nPointCNN [28]               1k    92.2       PointNet++ [39]    84.3 82.3 77.9\\r\\nSpiderCNN [61]              1k    92.4       PointCNN [28]      85.5 86.1 78.5\\r\\nPointWeb [70]               1k    92.3       SpiderCNN [61]     79.5 77.1 73.7\\r\\nPointConv [57]              1k    92.5       DGCNN [55]         86.2 82.8 78.1\\r\\nDGCNN [55]                  1k    92.9       BGA-DGCNN [51]      –    – 79.7\\r\\nKPConv [48]                 1k    92.9       BGA-PN++ [51]       –    – 80.2\\r\\nDensePoint [30]             1k    93.2       PointViT           80.6 79.9 77.2\\r\\nPosPool [32]                5k    93.2       PointViT-OcCo [53] 85.5 84.9 78.8\\r\\nRSCNN [31]                  5k    93.6       Point-BERT [65]    88.1 87.4 83.1\\r\\n[T] Point Trans. [16]       1k    92.8       MaskPoint (Ours) 89.3 88.1 84.3\\r\\n[T] Point Trans. [71]        –    93.7       Table 2: Shape Classification\\r\\n[T] PCT [19]                1k    93.2       on ScanObjectNN [51]. OBJ:\\r\\n[ST] PointViT               1k    91.4       object-only; BG: with background;\\r\\n[ST] PointViT-OcCo [53] ✓   1k    92.1\\r\\n                                             PB: BG with manual perturbation.\\r\\n[ST] Point-BERT [65]    ✓   1k    93.2\\r\\n[ST] MaskPoint (Ours)   ✓   1k 93.8\\r\\n                                                                        mIoU\\r\\nTable 1: Shape Classification on              Method\\r\\n                                                                    cat.    ins.\\r\\nModelNet40 [58]. With a standard              PointNet [38]         80.4   83.7\\r\\nTransformer backbone, our approach            PointNet++ [39]       81.9   85.1\\r\\nsignificantly outperforms training-from-      DGCNN [55]            82.3   85.2\\r\\nscratch baselines and SOTA pretrain-          PointViT              83.4   85.1\\r\\ning methods. It even outperforms Point-       PointViT-OcCo [53]    83.4   85.1\\r\\nTransformer [71], which uses an attention     Point-BERT [65]       84.1   85.6\\r\\noperator specifically designed for point      MaskPoint (Ours)      84.4   86.0\\r\\nclouds. *SSL: Self-supervised pretrain-      Table 3: Part Segmentation on\\r\\ning. [T]: Transformer-based networks         ShapeNetPart [64]. Our method\\r\\nwith special designs for point clouds.       also works well on dense prediction\\r\\n[ST]: Standard Transformer network.          tasks like segmentation.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nconsists of 2902 unique objects in 15 categories collected from noisy real-world\\r\\nscans. It has three splits: OBJ (object only), BG (with background), PB (with\\r\\nbackground and manually added perturbations). We evaluate the overall accu-\\r\\nracy (OA) over all test samples on all three splits.\\r\\n\\r\\nModelNet40 Results. Table 1 shows ModelNet40 [58] results. With 1k points,\\r\\nour approach achieves a significant 2.4% OA improvement compared to training\\r\\nfrom scratch (PointViT). It also brings a 1.7% gain over OcCo [53] pretraining,\\r\\nand 0.6% gain over Point-BERT [65] pretraining. The significant improvement\\r\\nover the baselines indicates the effectiveness of our pre-training method. Notably,\\r\\nfor the first time, with 1k points, a standard vision transformer architecture pro-\\r\\nduces competitive performance compared to sophisticatedly designed attention\\r\\noperators from PointTransformer [71] (93.8% vs 93.7%).\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds    11\\r\\n\\r\\n                                   5-way                10-way\\r\\n         Method\\r\\n                             10-shot    20-shot    10-shot    20-shot\\r\\n         DGCNN [53]         91.8 ± 3.7 93.4 ± 3.2 86.3 ± 6.2 90.9 ± 5.1\\r\\n         DGCNN-OcCo [53] 91.9 ± 3.3 93.9 ± 3.2 86.4 ± 5.4 91.3 ± 4.6\\r\\n         PointViT           87.8 ± 5.3 93.3 ± 4.3 84.6 ± 5.5 89.4 ± 6.3\\r\\n         PointViT-OcCo [53] 94.0 ± 3.6 95.9 ± 2.3 89.4 ± 5.1 92.4 ± 4.6\\r\\n         Point-BERT [65]    94.6 ± 3.1 96.3 ± 2.7 91.0 ± 5.4 92.7 ± 5.1\\r\\n         MaskPoint (Ours) 95.0 ± 3.7 97.2 ± 1.7 91.4 ± 4.0 93.4 ± 3.5\\r\\n          Table 4: Few-shot classfication on ModelNet40 [58].\\r\\n\\r\\n\\r\\n\\r\\nScanObjectNN Results. We next conduct experiments using the real-world\\r\\nscan dataset ScanObjectNN [51]. Table 2 shows the results. Our approach achieves\\r\\nSOTA performance on all three splits. On the hardest PB split, our approach\\r\\nachieves a large 7.1% OA improvement compared to training from scratch (Point-\\r\\nViT). It achieves a 5.5% gain over OcCo [53] pretraining, and a 1.2% gain over\\r\\nPoint-BERT [65] pretraining. The large improvement over the baselines high-\\r\\nlights the transferability of our model’s self-supervised representation, as there\\r\\nis a significant domain gap between the clean synthetic ShapeNet [4] dataset\\r\\nused for pretraining and the noisy real-world ScanObjectNN [51] dataset.\\r\\n    We believe the performance gain over OcCo [53] and Point-BERT [65] is\\r\\nmainly because of our discriminative pretext task. OcCo suffers from the sam-\\r\\npling variance issue (Sec. 3.2) as it uses a reconstruction-based objective in\\r\\npretraining. Compared to Point-BERT, we do not use the point patch mixing\\r\\ntechnique, which mixes two different point clouds. This could introduce unneces-\\r\\nsary noise and domain shifts to pretraining and harm downstream performance.\\r\\n\\r\\n4.2   3D Part Segmentation\\r\\nDataset. ShapeNetPart [64] consists of 16,880 models from 16 shape categories,\\r\\nwith 14,006 models for training and 2,874 for testing. It contains 50 different\\r\\nparts in total, and the number of parts for each category is between 2 and 6.\\r\\nWe use the sampled point sets produced by [39] for a fair comparison with prior\\r\\nwork. We report per-category mean IoU (cat. mIoU) and mean IoU averaged\\r\\nover all test instances (ins. mIoU).\\r\\nResults. Table 3 shows the results (per-category IoU is in supp). Our approach\\r\\noutperforms the training from the scratch (PointViT) and OcCo-pretraining\\r\\nbaselines by 1.0%/0.9% in cat./ins. mIoU. It also produces a 0.3%/0.4% gain\\r\\ncompared to Point-BERT. Thanks to our dense discriminative pretraining ob-\\r\\njective, in which we densely classify points over the 3D space, we are able to\\r\\nobtain good performance when scaling to dense prediction tasks.\\r\\n\\r\\n4.3   Few-shot Classification\\r\\nWe conduct few-shot classification experiments on ModelNet40 [58], following\\r\\nthe settings in [65]. The standard experiment setting is the “K-way N -shot”\\r\\n\\x0c\\n\\n12     Liu et al.\\r\\n\\r\\nconfiguration, where K classes are first randomly selected, and then N + 20\\r\\nobjects are sampled for each class. We train the model on K×N samples (support\\r\\nset), and evaluate on the remaining K × 20 samples (query set). We compare our\\r\\napproach with OcCo and Point-BERT, which are the current state-of-the-art.\\r\\n    We perform experiments with 4 settings, where for each setting, we run the\\r\\ntrain/evaluation on 10 different sampled splits, and report the mean and std over\\r\\nthe 10 runs. Table 4 shows the results. Our approach achieves the best perfor-\\r\\nmance for all settings. It demonstrate an absolute gain of 7.2%/3.8%/4.6%/2.5%\\r\\nover the PointViT training from the scratch baseline. When comparing to pre-\\r\\ntraining baselines, it outperforms OcCo by 1.0%/1.3%/1.5%/1.0%, and outper-\\r\\nforms Point-BERT by 0.4%/0.9%/0.4%/0.7%. It also clearly outperforms the\\r\\nDGCNN baselines. Our state-of-the-art performance on few-shot classification\\r\\nfurther demonstrates the effectiveness of our pretraining approach.\\r\\n\\r\\n4.4   3D Object Detection\\r\\nOur most closely related work, Point-BERT [65] showed experiments only on\\r\\nobject-level classification and segmentation tasks. In this paper, we evaluate\\r\\na model’s pretrained representation on a more challenging scene-level down-\\r\\nstream task: 3D object detection on ScanNetV2 [9], which consists of real-world\\r\\nrichly-annotated 3D reconstructions of indoor scenes. It comprises 1201 training\\r\\nscenes, 312 validation scenes and 100 hidden test scenes. Axis-aligned bounding\\r\\nbox labels are provided for 18 object categories. For this experiment, we adopt\\r\\n3DETR [34] as the downstream model for both our method and Point-BERT.\\r\\n3DETR is an end-to-end transformer-based 3D object detection pipeline. Dur-\\r\\ning finetuning, the input point cloud is first downsampled to 2048 points via\\r\\na VoteNet-style Set Aggregation (SA) layer [37,39], which then goes through\\r\\n3-layer self-attention blocks. The decoder is composed of 8-layer cross-attention\\r\\nblocks. For a fair comparison with the 3DETR train-from-scratch baseline, we\\r\\nstrictly follow its architecture of SA layer and encoder during pretraining, whose\\r\\nweights are transferred during finetuning. Our pretraining dataset is ScanNet\\r\\nMedium, as described in Sec. 4.\\r\\n    Table 5 shows that our method surpasses the 3DETR train-from-scratch\\r\\nbaseline by a large margin (+1.3mAP@0.25 and +2.7mAP@0.50). Interestingly,\\r\\nPoint-BERT brings nearly no improvement compared to training from scratch.\\r\\nThe low mask rate and discrete tokens learned from dVAE may impede Point-\\r\\nBERT from learning meaningful representations for detection. Also, the 3DETR\\r\\npaper [34] found that increasing the number of encoding layers in 3DETR brings\\r\\nonly a small benefit to its detection performance. Here we increase the num-\\r\\nber of layers from 3 to 12, which leads to a large performance improvement\\r\\n(+2.1mAP@0.25 and +4.1mAP@0.50) for our approach compared to training\\r\\nfrom scratch. This result demonstrates that by pre-training on a large unlabeled\\r\\ndataset, we can afford to increase the model’s encoder capacity to learn richer\\r\\nrepresentations. Finally, note that we also include VoteNet based methods at\\r\\nthe top of Table 5 as a reference, but the numbers are not directly comparable\\r\\nas they are using a different (non Transformer-based) detector.\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds      13\\r\\n\\r\\n       Methods                         SSL   Pretrained Input   AP25   AP50\\r\\n       VoteNet [37]                                  -          58.6   33.5\\r\\n       STRL [24]                       ✓           Geo          59.5   38.4\\r\\n       Implicit Autoencoder [62]       ✓           Geo          61.5   39.8\\r\\n       RandomRooms [42]                ✓           Geo          61.3   36.2\\r\\n       PointContrast [60]              ✓           Geo          59.2   38.0\\r\\n       DepthContrast [68]              ✓           Geo          61.3    –\\r\\n       DepthContrast [68]              ✓       Geo + RGB        64.0   42.9\\r\\n       3DETR [34]                                    -          62.1   37.9\\r\\n       Point-BERT [65]                 ✓           Geo          61.0   38.3\\r\\n       MaskPoint (Ours)                ✓           Geo          63.4   40.6\\r\\n       MaskPoint (Ours, 12 Enc)        ✓           Geo          64.2   42.0\\r\\nTable 5: 3D object detection results on ScanNet validation set. The backbone of\\r\\nour pretraining model and Point-BERT [65] is 3DETR [34]. All other methods\\r\\nuse VoteNet [38] as the finetuning backbone. Only geometry information is fed\\r\\ninto the downstream task. “Input” column denotes input type for the pretraining\\r\\nstage. “Geo” denotes geometry information. Note that DepthContrast (Geo +\\r\\nRGB) model uses a heavier backbone (PointNet 3x) for downstream tasks.\\r\\n\\r\\n\\r\\n        Original   Masked     Recon.         Original     Masked       Recon.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3: Reconstruction results. By reformulating reconstruction as a discrim-\\r\\ninative occupancy classification task, we achieve a similar learning objective to\\r\\ngenerative reconstruction while being robust to point sampling variance. Even\\r\\nwith a high 90% mask ratio, our approach recovers the overall shape of the\\r\\noriginal point cloud, without overfitting.\\r\\n\\r\\n\\r\\n4.5   Qualitative Reconstructions\\r\\nAlthough our model is not trained with the reconstruction objective, we can still\\r\\nreconstruct the point cloud with our decoder by classifying the densely sampled\\r\\npoints from the point cloud’s full 3D bounding box space: Prec = {x|D(x|L) =\\r\\n1}. Fig. 3 shows the reconstruction results with masking ratio of 90%. Even with\\r\\nsuch a large masking rate, our model is able to reconstruct the overall shape of\\r\\nthe original point cloud, without overfitting. Fig. 3 (bottom right) shows how our\\r\\nmodel uses its learned prior to perform reasonable reconstructions: a shoe shelf\\r\\nwhere the stand on the right is masked. We hypothesize that it first recognizes\\r\\nit as a shoe shelf (without any labels), and learns that shoe shelves are usually\\r\\nsymmetric from left-to-right, to reconstruct the right part.\\r\\n\\x0c\\n\\n14       Liu et al.\\r\\n\\r\\n      ratio   OA           ratio   OA           # queries   OA       # dec.   OA\\r\\n      0.25    83.2         0.25    82.4             64      83.7       1      84.3\\r\\n      0.50    83.7         0.50    83.8            256      84.3       3      83.7\\r\\n      0.75    84.1         0.75    83.7           1024      83.9       6      83.9\\r\\n      0.90    84.3         0.90    84.1\\r\\n\\r\\n(a) Mask rate (random) (b) Mask rate (block)   (c) # dec. queries   (d) # dec. layers\\r\\n\\r\\nTable 6: Ablations on ScanObjectNN [51] (PB split). Our findings are: a larger\\r\\nmasking ratio generally yields better performance; random masking is slightly\\r\\nbetter than block masking; 256-query provides a good balance between informa-\\r\\ntion and noise; a thin decoder ensures rich feature representation in the encoder\\r\\nand benefits downstream performance.\\r\\n\\r\\n\\r\\n\\r\\n4.6    Ablation Studies\\r\\nMasking Strategy. We show the influence of different masking strategies in\\r\\nTable 6a, 6b. First, we observe that a higher masking ratio generally yields\\r\\nbetter performance, regardless of sampling type. This matches our intuition that\\r\\na higher masking ratio creates a harder and more meaningful pretraining task.\\r\\nFurther, with a higher masking ratio, random masking is slightly better than\\r\\nblock masking. Therefore, we use a high mask rate of 90% with random masking.\\r\\nPretraining Decoder Design. We study the design of the pretraining de-\\r\\ncoder in Table 6c, 6d, by varying the number of decoder queries and layers. The\\r\\nnumber of decoder queries influence the balance between the classification of\\r\\nthe real points and fake points. We find that 256-query is the sweet spot, where\\r\\nmore queries could introduce too much noise, and fewer queries could result in\\r\\ninsufficient training information.\\r\\n    The modeling power of the decoder affects the effectiveness of the pretraining:\\r\\nideally, we want the encoder to only encode the features, while the decoder only\\r\\nprojects the features to the pre-training objective. Any imbalance in either way\\r\\ncan harm the model’s performance on downstream tasks. We find that a single-\\r\\nlayer decoder is sufficient for performing our proposed point discrimination task,\\r\\nand having more decoder layers harms the model’s performance.\\r\\n\\r\\n\\r\\n5     Conclusion\\r\\nWe proposed a discriminative masked point cloud pretraining framework, which\\r\\nfacilitates a variety of downstream tasks while significantly reducing the pre-\\r\\ntraining time compared to the prior Transformer-based state-of-the-art method.\\r\\nWe adopted occupancy values to represent the point cloud, forming a simpler\\r\\nyet effective binary pretraining objective function. Extensive experiments on 3D\\r\\nshape classification, detection, and segmentation demonstrated the strong per-\\r\\nformance of our approach. Currently, we randomly mask local point groups to\\r\\npartition the point cloud into masked and unmasked sets. It could be interesting\\r\\nto explore ways to instead learn how to mask the points. We hope our research\\r\\ncan raise more attention to mask based self-supervised learning on point clouds.\\r\\n\\x0c\\n\\n           Masked Discrimination for Self-Supervised Learning on Point Clouds                                 15\\r\\n\\r\\nSupplementary Material\\r\\nA       More results\\r\\nShapeNetPart In Table 7, we compare the categorical mIoU on ShapeNet-\\r\\nPart with other methods. With a PointViT backbone, we get the highest class\\r\\nmIoU at 84.4% and the highest instance mIoU at 86.0%, outperforming pre-\\r\\nvious self-supervised learning approaches (OcCo [53] and Point-BERT [65]). It\\r\\nalso outperforms standard train-from-scratch point cloud backbones like Point-\\r\\nNet++ [39] and DGCNN [55]. For all categories, our method either has the\\r\\nhighest accuracy or is among the best. Thanks to our dense discriminative pre-\\r\\ntraining objective, in which we densely classify points over the 3D space, we are\\r\\nable to obtain good performance when scaling to dense prediction tasks like part\\r\\nsegmentation.\\r\\n\\r\\n\\r\\nMethods             cls. ins. aero bag cap car chair earp. guit. knif. lamp lapt. mot. mug pist. rock. skt. table\\r\\nPointNet [38]       80.4 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6\\r\\nPN++ [39]           81.9 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6\\r\\nDGCNN [55]          82.3 85.2 84.0 83.4 86.7 77.8 90.6 74.7 91.2 87.5 82.8 95.7 66.3 94.9 81.1 63.5 74.5 82.6\\r\\nPointViT         83.4 85.1 82.9 85.4 87.7 78.8 90.5 80.8 91.1 87.7 85.3 95.6 73.9 94.9 83.5 61.2 74.9 80.6\\r\\nOcCo [53]        83.4 85.1 83.3 85.2 88.3 79.9 90.7 74.1 91.9 87.6 84.7 95.4 75.5 94.4 84.1 63.1 75.7 80.8\\r\\nPN-BERT [65]     84.1 85.6 84.3 84.8 88.0 79.8 91.0 81.7 91.6 87.9 85.2 95.6 75.6 94.7 84.3 63.4 76.3 81.5\\r\\nMaskPoint (Ours) 84.4 86.0 84.2 85.6 88.1 80.3 91.2 79.5 91.9 87.8 86.2 95.3 76.9 95.0 85.3 64.4 76.9 81.8\\r\\nTable 7: Part segmentation results on ShapeNetPart [64]. Bold and underline\\r\\nnumbers denote best and second best performance, respectively.\\r\\n\\r\\n\\r\\nScanNet Table 8 reports per-class average precision on 18 classes of Scan-\\r\\nNetV2 with a 0.25 box IoU threshold. Relying on purely geometric information,\\r\\nour method exceeds 3DETR [34] in detecting objects like curtain, garbagebin,\\r\\ntable, desk, etc, where geometry is a strong cue for recognition. These results\\r\\nindicate that our mask based discriminative pretraining framework is effective\\r\\nin learning strong geometric representations. More importantly, our model out-\\r\\nperforms 3DETR on classes where it has relatively low AP, e.g., picture, door,\\r\\ncurtain, refrigerator, etc, which demonstrates the usefulness of pretraining: with\\r\\nthe pretrained knowledge relevant to those hard classes, the model is able to\\r\\nmake more accurate predictions than the training from the scratch baseline.\\r\\n\\r\\n\\r\\nModel           AP25 cab. bed cha. sofa tab. door win. boo. pic. cou. desk cur. ref. sho. toi. sink bat. gar.\\r\\n3DETR [34] 62.2 50.2 87.0 86.0 87.1 61.6 46.6 40.1 54.5 9.1 62.8 69.5 48.4 50.9 68.4 97.9 67.6 85.9 45.8\\r\\nOurs       63.4 51.8 82.5 85.9 86.8 69.8 50.9 36.9 47.3 10.7 59.6 76.3 65.9 55.6 66.4 99.1 61.5 83.7 49.8\\r\\nOurs (12×) 64.2 49.5 81.0 87.2 86.3 65.2 51.3 42.6 56.7 16.2 56.8 73.8 59.6 56.0 77.0 97.8 66.6 85.0 47.7\\r\\nTable 8: 3D object detection scores per category on the ScanNetV2 dataset,\\r\\nevaluated with bbox mIoU 0.25. Ours (12×): 12 encoder blocks.\\r\\n\\x0c\\n\\n16          Liu et al.\\r\\n\\r\\n Original    Masked p>0.01          p>0.025   p>0.05   p>0.075   p>0.10   p>0.20   p>0.30   p>0.40   p>0.50\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  low                                                                                                  high\\r\\n\\r\\n\\r\\nFig. 4: Reconstruction results. We densely perform the discriminative occu-\\r\\npancy classification task in 3D space, and visualize the predicted occupancy\\r\\nprobability. By varying the confidence threshold p̂, we show that our model is\\r\\nable to predict a continuous probability distribution of the occupancy function.\\r\\n\\r\\n                         Original                 Masked                     Recon            high\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                              low\\r\\n\\r\\n\\r\\nFig. 5: A closer look at occupancy distribution. Although there are no\\r\\npoints present in both red and purple regions of the masked point cloud, the\\r\\nreconstructed probability distribution correctly reflects that of the original point\\r\\ncloud: a lower occupancy in red region, and a higher occupancy in purple region.\\r\\n\\r\\n\\r\\n\\r\\nMore reconstruction visualizations We densely perform the discriminative\\r\\noccupancy classification task in 3D space, and visualize in Fig. 4 the predicted\\r\\noccupancy probability. In different columns, we vary the occupancy threshold τ ,\\r\\nand only show the points with occupancy probability prediction that is higher\\r\\nthan the given threshold. We can see that our model is able to output a continu-\\r\\nous probability distribution of the occupancy function, even if it is only trained\\r\\nwith discrete occupancy values from the sampled points.\\r\\n    When we take a closer look at the occupancy distribution, we find several\\r\\ninteresting clues on how the model is modeling the probability distribution im-\\r\\npressively well. We show our findings in Fig. 5. There are no points present in\\r\\nboth red and purple regions of the masked point cloud, while in the original point\\r\\ncloud, there are points present in the purple region, and no points are in the red\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds               17\\r\\n\\r\\nregion. In the reconstructed probability distribution, the model predicts a low\\r\\noccupancy probability in the red region, and a high occupancy in the purple\\r\\nregion.\\r\\n    We find such predictions align with how a human might understand the\\r\\nscene. First, although there are no points in the purple region of the masked\\r\\npoint cloud, given the partial view of the top-left region of the desk top, and\\r\\nthe regions where the desk legs are present, it is very likely that there are points\\r\\npresent in the purple region (top-right region of the desk top). As for the red\\r\\nregion, the model’s prediction can be interpreted as follows: usually desk tops\\r\\nare rectangle-shaped; however, there do exist desks whose surface shrinks inside\\r\\nthe region where the person sits. Given that there is not a decisive evidence\\r\\nthat indicates how this particular desk instance is shaped, the model produces\\r\\npredictions with probability around 0.7, which is lower than other regions that\\r\\nare more certain (yellow points in Fig. 5 “Recon”, with p>0.9).\\r\\n    These two intriguing and encouraging visualizations suggest that our pre-\\r\\ntrained model is capable of modeling a continuous occupancy probability distri-\\r\\nbution, and it has learned a deep understanding of the input scene.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  (a) Training from scratch   (b) Pretrained on ShapeNet   (c) Finetuned on ModelNet40\\r\\n\\r\\nFig. 6: t-SNE visualization of the encoder features for ModelNet40 under three\\r\\nsettings: (a) training from scratch, (b) pretrained on ShapeNet, and (c) finetuned\\r\\non ModelNet40.\\r\\n\\r\\nt-SNE visualizations We show the t-SNE visualizations of the extracted fea-\\r\\nture vectors from our approach in Fig. 6. We use the class token from the encoder\\r\\noutput as the high dimensional feature representation for t-SNE. Three setting\\r\\nare adopted here: (a) training from scratch, (b) pretraining on ShapeNet [4], and\\r\\n(c) finetuning on ModelNet40 [58].\\r\\n    When training the ModelNet40 classification model from scratch, the result-\\r\\ning features from different categories become heavily entangled, which can leads\\r\\nto less interpretable and robust predictions for new test-time inputs. In con-\\r\\ntrast, when pretraining the model on ShapeNet using our proposed MaskPoint,\\r\\nthe features are much more distinguishable from each other. Furthermore, after\\r\\nfinetuning on ModelNet40, the projected features from different classes become\\r\\nclearly separable from each other, which indicates the effectiveness of our ap-\\r\\nproach. Interestingly, the feature clusters in our approach are quite tight. Such\\r\\n\\x0c\\n\\n18     Liu et al.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n         (a)              (b)              (c)            (d)           (e)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n         (f)              (g)              (h)             (i)           (j)\\r\\n\\r\\nFig. 7: Qualitative results of 3D object detection on ScanNetV2 [9]. We\\r\\nshow ground truth in green and predictions in red bounding boxes.\\r\\n\\r\\n\\r\\nfeature layout indicates that we can learn a more compact and disjoint decision\\r\\nboundary, which has been evaluated to be critical in machine learning applica-\\r\\ntions such mixup [66,49] and uncertainty estimation in deep learning [15].\\r\\n3D object detection visualizations We show 3D object detection visualiza-\\r\\ntions of ScanNetV2 in Figure 7 with green ground truth bounding boxes and red\\r\\npredicted bounding boxes. Our model is capable of precisely localizing the obe-\\r\\njct (Fig. 7b and Fig. 7j). Results indicate that our masked based discriminative\\r\\npretraining can not only produce high-quality bounding boxes for the previously\\r\\nannotated objects, but also discover objects that are not annotated. For exam-\\r\\nple, in Fig. 7c, our model produces the a bounding box for the bookshelf in the\\r\\nlower region; in Fig. 7f, it correctly locates the sofa in the center of the room.\\r\\n\\r\\n\\r\\nB     Additional Implementation Details\\r\\nB.1   Pretraining\\r\\nTransformer Encoder. We follow the standard Transformer design in [52,65]\\r\\nto construct our point cloud Transformer backbone, PointViT. It consists of a\\r\\nlinear stack of 12 Transformer blocks, where each Transformer block contains\\r\\na multi-head self-attention (MHSA) layer and a feed-forward network (FFN).\\r\\nLayerNorm (LN) is adopted in both layers. Following [65], we use the MLP-\\r\\nbased positional embedding. We set the Transformer hidden dimension to 384,\\r\\nMHSA head number to 6, expansion rate of FFN to 4, stochastic drop path [23]\\r\\nrate to 0.1.\\r\\nFeed-forward network (FFN). Following [65], we use a two-layer MLP with\\r\\nReLU and dropout as the feed-forward network. Dropout rate is set to 0.1.\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds    19\\r\\n\\r\\n\\r\\n      Module                 Block     Cin   Cout   k   Nout     Cmiddle\\r\\n      Positional Embed.      MLP        3    384                   128\\r\\n      Point Classify Head    MLP       384    2                     64\\r\\n      Classification Head    MLP       768   Ncls                512, 256\\r\\n                             MLP       387   384                  384×4\\r\\n                            DGCNN      384   512    4    128\\r\\n                            DGCNN      512   384    4    128\\r\\n                            DGCNN      384   512    4    256\\r\\n      Segmentation Head     DGCNN      512   384    4    256\\r\\n                            DGCNN      384   512    4    512\\r\\n                            DGCNN      512   384    4    512\\r\\n                            DGCNN      384   512    4   2048\\r\\n                            DGCNN      512   384    4   2048\\r\\nTable 9: Detailed module design of MaskPoint. Cin /Cout denotes the in-\\r\\nput/output channels, Cmiddle denotes the hidden channels of MLP modules,\\r\\nNout denotes the cardinality of the output point/feature set, k is the number of\\r\\nneighbors used in the k-NN operator.\\r\\n\\r\\n\\r\\nPositional Embeddings. Following [65], we use a two-layer MLP with GELU [22]\\r\\nas the positional embedding module. All Transformer modules share the same\\r\\npositional embedding MLP module. Detailed configuration is shown in Table 9.\\r\\n\\r\\nPoint Classification Head. We use a simple two-layer MLP with GELU [22]\\r\\nfor the point classification pretext task in pretraining. We use the binary focal\\r\\nloss [29] to balance the information from positive and negative samples. Detailed\\r\\nconfiguration is shown in Table 9.\\r\\n\\r\\nScanNet-Medium Pretraining Note that for ScanNet-Medium pretraining,\\r\\nwe use the encoder with 3 Transformer blocks, where each block still consists\\r\\nof a MHSA layer and a FFN layer. LN and MLP positional embedding are also\\r\\nutilized in the encoder. Following the downstream architecture of 3DETR [34],\\r\\nwe set the hidden dimension to be 256, the number of MHSA heads to be 4, and\\r\\nDropout rate to be 0.1 for the Transformer. The hidden dimension is set to be\\r\\n128 for the FFN layer.\\r\\n    For other settings such as positional embedding and classification head, the\\r\\nsetting is exactly the same as the ShapeNet pretraining setting.\\r\\n\\r\\n\\r\\nB.2   Finetuning\\r\\n\\r\\nClassification We use a three-layer MLP with dropout for the classification\\r\\nhead. The input feature to the classification head consists of two parts from the\\r\\nTransformer encoder: (1) the CLS token; (2) the max-pooled feature of other\\r\\noutput features. These two features are concatenated together and fed into the\\r\\nclassification head. Detailed configuration is shown in Table 9.\\r\\n\\x0c\\n\\n20      Liu et al.\\r\\n\\r\\n config              value                   config                   value\\r\\n epochs              300                     epochs                    300\\r\\n optimizer           AdamW                   optimizer              AdamW\\r\\n learning rate       5e-4                    learning rate             5e-4\\r\\n weight decay        5e-2                    weight decay              5e-2\\r\\n LR schedule         cosine decay            LR schedule          cosine decay\\r\\n warmup epochs       3                       warmup epochs              10\\r\\n augmentation        Scale/Translate\\r\\n                                             augmentation        Scale/Translate\\r\\n batch size          128\\r\\n                                             batch size          32(cls), 16(seg)\\r\\n # points            1024                    # points          1024(cls), 2048(seg)\\r\\n # patches           64                      # patches          64(cls), 128(seg)\\r\\n patch size          32                      patch size                 32\\r\\n mask ratio          0.90\\r\\n mask type           random                 Table 11: Finetuning setting on\\r\\nTable 10: Pretraining setting on            classification (cls) and segmentation\\r\\nShapeNet [4].                               (seg).\\r\\n\\r\\nPart Segmentation The standard Transformer only has a single-scale fea-\\r\\nture output, which is not suitable for common head designs for dense prediction\\r\\ntasks like segmentation. Following [65], after getting the feature outputs from\\r\\nthe Transformer encoder, we perform segmentation in two steps: (1) generating\\r\\na multi-scale feature pyramid from the Transformer encoder outputs; (2) apply-\\r\\ning a standard feature propagation head for point cloud segmentation on the\\r\\ngenerated multi-scale feature maps to generate dense predictions.\\r\\n    We obtain the feature maps f{4,8,12} ∈ RN3 ×d from the 4th, 8th, 12th layer,\\r\\nand our goal is to convert them to a feature pyramid with different cardinality\\r\\nN{0,1,2,3} , where N0 is the cardinality of the original point cloud P, and N{1,2,3}\\r\\nare the desired cardinality of the feature maps at different scales; in our case,\\r\\nN{0,1,2,3} = {2048, 512, 256, 128}.\\r\\n    First, we use furthest point sampling (FPS) to downsample the original point\\r\\ncloud P0 to different resolutions: P{1,2,3} ∈ RN{1,2,3} ×3 , then a feature propaga-\\r\\ntion module is used to upsample the feature maps f{4,8,12} to the corresponding\\r\\n               up\\r\\ncardinality f{4,8,12} ∈ RN{1,2,3} ×d .\\r\\n    After obtaining the multi-scale feature maps, we then apply the DGCNN\\r\\n                                                                              up\\r\\nmodule to propagate the features through different scales, fˆ4 = DGCNN(f{4,8,12}     ).\\r\\n                                                            ˆ\\r\\nAnother feature propagation layer is then applied on f4 for upsampling to the\\r\\nhighest resolution fˆ0 ∈ RN0 ×d .\\r\\n    Finally, we apply a pointwise MLP classifier on the features at the highest\\r\\nresolution fˆ0 to obtain the segmentation results. Detailed configuration is shown\\r\\nin Table 9.\\r\\n\\r\\n3D Object Detection We strictly follow the setting of the original 3DETR [34]\\r\\nmodel as the downstream 3D object detector. The points are first donwsampled\\r\\nto 2048 points using a Set-Aggregation (SA) layer. The encoder is composed\\r\\nof 3 standard Transformer blocks. The decoder is comprised of 8 Transformer\\r\\nblocks using cross attention. During finetuning, only the weights of the SA layer\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds            21\\r\\n\\r\\nand the encoder are transferred to the downstream tasks. The finetuning epoch\\r\\nnumber is 1080, the optimizer is AdamW with learning rate of 5 × 10−4 and\\r\\nweight decay of 0.1, the batch size is 8.\\r\\n\\r\\n\\r\\nReferences\\r\\n\\r\\n 1. Achituve, I., Maron, H., Chechik, G.: Self-supervised learning for domain adap-\\r\\n    tation on point clouds. In: Proceedings of the IEEE/CVF Winter Conference on\\r\\n    Applications of Computer Vision. pp. 123–133 (2021)\\r\\n 2. Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv\\r\\n    preprint arXiv:2106.08254 (2021)\\r\\n 3. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\\r\\n    to-end object detection with transformers. In: European conference on computer\\r\\n    vision. pp. 213–229. Springer (2020)\\r\\n 4. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,\\r\\n    Savarese, S., Savva, M., Song, S., Su, H., et al.: Shapenet: An information-rich\\r\\n    3d model repository. arXiv preprint arXiv:1512.03012 (2015)\\r\\n 5. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-\\r\\n    trastive learning of visual representations. In: III, H.D., Singh, A. (eds.) Proceed-\\r\\n    ings of the 37th International Conference on Machine Learning. Proceedings of\\r\\n    Machine Learning Research, vol. 119, pp. 1597–1607. PMLR (13–18 Jul 2020),\\r\\n    https://proceedings.mlr.press/v119/chen20j.html\\r\\n 6. Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.: Info-\\r\\n    gan: Interpretable representation learning by information maximizing generative\\r\\n    adversarial nets. Advances in neural information processing systems 29 (2016)\\r\\n 7. Chen, X., Ding, M., Wang, X., Xin, Y., Mo, S., Wang, Y., Han, S., Luo, P., Zeng, G.,\\r\\n    Wang, J.: Context autoencoder for self-supervised representation learning. arXiv\\r\\n    preprint arXiv:2202.03026 (2022)\\r\\n 8. Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings\\r\\n    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\\r\\n    15750–15758 (2021)\\r\\n 9. Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M.: Scannet:\\r\\n    Richly-annotated 3d reconstructions of indoor scenes. In: Proceedings of the IEEE\\r\\n    conference on computer vision and pattern recognition. pp. 5828–5839 (2017)\\r\\n10. Danila Rukhovich, Anna Vorontsova, A.K.: Fcaf3d: Fully convolutional anchor-free\\r\\n    3d object detection. arXiv preprint arXiv:2112.00322 (2021)\\r\\n11. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-\\r\\n    scale hierarchical image database. In: 2009 IEEE conference on computer vision\\r\\n    and pattern recognition. pp. 248–255. Ieee (2009)\\r\\n12. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\\r\\n    bidirectional transformers for language understanding. In: Proceedings of the 2019\\r\\n    Conference of the North American Chapter of the Association for Computational\\r\\n    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\\r\\n    pp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota\\r\\n    (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology.org/\\r\\n    N19-1423\\r\\n13. DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net-\\r\\n    works with cutout. arXiv preprint arXiv:1708.04552 (2017)\\r\\n\\x0c\\n\\n22      Liu et al.\\r\\n\\r\\n14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\\r\\n    T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\\r\\n    An image is worth 16x16 words: Transformers for image recognition at scale. ICLR\\r\\n    (2021)\\r\\n15. Du, X., Wang, X., Gozum, G., Li, Y.: Unknown-aware object detection: Learning\\r\\n    what you don’t know from videos in the wild. Proceedings of the IEEE/CVF\\r\\n    Conference on Computer Vision and Pattern Recognition (2022)\\r\\n16. Engel, N., Belagiannis, V., Dietmayer, K.: Point transformer. IEEE Access 9,\\r\\n    134826–134840 (2021)\\r\\n17. Ghiasi, G., Lin, T.Y., Le, Q.V.: Dropblock: A regularization method for convolu-\\r\\n    tional networks. In: NeurIPS (2018)\\r\\n18. Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by pre-\\r\\n    dicting image rotations. In: International Conference on Learning Representations\\r\\n    (2018), https://openreview.net/forum?id=S1v4N2l0-\\r\\n19. Guo, M.H., Cai, J.X., Liu, Z.N., Mu, T.J., Martin, R.R., Hu, S.M.: Pct: Point\\r\\n    cloud transformer. Computational Visual Media 7(2), 187–199 (2021)\\r\\n20. He, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., Girshick, R.: Masked autoencoders\\r\\n    are scalable vision learners. arXiv preprint arXiv:2111.06377 (2021)\\r\\n21. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised\\r\\n    visual representation learning. In: Proceedings of the IEEE/CVF conference on\\r\\n    computer vision and pattern recognition. pp. 9729–9738 (2020)\\r\\n22. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint\\r\\n    arXiv:1606.08415 (2016)\\r\\n23. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with\\r\\n    stochastic depth. In: European conference on computer vision. pp. 646–661.\\r\\n    Springer (2016)\\r\\n24. Huang, S., Xie, Y., Zhu, S.C., Zhu, Y.: Spatio-temporal self-supervised represen-\\r\\n    tation learning for 3d point clouds. arXiv preprint arXiv:2109.00179 (2021)\\r\\n25. Jiang, Y., Chang, S., Wang, Z.: Transgan: Two pure transformers can make one\\r\\n    strong gan, and that can scale up. Advances in Neural Information Processing\\r\\n    Systems 34 (2021)\\r\\n26. Joshi, M., Chen, D., Liu, Y., Weld, D.S., Zettlemoyer, L., Levy, O.: Spanbert:\\r\\n    Improving pre-training by representing and predicting spans. Transactions of the\\r\\n    Association for Computational Linguistics 8, 64–77 (2020)\\r\\n27. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: A\\r\\n    lite bert for self-supervised learning of language representations. In: International\\r\\n    Conference on Learning Representations (2020), https://openreview.net/forum?\\r\\n    id=H1eA7AEtvS\\r\\n28. Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B.: Pointcnn: Convolution on x-\\r\\n    transformed points. Advances in neural information processing systems 31 (2018)\\r\\n29. Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollar, P.: Focal loss for dense object\\r\\n    detection. In: ICCV (2017)\\r\\n30. Liu, Y., Fan, B., Meng, G., Lu, J., Xiang, S., Pan, C.: Densepoint: Learning densely\\r\\n    contextual representation for efficient point cloud processing. In: Proceedings of the\\r\\n    IEEE/CVF International Conference on Computer Vision. pp. 5239–5248 (2019)\\r\\n31. Liu, Y., Fan, B., Xiang, S., Pan, C.: Relation-shape convolutional neural network\\r\\n    for point cloud analysis. In: Proceedings of the IEEE/CVF Conference on Com-\\r\\n    puter Vision and Pattern Recognition. pp. 8895–8904 (2019)\\r\\n32. Liu, Z., Hu, H., Cao, Y., Zhang, Z., Tong, X.: A closer look at local aggregation\\r\\n    operators in point cloud analysis. In: European Conference on Computer Vision.\\r\\n    pp. 326–342. Springer (2020)\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds           23\\r\\n\\r\\n33. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\\r\\n    arXiv:1711.05101 (2017)\\r\\n34. Misra, I., Girdhar, R., Joulin, A.: An End-to-End Transformer Model for 3D Object\\r\\n    Detection. In: ICCV (2021)\\r\\n35. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving\\r\\n    jigsaw puzzles. In: European conference on computer vision. pp. 69–84. Springer\\r\\n    (2016)\\r\\n36. Poursaeed, O., Jiang, T., Qiao, H., Xu, N., Kim, V.G.: Self-supervised learning of\\r\\n    point clouds via orientation estimation. In: 2020 International Conference on 3D\\r\\n    Vision (3DV). pp. 1018–1028. IEEE (2020)\\r\\n37. Qi, C.R., Litany, O., He, K., Guibas, L.J.: Deep hough voting for 3d object de-\\r\\n    tection in point clouds. In: Proceedings of the IEEE International Conference on\\r\\n    Computer Vision (2019)\\r\\n38. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets\\r\\n    for 3d classification and segmentation. In: Proceedings of the IEEE conference on\\r\\n    computer vision and pattern recognition. pp. 652–660 (2017)\\r\\n39. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learn-\\r\\n    ing on point sets in a metric space. arXiv preprint arXiv:1706.02413 (2017)\\r\\n40. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\\r\\n    Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from\\r\\n    natural language supervision. In: International Conference on Machine Learning.\\r\\n    pp. 8748–8763. PMLR (2021)\\r\\n41. Radford, A., Sutskever, I.: Improving language understanding by generative pre-\\r\\n    training. In: arxiv (2018)\\r\\n42. Rao, Y., Liu, B., Wei, Y., Lu, J., Hsieh, C.J., Zhou, J.: Randomrooms: Unsu-\\r\\n    pervised pre-training from synthetic shapes and randomized layouts for 3d object\\r\\n    detection. In: Proceedings of the IEEE/CVF International Conference on Com-\\r\\n    puter Vision. pp. 3283–3292 (2021)\\r\\n43. Rolfe, J.T.: Discrete variational autoencoders. In: ICLR (2017)\\r\\n44. Singh, K.K., Lee, Y.J.: Hide-and-seek: Forcing a network to be meticulous for\\r\\n    weakly-supervised object and action localization. In: 2017 IEEE international con-\\r\\n    ference on computer vision (ICCV). pp. 3544–3553. IEEE (2017)\\r\\n45. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:\\r\\n    Dropout: A simple way to prevent neural networks from overfitting. Journal of\\r\\n    Machine Learning Research 15(56), 1929–1958 (2014), http://jmlr.org/papers/\\r\\n    v15/srivastava14a.html\\r\\n46. Steiner, A., Kolesnikov, A., , Zhai, X., Wightman, R., Uszkoreit, J., Beyer, L.: How\\r\\n    to train your vit? data, augmentation, and regularization in vision transformers.\\r\\n    arXiv preprint arXiv:2106.10270 (2021)\\r\\n47. Thabet, A., Alwassel, H., Ghanem, B.: Self-supervised learning of local features\\r\\n    in 3d point clouds. In: Proceedings of the IEEE/CVF Conference on Computer\\r\\n    Vision and Pattern Recognition Workshops. pp. 938–939 (2020)\\r\\n48. Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.J.:\\r\\n    Kpconv: Flexible and deformable convolution for point clouds. In: Proceedings of\\r\\n    the IEEE/CVF international conference on computer vision. pp. 6411–6420 (2019)\\r\\n49. Thulasidasan, S., Chennupati, G., Bilmes, J.A., Bhattacharya, T., Michalak, S.: On\\r\\n    mixup training: Improved calibration and predictive uncertainty for deep neural\\r\\n    networks. Advances in Neural Information Processing Systems 32 (2019)\\r\\n50. Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T.,\\r\\n    Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., Dosovitskiy, A.: Mlp-\\r\\n    mixer: An all-mlp architecture for vision. arXiv preprint arXiv:2105.01601 (2021)\\r\\n\\x0c\\n\\n24      Liu et al.\\r\\n\\r\\n51. Uy, M.A., Pham, Q.H., Hua, B.S., Nguyen, T., Yeung, S.K.: Revisiting point cloud\\r\\n    classification: A new benchmark dataset and classification model on real-world\\r\\n    data. In: Proceedings of the IEEE/CVF international conference on computer vi-\\r\\n    sion. pp. 1588–1597 (2019)\\r\\n52. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\\r\\n    Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg,\\r\\n    U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R.\\r\\n    (eds.) Advances in Neural Information Processing Systems. vol. 30. Curran\\r\\n    Associates, Inc. (2017), https://proceedings.neurips.cc/paper/2017/file/\\r\\n    3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\\r\\n53. Wang, H., Liu, Q., Yue, X., Lasenby, J., Kusner, M.J.: Unsupervised point cloud\\r\\n    pre-training via occlusion completion. In: ICCV (2021)\\r\\n54. Wang, H., Zhu, Y., Adam, H., Yuille, A., Chen, L.C.: MaX-DeepLab: End-to-end\\r\\n    panoptic segmentation with mask transformers. In: CVPR (2021)\\r\\n55. Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M.: Dynamic\\r\\n    graph cnn for learning on point clouds. Acm Transactions On Graphics (tog) 38(5),\\r\\n    1–12 (2019)\\r\\n56. Wei, C., Fan, H., Xie, S., Wu, C.Y., Yuille, A., Feichtenhofer, C.: Masked feature\\r\\n    prediction for self-supervised visual pre-training. arXiv preprint arXiv:2112.09133\\r\\n    (2021)\\r\\n57. Wu, W., Qi, Z., Fuxin, L.: Pointconv: Deep convolutional networks on 3d point\\r\\n    clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\\r\\n    Pattern Recognition. pp. 9621–9630 (2019)\\r\\n58. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets: A\\r\\n    deep representation for volumetric shapes. In: Proceedings of the IEEE conference\\r\\n    on computer vision and pattern recognition. pp. 1912–1920 (2015)\\r\\n59. Xiao, T., Dollar, P., Singh, M., Mintun, E., Darrell, T., Girshick, R.: Early convo-\\r\\n    lutions help transformers see better. Advances in Neural Information Processing\\r\\n    Systems 34 (2021)\\r\\n60. Xie, S., Gu, J., Guo, D., Qi, C.R., Guibas, L., Litany, O.: Pointcontrast: Unsuper-\\r\\n    vised pre-training for 3d point cloud understanding. In: European conference on\\r\\n    computer vision. pp. 574–591. Springer (2020)\\r\\n61. Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y.: Spidercnn: Deep learning on point sets\\r\\n    with parameterized convolutional filters. In: Proceedings of the European Confer-\\r\\n    ence on Computer Vision (ECCV). pp. 87–102 (2018)\\r\\n62. Yan, S., Yang, Z., Li, H., Guan, L., Kang, H., Hua, G., Huang, Q.: Implicit au-\\r\\n    toencoder for point cloud self-supervised representation learning. arXiv preprint\\r\\n    arXiv:2201.00785 (2022)\\r\\n63. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet:\\r\\n    Generalized autoregressive pretraining for language understanding. Advances in\\r\\n    neural information processing systems 32 (2019)\\r\\n64. Yi, L., Kim, V.G., Ceylan, D., Shen, I.C., Yan, M., Su, H., Lu, C., Huang, Q.,\\r\\n    Sheffer, A., Guibas, L.: A scalable active framework for region annotation in 3d\\r\\n    shape collections. ACM Transactions on Graphics (ToG) 35(6), 1–12 (2016)\\r\\n65. Yu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., Lu, J.: Point-bert: Pre-\\r\\n    training 3d point cloud transformers with masked point modeling. arXiv preprint\\r\\n    arXiv:2111.14819 (2021)\\r\\n66. Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization\\r\\n    strategy to train strong classifiers with localizable features. In: ICCV (2019)\\r\\n67. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: European confer-\\r\\n    ence on computer vision. pp. 649–666. Springer (2016)\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds            25\\r\\n\\r\\n68. Zhang, Z., Girdhar, R., Joulin, A., Misra, I.: Self-supervised pretraining of 3d\\r\\n    features on any point-cloud. In: Proceedings of the IEEE/CVF International Con-\\r\\n    ference on Computer Vision (ICCV). pp. 10252–10263 (October 2021)\\r\\n69. Zhang, Z., Sun, B., Yang, H., Huang, Q.: H3dnet: 3d object detection using hybrid\\r\\n    geometric primitives. In: European Conference on Computer Vision. pp. 311–329.\\r\\n    Springer (2020)\\r\\n70. Zhao, H., Jiang, L., Fu, C.W., Jia, J.: Pointweb: Enhancing local neighborhood\\r\\n    features for point cloud processing. In: Proceedings of the IEEE/CVF conference\\r\\n    on computer vision and pattern recognition. pp. 5565–5573 (2019)\\r\\n71. Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: Proceed-\\r\\n    ings of the IEEE/CVF International Conference on Computer Vision. pp. 16259–\\r\\n    16268 (2021)\\r\\n72. Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data augmenta-\\r\\n    tion. arXiv preprint arXiv:1708.04896 (2017)\\r\\n\\x0c',\n",
       " '                                          Prepared for submission to JHEP\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          Renormalization of gluonic leading-twist Operators in\\r\\n                                          covariant Gauges\\r\\narXiv:2203.11181v1 [hep-ph] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          Giulio Falcioni,a Franz Herzoga\\r\\n                                          a\\r\\n                                              Higgs Centre for Theoretical Physics, School of Physics and Astronomy, The University of Edin-\\r\\n                                              burgh, Edinburgh EH9 3FD, Scotland, UK\\r\\n\\r\\n                                              E-mail: giulio.falcioni@ed.ac.uk, fherzog@ed.ac.uk\\r\\n\\r\\n                                          Abstract: We provide the all-loop structure of gauge-variant operators required for the\\r\\n                                          renormalisation of Green’s functions with insertions of twist-two operators in Yang-Mills\\r\\n                                          theory. Using this structure we work out an explicit basis valid up to 4-loop order for\\r\\n                                          an arbitrary compact simple gauge group. To achieve this we employ a generalised gauge\\r\\n                                          symmetry, originally proposed by Dixon and Taylor, which arises after adding to the Yang-\\r\\n                                          Mills Lagrangian also operators proportional to its equation of motion. Promoting this\\r\\n                                          symmetry to a generalised BRST symmetry allows to generate the ghost operator from\\r\\n                                          a single exact operator in the BRST-generalised sense. We show that our construction\\r\\n                                          complies with the theorems by Joglekar and Lee. We further establish the existence of a\\r\\n                                          generalised anti-BRST symmetry which we employ to derive non-trivial relations among the\\r\\n                                          anomalous dimension matrices of ghost and equation-of-motion operators. For the purpose\\r\\n                                          of demonstration we employ the formalism to compute the N = 2, 4 Mellin moments of the\\r\\n                                          gluonic splitting function up to 4 loops and its N = 6 Mellin moment up to 3 loops, where\\r\\n                                          we also take advantage of additional simplifications of the background field formalism.\\r\\n\\x0c\\n\\nContents\\r\\n\\r\\n1 Introduction                                              2\\r\\n\\r\\n2 Background                                                4\\r\\n  2.1 Yang-Mills Lagrangian                                 4\\r\\n  2.2 Gauge-fixing, Ghosts and BRST                         4\\r\\n  2.3 Gluonic twist-2 operators                             6\\r\\n\\r\\n3 EOM operators and generalised Gauge Symmetry              7\\r\\n  3.1 General formalism                                     7\\r\\n  3.2 Generalised Gauge symmetry                            8\\r\\n  3.3 Explicit construction up to four loops                9\\r\\n\\r\\n4 Ghost operators and generalised BRST symmetry             14\\r\\n  4.1 Generalised BRST symmetry                             14\\r\\n  4.2 Compatibility with the Theorems of Joglekar and Lee   15\\r\\n  4.3 Ghost operators up to four loops                      17\\r\\n  4.4 Generalised anti-BRST symmetry                        18\\r\\n\\r\\n5 Operator Bases Construction for fixed N                   20\\r\\n  5.1 N = 2 operators                                       21\\r\\n  5.2 N = 4 operators                                       21\\r\\n  5.3 N = 6 operators                                       23\\r\\n  5.4 Operators of higher N                                 26\\r\\n\\r\\n6 Background-field formulation                              27\\r\\n  6.1 Bases of operators up to four loops                   29\\r\\n\\r\\n7 Calculations and results                                  30\\r\\n  7.1 Mixing with EOM and Ghost Operators                   31\\r\\n  7.2 Renormalisation of physical operators                 35\\r\\n\\r\\n8 Conclusions                                               37\\r\\n\\r\\nA Computing anomalous dimension in QCD                      39\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                       –1–\\r\\n\\x0c\\n\\n1   Introduction\\r\\n\\r\\nThe increasing precision with which particle collisions are being measured at the Large\\r\\nHadron Collider have pushed theoretical predictions in QCD to the next-to-next-to-next-\\r\\nto-leading order (N3LO) in perturbative QCD. First calculations at this order were com-\\r\\npleted for Higgs boson production [1–6] and the Drell-Yan process [4, 7]. Even 2 → 2\\r\\nreactions may become feasible at this order in the not-too-far future as first results for\\r\\n3-loop dijet production amplitudes have become available [8, 9]. One of the dominant re-\\r\\nmaining theoretical uncertainties associated to such N3LO calculations is now related to\\r\\nthe lack of knowledge of the 4-loop splitting functions, which determine the evolution of\\r\\nthe parton densities at the relevant perturbative order, and which are known completely\\r\\nonly up to three loops [10–28].\\r\\n     Substantial efforts to improve this situation have already been made. In the non-singlet\\r\\nsector, after pioneering calculations of lower moments [29], numerical approximations for\\r\\nthe 4-loop splitting functions are now known to high accuracy. An analytic reconstruction\\r\\nwas achieved in the limit of leading number of colours [30] and for subleading corrections\\r\\nin the number of quark flavors, nf , [31]; the leading nf -contributions were known already\\r\\nfor some time [32] to all orders in perturbation theory. Even some low N -moments at\\r\\nfive loops [33] are already known. In the singlet sector instead only a handful of lower\\r\\nmoments have been calculated so far at the four-loop level [34] and this input was found\\r\\nto be insufficient for reliable numerical approximations.\\r\\n     The reason for why calculations in the non-singlet sector are so much more advanced\\r\\nthan those in the singlet sector is not only due to more numerous and complex Feynman\\r\\ndiagrams but also due to a more powerful framework. This framework is based on the\\r\\noperator product expansion (OPE)[35, 36], which allows the extraction of Mellin moments\\r\\nfrom the anomalous dimensions of leading-twist light-cone operators. While the renormali-\\r\\nsation of such operators entering in the non-singlet sector is relatively straight forward, the\\r\\nrenormalisation of the corresponding gluonic operators in the off-shell formalism (likely the\\r\\nmost promising framework to allow for progress at four loops at this time) is non-trivial due\\r\\nto the mixing into unphysical operators. The mixing into these operators arises from sub-\\r\\ngraphs with external gluons and ghosts which contain the insertion of the singlet operator;\\r\\nschematic examples appearing at four loops are depicted in figure 1.\\r\\n     An explicit basis for these unphysical operators, valid up to the two-loop level, was\\r\\nworked out by Dixon and Taylor already almost fifty years ago [37] and was employed in\\r\\nthe computation of one-loop Mellin moments in the Feynman gauge. Hamberg and Van\\r\\nNeerven about twenty years later managed to successfully employ the same framework to\\r\\nperform calculations at the 2-loop order in dimensional regularisation [13]. This calculation\\r\\nwas also repeated very recently [38]. It is interesting to note that the calculation by\\r\\nHamberg and Van Neerven managed to successfully resolve a number of conflicting results\\r\\n[10–12] which were present at that time because of negligence of the mixing into unphysical\\r\\noperators.\\r\\n     Nevertheless the structure of the basis proposed by Dixon and Taylor remained some-\\r\\nwhat mysterious. This was pointed out in particular by Collins [39], who argued that the\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            –2–\\r\\n\\x0c\\n\\n        (a)                       (b)                        (c)                       (d)\\r\\n\\r\\nFigure 1. The grey blobs denote examples of multi-loop multi-gluon subgraphs contributing at the\\r\\nfour loop order, which contain insertions of the gauge invariant operator, and whose UV-divergences\\r\\nlead to mixing with unphysical operators under renormalisation.\\r\\n\\r\\n\\r\\nunphysical operators appeared to be in conflict with a general theorem which states that\\r\\nthe unphysical operators should be either proportional to the equation of motion (EOM)\\r\\nor BRST-exact. This theorem was in part first conjectured by Kluberg-Stern and Zu-\\r\\nber [40, 41] before it was proven by Joglekar and Lee [42–44]. Another proof based on\\r\\ncohomology theory was later provided by Henneaux [45].\\r\\n     In the present paper we revisit the problem. More concretely we reinterpret the basis\\r\\nof Dixon and Taylor as being build up from EOM and ghost operators. The former are\\r\\nproportional to the EOM of the gauge invariant part of the Yang-Mills Lagrangian. Using\\r\\nthis notion we are able to write down the general form of the EOM operators at arbitrary\\r\\nloop orders. As was noted already by Dixon and Taylor the combined Lagrangian consisting\\r\\nof the Yang-Mills Lagrangian and their EOM operators is invariant under a generalised\\r\\ngauge symmetry. This generalised gauge symmetry can then be promoted to a generalised\\r\\nBRST symmetry. We prove that the generalised BRST transformation is nilpotent and\\r\\nthat the ghost operator can be constructed from a single BRST-exact operator in the\\r\\ngeneralised BRST sense. While the generalised BRST symmetry was in part pointed out\\r\\nalready by Hamberg and Van Neerven in [13], it was not clearly spelt out how to use it to\\r\\nconstruct the ghost Lagrangian. We apply the formalism to work out the explicit form of\\r\\nEOM and ghost operators up to four-loop order. We also show that the basis is in accord\\r\\nwith the theorem of Joglekar and Lee.\\r\\n     To further simplify calculations in the OPE framework we explore two further sym-\\r\\nmetry principles. We observe that the ghost term of the unphysical operator can also be\\r\\ngenerated from a generalised anti-BRST symmetry [46–49] and a corresponding generalised\\r\\nanti-BRST-exact operator. In particular we employ this alternative formulation to derive\\r\\na set of nontrivial identities among the anomalous dimensions of the EOM and ghost op-\\r\\nerators. We also make use of the background field formalism [40, 41, 50–52, 52, 53, 53, 54].\\r\\nBackground field gauge invariance allows one to reduce by one the maximum number of\\r\\nloops at which the anomalous dimensions of the EOM and ghost operators are required -\\r\\nthereby yielding another welcome simplification for the calculation of unphysical countert-\\r\\nerms. For the purpose of demonstration we will employ the formalism to re-calculate the\\r\\nN = 2 and N = 4 moments of the purely gluonic contributions in the singlet sector up to\\r\\n4 loops and the N = 6 moment up to three loops.\\r\\n     In the following we give a brief outline of the paper. In section 2 we summarise\\r\\nour conventions and review some of the relevant background material. The construction\\r\\nof EOM operators and the generalised gauge symmetry is discussed in section 3. The\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              –3–\\r\\n\\x0c\\n\\nconstruction of ghost operators and the generalised BRST and anti-BRST invariance is\\r\\ndiscussed in section 4. There also the compatibility of our construction with the theorems\\r\\nof Joglekar and Lee is shown. The concepts are employed to build an independent basis\\r\\nof gauge-variant operators in section 5 for various fixed values of N . The background-field\\r\\nformulation is presented in section 6 and employed in section 7 for the computation of\\r\\nMellin moments up to 4-loop order. We conclude in section 8.\\r\\n\\r\\n2     Background\\r\\n\\r\\n2.1    Yang-Mills Lagrangian\\r\\nIn the following we summarise our conventions for the Yang Mills Lagrangian. We define\\r\\nthe field strength tensor as\\r\\n                             a\\r\\n                            Fµν = ∂µ Aaν − ∂ν Aaµ + gf abc Abµ Acν ,                   (2.1)\\r\\n\\r\\nsuch that the gauge invariant part of the Yang-Mills action is given by\\r\\n                              Z\\r\\n                                                      1\\r\\n                        S0 = dd x L0 ,        L0 = − Faµν Fµν  a\\r\\n                                                                  .                    (2.2)\\r\\n                                                      4\\r\\nLet us further define the covariant derivative in the adjoint representation,\\r\\n\\r\\n                                  Dµac = ∂µ δac + gf abc Abµ .                         (2.3)\\r\\n\\r\\nWith this definition the EOM of the Yang-Mills Lagrangian is written compactly as\\r\\n                                     δS0\\r\\n                                          = (Dν F νµ )a .                              (2.4)\\r\\n                                     δAµa\\r\\nThe action S0 is of course invariant under infinitesimal gauge transformations\\r\\n\\r\\n                       Aaµ → Aaµ + δω Aaµ ,   with    δω Aaµ = (Dµ ω)a .               (2.5)\\r\\n\\r\\n2.2    Gauge-fixing, Ghosts and BRST\\r\\nThe gauge invariance is broken by the gauge fixing (GF) and ghost (G) terms, the latter\\r\\nbeing required to cancel unphysical degrees of freedom of the gauge field. For the commonly\\r\\nused choice of the linear covariant gauge the gauge-fixing and ghost contributions to the\\r\\nLagrangian are\\r\\n                                        1\\r\\n                           LGF+G = − (∂ µ Aaµ )2 − ca ∂ µ Dµab cb ,                    (2.6)\\r\\n                                        2ξ\\r\\nwhere ca and c̄a are respectively the ghost and anti-ghost fields. The complete gauge-fixed\\r\\nYang-Mills action is then given by\\r\\n                           Z\\r\\n                       S = dd x L ,        with     L = L0 + LGF+G ,                   (2.7)\\r\\n\\r\\nand its EOM is given by\\r\\n                       δS         νµ   1 µ ν a      abc µ b c\\r\\n                         µ = (Dν F )a + ∂ ∂ Aν − gf    (∂ c )c .                       (2.8)\\r\\n                      δAa              ξ\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              –4–\\r\\n\\x0c\\n\\nWhile eq. (2.6) breaks gauge invariance it does remain invariant under nilpotent BRST\\r\\ntransformations. This feature becomes most transparent after the introduction of an aux-\\r\\niliary field ba (x), also known as the Nakanishi-Lautrup field [55, 56]. In this formulation\\r\\nthe Lagrangian is written as\\r\\n                                                 ξ\\r\\n                            LGF+G = −ba ∂ µ Aaµ + (ba )2 − ca ∂ µ Dµab cb .                    (2.9)\\r\\n                                                 2\\r\\nEq. (2.9) can be seen to be equivalent to eq. (2.6) after substituting the solution of the EOM\\r\\nba = 1ξ ∂ µ Aaµ . The BRST variation leaving this Lagrangian invariant [57, 58] is defined as\\r\\n\\r\\n                                         δBRST (•) ≡ θ s(•) ,                                 (2.10)\\r\\n\\r\\nwhere θ is a Grassmann number, s denotes the BRST operator, which being nilpotent\\r\\nsatisfies s2 (•) = 0, and whose action on the fields is given by\\r\\n                                                         g\\r\\n              sba = 0 ,      sAaµ = (Dµ c)a ,     sca = − f abc cb cc ,       sca = −ba .     (2.11)\\r\\n                                                         2\\r\\nThe BRST invariance of eq. (2.9) can be made manifest by writing it in BRST-exact form,\\r\\nthat is as the BRST variation of an ancestor operator:\\r\\n                                                                \\x02                \\x03\\r\\n                  LGF+G = sOancestor ,            Oancestor = ca ∂ µ Aaµ − 12 ξba .           (2.12)\\r\\n\\r\\nAn interpretation of the BRST symmetry is that it corresponds to a certain subclass of\\r\\ngauge transformations, where the parameter ω(x) of the gauge transformation is identified\\r\\nwith the ghost field times a Grassmann number. There exists in fact a second such symme-\\r\\ntry in the gauge-fixed Lagrangian where the role of the ghost field in the BRST variations is\\r\\nreplaced with that of the anti-ghost field. This leads to the so-called anti-BRST symmetry\\r\\n[46–49]. To discuss this symmetry we first introduce another auxiliary field:\\r\\n\\r\\n                                        b̄a = −ba + gf abc c̄b cc                             (2.13)\\r\\n\\r\\nThe anti-BRST variation is then given by\\r\\n\\r\\n                                         δBRST (•) ≡ θ̄ s̄(•) ,                               (2.14)\\r\\n\\r\\nwith θ̄ another Grassmann number and\\r\\n                                                           g\\r\\n              s̄b̄a = 0 ,    s̄Aaµ = (Dµ c̄)a ,   s̄c̄a = − f abc c̄b c̄c ,   s̄ca = −b̄a .   (2.15)\\r\\n                                                           2\\r\\nThe BRST and anti-BRST variations fulfil the following consistency condition:\\r\\n\\r\\n                                 ss(•) = s̄s̄(•) = 0 = s̄s(•) + ss̄(•)                        (2.16)\\r\\n\\r\\nSimilarly to eq. (2.12) the anti-BRST symmetry of eq. (2.9) can be made manifest by\\r\\nwriting it in anti-BRST exact form, that is as the anti-BRST variation of another ancestor\\r\\noperator:\\r\\n                                                          \\x02                \\x03\\r\\n                   LGF+G = s̄ Oancestor ,   Oancestor = ca 21 ξba − ∂ µ Aaµ .       (2.17)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                  –5–\\r\\n\\x0c\\n\\n2.3     Gluonic twist-2 operators\\r\\nLet us now consider the extension of the Yang-Mills Lagrangian to include also a general\\r\\ngauge invariant gluonic twist-2 spin-N operator\\r\\n                               1 \\x02 a1                a     aN−1 aN−1 ;µ \\x03\\r\\n                  )\\r\\n           Oµ(N1 ...µ  (x) =     S F µµ1 Dµa12a2 ...DµN−2\\r\\n                                                       N−1     F       µN + traceless ,                        (2.18)\\r\\n                     N\\r\\n                               2\\r\\nwhere we have indicated\\r\\n\\r\\n      • the sum over all permutations of µ1 , ..., µN via the operation S,\\r\\n                                                                        (N )\\r\\n      • and the presence of further terms which make Oµ1 ...µN (x) traceless, i.e. the sum\\r\\n        vanishes when any two of its Lorentz indices are contracted, by the term ‘+traceless’.\\r\\n\\r\\nA well known trick to simplify this expression is to contract it with N identical light-like\\r\\nvectors which we denote by ∆µ and which satisfy ∆.∆ = 0. It is then conventional to\\r\\nintroduce the notation\\r\\n\\r\\n          F µ;a = ∆ν F µν;a ,        Aa = ∆µ Aµ;a ,         D = ∆µ D µ ,              ∂ = ∆µ ∂ µ .             (2.19)\\r\\n\\r\\nUsing this notation we then define the scalarised version of eq. (2.18):\\r\\n\\r\\n                        (N )                                  1 \\x02              \\x03\\r\\n                                       )\\r\\n                       O1 (x) = Oµ(N1 ...µ  (x)∆ µ1\\r\\n                                                    ...∆ µN\\r\\n                                                            =   Tr Fν D N −2 ν\\r\\n                                                                            F    .                             (2.20)\\r\\n                                          N\\r\\n                                                              2\\r\\n                                              (N )\\r\\nIt is well known that the operator O1 (x) when inserted into general Green’s functions\\r\\nmixes with non-physical operators under renormalisation. A basis for these non-physical\\r\\noperators will be constructed in the following sections consisting of two kinds of operators,\\r\\nnamely operators proportional to the EOM, defined in eq. (2.4), and operators containing\\r\\n                                             (N )                         (N )        (N )\\r\\nghosts (G). We therefore include besides O1 (x) also the operators OEOM and OG (x).\\r\\nThe complete Lagrangian is then given by\\r\\n                                                        (N )    (N )           (N )        (N )\\r\\n                 L̃(A, c̄, c; g, ξ) = L0 + LGF+G + C1          O1 (x) + OEOM + OG (x) ,                        (2.21)\\r\\n          (N )                                                  (N )                                          (N )\\r\\nwhere C1 is the Wilson coefficient associated to O1 (x). The mass dimension of O1 (x),\\r\\n  (N )               (N )\\r\\nOEOM and OG (x) equals the dimension of space-time, d, this is achieved by defining ∆\\r\\nto carry a mass dimension of 2/N − 1.\\r\\n     Let us now briefly discuss the renormalisation of L̃, which in eq. (2.21) was defined\\r\\nin terms of physical or equivalently renormalised fields and couplings. The counterterms\\r\\nrequired to make finite all correlators of the fields Aaµ , ca and c̄a at distinct positions can\\r\\nbe readily generated by replacing the fields and couplings with their bare counterparts in\\r\\nL̃(Ab , c̄b , cb ; gb , ξ b ) with\\r\\n                        1/2\\r\\n         Ab;µ         µ\\r\\n          a (x) = Z3 Aa (x) ,                    c̄ba (x) = Zc1/2 c̄ba (x) ,      cba (x) = Zc1/2 cba (x) ,\\r\\n                 gb = µǫ gZg ,                        ξ b = ξZ3 .                                              (2.22)\\r\\n                                                                                                         (N )\\r\\nThis replacement is not sufficient to renormalise correlators with an insertion of O1 . For\\r\\nthis purpose it is convenient to introduce the vector notation O   ~ (N ) = (O(N ) , ..., On(N ) ),\\r\\n                                                                               1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                     –6–\\r\\n\\x0c\\n\\n         (N )                                                                                       (N )\\r\\nwith O1 defined in eq. (2.20) and the remaining components Oi>1 , to which we asso-\\r\\nciate Wilson coefficients Ci , forming a basis of operators spanning the space of EOM and\\r\\n                   (N )          (N )\\r\\nghost operators OEOM and OG . The required counterterms are obtained by taking into\\r\\naccount the mixing of the operators under renormalisation. This is achieved by making\\r\\nthe replacement\\r\\n                                       (N ),b    (N ) (N )\\r\\n                                      Ci      = Zji Cj .\\r\\nThe bare Lagrangian then takes the form\\r\\n                                                                                              X       (N )    (N )   (N ),b\\r\\n L̃(Ab , c̄b , cb ; Cib , g b , ξ b ) = L0 (Ab ; gb ) + LGF+G (Ab , c̄b , cb ; gb , ξ b ) +         Ci       Zij Oj           (2.23)\\r\\n                                                                                              i,j\\r\\n\\r\\n                 (N ),b\\r\\nwhere the Oi     denote the operators written in terms of bare couplings and fields. It is\\r\\nwell known [40–45] that the structure of Zij is block triangular in that the physical operator\\r\\n (N )                (N )\\r\\nO1 may mix into Oi>1 but not vice versa. This is discussed further in section 4.2.\\r\\n\\r\\n3     EOM operators and generalised Gauge Symmetry\\r\\n\\r\\nFor the sake of keeping the notation as light as possible we will in the following discuss\\r\\nsymmetry properties of the Lagrangian L̃ at the level of renormalised fields and parameters.\\r\\nNote that all of these properties can be directly translated to the bare Lagrangian given\\r\\nthat it has the same functional form.\\r\\n\\r\\n3.1     General formalism\\r\\nIn this section we will elucidate the general structure of the EOM operator. It is well\\r\\nknown that Green’s functions are not invariant under field redefinitions\\r\\n\\r\\n                                    Aaµ → Aaµ + Gµa (Aα , ∂α Aβ , ∂α ∂β Aρ , ...) ,\\r\\n\\r\\nwhere G is a general local, i.e. polynomial, function of the gauge field A and its derivatives.\\r\\nTo leading order in G the variation of the Yang-Mills action in eq. (2.2) can then be written\\r\\nas follows:\\r\\n             Z                        Z\\r\\n                      δS0\\r\\n       δS0 = dd x µ          Gµa (x) = dd x (Dν F νµ )a Gµa (Aα , ∂α Aβ , ∂α ∂β Aρ , ...) . (3.1)\\r\\n                    δAa (x)\\r\\n\\r\\nFor a general form of the function G, as we shall see later, this is actually the most general\\r\\n                                  (N )\\r\\nsuch EOM operator into which O1 can mix under renormalisation, leading us to write\\r\\n                                  (N )\\r\\n                               OEOM = (Dν F νµ )a Gµa (Aα , ∂α Aβ , ∂α ∂β Aρ , ...) .                                          (3.2)\\r\\n\\r\\nA number of constraints on the structure of this EOM operator derive from the overall\\r\\n                                           (N )\\r\\nmass dimension and the twist-2 nature of O1 . This implies that Gµa must be N -linear\\r\\nin ∆ and for the mass dimensions to work out the total number of As and ∂s entering in\\r\\nevery monomial of G must then equal N − 1. It follows that G itself must be proportional\\r\\nto ∆ and that every single A or ∂ entering in G must itself also be contracted with ∆.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            –7–\\r\\n\\x0c\\n\\nThese considerations therefore pin down the general structure of the EOM operator to be\\r\\nas follows:\\r\\n                            (N )\\r\\n                           OEOM = (D.F )a G a (A, ∂A, ∂ 2 A, ...) ,               (3.3)\\r\\nwhere Gµa = ∆µ G a . By expanding G over all possible monomials which satisfy the power\\r\\ncounting constraints we then obtain\\r\\n            ∞\\r\\n            X                                                            X\\r\\n (N )              (N ),k              (N ),k                                                        \\x01           \\x01\\r\\nOEOM =            OEOM      with     OEOM = gk−1 (D · F )a                      Cia;a 1 ..ak\\r\\n                                                                                  1 ..ik\\r\\n                                                                                             ∂ i1 Aa1 .. ∂ ik Aak .\\r\\n            k=1                                                     i1 +..+ik\\r\\n                                                                    =N −k−1\\r\\n                                                                                                             (3.4)\\r\\n\\r\\nHere the coefficients Cia;a   1 ..ak\\r\\n                          1 ..ik\\r\\n                                     are in general color-dependent coupling constants which can\\r\\nbe further decomposed into some basis of group-invariant color structures. Let us for\\r\\nexample consider the case k = 2, whose general decomposition1 can be written as\\r\\n                   (N ),2\\r\\n                                             X                             \\x01        \\x01\\r\\n                 OEOM = g (D · F )a               κi1 i2 f a a1 a2 ∂ i1 Aa1 ∂ i2 Aa2 ,      (3.5)\\r\\n                                                i1 +i2\\r\\n                                                =N −3\\r\\n\\r\\nwhere the κij s are Wilson coefficients, to be discussed further below. Let us also remark\\r\\nthat there exists a general constraint on the C-coefficients which derives from the fact\\r\\nthat the operators are colour singlets. For general k, the coefficients obey the following\\r\\ninvariance relation\\r\\n\\r\\n                      Cib;a 1 ..ak b a x\\r\\n                         1 i2 ..ik\\r\\n                                   f     + Cia;b..a k\\r\\n                                             1 i2 ..ik\\r\\n                                                       f b a1 x + .. + Cia;a1 a2 ..b b ak x\\r\\n                                                                         1 i2 ..ik\\r\\n                                                                                    f       = 0.             (3.6)\\r\\n\\r\\nWe now study the symmetry properties of the lagrangian in eq. (2.21).\\r\\n\\r\\n3.2     Generalised Gauge symmetry\\r\\n                                                                  (N )\\r\\nWhile gauge transformations leave both L0 and O1 invariant, the same can not be said\\r\\nabout the general EOM operator. To cancel its variation we will now contruct a generalised\\r\\ngauge transformation,\\r\\n                              Aaµ → Aaµ + δω Aaµ + δω∆ Aaµ ,                         (3.7)\\r\\n                                                                                             (N )\\r\\nwhere δω∆ is multi-linear in ∆ and is such that the gauge variation of OEOM is cancelled by\\r\\nthe generalised gauge variation of L0 , i.e. δω∆ L0 . This leads to\\r\\n                                                                    (N )\\r\\n                                    (D.F )aµ δω∆ Aµa (x) + δω OEOM = 0 ,                                     (3.8)\\r\\n\\r\\nand combined with eq. (3.3) it then follows that the generalised gauge variation satisfies:\\r\\n\\r\\n                                     δω∆ Aaµ + δω Gµa − g f abc Gµb ω c = 0 .                                (3.9)\\r\\n  1\\r\\n      Note we ignore here the fully symmetric rank 3 tensor dabc as it can not appear in Yang-Mills theory.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         –8–\\r\\n\\x0c\\n\\nUsing eqs. (3.3) and (3.4) we then find the following general solution:\\r\\n                       ∞\\r\\n                       X X                          \\x01                \\x01 X a;aσ(1) ..aσ(k)\\r\\n       δω∆ Aaµ = −∆µ                        ∂ i1 Aa1 .. ∂ ik +1 ω ak    Ciσ(1) ..iσ(k)\\r\\n                       k=1 i1 +..+ik                                 σ∈Zk\\r\\n                            N −k−1\\r\\n                                                                  k\\r\\n                                                                                                       !\\r\\n                         X                  \\x01       \\x01 i +1 a \\x01 X                       im + ik+1 + 1\\r\\n                                      i1 a1   ik ak\\r\\n             + g∆µ                   ∂ A .. ∂ A       ∂ k+1\\r\\n                                                            ω k+1\\r\\n\\r\\n                      i1 +..+ik+1                                               m=1\\r\\n                                                                                            im\\r\\n                        N −k−2\\r\\n                                 a;a ..am−1 bam+1 ..ak b am ak+1\\r\\n                            × Ci1 ..i1m+i k+1 +1..ik\\r\\n                                                      f                                                (3.10)\\r\\n\\r\\nwhere we have used eq. (3.6) and symmetry to cancel all terms which contain ω without\\r\\nderivatives. By collecting terms of identical field content and powers of g we can bring\\r\\neq. (3.10) into the following form\\r\\n                      ∞\\r\\n                      X               X                              \\x01               \\x01            \\x01\\r\\n      δω∆ Aaµ = −∆µ         gk−1                e a;a1 .. ak ∂ i1 Aa1 .. ∂ ik−1 Aak−1 ∂ ik +1 ω ak .\\r\\n                                                C                                                      (3.11)\\r\\n                                                  i1 .. ik\\r\\n                      k=1          i1 +..+ik\\r\\n                                   =N −k+1\\r\\n\\r\\nThe Ce a;a1 .. ak can be extracted from the building blocks of C a;a1 .. ak once a color basis has\\r\\n      i1 .. ik                                                  i1 .. ik\\r\\nbeen specified. We will construct an explicit solution valid up to four loops in the next\\r\\nsubsection.\\r\\n\\r\\n3.3     Explicit construction up to four loops\\r\\nThe loop order puts stringent constraints on the type of the EOM operators actually\\r\\n                                                                                           (N )\\r\\nrequired. The quantity from which we wish to extract the anomalous dimension of O1\\r\\n                                                                                   (N )\\r\\nis naturally the gluon 2-point 1PI correlator with an insertion of the operator O1 :\\r\\n                              Z\\r\\n                 (N ) µν                                            (N )\\r\\n              (Γ1;gg )ab (Q) = dd x dd z eiQ.x h0|T {Aµa (x)Aνb (0)O1 (z)}|0i1PI .      (3.12)\\r\\n\\r\\nAt one-loop there are no subdivergences and we only require counterterms with two external\\r\\n                                                       (N )        (N ),1\\r\\ngluons. We thus only need the one-loop mixing of O1 into OEOM , as this is the only\\r\\nEOM operator contributing to the two gluon vertex. At two loops we then require two-loop\\r\\n             (N )       (N ),1                          (N )        (N ),2\\r\\nmixing of O1 into OEOM , and one-loop mixing of O1 into OEOM , given that at two\\r\\nloops we can have one-loop subgraphs with three external gluons. This reasoning can be\\r\\ncontinued at higher loop orders leading to more EOM operators. Diagrams with subgraphs\\r\\nhighlighting this pattern are shown in table 1 and the corresponding loop numbers, from\\r\\nwhich we require certain EOMs, are also summarised again in table 2. We can therefore\\r\\nignore terms in eq. (3.3) from k = 4 onwards leading to the following set of EOM operators\\r\\nrequired up to 4 loops:\\r\\n                                   (N ),1\\r\\n                              OEOM = η (D.F )a ∂ N −2 Aa                                               (3.13)\\r\\n\\r\\n\\r\\n                                   (N ),2\\r\\n                                                         X\\r\\n                              OEOM = g(D.F )a                       (∂ A )(∂ j Ac )\\r\\n                                                                 abc i b\\r\\n                                                                Cij                                    (3.14)\\r\\n                                                         i+j=\\r\\n                                                         N −3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                        –9–\\r\\n\\x0c\\n\\n               (N ),≤1                       (N ),≤2                        (N ),≤3              (N ),≤4\\r\\n L           OEOM                         OEOM                          OEOM                    OEOM\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n 1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n 2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n 3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n 4\\r\\n\\r\\n\\r\\n\\r\\nTable 1. In the Lth row the table gives examples of diagrams contributing to the L-loop contri-\\r\\n           (N )                                                                       (N ),k\\r\\nbution to Γgg . Subgraphs whose UV-counterterms require the various EOM operators OEOM are\\r\\nhighlighted with dashed boxes.\\r\\n\\r\\n                  (N ),3\\r\\n                                         X\\r\\n                OEOM = g2 (D.F )a                abcd\\r\\n                                                Cijk  (∂ i Ab )(∂ j Ac )(∂ k Ad )                          (3.15)\\r\\n                                        i+j+k\\r\\n                                        =N −4\\r\\n                  (N ),4\\r\\n                                          X\\r\\n                OEOM = g3 (D.F )a                  abcde\\r\\n                                                  Cijkl  (∂ i Ab )(∂ j Ac )(∂ k Ad )(∂ l Ae )              (3.16)\\r\\n                                        i+j+k+l\\r\\n                                         =N −5\\r\\n\\r\\nLet us now discuss the color decomposition of the C-coefficients. While at rank two and\\r\\nthree possible color structures are limited to δab and f abc , color decompositions for operators\\r\\nof higher rank are in general non-trivial, in particular when keeping the color gauge group\\r\\ngeneral as we do here. However the fact that we only require counterterms valid up to\\r\\ncertain loop orders imposes strong contstraints and allows us to identify the following color\\r\\ndecompositions:\\r\\n                              abc\\r\\n                             Cij  = f abc κij                                                              (3.17)\\r\\n                            abcd                  (1)           (2)             (3)\\r\\n                           Cijk  = (f f )abcd κijk + dabcd\\r\\n                                                      4    κijk + dabcd\\r\\n                                                                   d\\r\\n                                                                   4f\\r\\n                                                                        κ\\r\\n                                                                      f ijk\\r\\n                                                                                                           (3.18)\\r\\n                            abcde                   (1)               (2)\\r\\n                           Cijkl  = (f f f )abcde κijkl + dabcde\\r\\n                                                           4f    κijkl ,                                   (3.19)\\r\\n\\r\\nwhere the different color structures are defined as\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                  – 10 –\\r\\n\\x0c\\n\\n                                       (N ),1    (N ),2       (N ),3     (N ),4\\r\\n                               L      OEOM      OEOM        OEOM       OEOM\\r\\n                                1       1         0            0          0\\r\\n                                2       2         1            0          0\\r\\n                                3       3         2            1          0\\r\\n                                4       4         3            2          1\\r\\n\\r\\n                                                                                  (N )          (N ),k\\r\\nTable 2. The table summarizes the loop orders for which the mixing of O1                 into OEOM is required,\\r\\ngiven a certain loop order of ΓN\\r\\n                               gg .\\r\\n\\r\\n\\r\\n\\r\\n         (f f )abcd = f abe f cde ,                       (f f f )abcde = f abm f mcn f nde ,\\r\\n            dabcde\\r\\n             4f    = dabcm\\r\\n                      4    f mde ,                              dabcd    abmn mce edn\\r\\n                                                                 4f f = d4   f   f    ,                  (3.20)\\r\\n                              1\\r\\n             dabcd\\r\\n              d    = dabcd\\r\\n                      4f f − CA d4\\r\\n                                   abcd\\r\\n                                        ,\\r\\n              4f f            3\\r\\nand the symmetrised trace is defined by\\r\\n                               1\\r\\n                    dabcd\\r\\n                     4    =       [Tr(TAa TAb TAc TAd ) + symmetric permutations] ,                      (3.21)\\r\\n                               4!\\r\\nwhere (TA )bac = if abc . Going beyond four loops would not only require further operators,\\r\\n       (N ),k>4\\r\\ni.e. OEOM , but also further color structures in the definitions of Cijk and Cijkl . In fact to\\r\\narbitrary loop-order, there are arbitrarily many independent color structures contributing\\r\\nto Cijk and Cijkl . If one was to work in a fixed gauge group this task would be far\\r\\nsimpler. For instance in SU(Nc ) we know that the complete basis at 4 and 5 points is\\r\\nexpressible in terms of single and double traces of permutations of the generators in the\\r\\nfundamental representation. The penalty for working in an arbitrary gauge group thus\\r\\nbecomes increasingly higher at higher loops, but is still mild at the four-loop level.\\r\\n     Let us now come to the definition of the sums appearing in eqs. (3.13)-(3.16). These\\r\\nare defined such that we sum over all non-negative integer values of the indices i, j, k, l,\\r\\nappearing in the sum which satisfy the respective constraint, e.g. i + j + k = N − 4. While\\r\\nthis sum notation leads to reasonably compact definitions of the EOM operators, it does also\\r\\nlead to overcounting. For instance in the order g term we sum over all indices i+j = N −3,\\r\\nbut since the associated color tensor, f abc , is asymmetric under exchange of b and c the\\r\\noperators appearing in the sum for j > i are related to those with j < i. To compensate\\r\\nthis over-counting of independent operators we impose relations on the κ-coefficients. In\\r\\ngeneral there exists a lot of freedom in how to choose these relations. A particularly\\r\\nconvenient choice of constraints is obtained by demanding the κ-coefficients to satisfy\\r\\nthe same relations as their respective color factors. That this works can be understood as\\r\\nfollows. If we were to use up all the color identities, we would clearly land in an independent\\r\\nbasis of operators. By imposing the same identities on the κ-coefficients, we effectively\\r\\nensure that solving these identities would lead to the right degrees of freedom - that is the\\r\\nright number of independent κ-coefficients. This choice is in spirit not dissimilar to the\\r\\nBCJ-choice of numerators [59] for Feynman diagrams where the numerators of Feynman\\r\\ndiagrams are chosen such that they satisfy the same constraints as the corresponding color\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                  – 11 –\\r\\n\\x0c\\n\\nfactors. Here however the motivation is solely to make manipulations with these operators\\r\\nmore manageable.\\r\\n     These considerations then finally lead us to impose the following relations on the κ-\\r\\ncoefficients:\\r\\n\\r\\n        κij + κji = 0,                                          (antisymmetry of f )      (3.22)\\r\\n         (1)     (1)\\r\\n        κijk + κikj = 0,                                        (antisymmetry of f )      (3.23)\\r\\n         (1)      (1)\\r\\n        κijkl + κijlk = 0,                                      (antisymmetry of f )      (3.24)\\r\\n         (1)     (1)    (1)\\r\\n        κijk + κjki + κkij = 0,                                                (Jacobi)   (3.25)\\r\\n         (1)      (1)     (1)\\r\\n        κijkl + κiklj + κiljk = 0,                                             (Jacobi)   (3.26)\\r\\n         (1)      (1)     (1)    (1)\\r\\n        κijkl + κjilk + κlkji + κklij = 0,                            (double Jacobi)     (3.27)\\r\\n         (2)     (2)     (2)    (2)     (2) (2)\\r\\n        κijk = κjik = κikj = κkji = κjki = κkij ,                   (symmetry of d4 )     (3.28)\\r\\n         (3)     (3)\\r\\n        κijk = κikj ,                                           (antisymmetry of f )      (3.29)\\r\\n         (3)     (3)    (3)\\r\\n        κijk + κjki + κkij = 0,                                 (generalised Jacobi )     (3.30)\\r\\n         (2)      (2)\\r\\n        κijkl + κijlk = 0,                                      (antisymmetry of f )      (3.31)\\r\\n         (2)      (2)\\r\\n        κijkl = κjikl ,                                             (symmetry of d4 )     (3.32)\\r\\n\\r\\nAn independent set of operators is then found for any given N by solving these relations.\\r\\nFixing N this is in principle a straight forward exercise, but is somewat difficult to do\\r\\nkeeping N general.\\r\\n     We now give the color identities which lead to eqs. (3.22)-(3.32). The Jacobi relation\\r\\nis as usual,\\r\\n                           (f f )abcd + (f f )acdb + (f f )adbc = 0 .                (3.33)\\r\\nBy the double Jacobi relation we refer to the identity\\r\\n\\r\\n                  (f f f )abcde + (f f f )acbed + (f f f )adebc + (f f f )aedcb = 0 ,     (3.34)\\r\\n\\r\\nwhich itself can be derived by repeated use of the Jacobi relation. Another consequence of\\r\\nthe Jacobi relation is what is sometimes refered to as a generalised Jacobi relation [60]:\\r\\n\\r\\n                             dabcde\\r\\n                              4f    + dbcdae\\r\\n                                       4f    + dcdabe\\r\\n                                                4f    + ddabce\\r\\n                                                         4f    = 0.                       (3.35)\\r\\n                                                                                 (2)\\r\\nThis identity does not lead to any relations among the coefficients κijkl , since we fix the\\r\\nposition of the index a, which contracts the EOM, to be in the d4 . The relation would\\r\\nconnect it to operators where the a would be attached to the corresponding f . However\\r\\ncontracting this relation with f deg leads to\\r\\n\\r\\n                             CA dabcd\\r\\n                                 4    + dabcd    bcad    cabd\\r\\n                                         4f f + d4f f + d4f f = 0 .                       (3.36)\\r\\n\\r\\nCombining this equation with its permutations, and using the symmetry properties,\\r\\n\\r\\n                                 dabcd    abdc    bacd    badc\\r\\n                                  4f f = d4f f = d4f f = d4f f ,                          (3.37)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                               – 12 –\\r\\n\\x0c\\n\\nwhich follow directly from the definition in eq. (3.20), one can further derive the less obvious\\r\\nrelation\\r\\n                                        dabcd      cdab\\r\\n                                         4f f = d4f f .                                   (3.38)\\r\\nCombining eqs. (3.36)-(3.38) we then derive\\r\\n\\r\\n                                         CA dabcd  abcd    acdb    adbc\\r\\n                                             4f + d4f f + d4f f + d4f f = 0 .                                                            (3.39)\\r\\n\\r\\nThis relation implies that operators with color structures d4 and d4f f are linearly depen-\\r\\ndent. To avoid this undesirable feature we introduced the modified color factor d4f\\r\\n                                                                                  d f\\r\\n                                                                                      which\\r\\nsatisfies\\r\\n                                 dabcd\\r\\n                                  d\\r\\n                                  4f f\\r\\n                                       + dacdb\\r\\n                                          d\\r\\n                                          4f f\\r\\n                                               + dadbc\\r\\n                                                  d\\r\\n                                                  4f f\\r\\n                                                       = 0,                           (3.40)\\r\\n\\r\\nand is therefore independent of dabcd\\r\\n                                    4f . Having discussed an explicit basis of the EOM\\r\\noperators and their color structures we can now consider the generalised gauge invariance\\r\\ndiscussed in section 3.2. To 4-loop order the ∆-dependent part of the transformation reads\\r\\n                    \"\\r\\n                                 \\x01     X a;a a            \\x01             \\x01\\r\\n      δw Aµ = −∆µ η ∂ N −1 ω a + g\\r\\n       ∆ a                                  e 1 2 ∂ i1 Aa1 ∂ i2 +1 ω a2\\r\\n                                            C                         i1 i2\\r\\n                                                       i1 +i2\\r\\n                                                       =N −3\\r\\n                           X                                  \\x01                  \\x01                   \\x01\\r\\n               + g2                   e a;a1 a2 a3 ∂ i1 Aa1\\r\\n                                      C                               ∂ i2 Aa2        ∂ i3 +1 ω a3                                       (3.41)\\r\\n                                        i1 i2 i3\\r\\n                         i1 +i2 +i3\\r\\n                          =N −4\\r\\n                                                                                                                                     #\\r\\n                           X                                      \\x01                   \\x01             \\x01                   \\x01\\r\\n               +g    3               e a;a1 a2 a3 a4\\r\\n                                     C                  i1\\r\\n                                                       ∂ A   a1\\r\\n                                                                        ∂ A i2   a2       i3\\r\\n                                                                                          ∂ A  a3\\r\\n                                                                                                         ∂   i4 +1 a4\\r\\n                                                                                                                   ω             4\\r\\n                                                                                                                            + O(g ) ,\\r\\n                                       i1 i2 i3 i4\\r\\n                         i1 +..+i4\\r\\n                          =N −5\\r\\n\\r\\n      e a;a1 ..an involve the same colour structures which appear in eqs. (3.13)-(3.16)\\r\\nwhere C i1 ..in\\r\\n\\r\\n        e a;a1 a2 = η (1) f a;a1 a2 ,\\r\\n        C                                                                                                                                (3.42)\\r\\n          i1 i2      i1 i2\\r\\n      e a;a1 a2 a3 = η (1) (f f )aa1 a2 a3 + η (2) daa1 a2 a3 + η (3) daa1 a2 a3 ,\\r\\n      C                                                                                                                                  (3.43)\\r\\n        i1 i2 i3      i1 i2 i3                i1 i2 i3           i1 i2 i3 4f\\r\\n                                                                          d  f\\r\\n    e a;a1 a2 a3 a4 = η (1)\\r\\n    C                                      aa1 a2 a3 a4    (2a)\\r\\n                                                        + ηi1 i2 i3 i4 daa 1 a2 a3 a4    (2b)\\r\\n                                                                                      + ηi1 i2 i3 i4 daa 4 a1 a2 a3\\r\\n      i1 i2 i3 i4      i1 i2 i3 i4 (f f f )                             4f                            4f            .                    (3.44)\\r\\n\\r\\n                         (k)                                                                                 (k)\\r\\nThe coefficients ηi1 ..in are then fixed in terms of the coefficients κi1 ..in of eqs. (3.13)-(3.16)\\r\\nby means of eq. (3.10) and eq. (3.11). We obtain the following relations\\r\\n                                                       \\x10i +i +1\\x11\\r\\n                               (1)                      1         2\\r\\n                           ηi1 i2 = 2 κi1 i2 + η                                 ,                                                       (3.45)\\r\\n                                                                 i1\\r\\n                                                        \\x10i +i +1\\x11                        h                     i\\r\\n                           (1)                               2          3                  (1)         (1)\\r\\n                         ηi1 i2 i3 = 2κi1 (i2 +i3 +1)                                 + 2 κi1 i2 i3 + κi3 i2 i1 ,                        (3.46)\\r\\n                                                                      i2\\r\\n                           (2)             (2)\\r\\n                         ηi1 i2 i3 = 3κi1 i2 i3 ,                                                                                        (3.47)\\r\\n                                      h                     i\\r\\n                          (3)             (3)       (3)\\r\\n                         ηi1 i2 i3 = 2 κi1 i2 i3 − κi3 i2 i1 ,                                                                           (3.48)\\r\\n                                     h                                       i\\x10i +i +1\\x11\\r\\n                      (1)              (1)                  (1)                 3  4\\r\\n                     ηi1 i2 i3 i4 = 2 κi1 i2 (i3 +i4 +1) + κ(i3 +i4 +1)i2 i1\\r\\n                                                                                  i3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 – 13 –\\r\\n\\x0c\\n\\n                               h                                                         i\\r\\n                                 (1)            (1)            (1)            (1)\\r\\n                            + 2 κi1 i2 i3 i4 + κi1 i4 i3 i2 + κi4 i1 i3 i2 + κi4 i3 i1 i2 ,             (3.49)\\r\\n                                                      \\x10i +i +1\\x11\\r\\n                 (2a)              (2)                   3     4              (2)\\r\\n                ηi1 i2 i3 i4 = 3κi1 i2 (i3 +i4 +1)                     + 2 κi1 i2 i3 i4 ,               (3.50)\\r\\n                                                             i3\\r\\n                 (2b)               (2)\\r\\n                ηi1 i2 i3 i4 = 2 κi4 i1 i2 i3 .                                                         (3.51)\\r\\n\\r\\nThe use and power of these relations will become clear in the next section, where we discuss\\r\\nhow the generalised gauge symmetry is promoted to a generalised BRST symmetry.\\r\\n\\r\\n4     Ghost operators and generalised BRST symmetry\\r\\n\\r\\n4.1    Generalised BRST symmetry\\r\\nThe main virtue of the generalised gauge transformation, δω + δω∆ , which we established in\\r\\nsection 3.2, is that we can promote it to a generalised BRST (gBRST) transformation:\\r\\n                                  ′\\r\\n                                 δBRST (•) ≡ θ s′ (•) ,             s′ = s + s∆ .                        (4.1)\\r\\n\\r\\nHere s is the action of the usual BRST transformation and s∆ is the new ∆-dependent\\r\\npart. To define the action of this symmetry on the fields we follow [15, 42]. The only non-\\r\\nvanishing action is the variation of the gauge field. It is constructed simply by replacing\\r\\nthe gauge parameter ω(x)a in eq. (3.41) with the ghost field c(x)a . We thus obtain\\r\\n\\r\\n                               s ∆ ba = 0 ,            s∆ ca = 0,       s∆ ca = 0 ,                      (4.2)\\r\\n\\r\\nand\\r\\n                     ∞\\r\\n                     X               X                                 \\x01               \\x01           \\x01\\r\\n      s∆ Aaµ = −∆µ         gk−1                   e a;a1 .. a4 ∂ i1 Aa1 .. ∂ ik−1 Aak−1 ∂ ik +1 cak .\\r\\n                                                  C                                                      (4.3)\\r\\n                                                    i1 .. i4\\r\\n                     k=0          i1 +..+ik\\r\\n                                  =N −k+1\\r\\n\\r\\nFurthermore eq. (3.9) can be promoted to an idenitity for the corresponding BRST varia-\\r\\ntions:\\r\\n                           s∆ Aµa (x) + sGaµ − g fabc Gbµ cc = 0 ,                (4.4)\\r\\nThis relation is very useful. For instance it allows us to show that s′ is nilpotent. In the\\r\\nfollowing we prove this up to terms of order ∆2 , which is all we require for the renor-\\r\\nmalisation of Green’s functions with single insertions of twist-2 operators. First we note\\r\\nthat\\r\\n                           2\\r\\n                         s′ = s2 + ss∆ + s∆ s + O(∆2 ) = O(∆2 ) .                      (4.5)\\r\\nGiven s2 = 0 we therefore require\\r\\n\\r\\n                                                   ss∆ + s∆ s = 0 .                                      (4.6)\\r\\n\\r\\nTo prove this identity it is sufficient to show that it holds for the gauge field. We start\\r\\nwith\\r\\n\\r\\n                        ss∆ Aaµ = −s(sGµa − g f abc Gµb cc ) = g f abc s(Gµb cc )\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         – 14 –\\r\\n\\x0c\\n\\n                                                   g2 b abc cde d e\\r\\n                               = g f abc sGµb cc −    G f f c c                           (4.7)\\r\\n                                                    2 µ\\r\\n                               = g f abc sGµb cc − g2 f abc f cde Gµd cb ce\\r\\n\\r\\nwhere we used eq. (4.4) and nilpotence of s in the first line and the Jacobi identity and\\r\\nindex relabeling to get to the last line. Next we now consider\\r\\n\\r\\n                   s∆ sAaµ = s∆ (Dµac cc ) = gf abc s∆ Abµ cc\\r\\n                           = −gf abc sGµb cc + g2 f abc f bde Gµd ce cc                   (4.8)\\r\\n                           = −gf abc sGµb cc + g2 f abc f cde Gµd cb ce = −ss∆ Aaµ\\r\\n\\r\\nThis proves eq. (4.5). The nilpotence is thus a direct consequence of the generalised gauge\\r\\ninvariance.\\r\\n     Let us now come to the general form of the gauge-fixing+ghost Lagrangian required\\r\\n                                                                      (N )\\r\\nto renormalise arbitrary Green’s functions with single insertions of O1 . We propose that\\r\\nit can be represented as follows:\\r\\n\\r\\n                                        L′GF +G = s′ Oancestor ,                          (4.9)\\r\\n\\r\\nwhere the ancestor is the same one which appears in the usual gauge-fixing and ghost term\\r\\nrequired for the Yang-Mills Lagrangian; that is the one we defined in eq. (2.12). Expanding\\r\\nout s′ we thus obtain\\r\\n                                                        (N )\\r\\n                                L′GF +G = LGF +G + OG                                 (4.10)\\r\\nwith\\r\\n                                (N )\\r\\n                              OG       = s∆ Oancestor = −ca ∂ µ s∆ Aaµ .                 (4.11)\\r\\nWe can then rewrite the complete Lagrangian, introduced in eq. (2.21), as\\r\\n\\r\\n                                         L̃ = LEGI + L′GF +G ,                           (4.12)\\r\\n\\r\\nwith\\r\\n                                                       (N )      (N )\\r\\n                                 LEGI = LYM + O1              + OEOM .                   (4.13)\\r\\nIn this formulation the Lagrangian is then manifestly invariant under the generalised BRST\\r\\n                   ′\\r\\ntransformation δBRST    . For LEGI this follows immediately from its invariance under gener-\\r\\nalised gauge transformations. And given the nilpotence of s′ it also follows that L′GF +G is\\r\\ninvariant under the symmetry, as it lies in the image of s′ . Instead LEGI lies in the kernel of\\r\\ns′ . The cohomology of the generalised BRST transformation, defined as the kernel modulo\\r\\nthe image of s′ , is thus unaffected of the details of the gauge fixing function - an important\\r\\nfeature which underlies also the usual BRST symmetry.\\r\\n\\r\\n4.2    Compatibility with the Theorems of Joglekar and Lee\\r\\nLet us now come to an important issue concerning the mixing between gauge invariant\\r\\nand gauge variant operators. For physics to be independent of the gauge variant operators\\r\\n                             (N )\\r\\nthe renormalisation matrix Zij should be block triangular. This theorem was proven by\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 – 15 –\\r\\n\\x0c\\n\\nJoglekar and Lee [42] and states, essentially, that the block triangular structure is present\\r\\nas long as the unphysical operators belong to two different operator classes2 :\\r\\n\\r\\n       • Class I operators:\\r\\n                                              δS δF [A, c, c̄]\\r\\n                                    OI =                       + sF [A, c, c̄]                    (4.14)\\r\\n                                             δAµa δ(∂ µ c̄a )\\r\\n       • Class II operators:\\r\\n                                                      δS a\\r\\n                                              OII =       X [A, c, c̄]                            (4.15)\\r\\n                                                      δca\\r\\nwhere X and F are local (polynomial) functionals of the fields. For our construction this\\r\\n                           (N )\\r\\nwould then imply that Zj1 = 0. However it is not obvious that the ghost and EOM\\r\\noperators presented here fall into these classes. We will now show that they do. Using\\r\\neq. (4.4) we can write eq. (4.11) as follows:\\r\\n                                 (N )\\r\\n                               OG       = ca ∂ µ sGµa − gf abc ca ∂ µ Gµb cc .                    (4.16)\\r\\n\\r\\nUsing now that s(c̄a ∂ µ Gµa ) = ∂µ ba G µ;a + ∂µ c̄a sGaµ and the EOM of the ba -field, eq. (4.16)\\r\\nbecomes\\r\\n                     (N )                     \\x021                         \\x03\\r\\n                  OG = −s(c̄a ∂G a ) + (∂∂ν Ab;ν ) + gf abc (∂ca )cc G b .                   (4.17)\\r\\n                                               ξ\\r\\nCombining this expression with eq. (3.3) and eq. (2.8) we thus obtain\\r\\n                                 (N )       (N )                     δS a\\r\\n                               OG + OEOM = s(∂c̄a G a ) +               G .                       (4.18)\\r\\n                                                                    δAa\\r\\nIt is thus apparent that our expressions for the ghost and EOM operators are just Class\\r\\nI operators, and therefore comply with the theorems of Joglekar and Lee, if we identify\\r\\nF = ∂c̄a G a in eq. (4.14). In our case the class II operators can not actually contribute due\\r\\n                                                                  δS\\r\\nto the leading twist nature. This follows as the ghost EOM δc      a is already twist 3. For a\\r\\n                  a\\r\\nsimilar reason G can also not depend on ghost and anti-ghost fields at twist two.\\r\\n                                                         (N )\\r\\n     The structure of the renormalisation matrix Zi j is therefore, by the theorem of\\r\\nJoglekar and Lee, expected to be of the form\\r\\n                                        \\uf8eb                       \\uf8f6\\r\\n                                            (N )   (N )    (N )\\r\\n                                          Z1 1 Z1 2 .. Z1 n\\r\\n                                        \\uf8ec                       \\uf8f7\\r\\n                                        \\uf8ec          (N )\\r\\n                                                                \\uf8f7\\r\\n                                                           (N ) \\uf8f7\\r\\n                                  (N )  \\uf8ec 0      Z2 2 .. Z2 n \\uf8f7 .\\r\\n                                Z      =\\uf8ec                                                (4.19)\\r\\n                                        \\uf8ec                       \\uf8f7\\r\\n                                        \\uf8ed ..       ...     .. \\uf8f8\\r\\n                                                   (N )    (N )\\r\\n                                            0    Zn 2 .. Zn n\\r\\n              (N )                                                    (N )\\r\\nSo while O1 may mix into the unphysical operators Oi>1 , the unphysical operators can\\r\\nonly mix among themselves. For calculations of physical quantities, such as S-matrix ele-\\r\\n                                              (N )\\r\\nments, it is thus fully sufficient to know Z1 1 . We require Z1 i>1 only when renormalising\\r\\n                                         (N )               (N )\\r\\nGreen’s functions with insertions of O1 . Instead the Zi>1 j>1 are only required for cal-\\r\\nculations of Green’s functions with insertions of the unphysical operators.\\r\\n   2\\r\\n     Note that this is slightly different from the classification into EOM and BRST-exact operators which\\r\\nis often stated and which can be found for instance in [39].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                   – 16 –\\r\\n\\x0c\\n\\n4.3     Ghost operators up to four loops\\r\\nWe will now work out the structure of the ghost operator, given in eq. (4.11) through\\r\\nfour-loop order. This requires the BRST variation of the gauge field up to 4 loops, which\\r\\nis given by\\r\\n                 \"\\r\\n                            \\x01      X a;a a            \\x01          \\x01\\r\\n   s∆ Aaµ = −∆µ η ∂ N −1 ca + g       e 1 2 ∂ i1 Aa1 ∂ i2 +1 ca2\\r\\n                                      C                       i1 i2\\r\\n                                                 i1 +i2\\r\\n                                                 =N −3\\r\\n                     X                                    \\x01               \\x01                  \\x01\\r\\n            + g2                e a;a1 a2 a3 ∂ i1 Aa1\\r\\n                                C                             ∂ i2 Aa2        ∂ i3 +1 ca3\\r\\n                                  i1 i2 i3\\r\\n                   i1 +i2 +i3\\r\\n                    =N −4\\r\\n                                                                                                 #\\r\\n                     X                                   \\x01        \\x01        \\x01           \\x01\\r\\n            + g3               e a;a1 a2 a3 a4\\r\\n                               C                 ∂ i1 Aa1 ∂ i2 Aa2 ∂ i3 Aa3 ∂ i4 +1 ca4 + O(g 4 ) ,                             (4.20)\\r\\n                                 i1 i2 i3 i4\\r\\n                   i1 +..+i4\\r\\n                    =N −5\\r\\n\\r\\n                      e a;a1 a2 .. defined in eqs. (3.42)-(3.44) in terms of a range of η-coefficients,\\r\\nwith the coefficients C i1 i2 ..\\r\\nwhich in turn are related to the κ-coefficients, defined in eqs. (3.45)-(3.51) and which are\\r\\nattributed to the EOM operators. The ghost operator for arbitrary N as required for\\r\\ncalculations up to the four loop level is thus determined to be\\r\\n                                            (N )\\r\\n                                                    X (N ),k\\r\\n                                          OG =         OG ,                                     (4.21)\\r\\n                                                                      k\\r\\n\\r\\nwith\\r\\n       (N ),1                      \\x01\\r\\n      OG      = −η (∂ca ) ∂ N −1 ca ,                                                                                           (4.22)\\r\\n       (N ),2\\r\\n                    X a;a a                   \\x01           \\x01\\r\\n      OG      = −g       e 1 2 (∂ca ) ∂ i1 Aa1 ∂ i2 +1 ca2 ,\\r\\n                         C                                                                                                      (4.23)\\r\\n                           i1 i2\\r\\n                      i1 +i2\\r\\n                      =N −3\\r\\n       (N ),3\\r\\n                          X                                               \\x01              \\x01              \\x01\\r\\n      OG        = −g2                e a;a1 a2 a3 (∂ca ) ∂ i1 Aa1\\r\\n                                     C                                        ∂ i2 Aa2       ∂ i3 +1 ca3 ,                      (4.24)\\r\\n                                       i1 i2 i3\\r\\n                        i1 +i2 +i3\\r\\n                         =N −4\\r\\n       (N ),4\\r\\n                          X                                                   \\x01              \\x01              \\x01              \\x01\\r\\n      OG        = −g3                e a;a1 a2 a3 a4 (∂ca ) ∂ i1 Aa1\\r\\n                                     C                                            ∂ i2 Aa2       ∂ i3 Aa3       ∂ i4 +1 ca4 .   (4.25)\\r\\n                                       i1 i2 i3 i4\\r\\n                        i1 +..+i4\\r\\n                         =N −5\\r\\n\\r\\nThe ghost operator is therefore completely determined by the generalised BRST symme-\\r\\ntry, or, equivalently, by the generalised gauge invariance and the particular form of the\\r\\ngauge-fixing term. Since the couplings appearing in eq. (4.21) are determined as linear\\r\\ncombinations of an independent set of the κ-couplings of the EOM operators we can effec-\\r\\ntively combine the independent parts of the ghost operator with those of the different EOM\\r\\noperators, collecting terms together which share common κ-coupling coefficients, into what\\r\\nJoglekar and Lee called Class I operators.\\r\\n     One welcome result is thus that the generalised BRST symmetry vastly reduces the\\r\\nindependent set of operators one needs to consider. This was of course already oberserved\\r\\nin [37] and [13] although it was accounted for in slightly different ways. In [37] the relations\\r\\namong the couplings were derived by enforcing the Lie algebra structure on the generalised\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                               – 17 –\\r\\n\\x0c\\n\\ngauge invariance. Instead in [13] they followed from the generalised BRST invariance of\\r\\nthe complete Lagrangian. We like to stress here that in both these references the basis was\\r\\nonly considered to two-loop level, and that no connection to EOM operators and BRST\\r\\nexact operators was made. The explicit form of the gauge variant operators and their\\r\\nconnection to the ghost operators was thus rather non-trivial and somewhat mysterious.\\r\\nWe hope that our presentation finally sheds some light into this long-standing puzzle.\\r\\n    Another advantage of the formalism is that in order to compute the full anomalous\\r\\n                                                                  (N )\\r\\ndimension mixing matrix we only need to consider the mixing of O1 into ghost operators,\\r\\nwhich depending on the method of computation may also require the mixing of the ghost\\r\\noperators among themselves. This is of course much easier to compute then the mixing\\r\\n     (N )\\r\\nof O1 into the EOM operators, whose renormalisation would naively require the com-\\r\\nputation of multi-gluon correlators. Instead the anomalous dimensions of ghost operators\\r\\ncan be extracted from multi-gluon correlators with a ghost anti-ghost pair; which yields a\\r\\nwelcome reduction of complexity. This point will be discussed in more detail in section 7\\r\\nwith reference to specific examples.\\r\\n\\r\\n4.4   Generalised anti-BRST symmetry\\r\\nIn the following we will discuss a rather remarkable fact: there exists a second formulation of\\r\\nthe generalised gauge-fixing and ghost lagrangian L′GF +G introduced in eq. (4.10). Rather\\r\\nthan writing it as a gBRST-exact operator we can write it as an anti-gBRST exact operator\\r\\nwith the anti-ancestor operator defined in eq. (2.17):\\r\\n                                                                                          (N )\\r\\n                                      L′GF +G = s̄′ Ōancestor = LGF +G + OG .                                                    (4.26)\\r\\n\\r\\nwhere s̄′ = s̄ + s̄∆ and the anti-gBRST transformation is defined as a generalised gauge\\r\\ntransformation with ω(x) → c̄(x), that is\\r\\n\\r\\n       s∆ ca = 0,              s∆ c̄a = 0,         s∆ b̄a = 0,\\r\\n                           \"\\r\\n                                          \\x01    X a;a a         \\x01            \\x01\\r\\n      s∆ Aaµ = −∆µ             η ∂ N −1 ca + g  Ce 1 2 ∂ i1 Aa1 ∂ i2 +1 ca2\\r\\n                                                  i1 i2\\r\\n                                                    i1 +i2\\r\\n                                                    =N −3\\r\\n                          X                                  \\x01              \\x01                 \\x01\\r\\n               + g2                  e a;a1 a2 a3 ∂ i1 Aa1\\r\\n                                     C                           ∂ i2 Aa2       ∂ i3 +1 ca3                                       (4.27)\\r\\n                                       i1 i2 i3\\r\\n                        i1 +i2 +i3\\r\\n                         =N −4\\r\\n                                                                                                                              #\\r\\n                          X                                      \\x01              \\x01             \\x01                  \\x01\\r\\n               +g   3                e a;a1 a2 a3 a4 ∂ i1 Aa1\\r\\n                                     C                               i2\\r\\n                                                                     ∂ A  a2        i3\\r\\n                                                                                    ∂ A  a3\\r\\n                                                                                                  ∂   i4 +1 a4\\r\\n                                                                                                          c               4\\r\\n                                                                                                                     + O(g ) ,\\r\\n                                       i1 i2 i3 i4\\r\\n                        i1 +..+i4\\r\\n                         =N −5\\r\\n\\r\\nThe fact that it is possible to define an anti-gBRST transformation and use it to construct\\r\\nthe ghost operator may not be too surprising given that this was also possible for the usual\\r\\nrenormalisable gauge-fixing+ghost Lagrangian. What is more surprising is that the ghost\\r\\noperator generated by the anti-gBRST exact operator,\\r\\n        (N )\\r\\n      OG       = −ca ∂ µ s∆ Aaµ ,                                                                                                 (4.28)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                             – 18 –\\r\\n\\x0c\\n\\n                                \"\\r\\n                                      \\x01    X a;a a         \\x01            \\x01\\r\\n                   = (∂c ) η ∂ N −1 ca + g\\r\\n                            a\\r\\n                                            Ce 1 2 ∂ i1 Aa1 ∂ i2 +1 ca2\\r\\n                                              i1 i2\\r\\n                                                                i1 +i2\\r\\n                                                                =N −3\\r\\n                                X                                     \\x01               \\x01                    \\x01\\r\\n                  + g2                  e a;a1 a2 a3 ∂ i1 Aa1\\r\\n                                        C                                  ∂ i2 Aa2        ∂ i3 +1 ca3\\r\\n                                          i1 i2 i3\\r\\n                           i1 +i2 +i3\\r\\n                            =N −4\\r\\n                                                                                                                                           #\\r\\n                             X                                             \\x01              \\x01                \\x01                  \\x01\\r\\n                  +g   3               e a;a1 a2 a3 a4\\r\\n                                       C                    ∂ A  i1   a1       i2\\r\\n                                                                               ∂ A   a2         i3\\r\\n                                                                                                ∂ A  a3\\r\\n                                                                                                               ∂   i4 +1 a4\\r\\n                                                                                                                       c               4\\r\\n                                                                                                                                  + O(g ) ,\\r\\n                                         i1 i2 i3 i4\\r\\n                           i1 +..+i4\\r\\n                            =N −5\\r\\n\\r\\nis at first sight not equivalent to its gBRST generated cousin. Equating the two with each\\r\\nother,\\r\\n                                   ca ∂ µ s∆ Aaµ = ca ∂ µ s∆ Aaµ ,                  (4.29)\\r\\ntherefore generates non-trivial identities among the various η-coefficients. Identifying the\\r\\nRHS of eqs. (4.21) and (4.28) we then find, after using integration by parts and the product\\r\\nrule, the following relation:\\r\\n                                                             (\\r\\n        X                     \\x01               \\x01            \\x01\\r\\n 0=            (∂ca ) ∂ i1 Aa1 .. ∂ in−1 Aan−1 ∂ in +1 can     e a;a1 ..an −\\r\\n                                                               C                       (4.30)    i1 ..in\\r\\n      i1 +..+in\\r\\n      =N −n−1\\r\\n i1         in−1                                                                                     )\\r\\n X          X (s1 + .. + sn−1 + in )!\\r\\n                                            s1 +..sn−1 +in e an ;a1 ..an−1 a\\r\\n         ..                           × (−1)              C(i1 −s1 )..(in−1 −sn−1 )(in +s1 +..+sn−1 ) .\\r\\n                   s1 !..sn−1 !in !\\r\\n s1 =0    sn−1 =0\\r\\n\\r\\nLet us remark that the summand does not necessarily vanish independently here. It does\\r\\nonly as long as the field contents and its derivatives are independent in each term in the\\r\\nsum. One therefore has to be careful when applying this identity. Using the definitions\\r\\nin eqs. (3.42)-(3.44) we then derive the following set of constraints on the couplings of the\\r\\nghost operators:\\r\\n                           i1\\r\\n                           X                     \\x10s +i \\x11\\r\\n            (1)                                       1          2       (1)\\r\\n           ηi1 i2 = −            (−1)s1 +i2                           η(i1 −s1 )(i2 +s1 ) ,                                                    (4.31)\\r\\n                           s1 =0\\r\\n                                                           s1\\r\\n                          i2\\r\\n                       i1 X\\r\\n                       X\\r\\n           (1)                          (s1 + s2 + i3 )!                 (1)\\r\\n         ηi1 i2 i3 =                                     (−1)s1 +s2 +i3 η(i2 −s2 )(i1 −s1 )(i3 +s1 +s2 ) ,                                     (4.32)\\r\\n                                           s1 !s2 !i3 !\\r\\n                       s1 =0 s2 =0\\r\\n                          i2\\r\\n                       i1 X\\r\\n                       X\\r\\n          (2)                (s1 + s2 + i3 )!                                             (2)\\r\\n         ηi1 i2 i3 =                                             (−1)s1 +s2 +i3 η(i1 −s1 )(i2 −s2 )(i3 +s1 +s2 ) ,                             (4.33)\\r\\n                                            s1 !s2 !i3 !\\r\\n                       s1 =0 s2 =0\\r\\n                       i1 X\\r\\n                       X  i2\\r\\n          (3)                (s1 + s2 + i3 )!                                             (3)\\r\\n         ηi1 i2 i3 =                                             (−1)s1 +s2 +i3 η(i2 −s2 )(i1 −s1 )(i3 +s1 +s2 ) ,                             (4.34)\\r\\n                       s1 =0 s2 =0\\r\\n                                            s1 !s2 !i3 !\\r\\n                           i1 X\\r\\n                           X  i2 X\\r\\n                                 i3\\r\\n      (1)                           (s1 + s2 + s3 + i4 )!\\r\\n     ηi1 i2 i3 i4 = −\\r\\n                           s1 =0 s2 =0 s3 =0\\r\\n                                                       s1 !s2 !s3 !i4 !\\r\\n\\r\\n                                                                               (3)\\r\\n                                            × (−1)s1 +s2 +s3 +i4 η(i3 −s3 )(i2 −s2 )(i1 −s1 )(i4 +s1 +s2 +s3 ) ,                               (4.35)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                      – 19 –\\r\\n\\x0c\\n\\n                       i1 X\\r\\n                       X  i2 X\\r\\n                             i3\\r\\n     (2a)                       (s1 + s2 + s3 + i4 )!\\r\\n    ηi1 i2 i3 i4 = −\\r\\n                       s1 =0 s2 =0 s3 =0\\r\\n                                                      s1 !s2 !s3 !i4 !\\r\\n\\r\\n                                                                         (2a)\\r\\n                                          × (−1)s1 +s2 +s3 +i4 η(i1 −s1 )(i2 −s2 )(i3 −s3 )(i4 +s1 +s2 +s3 ) ,         (4.36)\\r\\n\\r\\n                                                          i2 X\\r\\n                                                       i1 X\\r\\n                                                       X     i3\\r\\n      (2b)            (2a)               (2a)                   (s1 + s2 + s3 + i4 )!\\r\\n    ηi1 i2 i3 i4 = ηi1 i3 i2 i4 − ηi1 i2 i3 i4 +\\r\\n                                                                                  s1 !s2 !s3 !i4 !\\r\\n                                                       s1 =0 s2 =0 s3 =0\\r\\n                                                                           (2b)\\r\\n                                                × (−1)s1 +s2 +s3 +i4 η(i1 −s1 )(i2 −s2 )(i3 −s3 )(i4 +s1 +s2 +s4 ) .   (4.37)\\r\\n\\r\\nTo the best of our knowledge the existence of these kind of identities was not known by\\r\\nthe authors of the previous works [13, 37]. But we can use their one-loop all-N results for\\r\\n (1)\\r\\nηij , which in their work was named ηi to check eq. (4.31) at this order. Their one-loop\\r\\nresult in our notation is given by\\r\\n\\r\\n                                             1      h          \\x10 N − 2 \\x11 \\x10 N − 2 \\x11i\\r\\n                             (1),1\\r\\n                       ǫηij          =                (−1)i − 3         −           .                                  (4.38)\\r\\n                                         2N (N − 1)                i       i+1\\r\\n\\r\\nSubstituting this result into eq. (4.31) we then find:\\r\\n                              i\\r\\n                              X                    \\x10s+j \\x11                            3 (−1)N − 1 \\x10 N − 2 \\x11\\r\\n                  (1),1                                          (1),1\\r\\n                ηij       +          (−1)s+j                    η(i−s)(j+s) =                                          (4.39)\\r\\n                                                        s                            2ǫ N (N − 1) 1 + i\\r\\n                              s=0\\r\\n\\r\\nThe right hand side thus indeed vanishes for all positive even values of N , as required.\\r\\n     We initially found these identities after inspecting the results of explicit calculations.\\r\\nWe could explain the extra relations by imposing a ghost-antighost exchange symmetry;\\r\\nwhose origin we then finally derived as a consequence of the anti-gBRST symmetry. Since\\r\\nthe η-coefficients can be written in terms of the κ-coefficients it then follows that the set\\r\\nof κ-coefficients associated to the different EOM operators is not actually independent.\\r\\nThat is there are nontrivial relations among the EOM-operators. To solve these relations\\r\\nin closed form is in general difficult but it is not too hard to solve them for fixed N on\\r\\na case-by-case basis. We will give examples and demonstrate the use of these relations in\\r\\nsection 5 where we construct minimal bases of operators for the lowest values of N , and\\r\\nstudy the size of the basis for higher N .\\r\\n\\r\\n5   Operator Bases Construction for fixed N\\r\\n\\r\\nIn this section we will construct explicit bases of the unphysical (EOM and ghost) operators\\r\\n                                                       (N )\\r\\nwhich can mix with the gauge-invariant operator O1 for fixed N , valid up to the four-loop\\r\\n                                                (N ),k\\r\\nlevel. The structure of the EOM operator OEOM up to four loops was discussed in section\\r\\n3.3. As explained there, we only require k ≤ 4 for general N when working up to four\\r\\nloops. The corresponding EOM operators were presented in eqs. (3.13)- (3.16). Another\\r\\nconstraint on k arises for fixed N since we only have a total budget of N − 1 ∂s and As to\\r\\n                                                                      (k)\\r\\nspend in the G-function multiplying the EOM in eq. (3.4). Since OEOM requires at least k\\r\\nAs, this leads to k ≤ N − 1.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 – 20 –\\r\\n\\x0c\\n\\n    Being determined by the gBRST symmetry explicit expressions for all ghost operators\\r\\nrequired up to four loops are given in eqs. (4.22)-(4.25). Their color decompositions are\\r\\ngiven in eqs.(3.42)-(3.44), with the Wilson coefficients being related to those of the EOM\\r\\noperators via eqs.(3.45)-(3.51). Finally, the generalised anti-BRST symmetry imposes fur-\\r\\nther constraints given in eqs.(4.31)-(4.37), reducing unphysical operators to a yet smaller\\r\\nbasis. Having all these definitions at our deposal we are now in a position to construct\\r\\nexplicit and minimal bases. In the remaining part of this section we provide explicitly the\\r\\n                                                      (N )\\r\\nbases that are relevant for the renormalisation of O1 , with N = 2, 4 and N = 6, and we\\r\\ndescribe the space of independent operators for higher N .\\r\\n\\r\\n5.1   N = 2 operators\\r\\n                                                                                     (2)\\r\\nThe construction of a basis of unphysical operators mixing with O1 , which has dimension\\r\\n                                                          (2),1\\r\\n4, is straightforward. There is a single EOM operator, OEOM , defined in eq. (3.13). The\\r\\n                                 (2),1\\r\\ncorresponding ghost operator, OG , is given in eq. (4.22). They read\\r\\n                           (2),1                                 (2),1\\r\\n                       OEOM = η (D.F )a Aa ,                   OG        = −η ∂c̄a ∂ca .       (5.1)\\r\\n\\r\\nWe note that both operators feature the same coupling constant, η, which follows from\\r\\nthe generalised BRST symmetry. In practice, this fact has important consequences for\\r\\n                                                (2)         (2)         (N )\\r\\nrenormalisation, because it implies that the OEOM and OG mix with O1 with the same\\r\\ncounterterm. In other words, we find only one unphysical operator mixing with the gauge\\r\\ninvariant operator of N = 2. Following the vector notation introduced in sec.2, the twist-2\\r\\noperators of dimension 4 are written as O~ (2) = (O(2) , O(2) ) with\\r\\n                                                    1     2\\r\\n\\r\\n                                         (2) 1 \\x02        \\x03\\r\\n                                        O1 =   Tr Fν F ν ,                                     (5.2)\\r\\n                                             2\\r\\n                                         (2)\\r\\n                                        O2 = (D.F )a Aa + ca ∂ 2 ca .                          (5.3)\\r\\n\\r\\n5.2   N = 4 operators\\r\\n                                            (4)\\r\\nThe mass dimension-6 operator O1 undergoes a less trivial mixing pattern. All EOM\\r\\n                (4),k\\r\\noperators in OEOM with k ≤ 3 are relevant and each sector generates associated ghost\\r\\n                                                                                (N ),1\\r\\noperators. As for the case N = 2, we can readily write down the EOM operator OEOM\\r\\nand its associated ghost operator\\r\\n                                           (4),1\\r\\n                                         OEOM = η (D.F )a ∂ 2 Aa ,                             (5.4)\\r\\n                                            (4),1\\r\\n                                          OG        = −η ∂c̄a ∂ 3 ca .                         (5.5)\\r\\n                   (4),2\\r\\nNext we consider OEOM , eq. (3.14), which involves only one operator, due to the antisym-\\r\\nmetry of the coefficients κij , eq. (3.22). It reads\\r\\n                                (4),2\\r\\n                             OEOM = 2 gκ01 f aa1 a2 (D.F )a Aa1 ∂Aa2 .                         (5.6)\\r\\n                        (4),2\\r\\nThe ghost operator, OG          , is defined in eq. (4.23) in terms of the coefficients C̃iaa 1 a2\\r\\n                                                                                           1 i2\\r\\n                                                                                                   of\\r\\neq. (3.42) as\\r\\n                                             h                               i\\r\\n                    (4),2                      (1)               (1)\\r\\n                  OG        = −gf aa1 a2 ∂c̄a η01 Aa1 ∂ 2 ca2 + η10 ∂Aa1 ∂ca2 .                (5.7)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      – 21 –\\r\\n\\x0c\\n\\n                                                                                   (1)       (1)\\r\\nThe generalised BRST symmetry imposes that the coefficients η01 and η10 are related to\\r\\n                     (4),1   (4),2\\r\\nthe parameters in OEOM and OEOM , respectively η and κ01 . These relations are given in\\r\\neq. (3.45) and lead to\\r\\n                    h      \\x10                     \\x11           \\x10                       \\x11i\\r\\n    (4),2\\r\\n  OG = −gf aa1 a2 η ∂c̄a 2∂Aa1 ∂ca2 + Aa1 ∂ 2 ca2 + 2κ01 ∂c̄a Aa1 ∂ 2 ca2 − ∂Aa1 ∂ca2 .\\r\\n                                                                                                           (5.8)\\r\\n\\r\\nThe unphysical operators in eqs. (5.4), (5.5), (5.6) and (5.8) contribute to the renormali-\\r\\n           (4)\\r\\nsation of O1 starting from two loops [13, 37]. From three loops onwards, we must take\\r\\n                                       (4),3\\r\\ninto account also the EOM operator OEOM , eq. (3.15), which reads\\r\\n                              (4),3           (2)\\r\\n                            OEOM = g2 κ000 daa1 a2 a3 (D.F )a Aa1 Aa2 Aa3 ,                                (5.9)\\r\\n\\r\\nwhere we applied eqs. (3.23), (3.25), (3.28), (3.29) and (3.30) to restrict the independent\\r\\ncouplings to a single operator at mass dimension 6. Due to the fully symmetric nature\\r\\nof the colour structure daa1 a2 a3 of eq. (5.9), we find that two-point correlators with an\\r\\n                (4),3                                                                                   (4),3\\r\\ninsertion of OEOM vanish automatically at one and at two loops. This implies that OEOM\\r\\n                                 (N )                                                                   (4),3\\r\\nenters the renormalisation of O1 only at four loops. We derive the ghost operator OG ,\\r\\neq. (4.24), by computing the coefficients (3.46)-(3.48), which enter C        e aa1 a2 a3 in eq. (3.43).\\r\\n                                                                                i1 i2 i3\\r\\nWe get\\r\\n                                         h                                                          i\\r\\n          (4),3                            (1)                (2)                 (3)\\r\\n        OG = −g2 ∂c̄a Aa1 Aa2 ∂ca3 η000 (f f )aa1 a2 a3 + η000 daa1 a2 a3 + η000 daa     d\\r\\n                                                                                            1 a2 a3\\r\\n                                                                                         4f f\\r\\n                                         h                                      i\\r\\n                      2 a a1 a2       a3          aa1 a2 a3       (2) aa1 a2 a3\\r\\n                = −g ∂c̄ A A ∂c 2κ01 (f f )                 + 3κ000 d             .                   (5.10)\\r\\n\\r\\nBy taking into account only the relations deriving from the generalised BRST symmetry,\\r\\nwe obtained a set of three unphysical operators, each of them corresponding to the terms\\r\\nin eqs. (5.4)-(5.6) and (5.8)-(5.10) that are proportional to the coefficients η, κ01 and\\r\\n  (2)\\r\\nκ000 , respectively. However, the generalised anti-BRST symmetry introduces an additional\\r\\nconstraint on these coefficients and reduces the set of independent operators further. By\\r\\nspecialising i1 , i2 = 0, 1 in eq. (4.31) we find\\r\\n                                        (1)         (1)\\r\\n                                      2η10 = η01 ⇐⇒ 2κ01 = η.                                             (5.11)\\r\\n\\r\\nThis identity is surprising, because it relates the couplings of different EOM operators,\\r\\nwhich are free a priori. Therefore the EOM and ghost Lagrangian feature only two in-\\r\\n                                        (2)\\r\\ndependent parameters, e.g. η and κ000 , which are chosen as coupling constants of two\\r\\nindependent unphysical operators. In conclusion, we obtain a basis of operators with spin\\r\\n4 (and dimension 6) O~ (4) = (O(4) , O(4) , O(4) ) with\\r\\n                               1      2      3\\r\\n\\r\\n (4) 1 \\x02          \\x03\\r\\nO1 = Tr Fν D 2 F ν ,                                                                                      (5.12)\\r\\n     2    h                                         i                               h                           i\\r\\n  (4)\\r\\nO2 = (D.F )a ∂ 2 Aa + gf aa1 a2 Aa1 ∂Aa2                − ∂c̄a ∂ 3 ca − gf aa1 a2 ∂c̄a 2Aa1 ∂ 2 ca2 + ∂Aa1 ∂ca2\\r\\n     − g2 (f f )aa1 a2 a3 ∂c̄a Aa1 Aa2 ∂ca3 ,                                                             (5.13)\\r\\n                 h                                       i\\r\\n (4)\\r\\nO3 = daa1 a2 a3 (D.F )a Aa1 Aa2 Aa3 − 3 ∂c̄a Aa1 Aa2 ∂ca3 .                                               (5.14)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         – 22 –\\r\\n\\x0c\\n\\n5.3    N = 6 operators\\r\\nThe basis of operators at mass dimension 8, which includes the gauge invariant operator\\r\\n  (6)                                           (6),k\\r\\nO1 , is generated by the full set of operators OEOM , with k ≤ 4. For k = 1 we get\\r\\nimmediately the EOM and ghost operators\\r\\n                   (6),1                                         (6),1\\r\\n                 OEOM = η (D.F )a ∂ 4 Aa ,                     OG        = −η ∂c̄a ∂ 5 ca .       (5.15)\\r\\n\\r\\nThe definition in eq. (3.14) and antisymmetry of the coefficents κij , eq. (3.22), imply that\\r\\n (6),2\\r\\nOEOM includes two independent terms\\r\\n                                       h                                      i\\r\\n                 (6),2\\r\\n               OEOM = gf aa1 a2 (D.F )a 2κ03 Aa1 ∂ 3 Aa2 + 2κ12 (∂Aa1 )∂ 2 Aa2 .       (5.16)\\r\\n\\r\\n                                  (6),2\\r\\nTo get the ghost sector OG , we expand out eq. (4.23) with i1 , i2 = 0, .., 3 and we use the\\r\\ndefinitions in eqs. (3.42) and (3.45), to get\\r\\n                            n h                                                               i\\r\\n      (6),2\\r\\n    OG = − gf aa1 a2 ∂c̄a η Aa1 ∂ 4 ca2 + 4 ∂Aa1 ∂ 3 ca2 + 6 ∂ 2 Aa1 ∂ 2 ca2 + 4 ∂ 3 Aa1 ∂ca2\\r\\n                     h                         i      h                               io\\r\\n              + 2κ03 Aa1 ∂ 4 ca2 − ∂ 3 Aa1 ∂ca2 + 2κ12 ∂Aa1 ∂ 3 ca2 − ∂ 2 Aa1 ∂ 2 ca2 .       (5.17)\\r\\n\\r\\n                                               (6),3\\r\\nSimilarly, we write down the operator OEOM , following the definition in eq. (3.15) and the\\r\\nrelations eq. (3.23), (3.25) and (3.28)-(3.30) on the coefficients, to obtain\\r\\n                                                 h                                          i\\r\\n          (6),3                                      (1)                    (1)\\r\\n        OEOM = + 2g 2 (f f )aa1 a2 a3 (D.F )a κ002 Aa1 Aa2 ∂ 2 Aa3 + κ101 ∂Aa1 Aa2 ∂Aa3\\r\\n                                             h                                            i\\r\\n                                                (2)                    (2)\\r\\n                  + 3g 2 daa1 a2 a3 (D.F )a κ002 Aa1 Aa2 ∂ 2 Aa3 + κ011 Aa1 ∂Aa2 ∂Aa3\\r\\n                                             h\\r\\n                                                (3)\\r\\n                  + 2g 2 daa\\r\\n                          d\\r\\n                             1 a2 a3\\r\\n                                     (D.F )a\\r\\n                                               κ002 (Aa1 Aa2 ∂ 2 Aa3 − ∂ 2 Aa1 Aa2 Aa3 )\\r\\n                          4f f\\r\\n                                                                                         i\\r\\n                                                 (3)\\r\\n                                            + κ101 (∂Aa1 Aa2 ∂Aa3 − Aa1 ∂Aa2 ∂Aa3 ) .         (5.18)\\r\\n\\r\\nBy expanding out eq. (4.24) for i1 , i2 , i3 = 0..2 and by using the definitions in eqs. (3.43),\\r\\n(3.46), (3.47) and (3.48) we obtain the related ghost operator\\r\\n                                            h                                                       i\\r\\n    (6),3          (1)\\r\\n  OG = − 2g2 κ03 (f f )aa1 a2 a3 ∂c̄a Aa1 Aa2 ∂ 3 ca3 + 3Aa1 ∂Aa2 ∂ 2 ca3 + 3Aa1 ∂ 2 Aa2 ∂ca3\\r\\n                                            h                                                         i\\r\\n                   (1)\\r\\n            − 2g2 κ12 (f f )aa1 a2 a3 ∂c̄a ∂Aa1 Aa2 ∂ 2 ca3 − ∂ 2 Aa1 Aa2 ∂ca3 + 2 ∂Aa1 ∂Aa2 ∂ca3\\r\\n                                            h                                                      i\\r\\n                   (1)\\r\\n            − 2g2 κ002 (f f )aa1 a2 a3 ∂c̄a Aa1 Aa2 ∂ 3 ca3 + ∂ 2 Aa1 Aa2 ∂ca3 − 2Aa1 ∂ 2 Aa2 ∂ca3\\r\\n                                            h                                                         i\\r\\n                   (1)\\r\\n            − 2g2 κ101 (f f )aa1 a2 a3 ∂c̄a 2 ∂Aa1 Aa2 ∂ 2 ca3 − Aa1 ∂Aa2 ∂ 2 ca3 − ∂Aa1 ∂Aa2 ∂ca3\\r\\n                                        h                                    i\\r\\n                   (2)\\r\\n            − 3g2 κ002 daa1 a2 a3 ∂c̄a Aa1 Aa2 ∂ 3 ca3 + 2Aa1 ∂ 2 Aa2 ∂ca3\\r\\n                                        h                                        i\\r\\n                   (2)\\r\\n            − 3g2 κ011 daa1 a2 a3 ∂c̄a ∂Aa1 ∂Aa2 ∂ca3 + 2Aa1 ∂Aa2 ∂ 2 ca3\\r\\n                                        h                                  i\\r\\n                   (3)\\r\\n            − 6g2 κ002 daa\\r\\n                        d\\r\\n                           1 a2 a3\\r\\n                                   ∂c̄a\\r\\n                                          Aa1 a2 3 a3\\r\\n                                               A  ∂ c   −  ∂ 2 a1 a2\\r\\n                                                              A    A  ∂ca3\\r\\n                        4f f\\r\\n                                        h                                      i\\r\\n                   (3)\\r\\n            − 6g2 κ101 daa\\r\\n                        d\\r\\n                           1 a2 a3\\r\\n                                   ∂c̄a\\r\\n                                          ∂A  a1\\r\\n                                                 ∂Aa2\\r\\n                                                      ∂ca3\\r\\n                                                           −  A a1\\r\\n                                                                   ∂Aa2 2 a3\\r\\n                                                                       ∂  c      .                 (5.19)\\r\\n                           4f f\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 – 23 –\\r\\n\\x0c\\n\\n                      (6),4       (6),4\\r\\nWe construct OEOM and OG , by expanding out eqs. (3.16) and (4.25) with i1 ..i4 = 0, 1.\\r\\nAfter imposing the relations in eqs.(3.24), (3.26), (3.27), (3.31) and (3.32), which constrain\\r\\n                                                                     (1)       (2)\\r\\nthe coefficients of Cia;a 1 ..a4\\r\\n                      1 ..i4\\r\\n                                 , defined in eq. (3.19), we choose κ0001 and κ0001 as independent\\r\\n                      (6),4                     (6),4\\r\\nparameters in OEOM . At this point, OG is written in terms of the coefficients appearing\\r\\n    (6),4      (6),3\\r\\nin OEOM and OEOM , by means of eqs.(3.44), (3.49), (3.50) and (3.51), which give\\r\\n                        (6),4             (1)\\r\\n                      OEOM = + 2g3 κ0001 (f f f )aa1 a2 a3 a4 (D.F )a Aa1 Aa2 Aa3 ∂Aa4\\r\\n                                          (2)\\r\\n                                + 2g3 κ0001 daa\\r\\n                                             4f\\r\\n                                                1 a2 a3 a4\\r\\n                                                           (D.F )a Aa1 Aa2 Aa3 ∂Aa4 ,                   (5.20)\\r\\n\\r\\n                                                        h                                           i\\r\\n      (6),4             (1)\\r\\n   OG         = − 2g 3 κ002 (f f f )aa1 a2 a3 a4 a5 ∂c̄a Aa1 Aa2 Aa3 ∂ 2 ca4 + 2Aa1 Aa2 (∂Aa3 )∂ca4\\r\\n                                                        h                                        i\\r\\n                        (1)\\r\\n                + 2g3 κ101 (f f f )aa1 a2 a3 a4 a5 ∂c̄a Aa1 ∂Aa2 Aa3 ∂ca4 − ∂Aa1 Aa2 Aa3 ∂ca4\\r\\n                                                h                                           i\\r\\n                        (2)\\r\\n                − 3g3 κ002 daa\\r\\n                             4f\\r\\n                                1 a2 a3 a4\\r\\n                                           ∂c̄a\\r\\n                                                  Aa1 a2 a3 2 a4\\r\\n                                                      A   A  ∂  c  +   2A  a1 a2\\r\\n                                                                             A   ∂A a3\\r\\n                                                                                       ∂ca4\\r\\n\\r\\n                          (2)\\r\\n                − 6g3 κ011 daa\\r\\n                            4f\\r\\n                               1 a2 a3 a4\\r\\n                                          ∂c̄a ∂Aa1 Aa2 Aa3 ∂ca4\\r\\n                                                        h\\r\\n                       (1)\\r\\n                − 2g3 κ0001 (f f f )aa1 a2 a3 a4 a5 ∂c̄a Aa1 Aa2 Aa3 ∂ 2 ca4 + 3Aa1 ∂Aa2 Aa3 ∂ca4\\r\\n                                                                                                         i\\r\\n                                                        − 3Aa1 Aa2 ∂Aa3 ∂ca4 − ∂Aa1 Aa2 Aa3 ∂ca4\\r\\n                                h\\r\\n                       (2)\\r\\n                − 2g3 κ0001 ∂c̄a daa\\r\\n                                  4f\\r\\n                                     1 a2 a3 a4\\r\\n                                                (Aa1 Aa2 Aa3 ∂ 2 ca4 − Aa1 Aa2 ∂Aa3 ∂ca4 )\\r\\n                                                                        i\\r\\n                                 + 2daa4f\\r\\n                                          4 a1 a2 a3\\r\\n                                                     ∂ca4 a1 a3\\r\\n                                                         A  A     ∂A a3\\r\\n                                                                          .                             (5.21)\\r\\n\\r\\nAt mass dimension 8, we found a total of eleven unphysical operators, parameterised by\\r\\n                                                                                         (6)\\r\\nan equal number of free coefficients η, κ03 , κ12 .. , that are required to renormalise O1 up\\r\\nto four loops. This picture simplifies significantly by taking into account the anti-BRST\\r\\nrelations. For instance, by evaluating eq. (4.31) for i1 , i2 = 0..3, we obtain\\r\\n                              (\\r\\n                                    (1)      (1)\\r\\n                                 2η12 − 3η03 = 0\\r\\n                                  (1)    (1)     (1)      (1)                            (5.22)\\r\\n                                 η03 + η12 − η21 + 2η30 = 0\\r\\n                (1)\\r\\nwhere the ηij depend on η, κ12 and κ03 , as in eq. (3.45). The equations above are both\\r\\nsolved simultanously by imposing\\r\\n\\r\\n                                            5η + 4κ12 − 6κ03 = 0.                                       (5.23)\\r\\n\\r\\nSimilarly, we derive further constraints by expanding eqs. (4.32) - (4.37), which lead to\\r\\n\\r\\n                                      (1)      (1)  5  5\\r\\n                                     κ101 − 2κ002 + η + κ12 = 0,                                        (5.24)\\r\\n                                                    6  3\\r\\n                                      (2)    (2)\\r\\n                                     κ011 − κ002 = 0,                                                   (5.25)\\r\\n                                      (3)     (3)\\r\\n                                     κ101 + 2κ002 = 0,                                                  (5.26)\\r\\n                                        (1)                (1)\\r\\n                                     3κ0001 + η + 2κ12 − 3κ002 = 0,                                     (5.27)\\r\\n                                        (2)      (2)\\r\\n                                     2κ0001 − 3κ002 = 0.                                                (5.28)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                    – 24 –\\r\\n\\x0c\\n\\nIn conclusion, by imposing the relations on the coefficients of eqs. (5.15)-(5.21), which are\\r\\ngiven in eqs.(5.23)-(5.28), we obtain a minimal basis of only five independent unphysical\\r\\noperators at dimension 8. For instance, we might solve eqs.(5.23)-(5.28) in terms of η, κ12 ,\\r\\n (1)    (2)    (3)\\r\\nκ002 , κ002 , κ002 and pick the following basis of independent operators\\r\\n\\r\\n                                       (6)  1 h           i\\r\\n                                     O1 = Tr Fν D 4 F ν ,                              (5.29)\\r\\n                                            2\\r\\n\\r\\n                                                  h5                                 \\x108\\r\\n  (6)\\r\\nO2 = (D.F )a ∂ 4 Aa − ∂c̄a ∂ 5 ca + gf a1 a2 a3       (D.F )a1 Aa2 ∂ 3 Aa3 − ∂c̄a1 Aa2 ∂ 4 ca3\\r\\n                                                    3                                 3\\r\\n             a2 3 a2        2 a2 2 a3     7 3 a2 a3 \\x11i          2\\r\\n                                                                                 h\\r\\n                                                                        aa1 a2 a3 5\\r\\n        + 4∂A ∂ c + 6∂ A ∂ c + ∂ A ∂c                        + g (f f )              (D.F )a ∂Aa1 ∂Aa2 Aa3\\r\\n                                          3                                        3\\r\\n          5   \\x10\\r\\n        − ∂c̄a Aa1 Aa2 ∂ 3 ca3 + 4Aa1 ∂Aa2 ∂ 2 ca3 + 3Aa1 ∂ 2 Aa2 ∂ca3 + ∂Aa1 ∂Aa2 ∂ca3\\r\\n          3\\r\\n                           \\x11i                        h2\\r\\n        − 2∂Aa1 Aa2 ∂ 2 ca3 − g3 (f f f )aa1 a2 a3 a4 (D.F )a Aa1 Aa2 Aa3 ∂Aa4\\r\\n                                                       3\\r\\n          1 a \\x10 a1 a2 a3 2 a4\\r\\n        + ∂c̄ 2A A A ∂ c + 6Aa1 Aa2 ∂Aa3 ∂ca4 − Aa1 ∂Aa2 Aa3 ∂ca4\\r\\n          3                   \\x11i\\r\\n        − 8∂Aa1 Aa2 Aa3 ∂ca4                                                                        (5.30)\\r\\n\\r\\n\\r\\n               h        \\x10                    4              \\x11 4         \\x10                3\\r\\n (6)\\r\\nO3 = gf aa1 a2 (D.F )a 2∂Aa1 ∂ 2 Aa2 + Aa1 ∂ 3 Aa2 − ∂c̄a aa1 ∂ 4 ca2 + ∂Aa1 ∂ 2 ca2\\r\\n                                             3                   3                       2\\r\\n       3 2 a1 2 a2                     \\x11i                     h 10\\r\\n                           3 a1     a2       2      aa1 a2 a3\\r\\n     − ∂ A ∂ c − ∂ A ∂c                   + g (f f )                           a\\r\\n                                                               − (D.F ) ∂A A ∂Aa3   a1 a2\\r\\n       2                                                          3\\r\\n           \\x104                    22                                             14\\r\\n     − ∂c̄a Aa1 Aa2 ∂ 3 ca3 + Aa1 ∂Aa2 ∂ 2 ca3 + Aa1 ∂ 2 Aa2 ∂ca3 − ∂Aa1 Aa2 ∂ 2 ca3\\r\\n             3                    3                                              3\\r\\n          2 a1 a2    a3     22 a1 a2 a3 \\x11i 4 3                     aa1 a2 a3 a4\\r\\n                                                                                h\\r\\n     − 2∂ A A ∂c + ∂A ∂A ∂c                        − g (f f f )                   (D.F )a Aa1 Aa2 Aa3 ∂Aa4\\r\\n                             3                        3\\r\\n           \\x10                                                Aa1 ∂Aa2 Aa3 ∂ca4                            \\x11i\\r\\n     − ∂c̄a Aa1 Aa2 Aa3 ∂ 2 ca4 − 3Aa1 Aa2 ∂Aa3 ∂ca4 +                             + 4∂Aa1 Aa2 Aa3 ∂ca4\\r\\n                                                                     2\\r\\n                                                                                                     (5.31)\\r\\n\\r\\n                         h       \\x10                                \\x11      \\x10\\r\\n (6)\\r\\nO4 = 2g 2 (f f )aa1 a2 a3 (D.F )a Aa1 Aa2 ∂ 2 Aa3 + 2∂Aa1 Aa2 ∂Aa3 − ∂c̄a Aa1 Aa2 ∂ 3 ca3\\r\\n        − 2Aa1 ∂Aa2 ∂ 2 ca3 − 2Aa1 ∂ 2 Aa2 ∂ca3 + 4∂Aa1 Aa2 ∂ 2 ca3 + ∂ 2 Aa1 Aa2 ∂ca3\\r\\n                            \\x11i                      h\\r\\n        − 2∂Aa1 ∂Aa2 ∂ca3 + 2g3 (f f f )aa1 a2 a3 a4 (D.F )a Aa1 Aa2 Aa3 ∂Aa4\\r\\n              \\x10                                                                                  \\x11i\\r\\n        − ∂c̄a 2Aa1 Aa2 Aa3 ∂ 2 ca4 − Aa1 Aa2 ∂Aa3 ∂ca4 + Aa1 ∂Aa2 Aa3 ∂ca4 + 3∂Aa1 Aa2 Aa3 ∂ca4\\r\\n                                                                                                    (5.32)\\r\\n\\r\\n                        h         \\x10                              \\x11       \\x10\\r\\n (6)\\r\\nO5 = 3g 2 daa1 a2 a3 (D.F )a Aa1 Aa2 ∂ 2 Aa3 + Aa1 ∂Aa2 ∂Aa3 − ∂c̄a Aa1 Aa2 ∂ 3 ca3\\r\\n                                                                \\x11i\\r\\n     + 2Aa1 ∂Aa2 ∂ 2 ca3 + 2Aa1 ∂ 2 Aa2 ∂ca3 + ∂Aa1 ∂Aa2 ∂ca3\\r\\n                           h                              \\x10\\r\\n     + 3g 3 daa\\r\\n             4f\\r\\n                1 a2 a3 a4\\r\\n                             (D.F )a a1 a2 a3\\r\\n                                    A  A  A   ∂Aa4\\r\\n                                                   − ∂c̄a\\r\\n                                                           2AA1 Aa2 Aa3 ∂ 2 ca4 + Aa1 Aa2 ∂Aa3 ∂ca4\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                  – 25 –\\r\\n\\x0c\\n\\n                                 \\x11i\\r\\n          + 2Aa1 ∂Aa2 Aa3 ∂ca4        − 6g3 daa\\r\\n                                             4f\\r\\n                                                4 a1 a2 a3\\r\\n                                                           ∂c̄a Aa1 Aa2 ∂Aa3 ∂ca4                       (5.33)\\r\\n\\r\\n\\r\\n                               h         \\x10\\r\\n           (6)       aa1 a2 a3\\r\\n          O6 = 2g2 d4f f         (D.F )a\\r\\n                                           Aa1 Aa2 ∂ 2 Aa3 + 2Aa1 ∂Aa2 ∂Aa3 − ∂ 2 Aa1 Aa2 Aa3\\r\\n                                   \\x11         \\x10\\r\\n               − 2∂Aa1 Aa2 ∂Aa3 − 6∂c̄a Aa1 Aa2 ∂ 3 ca3 + 2Aa1 ∂Aa2 ∂ 2 ca3 − ∂ 2 Aa1 Aa2 ∂ca3\\r\\n                                   \\x11i\\r\\n               − 2∂Aa1 ∂Aa2 ∂ca3                                                              (5.34)\\r\\n\\r\\n5.4        Operators of higher N\\r\\nThe construction of an operator basis to renormalise twist-2 operators of higher spin N is\\r\\nsummarised by the following steps.\\r\\n                                                 (N ),k\\r\\n        1 List all the EOM operators, OEOM , defined in eq. (3.4). Up to four loops, only\\r\\n          the terms with k ≤ 4, given in eqs. (3.13)-(3.16) are relevant. All these operators\\r\\n          have been written in terms of the colour structures in eqs.(3.17)-(3.19) and associ-\\r\\n          ated parameters. The latter obey the relations in eqs.(3.22)-(3.32), which define an\\r\\n          independent set of EOM operators, considering Bose symmetry only.\\r\\n\\r\\n        2 The structure of ghost operators is dictated by the generalised BRST symmetry,\\r\\n                                                        (N )\\r\\n          eq. (4.11). The operators that mix with O1 up to four loops are given, for ev-\\r\\n          ery value of N , in eqs. (4.22)-(4.25). They involve the colour structures given in\\r\\n          eqs.(3.42)-(3.44). Eqs. (3.45)-(3.51) uniquely determine all parameters of the ghost\\r\\n          Lagrangian, in terms of the parameters of the EOM operators.\\r\\n\\r\\n        3 Impose the anti-BRST symmetry, eq. (4.28). The latter implies relations among the\\r\\n          coefficients of the EOM operators via eqs. (4.31)-(4.37). These reduce the number of\\r\\n          independent operators to a minimal set.\\r\\n\\r\\nThe steps above allow to automate easily the construction of the operators, e.g. in FORM [61].\\r\\nFinding independent operators boils down to finding a set of coefficients which solves the\\r\\nlinear relations3 in eqs.(3.22)-(3.32) and (4.31)-(4.36), using the definitions in eqs.(3.45)-\\r\\n(3.51). By solving these, we determine the number of independent unphysical operators of\\r\\nhigher spin N . For up to N = 16 the size of the basis is given in table 3. The second line\\r\\nin table 3 gives the size of the basis without using anti-BRST relations, while the first line\\r\\nincludes them. While the basis grows significantly with the spin N , we find that most of\\r\\n                                                        (N ),4                (N ),4\\r\\nthe free parameters are associated to the operators OEOM . For instance, OEOM generates\\r\\n                                                                                 (N ),4\\r\\n112 out of the 140 unphysical operators at N = 16. Since mixing with OEOM is only\\r\\nrelevant at one loop, see table 2, these operators do not introduce prohibitive obstacles.\\r\\n    3\\r\\n    We notice that both eqs. (4.36) and (4.37) originate from the structure associated to the coefficients\\r\\n (2)          (N),4\\r\\n         in OEOM . We checked explicitly up to N = 10 that eq. (4.37) is automatically satisfied by the\\r\\nκi1 i2 i3 i4\\r\\nsolutions of eq. (4.36), which rely also on eq. (4.33), and therefore it doesn’t provide further simplifications\\r\\nof the basis.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      – 26 –\\r\\n\\x0c\\n\\n                   Spin N            2   4     6       8     10     12       14      16\\r\\n\\r\\n                   w aBRST           1   2      5      12    25     50      87      140\\r\\n                   w/o aBRST         1   3     11      30    66     126     215     339\\r\\n\\r\\n\\r\\nTable 3. Table showing the number of independent operators with and without the use of anti-\\r\\nBRST (aBRST) relations.\\r\\n\\r\\n\\r\\n6   Background-field formulation\\r\\n\\r\\nA powerful trick to simplify calculations of anomalous dimensions is to use the background\\r\\nfield method. The basic idea is to split the gauge field into a classical (non-propagating)\\r\\nbackground field component B and a purely Quantum field component Q as follows:\\r\\n\\r\\n                                   Aµa (x) = Baµ (x) + Qµa (x) .                             (6.1)\\r\\n\\r\\nOne can then consider Green’s functions with external background fields. By using a clever\\r\\ngauge fixing and ghost ghost Lagrangian, for the Quantum field [50–53]\\r\\n                                                    1\\r\\n                  LBGF+BG (Q, B, c̄, c) = −            (D̄ µ Aµ )2 − c̄a D̄µab D µ;bc cc ,   (6.2)\\r\\n                                                    2ξ\\r\\nwhere the background- and background+quantum-field covariant derivatives are defined as\\r\\n\\r\\n               D̄µac = ∂µ δac + gf abc Bµb ,        Dµac = ∂µ δac + gf abc (B + Q)bµ ,       (6.3)\\r\\n\\r\\nit then follows that the quantum gauge-fixed Lagrangian,\\r\\n\\r\\n                   LB (Q, B, c̄, c) = L0 (Q + B) + LBGF+BG (Q, B, c̄, c) ,                   (6.4)\\r\\n\\r\\nstays invariant under background-field gauge transformations\\r\\n                                    B a\\r\\n                                   δw Bµ (x) = D̄µac ω(x)c ,\\r\\n                                    B a\\r\\n                                   δw Qµ (x) = gf abc Qbµ ω(x)c .                            (6.5)\\r\\n\\r\\nWe now wish to discuss the form of the complete Lagrangian L̃(A, c̄, c), introduced in\\r\\neq. (2.21), which contains besides the Yang-Mills, gauge fixing and ghost terms also the\\r\\n                                                                             (N )\\r\\ntwist-2 gauge invariant gluonic operator ON (A), the EOM operator OEOM (A) and the\\r\\n                  (N )\\r\\nghost operator OG (A). Here we have purposefully included a dependence on A, although\\r\\nwe will not write out explicitly the dependence on its derivatives.\\r\\n     The lifting of L̃(A, c̄, c) into the background field formalism is straight forward for\\r\\nthe gauge invariant part but requires some minor modifications to the EOM and ghost\\r\\n                                                                        (N )\\r\\noperator. We therefore introduce their background field versions OBEOM (Q, B, c̄, c) and\\r\\n  (N )\\r\\nOBG (Q, B). Before giving a detailed derivation of the form of the Lagrangian we will state\\r\\ntheir form below. The complete Lagrangian then reads\\r\\n                                                                                   (N )\\r\\n            L̃B (A, B, c̄, c) =L0 (Q + B) + LBGF+BG (Q, B, c̄, c) + O1 (Q + B)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                               – 27 –\\r\\n\\x0c\\n\\n                                              (N )                        (N )\\r\\n                                       + OBEOM (Q, B, c̄, c) + OBG (Q, B) ,                                  (6.6)\\r\\n\\r\\nwhere                                    X                                   X\\r\\n                               (N )               (N ),k           (N )               (N ),k\\r\\n                             OBG =             OBG ,             OBEOM =             OBEOM ,                 (6.7)\\r\\n                                          k                                      k\\r\\n\\r\\n\\r\\n           (N ),k\\r\\n                                                           X                             \\x01            \\x01\\r\\n         OBEOM = g k−1 (D · F (B + Q))a                            Cia;a 1 ..ak\\r\\n                                                                     1 ..ik\\r\\n                                                                                D̄ i1 Qa1 .. D̄ ik Qak ,     (6.8)\\r\\n                                                      i1 +..+ik\\r\\n                                                      =N −k−1\\r\\n\\r\\n\\r\\n        (N ),k\\r\\n                             X                              \\x01            \\x01                \\x01            \\x01\\r\\n    OBG          = −gk−1               e a;a1 .. a4 D̄ca\\r\\n                                       C                        D̄ i1 Qa1 .. D̄ ik−1 Qak−1 D̄ ik +1 cak .    (6.9)\\r\\n                                         i1 .. i4\\r\\n                           i1 +..+ik\\r\\n                           =N −k+1\\r\\n\\r\\n                                                       e abc.. are identical in their definitions\\r\\n                                             abc.. and C\\r\\nNote in particular that the coefficients Cijk..          ijk..\\r\\nto those defined respectively in eqs. (3.17)-(3.19) and (3.42)-(3.44). The set of EOM and\\r\\nghost operators in the background gauge formalism is thus directly related to those in the\\r\\nstandard formulation.\\r\\n    To understand the structure of the EOM operator note that it should be generated from\\r\\nan infinitesimal field transformation of the kind Q → Q + G B (Q, B, ∂Q, ∂B, ...), since the\\r\\nQuantum effective action contains a path integral only over the field Q being a functional\\r\\nof B. This fixes the form of the EOM operator as follows:\\r\\n                             Z\\r\\n                    (N )            δS0 (A + Q) B;a µ µ µ\\r\\n                 OBEOM = dD x                      Gµ (Q , B , ∂ Q, ∂ µ B, ...)\\r\\n                                      δQµa (x)\\r\\n                                           \\x01a\\r\\n                           = D.F (Q + B) G B;a (Q, B, ∂Q, ∂B, ...)                         (6.10)\\r\\n\\r\\nwhere we have used also our earlier considerations about the mass dimension and counting\\r\\nof ∆-contractions. Finally we make the assertion that\\r\\n\\r\\n                                GµB;a (Q, B, ∂Q, ∂B, ...) = Gµa (Q, D̄Q, ...) .                             (6.11)\\r\\n\\r\\nwith Gµa defined in eqs. (3.2) and (3.3). There are a number of considerations which fix\\r\\nthis relation. First we require G B;a to be background-field gauge covariant - thus it can\\r\\nonly depend on Q or on its background-field covariant derivatives. However this fixes only\\r\\nits dependence on Q and B but not its functional form, G B = G. To fix this form we\\r\\nset B = 0, Q = A in the complete Lagrangian, i.e. we consider L̃B (A, 0, c̄, c). For this\\r\\nLagrangian to generate the same Green’s functions as L̃(A, c̄, c) (note their gauge-invariant\\r\\nparts are now identical) we therefore require:\\r\\n\\r\\n                                              L̃B (A, 0, c̄, c) = L̃(A, c̄, c)                              (6.12)\\r\\n\\r\\nFrom this it immediately follows that\\r\\n\\r\\n                    GµB;a (Q, D̄Q, ...)                    = GµB;a (A, ∂A, ...) = Gµa (A, ∂A, ...)          (6.13)\\r\\n                                           B=0,Q=A\\r\\n\\r\\nand we see that eq. (6.11) satisfies these constraints uniquely.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            – 28 –\\r\\n\\x0c\\n\\n     Let us now turn our attention to the ghost operator in the background formalism.\\r\\n                                                                        (N )\\r\\nAgain we need to satisfy the constraints that it should coincide with OG when we set\\r\\nB = 0, Q = A and that it should be background gauge invariant. A simple recipe which\\r\\n                                                                                  (N )\\r\\nsatisfies all these constraints is to to make the replacements A → Q, ∂ → D̄ in OG . A\\r\\nmore thorough path to arrive at the same answer would involve working out the generalised\\r\\ngauge invariance and its associated generalised BRST symmetry. In turn one could write\\r\\nthe ghost operator in gBRST exact form, in the background field formalism.\\r\\n\\r\\n6.1   Bases of operators up to four loops\\r\\n                                                                                                (N )\\r\\nIn the background field method, we determine the renormalisation constants of O1 by\\r\\ncomputing the counterterms of correlators of the background field\\r\\n   \\x10         \\x11a1 a2             Z                              h                            i\\r\\n      (N )                                                                             (N )\\r\\n     ΓOi ;BB                µ\\r\\n                    (g, ξ; p ) = dd x1 dd x2 eip·(x1 −x2 ) h0|T Bνa11 (x1 )Bνa22 (x2 )Oi (0) |0i1PI ,\\r\\n                 ν1 ν2\\r\\n                                                                                     (6.14)\\r\\nwhere the subscript 1PI indicates one-particle-irreducibe, amputated Green’s functions. In\\r\\n                                      (N )   (N )\\r\\nthe equation above, the operator Oi = Oi (B + Q) is inserted with zero momentum.\\r\\n                                 (N )        (N )\\r\\nCounterterms proportional to OBEOM and OBG are required in order to cancel divergences\\r\\nof the diagrams that contribute to eq. (6.14). Notably, these unphysical operators always\\r\\ninvolve at least one quantum gluon or a ghost-antighost pair, as it follows from the def-\\r\\ninitions in eqs. (6.8) and (6.9). Therefore, EOM and ghost operators are only required\\r\\nfrom the two-loop level onwards, in order to cancel the subdivergences of the correlator in\\r\\neq. (6.14); and no unphysical counterterm can arise at one loop [54, 62]. In table 4 we re-\\r\\n                                                     (N )\\r\\nport example diagrams showing subdivergences of ΓOi ;BB , which are renormalised by each\\r\\n        (N ),k                                                                                    (N ),k\\r\\nterm OEOM . Table 5 summarises the maximal loop order at which each operator OEOM\\r\\nenters the renormalisation of eq. (6.14). By comparing the last line of tables 2 and 5 we\\r\\nfind that there is an advantage in renormalising correlators of background fields, in that\\r\\nunphysical counterterms are needed only up to 3 loops. In contrast without background-\\r\\n                                    (N ),1\\r\\nfield invariance the counterterm OEOM would be required up to 4 loops, as in table 2.\\r\\n\\r\\n     In the next section we will compute the counterterms required to renormalise these\\r\\nsubdivergences. To this end, it is convenient to reduce to a basis of independent operators.\\r\\nIn the background-field method a basis for a given fixed value of N is obtained by modifying\\r\\nthe corresponding basis obtained without background field, according to the replacements:\\r\\n\\r\\n                   (D.F )a −→ (D.F (Q + B))a ,                      (∂ i Aa ) −→ (D̄ i Q)a ,    (6.15)\\r\\n                         (∂c̄a ) −→ (D̄c̄)a ,                        (∂ i ca ) −→ (D̄ i c)a .   (6.16)\\r\\n\\r\\nFor instance, the basis for N = 2 can be directly read off eq. (5.3), giving\\r\\n                                   (2)\\r\\n                                O1 = Fνa (Q + B)F ν;a (Q + B) ,                                 (6.17)\\r\\n                                   (2)\\r\\n                                O2 = (D.F (Q + B))a Qa + ca D̄ aa1 D̄ a1 a2 ca2 .               (6.18)\\r\\n\\r\\nSimilarly, bases for N = 4 and N = 6 are obtained by applying eqs. (6.15) and (6.16) to\\r\\neqs.(5.12)-(5.14) and to eqs.(5.29)-(5.34), respectively.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                     – 29 –\\r\\n\\x0c\\n\\n                  (N ),≤1                    (N ),≤2                         (N ),≤3                     (N ),≤4\\r\\n    L        OBEOM                      OBEOM                             OBEOM                      OBEOM\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    4\\r\\n\\r\\n\\r\\n\\r\\nTable 4. In the Lth row the table gives examples of diagrams contributing to the L-loop contri-\\r\\n            (N )                                                                          (N ),k\\r\\nbution to ΓO1 ;BB . Subgraphs whose UV-counterterms require the various EOM operators OEOM\\r\\nare highlighted with dashed boxes.\\r\\n\\r\\n                              (N )     (N ),1          (N ),2          (N ),3          (N ),4\\r\\n                            ΓO1 ;BB   OBEOM        OBEOM          OBEOM            OBEOM\\r\\n                                1        0               0               0               0\\r\\n                                2        1               1               0               0\\r\\n                                3        2               2               1               0\\r\\n                                4        3               3               2               1\\r\\n\\r\\n                                                                                          (N )           (N ),k\\r\\nTable 5. The table summarizes the loop orders for which the mixing of O1                         into OEOM is required,\\r\\n                               (N )\\r\\ngiven a certain loop order of ΓO1 ;BB .\\r\\n\\r\\n\\r\\n7   Calculations and results\\r\\n\\r\\nIn this section we renormalise gauge invariant operators of spin N = 2, 4 and 6, using the\\r\\nbases in eqs. (5.2)-(5.3), (5.12)-(5.14) and (5.29)-(5.34), respectively. In these bases, we\\r\\n                                                                   (N )\\r\\nproceed to calculate the associated renormalisation constants Zi,j , defined in eq. (2.23),\\r\\nwhich in the MS scheme can be expanded as follows,\\r\\n                                                                          ∞\\r\\n                                                                          X\\r\\n                       (N )             (N )                    (N )        1          (N ),r\\r\\n                     Zi j = δi j + δZi j ,        with δZi j =                     Zi j         (αs ).             (7.1)\\r\\n                                                                          r=1\\r\\n                                                                              ǫr\\r\\n\\r\\n                                                                         (N )\\r\\nThe renormalisation matrix is block triangular with Zj>1 1 = 0, as described in eq. (4.19),\\r\\n           (N )\\r\\nand only Z1 1 is required to describe the scale evolution of the gauge invariant operator O1N\\r\\nin physical matrix elements. In particular, from the definition of the anomalous dimension\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                       – 30 –\\r\\n\\x0c\\n\\n                  (N ),≤1                        (N ),≤2                   (N ),≤3     (N ),≤4\\r\\n      L         OEOM+G                      OEOM+G                       OEOM+G      OEOM+G\\r\\n\\r\\n\\r\\n\\r\\n      2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      4\\r\\n\\r\\n\\r\\n\\r\\nTable 6. In the Lth row the table gives examples of diagrams containing a ghost-anti-ghost pair\\r\\n                                                               (N )\\r\\nand gluons whose UV-counterterms determine the mixing of O1 into both the EOM - and ghost\\r\\n                                                                           (N )\\r\\n- operators as required for the computation of the L-loop contribution to Γ1;BB .\\r\\n\\r\\n\\r\\nmatrix,\\r\\n                                    (N )               d2 (N ) −1\\r\\n                                  γij      = −µ2         Z    (Z )kj ,                           (7.2)\\r\\n                                                      dµ2 ik\\r\\none can obtain\\r\\n                                               (N )         ∂   (N ),1\\r\\n                                γ (N ) ≡ γ1 1 = αs            Z        (αs ) .                   (7.3)\\r\\n                                                           ∂αs 1 1\\r\\nOff-diagonal elements of the renormalisation matrix do not contribute to the anomalous\\r\\ndimension of the physical operators. However, the computational method that we adopt\\r\\n                (N )                                                       (N )\\r\\nto determine Z1 1 requires the knowledge of a set of mixing contributions Z1 i>1 . Below\\r\\nwe describe the calculation of these renormalisation constants and that of the physical\\r\\nanomalous dimension.\\r\\n\\r\\n7.1       Mixing with EOM and Ghost Operators\\r\\n                                        (N )\\r\\nThe renormalisation constants Z1 i , with i > 1, are determined by the counterterms\\r\\n                                                                                             (N )\\r\\nof one-particle-irreducible, amputated Green functions, with one insertion of O1 and\\r\\nexternal ghost and gluon fields. We list examples of diagrams contributing to such Green’s\\r\\nfunctions in table 6 for general N . In practice, if we work at fixed values of N , not all\\r\\nthese contributions enter. In table 7 we show the structure of the relevant counterterms\\r\\nfor N = 2, 4 and 6. Specifically, we consider the following correlators with an operator\\r\\ninsertion at zero momentum,\\r\\n                             Z\\r\\n         (N )                                               \\x02                 (N ) \\x03\\r\\n       (Γi;cc )ab (g, ξ, p) = dd x1 dd x2 eip·(x1 −x2 ) h0|T ca (x1 )cb (x2 )Oi (0) |0i1PI .      (7.4)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      – 31 –\\r\\n\\x0c\\n\\nExamples of Feynman diagrams contributing to eq. (7.4) are depicted in the first column\\r\\n                                       (N )                                   (N )    (N )\\r\\nof table 6. For every value of N , Γi;cc vanishes at tree level, unless Oi         = O2 , as it\\r\\ncan be seen by inspecting the operators bases in eqs. (5.3), (5.12)-(5.14) and (5.29)-(5.34).\\r\\nTherefore we write\\r\\n                                  \\uf8f1     h                             i\\r\\n                                  \\uf8f4  ab Γ(N ),0 (p) + δΓ(N ) (g, ξ, p)\\r\\n                                  \\uf8f2δ\\r\\n                                  \\uf8f4         i;cc̄        i;cc            if i = 2\\r\\n               (N ) ab\\r\\n             (Γi;cc ) (g, ξ, p) =                                                          (7.5)\\r\\n                                  \\uf8f4\\r\\n                                  \\uf8f4\\r\\n                                  \\uf8f3 δab δΓ(N ) (g, ξ, p)                if i 6= 2\\r\\n                                                 i;cc\\r\\n\\r\\n                                                                                       (N )\\r\\nin order to separate the tree level contribution from the term δΓi;cc̄ , which represents the\\r\\nsum of loop corrections to all orders, namely\\r\\n                                                     ∞\\r\\n                                                     X                    \\x10 α \\x11r\\r\\n                                  (N )                      (N ),r          s\\r\\n                                δΓi;cc (g, ξ, p) =         Γi;cc (ξ, p)            ,                 (7.6)\\r\\n                                                     r=1\\r\\n                                                                           4π\\r\\n\\r\\n          g    2                         (N )                                                 (N )\\r\\nwith αs = 4π . Counterterms of δΓi;cc̄ must therefore be proportional to O2 . In particular,\\r\\n               (N )\\r\\ninserting O1          into eq. (7.4), we get\\r\\n                                     h       i\\r\\n                                        (N )       (N ) (N ),0\\r\\n                                  Z δΓ1;cc̄ = Zc δZ1 2 Γ2,cc̄ ,             ∀N                       (7.7)\\r\\n\\r\\nwhere Z extracts the local counterterm of each Feynman diagram contributing to eq. (7.4).\\r\\nTo this end, we apply the R∗ operation [63–66], using a formulation that is valid for a\\r\\ngeneral Feynman rule of the inserted operator [67–70]\\r\\n                            h       i         h                 i\\r\\n                               (N )                   (N )\\r\\n                         Z δΓ1;cc̄ = −Kǫ R̄∗ Tp(N ) δΓ1;cc̄ |p=0 .                  (7.8)\\r\\n\\r\\n        (N )\\r\\nHere Tp   denotes a Taylor expansion operator which extracts the term of order pN . The\\r\\noperation Kǫ extracts the singular terms of Laurent series in ǫ\\r\\n                               \" ∞           #   −1\\r\\n                                 X               X\\r\\n                            Kǫ        f(k) ǫk =       f(k) ǫk ,                    (7.9)\\r\\n                                         k=−n                  k=−n\\r\\n\\r\\nand the operation R̄∗ isolates the local counterterm by subtracting all UV subdivergences\\r\\n                                                                                  (N )\\r\\nand IR divergences. In addition to eq. (7.7), we determined the elements Z1 2 of the mixing\\r\\nmatrix using an alternative approach, described in appendix A. In this way we obtain\\r\\n                                   \\x14                         \\x15 \\x10 \\x11\\r\\n    (2)     αs CA \\x10 αs \\x112 2 19                  5 ξF     35        αs 3 3 h       779\\r\\n δZ1 2 = −           +        CA         2\\r\\n                                            +         −       +          CA −\\r\\n            4π 2ǫ        4π          24ǫ       48 ǫ      48ǫ       4π            432ǫ3\\r\\n            \\x10\\r\\n          1 2807 35ξF          5ξ 2 \\x11     1  \\x10    16759 11ζ3        377ξF     5ζ3 ξF     65ξF2 \\x11i\\r\\n        + 2          −       + F +             −         −       +         +           −\\r\\n          ǫ    864      216    288         ǫ       7776      72      1728       72        1728\\r\\n              4\\r\\n        + O(αs ),                                                                              (7.10)\\r\\n                       \\x10    \\x11      \\x14                              \\x15 \\x10 \\x11           h\\r\\n    (4)     αs CA        αs 2 2        97          ξF      8641         αs 3 3 9437\\r\\n δZ1 2 = −           −        CA             2\\r\\n                                               −        +           +         CA\\r\\n            4π 12ǫ       4π          1440ǫ        320ǫ 86400ǫ           4π          86400ǫ3\\r\\n          1 \\x10      1520341    853ξF   \\x11      1 \\x10    166178237       ζ3     37199ξF       37ζ3 ξF \\x11i\\r\\n        + 2 −               +           +        −              −       +            +\\r\\n          ǫ       15552000    86400          ǫ      466560000 2400          648000         9600\\r\\n\\r\\n\\r\\n\\r\\n                                                     – 32 –\\r\\n\\x0c\\n\\n                        (N ≥2)                   (4)                 (6)                        (6)\\r\\n     L             O2                          O3                 O3                      Oi∈{3,4,5,6}\\r\\n\\r\\n\\r\\n\\r\\n     2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n     3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n     4\\r\\n\\r\\n\\r\\n\\r\\nTable 7. In the Lth row the table gives examples of diagrams containing a ghost-anti-ghost pair and\\r\\n                                                                                           (N ∈{2,4,6})\\r\\ngluons whose UV-counterterms determine the mixing of O(N ) into unphysical operators Oi\\r\\n                                                               (N ∈{2,4,6}),L\\r\\nas required for the computation of the L-loop contribution to Z11             in the background field\\r\\nmethod.\\r\\n\\r\\n\\r\\n           + O(α4s ),                                                                                    (7.11)\\r\\n                             \\x10 α \\x112        \\x14                                   \\x15\\r\\n     (6)        αs CA     s            2         653       19ξF   185093\\r\\n δZ1 2 = −             −              CA              2\\r\\n                                                        +       +         + O(α3s ).                     (7.12)\\r\\n                4π 30ǫ   4π                    10080ǫ     20160ǫ 4233600ǫ\\r\\n\\r\\nHere ξF = 1 − ξ is the gauge fixing parameter, such that ξF = 0 recovers the result in\\r\\nFeynman gauge.\\r\\n                                           (N >2)\\r\\n    In order to extract the terms Z1 i>2 , we compute the counterterms of three- and four-\\r\\npoint correlators, depicted in the second and in the third columns of table 6, respectively.\\r\\nFor this purpose we consider the three-point Green’s function\\r\\n                                  Z\\r\\n      (N )\\r\\n   (Γi;ccg )abc\\r\\n            µ   (g, ξ, p  ,\\r\\n                         1 2p ) =   dd x1 dd x2 dd x3 eip1 ·(x1 −x3 ) eip2 ·(x2 −x3 )\\r\\n                                                            h                              i\\r\\n                                                              a         b        c    (N )\\r\\n                                                  × h0|T c (x1 )c (x2 )Aµ (x3 )Oi (0) |0i, (7.13)\\r\\n\\r\\nwhich is expanded as follows:\\r\\n    \\x10           \\x11                      h\\x10       \\x11                 X∞ \\x10        \\x11                 \\x10 α \\x11r i\\r\\n         (N ) abc                         (N ),0 abc                   (N ),r abc                  s\\r\\n        Γi;cc̄g    (g, ξ, p1 , p2 ) = g Γi;cc̄g      (p1 , p2 ) +     Γi;cc̄g     (ξ, p1 , p2 )          ,\\r\\n                 µ                               µ                              µ                 4π\\r\\n                                                                  r=1\\r\\n                                                                 |                   {z                  }\\r\\n                                                                              \\x10            \\x11\\r\\n                                                                                    (N) abc\\r\\n                                                                                  δΓi;cc̄g\\r\\n                                                                                            µ\\r\\n                                                                                (7.14)\\r\\nwhere we separated the tree-level contribution from the loop corrections, similarly to\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                       – 33 –\\r\\n\\x0c\\n\\n                                                                                                      (N )\\r\\neqs. (7.5) and (7.6). The counterterm of eq. (7.13), with an insertion of O1 , reads\\r\\n                     \\x14\\x10       \\x11 \\x15          p X (N ) \\x10 (N ),0 \\x11abc\\r\\n                          (N ) abc\\r\\n                   Z δΓ1;cc̄g      = gZg Zc Z3        δZ1 k Γk,cc̄g      .           (7.15)\\r\\n                                           µ                                                 µ\\r\\n                                                                    k>1\\r\\n\\r\\n                  (N ),0                                                                                         (N )\\r\\nThe terms g(Γk,cc̄g )abc\\r\\n                      µ are ghost-antighost-gluon vertices generated by each operator Ok ,\\r\\nwith k > 1. Notably, there is no such counterterm for N = 2, as it can be seen by inspecting\\r\\n  (2)                                         (4)\\r\\nO2 in eq. (5.3). For N = 4, the operator O2 , given in eq. (5.13), generates both the ghost-\\r\\nantighost vertex and the ghost-antighost-gluon vertex. Therefore, the same counterterm\\r\\n   (4)\\r\\nδZ1 2 will suffice to renormalise both eqs. (7.7) and (7.15). For consistency, we verified that\\r\\n   (4)\\r\\nδZ1 2 extracted from eq. (7.15) agrees with the result in eq. (7.11). For N = 6 we find\\r\\n          \\x14\\x10         \\x11abc \\x15         p \\x14 (6) \\x10 (6),0 \\x11abc             \\x10      \\x11 \\x15\\r\\n                (6)                                              (6)   (6),0 abc\\r\\n       Z δΓ1;cc̄g           = gZg Zc Z3 δZ1 2 Γ2,cc̄g       + δZ1 3 Γ3,cc̄g      ,       (7.16)\\r\\n                           µ                                              µ                       µ\\r\\n\\r\\n                                     (6)\\r\\nwhich can be solved for δZ1 3 , upon computing the left hand-side, by means of the R∗\\r\\n                                              (6)\\r\\noperation, and by replacing the result for δZ1 2 , given in eq. (7.12). We get\\r\\n                                     \\x14                                 \\x15\\r\\n        (6)     αs CA \\x10 αs \\x112 2          2021        235813\\r\\n      δZ1 3 = −        −         CA             +             + O(ξF ) + O(α3s ), (7.17)\\r\\n                4π 48ǫ     4π          40320ǫ2      8467200ǫ\\r\\nwhere we performed the calculation in Feynman gauge, dropping terms proportional to ξF .\\r\\n                                                                                                                   (4)\\r\\n    Finally, we determine the remaining elements of the mixing matrices for operators O1\\r\\n      (6)\\r\\nand O1 , by computing the counterterms of the four-point functions\\r\\n                                        Z\\r\\n     (N )\\r\\n   (Γi;ccgg )abcd\\r\\n             µν   (g, ξ, p  , p\\r\\n                           1 2 3, p ) =    dd x1 dd x2 dd x3 dd x4 eip1 ·(x1 −x4 ) eip2 ·(x2 −x4 ) eip3 ·(x3 −x4 )\\r\\n                                                h                                               i\\r\\n                                                                                         (N )\\r\\n                                        × h0|T ca (x1 )cb (x2 )Acµ (x3 )Adν (x4 )Oi (0) |0i,                   (7.18)\\r\\n                           h\\x10         \\x11                     X∞ \\x10         \\x11                       \\x10 α \\x11r i\\r\\n                              (N ),0 abcd                        (N ),r abcd                        s\\r\\n                    ≡ g2     Γi;cc̄gg     (p1 , p2 , p3 ) +     Γi;cc̄gg      (ξ, p1 , p2 , p3 )          .\\r\\n                                       µν                                  µν                      4π\\r\\n                                                            r=1\\r\\n                                                            |                  {z                     }\\r\\n                                                                                  (N)\\r\\n                                                                               (δΓi;cc̄gg )abcd\\r\\n                                                                                           µν\\r\\n\\r\\n                                                                                                              (7.19)\\r\\n\\r\\nThe counterterms of eq. (7.18) are given by\\r\\n                  \\x14\\x10         \\x11abcd \\x15                X (N ) \\x10 (N ),0 \\x11abcd\\r\\n                       (N )\\r\\n               Z δΓ1;cc̄gg           = g2 Zg2 Zc Z3  δZ1 k Γk,cc̄gg       .                                   (7.20)\\r\\n                                           µν                                                µν\\r\\n                                                                    k>1\\r\\n\\r\\nBy specialising the equation above to N = 4, we find that it receives only one contribution\\r\\n                                                (4)\\r\\nfrom the vertex associated to the operator O3 , written in eq. (5.14). We get\\r\\n                      \\x14\\x10        \\x11abcd \\x15                       \\x10        \\x11abcd\\r\\n                           (4)                            (4)   (4),0\\r\\n                    Z δΓ1;cc̄gg         = g 2 Zg2 Zc Z3 δZ1 3 Γ3,cc̄gg       ,        (7.21)\\r\\n                                                µν                                      µν\\r\\n\\r\\nwhich leads to\\r\\n                                                     (4)   αs CA\\r\\n                                                δZ1 3 =           + O(α2s ).                                  (7.22)\\r\\n                                                           4π 24ǫ\\r\\n\\r\\n\\r\\n\\r\\n                                                           – 34 –\\r\\n\\x0c\\n\\n                                   (6)\\r\\nFor N = 6, all operators Oi>1 contribute to eq. (7.20). By plugging the known results for\\r\\n   (6)             (6)\\r\\nδZ1 2 and δZ1 3 , given in eqs. (7.12) and (7.17) respectively, into eq. (7.20), we get\\r\\n\\r\\n         (6)        αs CA         \\x01                (6)      αs CA         \\x01    (6)        \\x01\\r\\n       δZ1 4 = −           + O α2s ,            δZ1 5 =            + O α2s , δZ1 6 = O α2s .                        (7.23)\\r\\n                    4π 32ǫ                                  4π 24ǫ\\r\\n                                                                  (6)\\r\\nThe terms of O(α2s ) contribute to renormalise O1 only at four loops, because they arise\\r\\nfrom divergent four-point subdiagrams at two loops, such as the one depicted in the botton\\r\\nright entry of table 6. In this work we renormalise the gauge invariant operator of spin\\r\\nN = 6 up to three loops and therefore we don’t need to compute such contributions.\\r\\n    Eqs. (7.10)-(7.12), (7.17), (7.22) and (7.23) include all off-diagonal terms of the mixing\\r\\n           (N )                                                                         (N )\\r\\nmatrix δZ1 i , which are required to renormalise the gauge invariant operators O1 at\\r\\nN = 2 and 4 up to 4 loops and N = 6 up to three loops. The calculation of the physical\\r\\n                (N )\\r\\ncontribution Z1 1 is described in the remaining part of this section.\\r\\n\\r\\n7.2      Renormalisation of physical operators\\r\\n                                            (N )\\r\\nThe renormalisation constants Z1 1 , which determine the anomalous dimension of the\\r\\ngauge invariant operator via eq. (7.3), are best extracted from correlators of the background\\r\\nfield B. Using the definition in eq. (6.14) and the definition of the gauge invariant operators\\r\\nin eq. (2.18) we have\\r\\n                \\x10       \\x11ab              \\x10        \\x11ab\\r\\n                   (N )                      (N )\\r\\n                  Γ1;BB       (g, ξ, p) ≡ Γ1;BB              (g, ξ, p)∆µ1 ..∆µN .        (7.24)\\r\\n                              ν1 ν2                            ν1 ν2 ;µ1 ..µN\\r\\n\\r\\nThe renormalisation of eq. (7.24) requires a single counterterm\\r\\n                          \\x14\\x10       \\x11ab \\x15               \\x10       \\x11\\r\\n                              (N )                (N )   (N ),0 ab\\r\\n                       Z Γ1;BB            = ZB Z1 1 Γ1;BB          ,                                                (7.25)\\r\\n                                              ν1 ν2                               ν1 ν2\\r\\n\\r\\n            (N )\\r\\nwhere (Γ1;BB )ab\\r\\n               ν1 ν2 is the tree-level contribution to eq. (7.24). In practice, applying the R\\r\\n                                                                                               ∗\\r\\n\\r\\noperation becomes computationally challenging at higher loop orders or higher N -values .     4\\r\\n\\r\\nInstead, we renormalise the bare Green’s functions, which are defined by using bare fields\\r\\n             (N ),b\\r\\n(including Oi       ) and bare parameters in eq. (6.14). We compute the scalar quantities\\r\\n\\r\\n                                   δa1 a2 gν1 ν2 HN1 N (p) \\x10 (N ) \\x11a1 a2\\r\\n                                                   µ ..µ\\r\\n          (N )\\r\\n         Γi;BB (gB , ξB , p2 ) =                            Γi;BB                 (gB , ξB , p),                    (7.26)\\r\\n                                    NA          (d − 1)            ν1 ν2 ;µ1 ..µN\\r\\n\\r\\nwhere d = 4 − 2ǫ is the dimension of spacetime and NA the dimension of the adjoint\\r\\n                                      µ1 ..µN\\r\\nrepresentation of the gauge group. HN         (p) are the harmonic tensors introduced in refs.\\r\\n[20, 71, 72], which project the Green’s function on its symmetric and traceless component.\\r\\nThe harmonic projectors are defined to satisfy\\r\\n           µ1 ..µN                                       µ ..µi ..µj ..µN          µ ..µj ..µi ..µN\\r\\n          HN       (p) gµi µj = 0         and         HN1                   (p) = HN1                 (p)   ∀i, j\\r\\n             µ1 ..µN           µ ..µ\\r\\n            HN       (p)pµN = HN1−1 N−1 (p) p2 ,                                                                    (7.27)\\r\\n   4\\r\\n    The mass dimension of the operator increases with the spin, as d = N + 2, and therefore also the degree\\r\\nof divergence of the Feynman diagrams of eq. (7.24). This requires to compute high order terms in the\\r\\nTaylor expansion of the diagrams, see eq. (7.8), which can generate large numbers of terms.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         – 35 –\\r\\n\\x0c\\n\\nand they are explicitly constructed in [20]. We generated all the Feynman diagrams that\\r\\ncontribute to eq. (7.26) with QGRAF [73], we performed the color and Lorentz algebra with\\r\\ninhouse code, which is written in FORM [61] and makes use of the package COLOR [60]. All\\r\\nthe Feynman integrals that contribute to eq. (7.26) are massless two-point functions, also\\r\\ncalled p-integrals [74–77], which we computed with the code Forcer [78]. In order to\\r\\nrenormalise eq. (7.26), we separate the tree-level from loop contributions\\r\\n                                     \\uf8f1 (N ),0       (N )\\r\\n                                     \\uf8f4\\r\\n                                     \\uf8f2 Γi;BB + δΓi;BB (gB , ξB ), for i = 1\\r\\n                   (N )\\r\\n                  Γi;BB (gB , ξB ) =                                               (7.28)\\r\\n                                     \\uf8f4\\r\\n                                     \\uf8f3 (N )\\r\\n                                       δΓi;BB (gB , ξB ),         for i 6= 1.\\r\\n\\r\\nwhere we omit the dependence on p2 , which can be reconstructed via dimensional analysis,\\r\\nand with\\r\\n                                          ∞\\r\\n                                          X             \\x10 α \\x11r\\r\\n                         (N )                 (N ),r        s,B\\r\\n                      δΓi;BB (gB , ξB ) =   Γi;BB (ξB )         ,                 (7.29)\\r\\n                                                           4π\\r\\n                                               r=1\\r\\n                2 /(4π). Upon considering Γ       (N )\\r\\nwhere αs,B = gB                             1;BB (gB , ξB ) in eq. (7.26), we find the renor-\\r\\nmalised correlator to obey\\r\\n                  \"                                                       #\\r\\n                      h                     X                           i\\r\\n                         (N ) (N )                 (N )    (N )\\r\\n              Kǫ ZB Z1 1 Γ1;BB (gB , ξB ) +     δZ1 i δΓi;BB (gB , ξB ) = 0,           (7.30)\\r\\n                                                  i>1\\r\\n\\r\\nwhere ZB = Z12 is the renormalisation constant of the background field [52, 53]. Eq. (7.30)\\r\\n              g\\r\\n                                                                   (N )\\r\\ncan be solved in terms of the renormalisation constant Z1 1 of the gauge invariant operator.\\r\\nUsing identities eqs. (7.1) and (7.28), we then get\\r\\n                                           \\uf8ee                              \\uf8f9\\r\\n                     (N ) (N ),0     1          X    (N )  (N )\\r\\n                   δZ1 1 Γ1;BB = −      Kǫ \\uf8f0ZB      Z1 i δΓi;BB (gB , ξB )\\uf8fb .         (7.31)\\r\\n                                    ZB\\r\\n                                                     i≥1\\r\\n\\r\\n                                                                                                    (N )\\r\\nThe equation above holds to all loop orders. The renormalisation constants Z1 i>1 , on the\\r\\n                                                                                                    (N )\\r\\nright hand-side of eq. (7.31), are required to renormalise sub-divergences of Γ1;BB , which\\r\\ninvolve quantum gluons and/or a ghost-antighost pair. Each sub-divergence is proportional\\r\\n                                      (N )\\r\\nto one of the unphysical operators Oi>1 . This determines the maximal loop order at which\\r\\n (N )\\r\\nZ1 i has been computed, as shown in table 7. The diagonal renormalisation constant,\\r\\n   (N )                                                                       (N )\\r\\nδZ1 1 , appears on both sides of eq. (7.31). However, we notice that the Z1 1 appearing\\r\\n                                          (N )\\r\\non the right hand-side is multiplied by δΓ1;BB (gB , ξB ), which starts at O(αs ). Therefore,\\r\\n                                  (N )                                                      (N )\\r\\neq. (7.31) allows us to compute Z1 1 at L-loops, given the knowledge of Z1 i at l < L loops\\r\\n                                                       (N )\\r\\nas discussed before. We plug the l-loop values of Z1 i , given in eqs. (7.10)-(7.12), (7.17),\\r\\n(7.22) and (7.23) respectively, into eq. (7.31). After computing the relevant correlators\\r\\n   (N )\\r\\nδΓi;BB (gB , ξB ) at the required L − l loop order, we find\\r\\n  (2)            \\x01\\r\\n Z1 1 = 1 + O α5s ,                                                                                        (7.32)\\r\\n                         \\x10 α \\x112          \\x12                 \\x13       \\x10 α \\x113          \\x12\\r\\n   (4)        αs 21CA    s         2          28   7121                   s    3            1316\\r\\n Z1 1 = 1 +           +           CA             +             +              CA       −\\r\\n              4π 5ǫ     4π                   25ǫ2 1000ǫ              4π                    1125ǫ3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                – 36 –\\r\\n\\x0c\\n\\n                                  \\x13 \\x10 \\x11 \\x1a        \\x14\\r\\n             151441     103309639      αs 4    4   11186    1512989    5437269017\\r\\n           −          +             +         CA          +          −\\r\\n             45000ǫ2     4050000ǫ      4π          5625ǫ4   450000ǫ3   162000000ǫ2\\r\\n                                                 \\x15        \\x12                          \\x13\\x1b\\r\\n             1 \\x10 1502628149 1146397ζ3     126ζ5 \\x11     dAA 21623 3899 ζ3      1512 ζ5\\r\\n           +                +           −          +              +        −\\r\\n             ǫ 13500000          45000      5         NA     600ǫ     15ǫ      5ǫ\\r\\n           + O(α5s ),                                                                             (7.33)\\r\\n                            \\x10 α \\x112     \\x12                     \\x13       \\x10 α \\x113        \\x12\\r\\n     (6)         αs 83 CA       s    2    7885     1506899             s       3            465215\\r\\n    Z1 1 = 1 +             +       CA          2\\r\\n                                                 +               +            CA       −\\r\\n                 4π 14ǫ        4π        1176ǫ     148176ǫ            4π                   148176ǫ3\\r\\n                                       \\x13\\r\\n             243375989     96390174479           \\x01\\r\\n           +           2\\r\\n                         +               + O α4s ,                                                (7.34)\\r\\n             18670176ǫ     2613824640ǫ\\r\\n\\r\\nwhere dAA = dabcd4  dabcd\\r\\n                     4    , with dabcd\\r\\n                                  4    defined in eq. (3.21). As a check on our calculation, we\\r\\nverified that all non-local divergences of the form 1/ǫp logq (µ2 /p2 ), which appear in the bare\\r\\ncorrelators, cancel upon combining the required counterterms. Furthermore, we verified\\r\\nthat the dependence on the gauge parameter ξ cancels up to three loops in eqs. (7.32)\\r\\nand (7.33). The O(α4s )-terms in those equations were computed only in Feynman gauge.\\r\\nSimilarly, the calculation of the O(α3s )-terms in eq. (7.34) was performed in Feynman\\r\\n                                                                            (2)\\r\\ngauge and the cancellation of ξ was verified to two loops. The result Z1 1 = 1, in eq. (7.32),\\r\\n                                                                  (2)\\r\\nagrees with the findings of refs. [79, 80], which imply that O1 does not renormalise to all\\r\\norders. Finally, by extracting the anomalous dimension γ (N ) , as written in eq. (7.3), we\\r\\nfind agreement with the results at three and at four loops given in refs. [20] and [34].\\r\\n\\r\\n8     Conclusions\\r\\n\\r\\nIn this paper we generalised a method, originally by Dixon and Taylor [37], for the con-\\r\\nstruction of unphysical operators which are required for the renormalisation of Green’s\\r\\nfunctions with insertions of twist-two gluonic operators. As one increases the loop order\\r\\nof the Green’s function more unphysical operators are in general required for its renormal-\\r\\nisation. The previously known basis was restricted to two-loop calculations, and it was\\r\\nunclear how to systematically extend it to higher loop order, thereby preventing the OPE\\r\\nmethod to be used for calculations of the singlet splitting functions. We have uncovered\\r\\na general and systematic formalism for extending the basis to arbitrary loop order. Using\\r\\nthis formalism we then worked out the explicit basis for calculations up to four-loop order\\r\\nand used it to perform calculations of the N = 2, 4 Mellin moments at four loops and the\\r\\nN = 6 Mellin moment at three loops, obtaining the correct known results.\\r\\n     The formalism we developed can essentially be broken down to a few key concepts.\\r\\nThe first is that we identified the gluonic gauge-variant operators in the Dixon-Taylor\\r\\nbasis with EOM operators, these are not EOM operators of the gauge-fixed Lagrangian,\\r\\nbut EOM operators of the gauge invariant part of the Lagrangian. With this identification\\r\\nwe could easily write down the all-loop structure of the EOM operator. The second concept\\r\\nis that of a generalised gauge transformation which leaves invariant the Lagrangian made\\r\\nup of the gauge invariant and EOM operators. Following the works of Hamberg and Van\\r\\nNeerven [13] and Joglekar and Lee [42] this generalised gauge invariance is promoted to a\\r\\ngeneralised BRST symmetry. We then propose that the most general ghost operator can\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             – 37 –\\r\\n\\x0c\\n\\nbe written as the generalised BRST action acting on a single BRST ancestor operator. The\\r\\nghost operator is therefore identified as an BRST-exact operator in the BRST generalised\\r\\nsense. This proposition not only reproduces the previously known ghost operators required\\r\\nat two loops, but we also confirmed that it complies with the theorems of Joglekar and\\r\\nLee [42]. Indeed we show that the operators generated with our procedure can be always\\r\\nwritten as a sum of a BRST-exact term (in the sense of the original, not generalised, BRST\\r\\ntransformations) and a term that vanishes on the equation of motion of the complete Yang-\\r\\nMills lagrangian, as required by [42].\\r\\n      We explored two further symmetry principles to simplify calculations of unphysical\\r\\ncounterterms. The first is the anti-BRST symmetry which can be used to derive a ghost\\r\\nanti-ghost exchange symmetry of the ghost operators. This symmetry allows one to drasti-\\r\\ncally reduce the number of independent unphysical operators. Another symmetry principle\\r\\nis background field gauge invariance, which we employed in our calculations. Background\\r\\nfield invariance allows to do calculations without unphysical operators at the one-loop level,\\r\\nbeyond one-loop counterterms a number of unphysical operators are however still required\\r\\nto perform calculations.\\r\\n      The task of computing unphysical counterterms requires the extraction of local renor-\\r\\nmalisation counterterms of Green’s functions containing a ghost anti-ghost pair and mul-\\r\\ntiple gluons. For instance to determine the anomalous dimension of the gauge invariant\\r\\noperator at the four-loop level generally requires, among others, the counterterms associ-\\r\\nated to Green’s functions containing a ghost anti-ghost pair with two gluons at two loops\\r\\nand with one gluon at three loops. These quantities can thus not be extracted through\\r\\na naive calculation of a self energy diagram. In this work we employed a fully auto-\\r\\nmated implementation of the local R∗ -operation, an operation which allows to extract the\\r\\ncounterterms of Greens’s functions of arbitrary many external particles from self energy\\r\\ndiagrams, via the technique of IR rearrangement and IR subtractions. However the R∗ -\\r\\noperation becomes very expensive for higher moments, due to the many derivatives and\\r\\nmany counterterms one requires. Already at N = 6 we found that the calculations were\\r\\nbecoming prohibitively time-consuming even with substantial computing resources. It may\\r\\nbe possible with further optimisation to push the R∗ -approach to higher N , however we\\r\\nbelieve that a more streamlined approach could be more promising. We leave further\\r\\nimprovements of this task to the future.\\r\\n      Assuming that the problem of calculating these UV counterterms can be solved ef-\\r\\nficiently one can expect that the methods presented here should allow for a much more\\r\\nefficient approach to computing Mellin moments of gluonic splitting functions at N3LO than\\r\\nthe brute force approach which was currently used [34]. To extend the methods presented\\r\\nhere to singlet splitting functions containing also quarks will require further extensions of\\r\\nthe formalism. We do not believe these to give major complications.\\r\\n\\r\\nAcknowledgements\\r\\n\\r\\nWe would like to thank Sven Moch, Jos Vermaseren and Andreas Vogt for many insightful\\r\\ndiscussions and their continuous encouragement. G.F. would like to thank Arnd Behring\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                           – 38 –\\r\\n\\x0c\\n\\nand Mattia Dalla Brida for numerous discussions on related topics. F.H. is supported by\\r\\nthe NWO Vidi grant 680-47-551 and the UKRI FLF Mr/S03479x/1. G.F. is supported\\r\\nby the ERC Starting Grant 715049 ‘QCDforfuture’ with Principal Investigator Jennifer\\r\\nSmillie and by the STFC Consolidated Grant ‘Particle Physics at the Higgs Centre’.\\r\\n\\r\\nA    Computing anomalous dimension in QCD\\r\\n\\r\\nIt is convenient to spell out also a procedure to compute anomalous dimensions which does\\r\\nnot rely on the background field method, but involves instead only the calculation of bare\\r\\nGreen’s functions with external gluons or ghosts. These were defined in eqs. (3.12) and\\r\\n(7.4), respectively, and they read\\r\\n                          \\x10       \\x11                 \\x10       \\x11\\r\\n                            (N ) a1 a2                (N ) a1 a2\\r\\n                           Γi;gg         ≡ ∆µ1 ..∆µN Γi;gg                  ,        (A.1)\\r\\n                                   ν1 ν2                     ν1 ν2 ;µ1 ..µN\\r\\n                          \\x10       \\x11                 \\x10       \\x11\\r\\n                            (N ) a1 a2                (N ) a1 a2\\r\\n                           Γi;cc̄        ≡ ∆µ1 ..∆µN Γi;cc̄            .             (A.2)\\r\\n                                                                        µ1 ..µN\\r\\n\\r\\nWe compute these correlators with the help of FORCER, after applying harmonic and colour\\r\\nprojectors to reduce eqs. (A.1) and (A.2), as described below eq. (7.26)\\r\\n\\r\\n                 (N )                    δa1 a2 gν1 ν2 H µ1 ..µN (p) \\x10 (N ) \\x11a1 a2\\r\\n                Γi;gg (gB , ξB , p2 ) =                                 Γi;gg                  ,         (A.3)\\r\\n                                          NA          (d − 1)                   ν1 ν2 ;µ1 ..µN\\r\\n                                         δa1 a2 µ1 ..µN        \\x10       \\x11\\r\\n                 (N )                                              (N ) a1 a2\\r\\n                Γi;cc̄ (gB , ξB , p2 ) =        H        (p) Γi;cc̄              .                       (A.4)\\r\\n                                          NA                            µ1 ..µN\\r\\n\\r\\nBy definition, the ghost correlator Γi;cc̄ doesn’t vanish at tree level, only if we consider\\r\\n                           (N )                                           (N ),1\\r\\ninsertion of the operator O2 , which is chosen to contain the term OG            eq. (4.22), as\\r\\nwe have done in the construction of operator bases for N = 2, 4 and 6 in eqs. (5.3), (5.13)\\r\\n                                    (N )\\r\\nand (5.30). The gluon correlator Γi;gg receives contributions at tree level from both the\\r\\n                               (N )                   (N )                                     (N ),1\\r\\ngauge invariant operator O1 and from O2 , which includes the term OEOM , eq. (3.13),\\r\\n              (N ),1\\r\\nrelated to OG        by (generalised) BRST symmetry. We get\\r\\n                                   \\uf8f1 (N ),0              (N )\\r\\n                                   \\uf8f4           2                       2\\r\\n                                   \\uf8f2 Γi;gg (p ) + δΓi;gg (gB , ξB , p ) for i = 1, 2\\r\\n           (N )\\r\\n          Γi;gg (gB , ξB , p2 ) =                                                     (A.5a)\\r\\n                                   \\uf8f4\\r\\n                                   \\uf8f3 (N )                2\\r\\n                                     δΓi;gg (gB , ξB , p )                  for i > 2\\r\\n                                   \\uf8f1 (N ),0              (N )\\r\\n                                   \\uf8f4           2                       2\\r\\n                                   \\uf8f2 Γi;cc̄ (p ) + δΓi;cc̄ (gB , ξB , p ) for i = 2\\r\\n           (N )\\r\\n          Γi;cc̄ (gB , ξB , p2 ) =                                                    (A.5b)\\r\\n                                   \\uf8f4\\r\\n                                   \\uf8f3 (N )                2\\r\\n                                     δΓi;cc̄ (gB , ξB , p )               for i 6= 2\\r\\n                                                                 (N )\\r\\nIn order to compute the renormalisation constant Z1 1 , we renormalise the bare correlators\\r\\n (N )       (N )                                                        (N ),b\\r\\nΓ1;gg and Γ1;cc̄ , where we inserted the gauge invariant operator O1\\r\\n                                h X                               i\\r\\n                                              (N )\\r\\n                             Kǫ Z3      Z1 i Γi;gg (gB , ξB , p2 ) = 0,             (A.6a)\\r\\n                                          i≥1\\r\\n                                   h      X                                i\\r\\n                                                      (N )\\r\\n                               Kǫ Zc            Z1 i Γi;cc̄ (gB , ξB , p2 ) = 0.                        (A.6b)\\r\\n                                          i≥1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                     – 39 –\\r\\n\\x0c\\n\\nWe separate the contributions of the tree-level terms and of the loop corrections to the\\r\\nGreen’s functions, according to eqs. (A.5a) and (A.5b) and we solve the equations above\\r\\n     (N )      (N )\\r\\nfor Z1 1 and Z1 2 . We find\\r\\n\\r\\n                                            1    h X                                 i\\r\\n                   (N ),0          (N ),0                       (N )\\r\\n             Z1 1 Γ1;gg + δZ1 2 Γ2;gg = −      Kǫ Z3     Z1 i δΓi;gg (gB , ξB , p2 ) ,      (A.7)\\r\\n                                            Z3\\r\\n                                                     i≥1\\r\\n                                            1    h   X                               i\\r\\n                                   (N ),0                       (N )\\r\\n                            δZ1 2 Γ2;cc̄ = − Kǫ Zc       Z1 i δΓi;cc̄ (gB , ξB , p2 ) .     (A.8)\\r\\n                                            Zc\\r\\n                                                         i≥1\\r\\n\\r\\nWe solve the equations above order-by-order is αs . Provided we have knowledge of the\\r\\n                            (N )\\r\\nrenormalisation constants Z1 i up to L − 1 loops, which enter the right hand-side of both\\r\\n                                       (N )\\r\\neqs. (A.7) and (A.8), we determine δZ1 2 to L loops by means of eq. (A.8). We applied\\r\\n                          (N )\\r\\nthis method to compute Z1 2 in eqs.(7.10)-(7.12) with complete dependence on the gauge\\r\\n                                      (N )\\r\\nparameter ξ. Finally, by replacing δZ1 2 at L-loop in the left hand-side of eq. (A.7), we\\r\\n                                 (N )\\r\\ndetermine the renormalisation Z1 1 to L loops.\\r\\n\\r\\nReferences\\r\\n [1] C. Anastasiou, C. Duhr, F. Dulat, F. Herzog and B. Mistlberger, Higgs Boson Gluon-Fusion\\r\\n     Production in QCD at Three Loops, Phys. Rev. Lett. 114 (2015) 212001\\r\\n     [arXiv:1503.06056].\\r\\n [2] C. Anastasiou, C. Duhr, F. Dulat, E. Furlan, T. Gehrmann, F. Herzog et al., High precision\\r\\n     determination of the gluon fusion Higgs boson cross-section at the LHC,\\r\\n     JHEP 05 (2016) 058 [arXiv:1602.00695].\\r\\n [3] B. Mistlberger, Higgs boson production at hadron colliders at N3 LO in QCD,\\r\\n     JHEP 05 (2018) 028 [arXiv:1802.00833].\\r\\n [4] C. Duhr, F. Dulat and B. Mistlberger, Higgs Boson Production in Bottom-Quark Fusion to\\r\\n     Third Order in the Strong Coupling, Phys. Rev. Lett. 125 (2020) 051804\\r\\n     [arXiv:1904.09990].\\r\\n [5] C. Duhr, F. Dulat and B. Mistlberger, Drell-Yan Cross Section to Third Order in the Strong\\r\\n     Coupling Constant, Phys. Rev. Lett. 125 (2020) 172001 [arXiv:2001.07717].\\r\\n [6] X. Chen, T. Gehrmann, E.W.N. Glover, A. Huss, B. Mistlberger and A. Pelloni, Fully\\r\\n     Differential Higgs Boson Production to Third Order in QCD,\\r\\n     Phys. Rev. Lett. 127 (2021) 072002 [arXiv:2102.07607].\\r\\n [7] C. Duhr, F. Dulat and B. Mistlberger, Charged current Drell-Yan production at N3 LO,\\r\\n     JHEP 11 (2020) 143 [arXiv:2007.13313].\\r\\n [8] P. Bargiela, F. Caola, A. von Manteuffel and L. Tancredi, Three-loop helicity amplitudes for\\r\\n     diphoton production in gluon fusion, arXiv:2111.13595.\\r\\n [9] F. Caola, A. Chakraborty, G. Gambuti, A. von Manteuffel and L. Tancredi, Three-loop gluon\\r\\n     scattering in QCD and the gluon Regge trajectory, arXiv:2112.11097.\\r\\n[10] E.G. Floratos, D.A. Ross and C.T. Sachrajda, Higher Order Effects in Asymptotically Free\\r\\n     Gauge Theories. 2. Flavor Singlet Wilson Operators and Coefficient Functions,\\r\\n     Nucl. Phys. B 152 (1979) 493.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              – 40 –\\r\\n\\x0c\\n\\n[11] A. Gonzalez-Arroyo and C. Lopez, Second Order Contributions to the Structure Functions in\\r\\n     Deep Inelastic Scattering. 3. The Singlet Case, Nucl. Phys. B 166 (1980) 429.\\r\\n[12] W. Furmanski and R. Petronzio, Singlet Parton Densities Beyond Leading Order,\\r\\n     Phys. Lett. B 97 (1980) 437.\\r\\n[13] R. Hamberg and W.L. van Neerven, The Correct renormalization of the gluon operator in a\\r\\n     covariant gauge, Nucl. Phys. B 379 (1992) 143.\\r\\n[14] W. Vogelsang, A Rederivation of the spin dependent next-to-leading order splitting functions,\\r\\n     Phys. Rev. D 54 (1996) 2023 [hep-ph/9512218].\\r\\n[15] R. Mertig and W.L. van Neerven, The Calculation of the two loop spin splitting functions\\r\\n     P(ij)(1)(x), Z. Phys. C 70 (1996) 637 [hep-ph/9506451].\\r\\n[16] R.K. Ellis and W. Vogelsang, The Evolution of parton distributions beyond leading order:\\r\\n     The Singlet case, hep-ph/9602356.\\r\\n[17] Y. Matiounine, J. Smith and W.L. van Neerven, Two loop operator matrix elements\\r\\n     calculated up to finite terms, Phys. Rev. D 57 (1998) 6701 [hep-ph/9801224].\\r\\n[18] Y. Matiounine, J. Smith and W.L. van Neerven, Two loop operator matrix elements\\r\\n     calculated up to finite terms for polarized deep inelastic lepton - hadron scattering,\\r\\n     Phys. Rev. D 58 (1998) 076002 [hep-ph/9803439].\\r\\n[19] S.A. Larin, T. van Ritbergen and J.A.M. Vermaseren, The Next next-to-leading QCD\\r\\n     approximation for nonsinglet moments of deep inelastic structure functions,\\r\\n     Nucl. Phys. B 427 (1994) 41.\\r\\n[20] S.A. Larin, P. Nogueira, T. van Ritbergen and J.A.M. Vermaseren, The Three loop QCD\\r\\n     calculation of the moments of deep inelastic structure functions,\\r\\n     Nucl. Phys. B 492 (1997) 338 [hep-ph/9605317].\\r\\n[21] S. Moch, J.A.M. Vermaseren and A. Vogt, The Three loop splitting functions in QCD: The\\r\\n     Nonsinglet case, Nucl. Phys. B 688 (2004) 101 [hep-ph/0403192].\\r\\n[22] A. Vogt, S. Moch and J.A.M. Vermaseren, The Three-loop splitting functions in QCD: The\\r\\n     Singlet case, Nucl. Phys. B 691 (2004) 129 [hep-ph/0404111].\\r\\n[23] J. Ablinger, A. Behring, J. Blümlein, A. De Freitas, A. von Manteuffel and C. Schneider, The\\r\\n     3-loop pure singlet heavy flavor contributions to the structure function F2 (x, Q2 ) and the\\r\\n     anomalous dimension, Nucl. Phys. B 890 (2014) 48 [arXiv:1409.1135].\\r\\n[24] J. Ablinger, A. Behring, J. Blümlein, A. De Freitas, A. von Manteuffel and C. Schneider, The\\r\\n                                     (2)       (2,N )\\r\\n     three-loop splitting functions Pqg and Pgg F , Nucl. Phys. B 922 (2017) 1\\r\\n     [arXiv:1705.01508].\\r\\n[25] A. Behring, J. Blümlein, A. De Freitas, A. Goedicke, S. Klein, A. von Manteuffel et al., The\\r\\n     Polarized Three-Loop Anomalous Dimensions from On-Shell Massive Operator Matrix\\r\\n     Elements, Nucl. Phys. B 948 (2019) 114753 [arXiv:1908.03779].\\r\\n[26] J. Ablinger, A. Behring, J. Blümlein, A. De Freitas, A. von Manteuffel, C. Schneider et al.,\\r\\n     The three-loop single mass polarized pure singlet operator matrix element,\\r\\n     Nucl. Phys. B 953 (2020) 114945 [arXiv:1912.02536].\\r\\n[27] J. Blümlein, P. Marquard, C. Schneider and K. Schönwald, The three-loop unpolarized and\\r\\n     polarized non-singlet anomalous dimensions from off shell operator matrix elements,\\r\\n     Nucl. Phys. B 971 (2021) 115542 [arXiv:2107.06267].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              – 41 –\\r\\n\\x0c\\n\\n[28] J. Blümlein, P. Marquard, C. Schneider and K. Schönwald, The three-loop polarized singlet\\r\\n     anomalous dimensions from off-shell operator matrix elements, JHEP 01 (2022) 193\\r\\n     [arXiv:2111.12401].\\r\\n[29] V.N. Velizhanin, Four-loop anomalous dimension of the third and fourth moments of the\\r\\n     nonsinglet twist-2 operator in QCD, Int. J. Mod. Phys. A 35 (2020) 2050199\\r\\n     [arXiv:1411.1331].\\r\\n[30] S. Moch, B. Ruijl, T. Ueda, J.A.M. Vermaseren and A. Vogt, Four-Loop Non-Singlet Splitting\\r\\n     Functions in the Planar Limit and Beyond, JHEP 10 (2017) 041 [arXiv:1707.08315].\\r\\n[31] J. Davies, A. Vogt, B. Ruijl, T. Ueda and J.A.M. Vermaseren, Large-nf contributions to the\\r\\n     four-loop splitting functions in QCD, Nucl. Phys. B 915 (2017) 335 [arXiv:1610.07477].\\r\\n[32] J.A. Gracey, Anomalous dimensions of operators in polarized deep inelastic scattering at\\r\\n     O(1/N(f )), Nucl. Phys. B 480 (1996) 73 [hep-ph/9609301].\\r\\n[33] F. Herzog, S. Moch, B. Ruijl, T. Ueda, J.A.M. Vermaseren and A. Vogt, Five-loop\\r\\n     contributions to low-N non-singlet anomalous dimensions in QCD,\\r\\n     Phys. Lett. B 790 (2019) 436 [arXiv:1812.11818].\\r\\n[34] S. Moch, B. Ruijl, T. Ueda, J.A.M. Vermaseren and A. Vogt, Low moments of the four-loop\\r\\n     splitting functions in QCD, Phys. Lett. B 825 (2022) 136853 [arXiv:2111.15561].\\r\\n[35] D.J. Gross and F. Wilczek, Asymptotically free gauge theories. 2., Phys. Rev. D 9 (1974) 980.\\r\\n[36] H. Georgi and H.D. Politzer, Electroproduction scaling in an asymptotically free theory of\\r\\n     strong interactions, Phys. Rev. D 9 (1974) 416.\\r\\n[37] J.A. Dixon and J.C. Taylor, Renormalization of wilson operators in gauge theories,\\r\\n     Nucl. Phys. B 78 (1974) 552.\\r\\n[38] J. Blümlein, P. Marquard, C. Schneider and K. Schönwald, The Two-Loop Massless Off-Shell\\r\\n     QCD Operator Matrix Elements to Finite Terms, arXiv:2202.03216.\\r\\n[39] J.C. Collins and R.J. Scalise, The Renormalization of composite operators in Yang-Mills\\r\\n     theories using general covariant gauge, Phys. Rev. D 50 (1994) 4117 [hep-ph/9403231].\\r\\n[40] H. Kluberg-Stern and J.B. Zuber, Renormalization of Nonabelian Gauge Theories in a\\r\\n     Background Field Gauge. 1. Green Functions, Phys. Rev. D 12 (1975) 482.\\r\\n[41] H. Kluberg-Stern and J.B. Zuber, Renormalization of Nonabelian Gauge Theories in a\\r\\n     Background Field Gauge. 2. Gauge Invariant Operators, Phys. Rev. D 12 (1975) 3159.\\r\\n[42] S.D. Joglekar and B.W. Lee, General Theory of Renormalization of Gauge Invariant\\r\\n     Operators, Annals Phys. 97 (1976) 160.\\r\\n[43] S.D. Joglekar, Local Operator Products in Gauge Theories. 1., Annals Phys. 108 (1977) 233.\\r\\n[44] S.D. Joglekar, Local Operator Products in Gauge Theories. 2., Annals Phys. 109 (1977) 210.\\r\\n[45] M. Henneaux, Remarks on the renormalization of gauge invariant operators in Yang-Mills\\r\\n     theory, Phys. Lett. B 313 (1993) 35 [hep-th/9306101].\\r\\n[46] G. Curci and R. Ferrari, On a Class of Lagrangian Models for Massive and Massless\\r\\n     Yang-Mills Fields, Nuovo Cim. A 32 (1976) 151.\\r\\n[47] I. Ojima, Another BRS Transformation, Prog. Theor. Phys. 64 (1980) 625.\\r\\n[48] L. Baulieu and J. Thierry-Mieg, The Principle of BRS Symmetry: An Alternative Approach\\r\\n     to Yang-Mills Theories, Nucl. Phys. B 197 (1982) 477.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             – 42 –\\r\\n\\x0c\\n\\n[49] D. Binosi and A. Quadri, Anti-BRST symmetry and background field method,\\r\\n     Phys. Rev. D 88 (2013) 085036 [arXiv:1309.1021].\\r\\n[50] B.S. DeWitt, Quantum Theory of Gravity. 2. The Manifestly Covariant Theory,\\r\\n     Phys. Rev. 162 (1967) 1195.\\r\\n[51] G. ’t Hooft, The Background Field Method in Gauge Field Theories, in 12th Annual Winter\\r\\n     School of Theoretical Physics, 1975.\\r\\n[52] L.F. Abbott, The Background Field Method Beyond One Loop,\\r\\n     Nucl. Phys. B 185 (1981) 189.\\r\\n[53] L.F. Abbott, Introduction to the Background Field Method, Acta Phys. Polon. B 13 (1982)\\r\\n     33.\\r\\n[54] S. Sarkar and H. Strubbe, Anomalous Dimensions in Background Field Gauges,\\r\\n     Nucl. Phys. B 90 (1975) 45.\\r\\n[55] N. Nakanishi, Covariant Quantization of the Electromagnetic Field in the Landau Gauge,\\r\\n     Prog. Theor. Phys. 35 (1966) 1111.\\r\\n[56] B. Lautrup, Canonical Quantum Electrodynamics in covariant Gauges, .\\r\\n[57] C. Becchi, A. Rouet and R. Stora, Renormalization of Gauge Theories,\\r\\n     Annals Phys. 98 (1976) 287.\\r\\n[58] I.V. Tyutin, Gauge Invariance in Field Theory and Statistical Physics in Operator\\r\\n     Formalism, arXiv:0812.0580.\\r\\n[59] Z. Bern, J.J.M. Carrasco and H. Johansson, New Relations for Gauge-Theory Amplitudes,\\r\\n     Phys. Rev. D 78 (2008) 085011 [arXiv:0805.3993].\\r\\n[60] T. van Ritbergen, A.N. Schellekens and J.A.M. Vermaseren, Group theory factors for\\r\\n     Feynman diagrams, Int. J. Mod. Phys. A 14 (1999) 41 [hep-ph/9802376].\\r\\n[61] B. Ruijl, T. Ueda and J. Vermaseren, FORM version 4.2, arXiv:1707.06453.\\r\\n[62] P. Pascual and R. Tarrach, QCD: Renormalization for the Practitioner, vol. 194 (1984).\\r\\n[63] K.G. Chetyrkin and F.V. Tkachov, Infrared R Operation and ultraviolet Counterterms in the\\r\\n     MS scheme, Phys. Lett. 114B (1982) 340.\\r\\n[64] K.G. Chetyrkin and V.A. Smirnov, R* operation corrected, Phys. Lett. 144B (1984) 419.\\r\\n[65] V.A. Smirnov and K.G. Chetyrkin, R* Operation in the Minimal Subtraction Scheme,\\r\\n     Theor. Math. Phys. 63 (1985) 462.\\r\\n[66] K.G. Chetyrkin, Combinatorics of R-, R−1 -, and R∗ -operations and asymptotic expansions\\r\\n     of feynman integrals in the limit of large momenta and masses, arXiv:1701.08627.\\r\\n[67] F. Herzog and B. Ruijl, The R∗ -operation for Feynman graphs with generic numerators,\\r\\n     JHEP 05 (2017) 037 [arXiv:1703.03776].\\r\\n[68] J. de Vries, G. Falcioni, F. Herzog and B. Ruijl, Two- and three-loop anomalous dimensions\\r\\n     of Weinberg’s dimension-six CP-odd gluonic operator, Phys. Rev. D 102 (2020) 016010\\r\\n     [arXiv:1907.04923].\\r\\n[69] R. Beekveldt, M. Borinsky and F. Herzog, The Hopf algebra structure of the R*-operation,\\r\\n     JHEP 07 (2020) 061 [arXiv:2003.04301].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            – 43 –\\r\\n\\x0c\\n\\n[70] W. Cao, F. Herzog, T. Melia and J.R. Nepveu, Renormalization and non-renormalization of\\r\\n     scalar EFTs at higher orders, JHEP 09 (2021) 014 [arXiv:2105.12742].\\r\\n[71] S.G. Gorishnii, S.A. Larin and F.V. Tkachov, The Algorithm for OPE Coefficient Functions\\r\\n     in the MS scheme, Phys. Lett. B 124 (1983) 217.\\r\\n[72] S.G. Gorishnii and S.A. Larin, Coefficient Functions of Asymptotic Operator Expansions in\\r\\n     Minimal Subtraction Scheme, Nucl. Phys. B 283 (1987) 452.\\r\\n[73] P. Nogueira, Automatic Feynman graph generation, J. Comput. Phys. 105 (1993) 279.\\r\\n[74] P.A. Baikov and K.G. Chetyrkin, Four Loop Massless Propagators: An Algebraic Evaluation\\r\\n     of All Master Integrals, Nucl. Phys. B837 (2010) 186 [arXiv:1004.1153].\\r\\n[75] R.N. Lee, A.V. Smirnov and V.A. Smirnov, Master Integrals for Four-Loop Massless\\r\\n     Propagators up to Transcendentality Weight Twelve, Nucl. Phys. B 856 (2012) 95\\r\\n     [arXiv:1108.0732].\\r\\n[76] A. Georgoudis, V. Goncalves, E. Panzer and R. Pereira, Five-loop massless propagator\\r\\n     integrals, arXiv:1802.00803.\\r\\n[77] A. Georgoudis, V. Gonçalves, E. Panzer, R. Pereira, A.V. Smirnov and V.A. Smirnov,\\r\\n     Glue-and-cut at five loops, JHEP 09 (2021) 098 [arXiv:2104.08272].\\r\\n[78] B. Ruijl, T. Ueda and J.A.M. Vermaseren, Forcer, a FORM program for the parametric\\r\\n     reduction of four-loop massless propagator diagrams, arXiv:1704.06650.\\r\\n[79] D.Z. Freedman, I.J. Muzinich and E.J. Weinberg, On the Energy-Momentum Tensor in\\r\\n     Gauge Field Theories, Annals Phys. 87 (1974) 95.\\r\\n[80] D.Z. Freedman and E.J. Weinberg, The Energy-Momentum Tensor in Scalar and Gauge\\r\\n     Field Theories, Annals Phys. 87 (1974) 354.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            – 44 –\\r\\n\\x0c',\n",
       " '     Physics-driven Synthetic Data Learning for Biomedical Magnetic Resonance\\r\\n\\r\\n              Qinqin Yang1, Zi Wang1, Kunyuan Guo1, Congbo Cai1, and Xiaobo Qu1,*\\r\\n\\r\\n                                                   Abstract\\r\\n\\r\\n       Deep learning has innovated the field of computational imaging. One of its bottlenecks is\\r\\n\\r\\nunavailable or insufficient training data. This article reviews an emerging paradigm, imaging\\r\\n\\r\\nphysics-based data synthesis (IPADS), that can provide huge training data in biomedical\\r\\n\\r\\nmagnetic resonance without or with few real data. Following the physical law of magnetic\\r\\n\\r\\nresonance, IPADS generates signals from differential equations or analytical solution models,\\r\\n\\r\\nmaking the learning more scalable, explainable, and better protecting privacy. Key components\\r\\n\\r\\nof IPADS learning, including signal generation models, basic deep learning network structures,\\r\\n\\r\\nenhanced data generation and learning methods are discussed. Great potentials of IPADS have\\r\\n\\r\\nbeen demonstrated by representative applications in fast imaging, ultrafast signal reconstruction\\r\\n\\r\\nand accurate parameter quantification. Finally, open questions and future work have been\\r\\n\\r\\ndiscussed.\\r\\n\\r\\n                                                 Index Terms\\r\\n\\r\\n       Data synthesis, Physical model, Biomedical magnetic resonance, Deep learning\\r\\n\\r\\n                                            I. INTRODUCTION\\r\\n\\r\\n       Data learning has empowered the computational imaging with fast sampling, ultrafast\\r\\n\\r\\nsignal reconstruction and straightforward parameter quantification [1, 2]. In the age of deep\\r\\n\\r\\nlearning, a large amount of high-quality data is essentially important to achieve excellent\\r\\n\\r\\nperformance. However, in biomedical imaging, these data may be hard to be acquired in the\\r\\n\\r\\nchallenging applications, e.g., the blurred images of moving organs, the lengthy measuring of\\r\\n\\r\\nquantitative physical parameters, or the irreversible acquisition of physiological processes.\\r\\n\\r\\nThus, new data generation and learning schemes are highly desired to boost biomedical imaging\\r\\n\\r\\napplications.\\r\\n\\r\\n\\r\\n1\\r\\n    Department of Electronic Science, Biomedical Intelligent Cloud Research and Development Center, Fujian\\r\\n  Provincial Key Laboratory of Plasma and Magnetic Resonance, National Institute for Data Science in Health and\\r\\n  Medicine, Xiamen University, Xiamen, China, 361005\\r\\n* Corresponding author (quxiaobo@xmu.edu.cn)\\r\\n\\x0c\\n\\n     Recently, synthetic data starts to attract attention in computational imaging [3, 4]. Learning\\r\\n\\r\\nin synthetic data could reduce the dependence on paired real-world data, quickly generate\\r\\n\\r\\nmassive data, overcome the difficulties or even the impossibility to collect real data, and protect\\r\\n\\r\\nprivacy in biomedicine. Here, we focus on the imaging physics-based data synthesis (IPADS)\\r\\n\\r\\nbecause it follows plausible physical model and enables good interpretability [4]. To make the\\r\\n\\r\\ndiscussions compact, we limit the content to the biomedical Magnetic Resonance (MR) since\\r\\n\\r\\nIPADS learning (Fig. 1) has become frontiers in this area, such as fast quantitative imaging [5-\\r\\n\\r\\n15], signal reconstruction [16-21] and pulse sequence optimization [22, 23]. The concept of\\r\\n\\r\\nIPADS learning could be generalized to other computational imaging modalities as long as an\\r\\n\\r\\nappropriate physical model and learning network are included.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 1. Two lines of imaging physics-based data synthesis (IPADS) in biomedical magnetic resonance. Various\\r\\nsynthetic images can be generated from numerical tissue parameters through physical evolution or analytical models.\\r\\nNote: PD is short for proton density.\\r\\n\\r\\n\\r\\n     This article will give an overview of IPADS for deep learning magnetic resonance. Based\\r\\n\\r\\non the physical signal evolution or not, we first divide the IPADS into two lines, including the\\r\\n\\r\\nphysical signal evolution and analytical modeling (Fig. 1), and then discuss the enhanced\\r\\n\\r\\nlearning with realistic data adaption and advanced network structures. Representative\\r\\n\\r\\napplications and future works will be provided.\\r\\n\\r\\n                              II. MAGNETIC RESONANCE PHYSICS\\r\\n\\r\\n     The physics of MR governs signal formation, which involves spin dynamics, quantum\\r\\n\\r\\nmechanics and electromagnetism. For example, MR imaging (MRI) presents spatially structural\\r\\n\\x0c\\n\\ninformation with different contrasts, while MR spectroscopy (MRS) focuses on the spectral\\r\\n\\r\\nsignals of multiple molecules. The MR signal evolution may have analytical solutions under\\r\\n\\r\\nsome conditions or may not have them in general. Accordingly, IPADS methods are divided\\r\\n\\r\\ninto three categories and some representative works are summarized in Table 1.\\r\\n\\r\\n                      Table 1. A summary of IPADS in biomedical magnetic resonance\\r\\n  Category             Data generation                          Learning issues              Method\\r\\n                         Bloch simulation                   MR fingerprinting             DRONE [5]\\r\\n                         Bloch simulation                      T2 mapping                  OLED [6]\\r\\n   Physical              Bloch simulation                  Water-fat separation         SPEN-WFS [21]\\r\\n   evolution             Bloch simulation                Sequence optimization            MRzero [22]\\r\\n                         Bloch simulation             Cardiac motion tag tracking      SyntheticCMR [24]\\r\\n                     Bloch simulation + FDTD        Electrical properties tomography      DL-EPT [10]\\r\\n                     Dipole convolution model      Quantitative susceptibility mapping   DeepQSM [11]\\r\\n  Analytical       Multi-pool exponential model     Myelin water fraction estimation    ANN-MWF [14]\\r\\n    model           Bloch-McConnell equation         CEST z-spectra quantification      ANNCEST [15]\\r\\n                   Lorentzian line shape model            Spectra reconstruction         DL-NMR [16]\\r\\n                 Quantum mechanical simulation\\r\\n                                                           MRSI reconstruction           DL-MRSI [19]\\r\\n    Hybrid          + Exponential signal model\\r\\n   approach              Bloch simulation\\r\\n                                                              SPIO mapping               DeepSPIO [13]\\r\\n                    + Dipole convolution model\\r\\nFDTD: finite difference time domain method; DL: deep learning; MRSI: MR spectroscopic imaging; CEST:\\r\\nchemical exchange saturation transfer; RF: radio frequency; QSM: quantitative susceptibility mapping; EPT:\\r\\nelectrical properties tomography; SPIO: super paramagnetic iron oxide.\\r\\n\\r\\n\\r\\nA. Physical Evolution\\r\\n\\r\\n     Bloch equation [25] describes the time-dependent evolution of magnetization vector\\r\\n\\r\\nM(t)∈R3 under an external magnetic field B(t)∈R3. Two key phenomena in magnetized spin,\\r\\n\\r\\ni.e., precession and relaxation, can be formulated in the differential equations:\\r\\n\\r\\n                                   dM (t )\\r\\n                                           \\uf03d \\uf067 B (t ) \\uf0b4 M (t ) \\uf02b R (M (t )) ,                         (1)\\r\\n                                    dt       \\uf031\\uf034  \\uf034\\uf032\\uf034\\uf034       \\uf033 \\uf031   \\uf034\\uf032\\uf034     \\uf033\\r\\n                                                 Precession       Relaxation\\r\\n\\r\\n\\r\\nwhere γ is the gyromagnetic ratio (e.g., 42.6 MHz/T for hydrogen nuclei). For the precession\\r\\n\\r\\nterm, the motion of spins is determined by many factors, including the main static field B0, the\\r\\n\\r\\nradio-frequency (RF) pulse field B1(t), the chemical shift (frequency) ∆ω0 and the magnetic\\r\\n\\r\\nfield gradients G(t), at location r =(xˆ , yˆ , zˆ ) as:\\r\\n\\r\\n                                                    \\uf044\\uf0770\\r\\n                                   B(t ) \\uf03d ( B0 -         ) zˆ \\uf02b B1 (t ) xˆ \\uf02b r \\uf0d7 G (t ) ,            (2)\\r\\n                                                    \\uf067\\r\\n\\r\\nwhere the thermal equilibrium magnetization vector is tipped from the B0 direction (z) into the\\r\\n\\r\\ntransverse plane (x-y) under the effect of B1(t). The G(t) is usually used to modulate phase or\\r\\n\\x0c\\n\\nencode different spatial locations. In contrast, the relaxation term describes the process of\\r\\n\\r\\nmagnetization vector returning to its equilibrium state. Two relaxation time constants, T1 and\\r\\n\\r\\nT2, are used to characterize the regrowth of longitudinal magnetization (Mz) and the decay of\\r\\n\\r\\nthe transverse magnetization (Mx,y), respectively. These relaxation parameters are very valuable\\r\\n\\r\\nin clinics, such as lesion diagnosis [9].\\r\\n\\r\\n     MR pulse sequence describes a series of physical radio frequency pulses applied to the\\r\\n\\r\\nobjects, resulting in a particular image or spectrum appearance. It usually consists of a series of\\r\\n\\r\\nvarying B1(t) and G(t) in the form of timing diagram. Typically, the signal model for a simple\\r\\n\\r\\nMR pulse sequence has an analytical solution if steady-state is assumed. However, as the\\r\\n\\r\\ncomplexity of the pulse sequence increases, analytical solutions are hard to obtain due to spin\\r\\n\\r\\nhistory effects at unsteady-states and system imperfections. For example, in MR fingerprinting\\r\\n\\r\\n[26], various sequence components are varied in a pseudorandom pattern and MR signals are\\r\\n\\r\\nnot analyzed using the analytic expression but the dictionary matching.\\r\\n\\r\\nB. Analytical Model\\r\\n\\r\\n     Analytical model provides a clear closed form solution of MR signals and could\\r\\n\\r\\napproximate the physical evolution of MR signal under some assumptions or simplifications.\\r\\n\\r\\n     In MRI, consider a most common pulse sequence, the spin-echo sequence, Eq. (3) provides\\r\\n\\r\\nan analytical solution for image contrast as [27]:\\r\\n\\r\\n                          S (r, TE , TR ) \\uf03d M 0 (r ) \\uf0d7 (1 \\uf02d e \\uf02dTR /T1 (r ) ) \\uf0d7 e \\uf02dTE /T2 ( r ) ,   (3)\\r\\n\\r\\nassuming that the initial magnetization vector undergoes the action of 90° and 180° radio\\r\\n\\r\\nfrequency pulses, and the repetition time is much larger than echo time. M 0 (r ) represents\\r\\n\\r\\nequilibrium longitudinal magnetization. By adjusting the repetition and echo time, image\\r\\n\\r\\ncontrasts can be generated if the tissue parameters M 0 (r ) , T1 (r ) and T2 (r) are provided.\\r\\n\\r\\n     In addition to contrasts, magnetic susceptibility χ (r ) represents the ability of a\\r\\n\\r\\nsubstance to become magnetized under an applied magnetic field. Tissue-specific magnetic\\r\\n\\r\\nsusceptibility variations within the MRI scanner can cause inhomogeneity of the static magnetic\\r\\n\\r\\nfield B0 . To describe the field variations caused by χ (r ) , a dipole convolution model is\\r\\n\\r\\ntypically used as [11-13]:\\r\\n\\x0c\\n\\n                                     \\uf044B0 (r) \\uf03d B0 \\uf0d7 χ (r ) \\uf02a D(r) ,                              (4)\\r\\n\\r\\nin which, the induced field inhomogeneity \\uf044B0 (r) is expressed as the convolution between\\r\\n\\r\\nthe spatial distribution of susceptibility χ (r ) and a unit dipole response D (r ) .\\r\\n\\r\\n     Not limited to the image, in MRS, the spectral signal of each individual voxel comes from\\r\\n\\r\\nmultiple molecules. This signal is commonly modeled as [20, 28]:\\r\\n                                                  M\\r\\n                                     S (r, t ) \\uf03d \\uf0e5cm (r)vm (t )e\\r\\n                                                                       \\uf02dt T2,m (r )\\r\\n                                                                                        ,        (5)\\r\\n                                                 m\\uf03d1\\r\\n\\r\\n\\r\\nwhere the m denotes the mth molecule, cm (r ) and vm (t ) are its concentration and basis\\r\\n\\r\\nfunction, respectively. For the ex vivo biological MRS, which are used to determine the\\r\\n\\r\\nconcentrations of metabolites or structures of proteins, the objects (usually in liquid or solid)\\r\\n\\r\\nto be acquired are placed in a tube and treated as a whole. Thus, the spatial location r is\\r\\n\\r\\ncommonly ignored and only the S (t ) is acquired from scanners. In biological MRS, the\\r\\n\\r\\nbasis function could be expressed as a linear combination of multiple exponentials (Jm) as [28]:\\r\\n                                                 Jm\\r\\n                                      vm (t ) \\uf03d \\uf0e5 (a j ,me j ,m )e\\r\\n                                                           i\\uf066        i 2\\uf070 f j ,mt\\r\\n                                                                                    ,            (6)\\r\\n                                                 j \\uf03d1\\r\\n\\r\\n\\r\\nwhere i is the imaginary unit, a j , m , f j , m and \\uf066 j ,m are the amplitude, frequency, and phase\\r\\n\\r\\n        th\\r\\nof the j spectral peak, respectively. By performing the Fourier transform on S (t ) , a\\r\\n\\r\\nspectrum will be obtained and the spectral peaks follow the Lorentzian line shape [16, 17, 28].\\r\\n\\r\\n     For in vivo MRS, Eq. (5) is sub-optimal since it does not consider imperfect but real\\r\\n\\r\\nimaging conditions, e.g., field inhomogeneity and motions. More practically, the signal of\\r\\n\\r\\nMRS could be modeled as [19]:\\r\\n                                      M\\r\\n                          S (r, t ) \\uf03d \\uf0e5 cm (r )vm (t )em (t ;\\uf071 m (r )) \\uf02b b(r, t ) ,              (7)\\r\\n                                     m \\uf03d1\\r\\nwhere b(r, t ) is a baseline signal mainly contributed by macromolecules and commonly\\r\\n\\r\\nfollows the Gaussian line shape, em (t;\\uf071 m (r )) captures molecule-dependent time-domain\\r\\n\\r\\nmodulation functions that can be described by some experimental and physiological parameters\\r\\n\\r\\nin \\uf071m (r) .\\r\\n\\x0c\\n\\n                                  III. SYNTHETIC DATA LEARNING\\r\\n\\r\\nA. Physical Evolution\\r\\n\\r\\n       Physical evolution-based IPADS (PE-IPADS) and learning relies on simulating discrete\\r\\n\\r\\nspin motion at small time interval using the Bloch equation. This process can be described with\\r\\n\\r\\nsuccessive operators according to a specific MR pulse sequence. The signal formation, however,\\r\\n\\r\\nis computationally expensive, which involves large-scale matrix operations, integration and\\r\\n\\r\\ndifferentiation. Fortunately, several MR physical simulation tools have been developed, e.g.,\\r\\n\\r\\nSPROM [29], JEMRIS 1 , MRiLab 2 [30]. With these tools, one has to set proper physical\\r\\n\\r\\nparameters to make data generation in IPADS as realistic as possible.\\r\\n\\r\\n       Physical parameters include object-specific and experiment-specific parameters. The\\r\\n\\r\\nformer indicates the nature of the scanned objects, such as the proton density, relaxations (T1,\\r\\n\\r\\nT2 and T2*), diffusion and electromagnetic properties at a particular spatial location in MRI, or\\r\\n\\r\\namplitudes, resonance frequencies and metabolite concentrations in MRS. The latter takes the\\r\\n\\r\\npulse sequence and real imaging conditions into account, such as the repetition time, echo time,\\r\\n\\r\\nflip angle, the strength of the main field magnetic field (B0) and its inhomogeneity (∆B0), radio\\r\\n\\r\\nfrequency field inhomogeneity (B1), and eddy currents, etc.\\r\\n\\r\\n       Up to now, most PE-IPADS learning is on imaging [5-10, 21, 24], especially quantitative\\r\\n\\r\\nparametric imaging [5-10]. A representative application of PE-IPADS learning is the T2\\r\\n\\r\\nmapping with overlapping-echo detachment (OLED) [6-9]. OLED is an ultrafast imaging\\r\\n\\r\\nsequence that encodes information of multiple image into one image, allowing parametric\\r\\n\\r\\nimaging within very short time (within 10 s for a whole brain). How to estimate reliable\\r\\n\\r\\nquantitative parameters within the same imaging time is a main problem. Deep learning has\\r\\n\\r\\nbeen evidenced powerful but the lack of real-world pairs of quantitative parameters and images\\r\\n\\r\\nshould be addressed.\\r\\n\\r\\n       A typical flow of data generation for OLED is illustrated in Fig. 2. First, object-shape\\r\\n\\r\\ntemplates should be provided. They are common created by randomly filling blank templates\\r\\n\\r\\nwith hundreds of different 2D/3D basic geometric shapes [6-8, 11, 13, 21, 24]. Second, tissue-\\r\\n\\r\\n\\r\\n\\r\\n  1\\r\\n      https://www.jemris.org\\r\\n  2\\r\\n      http://mrilab.sourceforge.net\\r\\n\\x0c\\n\\nspecific parameters, such as the proton density and T2 relaxation at particular spatial locations\\r\\n\\r\\nare set in the ranges of [0, 1] and [20, 700] ms, respectively. Then, pulse sequence needs to be\\r\\n\\r\\nprogrammed and imaging parameters (echo times= 22, 52. 82, 110 ms, flip angle=30°) are set\\r\\n\\r\\nto be consistent with real imaging experiments. Last, an imperfection imaging condition, the\\r\\n\\r\\nradio frequency field inhomogeneity (B1), is generated by random polynomial functions.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2. An illustration of physical evolution-based data generation. (a) Parametric templates that have geometric\\r\\nshapes and object-parameters, (b) Magnetic resonance pulse sequence needs to be programmed according to the real-\\r\\nworld experiment, (c) A software (here is the MRiLab [30]) that enables physical signal evolution, (d) Massive\\r\\nsynthetic data.\\r\\n\\r\\n\\r\\n      Once the data is generated with physical simulation tools, a network structure should be\\r\\n\\r\\ndesigned to conduct the learning with PE-IPADS data. A direct and common chosen approach\\r\\n\\r\\nis to learn the mapping from the signal to quantitative parameters in the end-to-end way [5, 6,\\r\\n\\r\\n10]. For example, the OLED images were mapped to T2 parameters in Fig. 3. In general, for a\\r\\n\\r\\ndeep learning network \\uf04e\\uf072with trainable network parameter θ , an optimal learning is to\\r\\n\\r\\nminimize the loss          L \\uf028 \\uf0d7\\uf029                                               pˆ n \\uf03d \\uf04e (sn ; θ)\\r\\n                                    of estimated quantitative object-parameters \\uf072\\r\\n\\r\\naccording to:\\r\\n                                                   N\\r\\n                                    θˆ \\uf03d arg min \\uf0e5 L \\uf028 p n \\uf02d \\uf04e (s n ; θ) \\uf029 ，                                 (8)\\r\\n                                               θ\\r\\n                                                   n \\uf03d1\\r\\n\\r\\n\\r\\nwhere pn is the ground-truth but simulated quantitative parameters, sn is the generated images,\\r\\n\\r\\nand n denotes the nth sample and its total number is N. Thus, PE-IPADS learning tries to\\r\\n\\r\\napproximate the inverse process from physical quantitative parameters of scanned objects to\\r\\n\\r\\nthe output signal.\\r\\n\\x0c\\n\\n     Once the network is trained with sufficient samples, estimating quantitative parameters\\r\\n\\r\\np \\uf03d \\uf04e (s; θˆ ) from a target image s becomes a forward and fast process. For the PE-IPADS\\r\\n\\r\\nlearning for OLED imaging, 800 images were trained in around 22 hours and less than 1 second\\r\\n\\r\\nwas consumed to obtain faithful T2 maps (Figs. 3(b) and (c)). High fidelity was achieved on the\\r\\n\\r\\ncorrelation coefficients (0.999 and 0.997 in phantom and human brain data) to the T2 maps of\\r\\n\\r\\nconventional imaging pulse sequence, but the data acquisition time is reduced\\r\\n\\r\\nfrom 17 minutes to 10 seconds [7]. Besides, OLED with PE-IPADS avoids challenging motion\\r\\n\\r\\nartifacts in an epilepsy patient (Fig. 3(d)) [9].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3. Residual dense network structure for T2 mapping with OLED imaging sequence [7, 9]. (a) The network\\r\\narchitecture learns the end-to-end mapping from the images to T2 parameters, (b)-(c) Reconstructed T2 maps of\\r\\nOLED and the corresponding reference of conventional acquisition in phantom and in vivo brain data, respectively.\\r\\n(d) An image of clinical sequence and T2 map of OLED from an epilepsy patient.\\r\\n\\r\\n\\r\\n     The PE-IPADS has been applied to many other imaging scenarios due to its high flexibility\\r\\n\\r\\n[10, 21, 22, 24]. At the current early stage research, the network structure seems not the primary\\r\\n\\r\\nfocus. Most approaches adopted mainstream network structures, e.g., fully connected network\\r\\n\\r\\nin MR fingerprinting [5] and residual network in cardiac motion tag tracking [24]. Through\\r\\n\\r\\nsimulating the imaging process of water and fat, the SPEN-WFS [21] trained a U-Net on the\\r\\n\\r\\ngenerated data and fast separated water and fat in 0.46 s on a personal computer (tradition\\r\\n\\r\\nmethod costs 30.31 s). Additionally, with a 3D convolution neural network, the common Bloch\\r\\n\\r\\nequation evolution could be combined with electromagnetic simulation to obtain coil-specific\\r\\n\\r\\nradio frequency profiles and phases [10]. This approach leverages PE-IPADS into electrical\\r\\n\\r\\nproperty tomography, extending traditional MRI to new imaging modalities.\\r\\n\\x0c\\n\\nB. Analytical Model\\r\\n\\r\\n    Analytical model-based IPADS (AM-IPADS) skips the complex physical evolution\\r\\n\\r\\nprocess. It directly and quickly generates a large amount of training data by adjusting the\\r\\n\\r\\nphysical parameters in the closed form solution expression [11, 12, 14, 16-18]. It is interesting\\r\\n\\r\\nto see that most AM-IPADS learning is on spectra in MRS [15-20] and few applications on\\r\\n\\r\\nMRI [11, 12, 14]. Thus, in the following, we mainly discuss the spectra applications.\\r\\n\\r\\n    In ex vivo biological MRS, the spectral peaks are usually in Lorentzian lineshapes and the\\r\\n\\r\\neffect of system imperfection is negligible in the ideal scenario, i.e., only object-specific\\r\\n\\r\\nparameters need to be considered. Specifically, synthetic data are generated using general\\r\\n\\r\\nexponential functions[16-18].\\r\\n\\r\\n    The AM-IPADS are commonly used in the spectra reconstruction [16-18] that does not\\r\\n\\r\\ninvolve the physical evolution process. Reconstruction aims at estimating a high-quality\\r\\n\\r\\nspectrum from the undersampled or low signal-to-noise data. Although deep learning has shown\\r\\n\\r\\nastonishing performance in image reconstruction of MRI, the methodology was developed\\r\\n\\r\\nrelatively later in MRS due to the lack of paired realistic data. IPADS relaxes this requirement\\r\\n\\r\\nthrough data generation according to Eqs. (5)-(7). As the physical parameters, such as\\r\\n\\r\\namplitudes, resonance frequencies and metabolite concentrations can be simulated, IPADS\\r\\n\\r\\nstrongly increases the flexibility of the deep learning. In general, the network \\uf04e learns the\\r\\n\\r\\ntrainable parameters θ to minimize the total difference between the fully-sampled (or noise-\\r\\n\\r\\nfree) label signal s\\uf025 and the output of network \\uf04e (d n ; θ) as follows:\\r\\n\\r\\n                                              N\\r\\n                                θˆ \\uf03d argmin \\uf0e5 L \\uf028 s\\uf025n \\uf02d \\uf04e (dn ; θ)\\uf029 ,                        (9)\\r\\n                                          θ\\r\\n                                              n\\uf03d1\\r\\n\\r\\n\\r\\nwhere N is the number of training samples, L is the loss function such as the l2 norm loss.\\r\\n\\r\\nAfter obtaining the optimal network parameters θ̂ , a target signal s is reconstructed via\\r\\n\\r\\ns \\uf03d \\uf04e (d; θˆ ) for a given undersampled (or noisy) realistic input d . The reconstruction is\\r\\n\\r\\nalways ultrafast in realistic experiments, e.g., only 0.04/2.75 seconds for reconstructing 2D/3D\\r\\n\\r\\nprotein spectra and is about 30 times faster than the conventional compressed sensing methods\\r\\n\\r\\n[18]. The basic network architectures are convolutional neural networks (CNN) [18] or its\\r\\n\\r\\ndensely connected version [16, 17].\\r\\n\\x0c\\n\\n    Without any real data involved in training, a representative application of the AM-IPADS\\r\\n\\r\\nlearning is ultrafast MRS reconstruction [16-18]. Firstly, based on the exponential model in Eq.\\r\\n\\r\\n(5), we vary spectral parameters according to the uniform distribution, such as discretely\\r\\n\\r\\nrandomizing the number of peaks from 1 to 10, normalized amplitude from 0.05 to 1, and the\\r\\n\\r\\nnormalized frequency from 0.01 to 0.99 Hz [16-18]. In total, 40000 pairs of synthetic data\\r\\n\\r\\n(inputs are undersampled time-domain free induction decay, FID, signals and output labels are\\r\\n\\r\\nfully sampled spectra) are generated within several seconds. Then, a deep learning magnetic\\r\\n\\r\\nresonance network, DLNMR [16], is trained with the synthetic data in 5~31 hours. The\\r\\n\\r\\nflowchart of DLNMR (Fig. 4(a)) shows that the spectrum artifacts introduced by undersampling\\r\\n\\r\\nare first removed with dense CNN and then the intermediate spectra are further refined to\\r\\n\\r\\nmaintain the data consistency to the sampled FID. With the increase of the network phase,\\r\\n\\r\\nartifacts are gradually removed, and finally a clean spectrum can be reconstructed. The\\r\\n\\r\\nDLNMR is good at restoring high-intensity peaks (the 2nd row of Fig. 4(c)). It has been applied\\r\\n\\r\\nto reconstruct many real spectra of proteins, achieving the peak correlation up to 0.9996 in 2D\\r\\n\\r\\nspectra and the acceleration factor of sampling up to 10 in 3D spectra. Even though, small peaks\\r\\n\\r\\n(the 2nd row of Fig. 4(c)) may be comprised and even becomes worse (the 2nd row of Fig. 4(d))\\r\\n\\r\\nif the mismatch is existed between the training and target data. These observations imply that,\\r\\n\\r\\nwithout any real data in training, AM-IPADS has great protentional to boost deep learning but\\r\\n\\r\\nstill needs further improvements.\\r\\n\\x0c\\n\\nFig. 4. Exponential signal reconstructions with mismatched training data [16, 17]. (a) Recursive DLNMR framework\\r\\nthat alternates between the dense CNN and the data consistency. (b) A uniformly distribution I of peak intensities for\\r\\ntraining or reconstruction and a non-uniform distribution II that is only for reconstruction, (c) and (d) are\\r\\nreconstructed target signals that satisfy uniform and non-uniform distributions, respectively. Note: The sampling rate\\r\\nis 25%.\\r\\n\\r\\n\\r\\n     In addition to signal reconstruction, AM-IPADS deep learning has been explored in\\r\\n\\r\\nphysical parameter quantification, such as quantitative susceptibility mapping [11, 12] and\\r\\n\\r\\nwater fraction estimation [14]. As the loss function in Eq. (8) is defined on generated data but\\r\\n\\r\\nnot the physical evolution, this model can also be applied here. AM-IPADS enables the inverse\\r\\n\\r\\nlearning from the image to physical parameters. But this learning does not mean that a\\r\\n\\r\\nsufficiently faithful results can be obtained. A possible way is to use the forward analytical\\r\\n\\r\\nmodel \\uf046         to bridge the missed connection from physical parameters p                               to image\\r\\n\\r\\ns \\uf03d \\uf046 \\uf028 p \\uf029 , and then regularize the solution to has small errors of both physical parameters and\\r\\n\\r\\nimages as:\\r\\n                                    N\\r\\n                   θˆ \\uf03d arg min \\uf0e5 L \\uf028 p n \\uf02d \\uf04e (s n ; θ) \\uf029 \\uf02b Lmodel \\uf05b \\uf046 (p n ) \\uf02d \\uf046 ( \\uf04e (s n ; θ)) \\uf05d ,            (10)\\r\\n                               θ\\r\\n                                   n \\uf03d1\\r\\n\\r\\n\\r\\n\\r\\nwhere Lmodel is the additional loss with the analytical model. For example, in QSMnet+ [12],\\r\\n\\x0c\\n\\np is a susceptibility map, s is a local B0 inhomogeneity image and \\uf046 is an analytical\\r\\n\\r\\ndifferentiable dipole model defined in Eq. (4).\\r\\n\\r\\nC. Hybrid Approach\\r\\n\\r\\n       Hybrid approach (HB-IPADS) means that IPADS learning integrates physical evolution\\r\\n\\r\\nand analytical model together. The former could be used for some parts that involve Bloch\\r\\n\\r\\nequation, specific MR pulse equations, and resonance structures. The latter is used to directly\\r\\n\\r\\ngenerate a large amount of training data from the analytical model with randomized parameters.\\r\\n\\r\\n       The HB-IPADS has been successfully applied to in vivo MRS spectra reconstruction [19,\\r\\n\\r\\n20, 31], which considers the complex non-ideal acquisition conditions. The basis functions,\\r\\n\\r\\nvm (t ) in Eq. (7), which are invariant to different subjects, are first generated using physical\\r\\n\\r\\nquantum mechanical simulations tools, such as jMRUI1 and FID-A2. Then, all other physical\\r\\n\\r\\nsignals, which are related to metabolite concentrations of objects, unexpected macromolecule\\r\\n\\r\\nsignals and system imperfections, are directly generated according to the analytical expressions\\r\\n\\r\\nin Eq. (7), by varying parameters from an empirical range [19, 20, 31]. After minimizing the\\r\\n\\r\\nsame loss function in Eq. (9), the trained network can be applied to MRS denoising [19, 31]\\r\\n\\r\\nand separation [20]. With an auto-encoder network, the mapping from the low signal-to-noise-\\r\\n\\r\\nratio (SNR) spectra to the high SNR one is learnt on the IPADS data, and then applied to SNR\\r\\n\\r\\nimprovement of realistic MRS [19]. Compared with the conventional spatial smoothness and\\r\\n\\r\\nsubspace methods, auto-encoder reduces the mean square error of denoised 31P spectra on a\\r\\n\\r\\nnumerical phantom by 90% and 58%, respectively [19].\\r\\n\\r\\n       Moreover, the HB-IPADS is also extended to MRI deep learning quantification, such as\\r\\n\\r\\nsuper paramagnetic iron oxide (SPIO) particle quantification [13]. This method firstly uses the\\r\\n\\r\\nanalytical model in Eq. (4) to obtain the inhomogeneous local B0 field of SPIO concentration,\\r\\n\\r\\nand then generates slice-modulated MRI image through Bloch simulation. The network learns\\r\\n\\r\\nthe mapping from the generated images to the wavelet coefficients of spatial concentration\\r\\n\\r\\ndistribution, which is finally obtained by performing the inverse wavelet transform. The\\r\\n\\r\\nnetwork consists of encoder, bottleneck and four decoder sub-networks. This HB-IPADS\\r\\n\\r\\n\\r\\n  1\\r\\n      https://github.com/isi-nmr/jMRUI\\r\\n  2\\r\\n      https://github.com/CIC-methods/FID-A\\r\\n\\x0c\\n\\nmethod significantly outperforms traditional algorithms which are unreliable when the\\r\\n\\r\\nhigh concentration of SPIO has a large effect on the inhomogeneous field and phase.\\r\\n\\r\\n                                  IV. DATA ENHANCEMENT\\r\\n\\r\\n     Enhancing the IPADS data to fit for real applications is discussed here.\\r\\n\\r\\n     The experiment-specific characteristics, especially system imperfections, should be\\r\\n\\r\\nmatched to real applications. For example, in CEST imaging, the inhomogeneity of B0 has been\\r\\n\\r\\ncarefully chosen for phantom (-0.4~0.4 ppm) and human skeletal muscle (-0.25~0.25 ppm),\\r\\n\\r\\nrespectively [15]. Besides, a same range of imperfect parameters in training and testing data\\r\\n\\r\\ncould greatly improve the deep learning performance. In real-time spiral MRI [32], a major\\r\\n\\r\\nlimitation is image blurring introduced by off-resonance. A neural network trained with the\\r\\n\\r\\nsame off-resonance range significantly increases the peak signal-to-noise ratio of deblurred\\r\\n\\r\\nimages from 11.04 dB to 24.57 dB [32]. Furthermore, MRI images are usually acquired from\\r\\n\\r\\nmulti-coils and the correlation of them can be reflected by the coil sensitivity maps. To adapt\\r\\n\\r\\nfor real parallel imaging, these maps could be obtained on few real experimental data (5 subjects)\\r\\n\\r\\nfrom one scanner and then multiplied them onto single-channel images from other scanners or\\r\\n\\r\\npublic databases [9].\\r\\n\\r\\n     The object-specific parameter should pay attention to the non-uniform distribution in real\\r\\n\\r\\ndata. Many physical parameters of IPADS data are chosen from a given range with equal\\r\\n\\r\\nprobability, resulting in the uniform distributions of these parameters in the training data [16].\\r\\n\\r\\nThis type of data generation provides unbiased representation when no other prior knowledge\\r\\n\\r\\nis provided. For example, uniform distributions of amplitudes and frequencies were made for\\r\\n\\r\\nIPADS spectrum training and have been successfully applied to many real protein spectra [16-\\r\\n\\r\\n18]. However, the reconstruction performance will be compromised if the true distribution is\\r\\n\\r\\nnon-uniform. For example, many low-intensity peaks (the 2nd row of Fig. 4(c)) are lost in the\\r\\n\\r\\nreconstruction if they do not have a high ratio in the training data. Thus, a good estimation of\\r\\n\\r\\nthe distribution for real data is valuable.\\r\\n\\x0c\\n\\nFig. 5. Enhance the susceptibility distribution to a wider range [12]. (a) the original susceptibility distribution trained\\r\\nfrom healthy controls and the enhanced one. (b) Reconstructed QSM from hemorrhagic patients without and with\\r\\ndata enhancement. The artifacts were marked by yellow arrow. ppm: parts per million.\\r\\n\\r\\n\\r\\n      In imaging, spatial and temporal distributions of physical parameters should be carefully\\r\\n\\r\\ntuned. In quantitative susceptibility mapping, the hemorrhagic lesions have higher\\r\\n\\r\\nsusceptibility than healthy tissues. To better fit patient data, the susceptibility values were\\r\\n\\r\\nwidened through multiplying different factors at different image regions of estimated maps,\\r\\n\\r\\nresulting in the much better artifacts removal [12] (Fig. 5). The random synthetic shapes\\r\\n\\r\\nassigned with restricted parametric values have been used for network training in OLED T2\\r\\n\\r\\nmapping [6-8] and water-fat imaging [21]. Instead, the spatial distribution with texture\\r\\n\\r\\ninformation from real data could be introduced into enhance IPADS learning. More realistic\\r\\n\\r\\ntexture was achieved than the result of deep network trained with arbitrary random synthetic\\r\\n\\r\\nshapes [9, 21] (Fig. 6). In dynamic cardiac imaging, more motions were set at the beginning of\\r\\n\\r\\nthe cycle than at the end, making the synthetic motion paths more realistic [24].\\r\\n\\x0c\\n\\nFig. 6. Representative examples of spatial distribution enhanced IPADS learning. Upper row: Rich texture\\r\\ninformation from natural images were added into random shape-based templates for better water-fat imaging [21].\\r\\nLower row: More realistic textures were captured when real spatial distribution was introduced in T2 mapping [9].\\r\\n\\r\\n\\r\\n                            V. ENHANCED LEARNING WITH IPADS\\r\\n\\r\\n     Narrowing the gap between IPADS and real data is a main task of the network design.\\r\\n\\r\\n     Regularizing the solution with general signal priors can generalize the IPADS network to\\r\\n\\r\\npractical data. These priors could be the smoothness [20] and low-rankness [18], that were\\r\\n\\r\\nwidely adopted in traditional model-based reconstruction. A direct scheme is taking the output\\r\\n\\r\\nof a pre-trained network as an intermediate solution and then improving it with traditional\\r\\n\\r\\noptimization model. For example, the smoothness of image in MRSI is enforced with a finite-\\r\\n\\r\\ndifference operator [20]. This scheme allows convergence characterization thus makes the\\r\\n\\r\\nalgorithm more traceable. Another scheme is to design a network structure by imitating the\\r\\n\\r\\niterative process in the model-based reconstruction, e.g., the low-rank MRS reconstruction (Fig.\\r\\n\\r\\n6) [18]. This approach better preserves the low-intensity peaks and may handle the shifted\\r\\n\\r\\ndistributions of spectra intensities in the training IPADS and target data. Besides, interpretable\\r\\n\\r\\nbehavior of network, such as the progressive low-rank approximation, is also provided. Thus,\\r\\n\\r\\nthe general signal prior is still powerful to improve IPADS network.\\r\\n\\r\\n     Refining the network parameters may repair knowledge missed in IPADS learning. Under\\r\\n\\r\\nthe principle of meta learning, introducing a sub-network to adjust hyper-parameters of the\\r\\n\\r\\noriginal network can adapt well to real data, which goes beyond the IPADS training set [18].\\r\\n\\r\\nFor instance (see Fig. 7), threshold in the backbone of sparse learning network can be adjusted\\r\\n\\x0c\\n\\nso that each input had its own threshold to eliminate undersampling artifacts, resulting in the\\r\\n\\r\\nmitigation of reconstruction performance loss under unseen sampling rates in IPADS [18].\\r\\n\\r\\nAnother way is introducing few real data to adjust network parameters by transfer learning.\\r\\n\\r\\nThis approach has been explored in conventional network training without IPADS [33]. For\\r\\n\\r\\nexample, thousands of natural images were assembled with phase and coil information and then\\r\\n\\r\\nused to train MRI reconstruction network. By transfer learning from tens of real MRI images,\\r\\n\\r\\nnearly identical reconstruction performance is achieved as training thousands of real images\\r\\n\\r\\n[33]. Since IPADS learning has approximated physical laws in biomedical magnetic resonance,\\r\\n\\r\\nmuch less uncertainty needs to be addressed in the transfer learning. Thus, fewer real data may\\r\\n\\r\\nbe required by IPADS learning than those of conventional network.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 7. Hyper-parameter adjustment with meta-learning in spectrum reconstruction [18]. (a) is the backbone network\\r\\nwith soft-thresholding and the hyper-network that adjusts the hyper-parameter (threshold), (b) robust reconstruction\\r\\nto unseen sampling rates in IPADS, (c) a region of reconstructed spectra. Note: Higher peak intensity correlation\\r\\nmeans better reconstruction of spectral peaks.\\r\\n\\r\\n\\r\\n     Directly incorporating physical models into network structure may relax the requirement\\r\\n\\r\\nof paired dataset. A good example is the recently proposed physics-informed neural networks\\r\\n\\r\\nfor myocardial perfusion MRI quantification. By modeling the evolution of the concentration\\r\\n\\r\\nof contrast agent as differential equations, a physics-informed network is proposed to\\r\\n\\r\\nsimultaneously estimate concentrations of contrast agent and quantify kinetic parameters [34].\\r\\n\\r\\nThis strategy does not need a large database of paired samples, but costs relatively long running\\r\\n\\r\\ntime (1 hour per imaging slice) under a shallow network of 4 layers [34]. Since the solutions\\r\\n\\r\\nsatisfy the underlying physical laws, the degrees of freedom to describe the imaging physics\\r\\n\\r\\nwould be reduced. Thus, the requirement of paired data may be relaxed. Besides, the IPADS\\r\\n\\r\\ndata could be used to train the initial network parameters offline. This would be very helpful to\\r\\n\\x0c\\n\\nreduce the computational time and allows using deeper networks to improve parameter\\r\\n\\r\\nquantification performance.\\r\\n\\r\\n                                  VI. SUMMARY AND OUTLOOK\\r\\n\\r\\n       The deep learning has innovated the field of computational imaging. One of its bottlenecks\\r\\n\\r\\nis lacking of labels in challenging applications. This article reviews the recent progress of\\r\\n\\r\\nbiomedical magnetic resonance on deep learning with imaging physics-based data synthesis\\r\\n\\r\\n(IPADS). Non or few real data is required since IPADS generate signals from partial equations\\r\\n\\r\\nor analytical solution models following physical laws, making the learning more scalable,\\r\\n\\r\\nexplainable and better protecting privacy. Great potentials of IPADS have been evidenced in\\r\\n\\r\\nfast imaging, ultrafast signal reconstruction and accurate parameter quantification.\\r\\n\\r\\n       Mitigating the difference between synthetic with real data is still at the early stage. Many\\r\\n\\r\\ncurrent methods highly depend on the accuracy of physical modeling and the selection of\\r\\n\\r\\nparameter ranges. Although good performance has been achieved on unseen real data,\\r\\n\\r\\nunpredictability in clinical situations remains the biggest risk of synthetic databased approaches.\\r\\n\\r\\nTo generalize synthetic-data learning to real biomedicine and clinic, more effort should be made\\r\\n\\r\\nto mine the data/parameter distribution of real data, design robust networks and characterize\\r\\n\\r\\nthe performance. Few subjective evaluations from radiologists have been found in the existed\\r\\n\\r\\nIPADS learning and needs to strengthen.\\r\\n\\r\\n       Besides, a powerful, open and any time accessible computing platform of IPADS is highly\\r\\n\\r\\nexpected. As programming for physical signal evolution is non-trivial, an online platform that\\r\\n\\r\\ncan fast and accurately generate data with user-defined parameters will allow more researchers\\r\\n\\r\\nto obtain huge IPADS datasets. These data can be used as training set for learning network or\\r\\n\\r\\nstress test data for developed deep learning imaging approaches. Scalable programming, such\\r\\n\\r\\nas graphic pulse sequence design for MRI, and high-precision simulation, e.g., non-rigid motion,\\r\\n\\r\\nproton diffusion and intravoxel multi-spin dephasing, will enable challenging imaging\\r\\n\\r\\napplications. It is worth noting that we have developed the prototype of such IPADS cloud\\r\\n\\r\\nplatform (Fig. 8), called CloudBrain1, and will continue to improve it with online physical\\r\\n\\r\\nevolution tools to better serve the community.\\r\\n\\r\\n\\r\\n  1\\r\\n      https://csrc.xmu.edu.cn/XCloudAiImaging.html\\r\\n\\x0c\\n\\nFig. 8. A vision map of CloudBrain.\\r\\n\\r\\n\\r\\n                                          ACKNOWLEDGMENTS\\r\\n\\r\\n     This work was supported in part by the National Natural Science Foundation of China\\r\\n\\r\\n(62122064, 61871341, 82071913), the Xiamen University Nanqiang Outstanding Talents\\r\\n\\r\\nProgram. The authors are grateful to China Mobile for cloud computing resources. The authors\\r\\n\\r\\nthank Yirong Zhou, Jun Liu, Haoming Fang, Jiayu Li, Bangjun Chen for building a prototype\\r\\n\\r\\nof the CloudBrain; Haitao Huang, Xinran Chen, Chen Qian for providing experimental results\\r\\n\\r\\nand valuable discussions and guest editors for providing valuable suggestions.\\r\\n\\r\\n                                                REFERENCES\\r\\n\\r\\n[1] J. S. Duncan, M. F. Insana, and N. Ayache, “Biomedical imaging and analysis in the age of big data and deep\\r\\nlearning,” Proc. IEEE, vol. 108, no. 1, pp. 3-10, Jan, 2020. doi: 10.1109/jproc.2019.2956422.\\r\\n[2] M. Jacob, J. C. Ye, L. Ying, and M. Doneva, “Computational MRI: Compressive sensing and beyond,” IEEE\\r\\nSignal Process. Mag., vol. 37, no. 1, pp. 21-23, Jan, 2020. doi: 10.1109/msp.2019.2953993.\\r\\n[3] A. F. Frangi, S. A. Tsaftaris, and J. L. Prince, “Simulation and synthesis in medical imaging,” IEEE Trans. Med.\\r\\nImag., vol. 37, no. 3, pp. 673-679, Mar, 2018. doi: 10.1109/tmi.2018.2800298.\\r\\n[4] R. J. Chen, M. Y. Lu, T. Y. Chen, D. F. K. Williamson, and F. Mahmood, “Synthetic data in machine learning for\\r\\nmedicine and healthcare,” Nat. Biomed. Eng., vol. 5, no. 6, pp. 493-497, Jun, 2021. doi: 10.1038/s41551-021-00751-\\r\\n8.\\r\\n[5] O. Cohen, B. Zhu, and M. S. Rosen, “MR fingerprinting Deep RecOnstruction NEtwork (DRONE),” Magn.\\r\\nReson. Med., vol. 80, no. 3, pp. 885-894, Sep, 2018. doi: 10.1002/mrm.27198.\\r\\n[6] C. Cai, C. Wang, Y. Zeng, S. Cai, D. Liang, Y. Wu, Z. Chen, X. Ding, and J. Zhong, “Single-shot T-2 mapping\\r\\nusing overlapping-echo detachment planar imaging and a deep convolutional neural network,” Magn. Reson. Med.,\\r\\nvol. 80, no. 5, pp. 2202-2214, Nov, 2018. doi: 10.1002/mrm.27205.\\r\\n[7] J. Zhang, J. Wu, S. Chen, Z. Zhang, S. Cai, C. Cai, and Z. Chen, “Robust single-shot T2 mapping via multiple\\r\\noverlapping-echo acquisition and deep neural network,” IEEE Trans. Med. Imag., vol. 38, no. 8, pp. 1801-1811, Aug,\\r\\n2019. doi: 10.1109/tmi.2019.2896085.\\r\\n[8] S. M. Li, J. Wu, L. C. Ma, S. H. Cai, and C. B. Cai, “A simultaneous multi-slice T2 mapping framework based\\r\\n\\x0c\\n\\non overlapping-echo detachment planar imaging and deep learning reconstruction,” Magn. Reson. Med., vol. 87, no.\\r\\n5, pp. 2239-2253, May, 2022. doi: 10.1002/mrm.29128.\\r\\n[9] Q. Yang, J. Wang, J. Bao, X. Wang, L. Ma, Q. Yang, S. Cai, H. He, C. Cai, and J. Dong, “Model-based synthetic\\r\\ndata-driven learning (MOST-DL): Application in single-shot T2 mapping with severe head motion using\\r\\noverlapping-echo acquisition,” 2021. [Online]. Available: https://arxiv.org/abs/2107.14521.\\r\\n[10] S. Gavazzi, C. A. T. van den Berg, M. H. F. Savenije, H. P. Kok, P. de Boer, L. J. A. Stalpers, J. J. W. Lagendijk,\\r\\nH. Crezee, and A. van Lier, “Deep learning-based reconstruction of in vivo pelvis conductivity with a 3D patch-\\r\\nbased convolutional neural network trained on simulated MR data,” Magn. Reson. Med., vol. 84, no. 5, pp. 2772-\\r\\n2787, Nov, 2020. doi: 10.1002/mrm.28285.\\r\\n[11] S. Bollmann, K. G. B. Rasmussen, M. Kristensen, R. G. Blendal, L. R. Ostergaard, M. Plocharski, K. O\\'Brien,\\r\\nC. Langkammer, A. Janke, and M. Barth, “DeepQSM - using deep learning to solve the dipole inversion for\\r\\nquantitative   susceptibility    mapping,”     Neuroimage,      vol.   195,    pp.    373-383,     Jul,   2019.    doi:\\r\\n10.1016/j.neuroimage.2019.03.060.\\r\\n[12] W. Jung, J. Yoon, J. Y. Choi, J. M. Kim, Y. Nam, E. Y. Kim, J. Lee, and S. Ji, “Exploring linearity of deep neural\\r\\nnetwork trained QSM: QSMnet+,” Neuroimage, vol. 211, May, 2020. doi: 10.1016/j.neuroimage.2020.116619.\\r\\n[13] G. Della Maggiora, C. Castillo-Passi, W. Qiu, S. Liu, C. Milovic, M. Sekino, C. Tejos, S. Uribe, and P.\\r\\nIrarrazaval, “DeepSPIO: Super paramagnetic iron oxide particle quantification using deep learning in magnetic\\r\\nresonance imaging,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 1, pp. 143-153, Jan, 2022. doi:\\r\\n10.1109/tpami.2020.3012103.\\r\\n[14] S. Jung, H. Lee, K. Ryu, J. E. Song, M. Park, W. J. Moon, and D. H. Kim, “Artificial neural network for multi-\\r\\necho gradient echo-based myelin water fraction estimation,” Magn. Reson. Med., vol. 85, no. 1, pp. 394-403, Jan,\\r\\n2021. doi: 10.1002/mrm.28407.\\r\\n[15] L. Chen, M. Schar, K. W. Y. Chan, J. Huang, Z. Wei, H. Lu, Q. Qin, R. G. Weiss, P. C. M. van Zijl, and J. Xu,\\r\\n“In vivo imaging of phosphocreatine with artificial neural networks,” Nature Communications, vol. 11, no. 1, Feb,\\r\\n2020. doi: 10.1038/s41467-020-14874-0.\\r\\n[16] X. Qu, Y. Huang, H. Lu, T. Qiu, D. Guo, T. Agback, V. Orekhov, and Z. Chen, “Accelerated nuclear magnetic\\r\\nresonance spectroscopy with deep learning,” Angew. Chem. Int. Ed., vol. 59, no. 26, pp. 10297-10300, Jun, 2020.\\r\\ndoi: 10.1002/anie.201908162.\\r\\n[17] Y. Huang, J. Zhao, Z. Wang, V. Orekhov, D. Guo, and X. Qu, “Exponential signal reconstruction with deep\\r\\nhankel matrix factorization,” IEEE Trans. Neural Netw. Learn. Syst., (Epub 2021 Dec 23), 2021. doi:\\r\\n10.1109/tnnls.2021.3134717.\\r\\n[18] Z. Wang, D. Guo, Z. Tu, Y. Huang, Y. Zhou, J. Wang, L. Feng, D. Lin, Y. You, T. Agback, V. Orekhov, and X.\\r\\nQu, “A sparse model-inspired deep thresholding network for exponential signal reconstruction--Application in fast\\r\\nbiological spectroscopy,” IEEE Trans. Neural Netw. Learn. Syst., (Epub 2022 Feb 04), 2022. doi:\\r\\n10.1109/tnnls.2022.3144580.\\r\\n[19] F. Lam, Y. Li, and X. Peng, “Constrained magnetic resonance spectroscopic imaging by learning nonlinear low-\\r\\ndimensional models,” IEEE Trans. Med. Imag., vol. 39, no. 3, pp. 545-555, Mar, 2020. doi:\\r\\n10.1109/tmi.2019.2930586.\\r\\n[20] Y. Li, Z. Wang, R. Sun, and F. Lam, “Separation of metabolites and macromolecules for short-TE H-1-MRSI\\r\\nusing learned component-specific representations,” IEEE Trans. Med. Imag., vol. 40, no. 4, pp. 1157-1167, Apr,\\r\\n2021. doi: 10.1109/tmi.2020.3048933.\\r\\n[21] X. Chen, W. Wang, J. Huang, J. Wu, L. Chen, C. Cai, S. Cai, and Z. Chen, “Ultrafast water-fat separation using\\r\\ndeep learning-based single-shot MRI,” Magn. Reson. Med., (Epub 2022 Jam 31), 2022. doi: 10.1002/mrm.29172.\\r\\n[22] A. Loktyushin, K. Herz, N. Dang, F. Glang, A. Deshmane, S. Weinmuller, A. Doerfler, B. Scholkopf, K.\\r\\n\\x0c\\n\\nScheffler, and M. Zaiss, “MRzero - Automated discovery of MRI sequences using supervised learning,” Magn.\\r\\nReson. Med., vol. 86, no. 2, pp. 709-724, Aug, 2021. doi: 10.1002/mrm.28727.\\r\\n[23] Y. Zhang, K. Jiang, W. Jiang, N. Wang, A. J. Wright, A. Liu, and J. Wang, “Multi-task convolutional neural\\r\\nnetwork-based design of radio frequency pulse and the accompanying gradients for magnetic resonance imaging,”\\r\\nNMR Biomed., vol. 34, no. 2, Feb, 2021. doi: 10.1002/nbm.4443.\\r\\n[24] M. Loecher, L. E. Perotti, and D. B. Ennis, “Using synthetic data generation to train a cardiac motion tag\\r\\ntracking neural network,” Med. Image Anal., vol. 74, Dec, 2021. doi: 10.1016/j.media.2021.102223.\\r\\n[25] F. Bloch, W. W. Hansen, and M. Packard, “Nuclear induction,” Phys. Rev., vol. 69, no. 3-4, pp. 127-127, 1946.\\r\\n[26] D. Ma, V. Gulani, N. Seiberlich, K. C. Liu, J. L. Sunshine, J. L. Duerk, and M. A. Griswold, “Magnetic resonance\\r\\nfingerprinting,” Nature, vol. 495, no. 7440, pp. 187-192, Mar, 2013. doi: 10.1038/nature11971.\\r\\n[27] M. A. Bernstein, K. F. King, and X. J. Zhou, \"Chapter 14 – Basic Pulse Sequences,\" Handbook of MRI Pulse\\r\\nSequences, M. A. Bernstein, K. F. King and X. J. Zhou, eds., pp. 579-647, Burlington: Academic Press, 2004.\\r\\n[28] X. Qu, M. Mayzel, J.-F. Cai, Z. Chen, and V. Orekhov, “Accelerated NMR spectroscopy with low-rank\\r\\nreconstruction,” Angew. Chem. Int. Ed., vol. 54, no. 3, pp. 852-854, Jan, 2015. doi: 10.1002/anie.201409291.\\r\\n[29] C. B. Cai, M. J. Lin, Z. Chen, X. Chen, S. H. Cai, and J. H. Zhong, “SPROM - an efficient program for\\r\\nNMR/MRI simulations of inter- and intra-molecular multiple quantum coherences,” C. R. Phys., vol. 9, no. 1, pp.\\r\\n119-126, Jan, 2008. doi: 10.1016/j.crhy.2007.11.007.\\r\\n[30] F. Liu, J. V. Velikina, W. F. Block, R. Kijowski, and A. A. Samsonov, “Fast realistic MRI simulations based on\\r\\ngeneralized multi-pool exchange tissue model,” IEEE Trans. Med. Imag., vol. 36, no. 2, pp. 527-537, Feb, 2017. doi:\\r\\n10.1109/tmi.2016.2620961.\\r\\n[31] H. H. Lee, and H. Kim, “Intact metabolite spectrum mining by deep learning in proton magnetic resonance\\r\\nspectroscopy of the brain,” Magn. Reson. Med., vol. 82, no. 1, pp. 33-48, Jul, 2019. doi: 10.1002/mrm.27727.\\r\\n[32] Y. Lim, Y. Bliesener, S. Narayanan, and K. S. Nayak, “Deblurring for spiral real-time MRI using convolutional\\r\\nneural networks,” Magn. Reson. Med., vol. 84, no. 6, pp. 3438-3452, 2020. doi: 10.1002/mrm.28393.\\r\\n[33] S. U. Dar, M. Ozbey, A. B. Catli, and T. Cukur, “A transfer-learning approach for accelerated MRI using deep\\r\\nneural networks,” Magn. Reson. Med., vol. 84, no. 2, pp. 663-685, Aug, 2020. doi: 10.1002/mrm.28148.\\r\\n[34] R. L. M. van Herten, A. Chiribiri, M. Breeuwer, M. Veta, and C. M. Scannell, “Physics-informed neural\\r\\nnetworks for myocardial perfusion MRI quantification,” Med. Image Anal., vol. 78, pp. 102399, May, 2022. doi:\\r\\n10.1016/j.media.2022.102399.\\r\\n\\x0c',\n",
       " '                                              DBSOP: An Efficient Heuristic for Speedy\\r\\n                                                  MCMC Sampling on Polytopes\\r\\n                                                                              A preprint\\r\\n\\r\\n\\r\\n                                                           Christos Karras1       and Aristeidis Karras1\\r\\narXiv:2203.10916v1 [cs.CG] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         Computer Engineering and Informatics Department,\\r\\n                                                               University of Patras, Patras, Hellas\\r\\n                                                             {c.karras, akarras}@ceid.upatras.gr\\r\\n\\r\\n\\r\\n\\r\\n                                               Abstract. Markov Chain Monte Carlo (MCMC) techniques have long\\r\\n                                               been studied in computational geometry subjects whereabouts the prob-\\r\\n                                               lems to be studied are complex geometric objects which by their nature\\r\\n                                               require optimized techniques to be deployed or to gain useful insights\\r\\n                                               by them. MCMC approaches are directly answering to geometric prob-\\r\\n                                               lems we are attempting to answer, and how these problems could be de-\\r\\n                                               ployed from theory to practice. Polytope which is a limited volume in n-\\r\\n                                               dimensional space specified by a collection of linear inequality constraints\\r\\n                                               require specific approximation. Therefore, sampling across density based\\r\\n                                               polytopes can not be performed without the use of such methods in which\\r\\n                                               the amount of repetition required is defined as a property of error mar-\\r\\n                                               gin. In this work we propose a simple accurate sampling approach based\\r\\n                                               on the triangulation (tessellation) of a polytope. Moreover, we propose\\r\\n                                               an efficient algorithm named Density Based Sampling on Polytopes (DB-\\r\\n                                               SOP) for speedy MCMC sampling where the time required to perform\\r\\n                                               sampling is significantly lower compared    to existing approaches in low\\r\\n                                               dimensions with complexity O∗ n3 . Ultimately, we highlight possible\\r\\n                                                                                    \\x01\\r\\n\\r\\n                                               future aspects and how the proposed scheme can be further improved\\r\\n                                               with the integration of reservoir-sampling based methods resulting in\\r\\n                                               more speedy and efficient solution.\\r\\n\\r\\n                                               Keywords: Polytopes · Uniform Sampling · Data Engineering · Convex\\r\\n                                               Bodies · Markov Chain · Monte Carlo · Hit-and-Run Methods\\r\\n\\r\\n\\r\\n                                         1   Introduction\\r\\n\\r\\n                                         The sampling process across different distributions is a major topic in statistics,\\r\\n                                         probability, systems engineering, as well as other disciplines that use stochastic\\r\\n                                         models ([6],[11],[12],[25],[26]). Before Monte-Carlo techniques may be used to\\r\\n                                         estimate anticipated values and other integrals, sampling algorithms must first\\r\\n                                         be developed and implemented. In recent decades, Markov Chain Monte Carlo\\r\\n                                         (MCMC) algorithms have gained remarkable success; for example, the book [7]\\r\\n                                         and the references therein discuss this issue in great detail. These tactics are\\r\\n                                         predicated on the creation of a Markov model with a density function that\\r\\n\\x0c\\n\\n2        C. Karras et al.\\r\\n\\r\\nmatches the goal distribution in which the chain is simulated for a set number\\r\\nof steps to generate samples. MCMC algorithms offer the benefit of requiring\\r\\njust wisdom of the desired density up to a ratio constant, significantly reducing\\r\\nthe quantity of data required. On the other hand, theoretical knowledge of the\\r\\nMCMC methods that are employed in practice is far from adequate. It is critical\\r\\nto control the decomposition rate of a MCMC operation, which can be defined as\\r\\nthe amount of repetitions required as a property of error margin, issue element\\r\\nn, and other variables for the chain to land on a distribution that is well within\\r\\na specific range from the objective.\\r\\n\\r\\n1.1    Problem definition\\r\\nWe are concerned with the issue of sampling from a uniform density across a\\r\\nconvex polytope1 , which is a limited volume in n-dimensional space specified by a\\r\\ncollection of linear inequality constraints, and we are interested in sampling from\\r\\na convex polytope. There are several applications for this sampling issue, but we\\r\\nare particularly interested in its application to the sampling of weight vectors\\r\\nfor multi-class discriminant analysis (MCDA). Previous research has shown that\\r\\nthe method of Hit-n-Run may be often employed to this particular use case\\r\\n[23]. Hit-n-Run has the drawback of being a MCMC method, which necessitates\\r\\nthat use of convergence is required checking or oversampling to confirm that\\r\\nconvergence has been achieved. In this work, we investigate a straightforward\\r\\nprecise sampling procedure based on the triangulation (tesselation) of a polytope.\\r\\nTechnical abbreviations are defined the very first time they appear in the text.\\r\\nUltimately, the notation used in this work is given in table 1.\\r\\n\\r\\n\\r\\n                              Table 1. Notation of this work.\\r\\n\\r\\n             Symbol         Meaning                              First in\\r\\n             4\\r\\n             =              Definition or equality by definition Eq. (1)\\r\\n             |·|            Absolute value                       Eq. (4)\\r\\n             det(·)         Determinant                          Eq. (4)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2     Related Work\\r\\nWith a number of applications and methodologies, the challenge of equally sam-\\r\\npling from a polytope is crucial to the success of the process. A good exam-\\r\\nple is the basis for a number of ways of estimating randomised approxima-\\r\\ntions to polytope volumes, such as the one described here. A lengthy history\\r\\nof study on sampling strategies for generating randomised estimates to the di-\\r\\nmensions of polytopes and other convex structures can be found in works such\\r\\n1\\r\\n    Not to be confused with Polytropes.\\r\\n\\x0c\\n\\n                         Efficient and speedy MCMC sampling on polytopes          3\\r\\n\\r\\nas [21],[18],[3],[20],[8]. Aspects of polytope sampling that are particularly advan-\\r\\ntageous include the development of fast randomised algorithms for multiobjec-\\r\\ntive problems [4] and sampling situational tables [15], Additionally, randomised\\r\\nstrategies for approximated solving mixed - integer linear convex programmes\\r\\nare being studied and developed [13]. Polytope sampling, as indicated in [16], is\\r\\nalso associated with hard-disk model simulators in statistical physics, along with\\r\\nestimations of erroneous incidences for linear programming in communication [9].\\r\\n    In order to sample across a uniform distribution encompassing a targeted\\r\\npolytope, one approach follows the assumption to gain useful samples from a ho-\\r\\nmogeneous proposal density which is covering the targeted polytope, for instance,\\r\\na homogeneous density centred on a square hyperbox, or a Dirichlet distribution,\\r\\nboth of which are examples of uniform proposal densities. As demonstrated in\\r\\n[14], the Dirichlet population is uniform across the simplex when the density\\r\\nfactor is assigned to 1. This attribute was employed to establish homogeneity\\r\\nthroughout the simplex in the multi-class discriminant analysis (MCDA) sce-\\r\\nnario [19]. In order to avoid a situation where the proposal density is close to\\r\\nthe desired density, such techniques must include a rejection phase. Generally,\\r\\nthe rate of rejection grows in a exponential way proportionally with the size of\\r\\nthe sample space, making this strategy ineffective for large sample spaces [22].\\r\\nAdditionally, weights may be simulated using a variety of MCMC techniques,\\r\\nwhich are detailed below. Typically, a trade-off arises among the frequency of\\r\\nmixing and the rate of acceptance by the sampler. While dealing with homoge-\\r\\nneous joints and dependent distributions, a solitary-state sampler such as Gibbs\\r\\nis the ideal approach [10]. In this circumstance, the rate of rejection is zero by\\r\\ndefault, and the weights can be repeatedly replicated while adhering to the linear\\r\\nlimits and ratio limitations set out. It has been shown that using a systematic\\r\\nstrategy of repeated sampling, there are strong connections between drawings\\r\\nand delayed mixing [1],[5]. Improved mixing characteristics may be achieved\\r\\nby modelling the weights together using random walk methods, as opposed to\\r\\nsimulating them separately.\\r\\n    Numerous MCMC approaches have widely been investigated for sample pro-\\r\\ncesses through polytopes schemas and, more broadly, for convex bodies sampling\\r\\nprocesses. There are several preliminary observations of algorithms that perform\\r\\nsampling derived from broad convex bodies, including the Ball Walk shown in\\r\\n[21] and the hit-n-run approach proposed in [3],[20]. Despite the fact that these\\r\\napproaches are applicable to polytopes, they do not take use of the particular\\r\\nstructure presented by the issue. In contrast, the Dikin walk was introduced in\\r\\n[15], which is tailored for polytopes and so achieves greater convergence rates\\r\\nthan generic techniques. With its connection to methodologies for solving lin-\\r\\near programmes using interior point approaches, the Dikin walk was the first\\r\\nsampling process found globally. Additionally, as stated in greater detail later\\r\\nin this section, it generates proposal distributions beginning with the typical\\r\\nlogarithmic barrier for a polytope. As inducted in [24], it was shown that the\\r\\nDikin walk may be extended to generic curves with subconscious barriers, which\\r\\nwas proven in a further study.\\r\\n\\x0c\\n\\n4       C. Karras et al.\\r\\n\\r\\n3     Methodology\\r\\n3.1   Definitions and requisites\\r\\nDefinition 1 (Polytope). A bounded convex n-polytope or polytope is the group\\r\\nof points\\r\\n                             4\\r\\n                          P = { p : Ap ≤ b }                              (1)\\r\\nin Rn , where A is a r × n real matrix of coefficients, b is a r-vector, and the\\r\\nrelation ≤ is meant elementwise. A polytope can be determined by its vertices or\\r\\nextreme points V (V represenation of polytopes).\\r\\n\\r\\n\\r\\n\\r\\nDefinition 2 (Simplex). Let v0 , . . . , vn be points in general position in Rn .\\r\\nThe set\\r\\n            n                                     n\\r\\n                                                  X                           o\\r\\n          4\\r\\n        S = p : p = a0 v0 + . . . an vn , ai ≥ 0,   ai = 1 ∀ i = 0, . . . , n   (2)\\r\\n                                                   i=0\\r\\n                                  n\\r\\nis a n-simplex or simplex in R . S is an n-dimensional polytope.\\r\\nMore compactly, write V0 = (v1 − v0 , . . . , vn − v0 ) and a = (a1 , . . . , an )T , with\\r\\n(T ) denoting transpose. Then\\r\\n                           p=    a0 v0 + a1 v1 + . . . an vn\\r\\n                                   Pn              Pn\\r\\n                            = (1 − i=1 ai )v0 + i=1 ai vi\\r\\n                                       Pn\\r\\n                            =    v0 + i=1 ai (vi − v0 )\\r\\n                            =            v0 + V0 a,\\r\\nand (2) becomes\\r\\n\\r\\n                     S = {p : p = v0 + V0 a, a ≥ 0, a1T ≤ 1},                         (3)\\r\\nwhere ≥, ≤ are meant elementwise.\\r\\nThe volume of S is\\r\\n                                         | det(V0 )|\\r\\n                                 Vol(S) =            ,                           (4)\\r\\n                                             n!\\r\\nwhere |·| means absolute value and det(·) means determinant. Because v0 , . . . , vn\\r\\nare in general position, rank(V0 ) = n and det(V0 ) 6= 0.\\r\\n\\r\\n\\r\\n\\r\\nLemma 1. Simplicial decomposition of polytopes\\r\\nAn n-polytope P can be decomposed into n-dimensional simplices Sk , k = 1, . . . K\\r\\nsuch that P = S1 ∪ · · · ∪ SK and, for k 6= l, Sk ∩ Sl = ∅ or Sk ∩ Sl = T , where\\r\\nT is a lower-dimensional simplex.\\r\\nCorollary 1. If S1 , . . . , SK is a simplicial decomposition of P, then Vol(P) =\\r\\nPK\\r\\n k=1 Vol(Sk ).\\r\\n\\x0c\\n\\n                            Efficient and speedy MCMC sampling on polytopes     5\\r\\n\\r\\n3.2   Construction of a uniform density over a simplex S\\r\\nTheorem 1. Assume w = [wi ], i = 1, . . . , n to be a vector created at random\\r\\nwith wi ≥ 0, w1 +· · ·+wn ≤ 1, and density h(w). Then the vector p = v0 +V0 w ∈\\r\\nS has density\\r\\n                            g(p) = h(w)| det(V0 )|−1 ,                       (5)\\r\\nProof.\\r\\nThis preceding proof utilizes the multivariate principle of Change of Variables:\\r\\n                                   \\x10 dw \\x11\\r\\n                 g(p) = h(w) det           = h(w)| det(V0 )|−1 ,\\r\\n                                     dp\\r\\n       \\x10 \\x11\\r\\n                           −1\\r\\nwhere dwdp    = ∂w\\r\\n                 ∂pj = (V0 )ij .\\r\\n                   i\\r\\n\\r\\n             ij\\r\\n\\r\\n   To construct a uniform probability density in S define\\r\\n                                                       P a random variable w\\r\\nwith uniform density on a regular simplex W = {wi ≥ 0, wi ≤ 1, i = 0, . . . , n}.\\r\\nSuch is a n-dimensional Dirichlet distribution\\r\\n                            h(w) = Dirichlet(1) = (n!)−1 .                     (6)\\r\\nFrom Theorem 1 the choice (6) results in a random vector p with uniform density\\r\\ng(p) = (| det(V0 )|n!)−1 over S.\\r\\n\\r\\n3.3   Construction of a uniform density over a polytope P\\r\\nWe denote f (p) for the following cases as:\\r\\n                  (\\r\\n                    gk (p)P (p ∈ Sk ) , if p ∈ Sk ⊆ P ∀ k = 1, . . . , K\\r\\n         f (p) =                                                               (7)\\r\\n                    0                        /P\\r\\n                                      , if p ∈\\r\\nwhere P (p ∈ Sk ) the probability that p belongs to the k-th simplex of a decom-\\r\\nposition of P as per Lemma 1. Choose P (p ∈ Sk ) = Vol(Sk )/Vol(P) for all k.\\r\\nThen for the k-th simplex we obtain:\\r\\n                                        \\x10 Vol(S ) \\x11\\r\\n                                                k\\r\\n             gk (p)P (w ∈ Sk ) = gk (p)\\r\\n                                          Vol(P)\\r\\n                                        1       \\x10 | det(V )|/n! \\x11\\r\\n                                                            0k\\r\\n                               =                  PK\\r\\n                                 | det(V0k )|n!     j=1 | det(V0j )|/n!\\r\\n                                                 1\\r\\n                                   =        PK\\r\\n                                       n!    j=1 | det(V0j )|\\r\\n\\r\\nsubsequently (7) becomes\\r\\n                      (            PK                  \\x01−1\\r\\n                              n!       j=1 | det(V0j )|         , if p ∈ P\\r\\n                  f (p) =                                                  .   (8)\\r\\n                             0                                         /P\\r\\n                                                                , if p ∈\\r\\n\\r\\nAs per (8), the construction results in a uniform density distribution over the\\r\\npolytope P.\\r\\n\\x0c\\n\\n6       C. Karras et al.\\r\\n\\r\\n3.4   Proposed Algorithm for Sampling\\r\\nGiven the results derived above, we can sample uniformly from a convex polytope\\r\\nP as defined in algorithm 1.\\r\\n\\r\\n\\r\\nAlgorithm 1 Density Based Sampling On Polytope P (DBSOP)\\r\\nRequire: Vertices v0 , . . . , vn of a polytope P\\r\\nEnsure: Uniform sampling from a convex polytope P\\r\\n 1: Find the vertices v0 , . . . , vn of the polytope.\\r\\n    This can be achieved using the Avis-Fukuda pivoting algorithm as in [2].\\r\\n 2: Decompose P in n-simplices S1 , . . . , SK using, e.g., Delaunay triangulation\\r\\n    (any triangulation satifying Lemma 1 is appropriate).\\r\\n 3: Return the vertices V(Sk ) and content Vol(Sk ) of each simplex Sk from the\\r\\n    triangulation process.\\r\\n 4: Set q = (q1 , . . . , qK ) with qk = Vol(Sk )/Vol(P).\\r\\n 5: for the i-th of N samples do\\r\\n 6:    Draw a random vector wi form a regular n-simplex:\\r\\n       wi ∼ Dirichlet(1).\\r\\n 7:    Decide which simplex is sampled from ji ∼ Categorical(q).\\r\\n 8:    Compute the point pi as: pi = vji 0 + Vji 0 wi\\r\\n 9: end for\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n3.5   Implementation\\r\\nThe implementation of the proposed scheme is in RStudio where several libraries\\r\\nwere used for each step. To find the number of vertices the findVertices func-\\r\\ntion is used derived from the hitandrun package which in turn uses the rcdd\\r\\npackage. To perform the tessellation the delaunayn function is used obtained\\r\\nfrom the geometry package. The function simplex.sample to sample from the\\r\\ndegenerate Dirichlet is used acquired from the hitandrun package and the func-\\r\\ntion sample obtained from the base package is used to sample from the Cate-\\r\\ngorical.\\r\\n\\r\\n3.6   Complexity\\r\\nDue to the fact that the number of simplices created may scale up to n!, triangu-\\r\\nlation is by far the most dominant term for the complexity. Hence, the algorithm\\r\\nis not feasible in high-dimensional space. The overall complexity is O∗ n3 .\\r\\n                                                                            \\x01\\r\\n\\r\\n\\r\\n4     Results\\r\\nThe running times are shown in table 2 and they were obtained using a 5.2 GHz\\r\\nIntel Core i9-10850k CPU and 32 GB of RAM for a fairly simple polytope. We\\r\\nrefer to n as the number of n dimensions of polytopes and to k as interactions.\\r\\n\\x0c\\n\\n                                                                Efficient and speedy MCMC sampling on polytopes                                                                                                7\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      0.4\\r\\n                                             + ++++++\\r\\n                                             +++           ++++++\\r\\n                                                          ++           ++++\\r\\n                                                                  ++++++       +++++++++     ++    +++\\r\\n                                                                                               +++++    +++++      +++ ++\\r\\n                                                                                                              ++++++      +++++++\\r\\n                                                                                                                                   ++++++++           ++++++++++    ++ +++  ++\\r\\n                                                 +++ +++++\\r\\n                                                    ++    ++    +        +   +\\r\\n                                                                            ++++++++\\r\\n                                                                             ++       +++   ++      +++   ++++++       +++                     ++++\\r\\n                                                                                                                                                   +++ +++\\r\\n                                                                                                                                                   +++++   +++++  +++ +  +++++\\r\\n                                                ++++ ++++ +++++++++\\r\\n                                                                  +++++++ +++ ++++++++\\r\\n                                                                                     +++  ++\\r\\n                                                                                          ++++++++++++         +++\\r\\n                                                                                                                  +++\\r\\n                                                                                                                    +++ ++  +++++++    +++++\\r\\n                                                                                                                                           ++++           +++\\r\\n                                                                                                                                                          +  +++\\r\\n                                                                                                                                                               ++   +++++   ++++\\r\\n                                                   +++\\r\\n                                                      ++++++  +\\r\\n                                                               +\\r\\n                                                               +++\\r\\n                                                               +   + +  ++\\r\\n                                                                        ++ ++++   + + +++  +  +\\r\\n                                                                                              ++\\r\\n                                                                                               ++   +++++++++++\\r\\n                                                                                                              +       ++++ +  ++\\r\\n                                                                                                                               +   ++++    +     + +\\r\\n                                                                                                                                                   +\\r\\n                                                                                                                                                     ++\\r\\n                                                                                                                                                     ++++    + +\\r\\n                                                                                                                                                               +\\r\\n                                                                                                                                                                 ++\\r\\n                                                                                                                                                                 + +      ++\\r\\n                                                                                                                                                                           + +++\\r\\n                                                               ++        +            ++ ++  +      ++++                                   ++               +\\r\\n                                                     ++++++\\r\\n                                                            ++++++++\\r\\n                                                                 ++++\\r\\n                                                                     +++\\r\\n                                                                    +++  ++++++ ++\\r\\n                                                                                +\\r\\n                                                                                    ++\\r\\n                                                                                      +++ + ++ +++\\r\\n                                                                                               +   +\\r\\n                                                                                                   +\\r\\n                                                                                                   ++++++ ++++++++++++++++++       ++++\\r\\n                                                                                                                             ++++++++    +++\\r\\n                                                                                                                                                   +++++\\r\\n                                                                                                                                                 +++ +++++\\r\\n                                                                                                                                                                ++\\r\\n                                                                                                                                                           +++++++++\\r\\n                                                                                                                                                                        ++\\r\\n                                                                                                                                                                       ++\\r\\n                                                                                                                                                                        +\\r\\n                                                                                                                                                                          +++\\r\\n                                                                                                                                                                           +\\r\\n                                                                                                                                                                           ++\\r\\n                                                                                                                                                                               +\\r\\n                                                         +++   +\\r\\n                                                              +++ + ++ +\\r\\n                                                                + ++++++++\\r\\n                                                                         +++ +  +  ++ +  + ++++ +\\r\\n                                                                                                ++ +     ++ +    +  +  ++  ++  +  +         +  +     + + +   +   ++ +++ + + + +\\r\\n                                                                                 ++++ +++++++++     +      ++++++++++              ++         +++++++++++++++\\r\\n                                                              +++++              +                  +++++\\r\\n                                                                                                      +            +++++   +++++ +++ +++    +\\r\\n                                                                                                                                           ++++++                             ++\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      0.2\\r\\n                                                             ++       +++ +++++             ++++++++    ++ ++++++         +                            ++++++++      ++++\\r\\n                                                                +        ++++ ++ ++ +++++++                +        +++            +++           ++  ++ ++    ++++      +++++\\r\\n                                                                  ++++\\r\\n                                                                 ++   ++++   +   +++++++    ++++\\r\\n                                                                                              +++ ++   +     ++\\r\\n                                                                                                          ++++++++    ++++\\r\\n                                                                                                                     ++   +++++\\r\\n                                                                                                                            +  +++\\r\\n                                                                                                                                 ++\\r\\n                                                                                                                                   + ++++      +++ ++    +++        ++++++ +++ +\\r\\n                                                                    +++ +++  + +\\r\\n                                                                               +++  ++ +\\r\\n                                                                                        +\\r\\n                                                                                        +++\\r\\n                                                                                         +\\r\\n                                                                                          ++\\r\\n                                                                                           +++\\r\\n                                                                                             +\\r\\n                                                                                                  +\\r\\n                                                                                                  +\\r\\n                                                                                                     +++\\r\\n                                                                                                       + +    ++++\\r\\n                                                                                                                 +\\r\\n                                                                                                                  ++\\r\\n                                                                                                                   +\\r\\n                                                                                                                    +\\r\\n                                                                                                                    ++\\r\\n                                                                                                                     +\\r\\n                                                                                                                      ++++  +\\r\\n                                                                                                                             ++\\r\\n                                                                                                                             + ++  +++++++     +\\r\\n                                                                                                                                                 + +++\\r\\n                                                                                                                                                  ++  ++++  +++\\r\\n                                                                                                                                                            +\\r\\n                                                                                                                                                                +++++ + +\\r\\n                                                                                                                                                                        ++  ++\\r\\n                                                                        +++  +++ +++\\r\\n                                                                                           +\\r\\n                                                                                      ++++++++\\r\\n                                                                                    +++       ++ + +     ++\\r\\n                                                                                                     ++++++++          +\\r\\n                                                                                                                   ++++++++     ++++    +++     +++       +  +\\r\\n                                                                                                                                                      ++++++++++          +++++\\r\\n                                                                           ++++++ + +\\r\\n                                                                                    + ++   +\\r\\n                                                                                                ++\\r\\n                                                                                           ++++++   +++++++\\r\\n                                                                                                          +   ++\\r\\n                                                                                                              +  ++++ +++  ++\\r\\n                                                                                                                           + +\\r\\n                                                                                                                              ++++\\r\\n                                                                                                                               +   +  +  + +  +\\r\\n                                                                                                                                               ++\\r\\n                                                                                                                                                + + +++++++++++++\\r\\n                                                                                                                                                         + +              + ++++\\r\\n                                                                             +     +++++++++++      +  +    +  +++                 + +\\r\\n                                                                                                                        +++++++++++++++++           +  + +   +\\r\\n                                                                                                                                                      + ++++++   +++++++   + +\\r\\n                                                                              +++++  +++++  +        ++++++++++    +++++\\r\\n                                                                                                                       +++\\r\\n                                                                                                                                  ++  +++++ ++  ++++   ++ +++            +++++\\r\\n                                                                                                                                ++++                  ++++++++\\r\\n                                      0.0\\r\\n                                                                                 ++++++\\r\\n                                                                                    +++\\r\\n                                                                                         +++ +++  +++++            ++     +++                 ++\\r\\n                                                                                                                                         ++++++     +++              ++++\\r\\n                                                                                       +++\\r\\n                                                                                       +\\r\\n                                                                                         +\\r\\n                                                                                        +++   +++++\\r\\n                                                                                           ++++        ++ +\\r\\n                                                                                                         ++\\r\\n                                                                                                           ++\\r\\n                                                                                                            ++++++++++    +\\r\\n                                                                                                                        +++\\r\\n                                                                                                                        +  +++\\r\\n                                                                                                                             ++\\r\\n                                                                                                                               +\\r\\n                                                                                                                               ++++++       ++++\\r\\n                                                                                                                                                  ++ +++++++++++\\r\\n                                                                                       +      ++   + ++ +    +      +\\r\\n                                                                                                                    + ++\\r\\n                                                                                                                       +\\r\\n                                                                                                                       +  +   +    + ++       +    + +\\r\\n                                                                                           +              +     +\\r\\n                                                                                                           +++++++++        +   +       +\\r\\n                                                                                          ++ +++++\\r\\n                                                                                               +++  ++   ++            ++\\r\\n                                                                                                                    +++++ +\\r\\n                                                                                                                            +++    +++++\\r\\n                       simplex[, 2]\\r\\n\\r\\n\\r\\n                                                                                                      ++   ++++++\\r\\n                                                                                               +++    ++++        ++++++++\\r\\n                                                                                                                   ++\\r\\n                                                                                                   ++\\r\\n                                                                                                  ++    ++++++\\r\\n                                      −0.2\\r\\n\\r\\n                                                                                                     ++++\\r\\n                                      −0.4\\r\\n                                      −0.6\\r\\n                                      −0.8\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                        −0.6                                −0.4                                 −0.2                                   0.0        0.2   0.4      0.6\\r\\n\\r\\n                                                                                                                                                         simplex[, 1]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                Fig. 1. Polytope sampling using the proposed method\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                      10\\r\\n                      9\\r\\n                      8\\r\\n                      7\\r\\n        Samples (n)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                      6\\r\\n                      5\\r\\n                      4\\r\\n                      3\\r\\n                      2\\r\\n                                       0                              1000                                                           2000      3000                                            4000     5000\\r\\n                                                                                                                                        Time (sec)\\r\\n\\r\\n                                                                                            Fig. 2. Samples vs Time\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n   As depicted in figure 2, the algorithm achieves fast sampling up to n = 8 and\\r\\nshows a steady performance across n = 9, . . . n = 10.\\r\\n\\x0c\\n\\n8      C. Karras et al.\\r\\n\\r\\n\\r\\n                       500\\r\\n\\r\\n                       400\\r\\n        Vertices (v)   300\\r\\n\\r\\n                       200\\r\\n\\r\\n                       100\\r\\n\\r\\n                        0\\r\\n                             0       1000       2000      3000      4000       5000\\r\\n                                                   Time (sec)\\r\\n\\r\\n                                       Fig. 3. Vertices vs Time\\r\\n\\r\\n\\r\\n\\r\\n    As depicted in figure 3, the vertices found by the algorithm are ≈ 200 in a\\r\\nrelative short time interval while for ≥ 250 the process of finding vertices occurs\\r\\nwith a stable performance.\\r\\n\\r\\n\\r\\n\\r\\n                       500\\r\\n\\r\\n                       400\\r\\n\\r\\n                       300\\r\\n        Vertices (v)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                       200\\r\\n\\r\\n                       100\\r\\n\\r\\n                        0\\r\\n                             2   3          4   5       6       7   8      9     10\\r\\n                                                    Samples (n)\\r\\n\\r\\n                                      Fig. 4. Vertices vs Samples\\r\\n\\r\\n\\r\\n\\r\\n   As depicted in figure 4 the sampling of n-polytopes vs the vertices found can\\r\\nbe expressed in a f (x) = 2x − 1 way.\\r\\n\\x0c\\n\\n                         Efficient and speedy MCMC sampling on polytopes         9\\r\\n\\r\\n                             Table 2. Actual Results.\\r\\n\\r\\n                                   Actual Results\\r\\n                    n        t (s)      v             K\\r\\n                    2        0.142       3             1\\r\\n                    3        0.151       6             5\\r\\n                    4        0.154      12            44\\r\\n                    5        0.159      18           210\\r\\n                    6        0.218      39          2.486\\r\\n                    7        0.731      62         19.763\\r\\n                    8       10.218     127        359.214\\r\\n                    9       202.97     243       4.481.667\\r\\n                    10     5124.75     498      62.743.338\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    For n = 10, the triangulation started swapping out of RAM (31GB used out\\r\\nof 32 total), and therefore this is the last actual measurement taken. Noteworthy,\\r\\nthe running time is almost three minutes for n = 9. Moreover, the triangulation\\r\\nonly becomes a dominant cost at n = 7, and in lower dimensions the running\\r\\ntime could be reduced by about 45% through more efficient implementation of\\r\\nstep 3 of the algorithm.\\r\\n   Rejection sampling is similarly only feasible up to about n = 8 [3] (note that\\r\\ntheir n is our n + 1). However, our algorithm is significantly faster for n = 7 and\\r\\nn = 8, for example rejection sampling takes over 10 seconds for n = 8 and about\\r\\n200 seconds or 3 minutes and 20 seconds for n = 9.\\r\\n\\r\\n\\r\\n                           Table 3. Predicted Results.\\r\\n\\r\\n                                  Predicted Results\\r\\n   n            t (s)      t (days)             v                   K\\r\\n   11          13572          0.15            1.082            125.486.676\\r\\n   12          34638          0.40            3.246            376.460.028\\r\\n   13         271484            3            12.984           1.505.840.112\\r\\n   14         678605           7.8           64.920           7.529.200.564\\r\\n   15         1678609         19.4          389.520          45.175.203.247\\r\\n   16         4763672         55.1         2.726.640        316.226.423.531\\r\\n   17         8163851         94.4        21.813.120       2.529.811.388.174\\r\\n   18        21263149        246.1       196.318.080      22.768.302.493.218\\r\\n   19        72163554        835.2      2.159.498.880    227.683.024.934.355\\r\\n\\x0c\\n\\n10     C. Karras et al.\\r\\n\\r\\n    Because of lack of more RAM we used a machine learning model trained\\r\\non the results of table 2 to predict the results for n = 11, 12 . . . n = 19. Table\\r\\n3 depicts the results obtained by the machine learning model where the model\\r\\nshows that as with the actual results, the number of vertices as well as the K and\\r\\nthe time t(s) grows exponentially. Note that for n = 19, the prediction shows\\r\\nthat the time required to calculate the polytope will be approximately 835 days\\r\\nor almost 2 years. Hence, it is crucial to readjust the algorithm in step 3 rather\\r\\nthan trying to calculate higher dimensions or using more RAM.\\r\\n\\r\\n5    Evaluation\\r\\nIn this section we evaluate the proposed method to existing techniques such\\r\\nas bench and har [27]. Figure 5 depicts the average time required to perform\\r\\nsampling for n = 1, 2 . . . n = 10. As shown in the figure, the proposed method\\r\\noutperforms the other two existing approaches by ≈ 35%. The evaluation metrics\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n       Fig. 5. Comparison of the proposed method in terms of average time\\r\\n\\r\\n\\r\\nare shown in table 4. The proposed method outperforms the other two existing\\r\\nmethods across all three metrics (values shown are average) and the overall\\r\\nperformance achieved was higher. We moreover define shape compactness (SC)\\r\\nas the number of samples divided by each sampled dimension.\\r\\n\\r\\n\\r\\n                   Table 4. Evaluation of the proposed method.\\r\\n\\r\\n                     Evaluation Metric bench har DBSOP\\r\\n                     Z-value            2.1 4.9    6.4\\r\\n                     SCE                0.04 0.10 0.27\\r\\n                     SC                 6.4 8.4 11.2\\r\\n\\x0c\\n\\n                         Efficient and speedy MCMC sampling on polytopes           11\\r\\n\\r\\n6   Conclusions and Future Work\\r\\n\\r\\nIn the context of this work, a solution to the problem of sampling from a uni-\\r\\nform density over a convex polytope is presented, where polytope is a finite\\r\\nvolume in n-dimensional space characterized by a combination of linear inequal-\\r\\nity constraints. This sampling problem has a variety of applications, but we are\\r\\nespecially interested in how it might be used to the sampling of weight vectors\\r\\nfor multi-class discriminant analysis (MCDA). The outcome of the proposed al-\\r\\ngorithm resulted in a efficient and fast sampling scheme whereabouts the time\\r\\nrequired was significantly lower than existing methods in low dimensions. How-\\r\\never, for high dimensions we may require futher investigation of the rejection\\r\\nrate. Future directions of this work include the readjustment of step 3 of the\\r\\nalgorithm to decrease the time required to perform sampling. An efficient vari-\\r\\nation of this step could decrease the cost significantly resulting in a ≈ 45%\\r\\nreduction. Moreover, another future aspect is to transform the problem of sam-\\r\\npling in a CPU-based approach rather than using RAM memory, which will\\r\\nresult in a parallel execution of all steps without requiring significant amount of\\r\\nI/Os. Ultimately, a potential path for this work in the future is the integration of\\r\\nreservoir-based sampling techniques as in [17] where the selection of k elements\\r\\nrepresentative of the whole distribution will occur to enhance the overall per-\\r\\nformance and to further reduce the time as well as the cost required to perform\\r\\nsampling.\\r\\n\\r\\n\\r\\nReferences\\r\\n 1. Amit, Y., Grenander, U.: Comparing sweep strategies for stochastic relaxation.\\r\\n    Journal of multivariate analysis 37(2), 197–222 (1991)\\r\\n 2. Avis, D., Fukuda, K.: A pivoting algorithm for convex hulls and vertex enumeration\\r\\n    of arrangements and polyhedra. Discrete & Computational Geometry 8(1), 295–\\r\\n    313 (1992). https://doi.org/10.1007/BF02293050\\r\\n 3. Bélisle, C.J., Romeijn, H.E., Smith, R.L.: Hit-and-run algorithms for generating\\r\\n    multivariate distributions. Mathematics of Operations Research 18(2), 255–266\\r\\n    (1993)\\r\\n 4. Bertsimas, D., Vempala, S.: Solving convex programs by random walks. Journal of\\r\\n    the ACM (JACM) 51(4), 540–556 (2004)\\r\\n 5. Besag, J., Green, P., Higdon, D., Mengersen, K.: Bayesian computation and\\r\\n    stochastic systems. Statistical science pp. 3–41 (1995)\\r\\n 6. Brémaud, P.: Markov chains: Gibbs fields, Monte Carlo simulation, and queues,\\r\\n    vol. 31. Springer Science & Business Media (2013)\\r\\n 7. Brooks, S., Gelman, A., Jones, G., Meng, X.L.: Handbook of markov chain monte\\r\\n    carlo. CRC press (2011)\\r\\n 8. Cousins, B., Vempala, S.: A cubic algorithm for computing gaussian volume. In:\\r\\n    Proceedings of the twenty-fifth annual ACM-SIAM symposium on discrete algo-\\r\\n    rithms. pp. 1215–1228. SIAM (2014)\\r\\n 9. Feldman, J., Wainwright, M.J., Karger, D.R.: Using linear programming to decode\\r\\n    binary linear codes. IEEE Transactions on Information Theory 51(3), 954–972\\r\\n    (2005)\\r\\n\\x0c\\n\\n12      C. Karras et al.\\r\\n\\r\\n10. Gelfand, A.E.: Gibbs sampling. Journal of the American statistical Association\\r\\n    95(452), 1300–1304 (2000)\\r\\n11. Geman, S., Geman, D.: Stochastic relaxation, gibbs distributions, and the bayesian\\r\\n    restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelli-\\r\\n    gence PAMI-6(6), 721–741 (1984). https://doi.org/10.1109/TPAMI.1984.4767596\\r\\n12. Hastings, W.K.: Monte carlo sampling methods using markov chains and their\\r\\n    applications. Biometrika 57(1), 97–109 (1970)\\r\\n13. Huang, K.L., Mehrotra, S.: An empirical evaluation of walk-and-round heuristics\\r\\n    for mixed integer linear programs. Computational optimization and applications\\r\\n    55(3), 545–570 (2013)\\r\\n14. Jia, J., Fischer, G.W., Dyer, J.S.: Attribute weighting methods and decision quality\\r\\n    in the presence of response error: a simulation study. Journal of Behavioral Decision\\r\\n    Making 11(2), 85–105 (1998)\\r\\n15. Kannan, R., Narayanan, H.: Random walks on polytopes and an affine interior\\r\\n    point method for linear programming. Mathematics of Operations Research 37(1),\\r\\n    1–20 (2012)\\r\\n16. Kapfer, S.C., Krauth, W.: Sampling from a polytope and hard-disk monte carlo. In:\\r\\n    Journal of Physics: Conference Series. vol. 454, p. 012031. IOP Publishing (2013)\\r\\n17. Karras, C., Karras, A., Sioutas, S.: Pattern Recognition and Event\\r\\n    Detection on IoT Data-streams. arXiv preprint arXiv:2203.01114 (2022).\\r\\n    https://doi.org/10.48550/arXiv.2203.01114\\r\\n18. Lawrence, J.: Polytope volume computation. Mathematics of computation 57(195),\\r\\n    259–271 (1991)\\r\\n19. Li, T., Zhu, S., Ogihara, M.: Using discriminant analysis for multi-class classifi-\\r\\n    cation: an experimental investigation. Knowledge and information systems 10(4),\\r\\n    453–472 (2006)\\r\\n20. Lovász, L.: Hit-and-run mixes fast. Mathematical programming 86(3), 443–461\\r\\n    (1999)\\r\\n21. Lovász, L., Simonovits, M.: The mixing rate of markov chains, an isoperimetric in-\\r\\n    equality, and computing the volume. In: Proceedings [1990] 31st annual symposium\\r\\n    on foundations of computer science. pp. 346–354. IEEE (1990)\\r\\n22. Mackay, D.J.C.: Introduction to monte carlo methods. In: Learning in graphical\\r\\n    models, pp. 175–204. Springer (1998)\\r\\n23. Mete, H., Zabinsky, Z.: Pattern hit-and-run for sampling efficiently on polytopes.\\r\\n    Oper. Res. Lett. 40, 6–11 (01 2012). https://doi.org/10.1016/j.orl.2011.11.002\\r\\n24. Narayanan, H.: Randomized interior point methods for sampling and optimization.\\r\\n    The Annals of Applied Probability 26(1), 597–641 (2016)\\r\\n25. Revuz, D.: Markov chains. Elsevier (2008)\\r\\n26. Ripley, B.D.: Stochastic simulation. John Wiley & Sons (2009)\\r\\n27. Smith, R.L.: Efficient monte carlo procedures for generating points uniformly dis-\\r\\n    tributed over bounded regions. Operations Research 32(6), 1296–1308 (1984)\\r\\n\\x0c']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all the text into one string\n",
    "\n",
    "array_pdf_text=[]\n",
    "def pdf_to_text():\n",
    "    for i in array_link:\n",
    "        array_pdf_text.append(\"\\n\\n\".join(load_pdf(i[21:])))\n",
    "    return array_pdf_text\n",
    "pdf_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc82c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mihir.Prabhudesai', 'Anirudh.Goyal', 'Deepak.Pathak', 'Katerina.Fragkiadaki']\n",
      "                   Generating Fast and Slow:\r\n",
      "             Scene Decomposition via Reconstruction\r\n",
      "\r\n",
      "\r\n",
      "  Mihir Prabhudesai1      Anirudh Goyal2                  Deepak Pathak1         Katerina Fragkiadaki1\r\n",
      "               1                                           2\r\n",
      "                 Carnegie Mellon University                  Mila, University of Montreal\r\n",
      "           {mprabhud, dpathak, katef}@cs.cmu.edu                  anirudhgoyal9119@gmail.com\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Figure 1: Point-cloud and image decompositions with Generating Fast and Slow Networks (GFS-Nets).\r\n",
      "GFS-Nets parse completely novel scenes into familiar entities via slow inference, i.e., gradient descent on the\r\n",
      "reconstruction error of the scene example under consideration. Left: GFS-Nets outperform a state-of-the-art\r\n",
      "3D-DETR detector by 50% in segmentation accuracy in out-of-distribution 3D point clouds, when trained on\r\n",
      "the same training data. Right: GFS-Nets outperform state-of-the-art unsupervised generative models of Slot\r\n",
      "Attention [42] and uORF [67] in RGB image decomposition.\r\n",
      "\r\n",
      "                                                 Abstract\r\n",
      "         We consider the problem of segmenting scenes into constituent entities, i.e. under-\r\n",
      "         lying objects and their parts. Current supervised visual detectors though impressive\r\n",
      "         within their training distribution, often fail to segment out-of-distribution scenes\r\n",
      "         into their constituent entities. Recent slot-centric generative models break such\r\n",
      "         dependence on supervision, by attempting to segment scenes into entities unsuper-\r\n",
      "         vised, by reconstructing pixels. However, they have been restricted thus far to toy\r\n",
      "         scenes as they suffer from a reconstruction-segmentation trade-off: as the entity\r\n",
      "         bottleneck gets wider, reconstruction improves but then the segmentation collapses.\r\n",
      "         We propose GFS-Nets (Generating Fast and Slow Networks) that alleviate this issue\r\n",
      "         with two ingredients: i) curriculum training in the form of primitives, often missing\r\n",
      "         from current generative models and, ii) test-time adaptation per scene through gra-\r\n",
      "         dient descent on the reconstruction objective, what we call slow inference, missing\r\n",
      "         from current feed-forward detectors. We show the proposed curriculum suffices\r\n",
      "         to break the reconstruction-segmentation trade-off, and slow inference greatly\r\n",
      "         improves segmentation in out-of-distribution scenes. We evaluate GFS-Nets in 3D\r\n",
      "         and 2D scene segmentation benchmarks of PartNet, CLEVR, Room Diverse++,\r\n",
      "         and show large (∼ 50%) performance improvements against SOTA supervised\r\n",
      "         feed-forward detectors and unsupervised object discovery methods.\r\n",
      "\r\n",
      "Preprint. Under review.\r\n",
      "\f",
      "\n",
      "\n",
      "1    Introduction\r\n",
      "\r\n",
      "Scenes are composed of objects and objects are composed of parts, and this compositional organization\r\n",
      "of the perceptual representations is considered a critical component for the level of combinatorial\r\n",
      "generalization that humans are capable of, that extends far beyond their direct experiences [25].\r\n",
      "Even when encountered with a truly novel image, humans try to parse it in terms of known entity\r\n",
      "components [26]. Such an entity-centric understanding or reasoning of scenes allows humans to\r\n",
      "generalize known concepts to out-of-distribution examples [25, 39, 44].\r\n",
      "An expensive way to build this entity-centric understanding is to supervise the part parsing using\r\n",
      "human labels for training object and parts detection or segmentation in images and 3D point clouds [56,\r\n",
      "69, 5, 71]. However, these part detectors are category specific and often fail outside the training\r\n",
      "distribution [29, 19, 2]. Consider the abstract shape shown in Figure 1 (last row on the left). We can\r\n",
      "intuitively figure out the meaningful entities that this shape could be broken into. Yet, a state-of-the-art\r\n",
      "transformer based 3D-DETR detector [5], when trained supervised for segmenting chairs, fails to\r\n",
      "decompose these abstract shapes into entities, even though these shapes contain familiar (chair) parts.\r\n",
      "An underlying issue is that these models do not receive any top-down feedback [3] from entities, i.e.,\r\n",
      "they do not have any entity-centric reasoning rather, they are purely feed-forward in nature, i.e., the\r\n",
      "information is routed from input to output layers in a sequential manner. How do we enable top-down\r\n",
      "feedback between the scene and its constituent parts?\r\n",
      "There has been interest recently in building models that segment scenes into entities in an unsupervised\r\n",
      "way by optimizing a reconstruction objective [42, 17, 24, 63, 57, 35, 16, 4, 23, 68, 55, 13, 22, 73, 34].\r\n",
      "These methods differ in details but share the notion of having a fixed set of entities, also known as\r\n",
      "slots or object files. Each slot extracts information about a single entity during encoding, and it is\r\n",
      "“rendered\" back to the input during decoding, thus, exhibiting top down feedback. Yet, it has been\r\n",
      "hard to scale the results of these models beyond toy datasets.\r\n",
      "We argue this is because these models answer two questions at once: understanding what the entity is\r\n",
      "and where it is present in the input. This creates a chicken-and-egg problem because unless we have\r\n",
      "a representation for an entity, we can’t find where it is in the scene, and unless we localize the entity,\r\n",
      "we can’t learn a representation of what it is. Hence, a trivial solution often encountered is to assign\r\n",
      "one slot to the input and other slots to nothing, as noted in Engelcke et al. [15] and also confirmed by\r\n",
      "our experiments in Section 4.1.1 and 4.2.\r\n",
      "We propose Generating Fast and Slow Networks (GFS-Nets), a slot-centric autoencoder that segments\r\n",
      "scenes while optimizing a reconstruction objective. Our model builds on top of Slot Attention[42],\r\n",
      "with two key insights: First, it uses curriculum to learn the what question (what entities are) before\r\n",
      "going to the where question (where they are) it does this by training the model to autoencode primitive\r\n",
      "entities using a single slot bottleneck. This collection of primitive entities can either be collected\r\n",
      "by design [60] or can be taken from a collection of part segmentations [49]. Second, inference in\r\n",
      "GFS-Nets is performed in two ways: i) Fast inference where a visual scene is simply encoded and\r\n",
      "decoded and no model weights are updated. ii) Slow inference where the weights of the model are\r\n",
      "adapted by gradient descent while optimizing the reconstruction objective. In this way, GFS-Nets\r\n",
      "adapts to a truly novel scene by projecting it onto a learned distribution of entities, while also\r\n",
      "slowly adapting the entity distribution. Slow inference in GFS-Nets successfully parses completely\r\n",
      "unfamiliar scenes into familiar entities, implementing a form of search for explanations of the scene\r\n",
      "at inference, as we show in Figure 3.\r\n",
      "Our slow inference is related to test-time adaptation or optimization [72, 54, 59, 58, 74, 65]. However,\r\n",
      "test-time adaptation has not been used so far in entity-centric models to improve scene decomposition.\r\n",
      "In fact, it has been recently noted that reconstruction loss can make entity segmentation worse, to\r\n",
      "quote Engelcke et al. [15], “If the bottleneck is too narrow, segmentation and reconstruction degrade.\r\n",
      "If it is too wide, reconstruction is reasonable but the model collapses to a single component and no\r\n",
      "useful segmentation is learned.\" GFS-Nets alleviate from this issue by coupling curriculum training\r\n",
      "with slow inference for entity-centric learning.\r\n",
      "\r\n",
      "    Project page: https://mihirp1998.github.io/project_pages/gfsnets/\r\n",
      "\r\n",
      "\r\n",
      "                                                     2\r\n",
      "\f",
      "\n",
      "\n",
      "We test GFS-Nets on the following datasets: PartNet [49], ShapeNet[7], CLEVR [30] and a difficult\r\n",
      "version of Room Diverse [67]. We evaluate GFS-Nets’s ability to parse out-of-distribution scenes\r\n",
      "and compare it against state of the art entity-centric generative models [42, 67], program synthesis\r\n",
      "models [60], 3D unsupervised part discovery models [21, 64] and state of the art supervised visual\r\n",
      "detectors [5, 43] trained with labeled data to segment entities. We show improvements across all\r\n",
      "baselines in our ability to segment novel scenes. Additionally, we ablate different design choices of\r\n",
      "GFS-Nets. We will make our code and datasets publicly available to the community.\r\n",
      "\r\n",
      "2   Related Work\r\n",
      "Entity-centric generative models for scene and structure decomposition Entity-centric mod-\r\n",
      "els attempt to segment a scene into objects and parts while autoencoding images. MONet[4],\r\n",
      "GENESIS[16] and IODINE[23] perform multiple steps between their encoder and the decoder to\r\n",
      "output a set of independent latent vectors that describe objects in the image. This iterative encode-\r\n",
      "decode inference helps them in receiving top-down feedback. However, this also results in making\r\n",
      "the system computationally inefficient and inflexible. Slot Attention [42] on the other hand receives\r\n",
      "top-down feedback within a single encoding step. The idea of having a single-encode step makes\r\n",
      "the process of extracting out symbols modular and computationally efficient. However, all methods\r\n",
      "above fail on non-toy scenes. We attribute this failure to the what or where problem, a.k.a. symmetry\r\n",
      "problem, pointed out in the ‘Instance Slots’ Section in Greff et al.[25]: “bottom-up information is\r\n",
      "unable to break the symmetry amongst slots and thus ends up assigning the same content to each\r\n",
      "one.\" In GFS-Nets, we propose to circumvent this problem using curriculum training.\r\n",
      "\r\n",
      "Shape program synthesis and analysis-by-synthesis GFS-Nets is also related to works in\r\n",
      "analysis-by-synthesis [37], program synthesis for shape prediction [60, 14, 41], as well as ear-\r\n",
      "lier works on Computer Vision, such as Marr’s 3D sketch [45] which involves representing a scene\r\n",
      "in terms of generalized cylinders and their syntactic relations to each other. In place of data-driven\r\n",
      "Markov Chain Monte Carlo search of analysis-by-synthesis methods that require good initialization,\r\n",
      "our slow inference searches in the space of primitives by gradient descent. In contrast to program\r\n",
      "synthesis methods, it does not require a predefined domain-specific language (DSL) or program\r\n",
      "annotations for visual structures [41], rather, it discovers compositions over primitives via its slow\r\n",
      "inference.\r\n",
      "\r\n",
      "Unsupervised 3D Part Discovery There are numerous methods that attempt the decomposition of\r\n",
      "complex 3D shapes into primitive parts without primitive supervision[33, 21, 51, 18, 12, 62, 11, 9].\r\n",
      "Traditional primitives include cuboids [62, 50], superquadrics [53, 51], and convexes [11, 8]. [20]\r\n",
      "proposes a 3D representation that decomposes space into a structured set of implicit functions [21].\r\n",
      "Neural Parts [52] represents arbitrarily complex genus-zero shapes and thus yields comparatively\r\n",
      "expressive parts. However the resulting parts of these methods[20, 52] are still not semantically\r\n",
      "meaningful and the decomposition is highly dependent on the number of parts initialized. The work\r\n",
      "of [66] does 3D part reconstruction directly from a 2D image input without access to any ground-truth\r\n",
      "3D shapes for training. However, both [66] and [52] take as input the number of parts, and different\r\n",
      "decompositions are predicted with varying part numbers. There is no clear way to select the right\r\n",
      "number of parts. In our case, parts can be quite complex: pairs of parallel surfaces, quadruplets of\r\n",
      "legs, as we use implicit functions to represent them. Moreover GFS-Nets’s dynamic attention-based\r\n",
      "routing allows it to infer different number of parts for each input scene.\r\n",
      "\r\n",
      "3   Generating Fast and Slow (GFS-Nets)\r\n",
      "The goal of GFS-Nets is to decompose an unfamiliar scene into familiar entities. The model encodes\r\n",
      "the scene, which can be a 3D point cloud or an RGB image, into a set of slot vectors and decodes\r\n",
      "back the scene using a decoder shared across slots. The model uses the slot attention feature-to-slot\r\n",
      "mapping of Locatello et al.[42], where visual features are softly partitioned across slots through\r\n",
      "attention, and update the slot vectors.\r\n",
      "GFS-Nets is trained with a curriculum. First, it is first trained to autoencode individual entities using\r\n",
      "a single slot in their bottleneck, as shown in the left of Figure 2 and described in Section 3.2. Next,\r\n",
      "GFS-Nets is trained to autoencode complex scenes using multiple slots in their bottleneck, as shown\r\n",
      "\r\n",
      "\r\n",
      "                                                   3\r\n",
      "\f",
      "\n",
      "\n",
      "Stage 1: Autoencoding primitive entities                                                                        Stage 2: Autoencoding scenes\r\n",
      "\r\n",
      "3D Object Decomposition                                                                                                                                       Competition\r\n",
      "                                                                                                                                                             Amongst Slots\r\n",
      "                                                                                                                                                                         sampling N slots\r\n",
      "                                                       sampling 1 slot\r\n",
      "                    Point                                                                                                             Point                                                   slot\r\n",
      "                                                                                                                                                                                               slot\r\n",
      "                                                                           slot                                                                                                                 slot         Max-Pool\r\n",
      "                 Transformer                                                                                                       Transformer                                              decoder\r\n",
      "                                                                                                                                                                                                  slot\r\n",
      "                                                                                                                                                                                             decoder\r\n",
      "                                                                         decoder                                                                                                              decoder\r\n",
      "                                                                                                                                                                                                    slot\r\n",
      "                   Encoder                                                                                                           Encoder                                                   decoder\r\n",
      "                                                                                                                                                                                                 decoder\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                      reconstruction loss                                                                                                          reconstruction loss\r\n",
      "\r\n",
      " 2D Scene Decomposition\r\n",
      "                                Competition                                                                                                             Competition\r\n",
      "                               Amongst Slots Foreground                                                                                                Amongst Slots Foreground\r\n",
      "                                         sampling 1 slot                                                                                                            sampling N slots\r\n",
      "                                                             slot                                                                                                                        slot\r\n",
      "                                                                                                                                                                                          slot             Weighted\r\n",
      "                                                               slot                                                                                                                        slot\r\n",
      "                                                           decoder                 Weighted                                                                                            decoder\r\n",
      "                                                                                                                                                                                            slot\r\n",
      "                                                                                                                                                                                       decoder             Average\r\n",
      "           CNN                                              decoder                                                                    CNN                                              decoder\r\n",
      "                                                                                                                                                                                              slot\r\n",
      "                                                                                   Average                                                                                               decoder\r\n",
      "                                                                                                                                                                                           decoder\r\n",
      "\r\n",
      "                                        sampling 1 slots                                                                                                        sampling 1 slots\r\n",
      "                                                     Background                                                                                                                 Background\r\n",
      "\r\n",
      "                                      reconstruction loss                                                                                                             reconstruction loss\r\n",
      "\r\n",
      "\r\n",
      "Figure 2: Stages of training in GFS-Nets. Left: Autoencoding primitive entities. Primitive entities are\r\n",
      "point clouds of individual parts in 3D, and single object RGB images in 2D. GFS-Nets use one and two slots\r\n",
      "respectively, for primitive entity autoencoding in 3D point clouds and RGB images. Right: Autoencoding scenes.\r\n",
      "Scenes can be 3D points clouds of whole object or multi-object RGB images. In this stage, GFS-Nets use more\r\n",
      "slots in their bottleneck. During inference, some slots get mapped to nothing in the input scene. In this way,\r\n",
      "GFS-Nets is able to infer decompositions with a varying number of entities, in an unsupervised manner.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "on the right of Figure 2 and described in Section 3.3. We first describe slot attention of [42] in Section\r\n",
      "3.1, as well as encoders and decoders for 3D point clouds [70, 40] and RGB images [47, 67] which\r\n",
      "our method builds upon.\r\n",
      "\r\n",
      "3.1     Background\r\n",
      "\r\n",
      "3.1.1     Routing from input to slots with iterative attention\r\n",
      "\r\n",
      "Many approaches instantiate slots (a.k.a. query vectors) from 2D visual feature maps or 3D point\r\n",
      "feature clouds [5, 48]. Most works use standard cross attention operations [61, 22, 42] or iterative\r\n",
      "cross (features to slots) and self-attention (slot-to-slots) operations [5] to map a set of N input feature\r\n",
      "vectors to a set of K slot vectors. Competition amongst slots and iterative routing introduced in\r\n",
      "[22, 42] encourages the slots not to encode information in a redundant manner.\r\n",
      "Given a visual scene encoded as a set of feature vectors M ∈ RN ×C and K randomly initialized slots\r\n",
      "sampled from a multivariate Gaussian distribution with a diagonal covariance s ∼ N (µ, Diag(σ 2 )) ∈\r\n",
      "RK×D , where µ, σ ∈ RC are learnable parameters of the Gaussian, Slot Attention[42] computes an\r\n",
      "attention map a between the feature map M and the slots s :\r\n",
      "\r\n",
      "                                                         a = \\mathrm {Softmax}(\\key (\\MM ) \\cdot \\query (s)^T, \\textrm {axis=1}) \\in \\mathbb {R}^{N \\times K}. \\label {eq:1}                                            (1)\r\n",
      "\r\n",
      "k, q, and v are learnable linear transformations that map inputs and slots to a common dimension\r\n",
      "D. The softmax normalization over slots ensures competition amongst them to attend to a specific\r\n",
      "feature vector in M. We then generate the update vector for each slot given the computed attention\r\n",
      "and input feature maps:\r\n",
      "\r\n",
      "                                                    updates = a^T v(M) \\in \\mathbb {R}^{K \\times C}, \\textrm {where} \\ a_{i,1} = \\dfrac {a_{i,1}}{\\sum _{i=0}^{N}a_{i,1}}                                               (2)\r\n",
      "\r\n",
      "\r\n",
      "which we then use to do a gated update on each slot feature using a GRU[10]: slots = GRU(state =\r\n",
      "slots, input = updates). We iterate over the steps of calculating attention and updating the slots for\r\n",
      "3 timesteps.\r\n",
      "Since the slot vectors are sampled from a shared Gaussian distribution, slot attention[42] can in-\r\n",
      "stantiate a variable number of slots in different scenes, in contrast to DETR encoder [5] where a\r\n",
      "constant number of (deterministic) slot vectors are learned and used in each scene. For a more\r\n",
      "detailed description, we refer the reader to [42].\r\n",
      "Slot attention is one of many ways of mapping inputs to slots. GFS-Nets is agnostic to the method of\r\n",
      "mapping visual features to slot vectors and we ablate different encoders in our experiment section\r\n",
      "4.1.2.\r\n",
      "\r\n",
      "\r\n",
      "                                                                                                                4\r\n",
      "\f",
      "\n",
      "\n",
      "3.1.2    Encoding and decoding 3D point clouds\r\n",
      "\r\n",
      "GFS-Nets featurize a 3D point cloud using a 3D point transformer [70] which maps the 3D input\r\n",
      "points to a set of M feature vectors of C dimensions each. We set M to 128 and C to 64 in our\r\n",
      "experiments.\r\n",
      "GFS-Nets decodes 3D point clouds from each slot using implicit functions [46]. Specifically, each\r\n",
      "decoder takes in as input the slot vector si and an (X, Y, Z) location and returns the corresponding\r\n",
      "occupancy score ox,y,z = Dec(si , (x, y, z)) , where Dec is a multi-block ResNet MLP similar to that\r\n",
      "of Lal et al. [40].\r\n",
      "\r\n",
      "3.1.3    Encoding and decoding RGB images\r\n",
      "\r\n",
      "GFS-Nets featurize an RGB image concatenated with its pixel coordinates using a convolutional\r\n",
      "neural network (CNN) and produce a feature map of H × W × C where H, W are the spatial\r\n",
      "dimensions and C is the feature dimension. We reshape the output feature map to a set of vectors\r\n",
      "M ∈ R(H×W )×C . We set C as 64.\r\n",
      "GFS-Nets decodes RGB pixels from each slot using conditional NeRFs [47] that predict the color\r\n",
      "c and volume density σ at each 3D location of the latent 3D volume. Thus given slot vector\r\n",
      "si , spatial location x and viewing direction d, the MLP based decoder network Dec predicts\r\n",
      "(c, σ) = Dec(si , (x, d)). The color c and density σ of the points are composed together across\r\n",
      "                                                               PK             PK\r\n",
      "slots using density σ weighted averaging. Specifically, σ̂ = i=0 wi σi , ĉ = i=0 wi ci where\r\n",
      "            PK\r\n",
      "wi = σi / i=0 σi . We then use differentiable volume rendering of the composed σ̂ and ĉ using\r\n",
      "camera ray casting to generate the final RGB prediction similar to [67].\r\n",
      "\r\n",
      "\r\n",
      "3.2     Autoencoding primitive entities\r\n",
      "\r\n",
      "In this stage, we train GFS-Nets using a set of primitive entities. Primitive entities are generic or\r\n",
      "semantic parts of objects in 3D point clouds or single object multi-view images, as shown on the left\r\n",
      "of Figure 2. We experiment with different primitive sets in the experimental section.\r\n",
      "For encoding 3D point cloud primitive entities, we sample a single slot in the entity bottleneck of\r\n",
      "GFS-Nets, that is, s ∈ R1×D . For encoding a 2D RGB image, we sample two slots, one from a\r\n",
      "foreground learnable Gaussian distribution and one from a background learnable Gaussian distribution,\r\n",
      "that is s ∈ R2×D . In a forward pass of the encoder, we then update slots s as described in Section 3.1.\r\n",
      "\r\n",
      "\r\n",
      "3.3     Autoencoding scenes\r\n",
      "\r\n",
      "In this stage, we initialize the parameters of the model\r\n",
      "with the parameters learnt in the previous section. The\r\n",
      "only difference is that instead of instantiating one or two\r\n",
      "slots we now instantiate K slot vectors, where\r\n",
      "K is the maximum number of entities any example in the\r\n",
      "dataset could possibly have. For RGB input, we sample K\r\n",
      "slots from the foreground Gaussian and one slot from the\r\n",
      "background Gaussian since there can be many foreground\r\n",
      "                                                                Input & Target   Slow Inference\r\n",
      "objects but only a single background.\r\n",
      "Finally we pass each of our slot vectors through the slot de-   Figure 3: Slow Inference in GFS-Nets:\r\n",
      "coder outk = Dec(sk ), whose weights are shared across          First columns shows the input point cloud.\r\n",
      "slots. For decoding RGB images, GFS-Nets uses separate          In 2nd to 4th columns, we show the segmen-\r\n",
      "decoders for foreground and background. For 3D point            tation results throughout training iterations\r\n",
      "cloud input, outk represents the probability of sampled         of the autoencoding process. Segmentation\r\n",
      "                                                                improves over time with reconstruction qual-\r\n",
      "points being occupied by a given slot sk ; we aggregate         ity, despite that the autoencoding loss only\r\n",
      "them by max pooling across the slot dimension. We then          penalizes the reconstruction error.\r\n",
      "use binary cross entropy loss between the ground truth\r\n",
      "occupancies and the predictions.\r\n",
      "\r\n",
      "\r\n",
      "                                                    5\r\n",
      "\f",
      "\n",
      "\n",
      "For RGB input, outk represents the color and density of\r\n",
      "points being occupied by a given slot sk ; we aggregate across slots as described in Section 3.1 and a\r\n",
      "use pixel mean squared error loss.\r\n",
      "Fast and Slow Inference Fast inference refers to a single forward pass through the trained net-\r\n",
      "work. Slow inference refers to test-time adaptation of all the learnable parameters of the model for\r\n",
      "autoencoding a single scene using the method described in Section 3.3. Our experiment show that\r\n",
      "GFS-Nets with slow inference always achieves much better scene decomposition than GFS-Nets with\r\n",
      "fast inference, especially in out-of-distribution scenes. In contrast, slow inference on entity-centric\r\n",
      "generative models without curricula, such as [42], results in improved reconstruction accuracy but\r\n",
      "worse scene decomposition performance, due to the reconstruction-segmentation trade off noted in\r\n",
      "[15].\r\n",
      "\r\n",
      "4     Experiments\r\n",
      "We test GFS-Nets in the tasks of segmenting object parts in 3D point clouds and segmenting objects\r\n",
      "and predicting views in 2D multiview images under varying amounts of label supervision. We compare\r\n",
      "GFS-Nets against previous state-of-the-art models in each task. We further quantify contribution of\r\n",
      "various design choices, namely, slow inference through reconstruction feedback, curriculum learning,\r\n",
      "and encoder architecture. We evaluate segmentation performance on scenes within the training\r\n",
      "distribution as well as out-of-distribution scenes.\r\n",
      "In each setup, our experiments aim to answer the following questions:\r\n",
      "\r\n",
      "         • How does GFS-Nets compare against state-of-the-art 2D and 3D scene decomposition\r\n",
      "           models ([43, 67, 5]) under varying amounts of label supervision?\r\n",
      "         • How much, if any, slow inference through reconstruction feedback improves segmentation\r\n",
      "           accuracy in GFS-Nets and its variants?\r\n",
      "         • How much curriculum or supervision during training contributes to segmentation perfor-\r\n",
      "           mance?\r\n",
      "\r\n",
      "4.1     Segmenting parts in 3D object point clouds\r\n",
      "\r\n",
      "In this task, the input is a complete 3D point cloud of an object, and the goal is to segment the 3D\r\n",
      "point cloud into different semantic part instances. We consider two setups with different segmentation\r\n",
      "supervision:\r\n",
      "\r\n",
      "        1. Segmenting without any part segmentation labels.\r\n",
      "        2. Segmenting with part segmentation labels from a related object category.\r\n",
      "\r\n",
      "4.1.1    Segmenting 3D pointclouds without segmentation supervision\r\n",
      "In this setup, we test our model and baselines in segmenting the test objects of the Chair and Table\r\n",
      "categories in the PartNet benchmark [49] without access to any ground-truth 3D segmentations at\r\n",
      "training time. For training, we provide our model and baselines access to unsegmented objects of the\r\n",
      "Chair category in the trainset of the PartNet benchmark. We evaluate the segmentation performance\r\n",
      "of our model and baselines with the Adjusted Random Index (ARI) [28] using the implementation of\r\n",
      "Kabra et al. [32], which calculates the similarity between two-point clusters while being invariant to\r\n",
      "the ordering of the cluster centers. Neither our model nor the baselines have access to ground-truth\r\n",
      "3D segmentations during training; as a result, they may output 3D parts of coarser or finer resolution.\r\n",
      "PartNet contains three different levels of ground-truth segmentation labels with progressively finer\r\n",
      "segmentation granularity. We evaluate the ARI score across all three levels of PartNet and report the\r\n",
      "best of three for all the models.\r\n",
      "Our model and two amongst three of our baselines, specifically (PQ-Nets[64] of Wu et al. and\r\n",
      "Shape2Prog [60] of Tian et al.) expect access to a set of 3D primitive parts during training. In this\r\n",
      "setup, we consider the primitive part dataset introduced by Shape2Prog [60]. The dataset consists of\r\n",
      "differently sized cubes, cuboids, and discs; we call it the generic primitive set since it is not related\r\n",
      "to any semantic object category but rather consists of generic 3D parts that remind of generalized\r\n",
      "\r\n",
      "\r\n",
      "                                                   6\r\n",
      "\f",
      "\n",
      "\n",
      "cylinders of Marr et al.[45]. Please refer to the supplementary file for visualizations of the generic\r\n",
      "primitive set.\r\n",
      "Baselines: We compare our model against the following baselines: (i) PQ-Nets of Wu et al.[64]\r\n",
      "assume access to a set of primitive 3D parts, similar to our model, and learn a primitive part\r\n",
      "autoencoder. Whole object autoencoding uses a sequential model that encodes the 3D point cloud\r\n",
      "into a 1D latent vector and sequentially decodes parts using the part decoder. We use the publicly\r\n",
      "available code to train the model to autoencode the generic primitive set and the unlabelled 3D point\r\n",
      "clouds of the instances of the Chair category in PartNet. (ii) Shape2Prog of Tian et al.[60] is a shape\r\n",
      "program synthesis method that is trained supervised to predict shape programs from object 3D point\r\n",
      "clouds. The program represents the part category, location, and the symmetry relations among the\r\n",
      "parts (if any). Shape2Prog introduced two synthetically generated datasets that helped the model\r\n",
      "parse 3D pointclouds from ShapeNet [6] into shape programs without any supervision: i) the generic\r\n",
      "primitive set we discussed earlier which they use to train their part decoders, and ii) a synthetic whole\r\n",
      "shape dataset of chairs and tables generated programmatically alongside its respective ground-truth\r\n",
      "programs. Their model requires supervised pre-training on the dataset of synthetic whole shapes\r\n",
      "paired with programs. We therefore use their publicly available model weights trained on synthetic\r\n",
      "whole shapes to further train on PartNet Chairs. Note that no other baseline nor GFS-Nets assumes\r\n",
      "access to the synthetic whole shape dataset. (iii) StructImplicit of Genova et al.[21] encodes the\r\n",
      "object point cloud into a 1D latent and then uses an MLP to map it to a fixed number of part vectors,\r\n",
      "each represented with a 3D implicit function. We use the publicly available code to train the model\r\n",
      "on the unlabelled Chair PartNet dataset. We set the number of part vectors based on the maximum\r\n",
      "number of parts any example in the dataset can have.\r\n",
      "\r\n",
      "                                                   in-dist (Chairs)                  out-of-dist (Tables)\r\n",
      "        Method\r\n",
      "                                             Fast Infer.         Slow Infer.      Fast Infer.       Slow Infer.\r\n",
      "        Shape2Prog [60]                         0.28                0.53             0.23                 0.40\r\n",
      "        PQ-Nets [64]                            0.20                0.31             0.17                 0.21\r\n",
      "        StructImplicit [21]                     0.25                0.23             0.17                 0.17\r\n",
      "        GFS-Nets w/o curriculum                 0.41                0.35             0.47                 0.38\r\n",
      "        GFS-Nets-PrimitiveOnly                  0.42                0.48             0.49                 0.57\r\n",
      "        GFS-Nets                                0.57                0.62             0.60                 0.69\r\n",
      "\r\n",
      "Table 1: ARI Segmentation accuracy (higher the better) in the test set of Chair (in-distribution) and Table\r\n",
      "category of PartNet(out-of-distribution). GFS-Nets significantly outperform all of the baselines.\r\n",
      "\r\n",
      "We consider the following ablative versions of GFS-Nets: (i) GFS-Nets w/o curriculum is trained\r\n",
      "without the primitive learning stage. Instead, it is trained to directly autoencode the aggregated dataset\r\n",
      "of both the generic primitive set and 3D point clouds of training chairs (as described in Section 3.3).\r\n",
      "This version of our model coincides precisely with the previous work of Slot Attention of Locatello\r\n",
      "et al. [42]. (ii) GFS-Nets-PrimitiveOnly is trained only on the generic primitive set (as described in\r\n",
      "Section 3.2) and is not trained to autoencode and 3D point clouds of training chairs.\r\n",
      "                                                                                      Input     3D-DETR   Learning to Group   Ours\r\n",
      "\r\n",
      "Any model that has a decoder that reconstructs the input\r\n",
      "can be additionally trained with slow inference with re-\r\n",
      "construction feedback at each test scene independently.\r\n",
      "GFS-Nets, PQ-Nets [64], Shape2Prog [60], and StructIm-\r\n",
      "plicit [21] are all equipped with such decoders. We thus   Input  Shape2Prog [62] PQ-Nets [66] Structured Implicit [22] Ours\r\n",
      "\r\n",
      "test both our model and the baselines in both modes of\r\n",
      "fast (regular) and slow inference; we use the notation Figure 4: Unsupervised 3D segmentations of\r\n",
      "[modelname]-Slow and [modelname]-Fast to denote the out-of-distribution scenes for GFS-Nets and\r\n",
      "                                                         baselines.\r\n",
      "model with the corresponding inference type.\r\n",
      "We show quantitative and qualitative results of our model\r\n",
      "and the baselines in Table 4 and Figure 4 respectively. From the Table 4 we draw the following\r\n",
      "conclusions:\r\n",
      "Segregation into slot like entities using attention helps. GFS-Nets significantly outperform PQ-\r\n",
      "Nets [64] and StructImplicit [21] that map the input object 3D pointcloud into a 1D latent vec-\r\n",
      "\r\n",
      "\r\n",
      "                                                             7\r\n",
      "\f",
      "\n",
      "\n",
      "tor instead of maintaining segregated representations into multiple slot latent vectors. Moreover,\r\n",
      "GFS-Nets’s ability to dynamically route information per example via attention allows it to automati-\r\n",
      "cally estimate the number of parts required per input 3D pointcloud due to which, different instances\r\n",
      "can have a different number of active slots, while StructImplicit segregates all object instances into a\r\n",
      "constant number of parts, which is a hyperparameter of the model.\r\n",
      "Curriculum training helps. GFS-Nets-Fast outperform by a large margin GFS-Nets w/o curriculum-\r\n",
      "Fast, the version of GFS-Nets that uses exactly the same training data and inference method, but is\r\n",
      "trained without curriculum.\r\n",
      "Slow inference through reconstruction feedback helps in the presence of curriculum and hurts\r\n",
      "in the absence of it. GFS-Nets, GFS-Nets-PrimitiveOnly, PQ-Nets and Shape2Prog all have cur-\r\n",
      "riculum through a primitive learning stage, while GFS-Nets w/o curriculum and StructImplicit\r\n",
      "do not. GFS-Nets-Slow outperform GFS-Nets-Fast and GFS-Nets-PrimitiveOnly-Slow outperform\r\n",
      "GFS-Nets-PrimitiveOnly-Fast, PQ-Nets-Slow outperform PQ-Nets-Fast and ShapeProg-Slow outper-\r\n",
      "form ShapeProg-Fast. In contrast, GFS-Nets w/o curriculum-Slow performs worse than GFS-Nets\r\n",
      "w/o curriculum-Fast and StructImplicit-Slow does worse than StructImplicit-Fast. Such trade-off\r\n",
      "between reconstruction and segmentation in generative model for scene decomposition has been\r\n",
      "pointed out by Engelcke et al. [15] and our results verify their findings.\r\n",
      "Unsupervised autoencoding of complex (non primitive) 3D pointclouds helps. GFS-Nets outper-\r\n",
      "forms GFS-Nets-PrimitiveOnly.\r\n",
      "Please see the supplementary file for more experimental details. We visualize intermediate iterations\r\n",
      "during slow inference in our supplementary video.\r\n",
      "\r\n",
      "4.1.2    Segmenting 3D pointclouds with weak segmentation supervision\r\n",
      "In this setup, we test our model and baselines in segmenting the test objects of the Chair and Table\r\n",
      "categories in the PartNet benchmark [49], with access to ground-truth pointcloud segmentation\r\n",
      "labels of level-3 in the training 3D pointclouds of the Chair category in PartNet. We use the ARI\r\n",
      "segmentation metric to evaluate the level 3 ground-truth labels of PartNet for our model and baselines.\r\n",
      "GFS-Nets assume access to a set of 3D primitive parts for primitive entity learning. In this setup, we\r\n",
      "consider each 3D part from each training chair 3D pointcloud example as a separate 3D primitive. We\r\n",
      "augment these primitives by random rotations and translations; we call it the semantic primitive set.\r\n",
      "\r\n",
      "                                                            in-dist (Chair)                   out-dist (Table)\r\n",
      "        Method\r\n",
      "                                                 Fast Infer.            Slow Infer.       Fast Infer.   Slow Infer.\r\n",
      "        3D-DETR [5]                                  0.67                   n/a              0.41           n/a\r\n",
      "        Learning2Group [43]                          0.62                   n/a              0.46           n/a\r\n",
      "        GFS-Nets w/o supervision                     0.40                  0.44              0.31          0.48\r\n",
      "        GFS-Nets w/o curriculum                      0.61                  0.66              0.48          0.60\r\n",
      "        GFS-Nets w/o Gaussian                        0.65                  0.62              0.38          0.58\r\n",
      "        GFS-Nets w/o SlotAttention                   0.58                  0.55              0.31          0.44\r\n",
      "        GFS-Nets                                     0.64                  0.67              0.51          0.63\r\n",
      "\r\n",
      "Table 2: ARI Segmentation scores (higher the better) on the test set of the Chair category (in-distribution)\r\n",
      "and on the test set of the Table category (out-of-distribution). All the models are trained using the training set of\r\n",
      "the Chair category in PartNet.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "               Input    3D-DETR [5]   Learning2Group [45]    Ours       Cupboard   Lamp      Display    Laptop\r\n",
      "\r\n",
      "\r\n",
      "Figure 5: Out-of-distribution 3D segmentation results for GFS-Nets and baselines. Left: 3D segmentation\r\n",
      "results on out-of-distribution shapes for GFS-Nets and baselines. Right: 3D segmentation results for out-of-\r\n",
      "distribution categories of PartNet for GFS-Nets trained semi-supervised only on the Chair category.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                                    8\r\n",
      "\f",
      "\n",
      "\n",
      "Baselines We compare our model against the following baselines: (i) Learning2Group of Luo\r\n",
      "et al. [43] progressively groups points into segments by learning pairwise grouping decisions pa-\r\n",
      "rameterized by features of the two point clusters. Given the intermediate grouping decisions are not\r\n",
      "supervised, and the non-differentiability of their grouping functions they use reinforcement learning\r\n",
      "gradients for training using supervision from the final segmentation. We used the publicly available\r\n",
      "code and trained the model in the training set of the Chair category. (ii) 3D-DETR is the equivalent of\r\n",
      "2D-DETR of Carion et al. [5] for 3D point-cloud segmentation. A point transformer [70] encodes the\r\n",
      "input 3D point cloud into a set of point features which are mapped to a learnable set of 16 parametric\r\n",
      "queries using 6 layers of transformer encoder (self-attention) and decoder (cross-attention) blocks.\r\n",
      "Then, each query decodes a 3D point segmentation map by computing multi-head attention with the\r\n",
      "encoded point features. The attention for each query is then upsampled using a point transformer\r\n",
      "decoder. The predicted 3D part segments are then matched against ground-truth segments using\r\n",
      "Hungarian matching [36], and error is computed for each pair of matched ground-truth part and\r\n",
      "paired prediction through binary cross-entropy loss.\r\n",
      "We train GFS-Nets in a semi-supervised way combining an (unsupervised) autoencoding loss and a\r\n",
      "supervised part segmentation loss. Specifically alongside the autoencoding loss of the training 3D\r\n",
      "point clouds, we apply a part segmentation loss via Hungarian matching between the decoded 3D\r\n",
      "part shapes from the slots and the ground-truth part 3D segments in the training set, similar to our\r\n",
      "3D-DETR baseline model. For this case we did not consider the models of PQ-Nets [64], Shape2Prog\r\n",
      "[60] or [21] as it is not clear on how to train them in a semi-supervised manner using segmentation\r\n",
      "supervision.\r\n",
      "We consider the following ablative versions of GFS-Nets: (i) GFS-Nets w/o supervision is trained\r\n",
      "without the supervised part segmentation loss, so the model only receives gradients from autoencoding\r\n",
      "the primitives and whole chairs. (ii) GFS-Nets w/o Gaussian does not initialize the slot vectors by\r\n",
      "sampling from a learnable Gaussian but instead uses separate learnable query vectors for each slot,\r\n",
      "similar to DETR. (iii) GFS-Nets w/o SlotAttention does not use Slot Attention for mapping point\r\n",
      "features to slots. Instead it maps 3D point features to slots via iterative layers of cross (query to point)\r\n",
      "and self (query-to-query) attention layers on learnable query vectors similar to 3D-DETR and DETR\r\n",
      "[5]. Please note that slots and queries represent the same thing, but we use the terminology of DETR\r\n",
      "[5] in this case.\r\n",
      "We report the fast and slow inference results for all ablative versions of our model. Our baselines in\r\n",
      "this case, 3D-DETR and Learning2Group [43] are feedforward in nature, they are not equipped with\r\n",
      "decoders, and thus cannot be evaluated with slow inference.\r\n",
      "We show quantitative results in Table 2 and qualitative results in Figure 5. GFS-Nets significantly\r\n",
      "outperform the baselines, and GFS-Nets-Slow results in a significant boost in performance (∼ 50%)\r\n",
      "over the feedforward inference in our model, GFS-Nets-Fast. We draw the following conclusions\r\n",
      "from Table 2:\r\n",
      "Supervision can be used as a replacement to curriculum training. GFS-Nets only slightly out-\r\n",
      "performs GFS-Nets w/o curriculum in this setup.\r\n",
      "Generic primitives generalize a lot better than category-specific primitives or supervision.\r\n",
      "GFS-Nets in Table 4 (that uses the generic primitive set) outperforms GFS-Nets and GFS-Nets-\r\n",
      "w/o supervision in Table 2 (that use the semantic primitive set) in OOD generalization on the Table\r\n",
      "category.\r\n",
      "Competition amongst slots helps. GFS-Nets outperforms GFS-Nets w/o SlotAttention, thus show-\r\n",
      "ing competition amongst slot vectors during encoding helps generalization.\r\n",
      "Slow inference through reconstruction feedback helps OOD generalization of GFS-Nets. Our\r\n",
      "baselines 3D-DETR and Learning2Group [43] are feed-forward in nature, they lack any form of\r\n",
      "reconstruction feedback, and thus cannot adapt as our model through such feedback.\r\n",
      "\r\n",
      "4.2   RGB image segmentation and view prediction without supervision\r\n",
      "\r\n",
      "In this task, the input to the model is a single RGB image that contains multiple objects and the goal\r\n",
      "is to segment the image into objects and predict images from alternative scene viewpoints. We test\r\n",
      "our model in the following two benchmarks: ((i) CLEVR [31]: this is a commonly used benchmark\r\n",
      "for unsupervised image segmentation. The dataset contains scenes that consist of 5 to 7 spheres,\r\n",
      "cylinders and cubes, in various colors and materials. We use the multi-view version of this dataset\r\n",
      "introduced by uORF [67] as the training set.\r\n",
      "\r\n",
      "\r\n",
      "                                                     9\r\n",
      "\f",
      "\n",
      "\n",
      "    Input   Slot Attention [44]   uORF [69]   GFS-Nets-Fast   GFS-Nets-Slow      Input          Predicted Novel Views by GFS-Nets\r\n",
      "\r\n",
      "\r\n",
      "Figure 6: RGB image decomposition and view prediction for GFS-Nets and baselines. Left: From 2nd to 5th\r\n",
      "column we show reconstructed RGB image superimposed with the predicted segmentation masks for our model\r\n",
      "and baselines. Right: From 2nd to 5th column we show RGB renderings across multiple novel viewpoints for\r\n",
      "GFS-Nets.\r\n",
      "\r\n",
      "\r\n",
      "(ii) Room Diverse++: Room Diverse++ builds upon Room Diverse, which is a multi-view RGB\r\n",
      "dataset introduced by Yu et al.[67] where they placed multiple ShapeNet Chairs with different solid\r\n",
      "colors (Red, Blue etc) onto a textured background. We make it more challenging by applying real-\r\n",
      "world ShapeNet textures to the object meshes instead of solid colors. We also add more categories\r\n",
      "such as Beds, Benches, Tables, Cupboards, and Sofas to the Chair category. We sample about 6 to 10\r\n",
      "objects in each scene.\r\n",
      "\r\n",
      "Baselines We consider the following baselines: (i) Slot attention Locatello et al. [42] autoencodes\r\n",
      "single view RGB images, and is described in Section 3.1. We use the publicly available code and\r\n",
      "train the model on the training set for single image autoencoding.\r\n",
      "(ii) uORF Yu et al. [67] is a recent work that\r\n",
      "extends Slot Attention of [42] to multiview RGB                                Method                   CLEVR               Room Diverse++\r\n",
      "images.                                                                        uORF [67]                 0.86                    0.65\r\n",
      "During primitive training for our model, we as-                                Slot Attention [42]       0.04                    0.28\r\n",
      "sume access to posed RGB images of single                                      GFS-Nets-Fast             0.86                    0.74\r\n",
      "object scenes as our primitive dataset, as shown                               GFS-Nets-Slow             0.92                    0.81\r\n",
      "in Figure 2 bottom row left. Our implementa-\r\n",
      "tion builds upon [67] with the difference that we                             Table 3: ARI Segmentation accuracy (higher the bet-\r\n",
      "consider curriculum and slow inference at test                                ter) on CLEVR and RoomDiverse++ Dataset for RGB\r\n",
      "                                                                              input. As can be seen GFS-Nets-Slow outperform all\r\n",
      "time. We ablate slow and fast versions of our                                 other baselines in segmentation accuracy.\r\n",
      "model.\r\n",
      "We show quantitative results of GFS-Nets and\r\n",
      "the baselines in Table 3 and qualitative view prediction and segmentation results in Figure 6.\r\n",
      "GFS-Nets-slow outperforms the baselines significantly. This suggests that curriculum and slow\r\n",
      "inference are key to out-of-distribution generalization. Slot Attention [42] achieves very low accuracy\r\n",
      "on CLEVR, as it is unable to disentangle background from foreground, as also reported in Yu et\r\n",
      "al.[67]. We further visualize GFS-Nets’s ability to render novel viewpoints in the supplementary\r\n",
      "video.\r\n",
      "\r\n",
      "Limitations and Future Work: GFS-Nets has limitations that offer several promising directions\r\n",
      "for future work. First, GFS-Nets does not model pairwise interactions between slots. Incorporating\r\n",
      "such interactions is a direct avenue for future work. Second, entity decomposition is inherently\r\n",
      "hierarchical: objects - parts - subparts while GFS-Nets currently models a flat non-hierarchical list of\r\n",
      "entities. Third, amortizing the slow inference in feedforward predictions could be another interesting\r\n",
      "extension of the current framework. Finally, although GFS-Nets is tested on far more realistic data\r\n",
      "than considered in prior works, scaling the model to more realistic datasets used by mainstream\r\n",
      "supervised visual detectors, is a direct avenue for future work.\r\n",
      "\r\n",
      "5     Conclusion\r\n",
      "We presented GFS-Nets, a model that segments scenes into entities while autoencoding the input\r\n",
      "scene through slot-based encoders and shared slot decoders. We showed that curriculum training\r\n",
      "in the form of primitive entities, single object scenes, or labelled segmentations is necessary to\r\n",
      "break the trade-off between segmentation and reconstruction observed in previous works [15]. This\r\n",
      "permits GFS-Nets to use slow inference through reconstruction feedback and drastically improve the\r\n",
      "decompositions of out-of-distribution scenes. We believe that a compositional approach to AI, in\r\n",
      "terms of grounded symbol-like representations [44, 38] is of fundamental importance for realizing\r\n",
      "human-level generalization [1], and we hope that GFS-Nets may contribute towards that goal.\r\n",
      "\r\n",
      "\r\n",
      "                                                                          10\r\n",
      "\f",
      "\n",
      "\n",
      "References\r\n",
      " [1] Bahdanau, D., Murty, S., Noukhovitch, M., Nguyen, T.H., de Vries, H., Courville, A.: System-\r\n",
      "     atic generalization: what is required and can it be learned? arXiv preprint arXiv:1811.12889\r\n",
      "     (2018)\r\n",
      " [2] Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., Katz, B.:\r\n",
      "     Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\r\n",
      "     models. Advances in neural information processing systems 32 (2019)\r\n",
      " [3] van Bergen, R.S., Kriegeskorte, N.: Going in circles is the way forward: the role of re-\r\n",
      "     currence in visual inference. Current Opinion in Neurobiology 65, 176–193 (Dec 2020).\r\n",
      "     https://doi.org/10.1016/j.conb.2020.11.009, http://dx.doi.org/10.1016/j.conb.2020.\r\n",
      "     11.009\r\n",
      " [4] Burgess, C.P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., Lerchner, A.:\r\n",
      "     Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390\r\n",
      "     (2019)\r\n",
      " [5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end\r\n",
      "     object detection with transformers. In: European Conference on Computer Vision. pp. 213–229.\r\n",
      "     Springer (2020)\r\n",
      " [6] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\r\n",
      "     M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet: An Information-Rich 3D Model\r\n",
      "     Repository. Tech. Rep. arXiv:1512.03012 [cs.GR], Stanford University — Princeton University\r\n",
      "     — Toyota Technological Institute at Chicago (2015)\r\n",
      " [7] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\r\n",
      "     M., Song, S., Su, H., et al.: Shapenet: An information-rich 3d model repository. arXiv preprint\r\n",
      "     arXiv:1512.03012 (2015)\r\n",
      " [8] Chen, Z., Tagliasacchi, A., Zhang, H.: Bsp-net: Generating compact meshes via binary space\r\n",
      "     partitioning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\r\n",
      "     Recognition. pp. 45–54 (2020)\r\n",
      " [9] Chen, Z., Yin, K., Fisher, M., Chaudhuri, S., Zhang, H.: Bae-net: branched autoencoder\r\n",
      "     for shape co-segmentation. In: Proceedings of the IEEE/CVF International Conference on\r\n",
      "     Computer Vision. pp. 8490–8499 (2019)\r\n",
      "[10] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio,\r\n",
      "     Y.: Learning phrase representations using rnn encoder-decoder for statistical machine translation.\r\n",
      "     arXiv preprint arXiv:1406.1078 (2014)\r\n",
      "[11] Deng, B., Genova, K., Yazdani, S., Bouaziz, S., Hinton, G., Tagliasacchi, A.: Cvxnet: Learnable\r\n",
      "     convex decomposition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\r\n",
      "     Pattern Recognition. pp. 31–44 (2020)\r\n",
      "[12] Deprelle, T., Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: Learning elementary\r\n",
      "     structures for 3d shape generation and matching. arXiv preprint arXiv:1908.04725 (2019)\r\n",
      "[13] Du, Y., Smith, K., Ulman, T., Tenenbaum, J., Wu, J.: Unsupervised discovery of 3d physical\r\n",
      "     objects from video. arXiv preprint arXiv:2007.12348 (2020)\r\n",
      "[14] Ellis, K., Wong, C., Nye, M., Sable-Meyer, M., Cary, L., Morales, L., Hewitt, L., Solar-Lezama,\r\n",
      "     A., Tenenbaum, J.B.: Dreamcoder: Growing generalizable, interpretable knowledge with\r\n",
      "     wake-sleep bayesian program learning. arXiv preprint arXiv:2006.08381 (2020)\r\n",
      "[15] Engelcke, M., Jones, O.P., Posner, I.: Reconstruction bottlenecks in object-centric generative\r\n",
      "     models. arXiv preprint arXiv:2007.06245 (2020)\r\n",
      "[16] Engelcke, M., Kosiorek, A.R., Jones, O.P., Posner, I.: Genesis: Generative scene inference and\r\n",
      "     sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052 (2019)\r\n",
      "\r\n",
      "\r\n",
      "                                                 11\r\n",
      "\f",
      "\n",
      "\n",
      "[17] Eslami, S., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., Hinton,\r\n",
      "     G.E.: Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint\r\n",
      "     arXiv:1603.08575 (2016)\r\n",
      "[18] Gao, L., Yang, J., Wu, T., Yuan, Y.J., Fu, H., Lai, Y.K., Zhang, H.: Sdm-net: Deep generative\r\n",
      "     network for structured deformable mesh. ACM Transactions on Graphics (TOG) 38(6), 1–15\r\n",
      "     (2019)\r\n",
      "[19] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.: Imagenet-\r\n",
      "     trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness.\r\n",
      "     arXiv preprint arXiv:1811.12231 (2018)\r\n",
      "[20] Genova, K., Cole, F., Sud, A., Sarna, A., Funkhouser, T.: Local deep implicit functions for\r\n",
      "     3d shape. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\r\n",
      "     Recognition. pp. 4857–4866 (2020)\r\n",
      "[21] Genova, K., Cole, F., Vlasic, D., Sarna, A., Freeman, W.T., Funkhouser, T.: Learning shape\r\n",
      "     templates with structured implicit functions. In: Proceedings of the IEEE/CVF International\r\n",
      "     Conference on Computer Vision. pp. 7154–7164 (2019)\r\n",
      "[22] Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio, Y., Schölkopf, B.: Recurrent\r\n",
      "     independent mechanisms. arXiv preprint arXiv:1909.10893 (2019)\r\n",
      "[23] Greff, K., Kaufman, R.L., Kabra, R., Watters, N., Burgess, C., Zoran, D., Matthey, L., Botvinick,\r\n",
      "     M., Lerchner, A.: Multi-object representation learning with iterative variational inference. In:\r\n",
      "     International Conference on Machine Learning. pp. 2424–2433. PMLR (2019)\r\n",
      "[24] Greff, K., Rasmus, A., Berglund, M., Hao, T.H., Schmidhuber, J., Valpola, H.: Tagger: Deep\r\n",
      "     unsupervised perceptual grouping. arXiv preprint arXiv:1606.06724 (2016)\r\n",
      "[25] Greff, K., van Steenkiste, S., Schmidhuber, J.: On the binding problem in artificial neural\r\n",
      "     networks. CoRR abs/2012.05208 (2020), https://arxiv.org/abs/2012.05208\r\n",
      "[26] Guerin, F.: Projection: A mechanism for human-like reasoning in artificial intelligence. CoRR\r\n",
      "     abs/2103.13512 (2021), https://arxiv.org/abs/2103.13512\r\n",
      "[27] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro-\r\n",
      "     ceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770–778\r\n",
      "     (2016)\r\n",
      "[28] Hubert, L., Arabie, P.: Comparing partitions. Journal of classification 2(1), 193–218 (1985)\r\n",
      "[29] Jo, J., Bengio, Y.: Measuring the tendency of cnns to learn surface statistical regularities. arXiv\r\n",
      "     preprint arXiv:1711.11561 (2017)\r\n",
      "[30] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.:\r\n",
      "     Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In:\r\n",
      "     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2901–2910\r\n",
      "     (2017)\r\n",
      "[31] Johnson, J., Krishna, R., Stark, M., Li, L.J., Shamma, D., Bernstein, M., Fei-Fei, L.: Image\r\n",
      "     retrieval using scene graphs. In: Proceedings of the IEEE Conference on Computer Vision and\r\n",
      "     Pattern Recognition (CVPR) (June 2015)\r\n",
      "[32] Kabra, R., Burgess, C., Matthey, L., Kaufman, R.L., Greff, K., Reynolds, M., Lerchner, A.:\r\n",
      "     Multi-object datasets. https://github.com/deepmind/multi-object-datasets/ (2019)\r\n",
      "[33] Kato, H., Harada, T.: Learning view priors for single-view 3d reconstruction. In: Proceedings of\r\n",
      "     the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9778–9787 (2019)\r\n",
      "[34] Kipf, T., Elsayed, G.F., Mahendran, A., Stone, A., Sabour, S., Heigold, G., Jonschkowski,\r\n",
      "     R., Dosovitskiy, A., Greff, K.: Conditional object-centric learning from video. arXiv preprint\r\n",
      "     arXiv:2111.12594 (2021)\r\n",
      "\r\n",
      "\r\n",
      "                                                  12\r\n",
      "\f",
      "\n",
      "\n",
      "[35] Kosiorek, A., Kim, H., Teh, Y.W., Posner, I.: Sequential attend, infer, repeat: Generative\r\n",
      "     modelling of moving objects. Advances in Neural Information Processing Systems 31, 8606–\r\n",
      "     8616 (2018)\r\n",
      "\r\n",
      "[36] Kuhn, H.W.: The hungarian method for the assignment problem. Naval research logistics\r\n",
      "     quarterly 2(1-2), 83–97 (1955)\r\n",
      "\r\n",
      "[37] Kulkarni, T.D., Kohli, P., Tenenbaum, J.B., Mansinghka, V.: Picture: A probabilistic program-\r\n",
      "     ming language for scene perception. In: Proceedings of the ieee conference on computer vision\r\n",
      "     and pattern recognition. pp. 4390–4399 (2015)\r\n",
      "\r\n",
      "[38] Lake, B.M., Salakhutdinov, R., Tenenbaum, J.B.: Human-level concept learning through\r\n",
      "     probabilistic program induction. Science 350(6266), 1332–1338 (2015)\r\n",
      "\r\n",
      "[39] Lake, B.M., Ullman, T.D., Tenenbaum, J.B., Gershman, S.J.: Building machines that learn and\r\n",
      "     think like people. Behavioral and brain sciences 40 (2017)\r\n",
      "\r\n",
      "[40] Lal, S., Prabhudesai, M., Mediratta, I., Harley, A.W., Fragkiadaki, K.: Coconets: Continuous\r\n",
      "     contrastive 3d scene representations (2021)\r\n",
      "\r\n",
      "[41] Li, Y., Mao, J., Zhang, X., Freeman, W.T., Tenenbaum, J.B., Snavely, N., Wu, J.: Multi-plane\r\n",
      "     program induction with 3d box priors. arXiv preprint arXiv:2011.10007 (2020)\r\n",
      "\r\n",
      "[42] Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J.,\r\n",
      "     Dosovitskiy, A., Kipf, T.: Object-centric learning with slot attention (2020)\r\n",
      "\r\n",
      "[43] Luo, T., Mo, K., Huang, Z., Xu, J., Hu, S., Wang, L., Su, H.: Learning to group: A bottom-up\r\n",
      "     framework for 3d part discovery in unseen categories. arXiv preprint arXiv:2002.06478 (2020)\r\n",
      "\r\n",
      "[44] Marcus, G.F.: The algebraic mind: Integrating connectionism and cognitive science. MIT press\r\n",
      "     (2003)\r\n",
      "\r\n",
      "[45] Marr, D.: Vision: A Computational Investigation into the Human Representation and Processing\r\n",
      "     of Visual Information. W.H. Freeman and Co., New York, NY (1982)\r\n",
      "\r\n",
      "[46] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\r\n",
      "     Learning 3d reconstruction in function space. In: Proceedings of the IEEE/CVF Conference on\r\n",
      "     Computer Vision and Pattern Recognition. pp. 4460–4470 (2019)\r\n",
      "\r\n",
      "[47] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf:\r\n",
      "     Representing scenes as neural radiance fields for view synthesis. In: European conference on\r\n",
      "     computer vision. pp. 405–421. Springer (2020)\r\n",
      "\r\n",
      "[48] Misra, I., Girdhar, R., Joulin, A.: An end-to-end transformer model for 3d object detection. In:\r\n",
      "     Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2906–2917\r\n",
      "     (2021)\r\n",
      "\r\n",
      "[49] Mo, K., Zhu, S., Chang, A.X., Yi, L., Tripathi, S., Guibas, L.J., Su, H.: Partnet: A large-scale\r\n",
      "     benchmark for fine-grained and hierarchical part-level 3d object understanding. In: Proceedings\r\n",
      "     of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 909–918 (2019)\r\n",
      "\r\n",
      "[50] Niu, C., Li, J., Xu, K.: Im2struct: Recovering 3d shape structure from a single rgb image. In:\r\n",
      "     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4521–4529\r\n",
      "     (2018)\r\n",
      "\r\n",
      "[51] Paschalidou, D., Gool, L.V., Geiger, A.: Learning unsupervised hierarchical part decomposition\r\n",
      "     of 3d objects from a single rgb image. In: Proceedings of the IEEE/CVF Conference on\r\n",
      "     Computer Vision and Pattern Recognition. pp. 1060–1070 (2020)\r\n",
      "\r\n",
      "[52] Paschalidou, D., Katharopoulos, A., Geiger, A., Fidler, S.: Neural parts: Learning expressive 3d\r\n",
      "     shape abstractions with invertible neural networks. In: Proceedings of the IEEE/CVF Conference\r\n",
      "     on Computer Vision and Pattern Recognition. pp. 3204–3215 (2021)\r\n",
      "\r\n",
      "\r\n",
      "                                                 13\r\n",
      "\f",
      "\n",
      "\n",
      "[53] Paschalidou, D., Ulusoy, A.O., Geiger, A.: Superquadrics revisited: Learning 3d shape parsing\r\n",
      "     beyond cuboids. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\r\n",
      "     Recognition. pp. 10344–10353 (2019)\r\n",
      "[54] Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by self-supervised\r\n",
      "     prediction. In: International conference on machine learning. pp. 2778–2787. PMLR (2017)\r\n",
      "[55] Rahaman, N., Goyal, A., Gondal, M.W., Wuthrich, M., Bauer, S., Sharma, Y., Bengio, Y.,\r\n",
      "     Schölkopf, B.: S2rms: Spatially structured recurrent modules. arXiv preprint arXiv:2007.06533\r\n",
      "     (2020)\r\n",
      "[56] Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object detection with\r\n",
      "     region proposal networks. CoRR abs/1506.01497 (2015), http://arxiv.org/abs/1506.\r\n",
      "     01497\r\n",
      "[57] Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. arXiv preprint\r\n",
      "     arXiv:1710.09829 (2017)\r\n",
      "[58] Sitzmann, V., Chan, E., Tucker, R., Snavely, N., Wetzstein, G.: Metasdf: Meta-learning signed\r\n",
      "     distance functions. Advances in Neural Information Processing Systems 33, 10136–10147\r\n",
      "     (2020)\r\n",
      "[59] Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., Hardt, M.: Test-time training with self-\r\n",
      "     supervision for generalization under distribution shifts. In: International Conference on Machine\r\n",
      "     Learning. pp. 9229–9248. PMLR (2020)\r\n",
      "[60] Tian, Y., Luo, A., Sun, X., Ellis, K., Freeman, W.T., Tenenbaum, J.B., Wu, J.: Learning to infer\r\n",
      "     and execute 3d shape programs. arXiv preprint arXiv:1901.02875 (2019)\r\n",
      "[61] Tsai, Y.H.H., Srivastava, N., Goh, H., Salakhutdinov, R.: Capsules with inverted dot-product\r\n",
      "     attention routing. arXiv preprint arXiv:2002.04764 (2020)\r\n",
      "[62] Tulsiani, S., Su, H., Guibas, L.J., Efros, A.A., Malik, J.: Learning shape abstractions by\r\n",
      "     assembling volumetric primitives. In: Proceedings of the IEEE Conference on Computer Vision\r\n",
      "     and Pattern Recognition. pp. 2635–2643 (2017)\r\n",
      "[63] Van Steenkiste, S., Chang, M., Greff, K., Schmidhuber, J.: Relational neural expectation\r\n",
      "     maximization: Unsupervised discovery of objects and their interactions. arXiv preprint\r\n",
      "     arXiv:1802.10353 (2018)\r\n",
      "[64] Wu, R., Zhuang, Y., Xu, K., Zhang, H., Chen, B.: Pq-net: A generative part seq2seq network\r\n",
      "     for 3d shapes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\r\n",
      "     Recognition (CVPR) (June 2020)\r\n",
      "[65] Xia, W., Zhang, Y., Yang, Y., Xue, J.H., Zhou, B., Yang, M.H.: Gan inversion: A survey. arXiv\r\n",
      "     preprint arXiv:2101.05278 (2021)\r\n",
      "[66] Yao, C.H., Hung, W.C., Jampani, V., Yang, M.H.: Discovering 3d parts from image collections.\r\n",
      "     In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12981–\r\n",
      "     12990 (2021)\r\n",
      "[67] Yu, H.X., Guibas, L.J., Wu, J.: Unsupervised discovery of object radiance fields. arXiv preprint\r\n",
      "     arXiv:2107.07905 (2021)\r\n",
      "[68] Zablotskaia, P., Dominici, E.A., Sigal, L., Lehrmann, A.M.: Unsupervised video decomposition\r\n",
      "     using spatio-temporal iterative inference. arXiv preprint arXiv:2006.14727 (2020)\r\n",
      "[69] Zhang, N., Donahue, J., Girshick, R., Darrell, T.: Part-based r-cnns for fine-grained category\r\n",
      "     detection. In: European conference on computer vision. pp. 834–849. Springer (2014)\r\n",
      "[70] Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: Proceedings of the\r\n",
      "     IEEE/CVF International Conference on Computer Vision. pp. 16259–16268 (2021)\r\n",
      "[71] Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty-thousand classes\r\n",
      "     using image-level supervision. arXiv preprint arXiv:2201.02605 (2022)\r\n",
      "\r\n",
      "\r\n",
      "                                                  14\r\n",
      "\f",
      "\n",
      "\n",
      "[72] Zhu, J.Y., Krähenbühl, P., Shechtman, E., Efros, A.A.: Generative visual manipulation on the\r\n",
      "     natural image manifold. In: European conference on computer vision. pp. 597–613. Springer\r\n",
      "     (2016)\r\n",
      "[73] Zoran, D., Kabra, R., Lerchner, A., Rezende, D.J.: Parts: Unsupervised segmentation with\r\n",
      "     slots, attention and independence maximization. In: Proceedings of the IEEE/CVF International\r\n",
      "     Conference on Computer Vision. pp. 10439–10447 (2021)\r\n",
      "[74] Zuffi, S., Kanazawa, A., Berger-Wolf, T., Black, M.J.: Three-d safari: Learning to estimate\r\n",
      "     zebra pose, shape, and texture from images\" in the wild\". In: Proceedings of the IEEE/CVF\r\n",
      "     International Conference on Computer Vision. pp. 5359–5368 (2019)\r\n",
      "\r\n",
      "\r\n",
      "Appendix\r\n",
      "The structure of this appendix is as follows: In Section 6 we cover the details on the datasets. In\r\n",
      "Section 7 we specify further implementation details. In Section 8 we provide additional qualitative\r\n",
      "and quantitative results for the experiments in Section 4 of our main paper.\r\n",
      "\r\n",
      "6     Datasets\r\n",
      "6.1   Point Cloud\r\n",
      "\r\n",
      "For all the tasks we subsample the input point clouds to a standard size of 2048 points.\r\n",
      "\r\n",
      "Generic primitive part dataset. We use the primitive dataset of [60] for learning the primitive\r\n",
      "distribution in Section 4.1.1. The dataset consists of 200K primitive instances sampled from the\r\n",
      "primitive templates that are visualized in Figure 7. Instances are sampled from the templates by\r\n",
      "changing their sizes and placing them in random locations, similar to [60].\r\n",
      "\r\n",
      "Category-specific primitives from PartNet dataset. We seperate out individual segments from\r\n",
      "the level-3 instance segmentation masks from the official train-split of Chair category of the PartNet\r\n",
      "dataset. We additionally do rotation and translation based augmentations on individual segments. We\r\n",
      "visualize some of these primitive examples in Figure 8. We use this dataset for learning the primitive\r\n",
      "distribution in Section 4.1.2. The dataset in total consists of about 100K primitive examples.\r\n",
      "\r\n",
      "Synthetic whole shape dataset. This dataset was generated by Shape2Prog[60]. The segmentation\r\n",
      "labels from this dataset are used by them as a supervision signal for later generalizing to PartNet\r\n",
      "shapes. The dataset consists of about 120K synthetically generated Chairs and Tables, we visualize\r\n",
      "some of these synthetically generated tables and chairs in Figure 9. Note that no other baseline or\r\n",
      "GFS-Nets has access to this dataset.\r\n",
      "\r\n",
      "PartNet dataset. We use the official level-3 train-test split of PartNet[49]. We use the train split\r\n",
      "of Chair category as our training set, we consider test split of Table category in PartNet our test\r\n",
      "categories. We use this as the train-test split in Section 4.1 of the main paper. We further compare our\r\n",
      "model on the other PartNet categories in supplementary section 8. We set the value of number of\r\n",
      "slots K as 16 for this dataset.\r\n",
      "\r\n",
      "6.2   RGB\r\n",
      "\r\n",
      "CLEVR dataset. We use the train-test split of CLEVR dataset opensourced by [67]. Their dataset\r\n",
      "is built on top of the original CLEVR [30] dataset, where they add additional multi-view rendering.\r\n",
      "Their train/test dataset consists of 5,6,7 objects from random geometric primitives(cylinder, cube and\r\n",
      "sphere) and random colors(red, blue, purple, gray, cyan, yellow, green, brown) placed in random\r\n",
      "locations in the scene. The multi-view setting consists of randomly sampling from 360 degree\r\n",
      "azimuth angles with a fixed elevation angle. Our train data consists of 1000 single object scenes and\r\n",
      "1000 multi-object scenes. The only difference between their train-test data are the novel locations\r\n",
      "at which the objects would be placed in the scene. We set the value of number of slots K as 8. We\r\n",
      "visualize some of the primitive examples (multi-view single-object scenes) of the CLEVR dataset in\r\n",
      "Figure 10.\r\n",
      "\r\n",
      "\r\n",
      "                                                  15\r\n",
      "\f",
      "\n",
      "\n",
      "Figure 7: We visualize the generic primitive templates of [60], as you can see they mainly consists of Cubes,\r\n",
      "Cuboids and Discs.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "   Figure 8: We visualize the category-specific primitives extracted from level-3 Chair category of PartNet.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                      16\r\n",
      "\f",
      "\n",
      "\n",
      "Figure 9: We visualize the synthetic whole shape dataset of [60]. Shape2Prog supervises their model using the\r\n",
      "annotations from this dataset.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Figure 10: We visualize some examples of the primitives used in the CLEVR dataset. Primitives are represented\r\n",
      "as multi-view single object RGB scenes as shown in the figure.\r\n",
      "\r\n",
      "\r\n",
      "RoomDiverse++ dataset. We build RoomDiverse++ on top of the RoomDiverse dataset of [67].\r\n",
      "In which instead of using the solid CLEVR colors as textures of meshes we keep the original textures\r\n",
      "of the ShapeNet meshes, additionaly along with Chairs we also add other ShapeNet categories such\r\n",
      "as Benches, Cabinets, Beds, Tables and Sofas in the dataset. In total we use 20 object meshes and\r\n",
      "3 background textures to generate the dataset. We use the same multi-view rendering settings as\r\n",
      "CLEVR. Our train data consists of 1000 single object scenes and 2000 multi-object scenes. We use\r\n",
      "6,7,8 objects placed at random locations for generating our multi-object train dataset. We use 500 of\r\n",
      "7,8,9 object scenes as our test data. We set the value of number of slots K as 10. We visualize some\r\n",
      "of the primitive examples in the dataset in Figure 11.\r\n",
      "\r\n",
      "\r\n",
      "7     Implementation details\r\n",
      "\r\n",
      "7.1   Proposed Model\r\n",
      "\r\n",
      "Code, training details and computational complexity. Proposed model is implemented in\r\n",
      "Python/PyTorch. We use a batch size of 8 for point cloud input and a batch size of 2 on RGB\r\n",
      "input. We set our learning rate as 10−4 . We use the Adam optimizer with β1 = 0.9, β2 = 0.999. Our\r\n",
      "primitive learning model takes 24 hours (approximately 200k iterations) to converge. Similarly our\r\n",
      "\r\n",
      "\r\n",
      "                                                     17\r\n",
      "\f",
      "\n",
      "\n",
      "Figure 11: We visualize some examples of the primitives used in the Room Diverse++ dataset. Primitives are\r\n",
      "represented as multi-view single object RGB scenes as shown in the figure.\r\n",
      "\r\n",
      "\r\n",
      "primitive composition learning model takes about 12hr (approximately 100k iterations) to converge.\r\n",
      "Our slow inference per example takes about 1 min (500 iterations). A forward pass through the\r\n",
      "proposed model takes about 0.15 secs. We use a single V100 GPU for training and inference.\r\n",
      "\r\n",
      "Inputs. For RGB input we randomly select 2 views which include the RGB and ground-truth\r\n",
      "egomotion. We use a single RGB image for testing. For RGB, our input is a 64 × 64 × 3 tensor, for\r\n",
      "point cloud our input is a 2048 × 3\r\n",
      "\r\n",
      "Point Cloud Encoder. We adopt the point transformer[70] architecture as our encoder. Point\r\n",
      "transformer encoder is essentially layers of self attention blocks. Specifically a self attention block\r\n",
      "includes sampling of query points and updating them using their N most neighbouring points as\r\n",
      "key/value vectors. In the architecture we specifically apply 5 layers of self attention which look as\r\n",
      "follows: 2048-16-64, 2048-16-64, 512-16-64, 512-16-64, 128-16-64, 128-16-64. We use the notation\r\n",
      "of S-N -C, where S is the number of subsampled query points from the point cloud, N is the number\r\n",
      "of neighbouring points and C is the feature dimension. We thus get an output feature map of size\r\n",
      "128 × 64.\r\n",
      "\r\n",
      "RGB Encoder. Our RGB encoder is similar to that of [67], which is basically a U-net. We\r\n",
      "concatenate the RGB values with the pixel coordinates that are normalized in the range of -1 to 1. We\r\n",
      "then pass the concatenated 2D image through a convolutional feature extractor. The architecture of\r\n",
      "the CNN is as follows: 3-2-64, 3-2-64, 3-2-64, 3-1-64, 3-1-64, 3-1-64. We use the notation of k-s-c,\r\n",
      "where k is the kernel size, s is the number of strides and c is the feature channel. Our final output\r\n",
      "size is 64 × 64 × 64.\r\n",
      "\r\n",
      "Point Cloud Decoder. We obtain point occupancies by querying the slot feature vector slotk at\r\n",
      "discrete locations (x, y, z) specifically ox,y,z = Dec(slotk , (x, y, z)). The architecture of Dec is\r\n",
      "similar to that of [40]. Given slotk , which is one of the slot feature vector. We encode the coordinate\r\n",
      "(x, y, z) into a 64-D feature vector using a linear layer. We denote this vector as z. The inputs slotk\r\n",
      "and z are then processed as follows:\r\n",
      "\r\n",
      "   out_k = RB_i( RB_{i-1}( \\cdots RB_1( z + FC_1(slot_k)) \\\\ \\cdots ] + FC_{i-1}(slot_k)) + FC_{i}(slot_k)).\r\n",
      "                                                                                                               (3)\r\n",
      "We set i = 3. F Ci is a linear layer that outputs a 64 dimensional vector. RNi is a 2 layer ResNet\r\n",
      "MLP block [27]. The architecture of ResNet block is: ReLU, 64-64, ReLU, 64-64. Here, i − o\r\n",
      "represents a linear layer, where i and o are the input and output dimension. Finally outk is then\r\n",
      "passed through a ReLU activation function followed by a linear layer to generate a single value for\r\n",
      "occupancy.\r\n",
      "\r\n",
      "RGB Decoder. Our decoder follows the architecture of [67]. The decoder is a 8 layer MLP,\r\n",
      "specifically ReLU, 64-64, ReLU, 64-64 .... 64-4. The MLP takes in as input slotk feature and the\r\n",
      "location x, y, z and predicts the RGB and density values at that location.\r\n",
      "\r\n",
      "\r\n",
      "                                                       18\r\n",
      "\f",
      "\n",
      "\n",
      "Slot Extractor. Following is the pseudo-code of Slot Attention, which we use for learning primitive\r\n",
      "compositions.\r\n",
      "\r\n",
      "Algorithm 1 Slot Attention module. Input is N vectors of dimension Dinputs which is mapped to a\r\n",
      "set of K slots of dimension Dslots . We sample the learnable multivariate gaussian to get the initial\r\n",
      "slot features, here the learnable parameters are: µ ∈ RDslots and σ ∈ RDslots .For our experiments we\r\n",
      "set T = 3.\r\n",
      "1: Input: inputs ∈ RN ×Dinputs , slots ∼ N (µ, diag(σ)) ∈ RK×Dslots\r\n",
      "2: Layer params: k, q, v: linear projections; GRU; MLP; LayerNorm (x3)\r\n",
      "3: inputs = LayerNorm (inputs)\r\n",
      "4: for t = 0 . . . T\r\n",
      "5:     slots_prev = slots\r\n",
      "6:     slots = LayerNorm (slots)\r\n",
      "7:     attn = Softmax ( √1 k(inputs) · q(slots)T , axis=‘slots’)\r\n",
      "                             D\r\n",
      "8:     updates = Avg (weights=attn + ϵ, values=v(inputs))\r\n",
      "9:     slots = GRU (state=slots_prev, inputs=updates)\r\n",
      "10:      slots += MLP (LayerNorm (slots))\r\n",
      "11: return slots\r\n",
      "\r\n",
      "\r\n",
      "For primitive learning stage on PointCloud we set the value of number of slots K as 1. Additionally\r\n",
      "we replace the Softmax () activation in step 7 with a Sigmoid () activation. For primitive learning\r\n",
      "stage on RGB scenes we set the value of number of slots K as 2. This is due to RGB images having\r\n",
      "a background componenet in them.\r\n",
      "\r\n",
      "7.2   Baselines                               Training\r\n",
      "\r\n",
      "\r\n",
      "               Cross Attention                                                                              P\r\n",
      "                 With Heads                               Attention\r\n",
      "                                                                                  C                         O\r\n",
      "                                                            Map\r\n",
      "                                                                                  o                         I\r\n",
      "                                                                                  n                         N\r\n",
      "                                                                                  c                         T\r\n",
      "                                                          Attention               a FPN-Style\r\n",
      "            Encoder\r\n",
      "                                                            Map                   t Transformer             A\r\n",
      "                                                                                  e                         R\r\n",
      "                                                                                  n                         G\r\n",
      "                                                                                  a                         M\r\n",
      "                                                                                  t                         A\r\n",
      "                                                                                  e                         X\r\n",
      "                                                          Attention\r\n",
      "                                                            Map\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                 Head features Head features        Spatial Attention\r\n",
      "                                        T=1           T=2                Maps                     Hungarian Matching\r\n",
      "                                                                                                              +\r\n",
      "                                                                                                  Binary CrossEntropy Loss\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Figure 12: 3D-DETR model architecture. Our baseline model is very similar to 2D-DETR[5], where we replace\r\n",
      "their base 2D encoder and 2D FPN-style CNN with a 3D PointTransformer[70]. Given point clouds as input\r\n",
      "our encoder backbone featurizes the points into N feature vectors, we then do iterative self and cross attention\r\n",
      "using K learnable query heads, following the implementation of Carion et al.[5]. We then compute attention of\r\n",
      "the updated queries wrt to the encoded feature vectors. We then concatenate these attentions along the batch\r\n",
      "dimension and pass them through a FPN-style transformer that increases their resolution and outputs each query\r\n",
      "mask logits. We then do hungarian matching and binary cross entropy loss\r\n",
      "\r\n",
      "\r\n",
      "3D-DETR. 3D-DETR is a SOTA segmentation architecture, we build this architecture on top\r\n",
      "of 2D-DETR[5], where we replace their 2D encoders with a 3D PointTransformer[70]. For fair\r\n",
      "comparision we make sure the number of parameters in 3D-DETR is the same as GFS-Nets.\r\n",
      "The base encoder is a 5 layer architecture of 1024-16-64, 1024-16-64, 1024-16-64, 1024-16-64,\r\n",
      "512-16-64, following the notation of S,N ,C where S is the number of sampled query points, N is\r\n",
      "the number of selected neigbouring points and C is the feature dimension. We have 16 learnable\r\n",
      "\r\n",
      "\r\n",
      "                                                               19\r\n",
      "\f",
      "\n",
      "\n",
      "queries with six encoder-decoder layers of transformer attention, each layer consists of 8 heads. We\r\n",
      "then compute multi-head cross-attention between each query vector and the encoded 3d points. This\r\n",
      "gives us a map of size M × 512 × 16, where M is the number of heads in the multi-head attention.\r\n",
      "Each attention map is individually upsampled through a PointTransformer decoder of the architecture\r\n",
      "512-16-64, 1024-16-64, 1024-16-64, which gives us the final instance segmentation mask per query,\r\n",
      "similar to DETR[5]. We then use Hungarian matching to match the predicted masks against the\r\n",
      "ground truth masks and then apply binary cross entropy loss for each match. We aggregate the losses\r\n",
      "from each query and backpropagate. Figure 12 visualizes the architecture of 3D-DETR. Note that\r\n",
      "we do not follow the 2 stage training of [5], rather we train their model end-to-end for instance\r\n",
      "segmentation, similar to our model. We have found this trick to save compute and still not harm the\r\n",
      "end results.\r\n",
      "\r\n",
      "Learning2Group.[43] We use their open-sourced architecture and code for comparision with our\r\n",
      "mode. We train their model using our datasets from scratch.\r\n",
      "\r\n",
      "Shape2Prog[60] We use their open-sourced architecture, code and pretrained checkpoints for\r\n",
      "comparision with our model. We change the value of number of blocks similar to the number of slots\r\n",
      "in our model for each dataset.\r\n",
      "\r\n",
      "PQ-Nets.[64] We use their open-sourced architecture and code for comparision with our model.\r\n",
      "We train their model using our datasets from scratch. We change the value of number of slots in their\r\n",
      "model based on the maximum number of parts in the dataset.\r\n",
      "\r\n",
      "Struct Implicit.[21] We use their open-sourced architecture and code for comparision with our\r\n",
      "mode. We train their model using our datasets from scratch. We change the value of number of slots\r\n",
      "in their model based on the maximum number of parts in the dataset.\r\n",
      "\r\n",
      "uORF.[67] We use their open-sourced architecture, code and pretrained checkpoints for compari-\r\n",
      "sion with our model on the CLEVR dataset. For RoomDiverse++ dataset, we train their model on\r\n",
      "our dataset using the hyperparmeters they use for training their model on RoomDiverse dataset. We\r\n",
      "change the value of number of slots in their model similar to our model for each dataset.\r\n",
      "\r\n",
      "Slot Attention.[42] We use their open-sourced architecture and code for comparision with our\r\n",
      "model on the CLEVR and RoomDiverse++ dataset. We train their model using our datasets from\r\n",
      "scratch. We change the value of number of slots in their model similar to our model for each dataset.\r\n",
      "\r\n",
      "8   More Results\r\n",
      "Segmenting 3D pointclouds without segmentation supervision. In this section we show ad-\r\n",
      "ditional qualitative and quantitative results for Section 4.1.1 of the main paper. We follow the\r\n",
      "experimental setup of Section 4.1.1, where we use the generic part dataset (visualized in Fig 7) for\r\n",
      "primitive learning. In Table 4, we further extend Table 1 of the main paper to include different levels\r\n",
      "in Chair and Table category. Here (X/Y /Z) refers to (level 1/level 2/level 3) scores respectively. We\r\n",
      "pick the best performing level in fast inference and specifically report it’s slow inference results. We\r\n",
      "further qualitatively compare our model against Shape2Prog (best performing baseline) in Figure 13\r\n",
      "\r\n",
      "                                     in-dist (Chairs)                  out-of-dist (Tables)\r\n",
      "      Method\r\n",
      "                              Fast Infer.         Slow Infer.        Fast Infer.      Slow Infer.\r\n",
      "      Shape2Prog [60]         0.28/0.21/0.26          0.53        0.21/0.23/0.23          0.40\r\n",
      "      PQ-Nets [64]            0.20/0.18/0.16          0.31        0.17/0.14/0.16          0.21\r\n",
      "      StructImplicit [21]     0.18/0.22/0.25          0.23        0.14/0.15/0.17          0.17\r\n",
      "      GFS-Nets                0.51/0.48/0.57          0.62        0.51/0.55/0.60          0.69\r\n",
      "\r\n",
      "Table 4: ARI Segmentation accuracy (higher the better) in the test set of Chair (in-distribution) and Table\r\n",
      "category of PartNet(out-of-distribution). GFS-Nets significantly outperform all of the baselines.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                    20\r\n",
      "\f",
      "\n",
      "\n",
      "RGB image segmentation and view prediction without supervision In Figure 15, we show more\r\n",
      "qualitative results for Section 4.3 of the main paper, where we qualitatively compare our model for\r\n",
      "segmentation against uORF[67] and Slot Attention [42]. We see that slow inference prevents our\r\n",
      "model from missing to detect occluded objects in the scene, where as the baselines have a lot of False\r\n",
      "Positives i.e missed detections.\r\n",
      "\r\n",
      "                        Input          Shape2Prog          GFS-Nets-Fast   GFS-Nets-Slow\r\n",
      "                                       (Supervised)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "        Figure 13: Additional shape decomposition results using generic primitives. (Section 4.1.1)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                      21\r\n",
      "\f",
      "\n",
      "\n",
      "                                    Instance Segmentation\r\n",
      "\r\n",
      "\r\n",
      "                  3D-DETR          GFS-Nets-Slow        3D-DETR        GFS-Nets-Slow\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Figure 14: Additional shape decomposition results using category-specific primitives. (Section 4.1.2)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                   22\r\n",
      "\f",
      "\n",
      "\n",
      "          Ground Truth   Slot Attention   uORF   GFS-Nets-Fast GFS-Nets-Slow\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Figure 15: Additional RGB segmentation results on RoomDiverse++. (Section 4.3)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                          23\r\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "print(array_authors[0])\n",
    "print(array_pdf_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1220700",
   "metadata": {},
   "source": [
    "### Extract all word after the term: \"References\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e6468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                   Generating Fast and Slow:\\r\\n             Scene Decomposition via Reconstruction\\r\\n\\r\\n\\r\\n  Mihir Prabhudesai1      Anirudh Goyal2                  Deepak Pathak1         Katerina Fragkiadaki1\\r\\n               1                                           2\\r\\n                 Carnegie Mellon University                  Mila, University of Montreal\\r\\n           {mprabhud, dpathak, katef}@cs.cmu.edu                  anirudhgoyal9119@gmail.com\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 1: Point-cloud and image decompositions with Generating Fast and Slow Networks (GFS-Nets).\\r\\nGFS-Nets parse completely novel scenes into familiar entities via slow inference, i.e., gradient descent on the\\r\\nreconstruction error of the scene example under consideration. Left: GFS-Nets outperform a state-of-the-art\\r\\n3D-DETR detector by 50% in segmentation accuracy in out-of-distribution 3D point clouds, when trained on\\r\\nthe same training data. Right: GFS-Nets outperform state-of-the-art unsupervised generative models of Slot\\r\\nAttention [42] and uORF [67] in RGB image decomposition.\\r\\n\\r\\n                                                 Abstract\\r\\n         We consider the problem of segmenting scenes into constituent entities, i.e. under-\\r\\n         lying objects and their parts. Current supervised visual detectors though impressive\\r\\n         within their training distribution, often fail to segment out-of-distribution scenes\\r\\n         into their constituent entities. Recent slot-centric generative models break such\\r\\n         dependence on supervision, by attempting to segment scenes into entities unsuper-\\r\\n         vised, by reconstructing pixels. However, they have been restricted thus far to toy\\r\\n         scenes as they suffer from a reconstruction-segmentation trade-off: as the entity\\r\\n         bottleneck gets wider, reconstruction improves but then the segmentation collapses.\\r\\n         We propose GFS-Nets (Generating Fast and Slow Networks) that alleviate this issue\\r\\n         with two ingredients: i) curriculum training in the form of primitives, often missing\\r\\n         from current generative models and, ii) test-time adaptation per scene through gra-\\r\\n         dient descent on the reconstruction objective, what we call slow inference, missing\\r\\n         from current feed-forward detectors. We show the proposed curriculum suffices\\r\\n         to break the reconstruction-segmentation trade-off, and slow inference greatly\\r\\n         improves segmentation in out-of-distribution scenes. We evaluate GFS-Nets in 3D\\r\\n         and 2D scene segmentation benchmarks of PartNet, CLEVR, Room Diverse++,\\r\\n         and show large (∼ 50%) performance improvements against SOTA supervised\\r\\n         feed-forward detectors and unsupervised object discovery methods.\\r\\n\\r\\nPreprint. Under review.\\r\\n\\x0c\\n\\n1    Introduction\\r\\n\\r\\nScenes are composed of objects and objects are composed of parts, and this compositional organization\\r\\nof the perceptual representations is considered a critical component for the level of combinatorial\\r\\ngeneralization that humans are capable of, that extends far beyond their direct experiences [25].\\r\\nEven when encountered with a truly novel image, humans try to parse it in terms of known entity\\r\\ncomponents [26]. Such an entity-centric understanding or reasoning of scenes allows humans to\\r\\ngeneralize known concepts to out-of-distribution examples [25, 39, 44].\\r\\nAn expensive way to build this entity-centric understanding is to supervise the part parsing using\\r\\nhuman labels for training object and parts detection or segmentation in images and 3D point clouds [56,\\r\\n69, 5, 71]. However, these part detectors are category specific and often fail outside the training\\r\\ndistribution [29, 19, 2]. Consider the abstract shape shown in Figure 1 (last row on the left). We can\\r\\nintuitively figure out the meaningful entities that this shape could be broken into. Yet, a state-of-the-art\\r\\ntransformer based 3D-DETR detector [5], when trained supervised for segmenting chairs, fails to\\r\\ndecompose these abstract shapes into entities, even though these shapes contain familiar (chair) parts.\\r\\nAn underlying issue is that these models do not receive any top-down feedback [3] from entities, i.e.,\\r\\nthey do not have any entity-centric reasoning rather, they are purely feed-forward in nature, i.e., the\\r\\ninformation is routed from input to output layers in a sequential manner. How do we enable top-down\\r\\nfeedback between the scene and its constituent parts?\\r\\nThere has been interest recently in building models that segment scenes into entities in an unsupervised\\r\\nway by optimizing a reconstruction objective [42, 17, 24, 63, 57, 35, 16, 4, 23, 68, 55, 13, 22, 73, 34].\\r\\nThese methods differ in details but share the notion of having a fixed set of entities, also known as\\r\\nslots or object files. Each slot extracts information about a single entity during encoding, and it is\\r\\n“rendered\" back to the input during decoding, thus, exhibiting top down feedback. Yet, it has been\\r\\nhard to scale the results of these models beyond toy datasets.\\r\\nWe argue this is because these models answer two questions at once: understanding what the entity is\\r\\nand where it is present in the input. This creates a chicken-and-egg problem because unless we have\\r\\na representation for an entity, we can’t find where it is in the scene, and unless we localize the entity,\\r\\nwe can’t learn a representation of what it is. Hence, a trivial solution often encountered is to assign\\r\\none slot to the input and other slots to nothing, as noted in Engelcke et al. [15] and also confirmed by\\r\\nour experiments in Section 4.1.1 and 4.2.\\r\\nWe propose Generating Fast and Slow Networks (GFS-Nets), a slot-centric autoencoder that segments\\r\\nscenes while optimizing a reconstruction objective. Our model builds on top of Slot Attention[42],\\r\\nwith two key insights: First, it uses curriculum to learn the what question (what entities are) before\\r\\ngoing to the where question (where they are) it does this by training the model to autoencode primitive\\r\\nentities using a single slot bottleneck. This collection of primitive entities can either be collected\\r\\nby design [60] or can be taken from a collection of part segmentations [49]. Second, inference in\\r\\nGFS-Nets is performed in two ways: i) Fast inference where a visual scene is simply encoded and\\r\\ndecoded and no model weights are updated. ii) Slow inference where the weights of the model are\\r\\nadapted by gradient descent while optimizing the reconstruction objective. In this way, GFS-Nets\\r\\nadapts to a truly novel scene by projecting it onto a learned distribution of entities, while also\\r\\nslowly adapting the entity distribution. Slow inference in GFS-Nets successfully parses completely\\r\\nunfamiliar scenes into familiar entities, implementing a form of search for explanations of the scene\\r\\nat inference, as we show in Figure 3.\\r\\nOur slow inference is related to test-time adaptation or optimization [72, 54, 59, 58, 74, 65]. However,\\r\\ntest-time adaptation has not been used so far in entity-centric models to improve scene decomposition.\\r\\nIn fact, it has been recently noted that reconstruction loss can make entity segmentation worse, to\\r\\nquote Engelcke et al. [15], “If the bottleneck is too narrow, segmentation and reconstruction degrade.\\r\\nIf it is too wide, reconstruction is reasonable but the model collapses to a single component and no\\r\\nuseful segmentation is learned.\" GFS-Nets alleviate from this issue by coupling curriculum training\\r\\nwith slow inference for entity-centric learning.\\r\\n\\r\\n    Project page: https://mihirp1998.github.io/project_pages/gfsnets/\\r\\n\\r\\n\\r\\n                                                     2\\r\\n\\x0c\\n\\nWe test GFS-Nets on the following datasets: PartNet [49], ShapeNet[7], CLEVR [30] and a difficult\\r\\nversion of Room Diverse [67]. We evaluate GFS-Nets’s ability to parse out-of-distribution scenes\\r\\nand compare it against state of the art entity-centric generative models [42, 67], program synthesis\\r\\nmodels [60], 3D unsupervised part discovery models [21, 64] and state of the art supervised visual\\r\\ndetectors [5, 43] trained with labeled data to segment entities. We show improvements across all\\r\\nbaselines in our ability to segment novel scenes. Additionally, we ablate different design choices of\\r\\nGFS-Nets. We will make our code and datasets publicly available to the community.\\r\\n\\r\\n2   Related Work\\r\\nEntity-centric generative models for scene and structure decomposition Entity-centric mod-\\r\\nels attempt to segment a scene into objects and parts while autoencoding images. MONet[4],\\r\\nGENESIS[16] and IODINE[23] perform multiple steps between their encoder and the decoder to\\r\\noutput a set of independent latent vectors that describe objects in the image. This iterative encode-\\r\\ndecode inference helps them in receiving top-down feedback. However, this also results in making\\r\\nthe system computationally inefficient and inflexible. Slot Attention [42] on the other hand receives\\r\\ntop-down feedback within a single encoding step. The idea of having a single-encode step makes\\r\\nthe process of extracting out symbols modular and computationally efficient. However, all methods\\r\\nabove fail on non-toy scenes. We attribute this failure to the what or where problem, a.k.a. symmetry\\r\\nproblem, pointed out in the ‘Instance Slots’ Section in Greff et al.[25]: “bottom-up information is\\r\\nunable to break the symmetry amongst slots and thus ends up assigning the same content to each\\r\\none.\" In GFS-Nets, we propose to circumvent this problem using curriculum training.\\r\\n\\r\\nShape program synthesis and analysis-by-synthesis GFS-Nets is also related to works in\\r\\nanalysis-by-synthesis [37], program synthesis for shape prediction [60, 14, 41], as well as ear-\\r\\nlier works on Computer Vision, such as Marr’s 3D sketch [45] which involves representing a scene\\r\\nin terms of generalized cylinders and their syntactic relations to each other. In place of data-driven\\r\\nMarkov Chain Monte Carlo search of analysis-by-synthesis methods that require good initialization,\\r\\nour slow inference searches in the space of primitives by gradient descent. In contrast to program\\r\\nsynthesis methods, it does not require a predefined domain-specific language (DSL) or program\\r\\nannotations for visual structures [41], rather, it discovers compositions over primitives via its slow\\r\\ninference.\\r\\n\\r\\nUnsupervised 3D Part Discovery There are numerous methods that attempt the decomposition of\\r\\ncomplex 3D shapes into primitive parts without primitive supervision[33, 21, 51, 18, 12, 62, 11, 9].\\r\\nTraditional primitives include cuboids [62, 50], superquadrics [53, 51], and convexes [11, 8]. [20]\\r\\nproposes a 3D representation that decomposes space into a structured set of implicit functions [21].\\r\\nNeural Parts [52] represents arbitrarily complex genus-zero shapes and thus yields comparatively\\r\\nexpressive parts. However the resulting parts of these methods[20, 52] are still not semantically\\r\\nmeaningful and the decomposition is highly dependent on the number of parts initialized. The work\\r\\nof [66] does 3D part reconstruction directly from a 2D image input without access to any ground-truth\\r\\n3D shapes for training. However, both [66] and [52] take as input the number of parts, and different\\r\\ndecompositions are predicted with varying part numbers. There is no clear way to select the right\\r\\nnumber of parts. In our case, parts can be quite complex: pairs of parallel surfaces, quadruplets of\\r\\nlegs, as we use implicit functions to represent them. Moreover GFS-Nets’s dynamic attention-based\\r\\nrouting allows it to infer different number of parts for each input scene.\\r\\n\\r\\n3   Generating Fast and Slow (GFS-Nets)\\r\\nThe goal of GFS-Nets is to decompose an unfamiliar scene into familiar entities. The model encodes\\r\\nthe scene, which can be a 3D point cloud or an RGB image, into a set of slot vectors and decodes\\r\\nback the scene using a decoder shared across slots. The model uses the slot attention feature-to-slot\\r\\nmapping of Locatello et al.[42], where visual features are softly partitioned across slots through\\r\\nattention, and update the slot vectors.\\r\\nGFS-Nets is trained with a curriculum. First, it is first trained to autoencode individual entities using\\r\\na single slot in their bottleneck, as shown in the left of Figure 2 and described in Section 3.2. Next,\\r\\nGFS-Nets is trained to autoencode complex scenes using multiple slots in their bottleneck, as shown\\r\\n\\r\\n\\r\\n                                                   3\\r\\n\\x0c\\n\\nStage 1: Autoencoding primitive entities                                                                        Stage 2: Autoencoding scenes\\r\\n\\r\\n3D Object Decomposition                                                                                                                                       Competition\\r\\n                                                                                                                                                             Amongst Slots\\r\\n                                                                                                                                                                         sampling N slots\\r\\n                                                       sampling 1 slot\\r\\n                    Point                                                                                                             Point                                                   slot\\r\\n                                                                                                                                                                                               slot\\r\\n                                                                           slot                                                                                                                 slot         Max-Pool\\r\\n                 Transformer                                                                                                       Transformer                                              decoder\\r\\n                                                                                                                                                                                                  slot\\r\\n                                                                                                                                                                                             decoder\\r\\n                                                                         decoder                                                                                                              decoder\\r\\n                                                                                                                                                                                                    slot\\r\\n                   Encoder                                                                                                           Encoder                                                   decoder\\r\\n                                                                                                                                                                                                 decoder\\r\\n\\r\\n\\r\\n\\r\\n                                      reconstruction loss                                                                                                          reconstruction loss\\r\\n\\r\\n 2D Scene Decomposition\\r\\n                                Competition                                                                                                             Competition\\r\\n                               Amongst Slots Foreground                                                                                                Amongst Slots Foreground\\r\\n                                         sampling 1 slot                                                                                                            sampling N slots\\r\\n                                                             slot                                                                                                                        slot\\r\\n                                                                                                                                                                                          slot             Weighted\\r\\n                                                               slot                                                                                                                        slot\\r\\n                                                           decoder                 Weighted                                                                                            decoder\\r\\n                                                                                                                                                                                            slot\\r\\n                                                                                                                                                                                       decoder             Average\\r\\n           CNN                                              decoder                                                                    CNN                                              decoder\\r\\n                                                                                                                                                                                              slot\\r\\n                                                                                   Average                                                                                               decoder\\r\\n                                                                                                                                                                                           decoder\\r\\n\\r\\n                                        sampling 1 slots                                                                                                        sampling 1 slots\\r\\n                                                     Background                                                                                                                 Background\\r\\n\\r\\n                                      reconstruction loss                                                                                                             reconstruction loss\\r\\n\\r\\n\\r\\nFigure 2: Stages of training in GFS-Nets. Left: Autoencoding primitive entities. Primitive entities are\\r\\npoint clouds of individual parts in 3D, and single object RGB images in 2D. GFS-Nets use one and two slots\\r\\nrespectively, for primitive entity autoencoding in 3D point clouds and RGB images. Right: Autoencoding scenes.\\r\\nScenes can be 3D points clouds of whole object or multi-object RGB images. In this stage, GFS-Nets use more\\r\\nslots in their bottleneck. During inference, some slots get mapped to nothing in the input scene. In this way,\\r\\nGFS-Nets is able to infer decompositions with a varying number of entities, in an unsupervised manner.\\r\\n\\r\\n\\r\\n\\r\\non the right of Figure 2 and described in Section 3.3. We first describe slot attention of [42] in Section\\r\\n3.1, as well as encoders and decoders for 3D point clouds [70, 40] and RGB images [47, 67] which\\r\\nour method builds upon.\\r\\n\\r\\n3.1     Background\\r\\n\\r\\n3.1.1     Routing from input to slots with iterative attention\\r\\n\\r\\nMany approaches instantiate slots (a.k.a. query vectors) from 2D visual feature maps or 3D point\\r\\nfeature clouds [5, 48]. Most works use standard cross attention operations [61, 22, 42] or iterative\\r\\ncross (features to slots) and self-attention (slot-to-slots) operations [5] to map a set of N input feature\\r\\nvectors to a set of K slot vectors. Competition amongst slots and iterative routing introduced in\\r\\n[22, 42] encourages the slots not to encode information in a redundant manner.\\r\\nGiven a visual scene encoded as a set of feature vectors M ∈ RN ×C and K randomly initialized slots\\r\\nsampled from a multivariate Gaussian distribution with a diagonal covariance s ∼ N (µ, Diag(σ 2 )) ∈\\r\\nRK×D , where µ, σ ∈ RC are learnable parameters of the Gaussian, Slot Attention[42] computes an\\r\\nattention map a between the feature map M and the slots s :\\r\\n\\r\\n                                                         a = \\\\mathrm {Softmax}(\\\\key (\\\\MM ) \\\\cdot \\\\query (s)^T, \\\\textrm {axis=1}) \\\\in \\\\mathbb {R}^{N \\\\times K}. \\\\label {eq:1}                                            (1)\\r\\n\\r\\nk, q, and v are learnable linear transformations that map inputs and slots to a common dimension\\r\\nD. The softmax normalization over slots ensures competition amongst them to attend to a specific\\r\\nfeature vector in M. We then generate the update vector for each slot given the computed attention\\r\\nand input feature maps:\\r\\n\\r\\n                                                    updates = a^T v(M) \\\\in \\\\mathbb {R}^{K \\\\times C}, \\\\textrm {where} \\\\ a_{i,1} = \\\\dfrac {a_{i,1}}{\\\\sum _{i=0}^{N}a_{i,1}}                                               (2)\\r\\n\\r\\n\\r\\nwhich we then use to do a gated update on each slot feature using a GRU[10]: slots = GRU(state =\\r\\nslots, input = updates). We iterate over the steps of calculating attention and updating the slots for\\r\\n3 timesteps.\\r\\nSince the slot vectors are sampled from a shared Gaussian distribution, slot attention[42] can in-\\r\\nstantiate a variable number of slots in different scenes, in contrast to DETR encoder [5] where a\\r\\nconstant number of (deterministic) slot vectors are learned and used in each scene. For a more\\r\\ndetailed description, we refer the reader to [42].\\r\\nSlot attention is one of many ways of mapping inputs to slots. GFS-Nets is agnostic to the method of\\r\\nmapping visual features to slot vectors and we ablate different encoders in our experiment section\\r\\n4.1.2.\\r\\n\\r\\n\\r\\n                                                                                                                4\\r\\n\\x0c\\n\\n3.1.2    Encoding and decoding 3D point clouds\\r\\n\\r\\nGFS-Nets featurize a 3D point cloud using a 3D point transformer [70] which maps the 3D input\\r\\npoints to a set of M feature vectors of C dimensions each. We set M to 128 and C to 64 in our\\r\\nexperiments.\\r\\nGFS-Nets decodes 3D point clouds from each slot using implicit functions [46]. Specifically, each\\r\\ndecoder takes in as input the slot vector si and an (X, Y, Z) location and returns the corresponding\\r\\noccupancy score ox,y,z = Dec(si , (x, y, z)) , where Dec is a multi-block ResNet MLP similar to that\\r\\nof Lal et al. [40].\\r\\n\\r\\n3.1.3    Encoding and decoding RGB images\\r\\n\\r\\nGFS-Nets featurize an RGB image concatenated with its pixel coordinates using a convolutional\\r\\nneural network (CNN) and produce a feature map of H × W × C where H, W are the spatial\\r\\ndimensions and C is the feature dimension. We reshape the output feature map to a set of vectors\\r\\nM ∈ R(H×W )×C . We set C as 64.\\r\\nGFS-Nets decodes RGB pixels from each slot using conditional NeRFs [47] that predict the color\\r\\nc and volume density σ at each 3D location of the latent 3D volume. Thus given slot vector\\r\\nsi , spatial location x and viewing direction d, the MLP based decoder network Dec predicts\\r\\n(c, σ) = Dec(si , (x, d)). The color c and density σ of the points are composed together across\\r\\n                                                               PK             PK\\r\\nslots using density σ weighted averaging. Specifically, σ̂ = i=0 wi σi , ĉ = i=0 wi ci where\\r\\n            PK\\r\\nwi = σi / i=0 σi . We then use differentiable volume rendering of the composed σ̂ and ĉ using\\r\\ncamera ray casting to generate the final RGB prediction similar to [67].\\r\\n\\r\\n\\r\\n3.2     Autoencoding primitive entities\\r\\n\\r\\nIn this stage, we train GFS-Nets using a set of primitive entities. Primitive entities are generic or\\r\\nsemantic parts of objects in 3D point clouds or single object multi-view images, as shown on the left\\r\\nof Figure 2. We experiment with different primitive sets in the experimental section.\\r\\nFor encoding 3D point cloud primitive entities, we sample a single slot in the entity bottleneck of\\r\\nGFS-Nets, that is, s ∈ R1×D . For encoding a 2D RGB image, we sample two slots, one from a\\r\\nforeground learnable Gaussian distribution and one from a background learnable Gaussian distribution,\\r\\nthat is s ∈ R2×D . In a forward pass of the encoder, we then update slots s as described in Section 3.1.\\r\\n\\r\\n\\r\\n3.3     Autoencoding scenes\\r\\n\\r\\nIn this stage, we initialize the parameters of the model\\r\\nwith the parameters learnt in the previous section. The\\r\\nonly difference is that instead of instantiating one or two\\r\\nslots we now instantiate K slot vectors, where\\r\\nK is the maximum number of entities any example in the\\r\\ndataset could possibly have. For RGB input, we sample K\\r\\nslots from the foreground Gaussian and one slot from the\\r\\nbackground Gaussian since there can be many foreground\\r\\n                                                                Input & Target   Slow Inference\\r\\nobjects but only a single background.\\r\\nFinally we pass each of our slot vectors through the slot de-   Figure 3: Slow Inference in GFS-Nets:\\r\\ncoder outk = Dec(sk ), whose weights are shared across          First columns shows the input point cloud.\\r\\nslots. For decoding RGB images, GFS-Nets uses separate          In 2nd to 4th columns, we show the segmen-\\r\\ndecoders for foreground and background. For 3D point            tation results throughout training iterations\\r\\ncloud input, outk represents the probability of sampled         of the autoencoding process. Segmentation\\r\\n                                                                improves over time with reconstruction qual-\\r\\npoints being occupied by a given slot sk ; we aggregate         ity, despite that the autoencoding loss only\\r\\nthem by max pooling across the slot dimension. We then          penalizes the reconstruction error.\\r\\nuse binary cross entropy loss between the ground truth\\r\\noccupancies and the predictions.\\r\\n\\r\\n\\r\\n                                                    5\\r\\n\\x0c\\n\\nFor RGB input, outk represents the color and density of\\r\\npoints being occupied by a given slot sk ; we aggregate across slots as described in Section 3.1 and a\\r\\nuse pixel mean squared error loss.\\r\\nFast and Slow Inference Fast inference refers to a single forward pass through the trained net-\\r\\nwork. Slow inference refers to test-time adaptation of all the learnable parameters of the model for\\r\\nautoencoding a single scene using the method described in Section 3.3. Our experiment show that\\r\\nGFS-Nets with slow inference always achieves much better scene decomposition than GFS-Nets with\\r\\nfast inference, especially in out-of-distribution scenes. In contrast, slow inference on entity-centric\\r\\ngenerative models without curricula, such as [42], results in improved reconstruction accuracy but\\r\\nworse scene decomposition performance, due to the reconstruction-segmentation trade off noted in\\r\\n[15].\\r\\n\\r\\n4     Experiments\\r\\nWe test GFS-Nets in the tasks of segmenting object parts in 3D point clouds and segmenting objects\\r\\nand predicting views in 2D multiview images under varying amounts of label supervision. We compare\\r\\nGFS-Nets against previous state-of-the-art models in each task. We further quantify contribution of\\r\\nvarious design choices, namely, slow inference through reconstruction feedback, curriculum learning,\\r\\nand encoder architecture. We evaluate segmentation performance on scenes within the training\\r\\ndistribution as well as out-of-distribution scenes.\\r\\nIn each setup, our experiments aim to answer the following questions:\\r\\n\\r\\n         • How does GFS-Nets compare against state-of-the-art 2D and 3D scene decomposition\\r\\n           models ([43, 67, 5]) under varying amounts of label supervision?\\r\\n         • How much, if any, slow inference through reconstruction feedback improves segmentation\\r\\n           accuracy in GFS-Nets and its variants?\\r\\n         • How much curriculum or supervision during training contributes to segmentation perfor-\\r\\n           mance?\\r\\n\\r\\n4.1     Segmenting parts in 3D object point clouds\\r\\n\\r\\nIn this task, the input is a complete 3D point cloud of an object, and the goal is to segment the 3D\\r\\npoint cloud into different semantic part instances. We consider two setups with different segmentation\\r\\nsupervision:\\r\\n\\r\\n        1. Segmenting without any part segmentation labels.\\r\\n        2. Segmenting with part segmentation labels from a related object category.\\r\\n\\r\\n4.1.1    Segmenting 3D pointclouds without segmentation supervision\\r\\nIn this setup, we test our model and baselines in segmenting the test objects of the Chair and Table\\r\\ncategories in the PartNet benchmark [49] without access to any ground-truth 3D segmentations at\\r\\ntraining time. For training, we provide our model and baselines access to unsegmented objects of the\\r\\nChair category in the trainset of the PartNet benchmark. We evaluate the segmentation performance\\r\\nof our model and baselines with the Adjusted Random Index (ARI) [28] using the implementation of\\r\\nKabra et al. [32], which calculates the similarity between two-point clusters while being invariant to\\r\\nthe ordering of the cluster centers. Neither our model nor the baselines have access to ground-truth\\r\\n3D segmentations during training; as a result, they may output 3D parts of coarser or finer resolution.\\r\\nPartNet contains three different levels of ground-truth segmentation labels with progressively finer\\r\\nsegmentation granularity. We evaluate the ARI score across all three levels of PartNet and report the\\r\\nbest of three for all the models.\\r\\nOur model and two amongst three of our baselines, specifically (PQ-Nets[64] of Wu et al. and\\r\\nShape2Prog [60] of Tian et al.) expect access to a set of 3D primitive parts during training. In this\\r\\nsetup, we consider the primitive part dataset introduced by Shape2Prog [60]. The dataset consists of\\r\\ndifferently sized cubes, cuboids, and discs; we call it the generic primitive set since it is not related\\r\\nto any semantic object category but rather consists of generic 3D parts that remind of generalized\\r\\n\\r\\n\\r\\n                                                   6\\r\\n\\x0c\\n\\ncylinders of Marr et al.[45]. Please refer to the supplementary file for visualizations of the generic\\r\\nprimitive set.\\r\\nBaselines: We compare our model against the following baselines: (i) PQ-Nets of Wu et al.[64]\\r\\nassume access to a set of primitive 3D parts, similar to our model, and learn a primitive part\\r\\nautoencoder. Whole object autoencoding uses a sequential model that encodes the 3D point cloud\\r\\ninto a 1D latent vector and sequentially decodes parts using the part decoder. We use the publicly\\r\\navailable code to train the model to autoencode the generic primitive set and the unlabelled 3D point\\r\\nclouds of the instances of the Chair category in PartNet. (ii) Shape2Prog of Tian et al.[60] is a shape\\r\\nprogram synthesis method that is trained supervised to predict shape programs from object 3D point\\r\\nclouds. The program represents the part category, location, and the symmetry relations among the\\r\\nparts (if any). Shape2Prog introduced two synthetically generated datasets that helped the model\\r\\nparse 3D pointclouds from ShapeNet [6] into shape programs without any supervision: i) the generic\\r\\nprimitive set we discussed earlier which they use to train their part decoders, and ii) a synthetic whole\\r\\nshape dataset of chairs and tables generated programmatically alongside its respective ground-truth\\r\\nprograms. Their model requires supervised pre-training on the dataset of synthetic whole shapes\\r\\npaired with programs. We therefore use their publicly available model weights trained on synthetic\\r\\nwhole shapes to further train on PartNet Chairs. Note that no other baseline nor GFS-Nets assumes\\r\\naccess to the synthetic whole shape dataset. (iii) StructImplicit of Genova et al.[21] encodes the\\r\\nobject point cloud into a 1D latent and then uses an MLP to map it to a fixed number of part vectors,\\r\\neach represented with a 3D implicit function. We use the publicly available code to train the model\\r\\non the unlabelled Chair PartNet dataset. We set the number of part vectors based on the maximum\\r\\nnumber of parts any example in the dataset can have.\\r\\n\\r\\n                                                   in-dist (Chairs)                  out-of-dist (Tables)\\r\\n        Method\\r\\n                                             Fast Infer.         Slow Infer.      Fast Infer.       Slow Infer.\\r\\n        Shape2Prog [60]                         0.28                0.53             0.23                 0.40\\r\\n        PQ-Nets [64]                            0.20                0.31             0.17                 0.21\\r\\n        StructImplicit [21]                     0.25                0.23             0.17                 0.17\\r\\n        GFS-Nets w/o curriculum                 0.41                0.35             0.47                 0.38\\r\\n        GFS-Nets-PrimitiveOnly                  0.42                0.48             0.49                 0.57\\r\\n        GFS-Nets                                0.57                0.62             0.60                 0.69\\r\\n\\r\\nTable 1: ARI Segmentation accuracy (higher the better) in the test set of Chair (in-distribution) and Table\\r\\ncategory of PartNet(out-of-distribution). GFS-Nets significantly outperform all of the baselines.\\r\\n\\r\\nWe consider the following ablative versions of GFS-Nets: (i) GFS-Nets w/o curriculum is trained\\r\\nwithout the primitive learning stage. Instead, it is trained to directly autoencode the aggregated dataset\\r\\nof both the generic primitive set and 3D point clouds of training chairs (as described in Section 3.3).\\r\\nThis version of our model coincides precisely with the previous work of Slot Attention of Locatello\\r\\net al. [42]. (ii) GFS-Nets-PrimitiveOnly is trained only on the generic primitive set (as described in\\r\\nSection 3.2) and is not trained to autoencode and 3D point clouds of training chairs.\\r\\n                                                                                      Input     3D-DETR   Learning to Group   Ours\\r\\n\\r\\nAny model that has a decoder that reconstructs the input\\r\\ncan be additionally trained with slow inference with re-\\r\\nconstruction feedback at each test scene independently.\\r\\nGFS-Nets, PQ-Nets [64], Shape2Prog [60], and StructIm-\\r\\nplicit [21] are all equipped with such decoders. We thus   Input  Shape2Prog [62] PQ-Nets [66] Structured Implicit [22] Ours\\r\\n\\r\\ntest both our model and the baselines in both modes of\\r\\nfast (regular) and slow inference; we use the notation Figure 4: Unsupervised 3D segmentations of\\r\\n[modelname]-Slow and [modelname]-Fast to denote the out-of-distribution scenes for GFS-Nets and\\r\\n                                                         baselines.\\r\\nmodel with the corresponding inference type.\\r\\nWe show quantitative and qualitative results of our model\\r\\nand the baselines in Table 4 and Figure 4 respectively. From the Table 4 we draw the following\\r\\nconclusions:\\r\\nSegregation into slot like entities using attention helps. GFS-Nets significantly outperform PQ-\\r\\nNets [64] and StructImplicit [21] that map the input object 3D pointcloud into a 1D latent vec-\\r\\n\\r\\n\\r\\n                                                             7\\r\\n\\x0c\\n\\ntor instead of maintaining segregated representations into multiple slot latent vectors. Moreover,\\r\\nGFS-Nets’s ability to dynamically route information per example via attention allows it to automati-\\r\\ncally estimate the number of parts required per input 3D pointcloud due to which, different instances\\r\\ncan have a different number of active slots, while StructImplicit segregates all object instances into a\\r\\nconstant number of parts, which is a hyperparameter of the model.\\r\\nCurriculum training helps. GFS-Nets-Fast outperform by a large margin GFS-Nets w/o curriculum-\\r\\nFast, the version of GFS-Nets that uses exactly the same training data and inference method, but is\\r\\ntrained without curriculum.\\r\\nSlow inference through reconstruction feedback helps in the presence of curriculum and hurts\\r\\nin the absence of it. GFS-Nets, GFS-Nets-PrimitiveOnly, PQ-Nets and Shape2Prog all have cur-\\r\\nriculum through a primitive learning stage, while GFS-Nets w/o curriculum and StructImplicit\\r\\ndo not. GFS-Nets-Slow outperform GFS-Nets-Fast and GFS-Nets-PrimitiveOnly-Slow outperform\\r\\nGFS-Nets-PrimitiveOnly-Fast, PQ-Nets-Slow outperform PQ-Nets-Fast and ShapeProg-Slow outper-\\r\\nform ShapeProg-Fast. In contrast, GFS-Nets w/o curriculum-Slow performs worse than GFS-Nets\\r\\nw/o curriculum-Fast and StructImplicit-Slow does worse than StructImplicit-Fast. Such trade-off\\r\\nbetween reconstruction and segmentation in generative model for scene decomposition has been\\r\\npointed out by Engelcke et al. [15] and our results verify their findings.\\r\\nUnsupervised autoencoding of complex (non primitive) 3D pointclouds helps. GFS-Nets outper-\\r\\nforms GFS-Nets-PrimitiveOnly.\\r\\nPlease see the supplementary file for more experimental details. We visualize intermediate iterations\\r\\nduring slow inference in our supplementary video.\\r\\n\\r\\n4.1.2    Segmenting 3D pointclouds with weak segmentation supervision\\r\\nIn this setup, we test our model and baselines in segmenting the test objects of the Chair and Table\\r\\ncategories in the PartNet benchmark [49], with access to ground-truth pointcloud segmentation\\r\\nlabels of level-3 in the training 3D pointclouds of the Chair category in PartNet. We use the ARI\\r\\nsegmentation metric to evaluate the level 3 ground-truth labels of PartNet for our model and baselines.\\r\\nGFS-Nets assume access to a set of 3D primitive parts for primitive entity learning. In this setup, we\\r\\nconsider each 3D part from each training chair 3D pointcloud example as a separate 3D primitive. We\\r\\naugment these primitives by random rotations and translations; we call it the semantic primitive set.\\r\\n\\r\\n                                                            in-dist (Chair)                   out-dist (Table)\\r\\n        Method\\r\\n                                                 Fast Infer.            Slow Infer.       Fast Infer.   Slow Infer.\\r\\n        3D-DETR [5]                                  0.67                   n/a              0.41           n/a\\r\\n        Learning2Group [43]                          0.62                   n/a              0.46           n/a\\r\\n        GFS-Nets w/o supervision                     0.40                  0.44              0.31          0.48\\r\\n        GFS-Nets w/o curriculum                      0.61                  0.66              0.48          0.60\\r\\n        GFS-Nets w/o Gaussian                        0.65                  0.62              0.38          0.58\\r\\n        GFS-Nets w/o SlotAttention                   0.58                  0.55              0.31          0.44\\r\\n        GFS-Nets                                     0.64                  0.67              0.51          0.63\\r\\n\\r\\nTable 2: ARI Segmentation scores (higher the better) on the test set of the Chair category (in-distribution)\\r\\nand on the test set of the Table category (out-of-distribution). All the models are trained using the training set of\\r\\nthe Chair category in PartNet.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n               Input    3D-DETR [5]   Learning2Group [45]    Ours       Cupboard   Lamp      Display    Laptop\\r\\n\\r\\n\\r\\nFigure 5: Out-of-distribution 3D segmentation results for GFS-Nets and baselines. Left: 3D segmentation\\r\\nresults on out-of-distribution shapes for GFS-Nets and baselines. Right: 3D segmentation results for out-of-\\r\\ndistribution categories of PartNet for GFS-Nets trained semi-supervised only on the Chair category.\\r\\n\\r\\n\\r\\n\\r\\n                                                                    8\\r\\n\\x0c\\n\\nBaselines We compare our model against the following baselines: (i) Learning2Group of Luo\\r\\net al. [43] progressively groups points into segments by learning pairwise grouping decisions pa-\\r\\nrameterized by features of the two point clusters. Given the intermediate grouping decisions are not\\r\\nsupervised, and the non-differentiability of their grouping functions they use reinforcement learning\\r\\ngradients for training using supervision from the final segmentation. We used the publicly available\\r\\ncode and trained the model in the training set of the Chair category. (ii) 3D-DETR is the equivalent of\\r\\n2D-DETR of Carion et al. [5] for 3D point-cloud segmentation. A point transformer [70] encodes the\\r\\ninput 3D point cloud into a set of point features which are mapped to a learnable set of 16 parametric\\r\\nqueries using 6 layers of transformer encoder (self-attention) and decoder (cross-attention) blocks.\\r\\nThen, each query decodes a 3D point segmentation map by computing multi-head attention with the\\r\\nencoded point features. The attention for each query is then upsampled using a point transformer\\r\\ndecoder. The predicted 3D part segments are then matched against ground-truth segments using\\r\\nHungarian matching [36], and error is computed for each pair of matched ground-truth part and\\r\\npaired prediction through binary cross-entropy loss.\\r\\nWe train GFS-Nets in a semi-supervised way combining an (unsupervised) autoencoding loss and a\\r\\nsupervised part segmentation loss. Specifically alongside the autoencoding loss of the training 3D\\r\\npoint clouds, we apply a part segmentation loss via Hungarian matching between the decoded 3D\\r\\npart shapes from the slots and the ground-truth part 3D segments in the training set, similar to our\\r\\n3D-DETR baseline model. For this case we did not consider the models of PQ-Nets [64], Shape2Prog\\r\\n[60] or [21] as it is not clear on how to train them in a semi-supervised manner using segmentation\\r\\nsupervision.\\r\\nWe consider the following ablative versions of GFS-Nets: (i) GFS-Nets w/o supervision is trained\\r\\nwithout the supervised part segmentation loss, so the model only receives gradients from autoencoding\\r\\nthe primitives and whole chairs. (ii) GFS-Nets w/o Gaussian does not initialize the slot vectors by\\r\\nsampling from a learnable Gaussian but instead uses separate learnable query vectors for each slot,\\r\\nsimilar to DETR. (iii) GFS-Nets w/o SlotAttention does not use Slot Attention for mapping point\\r\\nfeatures to slots. Instead it maps 3D point features to slots via iterative layers of cross (query to point)\\r\\nand self (query-to-query) attention layers on learnable query vectors similar to 3D-DETR and DETR\\r\\n[5]. Please note that slots and queries represent the same thing, but we use the terminology of DETR\\r\\n[5] in this case.\\r\\nWe report the fast and slow inference results for all ablative versions of our model. Our baselines in\\r\\nthis case, 3D-DETR and Learning2Group [43] are feedforward in nature, they are not equipped with\\r\\ndecoders, and thus cannot be evaluated with slow inference.\\r\\nWe show quantitative results in Table 2 and qualitative results in Figure 5. GFS-Nets significantly\\r\\noutperform the baselines, and GFS-Nets-Slow results in a significant boost in performance (∼ 50%)\\r\\nover the feedforward inference in our model, GFS-Nets-Fast. We draw the following conclusions\\r\\nfrom Table 2:\\r\\nSupervision can be used as a replacement to curriculum training. GFS-Nets only slightly out-\\r\\nperforms GFS-Nets w/o curriculum in this setup.\\r\\nGeneric primitives generalize a lot better than category-specific primitives or supervision.\\r\\nGFS-Nets in Table 4 (that uses the generic primitive set) outperforms GFS-Nets and GFS-Nets-\\r\\nw/o supervision in Table 2 (that use the semantic primitive set) in OOD generalization on the Table\\r\\ncategory.\\r\\nCompetition amongst slots helps. GFS-Nets outperforms GFS-Nets w/o SlotAttention, thus show-\\r\\ning competition amongst slot vectors during encoding helps generalization.\\r\\nSlow inference through reconstruction feedback helps OOD generalization of GFS-Nets. Our\\r\\nbaselines 3D-DETR and Learning2Group [43] are feed-forward in nature, they lack any form of\\r\\nreconstruction feedback, and thus cannot adapt as our model through such feedback.\\r\\n\\r\\n4.2   RGB image segmentation and view prediction without supervision\\r\\n\\r\\nIn this task, the input to the model is a single RGB image that contains multiple objects and the goal\\r\\nis to segment the image into objects and predict images from alternative scene viewpoints. We test\\r\\nour model in the following two benchmarks: ((i) CLEVR [31]: this is a commonly used benchmark\\r\\nfor unsupervised image segmentation. The dataset contains scenes that consist of 5 to 7 spheres,\\r\\ncylinders and cubes, in various colors and materials. We use the multi-view version of this dataset\\r\\nintroduced by uORF [67] as the training set.\\r\\n\\r\\n\\r\\n                                                     9\\r\\n\\x0c\\n\\n    Input   Slot Attention [44]   uORF [69]   GFS-Nets-Fast   GFS-Nets-Slow      Input          Predicted Novel Views by GFS-Nets\\r\\n\\r\\n\\r\\nFigure 6: RGB image decomposition and view prediction for GFS-Nets and baselines. Left: From 2nd to 5th\\r\\ncolumn we show reconstructed RGB image superimposed with the predicted segmentation masks for our model\\r\\nand baselines. Right: From 2nd to 5th column we show RGB renderings across multiple novel viewpoints for\\r\\nGFS-Nets.\\r\\n\\r\\n\\r\\n(ii) Room Diverse++: Room Diverse++ builds upon Room Diverse, which is a multi-view RGB\\r\\ndataset introduced by Yu et al.[67] where they placed multiple ShapeNet Chairs with different solid\\r\\ncolors (Red, Blue etc) onto a textured background. We make it more challenging by applying real-\\r\\nworld ShapeNet textures to the object meshes instead of solid colors. We also add more categories\\r\\nsuch as Beds, Benches, Tables, Cupboards, and Sofas to the Chair category. We sample about 6 to 10\\r\\nobjects in each scene.\\r\\n\\r\\nBaselines We consider the following baselines: (i) Slot attention Locatello et al. [42] autoencodes\\r\\nsingle view RGB images, and is described in Section 3.1. We use the publicly available code and\\r\\ntrain the model on the training set for single image autoencoding.\\r\\n(ii) uORF Yu et al. [67] is a recent work that\\r\\nextends Slot Attention of [42] to multiview RGB                                Method                   CLEVR               Room Diverse++\\r\\nimages.                                                                        uORF [67]                 0.86                    0.65\\r\\nDuring primitive training for our model, we as-                                Slot Attention [42]       0.04                    0.28\\r\\nsume access to posed RGB images of single                                      GFS-Nets-Fast             0.86                    0.74\\r\\nobject scenes as our primitive dataset, as shown                               GFS-Nets-Slow             0.92                    0.81\\r\\nin Figure 2 bottom row left. Our implementa-\\r\\ntion builds upon [67] with the difference that we                             Table 3: ARI Segmentation accuracy (higher the bet-\\r\\nconsider curriculum and slow inference at test                                ter) on CLEVR and RoomDiverse++ Dataset for RGB\\r\\n                                                                              input. As can be seen GFS-Nets-Slow outperform all\\r\\ntime. We ablate slow and fast versions of our                                 other baselines in segmentation accuracy.\\r\\nmodel.\\r\\nWe show quantitative results of GFS-Nets and\\r\\nthe baselines in Table 3 and qualitative view prediction and segmentation results in Figure 6.\\r\\nGFS-Nets-slow outperforms the baselines significantly. This suggests that curriculum and slow\\r\\ninference are key to out-of-distribution generalization. Slot Attention [42] achieves very low accuracy\\r\\non CLEVR, as it is unable to disentangle background from foreground, as also reported in Yu et\\r\\nal.[67]. We further visualize GFS-Nets’s ability to render novel viewpoints in the supplementary\\r\\nvideo.\\r\\n\\r\\nLimitations and Future Work: GFS-Nets has limitations that offer several promising directions\\r\\nfor future work. First, GFS-Nets does not model pairwise interactions between slots. Incorporating\\r\\nsuch interactions is a direct avenue for future work. Second, entity decomposition is inherently\\r\\nhierarchical: objects - parts - subparts while GFS-Nets currently models a flat non-hierarchical list of\\r\\nentities. Third, amortizing the slow inference in feedforward predictions could be another interesting\\r\\nextension of the current framework. Finally, although GFS-Nets is tested on far more realistic data\\r\\nthan considered in prior works, scaling the model to more realistic datasets used by mainstream\\r\\nsupervised visual detectors, is a direct avenue for future work.\\r\\n\\r\\n5     Conclusion\\r\\nWe presented GFS-Nets, a model that segments scenes into entities while autoencoding the input\\r\\nscene through slot-based encoders and shared slot decoders. We showed that curriculum training\\r\\nin the form of primitive entities, single object scenes, or labelled segmentations is necessary to\\r\\nbreak the trade-off between segmentation and reconstruction observed in previous works [15]. This\\r\\npermits GFS-Nets to use slow inference through reconstruction feedback and drastically improve the\\r\\ndecompositions of out-of-distribution scenes. We believe that a compositional approach to AI, in\\r\\nterms of grounded symbol-like representations [44, 38] is of fundamental importance for realizing\\r\\nhuman-level generalization [1], and we hope that GFS-Nets may contribute towards that goal.\\r\\n\\r\\n\\r\\n                                                                          10\\r\\n\\x0c\\n\\nReferences\\r\\n [1] Bahdanau, D., Murty, S., Noukhovitch, M., Nguyen, T.H., de Vries, H., Courville, A.: System-\\r\\n     atic generalization: what is required and can it be learned? arXiv preprint arXiv:1811.12889\\r\\n     (2018)\\r\\n [2] Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., Katz, B.:\\r\\n     Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\\r\\n     models. Advances in neural information processing systems 32 (2019)\\r\\n [3] van Bergen, R.S., Kriegeskorte, N.: Going in circles is the way forward: the role of re-\\r\\n     currence in visual inference. Current Opinion in Neurobiology 65, 176–193 (Dec 2020).\\r\\n     https://doi.org/10.1016/j.conb.2020.11.009, http://dx.doi.org/10.1016/j.conb.2020.\\r\\n     11.009\\r\\n [4] Burgess, C.P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., Lerchner, A.:\\r\\n     Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390\\r\\n     (2019)\\r\\n [5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end\\r\\n     object detection with transformers. In: European Conference on Computer Vision. pp. 213–229.\\r\\n     Springer (2020)\\r\\n [6] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\\r\\n     M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet: An Information-Rich 3D Model\\r\\n     Repository. Tech. Rep. arXiv:1512.03012 [cs.GR], Stanford University — Princeton University\\r\\n     — Toyota Technological Institute at Chicago (2015)\\r\\n [7] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\\r\\n     M., Song, S., Su, H., et al.: Shapenet: An information-rich 3d model repository. arXiv preprint\\r\\n     arXiv:1512.03012 (2015)\\r\\n [8] Chen, Z., Tagliasacchi, A., Zhang, H.: Bsp-net: Generating compact meshes via binary space\\r\\n     partitioning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition. pp. 45–54 (2020)\\r\\n [9] Chen, Z., Yin, K., Fisher, M., Chaudhuri, S., Zhang, H.: Bae-net: branched autoencoder\\r\\n     for shape co-segmentation. In: Proceedings of the IEEE/CVF International Conference on\\r\\n     Computer Vision. pp. 8490–8499 (2019)\\r\\n[10] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio,\\r\\n     Y.: Learning phrase representations using rnn encoder-decoder for statistical machine translation.\\r\\n     arXiv preprint arXiv:1406.1078 (2014)\\r\\n[11] Deng, B., Genova, K., Yazdani, S., Bouaziz, S., Hinton, G., Tagliasacchi, A.: Cvxnet: Learnable\\r\\n     convex decomposition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\\r\\n     Pattern Recognition. pp. 31–44 (2020)\\r\\n[12] Deprelle, T., Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: Learning elementary\\r\\n     structures for 3d shape generation and matching. arXiv preprint arXiv:1908.04725 (2019)\\r\\n[13] Du, Y., Smith, K., Ulman, T., Tenenbaum, J., Wu, J.: Unsupervised discovery of 3d physical\\r\\n     objects from video. arXiv preprint arXiv:2007.12348 (2020)\\r\\n[14] Ellis, K., Wong, C., Nye, M., Sable-Meyer, M., Cary, L., Morales, L., Hewitt, L., Solar-Lezama,\\r\\n     A., Tenenbaum, J.B.: Dreamcoder: Growing generalizable, interpretable knowledge with\\r\\n     wake-sleep bayesian program learning. arXiv preprint arXiv:2006.08381 (2020)\\r\\n[15] Engelcke, M., Jones, O.P., Posner, I.: Reconstruction bottlenecks in object-centric generative\\r\\n     models. arXiv preprint arXiv:2007.06245 (2020)\\r\\n[16] Engelcke, M., Kosiorek, A.R., Jones, O.P., Posner, I.: Genesis: Generative scene inference and\\r\\n     sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052 (2019)\\r\\n\\r\\n\\r\\n                                                 11\\r\\n\\x0c\\n\\n[17] Eslami, S., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., Hinton,\\r\\n     G.E.: Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint\\r\\n     arXiv:1603.08575 (2016)\\r\\n[18] Gao, L., Yang, J., Wu, T., Yuan, Y.J., Fu, H., Lai, Y.K., Zhang, H.: Sdm-net: Deep generative\\r\\n     network for structured deformable mesh. ACM Transactions on Graphics (TOG) 38(6), 1–15\\r\\n     (2019)\\r\\n[19] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.: Imagenet-\\r\\n     trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness.\\r\\n     arXiv preprint arXiv:1811.12231 (2018)\\r\\n[20] Genova, K., Cole, F., Sud, A., Sarna, A., Funkhouser, T.: Local deep implicit functions for\\r\\n     3d shape. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition. pp. 4857–4866 (2020)\\r\\n[21] Genova, K., Cole, F., Vlasic, D., Sarna, A., Freeman, W.T., Funkhouser, T.: Learning shape\\r\\n     templates with structured implicit functions. In: Proceedings of the IEEE/CVF International\\r\\n     Conference on Computer Vision. pp. 7154–7164 (2019)\\r\\n[22] Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio, Y., Schölkopf, B.: Recurrent\\r\\n     independent mechanisms. arXiv preprint arXiv:1909.10893 (2019)\\r\\n[23] Greff, K., Kaufman, R.L., Kabra, R., Watters, N., Burgess, C., Zoran, D., Matthey, L., Botvinick,\\r\\n     M., Lerchner, A.: Multi-object representation learning with iterative variational inference. In:\\r\\n     International Conference on Machine Learning. pp. 2424–2433. PMLR (2019)\\r\\n[24] Greff, K., Rasmus, A., Berglund, M., Hao, T.H., Schmidhuber, J., Valpola, H.: Tagger: Deep\\r\\n     unsupervised perceptual grouping. arXiv preprint arXiv:1606.06724 (2016)\\r\\n[25] Greff, K., van Steenkiste, S., Schmidhuber, J.: On the binding problem in artificial neural\\r\\n     networks. CoRR abs/2012.05208 (2020), https://arxiv.org/abs/2012.05208\\r\\n[26] Guerin, F.: Projection: A mechanism for human-like reasoning in artificial intelligence. CoRR\\r\\n     abs/2103.13512 (2021), https://arxiv.org/abs/2103.13512\\r\\n[27] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro-\\r\\n     ceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770–778\\r\\n     (2016)\\r\\n[28] Hubert, L., Arabie, P.: Comparing partitions. Journal of classification 2(1), 193–218 (1985)\\r\\n[29] Jo, J., Bengio, Y.: Measuring the tendency of cnns to learn surface statistical regularities. arXiv\\r\\n     preprint arXiv:1711.11561 (2017)\\r\\n[30] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.:\\r\\n     Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In:\\r\\n     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2901–2910\\r\\n     (2017)\\r\\n[31] Johnson, J., Krishna, R., Stark, M., Li, L.J., Shamma, D., Bernstein, M., Fei-Fei, L.: Image\\r\\n     retrieval using scene graphs. In: Proceedings of the IEEE Conference on Computer Vision and\\r\\n     Pattern Recognition (CVPR) (June 2015)\\r\\n[32] Kabra, R., Burgess, C., Matthey, L., Kaufman, R.L., Greff, K., Reynolds, M., Lerchner, A.:\\r\\n     Multi-object datasets. https://github.com/deepmind/multi-object-datasets/ (2019)\\r\\n[33] Kato, H., Harada, T.: Learning view priors for single-view 3d reconstruction. In: Proceedings of\\r\\n     the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9778–9787 (2019)\\r\\n[34] Kipf, T., Elsayed, G.F., Mahendran, A., Stone, A., Sabour, S., Heigold, G., Jonschkowski,\\r\\n     R., Dosovitskiy, A., Greff, K.: Conditional object-centric learning from video. arXiv preprint\\r\\n     arXiv:2111.12594 (2021)\\r\\n\\r\\n\\r\\n                                                  12\\r\\n\\x0c\\n\\n[35] Kosiorek, A., Kim, H., Teh, Y.W., Posner, I.: Sequential attend, infer, repeat: Generative\\r\\n     modelling of moving objects. Advances in Neural Information Processing Systems 31, 8606–\\r\\n     8616 (2018)\\r\\n\\r\\n[36] Kuhn, H.W.: The hungarian method for the assignment problem. Naval research logistics\\r\\n     quarterly 2(1-2), 83–97 (1955)\\r\\n\\r\\n[37] Kulkarni, T.D., Kohli, P., Tenenbaum, J.B., Mansinghka, V.: Picture: A probabilistic program-\\r\\n     ming language for scene perception. In: Proceedings of the ieee conference on computer vision\\r\\n     and pattern recognition. pp. 4390–4399 (2015)\\r\\n\\r\\n[38] Lake, B.M., Salakhutdinov, R., Tenenbaum, J.B.: Human-level concept learning through\\r\\n     probabilistic program induction. Science 350(6266), 1332–1338 (2015)\\r\\n\\r\\n[39] Lake, B.M., Ullman, T.D., Tenenbaum, J.B., Gershman, S.J.: Building machines that learn and\\r\\n     think like people. Behavioral and brain sciences 40 (2017)\\r\\n\\r\\n[40] Lal, S., Prabhudesai, M., Mediratta, I., Harley, A.W., Fragkiadaki, K.: Coconets: Continuous\\r\\n     contrastive 3d scene representations (2021)\\r\\n\\r\\n[41] Li, Y., Mao, J., Zhang, X., Freeman, W.T., Tenenbaum, J.B., Snavely, N., Wu, J.: Multi-plane\\r\\n     program induction with 3d box priors. arXiv preprint arXiv:2011.10007 (2020)\\r\\n\\r\\n[42] Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J.,\\r\\n     Dosovitskiy, A., Kipf, T.: Object-centric learning with slot attention (2020)\\r\\n\\r\\n[43] Luo, T., Mo, K., Huang, Z., Xu, J., Hu, S., Wang, L., Su, H.: Learning to group: A bottom-up\\r\\n     framework for 3d part discovery in unseen categories. arXiv preprint arXiv:2002.06478 (2020)\\r\\n\\r\\n[44] Marcus, G.F.: The algebraic mind: Integrating connectionism and cognitive science. MIT press\\r\\n     (2003)\\r\\n\\r\\n[45] Marr, D.: Vision: A Computational Investigation into the Human Representation and Processing\\r\\n     of Visual Information. W.H. Freeman and Co., New York, NY (1982)\\r\\n\\r\\n[46] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\\r\\n     Learning 3d reconstruction in function space. In: Proceedings of the IEEE/CVF Conference on\\r\\n     Computer Vision and Pattern Recognition. pp. 4460–4470 (2019)\\r\\n\\r\\n[47] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf:\\r\\n     Representing scenes as neural radiance fields for view synthesis. In: European conference on\\r\\n     computer vision. pp. 405–421. Springer (2020)\\r\\n\\r\\n[48] Misra, I., Girdhar, R., Joulin, A.: An end-to-end transformer model for 3d object detection. In:\\r\\n     Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2906–2917\\r\\n     (2021)\\r\\n\\r\\n[49] Mo, K., Zhu, S., Chang, A.X., Yi, L., Tripathi, S., Guibas, L.J., Su, H.: Partnet: A large-scale\\r\\n     benchmark for fine-grained and hierarchical part-level 3d object understanding. In: Proceedings\\r\\n     of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 909–918 (2019)\\r\\n\\r\\n[50] Niu, C., Li, J., Xu, K.: Im2struct: Recovering 3d shape structure from a single rgb image. In:\\r\\n     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4521–4529\\r\\n     (2018)\\r\\n\\r\\n[51] Paschalidou, D., Gool, L.V., Geiger, A.: Learning unsupervised hierarchical part decomposition\\r\\n     of 3d objects from a single rgb image. In: Proceedings of the IEEE/CVF Conference on\\r\\n     Computer Vision and Pattern Recognition. pp. 1060–1070 (2020)\\r\\n\\r\\n[52] Paschalidou, D., Katharopoulos, A., Geiger, A., Fidler, S.: Neural parts: Learning expressive 3d\\r\\n     shape abstractions with invertible neural networks. In: Proceedings of the IEEE/CVF Conference\\r\\n     on Computer Vision and Pattern Recognition. pp. 3204–3215 (2021)\\r\\n\\r\\n\\r\\n                                                 13\\r\\n\\x0c\\n\\n[53] Paschalidou, D., Ulusoy, A.O., Geiger, A.: Superquadrics revisited: Learning 3d shape parsing\\r\\n     beyond cuboids. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition. pp. 10344–10353 (2019)\\r\\n[54] Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by self-supervised\\r\\n     prediction. In: International conference on machine learning. pp. 2778–2787. PMLR (2017)\\r\\n[55] Rahaman, N., Goyal, A., Gondal, M.W., Wuthrich, M., Bauer, S., Sharma, Y., Bengio, Y.,\\r\\n     Schölkopf, B.: S2rms: Spatially structured recurrent modules. arXiv preprint arXiv:2007.06533\\r\\n     (2020)\\r\\n[56] Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object detection with\\r\\n     region proposal networks. CoRR abs/1506.01497 (2015), http://arxiv.org/abs/1506.\\r\\n     01497\\r\\n[57] Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. arXiv preprint\\r\\n     arXiv:1710.09829 (2017)\\r\\n[58] Sitzmann, V., Chan, E., Tucker, R., Snavely, N., Wetzstein, G.: Metasdf: Meta-learning signed\\r\\n     distance functions. Advances in Neural Information Processing Systems 33, 10136–10147\\r\\n     (2020)\\r\\n[59] Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., Hardt, M.: Test-time training with self-\\r\\n     supervision for generalization under distribution shifts. In: International Conference on Machine\\r\\n     Learning. pp. 9229–9248. PMLR (2020)\\r\\n[60] Tian, Y., Luo, A., Sun, X., Ellis, K., Freeman, W.T., Tenenbaum, J.B., Wu, J.: Learning to infer\\r\\n     and execute 3d shape programs. arXiv preprint arXiv:1901.02875 (2019)\\r\\n[61] Tsai, Y.H.H., Srivastava, N., Goh, H., Salakhutdinov, R.: Capsules with inverted dot-product\\r\\n     attention routing. arXiv preprint arXiv:2002.04764 (2020)\\r\\n[62] Tulsiani, S., Su, H., Guibas, L.J., Efros, A.A., Malik, J.: Learning shape abstractions by\\r\\n     assembling volumetric primitives. In: Proceedings of the IEEE Conference on Computer Vision\\r\\n     and Pattern Recognition. pp. 2635–2643 (2017)\\r\\n[63] Van Steenkiste, S., Chang, M., Greff, K., Schmidhuber, J.: Relational neural expectation\\r\\n     maximization: Unsupervised discovery of objects and their interactions. arXiv preprint\\r\\n     arXiv:1802.10353 (2018)\\r\\n[64] Wu, R., Zhuang, Y., Xu, K., Zhang, H., Chen, B.: Pq-net: A generative part seq2seq network\\r\\n     for 3d shapes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition (CVPR) (June 2020)\\r\\n[65] Xia, W., Zhang, Y., Yang, Y., Xue, J.H., Zhou, B., Yang, M.H.: Gan inversion: A survey. arXiv\\r\\n     preprint arXiv:2101.05278 (2021)\\r\\n[66] Yao, C.H., Hung, W.C., Jampani, V., Yang, M.H.: Discovering 3d parts from image collections.\\r\\n     In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12981–\\r\\n     12990 (2021)\\r\\n[67] Yu, H.X., Guibas, L.J., Wu, J.: Unsupervised discovery of object radiance fields. arXiv preprint\\r\\n     arXiv:2107.07905 (2021)\\r\\n[68] Zablotskaia, P., Dominici, E.A., Sigal, L., Lehrmann, A.M.: Unsupervised video decomposition\\r\\n     using spatio-temporal iterative inference. arXiv preprint arXiv:2006.14727 (2020)\\r\\n[69] Zhang, N., Donahue, J., Girshick, R., Darrell, T.: Part-based r-cnns for fine-grained category\\r\\n     detection. In: European conference on computer vision. pp. 834–849. Springer (2014)\\r\\n[70] Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: Proceedings of the\\r\\n     IEEE/CVF International Conference on Computer Vision. pp. 16259–16268 (2021)\\r\\n[71] Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty-thousand classes\\r\\n     using image-level supervision. arXiv preprint arXiv:2201.02605 (2022)\\r\\n\\r\\n\\r\\n                                                  14\\r\\n\\x0c\\n\\n[72] Zhu, J.Y., Krähenbühl, P., Shechtman, E., Efros, A.A.: Generative visual manipulation on the\\r\\n     natural image manifold. In: European conference on computer vision. pp. 597–613. Springer\\r\\n     (2016)\\r\\n[73] Zoran, D., Kabra, R., Lerchner, A., Rezende, D.J.: Parts: Unsupervised segmentation with\\r\\n     slots, attention and independence maximization. In: Proceedings of the IEEE/CVF International\\r\\n     Conference on Computer Vision. pp. 10439–10447 (2021)\\r\\n[74] Zuffi, S., Kanazawa, A., Berger-Wolf, T., Black, M.J.: Three-d safari: Learning to estimate\\r\\n     zebra pose, shape, and texture from images\" in the wild\". In: Proceedings of the IEEE/CVF\\r\\n     International Conference on Computer Vision. pp. 5359–5368 (2019)\\r\\n\\r\\n\\r\\nAppendix\\r\\nThe structure of this appendix is as follows: In Section 6 we cover the details on the datasets. In\\r\\nSection 7 we specify further implementation details. In Section 8 we provide additional qualitative\\r\\nand quantitative results for the experiments in Section 4 of our main paper.\\r\\n\\r\\n6     Datasets\\r\\n6.1   Point Cloud\\r\\n\\r\\nFor all the tasks we subsample the input point clouds to a standard size of 2048 points.\\r\\n\\r\\nGeneric primitive part dataset. We use the primitive dataset of [60] for learning the primitive\\r\\ndistribution in Section 4.1.1. The dataset consists of 200K primitive instances sampled from the\\r\\nprimitive templates that are visualized in Figure 7. Instances are sampled from the templates by\\r\\nchanging their sizes and placing them in random locations, similar to [60].\\r\\n\\r\\nCategory-specific primitives from PartNet dataset. We seperate out individual segments from\\r\\nthe level-3 instance segmentation masks from the official train-split of Chair category of the PartNet\\r\\ndataset. We additionally do rotation and translation based augmentations on individual segments. We\\r\\nvisualize some of these primitive examples in Figure 8. We use this dataset for learning the primitive\\r\\ndistribution in Section 4.1.2. The dataset in total consists of about 100K primitive examples.\\r\\n\\r\\nSynthetic whole shape dataset. This dataset was generated by Shape2Prog[60]. The segmentation\\r\\nlabels from this dataset are used by them as a supervision signal for later generalizing to PartNet\\r\\nshapes. The dataset consists of about 120K synthetically generated Chairs and Tables, we visualize\\r\\nsome of these synthetically generated tables and chairs in Figure 9. Note that no other baseline or\\r\\nGFS-Nets has access to this dataset.\\r\\n\\r\\nPartNet dataset. We use the official level-3 train-test split of PartNet[49]. We use the train split\\r\\nof Chair category as our training set, we consider test split of Table category in PartNet our test\\r\\ncategories. We use this as the train-test split in Section 4.1 of the main paper. We further compare our\\r\\nmodel on the other PartNet categories in supplementary section 8. We set the value of number of\\r\\nslots K as 16 for this dataset.\\r\\n\\r\\n6.2   RGB\\r\\n\\r\\nCLEVR dataset. We use the train-test split of CLEVR dataset opensourced by [67]. Their dataset\\r\\nis built on top of the original CLEVR [30] dataset, where they add additional multi-view rendering.\\r\\nTheir train/test dataset consists of 5,6,7 objects from random geometric primitives(cylinder, cube and\\r\\nsphere) and random colors(red, blue, purple, gray, cyan, yellow, green, brown) placed in random\\r\\nlocations in the scene. The multi-view setting consists of randomly sampling from 360 degree\\r\\nazimuth angles with a fixed elevation angle. Our train data consists of 1000 single object scenes and\\r\\n1000 multi-object scenes. The only difference between their train-test data are the novel locations\\r\\nat which the objects would be placed in the scene. We set the value of number of slots K as 8. We\\r\\nvisualize some of the primitive examples (multi-view single-object scenes) of the CLEVR dataset in\\r\\nFigure 10.\\r\\n\\r\\n\\r\\n                                                  15\\r\\n\\x0c\\n\\nFigure 7: We visualize the generic primitive templates of [60], as you can see they mainly consists of Cubes,\\r\\nCuboids and Discs.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n   Figure 8: We visualize the category-specific primitives extracted from level-3 Chair category of PartNet.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      16\\r\\n\\x0c\\n\\nFigure 9: We visualize the synthetic whole shape dataset of [60]. Shape2Prog supervises their model using the\\r\\nannotations from this dataset.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 10: We visualize some examples of the primitives used in the CLEVR dataset. Primitives are represented\\r\\nas multi-view single object RGB scenes as shown in the figure.\\r\\n\\r\\n\\r\\nRoomDiverse++ dataset. We build RoomDiverse++ on top of the RoomDiverse dataset of [67].\\r\\nIn which instead of using the solid CLEVR colors as textures of meshes we keep the original textures\\r\\nof the ShapeNet meshes, additionaly along with Chairs we also add other ShapeNet categories such\\r\\nas Benches, Cabinets, Beds, Tables and Sofas in the dataset. In total we use 20 object meshes and\\r\\n3 background textures to generate the dataset. We use the same multi-view rendering settings as\\r\\nCLEVR. Our train data consists of 1000 single object scenes and 2000 multi-object scenes. We use\\r\\n6,7,8 objects placed at random locations for generating our multi-object train dataset. We use 500 of\\r\\n7,8,9 object scenes as our test data. We set the value of number of slots K as 10. We visualize some\\r\\nof the primitive examples in the dataset in Figure 11.\\r\\n\\r\\n\\r\\n7     Implementation details\\r\\n\\r\\n7.1   Proposed Model\\r\\n\\r\\nCode, training details and computational complexity. Proposed model is implemented in\\r\\nPython/PyTorch. We use a batch size of 8 for point cloud input and a batch size of 2 on RGB\\r\\ninput. We set our learning rate as 10−4 . We use the Adam optimizer with β1 = 0.9, β2 = 0.999. Our\\r\\nprimitive learning model takes 24 hours (approximately 200k iterations) to converge. Similarly our\\r\\n\\r\\n\\r\\n                                                     17\\r\\n\\x0c\\n\\nFigure 11: We visualize some examples of the primitives used in the Room Diverse++ dataset. Primitives are\\r\\nrepresented as multi-view single object RGB scenes as shown in the figure.\\r\\n\\r\\n\\r\\nprimitive composition learning model takes about 12hr (approximately 100k iterations) to converge.\\r\\nOur slow inference per example takes about 1 min (500 iterations). A forward pass through the\\r\\nproposed model takes about 0.15 secs. We use a single V100 GPU for training and inference.\\r\\n\\r\\nInputs. For RGB input we randomly select 2 views which include the RGB and ground-truth\\r\\negomotion. We use a single RGB image for testing. For RGB, our input is a 64 × 64 × 3 tensor, for\\r\\npoint cloud our input is a 2048 × 3\\r\\n\\r\\nPoint Cloud Encoder. We adopt the point transformer[70] architecture as our encoder. Point\\r\\ntransformer encoder is essentially layers of self attention blocks. Specifically a self attention block\\r\\nincludes sampling of query points and updating them using their N most neighbouring points as\\r\\nkey/value vectors. In the architecture we specifically apply 5 layers of self attention which look as\\r\\nfollows: 2048-16-64, 2048-16-64, 512-16-64, 512-16-64, 128-16-64, 128-16-64. We use the notation\\r\\nof S-N -C, where S is the number of subsampled query points from the point cloud, N is the number\\r\\nof neighbouring points and C is the feature dimension. We thus get an output feature map of size\\r\\n128 × 64.\\r\\n\\r\\nRGB Encoder. Our RGB encoder is similar to that of [67], which is basically a U-net. We\\r\\nconcatenate the RGB values with the pixel coordinates that are normalized in the range of -1 to 1. We\\r\\nthen pass the concatenated 2D image through a convolutional feature extractor. The architecture of\\r\\nthe CNN is as follows: 3-2-64, 3-2-64, 3-2-64, 3-1-64, 3-1-64, 3-1-64. We use the notation of k-s-c,\\r\\nwhere k is the kernel size, s is the number of strides and c is the feature channel. Our final output\\r\\nsize is 64 × 64 × 64.\\r\\n\\r\\nPoint Cloud Decoder. We obtain point occupancies by querying the slot feature vector slotk at\\r\\ndiscrete locations (x, y, z) specifically ox,y,z = Dec(slotk , (x, y, z)). The architecture of Dec is\\r\\nsimilar to that of [40]. Given slotk , which is one of the slot feature vector. We encode the coordinate\\r\\n(x, y, z) into a 64-D feature vector using a linear layer. We denote this vector as z. The inputs slotk\\r\\nand z are then processed as follows:\\r\\n\\r\\n   out_k = RB_i( RB_{i-1}( \\\\cdots RB_1( z + FC_1(slot_k)) \\\\\\\\ \\\\cdots ] + FC_{i-1}(slot_k)) + FC_{i}(slot_k)).\\r\\n                                                                                                               (3)\\r\\nWe set i = 3. F Ci is a linear layer that outputs a 64 dimensional vector. RNi is a 2 layer ResNet\\r\\nMLP block [27]. The architecture of ResNet block is: ReLU, 64-64, ReLU, 64-64. Here, i − o\\r\\nrepresents a linear layer, where i and o are the input and output dimension. Finally outk is then\\r\\npassed through a ReLU activation function followed by a linear layer to generate a single value for\\r\\noccupancy.\\r\\n\\r\\nRGB Decoder. Our decoder follows the architecture of [67]. The decoder is a 8 layer MLP,\\r\\nspecifically ReLU, 64-64, ReLU, 64-64 .... 64-4. The MLP takes in as input slotk feature and the\\r\\nlocation x, y, z and predicts the RGB and density values at that location.\\r\\n\\r\\n\\r\\n                                                       18\\r\\n\\x0c\\n\\nSlot Extractor. Following is the pseudo-code of Slot Attention, which we use for learning primitive\\r\\ncompositions.\\r\\n\\r\\nAlgorithm 1 Slot Attention module. Input is N vectors of dimension Dinputs which is mapped to a\\r\\nset of K slots of dimension Dslots . We sample the learnable multivariate gaussian to get the initial\\r\\nslot features, here the learnable parameters are: µ ∈ RDslots and σ ∈ RDslots .For our experiments we\\r\\nset T = 3.\\r\\n1: Input: inputs ∈ RN ×Dinputs , slots ∼ N (µ, diag(σ)) ∈ RK×Dslots\\r\\n2: Layer params: k, q, v: linear projections; GRU; MLP; LayerNorm (x3)\\r\\n3: inputs = LayerNorm (inputs)\\r\\n4: for t = 0 . . . T\\r\\n5:     slots_prev = slots\\r\\n6:     slots = LayerNorm (slots)\\r\\n7:     attn = Softmax ( √1 k(inputs) · q(slots)T , axis=‘slots’)\\r\\n                             D\\r\\n8:     updates = Avg (weights=attn + ϵ, values=v(inputs))\\r\\n9:     slots = GRU (state=slots_prev, inputs=updates)\\r\\n10:      slots += MLP (LayerNorm (slots))\\r\\n11: return slots\\r\\n\\r\\n\\r\\nFor primitive learning stage on PointCloud we set the value of number of slots K as 1. Additionally\\r\\nwe replace the Softmax () activation in step 7 with a Sigmoid () activation. For primitive learning\\r\\nstage on RGB scenes we set the value of number of slots K as 2. This is due to RGB images having\\r\\na background componenet in them.\\r\\n\\r\\n7.2   Baselines                               Training\\r\\n\\r\\n\\r\\n               Cross Attention                                                                              P\\r\\n                 With Heads                               Attention\\r\\n                                                                                  C                         O\\r\\n                                                            Map\\r\\n                                                                                  o                         I\\r\\n                                                                                  n                         N\\r\\n                                                                                  c                         T\\r\\n                                                          Attention               a FPN-Style\\r\\n            Encoder\\r\\n                                                            Map                   t Transformer             A\\r\\n                                                                                  e                         R\\r\\n                                                                                  n                         G\\r\\n                                                                                  a                         M\\r\\n                                                                                  t                         A\\r\\n                                                                                  e                         X\\r\\n                                                          Attention\\r\\n                                                            Map\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                 Head features Head features        Spatial Attention\\r\\n                                        T=1           T=2                Maps                     Hungarian Matching\\r\\n                                                                                                              +\\r\\n                                                                                                  Binary CrossEntropy Loss\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 12: 3D-DETR model architecture. Our baseline model is very similar to 2D-DETR[5], where we replace\\r\\ntheir base 2D encoder and 2D FPN-style CNN with a 3D PointTransformer[70]. Given point clouds as input\\r\\nour encoder backbone featurizes the points into N feature vectors, we then do iterative self and cross attention\\r\\nusing K learnable query heads, following the implementation of Carion et al.[5]. We then compute attention of\\r\\nthe updated queries wrt to the encoded feature vectors. We then concatenate these attentions along the batch\\r\\ndimension and pass them through a FPN-style transformer that increases their resolution and outputs each query\\r\\nmask logits. We then do hungarian matching and binary cross entropy loss\\r\\n\\r\\n\\r\\n3D-DETR. 3D-DETR is a SOTA segmentation architecture, we build this architecture on top\\r\\nof 2D-DETR[5], where we replace their 2D encoders with a 3D PointTransformer[70]. For fair\\r\\ncomparision we make sure the number of parameters in 3D-DETR is the same as GFS-Nets.\\r\\nThe base encoder is a 5 layer architecture of 1024-16-64, 1024-16-64, 1024-16-64, 1024-16-64,\\r\\n512-16-64, following the notation of S,N ,C where S is the number of sampled query points, N is\\r\\nthe number of selected neigbouring points and C is the feature dimension. We have 16 learnable\\r\\n\\r\\n\\r\\n                                                               19\\r\\n\\x0c\\n\\nqueries with six encoder-decoder layers of transformer attention, each layer consists of 8 heads. We\\r\\nthen compute multi-head cross-attention between each query vector and the encoded 3d points. This\\r\\ngives us a map of size M × 512 × 16, where M is the number of heads in the multi-head attention.\\r\\nEach attention map is individually upsampled through a PointTransformer decoder of the architecture\\r\\n512-16-64, 1024-16-64, 1024-16-64, which gives us the final instance segmentation mask per query,\\r\\nsimilar to DETR[5]. We then use Hungarian matching to match the predicted masks against the\\r\\nground truth masks and then apply binary cross entropy loss for each match. We aggregate the losses\\r\\nfrom each query and backpropagate. Figure 12 visualizes the architecture of 3D-DETR. Note that\\r\\nwe do not follow the 2 stage training of [5], rather we train their model end-to-end for instance\\r\\nsegmentation, similar to our model. We have found this trick to save compute and still not harm the\\r\\nend results.\\r\\n\\r\\nLearning2Group.[43] We use their open-sourced architecture and code for comparision with our\\r\\nmode. We train their model using our datasets from scratch.\\r\\n\\r\\nShape2Prog[60] We use their open-sourced architecture, code and pretrained checkpoints for\\r\\ncomparision with our model. We change the value of number of blocks similar to the number of slots\\r\\nin our model for each dataset.\\r\\n\\r\\nPQ-Nets.[64] We use their open-sourced architecture and code for comparision with our model.\\r\\nWe train their model using our datasets from scratch. We change the value of number of slots in their\\r\\nmodel based on the maximum number of parts in the dataset.\\r\\n\\r\\nStruct Implicit.[21] We use their open-sourced architecture and code for comparision with our\\r\\nmode. We train their model using our datasets from scratch. We change the value of number of slots\\r\\nin their model based on the maximum number of parts in the dataset.\\r\\n\\r\\nuORF.[67] We use their open-sourced architecture, code and pretrained checkpoints for compari-\\r\\nsion with our model on the CLEVR dataset. For RoomDiverse++ dataset, we train their model on\\r\\nour dataset using the hyperparmeters they use for training their model on RoomDiverse dataset. We\\r\\nchange the value of number of slots in their model similar to our model for each dataset.\\r\\n\\r\\nSlot Attention.[42] We use their open-sourced architecture and code for comparision with our\\r\\nmodel on the CLEVR and RoomDiverse++ dataset. We train their model using our datasets from\\r\\nscratch. We change the value of number of slots in their model similar to our model for each dataset.\\r\\n\\r\\n8   More Results\\r\\nSegmenting 3D pointclouds without segmentation supervision. In this section we show ad-\\r\\nditional qualitative and quantitative results for Section 4.1.1 of the main paper. We follow the\\r\\nexperimental setup of Section 4.1.1, where we use the generic part dataset (visualized in Fig 7) for\\r\\nprimitive learning. In Table 4, we further extend Table 1 of the main paper to include different levels\\r\\nin Chair and Table category. Here (X/Y /Z) refers to (level 1/level 2/level 3) scores respectively. We\\r\\npick the best performing level in fast inference and specifically report it’s slow inference results. We\\r\\nfurther qualitatively compare our model against Shape2Prog (best performing baseline) in Figure 13\\r\\n\\r\\n                                     in-dist (Chairs)                  out-of-dist (Tables)\\r\\n      Method\\r\\n                              Fast Infer.         Slow Infer.        Fast Infer.      Slow Infer.\\r\\n      Shape2Prog [60]         0.28/0.21/0.26          0.53        0.21/0.23/0.23          0.40\\r\\n      PQ-Nets [64]            0.20/0.18/0.16          0.31        0.17/0.14/0.16          0.21\\r\\n      StructImplicit [21]     0.18/0.22/0.25          0.23        0.14/0.15/0.17          0.17\\r\\n      GFS-Nets                0.51/0.48/0.57          0.62        0.51/0.55/0.60          0.69\\r\\n\\r\\nTable 4: ARI Segmentation accuracy (higher the better) in the test set of Chair (in-distribution) and Table\\r\\ncategory of PartNet(out-of-distribution). GFS-Nets significantly outperform all of the baselines.\\r\\n\\r\\n\\r\\n\\r\\n                                                    20\\r\\n\\x0c\\n\\nRGB image segmentation and view prediction without supervision In Figure 15, we show more\\r\\nqualitative results for Section 4.3 of the main paper, where we qualitatively compare our model for\\r\\nsegmentation against uORF[67] and Slot Attention [42]. We see that slow inference prevents our\\r\\nmodel from missing to detect occluded objects in the scene, where as the baselines have a lot of False\\r\\nPositives i.e missed detections.\\r\\n\\r\\n                        Input          Shape2Prog          GFS-Nets-Fast   GFS-Nets-Slow\\r\\n                                       (Supervised)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n        Figure 13: Additional shape decomposition results using generic primitives. (Section 4.1.1)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      21\\r\\n\\x0c\\n\\n                                    Instance Segmentation\\r\\n\\r\\n\\r\\n                  3D-DETR          GFS-Nets-Slow        3D-DETR        GFS-Nets-Slow\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 14: Additional shape decomposition results using category-specific primitives. (Section 4.1.2)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                   22\\r\\n\\x0c\\n\\n          Ground Truth   Slot Attention   uORF   GFS-Nets-Fast GFS-Nets-Slow\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 15: Additional RGB segmentation results on RoomDiverse++. (Section 4.3)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          23\\r\\n\\x0c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with one pdf first\n",
    "mypdftext=array_pdf_text[0]\n",
    "mypdftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a99fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      " [1] Bahdanau, D., Murty, S., Noukhovitch, M., Nguyen, T.H., de Vries, H., Courville, A.: System-\r\n",
      "     atic generalization: what is required and can it be learned? arXiv preprint arXiv:1811.12889\r\n",
      "     (2018)\r\n",
      " [2] Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., Katz, B.:\r\n",
      "     Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\r\n",
      "     models. Advances in neural information processing systems 32 (2019)\r\n",
      " [3] van Bergen, R.S., Kriegeskorte, N.: Going in circles is the way forward: the role of re-\r\n",
      "     currence in visual inference. Current Opinion in Neurobiology 65, 176–193 (Dec 2020).\r\n",
      "     https://doi.org/10.1016/j.conb.2020.11.009, http://dx.doi.org/10.1016/j.conb.2020.\r\n",
      "     11.009\r\n",
      " [4] Burgess, C.P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., Lerchner, A.:\r\n",
      "     Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390\r\n",
      "     (2019)\r\n",
      " [5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end\r\n",
      "     object detection with transformers. In: European Conference on Computer Vision. pp. 213–229.\r\n",
      "     Springer (2020)\r\n",
      " [6] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\r\n",
      "     M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet: An Information-Rich 3D Model\r\n",
      "     Repository. Tech. Rep. arXiv:1512.03012 [cs.GR], Stanford University — Princeton University\r\n",
      "     — Toyota Technological Institute at Chicago (2015)\r\n",
      " [7] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\r\n",
      "     M., Song, S., Su, H., et al.: Shapenet: An information-rich 3d model repository. arXiv preprint\r\n",
      "     arXiv:1512.03012 (2015)\r\n",
      " [8] Chen, Z., Tagliasacchi, A., Zhang, H.: Bsp-net: Generating compact meshes via binary space\r\n",
      "     partitioning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\r\n",
      "     Recognition. pp. 45–54 (2020)\r\n",
      " [9] Chen, Z., Yin, K., Fisher, M., Chaudhuri, S., Zhang, H.: Bae-net: branched autoencoder\r\n",
      "     for shape co-segmentation. In: Proceedings of the IEEE/CVF International Conference on\r\n",
      "     Computer Vision. pp. 8490–8499 (2019)\r\n",
      "[10] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio,\r\n",
      "     Y.: Learning phrase representations using rnn encoder-decoder for statistical machine translation.\r\n",
      "     arXiv preprint arXiv:1406.1078 (2014)\r\n",
      "[11] Deng, B., Genova, K., Yazdani, S., Bouaziz, S., Hinton, G., Tagliasacchi, A.: Cvxnet: Learnable\r\n",
      "     convex decomposition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\r\n",
      "     Pattern Recognition. pp. 31–44 (2020)\r\n",
      "[12] Deprelle, T., Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: Learning elementary\r\n",
      "     structures for 3d shape generation and matching. arXiv preprint arXiv:1908.04725 (2019)\r\n",
      "[13] Du, Y., Smith, K., Ulman, T., Tenenbaum, J., Wu, J.: Unsupervised discovery of 3d physical\r\n",
      "     objects from video. arXiv preprint arXiv:2007.12348 (2020)\r\n",
      "[14] Ellis, K., Wong, C., Nye, M., Sable-Meyer, M., Cary, L., Morales, L., Hewitt, L., Solar-Lezama,\r\n",
      "     A., Tenenbaum, J.B.: Dreamcoder: Growing generalizable, interpretable knowledge with\r\n",
      "     wake-sleep bayesian program learning. arXiv preprint arXiv:2006.08381 (2020)\r\n",
      "[15] Engelcke, M., Jones, O.P., Posner, I.: Reconstruction bottlenecks in object-centric generative\r\n",
      "     models. arXiv preprint arXiv:2007.06245 (2020)\r\n",
      "[16] Engelcke, M., Kosiorek, A.R., Jones, O.P., Posner, I.: Genesis: Generative scene inference and\r\n",
      "     sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052 (2019)\r\n",
      "\r\n",
      "\r\n",
      "                                                 11\r\n",
      "\f",
      "\n",
      "\n",
      "[17] Eslami, S., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., Hinton,\r\n",
      "     G.E.: Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint\r\n",
      "     arXiv:1603.08575 (2016)\r\n",
      "[18] Gao, L., Yang, J., Wu, T., Yuan, Y.J., Fu, H., Lai, Y.K., Zhang, H.: Sdm-net: Deep generative\r\n",
      "     network for structured deformable mesh. ACM Transactions on Graphics (TOG) 38(6), 1–15\r\n",
      "     (2019)\r\n",
      "[19] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.: Imagenet-\r\n",
      "     trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness.\r\n",
      "     arXiv preprint arXiv:1811.12231 (2018)\r\n",
      "[20] Genova, K., Cole, F., Sud, A., Sarna, A., Funkhouser, T.: Local deep implicit functions for\r\n",
      "     3d shape. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\r\n",
      "     Recognition. pp. 4857–4866 (2020)\r\n",
      "[21] Genova, K., Cole, F., Vlasic, D., Sarna, A., Freeman, W.T., Funkhouser, T.: Learning shape\r\n",
      "     templates with structured implicit functions. In: Proceedings of the IEEE/CVF International\r\n",
      "     Conference on Computer Vision. pp. 7154–7164 (2019)\r\n",
      "[22] Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio, Y., Schölkopf, B.: Recurrent\r\n",
      "     independent mechanisms. arXiv preprint arXiv:1909.10893 (2019)\r\n",
      "[23] Greff, K., Kaufman, R.L., Kabra, R., Watters, N., Burgess, C., Zoran, D., Matthey, L., Botvinick,\r\n",
      "     M., Lerchner, A.: Multi-object representation learning with iterative variational inference. In:\r\n",
      "     International Conference on Machine Learning. pp. 2424–2433. PMLR (2019)\r\n",
      "[24] Greff, K., Rasmus, A., Berglund, M., Hao, T.H., Schmidhuber, J., Valpola, H.: Tagger: Deep\r\n",
      "     unsupervised perceptual grouping. arXiv preprint arXiv:1606.06724 (2016)\r\n",
      "[25] Greff, K., van Steenkiste, S., Schmidhuber, J.: On the binding problem in artificial neural\r\n",
      "     networks. CoRR abs/2012.05208 (2020), https://arxiv.org/abs/2012.05208\r\n",
      "[26] Guerin, F.: Projection: A mechanism for human-like reasoning in artificial intelligence. CoRR\r\n",
      "     abs/2103.13512 (2021), https://arxiv.org/abs/2103.13512\r\n",
      "[27] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro-\r\n",
      "     ceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770–778\r\n",
      "     (2016)\r\n",
      "[28] Hubert, L., Arabie, P.: Comparing partitions. Journal of classification 2(1), 193–218 (1985)\r\n",
      "[29] Jo, J., Bengio, Y.: Measuring the tendency of cnns to learn surface statistical regularities. arXiv\r\n",
      "     preprint arXiv:1711.11561 (2017)\r\n",
      "[30] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.:\r\n",
      "     Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In:\r\n",
      "     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2901–2910\r\n",
      "     (2017)\r\n",
      "[31] Johnson, J., Krishna, R., Stark, M., Li, L.J., Shamma, D., Bernstein, M., Fei-Fei, L.: Image\r\n",
      "     retrieval using scene graphs. In: Proceedings of the IEEE Conference on Computer Vision and\r\n",
      "     Pattern Recognition (CVPR) (June 2015)\r\n",
      "[32] Kabra, R., Burgess, C., Matthey, L., Kaufman, R.L., Greff, K., Reynolds, M., Lerchner, A.:\r\n",
      "     Multi-object datasets. https://github.com/deepmind/multi-object-datasets/ (2019)\r\n",
      "[33] Kato, H., Harada, T.: Learning view priors for single-view 3d reconstruction. In: Proceedings of\r\n",
      "     the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9778–9787 (2019)\r\n",
      "[34] Kipf, T., Elsayed, G.F., Mahendran, A., Stone, A., Sabour, S., Heigold, G., Jonschkowski,\r\n",
      "     R., Dosovitskiy, A., Greff, K.: Conditional object-centric learning from video. arXiv preprint\r\n",
      "     arXiv:2111.12594 (2021)\r\n",
      "\r\n",
      "\r\n",
      "                                                  12\r\n",
      "\f",
      "\n",
      "\n",
      "[35] Kosiorek, A., Kim, H., Teh, Y.W., Posner, I.: Sequential attend, infer, repeat: Generative\r\n",
      "     modelling of moving objects. Advances in Neural Information Processing Systems 31, 8606–\r\n",
      "     8616 (2018)\r\n",
      "\r\n",
      "[36] Kuhn, H.W.: The hungarian method for the assignment problem. Naval research logistics\r\n",
      "     quarterly 2(1-2), 83–97 (1955)\r\n",
      "\r\n",
      "[37] Kulkarni, T.D., Kohli, P., Tenenbaum, J.B., Mansinghka, V.: Picture: A probabilistic program-\r\n",
      "     ming language for scene perception. In: Proceedings of the ieee conference on computer vision\r\n",
      "     and pattern recognition. pp. 4390–4399 (2015)\r\n",
      "\r\n",
      "[38] Lake, B.M., Salakhutdinov, R., Tenenbaum, J.B.: Human-level concept learning through\r\n",
      "     probabilistic program induction. Science 350(6266), 1332–1338 (2015)\r\n",
      "\r\n",
      "[39] Lake, B.M., Ullman, T.D., Tenenbaum, J.B., Gershman, S.J.: Building machines that learn and\r\n",
      "     think like people. Behavioral and brain sciences 40 (2017)\r\n",
      "\r\n",
      "[40] Lal, S., Prabhudesai, M., Mediratta, I., Harley, A.W., Fragkiadaki, K.: Coconets: Continuous\r\n",
      "     contrastive 3d scene representations (2021)\r\n",
      "\r\n",
      "[41] Li, Y., Mao, J., Zhang, X., Freeman, W.T., Tenenbaum, J.B., Snavely, N., Wu, J.: Multi-plane\r\n",
      "     program induction with 3d box priors. arXiv preprint arXiv:2011.10007 (2020)\r\n",
      "\r\n",
      "[42] Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J.,\r\n",
      "     Dosovitskiy, A., Kipf, T.: Object-centric learning with slot attention (2020)\r\n",
      "\r\n",
      "[43] Luo, T., Mo, K., Huang, Z., Xu, J., Hu, S., Wang, L., Su, H.: Learning to group: A bottom-up\r\n",
      "     framework for 3d part discovery in unseen categories. arXiv preprint arXiv:2002.06478 (2020)\r\n",
      "\r\n",
      "[44] Marcus, G.F.: The algebraic mind: Integrating connectionism and cognitive science. MIT press\r\n",
      "     (2003)\r\n",
      "\r\n",
      "[45] Marr, D.: Vision: A Computational Investigation into the Human Representation and Processing\r\n",
      "     of Visual Information. W.H. Freeman and Co., New York, NY (1982)\r\n",
      "\r\n",
      "[46] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\r\n",
      "     Learning 3d reconstruction in function space. In: Proceedings of the IEEE/CVF Conference on\r\n",
      "     Computer Vision and Pattern Recognition. pp. 4460–4470 (2019)\r\n",
      "\r\n",
      "[47] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf:\r\n",
      "     Representing scenes as neural radiance fields for view synthesis. In: European conference on\r\n",
      "     computer vision. pp. 405–421. Springer (2020)\r\n",
      "\r\n",
      "[48] Misra, I., Girdhar, R., Joulin, A.: An end-to-end transformer model for 3d object detection. In:\r\n",
      "     Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2906–2917\r\n",
      "     (2021)\r\n",
      "\r\n",
      "[49] Mo, K., Zhu, S., Chang, A.X., Yi, L., Tripathi, S., Guibas, L.J., Su, H.: Partnet: A large-scale\r\n",
      "     benchmark for fine-grained and hierarchical part-level 3d object understanding. In: Proceedings\r\n",
      "     of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 909–918 (2019)\r\n",
      "\r\n",
      "[50] Niu, C., Li, J., Xu, K.: Im2struct: Recovering 3d shape structure from a single rgb image. In:\r\n",
      "     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4521–4529\r\n",
      "     (2018)\r\n",
      "\r\n",
      "[51] Paschalidou, D., Gool, L.V., Geiger, A.: Learning unsupervised hierarchical part decomposition\r\n",
      "     of 3d objects from a single rgb image. In: Proceedings of the IEEE/CVF Conference on\r\n",
      "     Computer Vision and Pattern Recognition. pp. 1060–1070 (2020)\r\n",
      "\r\n",
      "[52] Paschalidou, D., Katharopoulos, A., Geiger, A., Fidler, S.: Neural parts: Learning expressive 3d\r\n",
      "     shape abstractions with invertible neural networks. In: Proceedings of the IEEE/CVF Conference\r\n",
      "     on Computer Vision and Pattern Recognition. pp. 3204–3215 (2021)\r\n",
      "\r\n",
      "\r\n",
      "                                                 13\r\n",
      "\f",
      "\n",
      "\n",
      "[53] Paschalidou, D., Ulusoy, A.O., Geiger, A.: Superquadrics revisited: Learning 3d shape parsing\r\n",
      "     beyond cuboids. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\r\n",
      "     Recognition. pp. 10344–10353 (2019)\r\n",
      "[54] Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by self-supervised\r\n",
      "     prediction. In: International conference on machine learning. pp. 2778–2787. PMLR (2017)\r\n",
      "[55] Rahaman, N., Goyal, A., Gondal, M.W., Wuthrich, M., Bauer, S., Sharma, Y., Bengio, Y.,\r\n",
      "     Schölkopf, B.: S2rms: Spatially structured recurrent modules. arXiv preprint arXiv:2007.06533\r\n",
      "     (2020)\r\n",
      "[56] Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object detection with\r\n",
      "     region proposal networks. CoRR abs/1506.01497 (2015), http://arxiv.org/abs/1506.\r\n",
      "     01497\r\n",
      "[57] Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. arXiv preprint\r\n",
      "     arXiv:1710.09829 (2017)\r\n",
      "[58] Sitzmann, V., Chan, E., Tucker, R., Snavely, N., Wetzstein, G.: Metasdf: Meta-learning signed\r\n",
      "     distance functions. Advances in Neural Information Processing Systems 33, 10136–10147\r\n",
      "     (2020)\r\n",
      "[59] Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., Hardt, M.: Test-time training with self-\r\n",
      "     supervision for generalization under distribution shifts. In: International Conference on Machine\r\n",
      "     Learning. pp. 9229–9248. PMLR (2020)\r\n",
      "[60] Tian, Y., Luo, A., Sun, X., Ellis, K., Freeman, W.T., Tenenbaum, J.B., Wu, J.: Learning to infer\r\n",
      "     and execute 3d shape programs. arXiv preprint arXiv:1901.02875 (2019)\r\n",
      "[61] Tsai, Y.H.H., Srivastava, N., Goh, H., Salakhutdinov, R.: Capsules with inverted dot-product\r\n",
      "     attention routing. arXiv preprint arXiv:2002.04764 (2020)\r\n",
      "[62] Tulsiani, S., Su, H., Guibas, L.J., Efros, A.A., Malik, J.: Learning shape abstractions by\r\n",
      "     assembling volumetric primitives. In: Proceedings of the IEEE Conference on Computer Vision\r\n",
      "     and Pattern Recognition. pp. 2635–2643 (2017)\r\n",
      "[63] Van Steenkiste, S., Chang, M., Greff, K., Schmidhuber, J.: Relational neural expectation\r\n",
      "     maximization: Unsupervised discovery of objects and their interactions. arXiv preprint\r\n",
      "     arXiv:1802.10353 (2018)\r\n",
      "[64] Wu, R., Zhuang, Y., Xu, K., Zhang, H., Chen, B.: Pq-net: A generative part seq2seq network\r\n",
      "     for 3d shapes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\r\n",
      "     Recognition (CVPR) (June 2020)\r\n",
      "[65] Xia, W., Zhang, Y., Yang, Y., Xue, J.H., Zhou, B., Yang, M.H.: Gan inversion: A survey. arXiv\r\n",
      "     preprint arXiv:2101.05278 (2021)\r\n",
      "[66] Yao, C.H., Hung, W.C., Jampani, V., Yang, M.H.: Discovering 3d parts from image collections.\r\n",
      "     In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12981–\r\n",
      "     12990 (2021)\r\n",
      "[67] Yu, H.X., Guibas, L.J., Wu, J.: Unsupervised discovery of object radiance fields. arXiv preprint\r\n",
      "     arXiv:2107.07905 (2021)\r\n",
      "[68] Zablotskaia, P., Dominici, E.A., Sigal, L., Lehrmann, A.M.: Unsupervised video decomposition\r\n",
      "     using spatio-temporal iterative inference. arXiv preprint arXiv:2006.14727 (2020)\r\n",
      "[69] Zhang, N., Donahue, J., Girshick, R., Darrell, T.: Part-based r-cnns for fine-grained category\r\n",
      "     detection. In: European conference on computer vision. pp. 834–849. Springer (2014)\r\n",
      "[70] Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: Proceedings of the\r\n",
      "     IEEE/CVF International Conference on Computer Vision. pp. 16259–16268 (2021)\r\n",
      "[71] Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty-thousand classes\r\n",
      "     using image-level supervision. arXiv preprint arXiv:2201.02605 (2022)\r\n",
      "\r\n",
      "\r\n",
      "                                                  14\r\n",
      "\f",
      "\n",
      "\n",
      "[72] Zhu, J.Y., Krähenbühl, P., Shechtman, E., Efros, A.A.: Generative visual manipulation on the\r\n",
      "     natural image manifold. In: European conference on computer vision. pp. 597–613. Springer\r\n",
      "     (2016)\r\n",
      "[73] Zoran, D., Kabra, R., Lerchner, A., Rezende, D.J.: Parts: Unsupervised segmentation with\r\n",
      "     slots, attention and independence maximization. In: Proceedings of the IEEE/CVF International\r\n",
      "     Conference on Computer Vision. pp. 10439–10447 (2021)\r\n",
      "[74] Zuffi, S., Kanazawa, A., Berger-Wolf, T., Black, M.J.: Three-d safari: Learning to estimate\r\n",
      "     zebra pose, shape, and texture from images\" in the wild\". In: Proceedings of the IEEE/CVF\r\n",
      "     International Conference on Computer Vision. pp. 5359–5368 (2019)\r\n",
      "\r\n",
      "\r\n",
      "Appendix\r\n",
      "The structure of this appendix is as follows: In Section 6 we cover the details on the datasets. In\r\n",
      "Section 7 we specify further implementation details. In Section 8 we provide additional qualitative\r\n",
      "and quantitative results for the experiments in Section 4 of our main paper.\r\n",
      "\r\n",
      "6     Datasets\r\n",
      "6.1   Point Cloud\r\n",
      "\r\n",
      "For all the tasks we subsample the input point clouds to a standard size of 2048 points.\r\n",
      "\r\n",
      "Generic primitive part dataset. We use the primitive dataset of [60] for learning the primitive\r\n",
      "distribution in Section 4.1.1. The dataset consists of 200K primitive instances sampled from the\r\n",
      "primitive templates that are visualized in Figure 7. Instances are sampled from the templates by\r\n",
      "changing their sizes and placing them in random locations, similar to [60].\r\n",
      "\r\n",
      "Category-specific primitives from PartNet dataset. We seperate out individual segments from\r\n",
      "the level-3 instance segmentation masks from the official train-split of Chair category of the PartNet\r\n",
      "dataset. We additionally do rotation and translation based augmentations on individual segments. We\r\n",
      "visualize some of these primitive examples in Figure 8. We use this dataset for learning the primitive\r\n",
      "distribution in Section 4.1.2. The dataset in total consists of about 100K primitive examples.\r\n",
      "\r\n",
      "Synthetic whole shape dataset. This dataset was generated by Shape2Prog[60]. The segmentation\r\n",
      "labels from this dataset are used by them as a supervision signal for later generalizing to PartNet\r\n",
      "shapes. The dataset consists of about 120K synthetically generated Chairs and Tables, we visualize\r\n",
      "some of these synthetically generated tables and chairs in Figure 9. Note that no other baseline or\r\n",
      "GFS-Nets has access to this dataset.\r\n",
      "\r\n",
      "PartNet dataset. We use the official level-3 train-test split of PartNet[49]. We use the train split\r\n",
      "of Chair category as our training set, we consider test split of Table category in PartNet our test\r\n",
      "categories. We use this as the train-test split in Section 4.1 of the main paper. We further compare our\r\n",
      "model on the other PartNet categories in supplementary section 8. We set the value of number of\r\n",
      "slots K as 16 for this dataset.\r\n",
      "\r\n",
      "6.2   RGB\r\n",
      "\r\n",
      "CLEVR dataset. We use the train-test split of CLEVR dataset opensourced by [67]. Their dataset\r\n",
      "is built on top of the original CLEVR [30] dataset, where they add additional multi-view rendering.\r\n",
      "Their train/test dataset consists of 5,6,7 objects from random geometric primitives(cylinder, cube and\r\n",
      "sphere) and random colors(red, blue, purple, gray, cyan, yellow, green, brown) placed in random\r\n",
      "locations in the scene. The multi-view setting consists of randomly sampling from 360 degree\r\n",
      "azimuth angles with a fixed elevation angle. Our train data consists of 1000 single object scenes and\r\n",
      "1000 multi-object scenes. The only difference between their train-test data are the novel locations\r\n",
      "at which the objects would be placed in the scene. We set the value of number of slots K as 8. We\r\n",
      "visualize some of the primitive examples (multi-view single-object scenes) of the CLEVR dataset in\r\n",
      "Figure 10.\r\n",
      "\r\n",
      "\r\n",
      "                                                  15\r\n",
      "\f",
      "\n",
      "\n",
      "Figure 7: We visualize the generic primitive templates of [60], as you can see they mainly consists of Cubes,\r\n",
      "Cuboids and Discs.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "   Figure 8: We visualize the category-specific primitives extracted from level-3 Chair category of PartNet.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                      16\r\n",
      "\f",
      "\n",
      "\n",
      "Figure 9: We visualize the synthetic whole shape dataset of [60]. Shape2Prog supervises their model using the\r\n",
      "annotations from this dataset.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Figure 10: We visualize some examples of the primitives used in the CLEVR dataset. Primitives are represented\r\n",
      "as multi-view single object RGB scenes as shown in the figure.\r\n",
      "\r\n",
      "\r\n",
      "RoomDiverse++ dataset. We build RoomDiverse++ on top of the RoomDiverse dataset of [67].\r\n",
      "In which instead of using the solid CLEVR colors as textures of meshes we keep the original textures\r\n",
      "of the ShapeNet meshes, additionaly along with Chairs we also add other ShapeNet categories such\r\n",
      "as Benches, Cabinets, Beds, Tables and Sofas in the dataset. In total we use 20 object meshes and\r\n",
      "3 background textures to generate the dataset. We use the same multi-view rendering settings as\r\n",
      "CLEVR. Our train data consists of 1000 single object scenes and 2000 multi-object scenes. We use\r\n",
      "6,7,8 objects placed at random locations for generating our multi-object train dataset. We use 500 of\r\n",
      "7,8,9 object scenes as our test data. We set the value of number of slots K as 10. We visualize some\r\n",
      "of the primitive examples in the dataset in Figure 11.\r\n",
      "\r\n",
      "\r\n",
      "7     Implementation details\r\n",
      "\r\n",
      "7.1   Proposed Model\r\n",
      "\r\n",
      "Code, training details and computational complexity. Proposed model is implemented in\r\n",
      "Python/PyTorch. We use a batch size of 8 for point cloud input and a batch size of 2 on RGB\r\n",
      "input. We set our learning rate as 10−4 . We use the Adam optimizer with β1 = 0.9, β2 = 0.999. Our\r\n",
      "primitive learning model takes 24 hours (approximately 200k iterations) to converge. Similarly our\r\n",
      "\r\n",
      "\r\n",
      "                                                     17\r\n",
      "\f",
      "\n",
      "\n",
      "Figure 11: We visualize some examples of the primitives used in the Room Diverse++ dataset. Primitives are\r\n",
      "represented as multi-view single object RGB scenes as shown in the figure.\r\n",
      "\r\n",
      "\r\n",
      "primitive composition learning model takes about 12hr (approximately 100k iterations) to converge.\r\n",
      "Our slow inference per example takes about 1 min (500 iterations). A forward pass through the\r\n",
      "proposed model takes about 0.15 secs. We use a single V100 GPU for training and inference.\r\n",
      "\r\n",
      "Inputs. For RGB input we randomly select 2 views which include the RGB and ground-truth\r\n",
      "egomotion. We use a single RGB image for testing. For RGB, our input is a 64 × 64 × 3 tensor, for\r\n",
      "point cloud our input is a 2048 × 3\r\n",
      "\r\n",
      "Point Cloud Encoder. We adopt the point transformer[70] architecture as our encoder. Point\r\n",
      "transformer encoder is essentially layers of self attention blocks. Specifically a self attention block\r\n",
      "includes sampling of query points and updating them using their N most neighbouring points as\r\n",
      "key/value vectors. In the architecture we specifically apply 5 layers of self attention which look as\r\n",
      "follows: 2048-16-64, 2048-16-64, 512-16-64, 512-16-64, 128-16-64, 128-16-64. We use the notation\r\n",
      "of S-N -C, where S is the number of subsampled query points from the point cloud, N is the number\r\n",
      "of neighbouring points and C is the feature dimension. We thus get an output feature map of size\r\n",
      "128 × 64.\r\n",
      "\r\n",
      "RGB Encoder. Our RGB encoder is similar to that of [67], which is basically a U-net. We\r\n",
      "concatenate the RGB values with the pixel coordinates that are normalized in the range of -1 to 1. We\r\n",
      "then pass the concatenated 2D image through a convolutional feature extractor. The architecture of\r\n",
      "the CNN is as follows: 3-2-64, 3-2-64, 3-2-64, 3-1-64, 3-1-64, 3-1-64. We use the notation of k-s-c,\r\n",
      "where k is the kernel size, s is the number of strides and c is the feature channel. Our final output\r\n",
      "size is 64 × 64 × 64.\r\n",
      "\r\n",
      "Point Cloud Decoder. We obtain point occupancies by querying the slot feature vector slotk at\r\n",
      "discrete locations (x, y, z) specifically ox,y,z = Dec(slotk , (x, y, z)). The architecture of Dec is\r\n",
      "similar to that of [40]. Given slotk , which is one of the slot feature vector. We encode the coordinate\r\n",
      "(x, y, z) into a 64-D feature vector using a linear layer. We denote this vector as z. The inputs slotk\r\n",
      "and z are then processed as follows:\r\n",
      "\r\n",
      "   out_k = RB_i( RB_{i-1}( \\cdots RB_1( z + FC_1(slot_k)) \\\\ \\cdots ] + FC_{i-1}(slot_k)) + FC_{i}(slot_k)).\r\n",
      "                                                                                                               (3)\r\n",
      "We set i = 3. F Ci is a linear layer that outputs a 64 dimensional vector. RNi is a 2 layer ResNet\r\n",
      "MLP block [27]. The architecture of ResNet block is: ReLU, 64-64, ReLU, 64-64. Here, i − o\r\n",
      "represents a linear layer, where i and o are the input and output dimension. Finally outk is then\r\n",
      "passed through a ReLU activation function followed by a linear layer to generate a single value for\r\n",
      "occupancy.\r\n",
      "\r\n",
      "RGB Decoder. Our decoder follows the architecture of [67]. The decoder is a 8 layer MLP,\r\n",
      "specifically ReLU, 64-64, ReLU, 64-64 .... 64-4. The MLP takes in as input slotk feature and the\r\n",
      "location x, y, z and predicts the RGB and density values at that location.\r\n",
      "\r\n",
      "\r\n",
      "                                                       18\r\n",
      "\f",
      "\n",
      "\n",
      "Slot Extractor. Following is the pseudo-code of Slot Attention, which we use for learning primitive\r\n",
      "compositions.\r\n",
      "\r\n",
      "Algorithm 1 Slot Attention module. Input is N vectors of dimension Dinputs which is mapped to a\r\n",
      "set of K slots of dimension Dslots . We sample the learnable multivariate gaussian to get the initial\r\n",
      "slot features, here the learnable parameters are: µ ∈ RDslots and σ ∈ RDslots .For our experiments we\r\n",
      "set T = 3.\r\n",
      "1: Input: inputs ∈ RN ×Dinputs , slots ∼ N (µ, diag(σ)) ∈ RK×Dslots\r\n",
      "2: Layer params: k, q, v: linear projections; GRU; MLP; LayerNorm (x3)\r\n",
      "3: inputs = LayerNorm (inputs)\r\n",
      "4: for t = 0 . . . T\r\n",
      "5:     slots_prev = slots\r\n",
      "6:     slots = LayerNorm (slots)\r\n",
      "7:     attn = Softmax ( √1 k(inputs) · q(slots)T , axis=‘slots’)\r\n",
      "                             D\r\n",
      "8:     updates = Avg (weights=attn + ϵ, values=v(inputs))\r\n",
      "9:     slots = GRU (state=slots_prev, inputs=updates)\r\n",
      "10:      slots += MLP (LayerNorm (slots))\r\n",
      "11: return slots\r\n",
      "\r\n",
      "\r\n",
      "For primitive learning stage on PointCloud we set the value of number of slots K as 1. Additionally\r\n",
      "we replace the Softmax () activation in step 7 with a Sigmoid () activation. For primitive learning\r\n",
      "stage on RGB scenes we set the value of number of slots K as 2. This is due to RGB images having\r\n",
      "a background componenet in them.\r\n",
      "\r\n",
      "7.2   Baselines                               Training\r\n",
      "\r\n",
      "\r\n",
      "               Cross Attention                                                                              P\r\n",
      "                 With Heads                               Attention\r\n",
      "                                                                                  C                         O\r\n",
      "                                                            Map\r\n",
      "                                                                                  o                         I\r\n",
      "                                                                                  n                         N\r\n",
      "                                                                                  c                         T\r\n",
      "                                                          Attention               a FPN-Style\r\n",
      "            Encoder\r\n",
      "                                                            Map                   t Transformer             A\r\n",
      "                                                                                  e                         R\r\n",
      "                                                                                  n                         G\r\n",
      "                                                                                  a                         M\r\n",
      "                                                                                  t                         A\r\n",
      "                                                                                  e                         X\r\n",
      "                                                          Attention\r\n",
      "                                                            Map\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                 Head features Head features        Spatial Attention\r\n",
      "                                        T=1           T=2                Maps                     Hungarian Matching\r\n",
      "                                                                                                              +\r\n",
      "                                                                                                  Binary CrossEntropy Loss\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Figure 12: 3D-DETR model architecture. Our baseline model is very similar to 2D-DETR[5], where we replace\r\n",
      "their base 2D encoder and 2D FPN-style CNN with a 3D PointTransformer[70]. Given point clouds as input\r\n",
      "our encoder backbone featurizes the points into N feature vectors, we then do iterative self and cross attention\r\n",
      "using K learnable query heads, following the implementation of Carion et al.[5]. We then compute attention of\r\n",
      "the updated queries wrt to the encoded feature vectors. We then concatenate these attentions along the batch\r\n",
      "dimension and pass them through a FPN-style transformer that increases their resolution and outputs each query\r\n",
      "mask logits. We then do hungarian matching and binary cross entropy loss\r\n",
      "\r\n",
      "\r\n",
      "3D-DETR. 3D-DETR is a SOTA segmentation architecture, we build this architecture on top\r\n",
      "of 2D-DETR[5], where we replace their 2D encoders with a 3D PointTransformer[70]. For fair\r\n",
      "comparision we make sure the number of parameters in 3D-DETR is the same as GFS-Nets.\r\n",
      "The base encoder is a 5 layer architecture of 1024-16-64, 1024-16-64, 1024-16-64, 1024-16-64,\r\n",
      "512-16-64, following the notation of S,N ,C where S is the number of sampled query points, N is\r\n",
      "the number of selected neigbouring points and C is the feature dimension. We have 16 learnable\r\n",
      "\r\n",
      "\r\n",
      "                                                               19\r\n",
      "\f",
      "\n",
      "\n",
      "queries with six encoder-decoder layers of transformer attention, each layer consists of 8 heads. We\r\n",
      "then compute multi-head cross-attention between each query vector and the encoded 3d points. This\r\n",
      "gives us a map of size M × 512 × 16, where M is the number of heads in the multi-head attention.\r\n",
      "Each attention map is individually upsampled through a PointTransformer decoder of the architecture\r\n",
      "512-16-64, 1024-16-64, 1024-16-64, which gives us the final instance segmentation mask per query,\r\n",
      "similar to DETR[5]. We then use Hungarian matching to match the predicted masks against the\r\n",
      "ground truth masks and then apply binary cross entropy loss for each match. We aggregate the losses\r\n",
      "from each query and backpropagate. Figure 12 visualizes the architecture of 3D-DETR. Note that\r\n",
      "we do not follow the 2 stage training of [5], rather we train their model end-to-end for instance\r\n",
      "segmentation, similar to our model. We have found this trick to save compute and still not harm the\r\n",
      "end results.\r\n",
      "\r\n",
      "Learning2Group.[43] We use their open-sourced architecture and code for comparision with our\r\n",
      "mode. We train their model using our datasets from scratch.\r\n",
      "\r\n",
      "Shape2Prog[60] We use their open-sourced architecture, code and pretrained checkpoints for\r\n",
      "comparision with our model. We change the value of number of blocks similar to the number of slots\r\n",
      "in our model for each dataset.\r\n",
      "\r\n",
      "PQ-Nets.[64] We use their open-sourced architecture and code for comparision with our model.\r\n",
      "We train their model using our datasets from scratch. We change the value of number of slots in their\r\n",
      "model based on the maximum number of parts in the dataset.\r\n",
      "\r\n",
      "Struct Implicit.[21] We use their open-sourced architecture and code for comparision with our\r\n",
      "mode. We train their model using our datasets from scratch. We change the value of number of slots\r\n",
      "in their model based on the maximum number of parts in the dataset.\r\n",
      "\r\n",
      "uORF.[67] We use their open-sourced architecture, code and pretrained checkpoints for compari-\r\n",
      "sion with our model on the CLEVR dataset. For RoomDiverse++ dataset, we train their model on\r\n",
      "our dataset using the hyperparmeters they use for training their model on RoomDiverse dataset. We\r\n",
      "change the value of number of slots in their model similar to our model for each dataset.\r\n",
      "\r\n",
      "Slot Attention.[42] We use their open-sourced architecture and code for comparision with our\r\n",
      "model on the CLEVR and RoomDiverse++ dataset. We train their model using our datasets from\r\n",
      "scratch. We change the value of number of slots in their model similar to our model for each dataset.\r\n",
      "\r\n",
      "8   More Results\r\n",
      "Segmenting 3D pointclouds without segmentation supervision. In this section we show ad-\r\n",
      "ditional qualitative and quantitative results for Section 4.1.1 of the main paper. We follow the\r\n",
      "experimental setup of Section 4.1.1, where we use the generic part dataset (visualized in Fig 7) for\r\n",
      "primitive learning. In Table 4, we further extend Table 1 of the main paper to include different levels\r\n",
      "in Chair and Table category. Here (X/Y /Z) refers to (level 1/level 2/level 3) scores respectively. We\r\n",
      "pick the best performing level in fast inference and specifically report it’s slow inference results. We\r\n",
      "further qualitatively compare our model against Shape2Prog (best performing baseline) in Figure 13\r\n",
      "\r\n",
      "                                     in-dist (Chairs)                  out-of-dist (Tables)\r\n",
      "      Method\r\n",
      "                              Fast Infer.         Slow Infer.        Fast Infer.      Slow Infer.\r\n",
      "      Shape2Prog [60]         0.28/0.21/0.26          0.53        0.21/0.23/0.23          0.40\r\n",
      "      PQ-Nets [64]            0.20/0.18/0.16          0.31        0.17/0.14/0.16          0.21\r\n",
      "      StructImplicit [21]     0.18/0.22/0.25          0.23        0.14/0.15/0.17          0.17\r\n",
      "      GFS-Nets                0.51/0.48/0.57          0.62        0.51/0.55/0.60          0.69\r\n",
      "\r\n",
      "Table 4: ARI Segmentation accuracy (higher the better) in the test set of Chair (in-distribution) and Table\r\n",
      "category of PartNet(out-of-distribution). GFS-Nets significantly outperform all of the baselines.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                    20\r\n",
      "\f",
      "\n",
      "\n",
      "RGB image segmentation and view prediction without supervision In Figure 15, we show more\r\n",
      "qualitative results for Section 4.3 of the main paper, where we qualitatively compare our model for\r\n",
      "segmentation against uORF[67] and Slot Attention [42]. We see that slow inference prevents our\r\n",
      "model from missing to detect occluded objects in the scene, where as the baselines have a lot of False\r\n",
      "Positives i.e missed detections.\r\n",
      "\r\n",
      "                        Input          Shape2Prog          GFS-Nets-Fast   GFS-Nets-Slow\r\n",
      "                                       (Supervised)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "        Figure 13: Additional shape decomposition results using generic primitives. (Section 4.1.1)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                      21\r\n",
      "\f",
      "\n",
      "\n",
      "                                    Instance Segmentation\r\n",
      "\r\n",
      "\r\n",
      "                  3D-DETR          GFS-Nets-Slow        3D-DETR        GFS-Nets-Slow\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Figure 14: Additional shape decomposition results using category-specific primitives. (Section 4.1.2)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                                   22\r\n",
      "\f",
      "\n",
      "\n",
      "          Ground Truth   Slot Attention   uORF   GFS-Nets-Fast GFS-Nets-Slow\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Figure 15: Additional RGB segmentation results on RoomDiverse++. (Section 4.3)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                          23\r\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#First filter to have only the References from a pdf\n",
    "\n",
    "def after_references(mypdftext): \n",
    "    keyword1 = 'References'\n",
    "    keyword2 = 'REFERENCES'\n",
    "    keyword3 = 'R EFERENCES'\n",
    "    keyword4 = 'Reference'\n",
    "    keyword5='[1]' \n",
    "\n",
    "    if keyword1 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword1)\n",
    "    elif keyword2 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword2)\n",
    "    elif keyword3 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword3)\n",
    "    elif keyword4 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword4)\n",
    "    elif keyword5 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword5)\n",
    "    else:\n",
    "        after_keyword = mypdftext[:10000]\n",
    "    return after_keyword\n",
    "\n",
    "#All references in a variable\n",
    "\n",
    "references=after_references(mypdftext)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439d489",
   "metadata": {},
   "source": [
    "## Preprocess to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e325730d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First cleaning\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns): \n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s) \n",
    "        return s\n",
    "\n",
    "replacer=RegexpReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e46e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess a text before using IA\n",
    "\n",
    "def preprocess_text(test):\n",
    "\n",
    "    #Removing Numbers\n",
    "    test=re.sub(r'\\d+','',test)\n",
    "\n",
    "    #Removing Letter alone\n",
    "    test = re.sub(r'\\b\\w\\b', ' ', test)\n",
    "    \n",
    "#     #Keep only letter and number\n",
    "#     test=re.sub(\"[^A-Za-z0-9]\",\" \",test)\n",
    "\n",
    "    #Removing white spaces\n",
    "    test=test.strip()\n",
    "    \n",
    "    #Replacer replace\n",
    "    text_replaced = replacer.replace(test)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text_replaced)\n",
    "\n",
    "    #Tokenize words\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = tokenizer.tokenize(sentences[i])\n",
    "\n",
    "    #Remove stop words\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stops=set(stopwords.words('english'))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [word for word in sentences[i] if word not in stops]\n",
    "\n",
    "    #Lemmatize\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer_output=WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            sentences[i][j] = lemmatizer_output.lemmatize(sentences[i][j])\n",
    "\n",
    "\n",
    "    #Join the words back into a sentence.\n",
    "    a=[' '.join(s) for s in sentences]\n",
    "    b=', '.join(a)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c077ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bahdanau Murty Noukhovitch Nguyen de Vries Courville System atic generalization required learned arXiv preprint arXiv Barbu Mayo Alverio Luo Wang Gutfreund Tenenbaum Katz Objectnet large scale bias controlled dataset pushing limit object recognition model Advances neural information processing system van Bergen Kriegeskorte Going circle way forward role currence visual inference Current Opinion Neurobiology Dec http doi org conb http dx doi org conb Burgess Matthey Watters Kabra Higgins Botvinick Lerchner Monet Unsupervised scene decomposition representation arXiv preprint arXiv Carion Massa Synnaeve Usunier Kirillov Zagoruyko End end object detection transformer In European Conference Computer Vision pp Springer Chang Funkhouser Guibas Hanrahan Huang Li Savarese Savva Song Su Xiao Yi Yu ShapeNet An Information Rich Model Repository Tech Rep arXiv c GR Stanford University Princeton University Toyota Technological Institute Chicago Chang Funkhouser Guibas Hanrahan Huang Li Savarese Savva Song Su et al Shapenet An information rich model repository arXiv preprint arXiv Chen Tagliasacchi Zhang Bsp net Generating compact mesh via binary space partitioning In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Chen Yin Fisher Chaudhuri Zhang Bae net branched autoencoder shape co segmentation In Proceedings IEEE CVF International Conference Computer Vision pp Cho Van Merri nboer Gulcehre Bahdanau Bougares Schwenk Bengio Learning phrase representation using rnn encoder decoder statistical machine translation arXiv preprint arXiv Deng Genova Yazdani Bouaziz Hinton Tagliasacchi Cvxnet Learnable convex decomposition In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Deprelle Groueix Fisher Kim Russell Aubry Learning elementary structure shape generation matching arXiv preprint arXiv Du Smith Ulman Tenenbaum Wu Unsupervised discovery physical object video arXiv preprint arXiv Ellis Wong Nye Sable Meyer Cary Morales Hewitt Solar Lezama Tenenbaum Dreamcoder Growing generalizable interpretable knowledge wake sleep bayesian program learning arXiv preprint arXiv Engelcke Jones Posner Reconstruction bottleneck object centric generative model arXiv preprint arXiv Engelcke Kosiorek Jones Posner Genesis Generative scene inference sampling object centric latent representation arXiv preprint arXiv Eslami Heess Weber Tassa Szepesvari Kavukcuoglu Hinton Attend infer repeat Fast scene understanding generative model arXiv preprint arXiv Gao Yang Wu Yuan Fu Lai Zhang Sdm net Deep generative network structured deformable mesh ACM Transactions Graphics TOG Geirhos Rubisch Michaelis Bethge Wichmann Brendel Imagenet trained cnns biased towards texture increasing shape bias improves accuracy robustness arXiv preprint arXiv Genova Cole Sud Sarna Funkhouser Local deep implicit function shape In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Genova Cole Vlasic Sarna Freeman Funkhouser Learning shape template structured implicit function In Proceedings IEEE CVF International Conference Computer Vision pp Goyal Lamb Hoffmann Sodhani Levine Bengio Sch lkopf Recurrent independent mechanism arXiv preprint arXiv Greff Kaufman Kabra Watters Burgess Zoran Matthey Botvinick Lerchner Multi object representation learning iterative variational inference In International Conference Machine Learning pp PMLR Greff Rasmus Berglund Hao Schmidhuber Valpola Tagger Deep unsupervised perceptual grouping arXiv preprint arXiv Greff van Steenkiste Schmidhuber On binding problem artificial neural network CoRR ab http arxiv org ab Guerin Projection mechanism human like reasoning artificial intelligence CoRR ab http arxiv org ab He Zhang Ren Sun Deep residual learning image recognition In Pro ceedings IEEE Conference Computer Vision Pattern Recognition pp Hubert Arabie Comparing partition Journal classification Jo Bengio Measuring tendency cnns learn surface statistical regularity arXiv preprint arXiv Johnson Hariharan Van Der Maaten Fei Fei Lawrence Zitnick Girshick Clevr diagnostic dataset compositional language elementary visual reasoning In Proceedings IEEE conference computer vision pattern recognition pp Johnson Krishna Stark Li Shamma Bernstein Fei Fei Image retrieval using scene graph In Proceedings IEEE Conference Computer Vision Pattern Recognition CVPR June Kabra Burgess Matthey Kaufman Greff Reynolds Lerchner Multi object datasets http github com deepmind multi object datasets Kato Harada Learning view prior single view reconstruction In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Kipf Elsayed Mahendran Stone Sabour Heigold Jonschkowski Dosovitskiy Greff Conditional object centric learning video arXiv preprint arXiv Kosiorek Kim Teh Posner Sequential attend infer repeat Generative modelling moving object Advances Neural Information Processing Systems Kuhn The hungarian method assignment problem Naval research logistics quarterly Kulkarni Kohli Tenenbaum Mansinghka Picture probabilistic program ming language scene perception In Proceedings ieee conference computer vision pattern recognition pp Lake Salakhutdinov Tenenbaum Human level concept learning probabilistic program induction Science Lake Ullman Tenenbaum Gershman Building machine learn think like people Behavioral brain science Lal Prabhudesai Mediratta Harley Fragkiadaki Coconets Continuous contrastive scene representation Li Mao Zhang Freeman Tenenbaum Snavely Wu Multi plane program induction box prior arXiv preprint arXiv Locatello Weissenborn Unterthiner Mahendran Heigold Uszkoreit Dosovitskiy Kipf Object centric learning slot attention Luo Mo Huang Xu Hu Wang Su Learning group bottom framework part discovery unseen category arXiv preprint arXiv Marcus The algebraic mind Integrating connectionism cognitive science MIT press Marr Vision Computational Investigation Human Representation Processing Visual Information Freeman Co New York NY Mescheder Oechsle Niemeyer Nowozin Geiger Occupancy network Learning reconstruction function space In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Mildenhall Srinivasan Tancik Barron Ramamoorthi Ng Nerf Representing scene neural radiance field view synthesis In European conference computer vision pp Springer Misra Girdhar Joulin An end end transformer model object detection In Proceedings IEEE CVF International Conference Computer Vision pp Mo Zhu Chang Yi Tripathi Guibas Su Partnet large scale benchmark fine grained hierarchical part level object understanding In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Niu Li Xu Imstruct Recovering shape structure single rgb image In Proceedings IEEE conference computer vision pattern recognition pp Paschalidou Gool Geiger Learning unsupervised hierarchical part decomposition object single rgb image In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Paschalidou Katharopoulos Geiger Fidler Neural part Learning expressive shape abstraction invertible neural network In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Paschalidou Ulusoy Geiger Superquadrics revisited Learning shape parsing beyond cuboid In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Pathak Agrawal Efros Darrell Curiosity driven exploration self supervised prediction In International conference machine learning pp PMLR Rahaman Goyal Gondal Wuthrich Bauer Sharma Bengio Sch lkopf Srms Spatially structured recurrent module arXiv preprint arXiv Ren He Girshick Sun Faster CNN towards real time object detection region proposal network CoRR ab http arxiv org ab Sabour Frosst Hinton Dynamic routing capsule arXiv preprint arXiv Sitzmann Chan Tucker Snavely Wetzstein Metasdf Meta learning signed distance function Advances Neural Information Processing Systems Sun Wang Liu Miller Efros Hardt Test time training self supervision generalization distribution shift In International Conference Machine Learning pp PMLR Tian Luo Sun Ellis Freeman Tenenbaum Wu Learning infer execute shape program arXiv preprint arXiv Tsai Srivastava Goh Salakhutdinov Capsules inverted dot product attention routing arXiv preprint arXiv Tulsiani Su Guibas Efros Malik Learning shape abstraction assembling volumetric primitive In Proceedings IEEE Conference Computer Vision Pattern Recognition pp Van Steenkiste Chang Greff Schmidhuber Relational neural expectation maximization Unsupervised discovery object interaction arXiv preprint arXiv Wu Zhuang Xu Zhang Chen Pq net generative part seqseq network shape In Proceedings IEEE CVF Conference Computer Vision Pattern Recognition CVPR June Xia Zhang Yang Xue Zhou Yang Gan inversion survey arXiv preprint arXiv Yao Hung Jampani Yang Discovering part image collection In Proceedings IEEE CVF International Conference Computer Vision pp Yu Guibas Wu Unsupervised discovery object radiance field arXiv preprint arXiv Zablotskaia Dominici Sigal Lehrmann Unsupervised video decomposition using spatio temporal iterative inference arXiv preprint arXiv Zhang Donahue Girshick Darrell Part based cnns fine grained category detection In European conference computer vision pp Springer Zhao Jiang Jia Torr Koltun Point transformer In Proceedings IEEE CVF International Conference Computer Vision pp Zhou Girdhar Joulin Kr henb hl Misra Detecting twenty thousand class using image level supervision arXiv preprint arXiv Zhu Kr henb hl Shechtman Efros Generative visual manipulation natural image manifold In European conference computer vision pp Springer Zoran Kabra Lerchner Rezende Parts Unsupervised segmentation slot attention independence maximization In Proceedings IEEE CVF International Conference Computer Vision pp Zuffi Kanazawa Berger Wolf Black Three safari Learning estimate zebra pose shape texture image wild In Proceedings IEEE CVF International Conference Computer Vision pp Appendix The structure appendix follows In Section cover detail datasets In Section specify implementation detail In Section provide additional qualitative quantitative result experiment Section main paper Datasets Point Cloud For task subsample input point cloud standard size point Generic primitive part dataset We use primitive dataset learning primitive distribution Section The dataset consists primitive instance sampled primitive template visualized Figure Instances sampled template changing size placing random location similar Category specific primitive PartNet dataset We seperate individual segment level instance segmentation mask official train split Chair category PartNet dataset We additionally rotation translation based augmentation individual segment We visualize primitive example Figure We use dataset learning primitive distribution Section The dataset total consists primitive example Synthetic whole shape dataset This dataset generated ShapeProg The segmentation label dataset used supervision signal later generalizing PartNet shape The dataset consists synthetically generated Chairs Tables visualize synthetically generated table chair Figure Note baseline GFS Nets access dataset PartNet dataset We use official level train test split PartNet We use train split Chair category training set consider test split Table category PartNet test category We use train test split Section main paper We compare model PartNet category supplementary section We set value number slot dataset RGB CLEVR dataset We use train test split CLEVR dataset opensourced Their dataset built top original CLEVR dataset add additional multi view rendering Their train test dataset consists object random geometric primitive cylinder cube sphere random color red blue purple gray cyan yellow green brown placed random location scene The multi view setting consists randomly sampling degree azimuth angle fixed elevation angle Our train data consists single object scene multi object scene The difference train test data novel location object would placed scene We set value number slot We visualize primitive example multi view single object scene CLEVR dataset Figure Figure We visualize generic primitive template see mainly consists Cubes Cuboids Discs Figure We visualize category specific primitive extracted level Chair category PartNet Figure We visualize synthetic whole shape dataset ShapeProg supervises model using annotation dataset Figure We visualize example primitive used CLEVR dataset Primitives represented multi view single object RGB scene shown figure RoomDiverse dataset We build RoomDiverse top RoomDiverse dataset In instead using solid CLEVR color texture mesh keep original texture ShapeNet mesh additionaly along Chairs also add ShapeNet category Benches Cabinets Beds Tables Sofas dataset In total use object mesh background texture generate dataset We use multi view rendering setting CLEVR Our train data consists single object scene multi object scene We use object placed random location generating multi object train dataset We use object scene test data We set value number slot We visualize primitive example dataset Figure Implementation detail Proposed Model Code training detail computational complexity Proposed model implemented Python PyTorch We use batch size point cloud input batch size RGB input We set learning rate We use Adam optimizer Our primitive learning model take hour approximately iteration converge Similarly Figure We visualize example primitive used Room Diverse dataset Primitives represented multi view single object RGB scene shown figure primitive composition learning model take hr approximately iteration converge Our slow inference per example take min iteration forward pas proposed model take sec We use single GPU training inference Inputs For RGB input randomly select view include RGB ground truth egomotion We use single RGB image testing For RGB input tensor point cloud input Point Cloud Encoder We adopt point transformer architecture encoder Point transformer encoder essentially layer self attention block Specifically self attention block includes sampling query point updating using neighbouring point key value vector In architecture specifically apply layer self attention look follows We use notation number subsampled query point point cloud number neighbouring point feature dimension We thus get output feature map size RGB Encoder Our RGB encoder similar basically net We concatenate RGB value pixel coordinate normalized range We pas concatenated image convolutional feature extractor The architecture CNN follows We use notation kernel size number stride feature channel Our final output size Point Cloud Decoder We obtain point occupancy querying slot feature vector slotk discrete location specifically ox Dec slotk The architecture Dec similar Given slotk one slot feature vector We encode coordinate feature vector using linear layer We denote vector The input slotk processed follows k RB RB cdots RB FC slot k cdots FC slot k FC slot k We set Ci linear layer output dimensional vector RNi layer ResNet MLP block The architecture ResNet block ReLU ReLU Here represents linear layer input output dimension Finally outk passed ReLU activation function followed linear layer generate single value occupancy RGB Decoder Our decoder follows architecture The decoder layer MLP specifically ReLU ReLU The MLP take input slotk feature location predicts RGB density value location Slot Extractor Following pseudo code Slot Attention use learning primitive composition Algorithm Slot Attention module Input vector dimension Dinputs mapped set slot dimension Dslots We sample learnable multivariate gaussian get initial slot feature learnable parameter RDslots RDslots For experiment set Input input RN Dinputs slot diag RK Dslots Layer params linear projection GRU MLP LayerNorm input LayerNorm input slot prev slot slot LayerNorm slot attn Softmax input slot axis slot update Avg weight attn value input slot GRU state slot prev input update slot MLP LayerNorm slot return slot For primitive learning stage PointCloud set value number slot Additionally replace Softmax activation step Sigmoid activation For primitive learning stage RGB scene set value number slot This due RGB image background componenet Baselines Training Cross Attention With Heads Attention Map Attention FPN Style Encoder Map Transformer Attention Map Head feature Head feature Spatial Attention Maps Hungarian Matching Binary CrossEntropy Loss Figure DETR model architecture Our baseline model similar DETR replace base encoder FPN style CNN PointTransformer Given point cloud input encoder backbone featurizes point feature vector iterative self cross attention using learnable query head following implementation Carion et al We compute attention updated query wrt encoded feature vector We concatenate attention along batch dimension pas FPN style transformer increase resolution output query mask logits We hungarian matching binary cross entropy loss DETR DETR SOTA segmentation architecture build architecture top DETR replace encoders PointTransformer For fair comparision make sure number parameter DETR GFS Nets The base encoder layer architecture following notation number sampled query point number selected neigbouring point feature dimension We learnable query six encoder decoder layer transformer attention layer consists head We compute multi head cross attention query vector encoded point This give u map size number head multi head attention Each attention map individually upsampled PointTransformer decoder architecture give u final instance segmentation mask per query similar DETR We use Hungarian matching match predicted mask ground truth mask apply binary cross entropy loss match We aggregate loss query backpropagate Figure visualizes architecture DETR Note follow stage training rather train model end end instance segmentation similar model We found trick save compute still harm end result LearningGroup We use open sourced architecture code comparision mode We train model using datasets scratch ShapeProg We use open sourced architecture code pretrained checkpoint comparision model We change value number block similar number slot model dataset PQ Nets We use open sourced architecture code comparision model We train model using datasets scratch We change value number slot model based maximum number part dataset Struct Implicit We use open sourced architecture code comparision mode We train model using datasets scratch We change value number slot model based maximum number part dataset uORF We use open sourced architecture code pretrained checkpoint compari sion model CLEVR dataset For RoomDiverse dataset train model dataset using hyperparmeters use training model RoomDiverse dataset We change value number slot model similar model dataset Slot Attention We use open sourced architecture code comparision model CLEVR RoomDiverse dataset We train model using datasets scratch We change value number slot model similar model dataset More Results Segmenting pointclouds without segmentation supervision In section show ad ditional qualitative quantitative result Section main paper We follow experimental setup Section use generic part dataset visualized Fig primitive learning In Table extend Table main paper include different level Chair Table category Here refers level level level score respectively We pick best performing level fast inference specifically report slow inference result We qualitatively compare model ShapeProg best performing baseline Figure dist Chairs dist Tables Method Fast Infer Slow Infer Fast Infer Slow Infer ShapeProg PQ Nets StructImplicit GFS Nets Table ARI Segmentation accuracy higher better test set Chair distribution Table category PartNet distribution GFS Nets significantly outperform baseline RGB image segmentation view prediction without supervision In Figure show qualitative result Section main paper qualitatively compare model segmentation uORF Slot Attention We see slow inference prevents model missing detect occluded object scene baseline lot False Positives missed detection Input ShapeProg GFS Nets Fast GFS Nets Slow Supervised Figure Additional shape decomposition result using generic primitive Section Instance Segmentation DETR GFS Nets Slow DETR GFS Nets Slow Figure Additional shape decomposition result using category specific primitive Section Ground Truth Slot Attention uORF GFS Nets Fast GFS Nets Slow Figure Additional RGB segmentation result RoomDiverse Section\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "\n",
    "references_clean= preprocess_text(references)\n",
    "print(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5af5e9",
   "metadata": {},
   "source": [
    "## Get_human_names Algorithme using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa296d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    \n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    for person in person_list:\n",
    "        person_split = person.split(\" \")\n",
    "        for name in person_split:\n",
    "            if wordnet.synsets(name):\n",
    "                if(name in person):\n",
    "                    person_names.remove(person)\n",
    "                    break\n",
    "    return person_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "864d30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 48 and without 36\n",
      "Ex: Murty Noukhovitch Nguyen\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "print(\"Len with preprocess\",len(get_human_names(references_clean)), \"and without\",len(get_human_names(references)))\n",
    "print(\"Ex:\",get_human_names(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8dd9ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Murty Noukhovitch Nguyen',\n",
       " 'Geirhos Rubisch Michaelis Bethge Wichmann Brendel Imagenet',\n",
       " 'Steenkiste Schmidhuber',\n",
       " 'Hubert Arabie',\n",
       " 'Springer Misra Girdhar Joulin An',\n",
       " 'Wetzstein Metasdf Meta',\n",
       " 'Shechtman Efros',\n",
       " 'Algorithm Slot',\n",
       " 'Nets Slow Figure Additional',\n",
       " 'V100 GPU',\n",
       " 'Loss Figure',\n",
       " 'Bergen Kriegeskorte',\n",
       " 'Guibas Hanrahan Huang Li Savarese Savva Song Su',\n",
       " 'Kim Russell Aubry Learning',\n",
       " 'Goyal Lamb Hoffmann Sodhani Levine Bengio Sch',\n",
       " 'Johnson Hariharan Van Der Maaten Fei Fei Lawrence Zitnick Girshick Clevr',\n",
       " 'Kim Teh Posner Sequential',\n",
       " 'Marr Vision Computational',\n",
       " 'Niu Li Xu Imstruct',\n",
       " 'Srms Spatially',\n",
       " 'Van Steenkiste Chang Greff',\n",
       " 'Misra Detecting',\n",
       " 'Figure Instances',\n",
       " 'Figure Implementation',\n",
       " 'Slot Extractor',\n",
       " 'Struct Implicit',\n",
       " 'False Positives',\n",
       " 'Ground Truth Slot',\n",
       " 'Current Opinion Neurobiology Dec',\n",
       " 'Rich Model Repository Tech Rep',\n",
       " 'Gulcehre Bahdanau Bougares Schwenk Bengio Learning',\n",
       " 'Morales Hewitt Solar Lezama Tenenbaum',\n",
       " 'Kabra Watters Burgess Zoran Matthey Botvinick Lerchner Multi',\n",
       " 'Bengio Measuring',\n",
       " 'Kabra Burgess Matthey Kaufman Greff Reynolds Lerchner Multi',\n",
       " 'Science Lake Ullman Tenenbaum Gershman Building',\n",
       " 'Luo Mo Huang Xu Hu Wang Su Learning',\n",
       " 'Zhu Chang Yi Tripathi Guibas Su Partnet',\n",
       " 'Paschalidou Katharopoulos Geiger Fidler Neural',\n",
       " 'Sabour Frosst Hinton Dynamic',\n",
       " 'Zhang Yang Xue Zhou Yang Gan',\n",
       " 'Jiang Jia Torr Koltun Point',\n",
       " 'Zuffi Kanazawa Berger Wolf Black Three',\n",
       " 'Figure Note',\n",
       " 'Model Code',\n",
       " 'Point Cloud',\n",
       " 'Dslots Layer',\n",
       " 'Spatial Attention Maps Hungarian Matching Binary',\n",
       " 'Nets Table ARI',\n",
       " 'Nets Fast']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_human_names(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fcb76",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "656d73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "def nlp_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    PROPN=[token.lemma_ for token in doc if token.pos_ == \"PROPN\"]\n",
    "    \n",
    "#     Remove duplicate\n",
    "    PROPN = list(dict.fromkeys(PROPN))\n",
    "#     Remove word with first letter as lowercase \n",
    "    for word in PROPN:\n",
    "        if word[0].islower():\n",
    "            PROPN.remove(word)\n",
    "    return PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4230cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492\n"
     ]
    }
   ],
   "source": [
    "final_names_prep_spacy=nlp_entities(references_clean)\n",
    "final_names_spacy=nlp_entities(references)\n",
    "print(len(final_names_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d123b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An other function more accurate\n",
    "\n",
    "def extract(text:str) :\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    doc = spacy_nlp(text.strip())\n",
    "    named_entities = []\n",
    "    \n",
    "    for i in doc.ents:\n",
    "        entry = str(i.lemma_).lower()\n",
    "        text = text.replace(str(i).lower(), \"\")\n",
    "        if i.label_ in [\"ART\", \"EVE\", \"NAT\", \"PERSON\"]:\n",
    "            named_entities.append(entry.title().replace(\" \", \"_\").replace(\"\\n\",\"_\"))\n",
    "        named_entities = list(dict.fromkeys(named_entities))\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f8b14fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 92 and without 74\n",
      "Ex: Barbu_Mayo_Alverio_Luo_Wang_Gutfreund\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess and With preprocess\n",
    "print(\"Len with preprocess\",len(extract(references_clean)), \"and without\",len(extract(references)))\n",
    "print(\"Ex:\",extract(references_clean)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "167b9906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barbu_Mayo_Alverio_Luo_Wang_Gutfreund',\n",
       " 'Monet',\n",
       " 'Kirillov_Zagoruyko',\n",
       " 'Chang',\n",
       " 'Guibas_Hanrahan',\n",
       " 'Huang_Li_Savarese_Savva_Song_Su_Xiao_Yi_Yu_Shapenet',\n",
       " 'Huang_Li_Savarese_Savva_Song_Su',\n",
       " 'Chen_Tagliasacchi_Zhang_Bsp',\n",
       " 'Chen_Yin_Fisher',\n",
       " 'Chaudhuri_Zhang_Bae',\n",
       " 'Cho_Van_Merri',\n",
       " 'Deprelle_Groueix',\n",
       " 'Kim_Russell',\n",
       " 'Wu_Unsupervised',\n",
       " 'Ellis_Wong_Nye_Sable_Meyer_Cary',\n",
       " 'Morales_Hewitt',\n",
       " 'Dreamcoder_Grow',\n",
       " 'Engelcke_Kosiorek_Jones_Posner_Genesis',\n",
       " 'Eslami_Heess_Weber',\n",
       " 'Hinton_Attend',\n",
       " 'Gao_Yang',\n",
       " 'Wu_Yuan',\n",
       " 'Wichmann_Brendel_Imagenet',\n",
       " 'Goyal_Lamb_Hoffmann',\n",
       " 'Levine_Bengio',\n",
       " 'Greff_Kaufman',\n",
       " 'Lerchner_Multi',\n",
       " 'Greff_Van_Steenkiste_Schmidhuber',\n",
       " 'Hubert_Arabie_Comparing',\n",
       " 'Jo_Bengio',\n",
       " 'Johnson_Hariharan_Van_Der_Maaten_Fei_Fei_Lawrence_Zitnick_Girshick_Clevr',\n",
       " 'Johnson_Krishna',\n",
       " 'Stark_Li',\n",
       " 'Shamma_Bernstein_Fei_Fei_Image',\n",
       " 'Kipf_Elsayed_Mahendran_Stone',\n",
       " 'Heigold_Jonschkowski',\n",
       " 'Kosiorek_Kim',\n",
       " 'Teh_Posner',\n",
       " 'Lal_Prabhudesai',\n",
       " 'Coconets_Continuous',\n",
       " 'Li_Mao_Zhang',\n",
       " 'Wu_Multi',\n",
       " 'Locatello_Weissenborn_Unterthiner',\n",
       " 'Luo_Mo_Huang_Xu',\n",
       " 'Mildenhall_Srinivasan',\n",
       " 'Barron_Ramamoorthi_Ng_Nerf',\n",
       " 'Springer_Misra',\n",
       " 'Mo_Zhu_Chang_Yi_Tripathi_Guibas_Su_Partnet',\n",
       " 'Niu_Li',\n",
       " 'Paschalidou_Gool_Geiger_Learning',\n",
       " 'Katharopoulos_Geiger_Fidler_Neural',\n",
       " 'Goyal_Gondal',\n",
       " 'Sharma_Bengio',\n",
       " 'Srm_Spatially',\n",
       " 'Sitzmann_Chan_Tucker',\n",
       " 'Wetzstein_Metasdf_Meta',\n",
       " 'Wang_Liu',\n",
       " 'Efros_Hardt',\n",
       " 'Wu_Learning',\n",
       " 'Tsai_Srivastava',\n",
       " 'Tulsiani_Su_Guibas',\n",
       " 'Wu_Zhuang_Xu',\n",
       " 'Zhang_Chen_Pq',\n",
       " 'Zhang_Yang',\n",
       " 'Xue_Zhou',\n",
       " 'Yang_Gan',\n",
       " 'Yao_Hung_Jampani_Yang_Discovering',\n",
       " 'Yu_Guibas',\n",
       " 'Wu',\n",
       " 'Zablotskaia_Dominici_Sigal_Lehrmann_Unsupervised',\n",
       " 'Zhang_Donahue',\n",
       " 'Girshick_Darrell',\n",
       " 'Springer_Zhao',\n",
       " 'Jia_Torr_Koltun_Point',\n",
       " 'Girdhar_Joulin_Kr',\n",
       " 'Zhu_Kr',\n",
       " 'Shechtman_Efros',\n",
       " 'Springer_Zoran',\n",
       " 'Zuffi_Kanazawa',\n",
       " 'Chairs_Tables',\n",
       " 'Cubes_Cuboids_Discs',\n",
       " 'Additionaly',\n",
       " 'Chairs',\n",
       " 'Ci',\n",
       " 'Rdslots_Rdslots',\n",
       " 'Input',\n",
       " 'Dslots_Layer',\n",
       " 'Avg',\n",
       " 'Struct_Implicit',\n",
       " 'Fig',\n",
       " 'Chair',\n",
       " 'Tables_Method_Fast_Infer_Slow_Infer_Fast_Infer_Slow_Infer']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract(references_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7a418",
   "metadata": {},
   "source": [
    "### NLTK - StanfordNERTagger (quite long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15b08f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('You', 'O'), ('can', 'O'), ('call', 'O'), ('me', 'O'), ('Billiy', 'PERSON'), ('Bubu', 'PERSON'), ('and', 'O'), ('I', 'O'), ('live', 'O'), ('in', 'O'), ('Amsterdam.', 'LOCATION')]\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "java_path = \"C:/Program Files/Java/jdk-17.0.1/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "st = StanfordNERTagger('stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner-2020-11-17/stanford-ner.jar')\n",
    "\n",
    "st = StanfordNERTagger('stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner-2020-11-17/stanford-ner.jar')\n",
    "print (st.tag('You can call me Billiy Bubu and I live in Amsterdam.'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee89f9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Stanford_array=[]\n",
    "def StanfordNER(text):\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        tokens = nltk.tokenize.word_tokenize(sent)\n",
    "        tags = st.tag(tokens)\n",
    "        for tag in tags:\n",
    "            if tag[1]=='PERSON': \n",
    "                Stanford_array.append(tag[0])\n",
    "    return Stanford_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82c14844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 273\n",
      "Ex: Bahdanau\n"
     ]
    }
   ],
   "source": [
    "#With preprocess\n",
    "final_names_prep_Stanford=StanfordNER(references_clean)\n",
    "print(\"Len with preprocess\",len(final_names_prep_Stanford))\n",
    "print(\"Ex:\",final_names_prep_Stanford[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dbd17",
   "metadata": {},
   "source": [
    "## TextBlob : TextBlob est une bibliothèque python et propose une API simple pour accéder à ses méthodes et effectuer des tâches NLP de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86155dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 984\n",
      "Ex: Bahdanau\n"
     ]
    }
   ],
   "source": [
    "final_names_TextBlob= TextBlob(references_clean)\n",
    "PROPN=[words for words, tag in final_names_TextBlob.tags if tag == \"NNP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cc696d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len with preprocess 984\n",
      "Ex: Bahdanau\n"
     ]
    }
   ],
   "source": [
    "#Without preprocess\n",
    "final_names_TextBlob=PROPN\n",
    "#With preprocess\n",
    "print(\"Len with preprocess\",len(final_names_TextBlob))\n",
    "print(\"Ex:\",final_names_TextBlob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60bbf0",
   "metadata": {},
   "source": [
    "Also efficence but not useful here because spacy hasa better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb67165",
   "metadata": {},
   "source": [
    "## Apply Spacy functions on all pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b585bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'                   Generating Fast and Slow:\\r\\n             Scene Decomposition via Reconstruction\\r\\n\\r\\n\\r\\n  Mihir Prabhudesai1      Anirudh Goyal2                  Deepak Pathak1         Katerina Fragkiadaki1\\r\\n               1                                           2\\r\\n                 Carnegie Mellon University                  Mila, University of Montreal\\r\\n           {mprabhud, dpathak, katef}@cs.cmu.edu                  anirudhgoyal9119@gmail.com\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 1: Point-cloud and image decompositions with Generating Fast and Slow Networks (GFS-Nets).\\r\\nGFS-Nets parse completely novel scenes into familiar entities via slow inference, i.e., gradient descent on the\\r\\nreconstruction error of the scene example under consideration. Left: GFS-Nets outperform a state-of-the-art\\r\\n3D-DETR detector by 50% in segmentation accuracy in out-of-distribution 3D point clouds, when trained on\\r\\nthe same training data. Right: GFS-Nets outperform state-of-the-art unsupervised generative models of Slot\\r\\nAttention [42] and uORF [67] in RGB image decomposition.\\r\\n\\r\\n                                                 Abstract\\r\\n         We consider the problem of segmenting scenes into constituent entities, i.e. under-\\r\\n         lying objects and their parts. Current supervised visual detectors though impressive\\r\\n         within their training distribution, often fail to segment out-of-distribution scenes\\r\\n         into their constituent entities. Recent slot-centric generative models break such\\r\\n         dependence on supervision, by attempting to segment scenes into entities unsuper-\\r\\n         vised, by reconstructing pixels. However, they have been restricted thus far to toy\\r\\n         scenes as they suffer from a reconstruction-segmentation trade-off: as the entity\\r\\n         bottleneck gets wider, reconstruction improves but then the segmentation collapses.\\r\\n         We propose GFS-Nets (Generating Fast and Slow Networks) that alleviate this issue\\r\\n         with two ingredients: i) curriculum training in the form of primitives, often missing\\r\\n         from current generative models and, ii) test-time adaptation per scene through gra-\\r\\n         dient descent on the reconstruction objective, what we call slow inference, missing\\r\\n         from current feed-forward detectors. We show the proposed curriculum suffices\\r\\n         to break the reconstruction-segmentation trade-off, and slow inference greatly\\r\\n         improves segmentation in out-of-distribution scenes. We evaluate GFS-Nets in 3D\\r\\n         and 2D scene segmentation benchmarks of PartNet, CLEVR, Room Diverse++,\\r\\n         and show large (\\xe2\\x88\\xbc 50%) performance improvements against SOTA supervised\\r\\n         feed-forward detectors and unsupervised object discovery methods.\\r\\n\\r\\nPreprint. Under review.\\r\\n\\x0c\\n\\n1    Introduction\\r\\n\\r\\nScenes are composed of objects and objects are composed of parts, and this compositional organization\\r\\nof the perceptual representations is considered a critical component for the level of combinatorial\\r\\ngeneralization that humans are capable of, that extends far beyond their direct experiences [25].\\r\\nEven when encountered with a truly novel image, humans try to parse it in terms of known entity\\r\\ncomponents [26]. Such an entity-centric understanding or reasoning of scenes allows humans to\\r\\ngeneralize known concepts to out-of-distribution examples [25, 39, 44].\\r\\nAn expensive way to build this entity-centric understanding is to supervise the part parsing using\\r\\nhuman labels for training object and parts detection or segmentation in images and 3D point clouds [56,\\r\\n69, 5, 71]. However, these part detectors are category specific and often fail outside the training\\r\\ndistribution [29, 19, 2]. Consider the abstract shape shown in Figure 1 (last row on the left). We can\\r\\nintuitively figure out the meaningful entities that this shape could be broken into. Yet, a state-of-the-art\\r\\ntransformer based 3D-DETR detector [5], when trained supervised for segmenting chairs, fails to\\r\\ndecompose these abstract shapes into entities, even though these shapes contain familiar (chair) parts.\\r\\nAn underlying issue is that these models do not receive any top-down feedback [3] from entities, i.e.,\\r\\nthey do not have any entity-centric reasoning rather, they are purely feed-forward in nature, i.e., the\\r\\ninformation is routed from input to output layers in a sequential manner. How do we enable top-down\\r\\nfeedback between the scene and its constituent parts?\\r\\nThere has been interest recently in building models that segment scenes into entities in an unsupervised\\r\\nway by optimizing a reconstruction objective [42, 17, 24, 63, 57, 35, 16, 4, 23, 68, 55, 13, 22, 73, 34].\\r\\nThese methods differ in details but share the notion of having a fixed set of entities, also known as\\r\\nslots or object files. Each slot extracts information about a single entity during encoding, and it is\\r\\n\\xe2\\x80\\x9crendered\" back to the input during decoding, thus, exhibiting top down feedback. Yet, it has been\\r\\nhard to scale the results of these models beyond toy datasets.\\r\\nWe argue this is because these models answer two questions at once: understanding what the entity is\\r\\nand where it is present in the input. This creates a chicken-and-egg problem because unless we have\\r\\na representation for an entity, we can\\xe2\\x80\\x99t find where it is in the scene, and unless we localize the entity,\\r\\nwe can\\xe2\\x80\\x99t learn a representation of what it is. Hence, a trivial solution often encountered is to assign\\r\\none slot to the input and other slots to nothing, as noted in Engelcke et al. [15] and also confirmed by\\r\\nour experiments in Section 4.1.1 and 4.2.\\r\\nWe propose Generating Fast and Slow Networks (GFS-Nets), a slot-centric autoencoder that segments\\r\\nscenes while optimizing a reconstruction objective. Our model builds on top of Slot Attention[42],\\r\\nwith two key insights: First, it uses curriculum to learn the what question (what entities are) before\\r\\ngoing to the where question (where they are) it does this by training the model to autoencode primitive\\r\\nentities using a single slot bottleneck. This collection of primitive entities can either be collected\\r\\nby design [60] or can be taken from a collection of part segmentations [49]. Second, inference in\\r\\nGFS-Nets is performed in two ways: i) Fast inference where a visual scene is simply encoded and\\r\\ndecoded and no model weights are updated. ii) Slow inference where the weights of the model are\\r\\nadapted by gradient descent while optimizing the reconstruction objective. In this way, GFS-Nets\\r\\nadapts to a truly novel scene by projecting it onto a learned distribution of entities, while also\\r\\nslowly adapting the entity distribution. Slow inference in GFS-Nets successfully parses completely\\r\\nunfamiliar scenes into familiar entities, implementing a form of search for explanations of the scene\\r\\nat inference, as we show in Figure 3.\\r\\nOur slow inference is related to test-time adaptation or optimization [72, 54, 59, 58, 74, 65]. However,\\r\\ntest-time adaptation has not been used so far in entity-centric models to improve scene decomposition.\\r\\nIn fact, it has been recently noted that reconstruction loss can make entity segmentation worse, to\\r\\nquote Engelcke et al. [15], \\xe2\\x80\\x9cIf the bottleneck is too narrow, segmentation and reconstruction degrade.\\r\\nIf it is too wide, reconstruction is reasonable but the model collapses to a single component and no\\r\\nuseful segmentation is learned.\" GFS-Nets alleviate from this issue by coupling curriculum training\\r\\nwith slow inference for entity-centric learning.\\r\\n\\r\\n    Project page: https://mihirp1998.github.io/project_pages/gfsnets/\\r\\n\\r\\n\\r\\n                                                     2\\r\\n\\x0c\\n\\nWe test GFS-Nets on the following datasets: PartNet [49], ShapeNet[7], CLEVR [30] and a difficult\\r\\nversion of Room Diverse [67]. We evaluate GFS-Nets\\xe2\\x80\\x99s ability to parse out-of-distribution scenes\\r\\nand compare it against state of the art entity-centric generative models [42, 67], program synthesis\\r\\nmodels [60], 3D unsupervised part discovery models [21, 64] and state of the art supervised visual\\r\\ndetectors [5, 43] trained with labeled data to segment entities. We show improvements across all\\r\\nbaselines in our ability to segment novel scenes. Additionally, we ablate different design choices of\\r\\nGFS-Nets. We will make our code and datasets publicly available to the community.\\r\\n\\r\\n2   Related Work\\r\\nEntity-centric generative models for scene and structure decomposition Entity-centric mod-\\r\\nels attempt to segment a scene into objects and parts while autoencoding images. MONet[4],\\r\\nGENESIS[16] and IODINE[23] perform multiple steps between their encoder and the decoder to\\r\\noutput a set of independent latent vectors that describe objects in the image. This iterative encode-\\r\\ndecode inference helps them in receiving top-down feedback. However, this also results in making\\r\\nthe system computationally inefficient and inflexible. Slot Attention [42] on the other hand receives\\r\\ntop-down feedback within a single encoding step. The idea of having a single-encode step makes\\r\\nthe process of extracting out symbols modular and computationally efficient. However, all methods\\r\\nabove fail on non-toy scenes. We attribute this failure to the what or where problem, a.k.a. symmetry\\r\\nproblem, pointed out in the \\xe2\\x80\\x98Instance Slots\\xe2\\x80\\x99 Section in Greff et al.[25]: \\xe2\\x80\\x9cbottom-up information is\\r\\nunable to break the symmetry amongst slots and thus ends up assigning the same content to each\\r\\none.\" In GFS-Nets, we propose to circumvent this problem using curriculum training.\\r\\n\\r\\nShape program synthesis and analysis-by-synthesis GFS-Nets is also related to works in\\r\\nanalysis-by-synthesis [37], program synthesis for shape prediction [60, 14, 41], as well as ear-\\r\\nlier works on Computer Vision, such as Marr\\xe2\\x80\\x99s 3D sketch [45] which involves representing a scene\\r\\nin terms of generalized cylinders and their syntactic relations to each other. In place of data-driven\\r\\nMarkov Chain Monte Carlo search of analysis-by-synthesis methods that require good initialization,\\r\\nour slow inference searches in the space of primitives by gradient descent. In contrast to program\\r\\nsynthesis methods, it does not require a predefined domain-specific language (DSL) or program\\r\\nannotations for visual structures [41], rather, it discovers compositions over primitives via its slow\\r\\ninference.\\r\\n\\r\\nUnsupervised 3D Part Discovery There are numerous methods that attempt the decomposition of\\r\\ncomplex 3D shapes into primitive parts without primitive supervision[33, 21, 51, 18, 12, 62, 11, 9].\\r\\nTraditional primitives include cuboids [62, 50], superquadrics [53, 51], and convexes [11, 8]. [20]\\r\\nproposes a 3D representation that decomposes space into a structured set of implicit functions [21].\\r\\nNeural Parts [52] represents arbitrarily complex genus-zero shapes and thus yields comparatively\\r\\nexpressive parts. However the resulting parts of these methods[20, 52] are still not semantically\\r\\nmeaningful and the decomposition is highly dependent on the number of parts initialized. The work\\r\\nof [66] does 3D part reconstruction directly from a 2D image input without access to any ground-truth\\r\\n3D shapes for training. However, both [66] and [52] take as input the number of parts, and different\\r\\ndecompositions are predicted with varying part numbers. There is no clear way to select the right\\r\\nnumber of parts. In our case, parts can be quite complex: pairs of parallel surfaces, quadruplets of\\r\\nlegs, as we use implicit functions to represent them. Moreover GFS-Nets\\xe2\\x80\\x99s dynamic attention-based\\r\\nrouting allows it to infer different number of parts for each input scene.\\r\\n\\r\\n3   Generating Fast and Slow (GFS-Nets)\\r\\nThe goal of GFS-Nets is to decompose an unfamiliar scene into familiar entities. The model encodes\\r\\nthe scene, which can be a 3D point cloud or an RGB image, into a set of slot vectors and decodes\\r\\nback the scene using a decoder shared across slots. The model uses the slot attention feature-to-slot\\r\\nmapping of Locatello et al.[42], where visual features are softly partitioned across slots through\\r\\nattention, and update the slot vectors.\\r\\nGFS-Nets is trained with a curriculum. First, it is first trained to autoencode individual entities using\\r\\na single slot in their bottleneck, as shown in the left of Figure 2 and described in Section 3.2. Next,\\r\\nGFS-Nets is trained to autoencode complex scenes using multiple slots in their bottleneck, as shown\\r\\n\\r\\n\\r\\n                                                   3\\r\\n\\x0c\\n\\nStage 1: Autoencoding primitive entities                                                                        Stage 2: Autoencoding scenes\\r\\n\\r\\n3D Object Decomposition                                                                                                                                       Competition\\r\\n                                                                                                                                                             Amongst Slots\\r\\n                                                                                                                                                                         sampling N slots\\r\\n                                                       sampling 1 slot\\r\\n                    Point                                                                                                             Point                                                   slot\\r\\n                                                                                                                                                                                               slot\\r\\n                                                                           slot                                                                                                                 slot         Max-Pool\\r\\n                 Transformer                                                                                                       Transformer                                              decoder\\r\\n                                                                                                                                                                                                  slot\\r\\n                                                                                                                                                                                             decoder\\r\\n                                                                         decoder                                                                                                              decoder\\r\\n                                                                                                                                                                                                    slot\\r\\n                   Encoder                                                                                                           Encoder                                                   decoder\\r\\n                                                                                                                                                                                                 decoder\\r\\n\\r\\n\\r\\n\\r\\n                                      reconstruction loss                                                                                                          reconstruction loss\\r\\n\\r\\n 2D Scene Decomposition\\r\\n                                Competition                                                                                                             Competition\\r\\n                               Amongst Slots Foreground                                                                                                Amongst Slots Foreground\\r\\n                                         sampling 1 slot                                                                                                            sampling N slots\\r\\n                                                             slot                                                                                                                        slot\\r\\n                                                                                                                                                                                          slot             Weighted\\r\\n                                                               slot                                                                                                                        slot\\r\\n                                                           decoder                 Weighted                                                                                            decoder\\r\\n                                                                                                                                                                                            slot\\r\\n                                                                                                                                                                                       decoder             Average\\r\\n           CNN                                              decoder                                                                    CNN                                              decoder\\r\\n                                                                                                                                                                                              slot\\r\\n                                                                                   Average                                                                                               decoder\\r\\n                                                                                                                                                                                           decoder\\r\\n\\r\\n                                        sampling 1 slots                                                                                                        sampling 1 slots\\r\\n                                                     Background                                                                                                                 Background\\r\\n\\r\\n                                      reconstruction loss                                                                                                             reconstruction loss\\r\\n\\r\\n\\r\\nFigure 2: Stages of training in GFS-Nets. Left: Autoencoding primitive entities. Primitive entities are\\r\\npoint clouds of individual parts in 3D, and single object RGB images in 2D. GFS-Nets use one and two slots\\r\\nrespectively, for primitive entity autoencoding in 3D point clouds and RGB images. Right: Autoencoding scenes.\\r\\nScenes can be 3D points clouds of whole object or multi-object RGB images. In this stage, GFS-Nets use more\\r\\nslots in their bottleneck. During inference, some slots get mapped to nothing in the input scene. In this way,\\r\\nGFS-Nets is able to infer decompositions with a varying number of entities, in an unsupervised manner.\\r\\n\\r\\n\\r\\n\\r\\non the right of Figure 2 and described in Section 3.3. We first describe slot attention of [42] in Section\\r\\n3.1, as well as encoders and decoders for 3D point clouds [70, 40] and RGB images [47, 67] which\\r\\nour method builds upon.\\r\\n\\r\\n3.1     Background\\r\\n\\r\\n3.1.1     Routing from input to slots with iterative attention\\r\\n\\r\\nMany approaches instantiate slots (a.k.a. query vectors) from 2D visual feature maps or 3D point\\r\\nfeature clouds [5, 48]. Most works use standard cross attention operations [61, 22, 42] or iterative\\r\\ncross (features to slots) and self-attention (slot-to-slots) operations [5] to map a set of N input feature\\r\\nvectors to a set of K slot vectors. Competition amongst slots and iterative routing introduced in\\r\\n[22, 42] encourages the slots not to encode information in a redundant manner.\\r\\nGiven a visual scene encoded as a set of feature vectors M \\xe2\\x88\\x88 RN \\xc3\\x97C and K randomly initialized slots\\r\\nsampled from a multivariate Gaussian distribution with a diagonal covariance s \\xe2\\x88\\xbc N (\\xc2\\xb5, Diag(\\xcf\\x83 2 )) \\xe2\\x88\\x88\\r\\nRK\\xc3\\x97D , where \\xc2\\xb5, \\xcf\\x83 \\xe2\\x88\\x88 RC are learnable parameters of the Gaussian, Slot Attention[42] computes an\\r\\nattention map a between the feature map M and the slots s :\\r\\n\\r\\n                                                         a = \\\\mathrm {Softmax}(\\\\key (\\\\MM ) \\\\cdot \\\\query (s)^T, \\\\textrm {axis=1}) \\\\in \\\\mathbb {R}^{N \\\\times K}. \\\\label {eq:1}                                            (1)\\r\\n\\r\\nk, q, and v are learnable linear transformations that map inputs and slots to a common dimension\\r\\nD. The softmax normalization over slots ensures competition amongst them to attend to a specific\\r\\nfeature vector in M. We then generate the update vector for each slot given the computed attention\\r\\nand input feature maps:\\r\\n\\r\\n                                                    updates = a^T v(M) \\\\in \\\\mathbb {R}^{K \\\\times C}, \\\\textrm {where} \\\\ a_{i,1} = \\\\dfrac {a_{i,1}}{\\\\sum _{i=0}^{N}a_{i,1}}                                               (2)\\r\\n\\r\\n\\r\\nwhich we then use to do a gated update on each slot feature using a GRU[10]: slots = GRU(state =\\r\\nslots, input = updates). We iterate over the steps of calculating attention and updating the slots for\\r\\n3 timesteps.\\r\\nSince the slot vectors are sampled from a shared Gaussian distribution, slot attention[42] can in-\\r\\nstantiate a variable number of slots in different scenes, in contrast to DETR encoder [5] where a\\r\\nconstant number of (deterministic) slot vectors are learned and used in each scene. For a more\\r\\ndetailed description, we refer the reader to [42].\\r\\nSlot attention is one of many ways of mapping inputs to slots. GFS-Nets is agnostic to the method of\\r\\nmapping visual features to slot vectors and we ablate different encoders in our experiment section\\r\\n4.1.2.\\r\\n\\r\\n\\r\\n                                                                                                                4\\r\\n\\x0c\\n\\n3.1.2    Encoding and decoding 3D point clouds\\r\\n\\r\\nGFS-Nets featurize a 3D point cloud using a 3D point transformer [70] which maps the 3D input\\r\\npoints to a set of M feature vectors of C dimensions each. We set M to 128 and C to 64 in our\\r\\nexperiments.\\r\\nGFS-Nets decodes 3D point clouds from each slot using implicit functions [46]. Specifically, each\\r\\ndecoder takes in as input the slot vector si and an (X, Y, Z) location and returns the corresponding\\r\\noccupancy score ox,y,z = Dec(si , (x, y, z)) , where Dec is a multi-block ResNet MLP similar to that\\r\\nof Lal et al. [40].\\r\\n\\r\\n3.1.3    Encoding and decoding RGB images\\r\\n\\r\\nGFS-Nets featurize an RGB image concatenated with its pixel coordinates using a convolutional\\r\\nneural network (CNN) and produce a feature map of H \\xc3\\x97 W \\xc3\\x97 C where H, W are the spatial\\r\\ndimensions and C is the feature dimension. We reshape the output feature map to a set of vectors\\r\\nM \\xe2\\x88\\x88 R(H\\xc3\\x97W )\\xc3\\x97C . We set C as 64.\\r\\nGFS-Nets decodes RGB pixels from each slot using conditional NeRFs [47] that predict the color\\r\\nc and volume density \\xcf\\x83 at each 3D location of the latent 3D volume. Thus given slot vector\\r\\nsi , spatial location x and viewing direction d, the MLP based decoder network Dec predicts\\r\\n(c, \\xcf\\x83) = Dec(si , (x, d)). The color c and density \\xcf\\x83 of the points are composed together across\\r\\n                                                               PK             PK\\r\\nslots using density \\xcf\\x83 weighted averaging. Specifically, \\xcf\\x83\\xcc\\x82 = i=0 wi \\xcf\\x83i , c\\xcc\\x82 = i=0 wi ci where\\r\\n            PK\\r\\nwi = \\xcf\\x83i / i=0 \\xcf\\x83i . We then use differentiable volume rendering of the composed \\xcf\\x83\\xcc\\x82 and c\\xcc\\x82 using\\r\\ncamera ray casting to generate the final RGB prediction similar to [67].\\r\\n\\r\\n\\r\\n3.2     Autoencoding primitive entities\\r\\n\\r\\nIn this stage, we train GFS-Nets using a set of primitive entities. Primitive entities are generic or\\r\\nsemantic parts of objects in 3D point clouds or single object multi-view images, as shown on the left\\r\\nof Figure 2. We experiment with different primitive sets in the experimental section.\\r\\nFor encoding 3D point cloud primitive entities, we sample a single slot in the entity bottleneck of\\r\\nGFS-Nets, that is, s \\xe2\\x88\\x88 R1\\xc3\\x97D . For encoding a 2D RGB image, we sample two slots, one from a\\r\\nforeground learnable Gaussian distribution and one from a background learnable Gaussian distribution,\\r\\nthat is s \\xe2\\x88\\x88 R2\\xc3\\x97D . In a forward pass of the encoder, we then update slots s as described in Section 3.1.\\r\\n\\r\\n\\r\\n3.3     Autoencoding scenes\\r\\n\\r\\nIn this stage, we initialize the parameters of the model\\r\\nwith the parameters learnt in the previous section. The\\r\\nonly difference is that instead of instantiating one or two\\r\\nslots we now instantiate K slot vectors, where\\r\\nK is the maximum number of entities any example in the\\r\\ndataset could possibly have. For RGB input, we sample K\\r\\nslots from the foreground Gaussian and one slot from the\\r\\nbackground Gaussian since there can be many foreground\\r\\n                                                                Input & Target   Slow Inference\\r\\nobjects but only a single background.\\r\\nFinally we pass each of our slot vectors through the slot de-   Figure 3: Slow Inference in GFS-Nets:\\r\\ncoder outk = Dec(sk ), whose weights are shared across          First columns shows the input point cloud.\\r\\nslots. For decoding RGB images, GFS-Nets uses separate          In 2nd to 4th columns, we show the segmen-\\r\\ndecoders for foreground and background. For 3D point            tation results throughout training iterations\\r\\ncloud input, outk represents the probability of sampled         of the autoencoding process. Segmentation\\r\\n                                                                improves over time with reconstruction qual-\\r\\npoints being occupied by a given slot sk ; we aggregate         ity, despite that the autoencoding loss only\\r\\nthem by max pooling across the slot dimension. We then          penalizes the reconstruction error.\\r\\nuse binary cross entropy loss between the ground truth\\r\\noccupancies and the predictions.\\r\\n\\r\\n\\r\\n                                                    5\\r\\n\\x0c\\n\\nFor RGB input, outk represents the color and density of\\r\\npoints being occupied by a given slot sk ; we aggregate across slots as described in Section 3.1 and a\\r\\nuse pixel mean squared error loss.\\r\\nFast and Slow Inference Fast inference refers to a single forward pass through the trained net-\\r\\nwork. Slow inference refers to test-time adaptation of all the learnable parameters of the model for\\r\\nautoencoding a single scene using the method described in Section 3.3. Our experiment show that\\r\\nGFS-Nets with slow inference always achieves much better scene decomposition than GFS-Nets with\\r\\nfast inference, especially in out-of-distribution scenes. In contrast, slow inference on entity-centric\\r\\ngenerative models without curricula, such as [42], results in improved reconstruction accuracy but\\r\\nworse scene decomposition performance, due to the reconstruction-segmentation trade off noted in\\r\\n[15].\\r\\n\\r\\n4     Experiments\\r\\nWe test GFS-Nets in the tasks of segmenting object parts in 3D point clouds and segmenting objects\\r\\nand predicting views in 2D multiview images under varying amounts of label supervision. We compare\\r\\nGFS-Nets against previous state-of-the-art models in each task. We further quantify contribution of\\r\\nvarious design choices, namely, slow inference through reconstruction feedback, curriculum learning,\\r\\nand encoder architecture. We evaluate segmentation performance on scenes within the training\\r\\ndistribution as well as out-of-distribution scenes.\\r\\nIn each setup, our experiments aim to answer the following questions:\\r\\n\\r\\n         \\xe2\\x80\\xa2 How does GFS-Nets compare against state-of-the-art 2D and 3D scene decomposition\\r\\n           models ([43, 67, 5]) under varying amounts of label supervision?\\r\\n         \\xe2\\x80\\xa2 How much, if any, slow inference through reconstruction feedback improves segmentation\\r\\n           accuracy in GFS-Nets and its variants?\\r\\n         \\xe2\\x80\\xa2 How much curriculum or supervision during training contributes to segmentation perfor-\\r\\n           mance?\\r\\n\\r\\n4.1     Segmenting parts in 3D object point clouds\\r\\n\\r\\nIn this task, the input is a complete 3D point cloud of an object, and the goal is to segment the 3D\\r\\npoint cloud into different semantic part instances. We consider two setups with different segmentation\\r\\nsupervision:\\r\\n\\r\\n        1. Segmenting without any part segmentation labels.\\r\\n        2. Segmenting with part segmentation labels from a related object category.\\r\\n\\r\\n4.1.1    Segmenting 3D pointclouds without segmentation supervision\\r\\nIn this setup, we test our model and baselines in segmenting the test objects of the Chair and Table\\r\\ncategories in the PartNet benchmark [49] without access to any ground-truth 3D segmentations at\\r\\ntraining time. For training, we provide our model and baselines access to unsegmented objects of the\\r\\nChair category in the trainset of the PartNet benchmark. We evaluate the segmentation performance\\r\\nof our model and baselines with the Adjusted Random Index (ARI) [28] using the implementation of\\r\\nKabra et al. [32], which calculates the similarity between two-point clusters while being invariant to\\r\\nthe ordering of the cluster centers. Neither our model nor the baselines have access to ground-truth\\r\\n3D segmentations during training; as a result, they may output 3D parts of coarser or finer resolution.\\r\\nPartNet contains three different levels of ground-truth segmentation labels with progressively finer\\r\\nsegmentation granularity. We evaluate the ARI score across all three levels of PartNet and report the\\r\\nbest of three for all the models.\\r\\nOur model and two amongst three of our baselines, specifically (PQ-Nets[64] of Wu et al. and\\r\\nShape2Prog [60] of Tian et al.) expect access to a set of 3D primitive parts during training. In this\\r\\nsetup, we consider the primitive part dataset introduced by Shape2Prog [60]. The dataset consists of\\r\\ndifferently sized cubes, cuboids, and discs; we call it the generic primitive set since it is not related\\r\\nto any semantic object category but rather consists of generic 3D parts that remind of generalized\\r\\n\\r\\n\\r\\n                                                   6\\r\\n\\x0c\\n\\ncylinders of Marr et al.[45]. Please refer to the supplementary file for visualizations of the generic\\r\\nprimitive set.\\r\\nBaselines: We compare our model against the following baselines: (i) PQ-Nets of Wu et al.[64]\\r\\nassume access to a set of primitive 3D parts, similar to our model, and learn a primitive part\\r\\nautoencoder. Whole object autoencoding uses a sequential model that encodes the 3D point cloud\\r\\ninto a 1D latent vector and sequentially decodes parts using the part decoder. We use the publicly\\r\\navailable code to train the model to autoencode the generic primitive set and the unlabelled 3D point\\r\\nclouds of the instances of the Chair category in PartNet. (ii) Shape2Prog of Tian et al.[60] is a shape\\r\\nprogram synthesis method that is trained supervised to predict shape programs from object 3D point\\r\\nclouds. The program represents the part category, location, and the symmetry relations among the\\r\\nparts (if any). Shape2Prog introduced two synthetically generated datasets that helped the model\\r\\nparse 3D pointclouds from ShapeNet [6] into shape programs without any supervision: i) the generic\\r\\nprimitive set we discussed earlier which they use to train their part decoders, and ii) a synthetic whole\\r\\nshape dataset of chairs and tables generated programmatically alongside its respective ground-truth\\r\\nprograms. Their model requires supervised pre-training on the dataset of synthetic whole shapes\\r\\npaired with programs. We therefore use their publicly available model weights trained on synthetic\\r\\nwhole shapes to further train on PartNet Chairs. Note that no other baseline nor GFS-Nets assumes\\r\\naccess to the synthetic whole shape dataset. (iii) StructImplicit of Genova et al.[21] encodes the\\r\\nobject point cloud into a 1D latent and then uses an MLP to map it to a fixed number of part vectors,\\r\\neach represented with a 3D implicit function. We use the publicly available code to train the model\\r\\non the unlabelled Chair PartNet dataset. We set the number of part vectors based on the maximum\\r\\nnumber of parts any example in the dataset can have.\\r\\n\\r\\n                                                   in-dist (Chairs)                  out-of-dist (Tables)\\r\\n        Method\\r\\n                                             Fast Infer.         Slow Infer.      Fast Infer.       Slow Infer.\\r\\n        Shape2Prog [60]                         0.28                0.53             0.23                 0.40\\r\\n        PQ-Nets [64]                            0.20                0.31             0.17                 0.21\\r\\n        StructImplicit [21]                     0.25                0.23             0.17                 0.17\\r\\n        GFS-Nets w/o curriculum                 0.41                0.35             0.47                 0.38\\r\\n        GFS-Nets-PrimitiveOnly                  0.42                0.48             0.49                 0.57\\r\\n        GFS-Nets                                0.57                0.62             0.60                 0.69\\r\\n\\r\\nTable 1: ARI Segmentation accuracy (higher the better) in the test set of Chair (in-distribution) and Table\\r\\ncategory of PartNet(out-of-distribution). GFS-Nets significantly outperform all of the baselines.\\r\\n\\r\\nWe consider the following ablative versions of GFS-Nets: (i) GFS-Nets w/o curriculum is trained\\r\\nwithout the primitive learning stage. Instead, it is trained to directly autoencode the aggregated dataset\\r\\nof both the generic primitive set and 3D point clouds of training chairs (as described in Section 3.3).\\r\\nThis version of our model coincides precisely with the previous work of Slot Attention of Locatello\\r\\net al. [42]. (ii) GFS-Nets-PrimitiveOnly is trained only on the generic primitive set (as described in\\r\\nSection 3.2) and is not trained to autoencode and 3D point clouds of training chairs.\\r\\n                                                                                      Input     3D-DETR   Learning to Group   Ours\\r\\n\\r\\nAny model that has a decoder that reconstructs the input\\r\\ncan be additionally trained with slow inference with re-\\r\\nconstruction feedback at each test scene independently.\\r\\nGFS-Nets, PQ-Nets [64], Shape2Prog [60], and StructIm-\\r\\nplicit [21] are all equipped with such decoders. We thus   Input  Shape2Prog [62] PQ-Nets [66] Structured Implicit [22] Ours\\r\\n\\r\\ntest both our model and the baselines in both modes of\\r\\nfast (regular) and slow inference; we use the notation Figure 4: Unsupervised 3D segmentations of\\r\\n[modelname]-Slow and [modelname]-Fast to denote the out-of-distribution scenes for GFS-Nets and\\r\\n                                                         baselines.\\r\\nmodel with the corresponding inference type.\\r\\nWe show quantitative and qualitative results of our model\\r\\nand the baselines in Table 4 and Figure 4 respectively. From the Table 4 we draw the following\\r\\nconclusions:\\r\\nSegregation into slot like entities using attention helps. GFS-Nets significantly outperform PQ-\\r\\nNets [64] and StructImplicit [21] that map the input object 3D pointcloud into a 1D latent vec-\\r\\n\\r\\n\\r\\n                                                             7\\r\\n\\x0c\\n\\ntor instead of maintaining segregated representations into multiple slot latent vectors. Moreover,\\r\\nGFS-Nets\\xe2\\x80\\x99s ability to dynamically route information per example via attention allows it to automati-\\r\\ncally estimate the number of parts required per input 3D pointcloud due to which, different instances\\r\\ncan have a different number of active slots, while StructImplicit segregates all object instances into a\\r\\nconstant number of parts, which is a hyperparameter of the model.\\r\\nCurriculum training helps. GFS-Nets-Fast outperform by a large margin GFS-Nets w/o curriculum-\\r\\nFast, the version of GFS-Nets that uses exactly the same training data and inference method, but is\\r\\ntrained without curriculum.\\r\\nSlow inference through reconstruction feedback helps in the presence of curriculum and hurts\\r\\nin the absence of it. GFS-Nets, GFS-Nets-PrimitiveOnly, PQ-Nets and Shape2Prog all have cur-\\r\\nriculum through a primitive learning stage, while GFS-Nets w/o curriculum and StructImplicit\\r\\ndo not. GFS-Nets-Slow outperform GFS-Nets-Fast and GFS-Nets-PrimitiveOnly-Slow outperform\\r\\nGFS-Nets-PrimitiveOnly-Fast, PQ-Nets-Slow outperform PQ-Nets-Fast and ShapeProg-Slow outper-\\r\\nform ShapeProg-Fast. In contrast, GFS-Nets w/o curriculum-Slow performs worse than GFS-Nets\\r\\nw/o curriculum-Fast and StructImplicit-Slow does worse than StructImplicit-Fast. Such trade-off\\r\\nbetween reconstruction and segmentation in generative model for scene decomposition has been\\r\\npointed out by Engelcke et al. [15] and our results verify their findings.\\r\\nUnsupervised autoencoding of complex (non primitive) 3D pointclouds helps. GFS-Nets outper-\\r\\nforms GFS-Nets-PrimitiveOnly.\\r\\nPlease see the supplementary file for more experimental details. We visualize intermediate iterations\\r\\nduring slow inference in our supplementary video.\\r\\n\\r\\n4.1.2    Segmenting 3D pointclouds with weak segmentation supervision\\r\\nIn this setup, we test our model and baselines in segmenting the test objects of the Chair and Table\\r\\ncategories in the PartNet benchmark [49], with access to ground-truth pointcloud segmentation\\r\\nlabels of level-3 in the training 3D pointclouds of the Chair category in PartNet. We use the ARI\\r\\nsegmentation metric to evaluate the level 3 ground-truth labels of PartNet for our model and baselines.\\r\\nGFS-Nets assume access to a set of 3D primitive parts for primitive entity learning. In this setup, we\\r\\nconsider each 3D part from each training chair 3D pointcloud example as a separate 3D primitive. We\\r\\naugment these primitives by random rotations and translations; we call it the semantic primitive set.\\r\\n\\r\\n                                                            in-dist (Chair)                   out-dist (Table)\\r\\n        Method\\r\\n                                                 Fast Infer.            Slow Infer.       Fast Infer.   Slow Infer.\\r\\n        3D-DETR [5]                                  0.67                   n/a              0.41           n/a\\r\\n        Learning2Group [43]                          0.62                   n/a              0.46           n/a\\r\\n        GFS-Nets w/o supervision                     0.40                  0.44              0.31          0.48\\r\\n        GFS-Nets w/o curriculum                      0.61                  0.66              0.48          0.60\\r\\n        GFS-Nets w/o Gaussian                        0.65                  0.62              0.38          0.58\\r\\n        GFS-Nets w/o SlotAttention                   0.58                  0.55              0.31          0.44\\r\\n        GFS-Nets                                     0.64                  0.67              0.51          0.63\\r\\n\\r\\nTable 2: ARI Segmentation scores (higher the better) on the test set of the Chair category (in-distribution)\\r\\nand on the test set of the Table category (out-of-distribution). All the models are trained using the training set of\\r\\nthe Chair category in PartNet.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n               Input    3D-DETR [5]   Learning2Group [45]    Ours       Cupboard   Lamp      Display    Laptop\\r\\n\\r\\n\\r\\nFigure 5: Out-of-distribution 3D segmentation results for GFS-Nets and baselines. Left: 3D segmentation\\r\\nresults on out-of-distribution shapes for GFS-Nets and baselines. Right: 3D segmentation results for out-of-\\r\\ndistribution categories of PartNet for GFS-Nets trained semi-supervised only on the Chair category.\\r\\n\\r\\n\\r\\n\\r\\n                                                                    8\\r\\n\\x0c\\n\\nBaselines We compare our model against the following baselines: (i) Learning2Group of Luo\\r\\net al. [43] progressively groups points into segments by learning pairwise grouping decisions pa-\\r\\nrameterized by features of the two point clusters. Given the intermediate grouping decisions are not\\r\\nsupervised, and the non-differentiability of their grouping functions they use reinforcement learning\\r\\ngradients for training using supervision from the final segmentation. We used the publicly available\\r\\ncode and trained the model in the training set of the Chair category. (ii) 3D-DETR is the equivalent of\\r\\n2D-DETR of Carion et al. [5] for 3D point-cloud segmentation. A point transformer [70] encodes the\\r\\ninput 3D point cloud into a set of point features which are mapped to a learnable set of 16 parametric\\r\\nqueries using 6 layers of transformer encoder (self-attention) and decoder (cross-attention) blocks.\\r\\nThen, each query decodes a 3D point segmentation map by computing multi-head attention with the\\r\\nencoded point features. The attention for each query is then upsampled using a point transformer\\r\\ndecoder. The predicted 3D part segments are then matched against ground-truth segments using\\r\\nHungarian matching [36], and error is computed for each pair of matched ground-truth part and\\r\\npaired prediction through binary cross-entropy loss.\\r\\nWe train GFS-Nets in a semi-supervised way combining an (unsupervised) autoencoding loss and a\\r\\nsupervised part segmentation loss. Specifically alongside the autoencoding loss of the training 3D\\r\\npoint clouds, we apply a part segmentation loss via Hungarian matching between the decoded 3D\\r\\npart shapes from the slots and the ground-truth part 3D segments in the training set, similar to our\\r\\n3D-DETR baseline model. For this case we did not consider the models of PQ-Nets [64], Shape2Prog\\r\\n[60] or [21] as it is not clear on how to train them in a semi-supervised manner using segmentation\\r\\nsupervision.\\r\\nWe consider the following ablative versions of GFS-Nets: (i) GFS-Nets w/o supervision is trained\\r\\nwithout the supervised part segmentation loss, so the model only receives gradients from autoencoding\\r\\nthe primitives and whole chairs. (ii) GFS-Nets w/o Gaussian does not initialize the slot vectors by\\r\\nsampling from a learnable Gaussian but instead uses separate learnable query vectors for each slot,\\r\\nsimilar to DETR. (iii) GFS-Nets w/o SlotAttention does not use Slot Attention for mapping point\\r\\nfeatures to slots. Instead it maps 3D point features to slots via iterative layers of cross (query to point)\\r\\nand self (query-to-query) attention layers on learnable query vectors similar to 3D-DETR and DETR\\r\\n[5]. Please note that slots and queries represent the same thing, but we use the terminology of DETR\\r\\n[5] in this case.\\r\\nWe report the fast and slow inference results for all ablative versions of our model. Our baselines in\\r\\nthis case, 3D-DETR and Learning2Group [43] are feedforward in nature, they are not equipped with\\r\\ndecoders, and thus cannot be evaluated with slow inference.\\r\\nWe show quantitative results in Table 2 and qualitative results in Figure 5. GFS-Nets significantly\\r\\noutperform the baselines, and GFS-Nets-Slow results in a significant boost in performance (\\xe2\\x88\\xbc 50%)\\r\\nover the feedforward inference in our model, GFS-Nets-Fast. We draw the following conclusions\\r\\nfrom Table 2:\\r\\nSupervision can be used as a replacement to curriculum training. GFS-Nets only slightly out-\\r\\nperforms GFS-Nets w/o curriculum in this setup.\\r\\nGeneric primitives generalize a lot better than category-specific primitives or supervision.\\r\\nGFS-Nets in Table 4 (that uses the generic primitive set) outperforms GFS-Nets and GFS-Nets-\\r\\nw/o supervision in Table 2 (that use the semantic primitive set) in OOD generalization on the Table\\r\\ncategory.\\r\\nCompetition amongst slots helps. GFS-Nets outperforms GFS-Nets w/o SlotAttention, thus show-\\r\\ning competition amongst slot vectors during encoding helps generalization.\\r\\nSlow inference through reconstruction feedback helps OOD generalization of GFS-Nets. Our\\r\\nbaselines 3D-DETR and Learning2Group [43] are feed-forward in nature, they lack any form of\\r\\nreconstruction feedback, and thus cannot adapt as our model through such feedback.\\r\\n\\r\\n4.2   RGB image segmentation and view prediction without supervision\\r\\n\\r\\nIn this task, the input to the model is a single RGB image that contains multiple objects and the goal\\r\\nis to segment the image into objects and predict images from alternative scene viewpoints. We test\\r\\nour model in the following two benchmarks: ((i) CLEVR [31]: this is a commonly used benchmark\\r\\nfor unsupervised image segmentation. The dataset contains scenes that consist of 5 to 7 spheres,\\r\\ncylinders and cubes, in various colors and materials. We use the multi-view version of this dataset\\r\\nintroduced by uORF [67] as the training set.\\r\\n\\r\\n\\r\\n                                                     9\\r\\n\\x0c\\n\\n    Input   Slot Attention [44]   uORF [69]   GFS-Nets-Fast   GFS-Nets-Slow      Input          Predicted Novel Views by GFS-Nets\\r\\n\\r\\n\\r\\nFigure 6: RGB image decomposition and view prediction for GFS-Nets and baselines. Left: From 2nd to 5th\\r\\ncolumn we show reconstructed RGB image superimposed with the predicted segmentation masks for our model\\r\\nand baselines. Right: From 2nd to 5th column we show RGB renderings across multiple novel viewpoints for\\r\\nGFS-Nets.\\r\\n\\r\\n\\r\\n(ii) Room Diverse++: Room Diverse++ builds upon Room Diverse, which is a multi-view RGB\\r\\ndataset introduced by Yu et al.[67] where they placed multiple ShapeNet Chairs with different solid\\r\\ncolors (Red, Blue etc) onto a textured background. We make it more challenging by applying real-\\r\\nworld ShapeNet textures to the object meshes instead of solid colors. We also add more categories\\r\\nsuch as Beds, Benches, Tables, Cupboards, and Sofas to the Chair category. We sample about 6 to 10\\r\\nobjects in each scene.\\r\\n\\r\\nBaselines We consider the following baselines: (i) Slot attention Locatello et al. [42] autoencodes\\r\\nsingle view RGB images, and is described in Section 3.1. We use the publicly available code and\\r\\ntrain the model on the training set for single image autoencoding.\\r\\n(ii) uORF Yu et al. [67] is a recent work that\\r\\nextends Slot Attention of [42] to multiview RGB                                Method                   CLEVR               Room Diverse++\\r\\nimages.                                                                        uORF [67]                 0.86                    0.65\\r\\nDuring primitive training for our model, we as-                                Slot Attention [42]       0.04                    0.28\\r\\nsume access to posed RGB images of single                                      GFS-Nets-Fast             0.86                    0.74\\r\\nobject scenes as our primitive dataset, as shown                               GFS-Nets-Slow             0.92                    0.81\\r\\nin Figure 2 bottom row left. Our implementa-\\r\\ntion builds upon [67] with the difference that we                             Table 3: ARI Segmentation accuracy (higher the bet-\\r\\nconsider curriculum and slow inference at test                                ter) on CLEVR and RoomDiverse++ Dataset for RGB\\r\\n                                                                              input. As can be seen GFS-Nets-Slow outperform all\\r\\ntime. We ablate slow and fast versions of our                                 other baselines in segmentation accuracy.\\r\\nmodel.\\r\\nWe show quantitative results of GFS-Nets and\\r\\nthe baselines in Table 3 and qualitative view prediction and segmentation results in Figure 6.\\r\\nGFS-Nets-slow outperforms the baselines significantly. This suggests that curriculum and slow\\r\\ninference are key to out-of-distribution generalization. Slot Attention [42] achieves very low accuracy\\r\\non CLEVR, as it is unable to disentangle background from foreground, as also reported in Yu et\\r\\nal.[67]. We further visualize GFS-Nets\\xe2\\x80\\x99s ability to render novel viewpoints in the supplementary\\r\\nvideo.\\r\\n\\r\\nLimitations and Future Work: GFS-Nets has limitations that offer several promising directions\\r\\nfor future work. First, GFS-Nets does not model pairwise interactions between slots. Incorporating\\r\\nsuch interactions is a direct avenue for future work. Second, entity decomposition is inherently\\r\\nhierarchical: objects - parts - subparts while GFS-Nets currently models a flat non-hierarchical list of\\r\\nentities. Third, amortizing the slow inference in feedforward predictions could be another interesting\\r\\nextension of the current framework. Finally, although GFS-Nets is tested on far more realistic data\\r\\nthan considered in prior works, scaling the model to more realistic datasets used by mainstream\\r\\nsupervised visual detectors, is a direct avenue for future work.\\r\\n\\r\\n5     Conclusion\\r\\nWe presented GFS-Nets, a model that segments scenes into entities while autoencoding the input\\r\\nscene through slot-based encoders and shared slot decoders. We showed that curriculum training\\r\\nin the form of primitive entities, single object scenes, or labelled segmentations is necessary to\\r\\nbreak the trade-off between segmentation and reconstruction observed in previous works [15]. This\\r\\npermits GFS-Nets to use slow inference through reconstruction feedback and drastically improve the\\r\\ndecompositions of out-of-distribution scenes. We believe that a compositional approach to AI, in\\r\\nterms of grounded symbol-like representations [44, 38] is of fundamental importance for realizing\\r\\nhuman-level generalization [1], and we hope that GFS-Nets may contribute towards that goal.\\r\\n\\r\\n\\r\\n                                                                          10\\r\\n\\x0c\\n\\nReferences\\r\\n [1] Bahdanau, D., Murty, S., Noukhovitch, M., Nguyen, T.H., de Vries, H., Courville, A.: System-\\r\\n     atic generalization: what is required and can it be learned? arXiv preprint arXiv:1811.12889\\r\\n     (2018)\\r\\n [2] Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., Katz, B.:\\r\\n     Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\\r\\n     models. Advances in neural information processing systems 32 (2019)\\r\\n [3] van Bergen, R.S., Kriegeskorte, N.: Going in circles is the way forward: the role of re-\\r\\n     currence in visual inference. Current Opinion in Neurobiology 65, 176\\xe2\\x80\\x93193 (Dec 2020).\\r\\n     https://doi.org/10.1016/j.conb.2020.11.009, http://dx.doi.org/10.1016/j.conb.2020.\\r\\n     11.009\\r\\n [4] Burgess, C.P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., Lerchner, A.:\\r\\n     Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390\\r\\n     (2019)\\r\\n [5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end\\r\\n     object detection with transformers. In: European Conference on Computer Vision. pp. 213\\xe2\\x80\\x93229.\\r\\n     Springer (2020)\\r\\n [6] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\\r\\n     M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet: An Information-Rich 3D Model\\r\\n     Repository. Tech. Rep. arXiv:1512.03012 [cs.GR], Stanford University \\xe2\\x80\\x94 Princeton University\\r\\n     \\xe2\\x80\\x94 Toyota Technological Institute at Chicago (2015)\\r\\n [7] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva,\\r\\n     M., Song, S., Su, H., et al.: Shapenet: An information-rich 3d model repository. arXiv preprint\\r\\n     arXiv:1512.03012 (2015)\\r\\n [8] Chen, Z., Tagliasacchi, A., Zhang, H.: Bsp-net: Generating compact meshes via binary space\\r\\n     partitioning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition. pp. 45\\xe2\\x80\\x9354 (2020)\\r\\n [9] Chen, Z., Yin, K., Fisher, M., Chaudhuri, S., Zhang, H.: Bae-net: branched autoencoder\\r\\n     for shape co-segmentation. In: Proceedings of the IEEE/CVF International Conference on\\r\\n     Computer Vision. pp. 8490\\xe2\\x80\\x938499 (2019)\\r\\n[10] Cho, K., Van Merri\\xc3\\xabnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio,\\r\\n     Y.: Learning phrase representations using rnn encoder-decoder for statistical machine translation.\\r\\n     arXiv preprint arXiv:1406.1078 (2014)\\r\\n[11] Deng, B., Genova, K., Yazdani, S., Bouaziz, S., Hinton, G., Tagliasacchi, A.: Cvxnet: Learnable\\r\\n     convex decomposition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\\r\\n     Pattern Recognition. pp. 31\\xe2\\x80\\x9344 (2020)\\r\\n[12] Deprelle, T., Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: Learning elementary\\r\\n     structures for 3d shape generation and matching. arXiv preprint arXiv:1908.04725 (2019)\\r\\n[13] Du, Y., Smith, K., Ulman, T., Tenenbaum, J., Wu, J.: Unsupervised discovery of 3d physical\\r\\n     objects from video. arXiv preprint arXiv:2007.12348 (2020)\\r\\n[14] Ellis, K., Wong, C., Nye, M., Sable-Meyer, M., Cary, L., Morales, L., Hewitt, L., Solar-Lezama,\\r\\n     A., Tenenbaum, J.B.: Dreamcoder: Growing generalizable, interpretable knowledge with\\r\\n     wake-sleep bayesian program learning. arXiv preprint arXiv:2006.08381 (2020)\\r\\n[15] Engelcke, M., Jones, O.P., Posner, I.: Reconstruction bottlenecks in object-centric generative\\r\\n     models. arXiv preprint arXiv:2007.06245 (2020)\\r\\n[16] Engelcke, M., Kosiorek, A.R., Jones, O.P., Posner, I.: Genesis: Generative scene inference and\\r\\n     sampling with object-centric latent representations. arXiv preprint arXiv:1907.13052 (2019)\\r\\n\\r\\n\\r\\n                                                 11\\r\\n\\x0c\\n\\n[17] Eslami, S., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., Hinton,\\r\\n     G.E.: Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint\\r\\n     arXiv:1603.08575 (2016)\\r\\n[18] Gao, L., Yang, J., Wu, T., Yuan, Y.J., Fu, H., Lai, Y.K., Zhang, H.: Sdm-net: Deep generative\\r\\n     network for structured deformable mesh. ACM Transactions on Graphics (TOG) 38(6), 1\\xe2\\x80\\x9315\\r\\n     (2019)\\r\\n[19] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.: Imagenet-\\r\\n     trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness.\\r\\n     arXiv preprint arXiv:1811.12231 (2018)\\r\\n[20] Genova, K., Cole, F., Sud, A., Sarna, A., Funkhouser, T.: Local deep implicit functions for\\r\\n     3d shape. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition. pp. 4857\\xe2\\x80\\x934866 (2020)\\r\\n[21] Genova, K., Cole, F., Vlasic, D., Sarna, A., Freeman, W.T., Funkhouser, T.: Learning shape\\r\\n     templates with structured implicit functions. In: Proceedings of the IEEE/CVF International\\r\\n     Conference on Computer Vision. pp. 7154\\xe2\\x80\\x937164 (2019)\\r\\n[22] Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio, Y., Sch\\xc3\\xb6lkopf, B.: Recurrent\\r\\n     independent mechanisms. arXiv preprint arXiv:1909.10893 (2019)\\r\\n[23] Greff, K., Kaufman, R.L., Kabra, R., Watters, N., Burgess, C., Zoran, D., Matthey, L., Botvinick,\\r\\n     M., Lerchner, A.: Multi-object representation learning with iterative variational inference. In:\\r\\n     International Conference on Machine Learning. pp. 2424\\xe2\\x80\\x932433. PMLR (2019)\\r\\n[24] Greff, K., Rasmus, A., Berglund, M., Hao, T.H., Schmidhuber, J., Valpola, H.: Tagger: Deep\\r\\n     unsupervised perceptual grouping. arXiv preprint arXiv:1606.06724 (2016)\\r\\n[25] Greff, K., van Steenkiste, S., Schmidhuber, J.: On the binding problem in artificial neural\\r\\n     networks. CoRR abs/2012.05208 (2020), https://arxiv.org/abs/2012.05208\\r\\n[26] Guerin, F.: Projection: A mechanism for human-like reasoning in artificial intelligence. CoRR\\r\\n     abs/2103.13512 (2021), https://arxiv.org/abs/2103.13512\\r\\n[27] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro-\\r\\n     ceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770\\xe2\\x80\\x93778\\r\\n     (2016)\\r\\n[28] Hubert, L., Arabie, P.: Comparing partitions. Journal of classification 2(1), 193\\xe2\\x80\\x93218 (1985)\\r\\n[29] Jo, J., Bengio, Y.: Measuring the tendency of cnns to learn surface statistical regularities. arXiv\\r\\n     preprint arXiv:1711.11561 (2017)\\r\\n[30] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.:\\r\\n     Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In:\\r\\n     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2901\\xe2\\x80\\x932910\\r\\n     (2017)\\r\\n[31] Johnson, J., Krishna, R., Stark, M., Li, L.J., Shamma, D., Bernstein, M., Fei-Fei, L.: Image\\r\\n     retrieval using scene graphs. In: Proceedings of the IEEE Conference on Computer Vision and\\r\\n     Pattern Recognition (CVPR) (June 2015)\\r\\n[32] Kabra, R., Burgess, C., Matthey, L., Kaufman, R.L., Greff, K., Reynolds, M., Lerchner, A.:\\r\\n     Multi-object datasets. https://github.com/deepmind/multi-object-datasets/ (2019)\\r\\n[33] Kato, H., Harada, T.: Learning view priors for single-view 3d reconstruction. In: Proceedings of\\r\\n     the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9778\\xe2\\x80\\x939787 (2019)\\r\\n[34] Kipf, T., Elsayed, G.F., Mahendran, A., Stone, A., Sabour, S., Heigold, G., Jonschkowski,\\r\\n     R., Dosovitskiy, A., Greff, K.: Conditional object-centric learning from video. arXiv preprint\\r\\n     arXiv:2111.12594 (2021)\\r\\n\\r\\n\\r\\n                                                  12\\r\\n\\x0c\\n\\n[35] Kosiorek, A., Kim, H., Teh, Y.W., Posner, I.: Sequential attend, infer, repeat: Generative\\r\\n     modelling of moving objects. Advances in Neural Information Processing Systems 31, 8606\\xe2\\x80\\x93\\r\\n     8616 (2018)\\r\\n\\r\\n[36] Kuhn, H.W.: The hungarian method for the assignment problem. Naval research logistics\\r\\n     quarterly 2(1-2), 83\\xe2\\x80\\x9397 (1955)\\r\\n\\r\\n[37] Kulkarni, T.D., Kohli, P., Tenenbaum, J.B., Mansinghka, V.: Picture: A probabilistic program-\\r\\n     ming language for scene perception. In: Proceedings of the ieee conference on computer vision\\r\\n     and pattern recognition. pp. 4390\\xe2\\x80\\x934399 (2015)\\r\\n\\r\\n[38] Lake, B.M., Salakhutdinov, R., Tenenbaum, J.B.: Human-level concept learning through\\r\\n     probabilistic program induction. Science 350(6266), 1332\\xe2\\x80\\x931338 (2015)\\r\\n\\r\\n[39] Lake, B.M., Ullman, T.D., Tenenbaum, J.B., Gershman, S.J.: Building machines that learn and\\r\\n     think like people. Behavioral and brain sciences 40 (2017)\\r\\n\\r\\n[40] Lal, S., Prabhudesai, M., Mediratta, I., Harley, A.W., Fragkiadaki, K.: Coconets: Continuous\\r\\n     contrastive 3d scene representations (2021)\\r\\n\\r\\n[41] Li, Y., Mao, J., Zhang, X., Freeman, W.T., Tenenbaum, J.B., Snavely, N., Wu, J.: Multi-plane\\r\\n     program induction with 3d box priors. arXiv preprint arXiv:2011.10007 (2020)\\r\\n\\r\\n[42] Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J.,\\r\\n     Dosovitskiy, A., Kipf, T.: Object-centric learning with slot attention (2020)\\r\\n\\r\\n[43] Luo, T., Mo, K., Huang, Z., Xu, J., Hu, S., Wang, L., Su, H.: Learning to group: A bottom-up\\r\\n     framework for 3d part discovery in unseen categories. arXiv preprint arXiv:2002.06478 (2020)\\r\\n\\r\\n[44] Marcus, G.F.: The algebraic mind: Integrating connectionism and cognitive science. MIT press\\r\\n     (2003)\\r\\n\\r\\n[45] Marr, D.: Vision: A Computational Investigation into the Human Representation and Processing\\r\\n     of Visual Information. W.H. Freeman and Co., New York, NY (1982)\\r\\n\\r\\n[46] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks:\\r\\n     Learning 3d reconstruction in function space. In: Proceedings of the IEEE/CVF Conference on\\r\\n     Computer Vision and Pattern Recognition. pp. 4460\\xe2\\x80\\x934470 (2019)\\r\\n\\r\\n[47] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf:\\r\\n     Representing scenes as neural radiance fields for view synthesis. In: European conference on\\r\\n     computer vision. pp. 405\\xe2\\x80\\x93421. Springer (2020)\\r\\n\\r\\n[48] Misra, I., Girdhar, R., Joulin, A.: An end-to-end transformer model for 3d object detection. In:\\r\\n     Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2906\\xe2\\x80\\x932917\\r\\n     (2021)\\r\\n\\r\\n[49] Mo, K., Zhu, S., Chang, A.X., Yi, L., Tripathi, S., Guibas, L.J., Su, H.: Partnet: A large-scale\\r\\n     benchmark for fine-grained and hierarchical part-level 3d object understanding. In: Proceedings\\r\\n     of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 909\\xe2\\x80\\x93918 (2019)\\r\\n\\r\\n[50] Niu, C., Li, J., Xu, K.: Im2struct: Recovering 3d shape structure from a single rgb image. In:\\r\\n     Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4521\\xe2\\x80\\x934529\\r\\n     (2018)\\r\\n\\r\\n[51] Paschalidou, D., Gool, L.V., Geiger, A.: Learning unsupervised hierarchical part decomposition\\r\\n     of 3d objects from a single rgb image. In: Proceedings of the IEEE/CVF Conference on\\r\\n     Computer Vision and Pattern Recognition. pp. 1060\\xe2\\x80\\x931070 (2020)\\r\\n\\r\\n[52] Paschalidou, D., Katharopoulos, A., Geiger, A., Fidler, S.: Neural parts: Learning expressive 3d\\r\\n     shape abstractions with invertible neural networks. In: Proceedings of the IEEE/CVF Conference\\r\\n     on Computer Vision and Pattern Recognition. pp. 3204\\xe2\\x80\\x933215 (2021)\\r\\n\\r\\n\\r\\n                                                 13\\r\\n\\x0c\\n\\n[53] Paschalidou, D., Ulusoy, A.O., Geiger, A.: Superquadrics revisited: Learning 3d shape parsing\\r\\n     beyond cuboids. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition. pp. 10344\\xe2\\x80\\x9310353 (2019)\\r\\n[54] Pathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by self-supervised\\r\\n     prediction. In: International conference on machine learning. pp. 2778\\xe2\\x80\\x932787. PMLR (2017)\\r\\n[55] Rahaman, N., Goyal, A., Gondal, M.W., Wuthrich, M., Bauer, S., Sharma, Y., Bengio, Y.,\\r\\n     Sch\\xc3\\xb6lkopf, B.: S2rms: Spatially structured recurrent modules. arXiv preprint arXiv:2007.06533\\r\\n     (2020)\\r\\n[56] Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object detection with\\r\\n     region proposal networks. CoRR abs/1506.01497 (2015), http://arxiv.org/abs/1506.\\r\\n     01497\\r\\n[57] Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. arXiv preprint\\r\\n     arXiv:1710.09829 (2017)\\r\\n[58] Sitzmann, V., Chan, E., Tucker, R., Snavely, N., Wetzstein, G.: Metasdf: Meta-learning signed\\r\\n     distance functions. Advances in Neural Information Processing Systems 33, 10136\\xe2\\x80\\x9310147\\r\\n     (2020)\\r\\n[59] Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., Hardt, M.: Test-time training with self-\\r\\n     supervision for generalization under distribution shifts. In: International Conference on Machine\\r\\n     Learning. pp. 9229\\xe2\\x80\\x939248. PMLR (2020)\\r\\n[60] Tian, Y., Luo, A., Sun, X., Ellis, K., Freeman, W.T., Tenenbaum, J.B., Wu, J.: Learning to infer\\r\\n     and execute 3d shape programs. arXiv preprint arXiv:1901.02875 (2019)\\r\\n[61] Tsai, Y.H.H., Srivastava, N., Goh, H., Salakhutdinov, R.: Capsules with inverted dot-product\\r\\n     attention routing. arXiv preprint arXiv:2002.04764 (2020)\\r\\n[62] Tulsiani, S., Su, H., Guibas, L.J., Efros, A.A., Malik, J.: Learning shape abstractions by\\r\\n     assembling volumetric primitives. In: Proceedings of the IEEE Conference on Computer Vision\\r\\n     and Pattern Recognition. pp. 2635\\xe2\\x80\\x932643 (2017)\\r\\n[63] Van Steenkiste, S., Chang, M., Greff, K., Schmidhuber, J.: Relational neural expectation\\r\\n     maximization: Unsupervised discovery of objects and their interactions. arXiv preprint\\r\\n     arXiv:1802.10353 (2018)\\r\\n[64] Wu, R., Zhuang, Y., Xu, K., Zhang, H., Chen, B.: Pq-net: A generative part seq2seq network\\r\\n     for 3d shapes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     Recognition (CVPR) (June 2020)\\r\\n[65] Xia, W., Zhang, Y., Yang, Y., Xue, J.H., Zhou, B., Yang, M.H.: Gan inversion: A survey. arXiv\\r\\n     preprint arXiv:2101.05278 (2021)\\r\\n[66] Yao, C.H., Hung, W.C., Jampani, V., Yang, M.H.: Discovering 3d parts from image collections.\\r\\n     In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12981\\xe2\\x80\\x93\\r\\n     12990 (2021)\\r\\n[67] Yu, H.X., Guibas, L.J., Wu, J.: Unsupervised discovery of object radiance fields. arXiv preprint\\r\\n     arXiv:2107.07905 (2021)\\r\\n[68] Zablotskaia, P., Dominici, E.A., Sigal, L., Lehrmann, A.M.: Unsupervised video decomposition\\r\\n     using spatio-temporal iterative inference. arXiv preprint arXiv:2006.14727 (2020)\\r\\n[69] Zhang, N., Donahue, J., Girshick, R., Darrell, T.: Part-based r-cnns for fine-grained category\\r\\n     detection. In: European conference on computer vision. pp. 834\\xe2\\x80\\x93849. Springer (2014)\\r\\n[70] Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: Proceedings of the\\r\\n     IEEE/CVF International Conference on Computer Vision. pp. 16259\\xe2\\x80\\x9316268 (2021)\\r\\n[71] Zhou, X., Girdhar, R., Joulin, A., Kr\\xc3\\xa4henb\\xc3\\xbchl, P., Misra, I.: Detecting twenty-thousand classes\\r\\n     using image-level supervision. arXiv preprint arXiv:2201.02605 (2022)\\r\\n\\r\\n\\r\\n                                                  14\\r\\n\\x0c\\n\\n[72] Zhu, J.Y., Kr\\xc3\\xa4henb\\xc3\\xbchl, P., Shechtman, E., Efros, A.A.: Generative visual manipulation on the\\r\\n     natural image manifold. In: European conference on computer vision. pp. 597\\xe2\\x80\\x93613. Springer\\r\\n     (2016)\\r\\n[73] Zoran, D., Kabra, R., Lerchner, A., Rezende, D.J.: Parts: Unsupervised segmentation with\\r\\n     slots, attention and independence maximization. In: Proceedings of the IEEE/CVF International\\r\\n     Conference on Computer Vision. pp. 10439\\xe2\\x80\\x9310447 (2021)\\r\\n[74] Zuffi, S., Kanazawa, A., Berger-Wolf, T., Black, M.J.: Three-d safari: Learning to estimate\\r\\n     zebra pose, shape, and texture from images\" in the wild\". In: Proceedings of the IEEE/CVF\\r\\n     International Conference on Computer Vision. pp. 5359\\xe2\\x80\\x935368 (2019)\\r\\n\\r\\n\\r\\nAppendix\\r\\nThe structure of this appendix is as follows: In Section 6 we cover the details on the datasets. In\\r\\nSection 7 we specify further implementation details. In Section 8 we provide additional qualitative\\r\\nand quantitative results for the experiments in Section 4 of our main paper.\\r\\n\\r\\n6     Datasets\\r\\n6.1   Point Cloud\\r\\n\\r\\nFor all the tasks we subsample the input point clouds to a standard size of 2048 points.\\r\\n\\r\\nGeneric primitive part dataset. We use the primitive dataset of [60] for learning the primitive\\r\\ndistribution in Section 4.1.1. The dataset consists of 200K primitive instances sampled from the\\r\\nprimitive templates that are visualized in Figure 7. Instances are sampled from the templates by\\r\\nchanging their sizes and placing them in random locations, similar to [60].\\r\\n\\r\\nCategory-specific primitives from PartNet dataset. We seperate out individual segments from\\r\\nthe level-3 instance segmentation masks from the official train-split of Chair category of the PartNet\\r\\ndataset. We additionally do rotation and translation based augmentations on individual segments. We\\r\\nvisualize some of these primitive examples in Figure 8. We use this dataset for learning the primitive\\r\\ndistribution in Section 4.1.2. The dataset in total consists of about 100K primitive examples.\\r\\n\\r\\nSynthetic whole shape dataset. This dataset was generated by Shape2Prog[60]. The segmentation\\r\\nlabels from this dataset are used by them as a supervision signal for later generalizing to PartNet\\r\\nshapes. The dataset consists of about 120K synthetically generated Chairs and Tables, we visualize\\r\\nsome of these synthetically generated tables and chairs in Figure 9. Note that no other baseline or\\r\\nGFS-Nets has access to this dataset.\\r\\n\\r\\nPartNet dataset. We use the official level-3 train-test split of PartNet[49]. We use the train split\\r\\nof Chair category as our training set, we consider test split of Table category in PartNet our test\\r\\ncategories. We use this as the train-test split in Section 4.1 of the main paper. We further compare our\\r\\nmodel on the other PartNet categories in supplementary section 8. We set the value of number of\\r\\nslots K as 16 for this dataset.\\r\\n\\r\\n6.2   RGB\\r\\n\\r\\nCLEVR dataset. We use the train-test split of CLEVR dataset opensourced by [67]. Their dataset\\r\\nis built on top of the original CLEVR [30] dataset, where they add additional multi-view rendering.\\r\\nTheir train/test dataset consists of 5,6,7 objects from random geometric primitives(cylinder, cube and\\r\\nsphere) and random colors(red, blue, purple, gray, cyan, yellow, green, brown) placed in random\\r\\nlocations in the scene. The multi-view setting consists of randomly sampling from 360 degree\\r\\nazimuth angles with a fixed elevation angle. Our train data consists of 1000 single object scenes and\\r\\n1000 multi-object scenes. The only difference between their train-test data are the novel locations\\r\\nat which the objects would be placed in the scene. We set the value of number of slots K as 8. We\\r\\nvisualize some of the primitive examples (multi-view single-object scenes) of the CLEVR dataset in\\r\\nFigure 10.\\r\\n\\r\\n\\r\\n                                                  15\\r\\n\\x0c\\n\\nFigure 7: We visualize the generic primitive templates of [60], as you can see they mainly consists of Cubes,\\r\\nCuboids and Discs.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n   Figure 8: We visualize the category-specific primitives extracted from level-3 Chair category of PartNet.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      16\\r\\n\\x0c\\n\\nFigure 9: We visualize the synthetic whole shape dataset of [60]. Shape2Prog supervises their model using the\\r\\nannotations from this dataset.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 10: We visualize some examples of the primitives used in the CLEVR dataset. Primitives are represented\\r\\nas multi-view single object RGB scenes as shown in the figure.\\r\\n\\r\\n\\r\\nRoomDiverse++ dataset. We build RoomDiverse++ on top of the RoomDiverse dataset of [67].\\r\\nIn which instead of using the solid CLEVR colors as textures of meshes we keep the original textures\\r\\nof the ShapeNet meshes, additionaly along with Chairs we also add other ShapeNet categories such\\r\\nas Benches, Cabinets, Beds, Tables and Sofas in the dataset. In total we use 20 object meshes and\\r\\n3 background textures to generate the dataset. We use the same multi-view rendering settings as\\r\\nCLEVR. Our train data consists of 1000 single object scenes and 2000 multi-object scenes. We use\\r\\n6,7,8 objects placed at random locations for generating our multi-object train dataset. We use 500 of\\r\\n7,8,9 object scenes as our test data. We set the value of number of slots K as 10. We visualize some\\r\\nof the primitive examples in the dataset in Figure 11.\\r\\n\\r\\n\\r\\n7     Implementation details\\r\\n\\r\\n7.1   Proposed Model\\r\\n\\r\\nCode, training details and computational complexity. Proposed model is implemented in\\r\\nPython/PyTorch. We use a batch size of 8 for point cloud input and a batch size of 2 on RGB\\r\\ninput. We set our learning rate as 10\\xe2\\x88\\x924 . We use the Adam optimizer with \\xce\\xb21 = 0.9, \\xce\\xb22 = 0.999. Our\\r\\nprimitive learning model takes 24 hours (approximately 200k iterations) to converge. Similarly our\\r\\n\\r\\n\\r\\n                                                     17\\r\\n\\x0c\\n\\nFigure 11: We visualize some examples of the primitives used in the Room Diverse++ dataset. Primitives are\\r\\nrepresented as multi-view single object RGB scenes as shown in the figure.\\r\\n\\r\\n\\r\\nprimitive composition learning model takes about 12hr (approximately 100k iterations) to converge.\\r\\nOur slow inference per example takes about 1 min (500 iterations). A forward pass through the\\r\\nproposed model takes about 0.15 secs. We use a single V100 GPU for training and inference.\\r\\n\\r\\nInputs. For RGB input we randomly select 2 views which include the RGB and ground-truth\\r\\negomotion. We use a single RGB image for testing. For RGB, our input is a 64 \\xc3\\x97 64 \\xc3\\x97 3 tensor, for\\r\\npoint cloud our input is a 2048 \\xc3\\x97 3\\r\\n\\r\\nPoint Cloud Encoder. We adopt the point transformer[70] architecture as our encoder. Point\\r\\ntransformer encoder is essentially layers of self attention blocks. Specifically a self attention block\\r\\nincludes sampling of query points and updating them using their N most neighbouring points as\\r\\nkey/value vectors. In the architecture we specifically apply 5 layers of self attention which look as\\r\\nfollows: 2048-16-64, 2048-16-64, 512-16-64, 512-16-64, 128-16-64, 128-16-64. We use the notation\\r\\nof S-N -C, where S is the number of subsampled query points from the point cloud, N is the number\\r\\nof neighbouring points and C is the feature dimension. We thus get an output feature map of size\\r\\n128 \\xc3\\x97 64.\\r\\n\\r\\nRGB Encoder. Our RGB encoder is similar to that of [67], which is basically a U-net. We\\r\\nconcatenate the RGB values with the pixel coordinates that are normalized in the range of -1 to 1. We\\r\\nthen pass the concatenated 2D image through a convolutional feature extractor. The architecture of\\r\\nthe CNN is as follows: 3-2-64, 3-2-64, 3-2-64, 3-1-64, 3-1-64, 3-1-64. We use the notation of k-s-c,\\r\\nwhere k is the kernel size, s is the number of strides and c is the feature channel. Our final output\\r\\nsize is 64 \\xc3\\x97 64 \\xc3\\x97 64.\\r\\n\\r\\nPoint Cloud Decoder. We obtain point occupancies by querying the slot feature vector slotk at\\r\\ndiscrete locations (x, y, z) specifically ox,y,z = Dec(slotk , (x, y, z)). The architecture of Dec is\\r\\nsimilar to that of [40]. Given slotk , which is one of the slot feature vector. We encode the coordinate\\r\\n(x, y, z) into a 64-D feature vector using a linear layer. We denote this vector as z. The inputs slotk\\r\\nand z are then processed as follows:\\r\\n\\r\\n   out_k = RB_i( RB_{i-1}( \\\\cdots RB_1( z + FC_1(slot_k)) \\\\\\\\ \\\\cdots ] + FC_{i-1}(slot_k)) + FC_{i}(slot_k)).\\r\\n                                                                                                               (3)\\r\\nWe set i = 3. F Ci is a linear layer that outputs a 64 dimensional vector. RNi is a 2 layer ResNet\\r\\nMLP block [27]. The architecture of ResNet block is: ReLU, 64-64, ReLU, 64-64. Here, i \\xe2\\x88\\x92 o\\r\\nrepresents a linear layer, where i and o are the input and output dimension. Finally outk is then\\r\\npassed through a ReLU activation function followed by a linear layer to generate a single value for\\r\\noccupancy.\\r\\n\\r\\nRGB Decoder. Our decoder follows the architecture of [67]. The decoder is a 8 layer MLP,\\r\\nspecifically ReLU, 64-64, ReLU, 64-64 .... 64-4. The MLP takes in as input slotk feature and the\\r\\nlocation x, y, z and predicts the RGB and density values at that location.\\r\\n\\r\\n\\r\\n                                                       18\\r\\n\\x0c\\n\\nSlot Extractor. Following is the pseudo-code of Slot Attention, which we use for learning primitive\\r\\ncompositions.\\r\\n\\r\\nAlgorithm 1 Slot Attention module. Input is N vectors of dimension Dinputs which is mapped to a\\r\\nset of K slots of dimension Dslots . We sample the learnable multivariate gaussian to get the initial\\r\\nslot features, here the learnable parameters are: \\xc2\\xb5 \\xe2\\x88\\x88 RDslots and \\xcf\\x83 \\xe2\\x88\\x88 RDslots .For our experiments we\\r\\nset T = 3.\\r\\n1: Input: inputs \\xe2\\x88\\x88 RN \\xc3\\x97Dinputs , slots \\xe2\\x88\\xbc N (\\xc2\\xb5, diag(\\xcf\\x83)) \\xe2\\x88\\x88 RK\\xc3\\x97Dslots\\r\\n2: Layer params: k, q, v: linear projections; GRU; MLP; LayerNorm (x3)\\r\\n3: inputs = LayerNorm (inputs)\\r\\n4: for t = 0 . . . T\\r\\n5:     slots_prev = slots\\r\\n6:     slots = LayerNorm (slots)\\r\\n7:     attn = Softmax ( \\xe2\\x88\\x9a1 k(inputs) \\xc2\\xb7 q(slots)T , axis=\\xe2\\x80\\x98slots\\xe2\\x80\\x99)\\r\\n                             D\\r\\n8:     updates = Avg (weights=attn + \\xcf\\xb5, values=v(inputs))\\r\\n9:     slots = GRU (state=slots_prev, inputs=updates)\\r\\n10:      slots += MLP (LayerNorm (slots))\\r\\n11: return slots\\r\\n\\r\\n\\r\\nFor primitive learning stage on PointCloud we set the value of number of slots K as 1. Additionally\\r\\nwe replace the Softmax () activation in step 7 with a Sigmoid () activation. For primitive learning\\r\\nstage on RGB scenes we set the value of number of slots K as 2. This is due to RGB images having\\r\\na background componenet in them.\\r\\n\\r\\n7.2   Baselines                               Training\\r\\n\\r\\n\\r\\n               Cross Attention                                                                              P\\r\\n                 With Heads                               Attention\\r\\n                                                                                  C                         O\\r\\n                                                            Map\\r\\n                                                                                  o                         I\\r\\n                                                                                  n                         N\\r\\n                                                                                  c                         T\\r\\n                                                          Attention               a FPN-Style\\r\\n            Encoder\\r\\n                                                            Map                   t Transformer             A\\r\\n                                                                                  e                         R\\r\\n                                                                                  n                         G\\r\\n                                                                                  a                         M\\r\\n                                                                                  t                         A\\r\\n                                                                                  e                         X\\r\\n                                                          Attention\\r\\n                                                            Map\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                 Head features Head features        Spatial Attention\\r\\n                                        T=1           T=2                Maps                     Hungarian Matching\\r\\n                                                                                                              +\\r\\n                                                                                                  Binary CrossEntropy Loss\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 12: 3D-DETR model architecture. Our baseline model is very similar to 2D-DETR[5], where we replace\\r\\ntheir base 2D encoder and 2D FPN-style CNN with a 3D PointTransformer[70]. Given point clouds as input\\r\\nour encoder backbone featurizes the points into N feature vectors, we then do iterative self and cross attention\\r\\nusing K learnable query heads, following the implementation of Carion et al.[5]. We then compute attention of\\r\\nthe updated queries wrt to the encoded feature vectors. We then concatenate these attentions along the batch\\r\\ndimension and pass them through a FPN-style transformer that increases their resolution and outputs each query\\r\\nmask logits. We then do hungarian matching and binary cross entropy loss\\r\\n\\r\\n\\r\\n3D-DETR. 3D-DETR is a SOTA segmentation architecture, we build this architecture on top\\r\\nof 2D-DETR[5], where we replace their 2D encoders with a 3D PointTransformer[70]. For fair\\r\\ncomparision we make sure the number of parameters in 3D-DETR is the same as GFS-Nets.\\r\\nThe base encoder is a 5 layer architecture of 1024-16-64, 1024-16-64, 1024-16-64, 1024-16-64,\\r\\n512-16-64, following the notation of S,N ,C where S is the number of sampled query points, N is\\r\\nthe number of selected neigbouring points and C is the feature dimension. We have 16 learnable\\r\\n\\r\\n\\r\\n                                                               19\\r\\n\\x0c\\n\\nqueries with six encoder-decoder layers of transformer attention, each layer consists of 8 heads. We\\r\\nthen compute multi-head cross-attention between each query vector and the encoded 3d points. This\\r\\ngives us a map of size M \\xc3\\x97 512 \\xc3\\x97 16, where M is the number of heads in the multi-head attention.\\r\\nEach attention map is individually upsampled through a PointTransformer decoder of the architecture\\r\\n512-16-64, 1024-16-64, 1024-16-64, which gives us the final instance segmentation mask per query,\\r\\nsimilar to DETR[5]. We then use Hungarian matching to match the predicted masks against the\\r\\nground truth masks and then apply binary cross entropy loss for each match. We aggregate the losses\\r\\nfrom each query and backpropagate. Figure 12 visualizes the architecture of 3D-DETR. Note that\\r\\nwe do not follow the 2 stage training of [5], rather we train their model end-to-end for instance\\r\\nsegmentation, similar to our model. We have found this trick to save compute and still not harm the\\r\\nend results.\\r\\n\\r\\nLearning2Group.[43] We use their open-sourced architecture and code for comparision with our\\r\\nmode. We train their model using our datasets from scratch.\\r\\n\\r\\nShape2Prog[60] We use their open-sourced architecture, code and pretrained checkpoints for\\r\\ncomparision with our model. We change the value of number of blocks similar to the number of slots\\r\\nin our model for each dataset.\\r\\n\\r\\nPQ-Nets.[64] We use their open-sourced architecture and code for comparision with our model.\\r\\nWe train their model using our datasets from scratch. We change the value of number of slots in their\\r\\nmodel based on the maximum number of parts in the dataset.\\r\\n\\r\\nStruct Implicit.[21] We use their open-sourced architecture and code for comparision with our\\r\\nmode. We train their model using our datasets from scratch. We change the value of number of slots\\r\\nin their model based on the maximum number of parts in the dataset.\\r\\n\\r\\nuORF.[67] We use their open-sourced architecture, code and pretrained checkpoints for compari-\\r\\nsion with our model on the CLEVR dataset. For RoomDiverse++ dataset, we train their model on\\r\\nour dataset using the hyperparmeters they use for training their model on RoomDiverse dataset. We\\r\\nchange the value of number of slots in their model similar to our model for each dataset.\\r\\n\\r\\nSlot Attention.[42] We use their open-sourced architecture and code for comparision with our\\r\\nmodel on the CLEVR and RoomDiverse++ dataset. We train their model using our datasets from\\r\\nscratch. We change the value of number of slots in their model similar to our model for each dataset.\\r\\n\\r\\n8   More Results\\r\\nSegmenting 3D pointclouds without segmentation supervision. In this section we show ad-\\r\\nditional qualitative and quantitative results for Section 4.1.1 of the main paper. We follow the\\r\\nexperimental setup of Section 4.1.1, where we use the generic part dataset (visualized in Fig 7) for\\r\\nprimitive learning. In Table 4, we further extend Table 1 of the main paper to include different levels\\r\\nin Chair and Table category. Here (X/Y /Z) refers to (level 1/level 2/level 3) scores respectively. We\\r\\npick the best performing level in fast inference and specifically report it\\xe2\\x80\\x99s slow inference results. We\\r\\nfurther qualitatively compare our model against Shape2Prog (best performing baseline) in Figure 13\\r\\n\\r\\n                                     in-dist (Chairs)                  out-of-dist (Tables)\\r\\n      Method\\r\\n                              Fast Infer.         Slow Infer.        Fast Infer.      Slow Infer.\\r\\n      Shape2Prog [60]         0.28/0.21/0.26          0.53        0.21/0.23/0.23          0.40\\r\\n      PQ-Nets [64]            0.20/0.18/0.16          0.31        0.17/0.14/0.16          0.21\\r\\n      StructImplicit [21]     0.18/0.22/0.25          0.23        0.14/0.15/0.17          0.17\\r\\n      GFS-Nets                0.51/0.48/0.57          0.62        0.51/0.55/0.60          0.69\\r\\n\\r\\nTable 4: ARI Segmentation accuracy (higher the better) in the test set of Chair (in-distribution) and Table\\r\\ncategory of PartNet(out-of-distribution). GFS-Nets significantly outperform all of the baselines.\\r\\n\\r\\n\\r\\n\\r\\n                                                    20\\r\\n\\x0c\\n\\nRGB image segmentation and view prediction without supervision In Figure 15, we show more\\r\\nqualitative results for Section 4.3 of the main paper, where we qualitatively compare our model for\\r\\nsegmentation against uORF[67] and Slot Attention [42]. We see that slow inference prevents our\\r\\nmodel from missing to detect occluded objects in the scene, where as the baselines have a lot of False\\r\\nPositives i.e missed detections.\\r\\n\\r\\n                        Input          Shape2Prog          GFS-Nets-Fast   GFS-Nets-Slow\\r\\n                                       (Supervised)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n        Figure 13: Additional shape decomposition results using generic primitives. (Section 4.1.1)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      21\\r\\n\\x0c\\n\\n                                    Instance Segmentation\\r\\n\\r\\n\\r\\n                  3D-DETR          GFS-Nets-Slow        3D-DETR        GFS-Nets-Slow\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 14: Additional shape decomposition results using category-specific primitives. (Section 4.1.2)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                   22\\r\\n\\x0c\\n\\n          Ground Truth   Slot Attention   uORF   GFS-Nets-Fast GFS-Nets-Slow\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 15: Additional RGB segmentation results on RoomDiverse++. (Section 4.3)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          23\\r\\n\\x0c', b'                                                                     Transforming Model Prediction for Tracking\\r\\n\\r\\n                                              Christoph Mayer         Martin Danelljan Goutam Bhat Matthieu Paul Danda Pani Paudel\\r\\n                                                                                   Fisher Yu Luc Van Gool\\r\\n                                                                     Computer Vision Lab, D-ITET, ETH Zu\\xcc\\x88rich, Switzerland\\r\\narXiv:2203.11192v1 [cs.CV] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 Abstract                                                                                              ToMP-50\\r\\n                                                                                                                   66.9                                                      ToMP-101\\r\\n                                                                                                                   66.7\\r\\n                                            Optimization based tracking methods have been widely                   66.4            TrDiMP                 KeepTrack\\r\\n                                                                                                                   66.2\\r\\n                                                                                                                                              TransT    STARK-ST101\\r\\n                                         successful by integrating a target model prediction mod-\\r\\n                                                                                                                   63.9\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                             NFS\\r\\n                                         ule, providing effective global reasoning by minimizing an                                                     STARK-ST50\\r\\n                                         objective function. While this inductive bias integrates                  65.2\\r\\n\\r\\n                                         valuable domain knowledge, it limits the expressivity of                  64.7                   ToMP-50+\\r\\n                                                                                                                          SuperDiMP         IoUNet\\r\\n                                         the tracking network. In this work, we therefore pro-\\r\\n                                                                                                                   63.9                            SiamRCNN\\r\\n                                         pose a tracker architecture employing a Transformer-based\\r\\n                                         model prediction module. Transformers capture global                               63.1   63.9     64.7              66.4   67.1       68.5\\r\\n                                         relations with little inductive bias, allowing it to learn                                                 LaSOT\\r\\n                                         the prediction of more powerful target models. We fur-\\r\\n                                                                                                             Figure 1. Performance improvements when transforming the\\r\\n                                         ther extend the model predictor to estimate a second set\\r\\n                                                                                                             model optimizer based tracker SuperDiMP [12] ( ) step-by-step.\\r\\n                                         of weights that are applied for accurate bounding box               First, we replace the model optimizer by a Transformer based\\r\\n                                         regression. The resulting tracker relies on training and            model predictor ( ). Secondly, we replace the probabilistic\\r\\n                                         on test frame information in order to predict all weights           IoUNet by a new regressor and predict its weights with the same\\r\\n                                         transductively. We train the proposed tracker end-to-end            model predictor ( ). The performance (success AUC) is reported\\r\\n                                         and validate its performance by conducting comprehensive            on NFS [23] and LaSOT [20] and compared with recent track-\\r\\n                                         experiments on multiple tracking datasets. Our tracker              ers ( ). ToMP-50 and ToMP-101 refer to the different employed\\r\\n                                         sets a new state of the art on three benchmarks, achiev-            backbones ResNet-50 [28] and ResNet-101 [28].\\r\\n                                         ing an AUC of 68.5% on the challenging LaSOT [20]\\r\\n                                         dataset. The code and trained models are available at               frames, providing effective global reasoning when learning\\r\\n                                         https://github.com/visionml/pytracking                              the model. However, it also imposes severe inductive bias\\r\\n                                                                                                             on the predicted target model. Since the target model is ob-\\r\\n                                                                                                             tained by solely minimizing an objective over the previous\\r\\n                                                                                                             frames, the model predictor has limited flexibility. For in-\\r\\n                                         1. Introduction\\r\\n                                                                                                             stance, it cannot integrate any learned priors in the predicted\\r\\n                                            Generic visual object tracking is one of the fundamental         target model. On the other hand, Transformers have also\\r\\n                                         problems in computer vision. The task involves estimating           been shown to provide strong global reasoning across mul-\\r\\n                                         the state of the target object in every frame of a video se-        tiple frames, thanks to the use of self and cross attention.\\r\\n                                         quence, given only the initial target location. One of the          Consequently, Transformers have been applied to generic\\r\\n                                         key problems in object tracking is learning to robustly de-         object tracking [7, 59, 63, 67] with considerable success.\\r\\n                                         tect the target object, given the scarce annotation. Among              In this work, we propose a novel tracking framework that\\r\\n                                         exiting methods, Discriminative Correlation Filters (DCF)           aims at bridging the gap between DCF and Transformer\\r\\n                                         [1, 5, 13, 14, 24, 29, 46, 54] have achieved much success.          based trackers. Our approach employs a compact target\\r\\n                                         These approaches learn a target model to localize the tar-          model for localizing the target, as in DCF. The weights of\\r\\n                                         get in each frame, by minimizing a discriminative objective         this model are however obtained using a Transformer-based\\r\\n                                         function. The target model, often set to a convolutional ker-       model predictor, allowing us to learn more powerful target\\r\\n                                         nel, provides a compact and generalizable representation of         models, compared to DCFs. This is achieved by introduc-\\r\\n                                         the tracked object, leading to the popularity of DCFs.              ing novel encodings of the target state, allowing the Trans-\\r\\n                                            The objective function in DCF integrates both fore-              former to effectively utilize this information. We further ex-\\r\\n                                         ground and background knowledge over the previous                   tend our model predictor to generate weights for a bounding\\r\\n\\r\\n\\r\\n                                                                                                         1\\r\\n\\x0c\\n\\nbox regressor network, in order to condition its predictions           trast, TransT [7] employs a feature fusion network that con-\\r\\non the current target. Our proposed approach ToMP obtains              sists of multiple self and cross attention modules. The fused\\r\\nsignificant improvement in tracking performance compared               output features are fed into a target classifier and a bound-\\r\\nto state-of-the-art DCF-based methods, while also outper-              ing box regressor. TrDiMP [59] adopts the DiMP [1] model\\r\\nforming recent Transformer based trackers (see Fig. 1).                predictor to produce the model weights given the output fea-\\r\\nContributions: In summary, our main contributions are the              tures of the Transformer Encoder as training samples. Af-\\r\\nfollowing: i) We propose a novel Transformer-based model               terwards, the target model computes the target score map\\r\\nprediction module in order to replace traditional optimiza-            by applying the predicted weights on the output features\\r\\ntion based model predictors. ii) We extend the model pre-              produced by the Transformer Decoder. TrDiMP adopts\\r\\ndictor to estimate a second set of weights that are applied            the probabilistic IoUNet [16] for bounding box regression.\\r\\nfor bounding box regression. iii) We develop two novel                 Similar to our tracker, TrDiMP encodes target state infor-\\r\\nencodings that incorporate target location and target extent           mation but integrates it via two different cross attention\\r\\nallowing the Transformer-based model predictor to utilize              modules in the Decoder instead of using two encoding mod-\\r\\nthis information. iv) We propose a parallel two stage track-           ules in front of the Transformer.\\r\\ning procedure at test time to decouple target localization and             In contrast to the aforementioned Transformer based\\r\\nbounding box regression in order to achieve robust and ac-             trackers, STARK [63] adopts the Transformer architecture\\r\\ncurate target detection. v) We perform a comprehensive set             from DETR [6]. Instead of fusing the training and test\\r\\nof ablation experiments to assess the contribution of each             features in the Transformer Decoder they are stacked and\\r\\nbuilding block of our tracking pipeline and evaluate it on             processed jointly by the full Transformer. A single object-\\r\\nseven tracking benchmarks. The proposed tracker ToMP                   query then produces the Decoder output that is fused with\\r\\nsets a new state of the art on three including LaSOT [20]              the Transformer Encoder features. These features are then\\r\\nwhere it achieves an AUC of 68.5% (see Fig. 1). In addition            further processed to directly predict the bounding box of\\r\\nwe show that our tracker ToMP outperforms other Trans-                 the target. In contrast, our tracker employs the same Trans-\\r\\nformer based trackers for every attribute of LaSOT [20].               former architecture from DETR [6] but to replace the model\\r\\n                                                                       optimizer. In the end, our resulting Transformer-based\\r\\n                                                                       model predictor estimates the weights of two separate mod-\\r\\n2. Related Work                                                        els: the target classifier and the bounding box regressor.\\r\\nDiscriminative Model Prediction: DCF based approaches\\r\\nlearn a target model to distinguish the target from back-              3. Method\\r\\nground by minimizing an objective. For long Fourier-                      In this work, we propose a Transformer-based target\\r\\ntransform based solvers were predominant for DCF based                 model prediction network for tracking called ToMP. We first\\r\\ntrackers [5, 15, 29, 46]. Danelljan et al. [13] employed               revisit existing optimization based model predictors and\\r\\na two layer Perceptron as target model and use Conju-                  discuss their limitations in Sec. 3.1. Next, we describe our\\r\\ngate Gradient to solve the optimization problem. Recently,             Transformer-based model prediction approach in Sec. 3.2.\\r\\nmultiple methods have been introduced that enable end-to-              We extend this approach to perform joint target classifi-\\r\\nend training by casting the tracking problem into a meta-              cation and bounding box regression in Sec. 3.3. Finally,\\r\\nlearning problem [1, 58, 72]. These methods are based on               we detail our offline training procedure and online tracking\\r\\nthe idea of unrolling the iterative optimization algorithm for         pipeline in Sec. 3.4 and Sec. 3.5, respectively.\\r\\na fixed number of iterations and to integrate it in the tracking\\r\\npipeline to allow end-to-end training. Bhat et al. [1] learn a         3.1. Background\\r\\ndiscriminative feature space and predict the weights of the               One of the popular paradigms for visual object tracking\\r\\ntarget model based on the target state in the initial frame and        is discriminative model prediction based tracking. These\\r\\nrefine the weights with an optimization algorithm.                     approaches, visualized in Fig. 2a, use a target model to lo-\\r\\nTransformers for Tracking: Recently, several trackers                  calize the target object in the test frame. The weights (pa-\\r\\nhave been introduced that use Transformers [7, 59, 63, 67].            rameters) of this target model are obtained from the model\\r\\nTransformers are typically employed to predict discrim-                optimizer, using the training frames and their annotation.\\r\\ninative features to localize the target object and regress             While a variety of target models are used in the litera-\\r\\nits bounding box. The training features are processed by               ture [1, 13, 33, 46, 54, 58, 72], discriminative trackers share\\r\\nthe Transformer Encoder whereas the Transformer Decoder                a common base formulation to produce the target model\\r\\nfuses training and test features using cross attention layers          weights. This involves solving an optimization problem\\r\\nto compute discriminative features [7, 59, 67].                        such that the target model produces the desired target states\\r\\n    DTT [67] feeds these features to two networks that pre-            yi \\xe2\\x88\\x88 Y for the training samples Strain \\xe2\\x88\\x88 {(xi , yi )}m     i=1 .\\r\\ndict the location and the bounding box of the target. In con-          Here, xi \\xe2\\x88\\x88 X refers to a deep feature map of frame i and m\\r\\n\\r\\n                                                                   2\\r\\n\\x0c\\n\\n                        Test\\r\\n                      Test    Frame\\r\\n                           Frame                                                                    Test Test\\r\\n                                                                                                         FrameFrame\\r\\n                                                                                                                                                   Model\\r\\n                                                                                                                                                 Model    Predictor\\r\\n                                                                                                                                                       Predictor\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                 Backbone\\r\\n                                      Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                      Backbone\\r\\n                                                                                                                                                                                   TargetTarget\\r\\n                                                                                                                                                                                          ScoresScores\\r\\n                                                     Target\\r\\n                                                  Target    Model\\r\\n                                                         Model                                                                              Test Test\\r\\n                                                                                                                                                 FrameFrame\\r\\n                                                                                                                                                  Encoding\\r\\n                                                                                                                                             Encoding\\r\\n\\r\\n                                                          Model\\r\\n                                                              ModelTarget Scores\\r\\n                                                                       Target Scores\\r\\n                                                          Weights                                                                                               Transformer\\r\\n                                                                                                                                                                     Transformer\\r\\n                                                             Weights\\r\\n            Training Frames\\r\\n               Training Frames                                                             Training Frames\\r\\n                                                                                               Training Frames                                                    Encoder\\r\\n                                                                                                                                                                       Encoder\\r\\n                                                                                                                                                                                          ModelModel\\r\\n                                                                                                                                                                                   TargetTarget\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                      Backbone\\r\\n                                      Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                 Backbone\\r\\n                                                                                                                                            Target StateState\\r\\n                                                                                                                                                Target          Transformer\\r\\n                                                                                                                                                                     Transformer\\r\\n                                                 Model Optimizer\\r\\n                                                   Model Optimizer\\r\\n                                                                                                                                             Encoding\\r\\n                                                                                                                                                  Encoding        Decoder\\r\\n                                                                                                                                                                       Decoder\\r\\n                                                                                                                                                                               ModelModel\\r\\n                                                                                                                                                                               Weights\\r\\n                                                                                                                                                                                   Weights\\r\\n\\r\\n\\r\\n         (a) Tracker with optimization based model prediction.                                   (b) Proposed tracker with Transformer based model prediction.\\r\\n\\r\\nFigure 2. Comparison between trackers that employ optimization based model prediction and our Transformer-based model prediction.\\r\\nThe model optimizer [\\x04] in Fig. 2a is replaced by the model predictor in Fig. 2b that consists of the proposed modules [\\x04,\\x04,\\x04,\\x04].\\r\\n\\r\\n\\r\\ndenotes the total number of training frames. The optimiza-                                  predictor based on Transformers (see Fig. 2b). Instead of\\r\\ntion problem reads as follows,                                                              explicitly minimizing an objective as stated in (1), our ap-\\r\\n                     X                                                                      proach learns to directly predict the target model purely\\r\\n    w = arg min              f (h(w\\xcc\\x83; x), y) + \\xce\\xbbg(w\\xcc\\x83). (1)                                  from data by end-to-end training. This allows the model\\r\\n             w\\xcc\\x83\\r\\n                     (x,y)\\xe2\\x88\\x88Strain                                                           predictor to integrate target specific priors in the predicted\\r\\nHere, the objective consists of the residual function f which                               model so that it can focus on characteristic features of the\\r\\ncomputes an error between the target model output h(w\\xcc\\x83; x)                                  target, in addition to the features that allow to differenti-\\r\\nand the ground truth label y. g(w\\xcc\\x83) denotes the regulariza-                                 ate the target from the seen background. Furthermore, our\\r\\ntion term weighted by a scalar \\xce\\xbb, while w represents the                                    model predictor also utilizes the current test frame features,\\r\\noptimal weights of the target model. Note that the training                                 in addition to the previous training features, to predict the\\r\\nset Strain contains the annotated first frame, as well as the                               target model in a transductive manner. As a result, the\\r\\nprevious tracked frames with the tracker\\xe2\\x80\\x99s predictions being                                model predictor can utilize the current frame information\\r\\nused as pseudo-labels.                                                                      to predict a more suitable target model. Finally, instead of\\r\\n    Learning the target model by explicitly minimizing the                                  applying the target model on a fixed feature space, defined\\r\\nobjective of (1) provides a robust target model that can dis-                               by the pre-trained feature extractor, our approach can uti-\\r\\ntinguish the target from the previously seen background.                                    lize the target information to dynamically construct a more\\r\\nHowever, such a strategy suffers from notable limita-                                       discriminative feature space for every frame.\\r\\ntions. The optimization based methods compute the tar-                                         An overview of the proposed tracker employing the\\r\\nget model using only limited information available in previ-                                Transformer-based model prediction is shown in Fig. 2b.\\r\\nously tracked frames. That is, they cannot integrate learned                                Similar to the optimization based trackers, it consists of a\\r\\npriors in the target model prediction so as to minimize future                              test and training branch. We first encode the target state in-\\r\\nfailures. Similarly, these methods typically lack the possi-                                formation in the training frames and fuse it with the deep\\r\\nbility to utilize the current test frame in a transductive man-                             image features [\\x04]. Similarly, we also add an encoding\\r\\nner when computing the model weights to improve tracking                                    to the test frame in order to mark it as test frame [\\x04].\\r\\nperformance. The optimization based methods also require                                    The features from both the training and test branches are\\r\\nsetting multiple optimizer hyper-parameters, and can over-                                  then jointly processed in the Transformer Encoder [\\x04] that\\r\\nfit/underfit on the training samples. Another limitation of                                 produces enhanced features by reasoning globally across\\r\\noptimization based trackers is their procedure that produces                                frames. Next, the Transformer Decoder [\\x04] predicts the tar-\\r\\nthe discriminative features. Usually, the features provided                                 get model weights [ ] using the output of the Transformer\\r\\nto the target model are simply the extracted test features.                                 Encoder. Finally, the predicted target model is applied on\\r\\nInstead of reinforced features by using the target state in-                                the enhanced test frame features to localize the target. Next,\\r\\nformation contained in the training frames. Extracting such                                 we describe the main components in our tracking pipeline.\\r\\nenhanced features would allow reliable differentiation be-\\r\\ntween the target and background regions in the test frame.                                  Target Location Encoding: We propose a target location\\r\\n                                                                                            encoding that allows the model predictor to incorporate the\\r\\n3.2. Transformer-based Target Model Prediction\\r\\n                                                                                            target state information from the training frames, when pre-\\r\\n   In order to overcome the aforementioned limitations of                                   dicting the target model. In particular, we use the embed-\\r\\noptimization based target localization approaches, we pro-                                  ding efg \\xe2\\x88\\x88 R1\\xc3\\x97C that represents foreground. Together with\\r\\npose to replace the model optimizer by a novel target model                                 a Gaussian yi \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x971 centered at the target location,\\r\\n\\r\\n\\r\\n                                                                                       3\\r\\n\\x0c\\n\\n                    Backbone\\r\\n  Test                                                                                                                              Target Classi cation\\r\\n                                                 +\\r\\n Frame\\r\\n                                  xtest                                                                                                     wcls\\r\\n                   etest            \\xc2\\xb5(etest )\\r\\n                                                                                                                                    h(wcls ; ztest )\\r\\nTraining Frames\\r\\n                                                                                                                                                                y\\xcc\\x82test\\r\\n                                                             v1            v2         vtest\\r\\n                    Backbone\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             +   +\\r\\n\\r\\n                                     xi\\r\\n                                                             Tenc ([v1 , v2 , vtest ])\\r\\n   d1       d2                                                                                ztest\\r\\n                                                               z1          z2         ztest\\r\\n                     (di )                                                                                                      \\xe2\\x87\\xa4\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                           +\\r\\n                                                                                                                wcls\\r\\n                                                                                              w                                                        CNN          d\\xcb\\x86test\\r\\n                                                     efg   Tdec ([z1 , z2 , ztest ], efg )          Linear\\r\\n                                                                                                                  wbbreg\\r\\n                               (yi , efg )\\r\\n\\r\\n   y1       y2           efg\\r\\n\\r\\n\\r\\n Feature Extraction and Target Encoding                    Transformer-based Model Prediction                                   Bounding Box Regression\\r\\nFigure 3. Overview of the entire ToMP tracking pipeline for joint model prediction. First, the training [\\x04] and test [\\x04] features are\\r\\nextracted using a backbone. Then the target location [\\x04] and bounding box [\\x04] encodings are added to the training features. For the test\\r\\nframe the test embedding is encoded [\\x04] and added to the test features. The features are then concatenated and jointly processed by the\\r\\nTransformer-based model predictor that produces the weights used for target classification [\\x04] and bounding box regression [\\x04].\\r\\n    fi\\r\\nwe define the target encoding function                                                   features, which serve as the input to the target model when\\r\\n                                                                                         localizing the target.\\r\\n                      \\xcf\\x88(yi , efg ) = yi \\xc2\\xb7 efg ,                          (2)                 Given multiple encoded training features vi \\xe2\\x88\\x88\\r\\n                                                                                         RH\\xc3\\x97W \\xc3\\x97C and an encoded test feature vtest \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97C ,\\r\\nwhere \\xe2\\x80\\x9d\\xc2\\xb7\\xe2\\x80\\x9d denotes point-wise multiplication with broadcast-                              we reshape the features to R(H\\xc2\\xb7W )\\xc3\\x97C and concatenate all m\\r\\ning. Note, that Him = s \\xc2\\xb7 H and Wim = s \\xc2\\xb7 W correspond to                                training features vi and the test feature vtest along the first\\r\\nthe spatial dimension of the image patch and s to the stride                             dimension. These concatenated features are then processed\\r\\nof the backbone network used to extract the deep features                                jointly in a Transformer Encoder\\r\\nx \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97C . Next, we combine the target encoding and\\r\\nthe deep image features x as follows                                                              [z1 , . . . , zm , ztest ] = Tenc ([v1 , . . . , vm , vtest ]).        (5)\\r\\n                                                                                         The Transformer Encoder consists of multi-headed self-\\r\\n                     vi = xi + \\xcf\\x88(yi , efg ).                             (3)             attention modules [56] that enable it to reason globally\\r\\n                                                                                         across a full frame and even across multiple training and\\r\\nThis provides us the training frame features vi \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x97C                                test frames. In addition, the encoded target state identifies\\r\\nwhich contain encoded target state information. Similarly,                               foreground and background regions and enables the Trans-\\r\\nwe also add a test encoding to identify the features corre-                              former to differentiate between both regions.\\r\\nsponding to the test frame as,\\r\\n                                                                                         Transformer Decoder: The outputs of the Transformer\\r\\n                   vtest = xtest + \\xc2\\xb5(etest ),                            (4)             Encoder (zi and ztest ) are used as inputs for the Transformer\\r\\n                                                                                         Decoder [6, 56] to predict the target model weights\\r\\nwhere \\xc2\\xb5(\\xc2\\xb7) repeats the token etest for each patch of xtest .                                              w = Tdec ([z1 , . . . , zm , ztest ], efg ).                   (6)\\r\\nTransformer Encoder: We aim to predict our target\\r\\n                                                                                         Note that the inputs zi and ztest are obtained by jointly rea-\\r\\nmodel using the foreground and background information\\r\\n                                                                                         soning over the whole training and test samples, allowing us\\r\\nfrom both the training, as well as the test frames. To achieve\\r\\n                                                                                         to predict a discriminative target model. We use the same\\r\\nthis, we use a Transformer Encoder [6, 56] module to first\\r\\n                                                                                         learned foreground embedding efg as used for target state\\r\\njointly process the features from the training frames and the\\r\\n                                                                                         encoding as input query of the Transformer Decoder such\\r\\ntest frame. The Transformer Encoder serves two purposes\\r\\n                                                                                         that the Decoder predicts the target model weights.\\r\\nin our approach. First, as described later, it computes the\\r\\n                                                                                         Target Model: We use the DCF target model to obtain the\\r\\nfeatures used by the Transformer Decoder module to pre-\\r\\n                                                                                         target classification scores\\r\\ndict the target model. Secondly, inspired by STARK [63],\\r\\nour Transformer Encoder also outputs enhanced test frame                                                         h(w, ztest ) = w \\xe2\\x88\\x97 ztest .                              (7)\\r\\n\\r\\n\\r\\n                                                                                 4\\r\\n\\x0c\\n\\nHere, the weights of the convolution filter w \\xe2\\x88\\x88 R1\\xc3\\x97C are                         gression. Concretely, we pass the output w of the Trans-\\r\\npredicted by the Transformer Decoder. Note that the tar-                         former Decoder through a linear layer to obtain the weights\\r\\nget model is applied on the output test features ztest of                        for bounding box regression wbbreg and target classification\\r\\nthe Transformer Encoder. These features are obtained after                       wcls . The weights wcls are then directly used within the tar-\\r\\njoint processing of training and test frames, and thus sup-                      get model h(wcls ; ztest ) as before. The weights wbbreg , on\\r\\nport the target model to reliably localize the target.                           the other hand, are used to condition the output test features\\r\\n                                                                                 ztest of the Transformer Encoder with target information for\\r\\n3.3. Joint Localization and Box Regression                                       bounding box regression, as explained next.\\r\\n    In the previous section, we presented our Transformer                        Bounding Box Regression: To make the encoder output\\r\\nbased architecture for predicting the target model. Although                     features ztest target aware, we follow Yan et al. [63] and\\r\\nthe target model can localize the object center in each frame,                   first compute an attention map wbbreg \\xe2\\x88\\x97 ztest using the pre-\\r\\na tracker needs to also estimate an accurate bounding box of                     dicted weights wbbreg . The attention weights are then mul-\\r\\nthe target. DCF based trackers typically employ a dedicated                      tiplied point-wise with the test features ztest before feeding\\r\\nbounding box regression network [13] for this task. While it                     them into a Convolutional Neural Network (CNN). The last\\r\\nis possible to follow a similar strategy, we decide to predict                   layer of the CNN uses an exponential activation function\\r\\nboth models jointly since target localization and bounding                       to produce the normalized bounding box prediction in the\\r\\nbox regression are related tasks that can benefit from one                       same ltrb representation as described in Eq. (8). In order to\\r\\nanother. In order to achieve this, we extend our model as                        obtain the final bounding box estimation, we first extract the\\r\\nfollows. First, instead of only using the target center loca-                    center location by applying the argmax(\\xc2\\xb7) function on the\\r\\ntion when generating the target state encoding, we also en-                      target score map y\\xcc\\x82test predicted by the target model. Next,\\r\\ncode target size information to provide a richer input to our                    we query the dense bounding box prediction d\\xcb\\x86test at the cen-\\r\\nmodel predictor. Secondly, we extend our model predictor                         ter location of the target object to obtain the bounding box.\\r\\nto estimate weights for a bounding box regression network,                       We use two dedicated networks for target localization and\\r\\nin addition to the target model weights. The resulting track-                    bounding box regression in contrast to Yan et al. [63] that\\r\\ning architecture is visualized in Fig. 3. Next, we describe                      uses one network trying to predict both. This allows us as\\r\\neach of these changes in detail.                                                 explained in Sec. 3.5 to decouple target localization from\\r\\nTarget Extent Encoding: In addition to the extracted                             bounding box regression during tracking.\\r\\ndeep image features xi and the target location encoding\\r\\n                                                                                 3.4. Offline Training\\r\\n\\xcf\\x88(yi , efg ), we add another encoding to incorporate infor-\\r\\nmation about the bounding box of the target. In order to                            In this section, we describe the protocol to train the pro-\\r\\nencode the bounding box bi = {bxi , byi , bw                 h\\r\\n                                                       i , bi } encompass-       posed tracker ToMP. Similar to recent end-to-end trained\\r\\ning the target object in the training frame i, we adopt the ltrb                 discriminative trackers [1, 16], we sample multiple training\\r\\nrepresentation [22, 55, 62, 67]. First, we map each location                     and test frames from a video sequence to form training sub-\\r\\n(j x , j y ) on the feature map xi back to the image domain                      sequences. In particular, we use two training frames and one\\r\\nusing (k x , k y ) = (b 2s c + s \\xc2\\xb7 j x , b 2s c + s \\xc2\\xb7 j y ). Then, we com-       test frame. In contrast to recent Transformer based track-\\r\\npute the normalized distance of each remapped location to                        ers [7, 63, 67] but similar to DCF based trackers [1, 13, 16],\\r\\nthe four sides of the bounding box bi as follows,                                we keep the same spatial resolution for training and test\\r\\n                                                                                 frames. We pair each image Ii with the corresponding\\r\\n   li = (k x \\xe2\\x88\\x92 bxi )/Wim ,       ri = (k x \\xe2\\x88\\x92 bxi \\xe2\\x88\\x92 bw\\r\\n                                                    i )/Wim ,                    bounding box bi . We use the target state of the training\\r\\n                                                                      (8)\\r\\n  ti = (k y \\xe2\\x88\\x92 byi )/Him ,        bi = (k y \\xe2\\x88\\x92 byi \\xe2\\x88\\x92 bhi )/Him ,                   frames to encode target information and use the bounding\\r\\n                                                                                 box of the test frame only to supervise training by comput-\\r\\nwhere Wim = s \\xc2\\xb7 W and Him = s \\xc2\\xb7 H. These four sides are                          ing two losses based on the predicted bounding boxes and\\r\\nused to produce the dense bounding box representation d =                        the derived center location of the target in the test frame.\\r\\n(l, t, r, b), where d \\xe2\\x88\\x88 RH\\xc3\\x97W \\xc3\\x974 . In this representation, we                        We employ the target classification loss from DiMP [1]\\r\\nencode the bounding box using a Multi-Layer Perceptron                           that consists of different losses for background and fore-\\r\\n(MLP) \\xcf\\x86 and thereby increase the number of dimensions                            ground regions. Further, we employ the generalized Inter-\\r\\nfrom 4 to C before adding the obtained encoding to Eq. (3)                       section over Union loss [52] using the ltrb bounding box\\r\\nsuch that                                                                        representation [55] to supervise bounding box regression\\r\\n                 vi = xi + \\xcf\\x88(yi , efg ) + \\xcf\\x86(di ).         (9)\\r\\n                                                                                                                                  \\xcb\\x86 d),\\r\\n                                                                                          Ltot = \\xce\\xbbcls Lcls (y\\xcc\\x82, y) + \\xce\\xbbgiou Lgiou (d,      (10)\\r\\nHere, vi is the resulting feature map which is used as input\\r\\nto the Transformer Encoder, see Fig. 3.                                          where \\xce\\xbbcls and \\xce\\xbbgiou are scalars weighting the contribution\\r\\nModel Prediction: We extend our architecture to predict                          of each loss. Note that in contrast to FCOS [55] and related\\r\\nweights for the target model, as well as bounding box re-                        trackers [22] we omit an additional centerness loss since\\r\\n\\r\\n                                                                             5\\r\\n\\x0c\\n\\nit would be redundant in addition to our classification loss          and ToMP-50 achieve 19.6 and 24.8 FPS and use a ResNet-\\r\\nthat serves the same purpose. A detailed study examining              101 [28] and ResNet-50 [28] as backbone respectively.\\r\\nthe impact of centerness is available in the supplementary.\\r\\nTraining Details: We train our tracker on the training splits         4.1. Ablation Study\\r\\nof the LaSOT [20], GOT10k [31], Trackingnet [50] and                      We perform a comprehensive analysis of the proposed\\r\\nMS-COCO [43] datasets. We sample 40k sub-sequences                    tracker. First, we analyze the contribution of the different\\r\\nand train for 300 epochs on two Nvidia Titan RTX GPUs.                proposed target state encodings and then examine the effect\\r\\nWe use ADAMW [45] with a learning rate of 0.0001 that we              of different inference settings. Finally, we report the perfor-\\r\\ndecay by a factor of 0.2 after 150 and 250 epochs and weight          mance achieved when replacing the target classifier or the\\r\\ndecay of 0.0001. We set \\xce\\xbbcls = 100 and \\xce\\xbbgiou = 1. We con-             bounding box regressor of SuperDiMP with ours. All abla-\\r\\nstruct a training sub-sequence by randomly sampling two               tion experiments in this part use a ResNet-50 as backbone.\\r\\ntraining frames and a test frame from a 200 frame window\\r\\n                                                                      Target State Encoding: In order to analyze the effect of\\r\\nwithin a video sequence. We then extract the image patches\\r\\n                                                                      the different target state encodings we train different vari-\\r\\nafter randomly translating and scaling the image relative to\\r\\n                                                                      ants of our network and evaluate them on multiple datasets.\\r\\nthe target bounding box. Moreover, we use random image\\r\\n                                                                      The first five rows of Tab. 1 correspond to versions with dif-\\r\\nflipping and color jittering for data augmentation. We set\\r\\n                                                                      ferent target location encodings. All other settings are kept\\r\\nthe spatial resolution of the target scores to 18 \\xc3\\x97 18 and set\\r\\n                                                                      the same. In addition to the foreground and test embedding,\\r\\nthe search area scale factor to 5.0. Further training and ar-\\r\\n                                                                      we include a learned background embedding (instead of set-\\r\\nchitecture details are provided in the supplementary Sec. A.\\r\\n                                                                      ting ebg = 0) to our analysis as follows: \\xcf\\x88(yi , efg , ebg ) =\\r\\n3.5. Online Tracking                                                  yi \\xc2\\xb7 efg + (1 \\xe2\\x88\\x92 yi ) \\xc2\\xb7 ebg . However, Tab. 1 shows (4th vs. 5th\\r\\n                                                                      row) that adding such a learned background embedding de-\\r\\n   During tracking, we use the annotated first frame, as              creases the tracking performance. We further observe that\\r\\nwell as previously tracked frames as our training set Strain .        setting the foreground embedding efg = 0 (1st row) and\\r\\nWhile we always keep the initial frame and its annotation,            only relying on the target extent encoding \\xcf\\x86(\\xc2\\xb7) still achieves\\r\\nwe include one previously tracked frame and replace it with           high tracking performance but clearly lacks behind all other\\r\\nthe most recent frame that achieves a target classifier confi-        versions that include the foreground embedding. We con-\\r\\ndence higher than a threshold. Hence, the training set Strain         clude that using only the foreground encoding efg and the\\r\\ncontains at most two frames.                                          test encoding etest leads to the best performance (4th row).\\r\\n   We observed that incorporating previous tracking re-\\r\\n                                                                          In the second part of Tab. 1 we choose the best settings\\r\\nsults in Strain improves the target localization considerably..\\r\\n                                                                      for the target location encoding and remove either the target\\r\\nHowever, including predicted bounding box estimations de-\\r\\n                                                                      extent encoding \\xcf\\x86(\\xc2\\xb7) or decouple the Transformer Decoder\\r\\ngrades the bounding box regression performance due to in-\\r\\n                                                                      query from the foreground embedding efg . We observe that\\r\\naccurate predictions, see Sec. 4.1. Hence, we run the model\\r\\n                                                                      using a separate query (6th row) decreases the overall per-\\r\\npredictor twice. First, we include intermediate predictions\\r\\n                                                                      formance. Similarly, we notice that incorporating target ex-\\r\\nin Strain to obtain the classifier weights. In the second pass,\\r\\n                                                                      tent information via the proposed encoding is crucial. Oth-\\r\\nwe only use the annotated initial frame to predict the bound-\\r\\n                                                                      erwise, the performance drops significantly (7th row).\\r\\ning box. Note that for efficiency both steps can be per-\\r\\nformed in parallel in a single forward pass. In particular,           Model Predictor: Since our model predictor estimates two\\r\\nwe reshape the feature map corresponding to two training              different model weights, it seems natural to use two differ-\\r\\nand one test frame to a sequence and duplicate it. Then, we           ent Transformer queries: one to produce the target model\\r\\nstack both in the batch dimension to process them jointly                   efg   ebg   etest   \\xcf\\x86(\\xc2\\xb7)   qdec = efg   LaSOT   NFS    OTB\\r\\nwith the model predictor. To only allow attention between              1    7     7      7      X         n.a.      66.0    64.8   68.2\\r\\nthe initial frame with ground truth annotation and the test            2    X     7      7      X          X        67.1    66.6   70.0\\r\\nframe when predicting the model for bounding box regres-               3    X     X      7      X          X        67.1    66.3   69.4\\r\\n                                                                       4    X     7      X      X          X        67.6    66.9   70.1\\r\\nsion, we make use of the so-called key padding mask that\\r\\n                                                                       5    X     X      X      X          X        67.4    66.0   69.5\\r\\nallows to ignore certain keys when computing attention.\\r\\n                                                                       6    X     7      X      X          7        66.0    66.2   69.9\\r\\n                                                                       7    X     7      X      7          X        63.1    64.2   64.0\\r\\n4. Experiments\\r\\n                                                                      Table 1. For efg , ebg and etest learning the embedding is denoted\\r\\n   We evaluate our proposed tracking architecture ToMP                by X whereas 7 means setting it to zero. Using the encoding \\xcf\\x86(\\xc2\\xb7)\\r\\non seven benchmarks. Our approach is based on PyTorch                 is denoted by X whereas 7 refers to omitting it. For qdec = efg the\\r\\n1.7 and is developed within the PyTracking [12] frame                 symbol X means sharing the learned embedding efg for encoding\\r\\nwork. PyTracking is available under the GNU GPL 3.0 li-               and querying the Decoder wheres 7 means learning two separate\\r\\ncense. On a single Nvidia RTX 2080Ti GPU, ToMP-101                    embeddings for both tasks. (Our final model is in the 4th row).\\r\\n\\r\\n\\r\\n                                                                  6\\r\\n\\x0c\\n\\n   Number of                        Decoder                                       4.2. Comparison to the State of the Art\\r\\n Decoder queries    Linear Layer   query qdec       LaSOT NFS         OTB\\r\\n        1                X         qdec = efg        67.6     66.9    70.1           We compare our tracker ToMP on seven tracking bench-\\r\\n        2                7         qdec 6= efg       63.7     62.8    67.9        marks. The same settings and parameters are used for all\\r\\n                                                                                  datasets. We recompute the metrics of all trackers using the\\r\\nTable 2. Analysis of different model predictor architectures and its\\r\\n                                                                                  raw predictions if available or otherwise report the results\\r\\nimpact on the tracking performance in terms of success AUC.\\r\\n                                                                                  given in the respective paper.\\r\\n   Two Stage               Previous                                               LaSOT [20]: First, we compare ToMP on the large-scale\\r\\n Model Prediction      Tracking Results          LaSOT      NFS      OTB          LaSOT dataset (280 test sequences with 2500 frames on\\r\\n        n.a.                   7                 65.7       65.3     67.8         average). The success plot in Fig. 5a shows the overlap\\r\\n         X                     X                 67.6       66.9     70.1         precision OPT as a function of the threshold T . Trackers\\r\\n         7                     X                 62.0       64.8     62.8\\r\\n                                                                                  are ranked w.r.t. their area-under-the-curve (AUC) score,\\r\\nTable 3. Analysis of different inference settings an of their impact              shown in the legend. Tab. 5 shows more results including\\r\\non the tracking performance in terms of success AUC.                              precision and normalized precision for each tracker. Both\\r\\n                                                                                  versions of ToMP with different backbones outperform the\\r\\n  Model         Bounding Box                                       LaSOT\\r\\n Predictor        Regressor         LaSOT         NFS    UAV       ExtSub         recent trackers STARK [63], TransT [7], TrDiMP [59] and\\r\\n DiMP [1]      Prob. IoUNet [16]      63.1        64.8   67.7        43.7\\r\\n                                                                                  DTT [67] in AUC and sets a new state-of-the-art result.\\r\\n  ToMP         Prob. IoUNet [16]      64.7        65.2   65.0        45.2         Note that even ToMP with ResNet-50 outperforms STARK-\\r\\n  ToMP               ToMP             67.6        66.9   69.0        45.4         ST101 with ResNet-101 (67.6 vs 67.1). Fig. 4 shows the\\r\\nTable 4. Impact of replacing DiMP [1] and the probabilistic                       success AUC gain of ToMP compared to recent Trans-\\r\\nIoUNet [16] with ToMP for localization and box regression.                        former based trackers for different attributes annotated in\\r\\n                                                                                  LaSOT [20]. We want to highlight that ToMP outper-\\r\\nweights and the other to obtain the bounding box regressor                        forms TransT [7] and TrDiMP [59] on each attribute by\\r\\nweights. However, this involves decoupling the query from                         more than one percent point. Similarly, ToMP achieves\\r\\nthe foreground embedding efg and the experiments in Tab. 2                        higher performance than STARK-ST101 for every attribute.\\r\\nshow a significant performance drop for this case.                                It achieves the highest gain over STARK for Background\\r\\nInference Settings: During online tracking, we use the                            Clutter, showing the disadvantage of using small templates\\r\\ninitial frame and its annotation as training frames. In addi-                     instead of training frames with a large field of view that\\r\\ntion, we include the most recent frame and its target predic-                     allow not only to leverage target, but also background in-\\r\\ntion if the classifier confidence is above a certain threshold.                   formation.\\r\\nTab. 3 shows that including previous tracking results leads                       LaSOTExtSub [19]: This dataset is an extension of\\r\\nto higher tracking performance than using only the initial                        LaSOT. It only contains test sequences assigned to 15\\r\\nframe. Disabling the described two stage model prediction                         new classes with 10 videos each. The sequences con-\\r\\napproach and predicting the weights of the target model and                       tain 2500 frames on average showing challenging track-\\r\\nbounding box regressor at once decreases the tracking per-                        ing scenarios of small, fast moving objects with distrac-\\r\\nformance drastically (-5.6 AUC on LaSOT). The reason is\\r\\nthe sensitivity of the bounding box predictor to inaccurate                                              ToMP ToMP STARK Keep STARK Alpha         Siam   Tr Super      STM        Pr\\r\\n                                                                                                          101  50 ST101 Track ST50 Refine TransT R-CNN DiMP DiMP SAOT Track DTT DiMP\\r\\npredicted boxes that are encoded and used for training.                                                              [63] [48] [63]  [64]   [7]    [57] [59] [12] [73] [22] [67] [16]\\r\\n                                                                                       Precision     73.5      72.2   72.2   70.2   71.2   68.0   69.0   68.4   66.3 65.3    -   63.3 - 60.8\\r\\nTransforming Model Prediction Step-by-Step:                Our                         Norm. Prec    79.2      78.0   76.9   77.2   76.3   73.2   73.8   72.2   73.0 72.2   70.8 69.3 - 68.8\\r\\n                                                                                       Success (AUC) 68.5      67.6   67.1   67.1   66.4   65.3   64.9   64.8   63.9 63.1   61.6 60.6 60.1 59.8\\r\\nmodel predictor can estimate model weights for the target\\r\\nmodel and bounding box regressor. In this part, we will                           Table 5. Comparison on the LaSOT [20] test set ordered by AUC.\\r\\ntransform an optimization based tracker step-by-step to as-\\r\\n                                                                                                    7   ToMP-101 vs TransT [4.38]\\r\\nsess the impact of each transformation step. Tab. 4 shows                                               ToMP-101 vs TrDiMP [4.04]\\r\\n                                                                                 Success AUC Gain\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                    6\\r\\nthat replacing the model optimizer in SuperDiMP (1st row)                                           5   ToMP-101 vs STARK-ST101 [1.58]\\r\\nwith our proposed model predictor to only predict the tar-                                          4\\r\\n                                                                                                    3\\r\\nget model (2nd row) outperforms SuperDiMP on three out                                              2\\r\\nof four datasets. Our tracker ToMP that jointly predicts                                            1\\r\\n                                                                                                    0\\r\\nmodel weights for target localization and bounding box re-\\r\\n                                                                                                                             n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                            Ou ion\\r\\n\\r\\n                                                                                                                            w\\r\\n                                                                                                         tio Blur\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                             r\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                             e\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                             e\\r\\n                                                                                                         Fu ation\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                           tio tion\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                               for n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                             n\\r\\n                                                                                                               nd n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                            n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                           oin tion\\r\\n                                                                                                        rtia lutte\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                       Sc hang\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                        ng\\r\\n                                                                                                       Ca clusio\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                           o\\r\\n                                                                                                                         io\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                       tio\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                           o\\r\\n                                                                                                       Lo f-Vie\\r\\n                                                                                                                          i\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                         i\\r\\n                                                                                                                    tat\\r\\n                                                                                                                        t\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                    lut\\r\\n                                                                                                    ckg clus\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                   ha\\r\\n                                                                                                                   Mo\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                   ria\\r\\n\\r\\n                                                                                                                   ma\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                      o\\r\\n                                                                                                  mi otion\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ngression (3rd row) achieves the best performance on all four\\r\\n                                                                                                                  ari\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                   Vie st M\\r\\n                                                                                                                   C\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                eso\\r\\n                                                                                                                 Ro\\r\\n\\r\\n                                                                                                                t-o\\r\\n                                                                                                               nC\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                tC\\r\\n                                                                                                                Va\\r\\n                                                                                                                 c\\r\\n                                                                                                                 c\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                 pe era\\r\\n                                                                                                             nV\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                             lO\\r\\n                                                                                                            ll O\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                          wR\\r\\n\\r\\n                                                                                                            Fa\\r\\n                                                                                                          ale\\r\\n                                                                                                            M\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                          De\\r\\n                                                                                                         rou\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                          m\\r\\n                                                                                                       Ra\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                       wp\\r\\n                                                                                                      na\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                     Pa\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ndatasets. We conclude that predicting the weights of the tar-\\r\\n                                                                                                    ct\\r\\n                                                                                                 Ba\\r\\n                                                                                              Illu\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                             As\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nget model improves the performance and likewise predict-\\r\\ning the weights of the bounding box regressor. Note that we                       Figure 4. Per attribute analysis on LaSOT [20] between ToMP and\\r\\nreport the average over five runs for all trackers based on                       recent Transformer based trackers. The bar heights correspond to\\r\\nthe probabilistic IoUNet due to its stochasticity.                                the gain of our tracker and the legend shows the average gain.\\r\\n\\r\\n\\r\\n                                                                             7\\r\\n\\x0c\\n\\n                        90\\r\\n                                            Success plot                                                  70\\r\\n                                                                                                                                    Success plot                                         ToMP ToMP STARK Super STARK\\r\\n                        80                                                                                                                                                                101  50   ST50 DiMP ST101 DPMT TRAT UPDT DiMP ATOM\\r\\n                                                                                                          60                                                                                         [63] [12, 35] [63] [35] [35] [3, 35] [1, 35] [13, 35]\\r\\n                        70\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                  Overlap Precision [%]\\r\\n                                                                                                          50                                                                   Accuracy 0.453 0.453     0.478.   0.477   0.481   0.492 0.464 0.465   0.457   0.462\\r\\n                        60    ToMP 101 [68.5]\\r\\n                              ToMP 50 [67.6]                                                              40\\r\\n                                                                                                                                                                               Robustness 0.814 0.789   0.799    0.728   0.775   0.745 0.744 0.755   0.734   0.734\\r\\n                        50    KeepTrack [67.1]\\r\\n                              STARK-ST101 [67.1]                                                                      KeepTrack [48.2]                                         EAO        0.309 0.297   0.308    0.305   0.303   0.303 0.280 0.278   0.274   0.271\\r\\n                        40    STARK-ST50 [66.4]                                                           30          ToMP 101 [45.9]\\r\\n                              AlphaRefine [65.9]                                                                      ToMP 50 [45.4]                                           Table 8. Comparison to the state of the art of bounding box only\\r\\n                        30    TransT [64.9]                                                                           Super DiMP [43.7]\\r\\n                              Siam R-CNN [64.8]                                                           20          LTMU [41.4]\\r\\n                        20    TrDiMP [63.9]\\r\\n                              Super DiMP [63.1]\\r\\n                                                                                                                      DiMP [39.2]\\r\\n                                                                                                                      ATOM [37.6]\\r\\n                                                                                                                                                                               methods on VOT2020ST [35] in terms of EAO score.\\r\\n                        10    STMTrack [60.6]                                                             10          DaSiamRPN [35.6]\\r\\n                              PrDiMP50 [59.8]                                                                         SiamRPN++ [34.0]\\r\\n                         00                                                                                00\\r\\n                                 0.2         0.4     0.6\\r\\n                                         Overlap threshold\\r\\n                                                                0.8          1                                           0.2        0.4       0.6\\r\\n                                                                                                                                  Overlap threshold\\r\\n                                                                                                                                                           0.8         1       and challenging sequences with distractors. Both versions\\r\\n                                   (a) LaSOT [20]                                                                     (b) LaSOTExtSub [19]\\r\\n                                                                                                                                                                               of ToMP exceed the performance of the current best method\\r\\n                                                                                                                                                                               KeepTrack [48] by +0.5% and +0.3%, see Tab. 7.\\r\\n  Figure 5. Success plots, showing OPT , on LaSOT [20] and LaSO-                                                                                                               VOT2020 [35]: Finally, we evaluate on the 2020 edition of\\r\\n  TExtSub [19] and AUC is reported in the legend.                                                                                                                              the Visual Object Tracking short-term challenge. We com-\\r\\n                                   ToMP ToMP STARK      STARK Siam Alpha STM          Tr Keep Super Pr Siam                                                                    pare with the top methods in the challenge [35], as well\\r\\n                                    101  50 ST101 TransT ST50 R-CNN Refine Track DTT DiMP Track DiMP DiMP FC++\\r\\n                                               [63] [7]   [63]  [57] [64] [22] [67] [59] [48] [12] [16] [62]                                                                   as more recent methods. The dataset contains 60 videos\\r\\n           Precision\\r\\n           Norm. Prec\\r\\n                         78.9\\r\\n                         86.4\\r\\n                                             78.6\\r\\n                                             86.2\\r\\n                                                     -\\r\\n                                                    86.9\\r\\n                                                             80.3\\r\\n                                                             86.7\\r\\n                                                                       -\\r\\n                                                                      86.1\\r\\n                                                                                 80.0\\r\\n                                                                                 85.4\\r\\n                                                                                                               78.3\\r\\n                                                                                                               85.6\\r\\n                                                                                                                       76.7 78.9 73.1 73.8 73.3 70.4 70.5\\r\\n                                                                                                                       85.1 85.0 83.3 83.5 83.5 81.6 80.0\\r\\n                                                                                                                                                                               annotated with segmentation masks. Since ToMP produces\\r\\n           Success (AUC) 81.5                81.2   82.0     81.4     81.3       81.2                          80.5    80.3 79.6 78.4 78.1 78.1 75.8 75.4\\r\\n                                                                                                                                                                               bounding boxes we only compare with trackers that produce\\r\\n                              Table 6. Comparison on the TrackingNet [50] test set.                                                                                            the bounding boxes as well. The trackers are evaluated fol-\\r\\n                              ToMP ToMP Keep\\r\\n                               101\\r\\n                                                   STARK              STARK Super Pr STM Siam Siam\\r\\n                                    50 Track CRACT ST101 TrDiMP TransT ST50 DiMP DiMP Track AttN R-CNN KYS DiMP\\r\\n                                                                                                                                                                               lowing the multi-start protocol and are ranked according to\\r\\n                                        [48] [21]    [63]  [59]   [7]   [63] [12] [16] [22] [68] [57] [2] [1]                                                                  the EAO metric that is based on tracking accuracy and ro-\\r\\n          UAV123 66.9                69.0    69.7   66.4     68.2     67.5       69.1                          69.1    67.7 68.0 64.7 65.0          64.9    \\xe2\\x80\\x93 65.3\\r\\n          OTB-100 70.1               70.1    70.9   72.6     68.1     71.1       69.4                          68.5    70.1 69.6 71.9 71.2          70.1   69.5 68.4           bustness, defined using IoU overlap and failure rate respec-\\r\\n          NFS     66.7               66.9    66.4   62.5     66.2     66.2       65.7                          65.2    64.8 63.5  \\xe2\\x80\\x93    \\xe2\\x80\\x93            63.9   63.5 62.0\\r\\n                                                                                                                                                                               tively. The results in Tab. 8 show that ToMP-101 achieves\\r\\n  Table 7. Comparison with the state of the art on the OTB-100 [61],\\r\\n                                                                                                                                                                               the best overall performance, with the highest robustness\\r\\n  NFS [23] and UAV123 [49] datasets in terms of AUC score.\\r\\n                                                                                                                                                                               and competitive accuracy compared to previous methods.\\r\\n  tors present. Fig. 5b shows the success plot where the re-\\r\\n  sults of most trackers are obtained from [19], e.g., DaSi-\\r\\n                                                                                                                                                                               4.3. Limitations\\r\\n  amRPN [74], SiamRPN++ [39], ATOM [13], DiMP [1] and                                                                                                                             Transformer Encoders consist of self-attention layers\\r\\n  LTMU [11]. ToMP exceeds the performance of all track-                                                                                                                        that compute similarity matrices between multiple training\\r\\n  ers except KeepTrack [48] that employs explicit distractor                                                                                                                   and test frame features and thus lead to a large memory foot-\\r\\n  matching between frames. In particular, we outperform Su-                                                                                                                    print that impacts training and inference run-time. Thus, in\\r\\n  perDiMP [12] that uses a model optimizer (+2.2%).                                                                                                                            future work this limitation should be addressed by evalu-\\r\\n  TrackingNet [50]: We evaluate ToMP on the large-scale                                                                                                                        ating alternatives such as [32, 34, 53] aiming at decreasing\\r\\n  TrackingNet dataset that contains 511 test sequences with-                                                                                                                   the memory burden. Another limiting factor of ToMP arises\\r\\n  out publicly available ground-truth. An online evaluation                                                                                                                    from challenging tracking sequences. In particular, distrac-\\r\\n  server is used to obtain the tracking metrics shown in Tab. 6                                                                                                                tors present while the target is occluded is a typical failure\\r\\n  by submitting the raw tracking results. Both versions of                                                                                                                     scenario of ToMP, since it is lacking explicit distractor han-\\r\\n  ToMP achieve competitive results close to the current state                                                                                                                  dling as in KeepTrack [48].\\r\\n  of the art. In particular, ToMP-101 achieves the second\\r\\n  best performance in terms of AUC behind STARK [63],                                                                                                                          5. Conclusion\\r\\n  outperforming other Transformer based trackers such as\\r\\n                                                                                                                                                                                  We propose a novel tracking architecture employing a\\r\\n  TransT [7] and TrDiMP [59].\\r\\n                                                                                                                                                                               Transformer-based model predictor. The model predictor\\r\\n  UAV123 [49]: The UAV dataset consists of 123 test videos                                                                                                                     estimates the weights of the compact DCF target model to\\r\\n  that contains small objects, target occlusion, and distractors.                                                                                                              localize the target in the test frame. In addition, the pre-\\r\\n  Tab. 7 shows the achieved results in terms of success AUC.                                                                                                                   dictor produces a second set of weights used for precise\\r\\n  Again, ToMP achieves competitive results compared to the                                                                                                                     bounding box regression. To achieve this, we develop two\\r\\n  current state of the art achieved by KeepTrack [48].                                                                                                                         new modules that encode target location and its bounding\\r\\n  OTB-100 [61]: We also report results on the OTB-100                                                                                                                          box in the training features. We conduct comprehensive ex-\\r\\n  dataset that contains 100 short sequences. Multiple trackers                                                                                                                 perimental validation and analysis of ToMP on several chal-\\r\\n  achieve results above 70% AUC. Among them are both ver-                                                                                                                      lenging datasets, and set a new state of the art on three.\\r\\n  sions of ToMP, see Tab. 7. ToMP achieve the same perfor-                                                                                                                     Acknowledgments: This work was partly supported by the\\r\\n  mance as SuperDiMP [12] but slightly higher results than                                                                                                                     ETH Zu\\xcc\\x88rich Fund (OK), Siemens Smart Infrastructure, the\\r\\n  TransT [7] and slightly lower than TrDiMP [59].                                                                                                                              ETH Future Computing Laboratory (EFCL) financed by a\\r\\n  NFS [23]: We compete on the NFS dataset (30FPS ver-                                                                                                                          gift from Huawei Technologies, an Amazon AWS grant,\\r\\n  sion) containing 100 test videos. It contains fast motions                                                                                                                   and an Nvidia hardware grant.\\r\\n\\r\\n\\r\\n                                                                                                                                                                           8\\r\\n\\x0c\\n\\nReferences                                                              [13] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\\r\\n                                                                             Michael Felsberg. ATOM: Accurate tracking by overlap\\r\\n [1] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu                   maximization. In Proceedings of the IEEE/CVF Conference\\r\\n     Timofte. Learning discriminative model prediction for track-            on Computer Vision and Pattern Recognition (CVPR), June\\r\\n     ing. In Proceedings of the IEEE/CVF International Confer-               2019. 1, 2, 5, 8, 16\\r\\n     ence on Computer Vision (ICCV), October 2019. 1, 2, 5, 7,          [14] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\\r\\n     8, 13, 15, 16                                                           Michael Felsberg. ECO: efficient convolution operators for\\r\\n [2] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu                   tracking. In Proceedings of the IEEE Conference on Com-\\r\\n     Timofte. Know your surroundings: Exploiting scene infor-                puter Vision and Pattern Recognition (CVPR), June 2017. 1,\\r\\n     mation for object tracking. In Proceedings of the European              16\\r\\n     Conference on Computer Vision (ECCV), August 2020. 8,              [15] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan,\\r\\n     16                                                                      and Michael Felsberg. Beyond correlation filters: Learning\\r\\n [3] Goutam Bhat, Joakim Johnander, Martin Danelljan, Fa-                    continuous convolution operators for visual tracking. In Pro-\\r\\n     had Shahbaz Khan, and Michael Felsberg. Unveiling the                   ceedings of the European Conference on Computer Vision\\r\\n     power of deep tracking. In Proceedings of the European                  (ECCV), October 2016. 2, 16\\r\\n     Conference on Computer Vision (ECCV), September 2018.              [16] Martin Danelljan, Luc Van Gool, and Radu Timofte. Prob-\\r\\n     8, 16                                                                   abilistic regression for visual tracking. In Proceedings of\\r\\n [4] Goutam Bhat, Felix Ja\\xcc\\x88remo Lawin, Martin Danelljan, An-                 the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     dreas Robinson, Michael Felsberg, Luc Van Gool, and Radu                Recognition (CVPR), June 2020. 2, 5, 7, 8, 16, 17\\r\\n     Timofte. Learning what to learn for video object segmenta-         [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\r\\n     tion. In Proceedings of the European Conference on Com-                 and Li Fei-Fei. Imagenet: A large-scale hierarchical image\\r\\n     puter Vision ECCV, August 2020. 15                                      database. In Proceedings of the IEEE/CVF Conference on\\r\\n [5] David S. Bolme, J. Ross Beveridge, Bruce A. Draper, and                 Computer Vision and Pattern Recognition (CVPR), 2009. 13\\r\\n     Yui Man Lui. Visual object tracking using adaptive correla-        [18] Xingping Dong, Jianbing Shen, Ling Shao, and Fatih Porikli.\\r\\n     tion filters. In CVPR, 2010. 1, 2                                       Clnet: A compact latent network for fast adjusting siamese\\r\\n [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas              trackers. In Proceedings of the European Conference on\\r\\n     Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-                 Computer Vision (ECCV), August 2020. 16\\r\\n     to-end object detection with transformers. In Proceedings          [19] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge\\r\\n     of the European Conference on Computer Vision (ECCV),                   Deng, Sijia Yu, Mingzhen Huang, Juehuan Liu, Yong Xu,\\r\\n     pages 213\\xe2\\x80\\x93229, August 2020. 2, 4, 13                                    et al. Lasot: A high-quality large-scale single object track-\\r\\n                                                                             ing benchmark. International Journal of Computer Vision\\r\\n [7] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang,\\r\\n                                                                             (IJCV), 129(2):439\\xe2\\x80\\x93461, 2021. 7, 8, 17, 18\\r\\n     and Huchuan Lu. Transformer tracking. In Proceedings of\\r\\n     the IEEE/CVF Conference on Computer Vision and Pattern             [20] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\\r\\n     Recognition (CVPR), June 2021. 1, 2, 5, 7, 8, 16, 17                    Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\\r\\n                                                                             Lasot: A high-quality benchmark for large-scale single ob-\\r\\n [8] Yiwei Chen, Jingtao Xu, Jiaqian Yu, Qiang Wang, Byungin\\r\\n                                                                             ject tracking. In Proceedings of the IEEE/CVF Conference\\r\\n     Yoo, and Jae Joon Han. AFOD: Adaptive focused discrimi-\\r\\n                                                                             on Computer Vision and Pattern Recognition (CVPR), June\\r\\n     native segmentation tracker. In Proceedings of the European\\r\\n                                                                             2019. 1, 2, 6, 7, 8, 13, 14, 15, 16, 17, 18\\r\\n     Conference on Computer Vision Workshops (ECCVW), Au-\\r\\n                                                                        [21] Heng Fan and Haibin Ling. Cract: Cascaded regression-\\r\\n     gust 2020. 15\\r\\n                                                                             align-classification for robust visual tracking. arXiv preprint\\r\\n [9] Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang,                   arXiv:2011.12483, 2020. 8, 16\\r\\n     and Rongrong Ji. Siamese box adaptive network for vi-              [22] Zhihong Fu, Qingjie Liu, Zehua Fu, and Yunhong Wang.\\r\\n     sual tracking. In Proceedings of the IEEE/CVF Conference                Stmtrack: Template-free visual tracking with space-time\\r\\n     on Computer Vision and Pattern Recognition (CVPR), June                 memory networks. In Proceedings of the IEEE/CVF Confer-\\r\\n     2020. 16                                                                ence on Computer Vision and Pattern Recognition (CVPR),\\r\\n[10] Janghoon Choi, Junseok Kwon, and Kyoung Mu Lee. Visual                  June 2021. 5, 7, 8, 16\\r\\n     tracking by tridentalign and context embedding. In Proceed-        [23] Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva\\r\\n     ings of the Asian Conference on Computer Vision (ACCV),                 Ramanan, and Simon Lucey. Need for speed: A benchmark\\r\\n     November 2020. 16                                                       for higher frame rate object tracking. In ICCV, 2017. 1, 8,\\r\\n[11] Kenan Dai, Yunhua Zhang, Dong Wang, Jianhua Li,                         13, 16, 17\\r\\n     Huchuan Lu, and Xiaoyun Yang. High-performance long-               [24] Hamed Kiani Galoogahi, Ashton Fagg, and Simon Lucey.\\r\\n     term tracking with meta-updater. In Proceedings of the                  Learning background-aware correlation filters for visual\\r\\n     IEEE/CVF Conference on Computer Vision and Pattern                      tracking. In Proceedings of the IEEE/CVF International\\r\\n     Recognition (CVPR), June 2020. 8, 16                                    Conference on Computer Vision (ICCV), October 2017. 1\\r\\n[12] Martin Danelljan and Goutam Bhat. PyTracking: Vi-                  [25] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. Graph\\r\\n     sual tracking library based on PyTorch. https : / /                     convolutional tracking. In Proceedings of the IEEE/CVF\\r\\n     github.com/visionml/pytracking, 2019. Ac-                               Conference on Computer Vision and Pattern Recognition\\r\\n     cessed: 1/05/2021. 1, 6, 7, 8, 13, 14, 15, 16, 18                       (CVPR), June 2019. 16\\r\\n\\r\\n\\r\\n                                                                    9\\r\\n\\x0c\\n\\n[26] Dongyan Guo, Yanyan Shao, Ying Cui, Zhenhua Wang,                          Abdelrahman Eldesokey, Jani Ka\\xcc\\x88pyla\\xcc\\x88, Gustavo Ferna\\xcc\\x81ndez,\\r\\n     Liyan Zhang, and Chunhua Shen. Graph attention tracking.                   and et al. The seventh visual object tracking vot2019 chal-\\r\\n     In Proceedings of the IEEE/CVF Conference on Computer                      lenge results. In Proceedings of the IEEE/CVF International\\r\\n     Vision and Pattern Recognition (CVPR), June 2021. 16                       Conference on Computer Vision Workshop (ICCVW), Octo-\\r\\n[27] Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, and                         ber 2019. 16\\r\\n     Shengyong Chen. Siamcar: Siamese fully convolutional                  [38] Matej Kristan, Jir\\xcc\\x8c\\xc4\\xb1\\xcc\\x81 Matas, Ales\\xcc\\x8c Leonardis, Michael Felsberg,\\r\\n     classification and regression for visual tracking. In Proceed-             Roman Pflugfelder, Joni-Kristian Ka\\xcc\\x88ma\\xcc\\x88ra\\xcc\\x88inen, Hyung Jin\\r\\n     ings of the IEEE/CVF Conference on Computer Vision and                     Chang, Martin Danelljan, Luka Cehovin, Alan Lukez\\xcc\\x8cic\\xcc\\x8c, On-\\r\\n     Pattern Recognition (CVPR), June 2020. 16                                  drej Drbohlav, Jani Ka\\xcc\\x88pyla\\xcc\\x88, Gustav Ha\\xcc\\x88ger, Song Yan, Jinyu\\r\\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.                     Yang, Zhongqun Zhang, and Gustavo Ferna\\xcc\\x81ndez. The ninth\\r\\n     Deep residual learning for image recognition. In Proceed-                  visual object tracking vot2021 challenge results. In Proceed-\\r\\n     ings of the IEEE/CVF Conference on Computer Vision and                     ings of the IEEE/CVF International Conference on Com-\\r\\n     Pattern Recognition (CVPR), June 2016. 1, 6, 15                            puter Vision (ICCV) Workshops, pages 2711\\xe2\\x80\\x932738, October\\r\\n[29] Joa\\xcc\\x83o F. Henriques, Rui Caseiro, Pedro Martins, and Jorge                  2021. 16\\r\\n     Batista. High-speed tracking with kernelized correlation fil-         [39] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing,\\r\\n     ters. IEEE Transactions on Pattern Analysis and Machine                    and Junjie Yan. Siamrpn++: Evolution of siamese vi-\\r\\n     Intelligence (TPAMI), 37(3):583\\xe2\\x80\\x93596, 2015. 1, 2                            sual tracking with very deep networks. In Proceedings of\\r\\n[30] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Globaltrack:                    the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     A simple and strong baseline for long-term tracking. In Pro-               Recognition (CVPR), June 2019. 8, 16\\r\\n     ceedings of the Conference on Artificial Intelligence (AAAI),         [40] Siyuan Li, Zhi Zhang, Ziyu Liu, Anna Wang, Linglong Qiu,\\r\\n     February 2020. 16                                                          and Feng Du. Tlpg-tracker: Joint learning of target local-\\r\\n[31] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A                      ization and proposal generation for visual tracking. In Pro-\\r\\n     large high-diversity benchmark for generic object tracking                 ceedings of the International Joint Conference on Artificial\\r\\n     in the wild. IEEE Transactions on Pattern Analysis and Ma-                 Intelligence, IJCAI, July 2020. 16\\r\\n     chine Intelligence (TPAMI), 43(5):1562\\xe2\\x80\\x931577, 2021. 6                  [41] Yiming Li, Changhong Fu, Fangqiang Ding, Ziyuan Huang,\\r\\n[32] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and                   and Geng Lu. Autotrack: Towards high-performance visual\\r\\n     Franc\\xcc\\xa7ois Fleuret. Transformers are RNNs: Fast autoregres-                 tracking for uav with automatic spatio-temporal regulariza-\\r\\n     sive transformers with linear attention. In Proceedings of                 tion. In Proceedings of the IEEE/CVF Conference on Com-\\r\\n     the International Conference on Machine Learning (ICML),                   puter Vision and Pattern Recognition (CVPR), June 2020. 16\\r\\n     pages 5156\\xe2\\x80\\x935165, July 2020. 8                                         [42] Bingyan Liao, Chenye Wang, Yayun Wang, Yaonong Wang,\\r\\n[33] Dai Kenan, Wang Dong, Lu Huchuan, Sun Chong, and Li                        and Jun Yin. Pg-net: Pixel to global matching network for\\r\\n     Jianhua. Visual tracking via adaptive spatially-regularized                visual tracking. In Proceedings of the European Conference\\r\\n     correlation filters. In Proceedings of the IEEE/CVF Confer-                on Computer Vision (ECCV), August 2020. 16\\r\\n     ence on Computer Vision and Pattern Recognition (CVPR),               [43] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.\\r\\n     2019. 2                                                                    Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva\\r\\n[34] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-                     Ramanan, Piotr Dolla\\xcc\\x81r, and C. Lawrence Zitnick. Microsoft\\r\\n     former: The efficient transformer. In Proceedings of the In-               COCO: common objects in context. In Proceedings of the\\r\\n     ternational Conference on Learning Representations (ICLR),                 European Conference on Computer Vision (ECCV), 2014. 6\\r\\n     2020. 8                                                               [44] Yuan Liu, Ruoteng Li, Yu Cheng, Robby T. Tan, and Xi-\\r\\n[35] Matej Kristan, Ales\\xcc\\x8c Leonardis, Jir\\xcc\\x8c\\xc4\\xb1\\xcc\\x81 Matas, Michael Fels-                ubao Sui. Object tracking using spatio-temporal networks\\r\\n     berg, Roman Pflugfelder, Joni-Kristian Ka\\xcc\\x88ma\\xcc\\x88ra\\xcc\\x88inen, Martin               for future prediction location. In Proceedings of the Euro-\\r\\n     Danelljan, Luka C\\xcc\\x8cehovin Zajc, Alan Lukez\\xcc\\x8cic\\xcc\\x8c, Ondrej Dr-                  pean Conference on Computer Vision (ECCV), August 2020.\\r\\n     bohlav, Linbo He, Yushan Zhang, Song Yan, Jinyu Yang,                      16\\r\\n     Gustavo Ferna\\xcc\\x81ndez, and et al. The eighth visual object track-        [45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\r\\n     ing vot2020 challenge results. In Proceedings of the Euro-                 regularization. In Proceedings of the International Confer-\\r\\n     pean Conference on Computer Vision Workshops (ECCVW),                      ence on Learning Representations (ICLR), 2019. 6\\r\\n     August 2020. 8, 15, 16                                                [46] Alan Lukezic, Toma\\xcc\\x81s Voj\\xc4\\xb1\\xcc\\x81r, Luka Cehovin Zajc, Jiri Matas,\\r\\n[36] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-                   and Matej Kristan. Discriminative correlation filter tracker\\r\\n     berg, Roman Pfugfelder, Luka Cehovin Zajc, Tomas Vojir,                    with channel and spatial reliability. International Journal of\\r\\n     Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey, Gus-                     Computer Vision (IJCV), 126(7):671\\xe2\\x80\\x93688, 2018. 1, 2\\r\\n     tavo Fernandez, and et al. The sixth visual object track-             [47] Ziang Ma, Linyuan Wang, Haitao Zhang, Wei Lu, and Jun\\r\\n     ing vot2018 challenge results. In Proceedings of the Euro-                 Yin. RPT: learning point set representation for siamese vi-\\r\\n     pean Conference on Computer Vision Workshops (ECCVW),                      sual tracking. In Proceedings of the European Conference on\\r\\n     September 2018. 16                                                         Computer Vision Workshops (ECCVW), August 2020. 15, 16\\r\\n[37] Matej Kristan, Jir\\xc4\\xb1\\xcc\\x81 Matas, Ales\\xcc\\x8c Leonardis, Michael Felsberg,        [48] Christoph Mayer, Martin Danelljan, Danda Pani Paudel, and\\r\\n     Roman Pflugfelder, Joni-Kristian Ka\\xcc\\x88ma\\xcc\\x88ra\\xcc\\x88inen, Luka Ce-                   Luc Van Gool. Learning target candidate association to keep\\r\\n     hovin Zajc, Ondrej Drbohlav, Alan Lukezic, Amanda Berg,                    track of what not to track. In Proceedings of the IEEE/CVF\\r\\n\\r\\n\\r\\n                                                                      10\\r\\n\\x0c\\n\\n     International Conference on Computer Vision (ICCV), pages             [61] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-\\r\\n     13444\\xe2\\x80\\x9313454, October 2021. 7, 8, 16, 17, 18                                ing benchmark. IEEE Transactions on Pattern Analysis and\\r\\n[49] Matthias Mueller, Neil Smith, and Bernard Ghanem. A                        Machine Intelligence (TPAMI), 37(9):1834\\xe2\\x80\\x931848, 2015. 8,\\r\\n     benchmark and simulator for uav tracking. In Proceedings                   13, 16, 17\\r\\n     of the European Conference on Computer Vision (ECCV),                 [62] Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and Gang Yu.\\r\\n     October 2016. 8, 16, 17                                                    Siamfc++: Towards robust and accurate visual tracking with\\r\\n[50] Matthias Mu\\xcc\\x88ller, Adel Bibi, Silvio Giancola, Salman Al-                   target estimation guidelines. In Proceedings of the Confer-\\r\\n     Subaihi, and Bernard Ghanem. Trackingnet: A large-scale                    ence on Artificial Intelligence (AAAI), February 2020. 5, 8,\\r\\n     dataset and benchmark for object tracking in the wild. In                  16\\r\\n     Proceedings of the European Conference on Computer Vi-                [63] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and\\r\\n     sion (ECCV), 2018. 6, 8                                                    Huchuan Lu. Learning spatio-temporal transformer for\\r\\n[51] Hyeonseob Nam and Bohyung Han. Learning multi-domain                       visual tracking. In Proceedings of the IEEE/CVF Inter-\\r\\n     convolutional neural networks for visual tracking. In Pro-                 national Conference on Computer Vision (ICCV), pages\\r\\n     ceedings of the IEEE/CVF Conference on Computer Vision                     10448\\xe2\\x80\\x9310457, October 2021. 1, 2, 4, 5, 7, 8, 14, 15, 16,\\r\\n     and Pattern Recognition (CVPR), June 2016. 16                              18\\r\\n[52] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir                   [64] Bin Yan, Xinyu Zhang, Dong Wang, Huchuan Lu, and Xi-\\r\\n     Sadeghian, Ian Reid, and Silvio Savarese. Generalized in-                  aoyun Yang. Alpha-refine: Boosting tracking performance\\r\\n     tersection over union: A metric and a loss for bounding box                by precise bounding box estimation. In Proceedings of\\r\\n     regression. In Proceedings of the IEEE/CVF Conference                      the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n     on Computer Vision and Pattern Recognition (CVPR), June                    Recognition (CVPR), June 2021. 7, 8, 15, 16, 18\\r\\n     2019. 5                                                               [65] Bin Yan, Haojie Zhao, Dong Wang, Huchuan Lu, and Xi-\\r\\n[53] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and                    aoyun Yang. \\xe2\\x80\\x99skimming-perusal\\xe2\\x80\\x99 tracking: A framework for\\r\\n     Hongsheng Li. Efficient attention: Attention with linear                   real-time and robust long-term tracking. In Proceedings of\\r\\n     complexities. In Proceedings of the IEEE/CVF Winter Con-                   the IEEE/CVF International Conference on Computer Vision\\r\\n     ference on Applications of Computer Vision (WACV), pages                   (ICCV), October 2019. 16\\r\\n     3531\\xe2\\x80\\x933539, January 2021. 8\\r\\n                                                                           [66] Tianyu Yang, Pengfei Xu, Runbo Hu, Hua Chai, and An-\\r\\n[54] Chong Sun, Dong Wang, Huchuan Lu, and Ming-Hsuan\\r\\n                                                                                toni B. Chan. Roam: Recurrently optimizing tracking model.\\r\\n     Yang. Correlation tracking via joint discrimination and relia-\\r\\n                                                                                In Proceedings of the IEEE/CVF Conference on Computer\\r\\n     bility learning. In Proceedings of the IEEE/CVF Conference\\r\\n                                                                                Vision and Pattern Recognition (CVPR), June 2020. 16\\r\\n     on Computer Vision and Pattern Recognition (CVPR), 2018.\\r\\n                                                                           [67] Bin Yu, Ming Tang, Linyu Zheng, Guibo Zhu, Jinqiao Wang,\\r\\n     1, 2\\r\\n                                                                                Hao Feng, Xuetao Feng, and Hanqing Lu. High-performance\\r\\n[55] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\r\\n                                                                                discriminative tracking with transformers. In Proceedings of\\r\\n     Fully convolutional one-stage object detection. In Proceed-\\r\\n                                                                                the IEEE/CVF International Conference on Computer Vision\\r\\n     ings of the IEEE/CVF International Conference on Com-\\r\\n                                                                                (ICCV), pages 9856\\xe2\\x80\\x939865, October 2021. 1, 2, 5, 7, 8, 16\\r\\n     puter Vision (ICCV), October 2019. 5, 14\\r\\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-               [68] Yuechen Yu, Yilei Xiong, Weilin Huang, and Matthew R.\\r\\n     reit, Llion Jones, Aidan N Gomez, \\xc5\\x81 ukasz Kaiser, and Illia                Scott. Deformable siamese attention networks for visual ob-\\r\\n     Polosukhin. Attention is all you need. In Advances in Neural               ject tracking. In Proceedings of the IEEE/CVF Conference\\r\\n     Information Processing Systems (NeurIPS), 2017. 4                          on Computer Vision and Pattern Recognition (CVPR), June\\r\\n[57] Paul Voigtlaender, Jonathon Luiten, Philip H.S. Torr, and                  2020. 8, 16\\r\\n     Bastian Leibe.       Siam R-CNN: Visual tracking by re-               [69] Zhipeng Zhang, Yihao Liu, Xiao Wang, Bing Li, and Weim-\\r\\n     detection. In IEEE/CVF Conference on Computer Vision and                   ing Hu. Learn to match: Automatic matching network de-\\r\\n     Pattern Recognition (CVPR), June 2020. 7, 8, 16                            sign for visual tracking. In Proceedings of the IEEE/CVF\\r\\n[58] Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong,                      International Conference on Computer Vision (ICCV), pages\\r\\n     and Wenjun Zeng. Tracking by instance detection: A meta-                   13339\\xe2\\x80\\x9313348, October 2021. 16\\r\\n     learning approach. In Proceedings of the IEEE/CVF Confer-             [70] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and\\r\\n     ence on Computer Vision and Pattern Recognition (CVPR),                    Weiming Hu. Ocean: Object-aware anchor-free tracking. In\\r\\n     June 2020. 2, 16                                                           Proceedings of the European Conference on Computer Vi-\\r\\n[59] Ning Wang, Wengang Zhou, Jie Wang, and Houqiang Li.                        sion (ECCV), August 2020. 16\\r\\n     Transformer meets tracker: Exploiting temporal context for            [71] Zikai Zhang, Bineng Zhong, Shengping Zhang, Zhenjun\\r\\n     robust visual tracking. In Proceedings of the IEEE/CVF                     Tang, Xin Liu, and Zhaoxiang Zhang. Distractor-aware fast\\r\\n     Conference on Computer Vision and Pattern Recognition                      tracking via dynamic convolutions and mot philosophy. In\\r\\n     (CVPR), June 2021. 1, 2, 7, 8, 16, 18                                      Proceedings of the IEEE/CVF Conference on Computer Vi-\\r\\n[60] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and                     sion and Pattern Recognition (CVPR), June 2021. 16\\r\\n     Philip H.S. Torr. Fast online object tracking and segmenta-           [72] Linyu Zheng, Ming Tang, Yingying Chen, Jinqiao Wang, and\\r\\n     tion: A unifying approach. In Proceedings of the IEEE/CVF                  Hanqing Lu. Learning feature embeddings for discriminant\\r\\n     Conference on Computer Vision and Pattern Recognition                      model based tracking. In Proceedings of the European Con-\\r\\n     (CVPR), June 2019. 16                                                      ference on Computer Vision (ECCV), August 2020. 2, 16\\r\\n\\r\\n\\r\\n                                                                      11\\r\\n\\x0c\\n\\n[73] Zikun Zhou, Wenjie Pei, Xin Li, Hongpeng Wang, Feng\\r\\n     Zheng, and Zhenyu He. Saliency-associated object track-\\r\\n     ing. In Proceedings of the IEEE/CVF International Confer-\\r\\n     ence on Computer Vision (ICCV), pages 9866\\xe2\\x80\\x939875, October\\r\\n     2021. 7, 16\\r\\n[74] Zheng Zhu, Qiang Wang, Li Bo, Wei Wu, Junjie Yan, and\\r\\n     Weiming Hu. Distractor-aware siamese networks for visual\\r\\n     object tracking. In Proceedings of the European Conference\\r\\n     on Computer Vision (ECCV), September 2018. 8, 16\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                  12\\r\\n\\x0c\\n\\nAppendices                                                                  the model predictor we extract features with a stride of 16\\r\\n                                                                            from the third block of the ResNet that we use as back-\\r\\n                                                                            bone. We initialize the backbone with the official weights\\r\\nIn this supplementary material, we first provide details\\r\\n                                                                            obtained by training the backbone on ImageNet [17] and\\r\\nabout training, model architecture and inference in Sec. A.\\r\\n                                                                            freeze the batch norm statistics during training. Since we\\r\\nFurther, we report visual results such as a comparison to\\r\\n                                                                            use a channel dimension of 256 for the Transformer and\\r\\nstate-of-the-art trackers, a comparison of different model\\r\\n                                                                            the ResNet features have 1024 channels we employ an sin-\\r\\npredictors and failure cases of our tracker in Sec. B. After-\\r\\n                                                                            gle convolutional layer to decrease the number of chan-\\r\\nwards, we provide more detailed results of the experiments\\r\\n                                                                            nels before feeding the features into the Transformer En-\\r\\nshown in the main paper in Sec. C.\\r\\n                                                                            coder. The Transformer Encoder consists of layers contain-\\r\\n                                                                            ing multi-headed self attention and a feed-forward network.\\r\\nA. Training, Architecture and Inference                                     We use eight heads and a hidden dimension of 2048 for\\r\\n   First, we provide additional details about the training                  the feed-forward network. Furthermore, we use Dropout\\r\\nfollowed by a detailed description of the architectures em-                 with probability 0.1 and layer normalization. The Trans-\\r\\nployed and finally we provide further inference details.                    former settings are adopted from DETR [6]. The predicted\\r\\n                                                                            target model weights for classification and bounding box\\r\\nA.1. Training and Architecture Details                                      regression consist of a single 1 \\xc3\\x97 1 filter with 256 chan-\\r\\n   For training we produce the target states y by using a                   nels. The bounding box regression CNN consists of four\\r\\nGaussian with standard deviation 1/4 relative to the base                   convolution-instance-normalization-ReLU layers and a fi-\\r\\ntarget size and by settting \\xcf\\x84 = 0.05 to differentiate be-                   nal convolution layer, followed by an exponential activa-\\r\\ntween foreground and background regions in the corre-                       tion. The MLP for target extent encoding \\xcf\\x86 consists of three\\r\\nsponding classification loss lcls adopted from DiMP [1]. For                layers (4 \\xe2\\x86\\x92 64 \\xe2\\x86\\x92 256 \\xe2\\x86\\x92 256) where each layer consists\\r\\n                                                                            of a linear projection, batch normalization and ReLU activa-\\r\\n                                                                            tion except the last that only consist of a linear projection.\\r\\n     Video Frame               SuperDiMP           ToMP-101                 The region-encoding tokens efg and etest are 256 dimen-\\r\\n                     #0037                                                  sional learnable embeddings.\\r\\n\\r\\n                                                                            A.2. Inference Details\\r\\n                                                                               In order to decide whether a previous tracking result\\r\\n                                                                            should be used for training of not we use the maximal value\\r\\n                                                                            of the target score map produced by the target model. In\\r\\n                     #0051\\r\\n                                                                            particular, we select the sample if its confidence value is\\r\\n                                                                            above a certain threshold \\xce\\xb7. Tab. 9 shows that the chosen\\r\\n                                                                            threshold of 0.9 leads to high performance on LaSOT [20],\\r\\n                                                                            NFS [23] and OTB-100 [61]. Furthermore, we follow Su-\\r\\n                                                                            perDiMP [12] and enter in the target not found state if the\\r\\n                                                                            maximal value of the target score map is bellow 0.25. More-\\r\\n                     #0162\\r\\n\\r\\n\\r\\n\\r\\n                                                                             training frames              NFS    OTB       UAV LaSOT LaSOTExtSub          Speed [FPS]\\r\\n                                                                             1 initial                    65.3   67.8      68.7   65.7          43.7          26.2\\r\\n                                                                             1 initial + 1 recent         66.9   70.1      69.0   67.6          45.4          24.8\\r\\n                                                                             2 initial + 1 recent         67.6   70.5      67.2   68.0          45.4          20.5\\r\\n                                                                             1 initial + 2 recent         66.7   70.8      69.4   67.6          44.4          21.8\\r\\n                                                                             1 initial + 3 recent         66.8   70.5      69.2   67.6          44.2          17.6\\r\\nFigure 6. Visual comparison of the target score maps resulting               1 initial + 4 recent         67.2   70.1      68.2   67.3          44.7          13.2\\r\\n                                                                             1 initial + 5 recent         66.8   70.1      69.1   67.2          43.9          11.3\\r\\nfrom different model predictors.\\r\\n                                                                            Table 10. Comparison of different number of training samples in\\r\\n   Two Stage          Previous     Confidence                               success AUC.\\r\\n Model Prediction Tracking Results Threshold \\xce\\xb7   LaSOT NFS OTB\\r\\n       X                X             0.85       67.3   66.9 70.3                                            Lcenterness   NFS    OTB    UAV      LaSOT   LaSOTExtSub\\r\\n       X                X             0.90       67.6   66.9 70.1           Classification                       7         66.9   70.1   69.0      67.6      45.4\\r\\n       X                X             0.95       67.4   66.0 69.8           Classification                       X         65.8   69.2   67.3      67.9      45.5\\r\\n                                                                            Centerness                           X         62.7   66.3   67.4      64.4      41.3\\r\\n                                                                            Classification \\xc2\\xb7 Centerness          X         63.7   67.8   68.7      65.8      45.3\\r\\nTable 9. Analysis of different inference settings an of their impact\\r\\non the tracking performance in terms of AUC of the success curve.           Table 11. Impact of centerness scores on training and inference.\\r\\n\\r\\n\\r\\n                                                                       13\\r\\n\\x0c\\n\\n                #0200                #0784                #1660                       #0002                 #0412                       #0746\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                #0090                #1136                #1407                       #0006                 #0362                       #1343\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                #0001                #1522                #4919                       #0010                 #1263                       #1853\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                #0030                #0686                #1194                       #0003                 #0616                       #1223\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      Annotation                             SuperDiMP                           STARK-ST101                                ToMP-101\\r\\n\\r\\nFigure 7. Visual comparison of different trackers (ToMP-101, SuperDiMP [12] and STARK-ST101 [63]) on different LaSOT [20] se-\\r\\nquences.\\r\\n\\r\\n\\r\\nover, we use the same spatial resolution of the target scores          therefore needed to identify the center location of the ob-\\r\\nof 18 \\xc3\\x97 18 and the same search area scale factor of 5.0 dur-           ject, used to select the bounding box offsets. In contrast,\\r\\ning inference and training.                                            our classification branch is directly trained to accurately lo-\\r\\n    Furthermore, we study the effect of using more than two            cate the object\\xe2\\x80\\x99s center. The additional centerness branch is\\r\\ntraining frames stored in the sample memory. Instead of                therefore redundant. Nonetheless, we train our best model\\r\\nusing only one initial and one recent training frame to pre-           with a centerness head and Lcenterness and report the results\\r\\ndict the network weights we test the impact of increasing              in Tab. 11 (2nd -4th rows). The 1st row shows the perfor-\\r\\nthe number of recent training frames and of using multiple             mance when omitting centerness for training. We achieve\\r\\ninitial training frames. We increase the number of initial             comparable results when using the model trained with cen-\\r\\ntraining frames with ground truth bounding box annotations             terness but applying only the classification scores to local-\\r\\nusing an augmentation (vertical flipping and random trans-             ize the target (2nd row). Using only the centerness scores\\r\\nlation). Tab. 10 shows the results for different combinations\\r\\nof multiple initial and recent training frames. Note, that we\\r\\n                                                                                Video Frame          Centerness Scores   Classi cation Scores\\r\\nuse the same network weights for all experiments trained\\r\\nwith one initial and one recent recent frame in all cases.\\r\\nWe observer that using more training frames can improve\\r\\nthe tracking performance but decreases the run-time. Fur-\\r\\nthermore, we observe that the tracker greatly benefits from\\r\\nincluding at least one recent frame for training.\\r\\n\\r\\nA.3. Centerness\\r\\n   Our proposed bounding box regression component is in-\\r\\nspired by FCOS [55] but in contrast to FCOS we omit\\r\\nan auxiliary centerness branch. The classification head of\\r\\nFCOS is trained to predict a high score for almost every               Figure 8. Visual Comparison between centerness and classifica-\\r\\nregion inside the bounding box. The centerness branch is               tion scores.\\r\\n\\r\\n\\r\\n                                                                  14\\r\\n\\x0c\\n\\n              ToMP ToMP                STARK  STARK Ocean Alpha                    Fast\\r\\n             101+AR 50 +AR     RPT ST50+AR ST101+AR Plus Refine AFOD LWTL Ocean\\r\\n                                                                                                      notation of the target object and the predictions of three dif-\\r\\n                              [35, 47]   [63]   [63] [8, 35] [35, 64] [35] [4, 35] [35]               ferent trackers: SuperDiMP [12], STARK-ST101 [63] and\\r\\nEAO           0.497   0.496     0.530   0.505   0.497     0.491   0.482   0.472   0.463 0.461\\r\\nAccuracy      0.750   0.754     0.700   0.759   0.763     0.685   0.754   0.713   0.719 0.693         ToMP-101. We observe that our tracker produces in most\\r\\nRobustness    0.798   0.793     0.869   0.817   0.789     0.842   0.777   0.795   0.798 0.803\\r\\n                                                                                                      sequences more robust and in some more accurate bound-\\r\\nTable 12. Comparison to the state of the art of segmentation only\\r\\n                                                                                                      ing box predictions than the related methods. In particular\\r\\nmethods on VOT2020ST [35] in terms of EAO score.                                                      it achieves solid robustness for scenarios where distractors\\r\\n                                                                                                      are present but the target object is at least partially visible\\r\\n                                                                                                      and not undergoing a full occlusion.\\r\\ndecreases the performance (3rd row) because centerness of-\\r\\nten fails to identify the target among distractors (see Fig. 8).                                      B.2. Target Model Prediction\\r\\nFinally, we follow FCOS and multiply the classification and                                               Fig. 6 shows the target score maps produced by the tar-\\r\\ncenterness scores point-wise to retrieve the target object (4th                                       get model when using two different model predictors for\\r\\nrow). We conclude that omitting the centerness branch for                                             three different sequences. In detail we compare the tar-\\r\\ntraining and during inference to localize the target achieves                                         get score map produced by SuperDiMP [12] that adopts\\r\\nthe best tracking performance.                                                                        the DiMP [1] model predictor with optimized settings. In\\r\\n                                                                                                      particular it uses a slightly smaller search area factor of 6\\r\\nB. Visual Results                                                                                     instead of 5 and a target score resolution of 22 instead of\\r\\n                                                                                                      18. Note, that our tracker uses 5 and 18 similar to DiMP [1]\\r\\n   In this part we provide visual results of our tracker. First,\\r\\n                                                                                                      as stated Sec. A.2. We observe that our model predictor\\r\\nwe show three frames of different sequences where our\\r\\n                                                                                                      leads to much cleaner and unambiguous target localization\\r\\ntracker outperforms the state of the art. Secondly, we com-\\r\\n                                                                                                      than DiMP. While the former often produces multiple local\\r\\npare the produced target score map of our tracker with score\\r\\n                                                                                                      maxima for distractors, our methods is able to almost fully\\r\\nmaps obtained by optimization based model prediction. Fi-\\r\\n                                                                                                      suppress these. An important design choice that enables this\\r\\nnally, we show some failure cases of our tracker.\\r\\n                                                                                                      is the transductive model weight and test feature prediction\\r\\nB.1. Visual Comparison to the State of the Art                                                        produced by our Transformer based model predictor. How-\\r\\n                                                                                                      ever, the cleaner score maps come with the risk, that once\\r\\n   Fig. 7 shows three frames of eight different LaSOT [20]                                            the target is lost and a distractor is tracked instead recov-\\r\\nsequences where each frame contains the ground truth an-                                              ering is less likely since our tracker effectively suppresses\\r\\n                                                                                                      distractors. Similarly, our method learns to produce a score\\r\\n                         #0012                          #0586                            #0776\\r\\n                                                                                                      map containing a Gaussian such that overall the maximum\\r\\n                                                                                                      score values are higher than by SuperDiMP. Thus, we chose\\r\\n                                                                                                      a relatively high threshold to decide whether to use a previ-\\r\\n                                                                                                      ous prediction as training sample or not.\\r\\n\\r\\n                                                                                                      B.3. Failure Cases\\r\\n                         #0016                          #0146                            #0926\\r\\n\\r\\n                                                                                                         Fig. 9 shows failure cases of our tracker. In particular, it\\r\\n                                                                                                      shows three frames of four different LaSOT [20] sequences\\r\\n                                                                                                      containing the ground truth annotations and the predicted\\r\\n                                                                                                      bounding boxes of our tracker using a ResNet-101 [28] as\\r\\n                        #0003                           #0781                           #1877\\r\\n                                                                                                      backbone. To summarize, our tracker typically fails if ob-\\r\\n                                                                                                      ject similar to the targets so called distractors are present.\\r\\n                                                                                                      While the sole presence of distractors typically does not\\r\\n                                                                                                      lead to tracking failure, our tracker shows difficulties in\\r\\n                                                                                                      sequences where the target is occluded and distractors are\\r\\n                        #1218                           #1512                           #2126\\r\\n                                                                                                      present (1st and 3rd row). Instead of detecting that the target\\r\\n                                                                                                      is occluded the tracker starts to track a distractor instead.\\r\\n                                                                                                      Another challenging scenario are sequences where the tar-\\r\\n                                                                                                      get and a distractor approach each other (2nd row in Fig. 9)\\r\\n                                                                                                      or one occludes the other (4th row in Fig. 9). The model\\r\\n                                                                                                      then detects only a single object instead of two in both sce-\\r\\n                      Annotation                        ToMP-101                                      narios. Once they diverge again and the tracker detects two\\r\\n                                                                                                      objects it typically fails to reliably differentiate between the\\r\\n       Figure 9. Visualization of failure cases of our tracker.                                       target and the distractor.\\r\\n\\r\\n\\r\\n                                                                                                 15\\r\\n\\x0c\\n\\n                        100\\r\\n                                                    Success plot                                                 100\\r\\n                                                                                                                                             Success plot                                                    100\\r\\n                                                                                                                                                                                                                                          Success plot\\r\\n\\r\\n                         80                                                                                       80                                                                                          80\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                         Overlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                     Overlap Precision [%]\\r\\n                                KeepTrack [69.7]\\r\\n                                TransT [69.1]\\r\\n                         60     ToMP 50 [69.0]                                                                    60     KeepTrack [70.9]                                                                     60\\r\\n                                PrDiMP50 [68.0]                                                                          TrDiMP [70.7]                                                                                ToMP 50 [66.9]\\r\\n                                Super DiMP [67.7]                                                                        UPDT [70.4]                                                                                  ToMP 101 [66.7]\\r\\n                                TrDiMP [67.5]                                                                            Super DiMP [70.1]                                                                            KeepTrack [66.4]\\r\\n                         40     ToMP 101 [66.9]                                                                   40     ToMP 50 [70.1]                                                                       40      TrDiMP [66.2]\\r\\n                                STMTrack [65.7]                                                                          ToMP 101 [70.1]                                                                              TransT [65.3]\\r\\n                                DiMP50 [65.3]                                                                            SiamRPN++ [69.6]                                                                             Super DiMP [64.8]\\r\\n                                SiamRPN++ [65.2]                                                                         PrDiMP50 [69.6]                                                                              PrDiMP50 [63.5]\\r\\n                                ATOM [64.2]                                                                              ECO [69.1]                                                                                   DiMP50 [61.9]\\r\\n                         20     DaSiamRPN [57.7]                                                                  20     DiMP50 [68.4]                                                                        20      ATOM [58.4]\\r\\n                                UPDT [54.5]                                                                              CCOT [68.2]                                                                                  UPDT [53.6]\\r\\n                                ECO [53.2]                                                                               ATOM [66.3]                                                                                  CCOT [48.8]\\r\\n                                CCOT [51.3]                                                                              DaSiamRPN [65.8]                                                                             ECO [46.6]\\r\\n                          00        0.2             0.4          0.6      0.8        1                             00        0.2             0.4       0.6          0.8          1                             00         0.2             0.4          0.6          0.8          1\\r\\n                                               Overlap threshold                                                                        Overlap threshold                                                                            Overlap threshold\\r\\n                                          (a) UAV123 [49]                                                                          (b) OTB-100 [61]                                                                                (c) NFS [23]\\r\\n\\r\\n  Figure 10. Success plots on the UAV123 [49], OTB-100 [61] and NFS [23] datasets in terms of overall AUC score, reported in the legend.\\r\\n\\r\\n                                ToMP ToMP Keep STARK Tr                STARK                                                        Super            Pr  Siam STM             Siam           Retina FCOS\\r\\n                                 101  50 Track ST101 DiMP TransT SAOT ST50                                                          DiMP           DiMP R-CNN Track DiMP KYS RPN++ ATOM UPDT MAML MAML\\r\\n                                          [48]   [63] [59]  [7]   [73]   [63]                                                        [12]           [16]  [57] [22]  [1]  [2]  [39] [13] [3]  [58]   [58]\\r\\n           UAV123 66.9                      69.0          69.7     68.2    67.5    69.1                          69.1     \\xe2\\x80\\x93           67.7          68.0      64.9        64.7   65.3                           \\xe2\\x80\\x93       61.3        64.2        54.5          \\xe2\\x80\\x93            \\xe2\\x80\\x93\\r\\n           OTB-100 70.1                     70.1          70.9     68.1    71.1    69.4                          68.5    71.4         70.1          69.6      70.1        71.9   68.4                          69.5     69.6        66.9        70.2         71.2         70.4\\r\\n           NFS     66.7                     66.9          66.4     66.2    66.2    65.7                          65.2    65.6         64.8          63.5      63.9         \\xe2\\x80\\x93     62.0                          63.5      \\xe2\\x80\\x93          58.4        53.7          \\xe2\\x80\\x93            \\xe2\\x80\\x93\\r\\n\\r\\n                                           Auto Auto                      Siam     Siam                                                                                   Siam                                          Siam       Siam             DaSiam\\r\\n                                Ocean STN Match Track                     BAN      CAR                       ECO DCFST PG-NET CRACT                          GCT          GAT CLNet TLPG                                AttN       FC++ MDNet CCOT RPN\\r\\n                                 [70] [44] [69]  [41]                      [9]      [27]                     [14] [72]   [42]  [21]                          [25]          [26] [18] [40]                                [68]       [62] [51]  [15]  [74]\\r\\n           UAV123   \\xe2\\x80\\x93                       64.9           \\xe2\\x80\\x93       67.1    63.1    61.4                          53.2     \\xe2\\x80\\x93            \\xe2\\x80\\x93            66.4      50.8        64.6   63.3                           \\xe2\\x80\\x93       65.0         \\xe2\\x80\\x93           \\xe2\\x80\\x93           51.3         57.7\\r\\n           OTB-100 68.4                     69.3          71.4      \\xe2\\x80\\x93      69.6     \\xe2\\x80\\x93                            69.1    70.9         69.1          72.6      64.8        71.0    \\xe2\\x80\\x93                            69.8     71.2        68.3        67.8         68.2         65.8\\r\\n           NFS      \\xe2\\x80\\x93                        \\xe2\\x80\\x93             \\xe2\\x80\\x93        \\xe2\\x80\\x93      59.4     \\xe2\\x80\\x93                            46.6    64.1          \\xe2\\x80\\x93            62.5       \\xe2\\x80\\x93           \\xe2\\x80\\x93     54.3                           \\xe2\\x80\\x93        \\xe2\\x80\\x93           \\xe2\\x80\\x93          41.9         48.8          \\xe2\\x80\\x93\\r\\n\\r\\n\\r\\n                   Table 13. Comparison with state-of-the-art on the OTB-100 [61], NFS [23] and UAV123 [49] datasets in terms of overall AUC score.\\r\\n\\r\\n                               ToMP ToMP STARK Keep STARK Alpha          Siam   Tr  Super                                                                          STM         Pr                                      DM         Auto\\r\\n                                101  50  ST101 Track ST50 Refine TransT R-CNN DiMP Dimp                                                                      SAOT Track DTT DiMP                                      Track       Match TLPG                 TACT         LTMU\\r\\n                                           [63] [48]  [63] [64]    [7]    [57] [59]  [12]                                                                     [73]  [22] [67] [16]                                     [71]        [69]  [40]                 [10]         [11]\\r\\n           LaSOT 68.5                     67.6            67.1     67.1     66.4     65.3                         64.9      64.8        63.9        63.1     61.6         60.6 60.1                          59.8      58.4        58.3         58.1         57.5         57.2\\r\\n\\r\\n                                                      Siam             Siam         Siam                          PG   FCOS Global      DaSiam Siam Siam      Siam Retina Siam\\r\\n                               DiMP Ocean             AttN       CRACT FC++         GAT                           NET MAML Track ATOM RPN BAN CAR CLNet RPN++ MAML Mask ROAM++ SPLT\\r\\n                                [1]  [70]              [68]       [21]  [62]         [26]                         [42]  [58] [30]  [13]  [74]\\xe2\\x80\\xa0  [9] [27] [18] [39]\\xe2\\x80\\xa0 [58] [60]\\xe2\\x80\\xa0 [66] [65]\\r\\n           LaSOT 56.9                     56.0            56.0     54.9     54.4     53.9                         53.1      52.3        52.1        51.5     51.5         51.4 50.7                          49.9      49.6        48.0         46.7         44.7         42.6\\r\\n\\r\\n\\r\\n  Table 14. Comparison with state-of-the-art on the LaSOT [20] test set in terms of overall AUC score. The symbol \\xe2\\x80\\xa0 marks results that were\\r\\n  produced by Fan et al. [20] otherwise they are obtained directly from the official paper.\\r\\n\\r\\n\\r\\n  C. Experiments                                                                                                                                     the target in each frame. In the main paper we compare\\r\\n                                                                                                                                                     our method with methods that produce bounding boxes.\\r\\n     We provide more detailed experiments to complement                                                                                              Thus, in addition, we compare our method on the VOT2020\\r\\n  the comparison to the state of-the art performed in the main                                                                                       short-term challenge to methods that produce a segmen-\\r\\n  paper. And provide results for the VOT2020ST [35] chal-                                                                                            tation mask in each frame. Since our method produces\\r\\n  lenge when using AlphaRefine [64] on top of our method in                                                                                          only a bounding box, we use AlphaRefine [64] that is able\\r\\n  order to compare with methods that produce a segmentation                                                                                          to produce a segmentation mask give the bounding box.\\r\\n  mask as output.                                                                                                                                    Tab. 12 shows that our method achieves competitive results.\\r\\n                                                                                                                                                     In particular ToMP-101 achieves the same EAO (for more\\r\\n  C.1. VOT2020 with AlphaRefine                                                                                                                      details on EAO we refer the reader to [35]) as STARK-\\r\\n     In contrast to previous years where the sequences in                                                                                            ST101+AR [63] that employs AlphaRefine too. Nonethe-\\r\\n  the VOT short-term challenge were annotated with bound-                                                                                            less, RPT [47] achieves higher EAO than our tracker. In\\r\\n  ing boxes [36, 37] the sequences of the more recent chal-                                                                                          particular it scores a higher robustness but a lower accuracy\\r\\n  lenges contain segmentation mask annotations [35, 38] of                                                                                           than our trackers.\\r\\n\\r\\n\\r\\n                                                                                                                                               16\\r\\n\\x0c\\n\\n                        90\\r\\n                                                   Success plot                                           90\\r\\n                                                                                                                  Normalized Precision plot\\r\\n                        80                                                                                80\\r\\n                        70                                                                                70\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                 Distance Precision [%]\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                              ToMP 101 [68.5]                                                                                                      ToMP 101 [79.2]\\r\\n                        60    ToMP 50 [67.6]                                                              60                                       ToMP 50 [78.0]\\r\\n                              KeepTrack [67.1]                                                                                                     KeepTrack [77.2]\\r\\n                        50    STARK-ST101 [67.1]                                                          50                                       STARK-ST101 [76.9]\\r\\n                              STARK-ST50 [66.4]                                                                                                    STARK-ST50 [76.3]\\r\\n                              AlphaRefine [65.9]                                                                                                   AlphaRefine [73.8]\\r\\n                        40    TransT [64.9]\\r\\n                              Siam R-CNN [64.8]\\r\\n                                                                                                          40                                       TransT [73.8]\\r\\n                                                                                                                                                   TrDiMP [73.0]\\r\\n                              TrDiMP [63.9]                                                                                                        Siam R-CNN [72.2]\\r\\n                        30    Super DiMP [63.1]                                                           30                                       Super DiMP [72.2]\\r\\n                              STMTrack [60.6]                                                                                                      STMTrack [69.3]\\r\\n                              PrDiMP50 [59.8]                                                                                                      PrDiMP50 [68.8]\\r\\n                        20    DMTrack [58.4]                                                              20                                       DMTrack [66.9]\\r\\n                              TACT [57.5]                                                                                                          LTMU [66.5]\\r\\n                        10    LTMU [57.2]                                                                 10                                       TACT [66.0]\\r\\n                              DiMP50 [56.9]                                                                                                        Ocean [65.1]\\r\\n                              Ocean [56.0]                                                                                                         DiMP50 [65.0]\\r\\n                         00         0.2            0.4           0.6   0.8   1                             00   0.1         0.2         0.3           0.4           0.5\\r\\n                                               Overlap threshold                                                      Location error threshold\\r\\n                                                   (a) Success                                                        (b) Normalized Precision\\r\\n\\r\\nFigure 11. Success and normalized precision plots on LaSOT [20]. Our approach outperforms all other methods by a large margin in AUC,\\r\\nreported in the legend.\\r\\n                        70\\r\\n                                                   Success plot                                           70\\r\\n                                                                                                                  Normalized Precision plot\\r\\n\\r\\n                        60                                                                                60\\r\\n                                                                                 Distance Precision [%]\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                        50                                                                                50\\r\\n\\r\\n                        40                                                                                40\\r\\n                                KeepTrack [48.2]                                                                                              KeepTrack [61.7]\\r\\n                        30      ToMP 101 [45.9]                                                           30                                  ToMP 101 [58.1]\\r\\n                                ToMP 50 [45.4]                                                                                                ToMP 50 [57.6]\\r\\n                                Super DiMP [43.7]                                                                                           Super DiMP [56.3]\\r\\n                        20      LTMU [41.4]                                                               20                                LTMU [53.6]\\r\\n                                DiMP [39.2]                                                                                                 DiMP [51.2]\\r\\n                        10      ATOM [37.6]                                                               10                                ATOM [49.6]\\r\\n                                DaSiamRPN [35.6]                                                                                            DaSiamRPN [48.0]\\r\\n                                SiamRPN++ [34.0]                                                                                            SiamRPN++ [45.3]\\r\\n                         00        0.2        0.4                0.6   0.8   1                             00   0.1         0.2         0.3         0.4       0.5\\r\\n                                               Overlap threshold                                                      Location error threshold\\r\\n                                                   (a) Success                                                        (b) Normalized Precision\\r\\n\\r\\nFigure 12. Success and normalized precision plots on LaSOTExtSub [19]. Our approach outperforms all other methods by a large margin\\r\\nin AUC, reported in the legend.\\r\\n\\r\\n\\r\\nC.2. UAV123, OTB-100 and NFS                                                                      Fig. 10c shows that our tracker is almost as robust as Keep-\\r\\n                                                                                                  Track [48] but achieves superior accuracy leading to a new\\r\\n   To complement the results detailed in the paper, we                                            state of the art. While we reported only the methods with\\r\\nprovide the success plots for the UAV123 [49] dataset in                                          the highest performances on these datasets in the main pa-\\r\\nFig. 10a, the OTB-100 [61] dataset in Fig. 10b and the                                            per, we compare our method in Tab. 13 with additional re-\\r\\nNFS [23] dataset in Fig. 10c. Fig. 10a shows that Keep-                                           lated methods.\\r\\nTrack [48] and PrDiMP50 [16] achieve higher robustness\\r\\n                                                                                                  C.3. LaSOT and LaSOTExtSub\\r\\nthan our tracker (T < 0.6) but that our trackers together\\r\\nwith TransT [7] reaches the highest accuracy among all                                               In addition to the success plots, we provide the normal-\\r\\ntrackers (T > 0.7) compensating for the lower robustness.                                         ized precision plots on the LaSOT [20] test set in Fig. 11\\r\\nFig. 10b reveals similar conclusions on OTB-100. For NFS                                          the LaSOTExtSub [19] test set in Fig. 12. The normalized\\r\\n\\r\\n\\r\\n                                                                                 17\\r\\n\\x0c\\n\\n               Illumination Partial             Motion Camera        Background Viewpoint Scale     Full     Fast                Low          Aspect\\r\\n                 Variation Occlusion Deformation Blur Motion Rotation Clutter    Change Variation Occlusion Motion Out-of-View Resolution Ration Change   Total\\r\\n LTMU             56.5      54.0       57.2     55.8    61.6   55.1      49.9        56.7    57.1     49.9    44.0      52.7      51.4        55.1        57.2\\r\\n PrDiMP50         63.7      56.9       60.8     57.9    64.2   58.1      54.3        59.2    59.4     51.3    48.4      55.3      53.5        58.6        59.8\\r\\n STMTrack         65.2      57.1       64.0     55.3    63.3   60.1      54.1        58.2    60.6     47.8    42.4      51.9      50.3        58.8        60.6\\r\\n SuperDiMP        67.8      59.7       63.4     62.0    68.0   61.4      57.3        63.4    62.9     54.1    50.7      59.0      56.4        61.6        63.1\\r\\n TrDiMP           67.5      61.1       64.4     62.4    68.1   62.4      58.9        62.8    63.4     56.4    53.0      60.7      58.1        62.3        63.9\\r\\n Siam R-CNN       64.6      62.2       65.2     63.1    68.2   64.1      54.2        65.3    64.5     55.3    51.5      62.2      57.1        63.4        64.8\\r\\n TransT           65.2      62.0       67.0     63.0    67.2   64.3      57.9        61.7    64.6     55.3    51.0      58.2      56.4        63.2        64.9\\r\\n AlphaRefine      69.4      62.3       66.3     65.2    70.0   63.9      58.8        63.1    65.4     57.4    53.6      61.1      58.6        64.1        65.3\\r\\n STARK-ST50       66.8      64.3       66.9     62.9    69.0   66.1      57.3        67.8    66.1     58.7    53.8      62.1      59.4        64.9        66.4\\r\\n STARK-ST101      67.5      65.1       68.3     64.5    69.5   66.6      57.4        68.8    66.8     58.9    54.2      63.3      59.6        65.6        67.1\\r\\n KeepTrack        69.7      64.1       67.0     66.7    71.0   65.3      61.2        66.9    66.8     60.1    57.7      64.1      62.0        65.9        67.1\\r\\n ToMP-50          66.8      64.9       68.5     64.6    70.2   67.3      59.1        67.2    67.5     59.3    56.1      63.7      61.1        66.5        67.6\\r\\n ToMP-101         69.0      65.3       69.4     65.2    71.7   67.8      61.5        69.2    68.4     59.1    57.9      64.1      62.5        67.2        68.5\\r\\n\\r\\n\\r\\nTable 15. LaSOT [20] attribute-based analysis. Each column corresponds to the results computed on all sequences in the dataset with the\\r\\ncorresponding attribute.\\r\\n\\r\\n\\r\\nprecision score NPrD measures the percentage of frames\\r\\nwhere the normalized distance (relative to the target size)\\r\\nbetween the predicted and ground-truth target center loca-\\r\\ntion is less than a threshold D \\xe2\\x88\\x88 [0, 0.5]. The ranking is de-\\r\\ntermined by computing the AUC of each tracker. The AUC\\r\\nis reported in the legend of Figs. 11b and 12b. We compare\\r\\nour tracker on LaSOT with the state of the art in Tab. 14 and\\r\\nshow their performance if available in Fig. 11. In Fig. 12 we\\r\\nshow results of methods produced by Fan et al. [19] except\\r\\nKeepTrack [48] and SuperDiMP [12] that we obtained from\\r\\nMayer et al. [48].\\r\\n\\r\\nC.3.1   Attributes\\r\\nTo support the attribute based analysis in the main paper,\\r\\nwhere we compared the performance of our tracker with\\r\\nother Transformer based trackers, we provide the detailed\\r\\nanalysis for multiple trackers and ToMP in Tab. 15. ToMP-\\r\\n101 achieves the best performance on all but three. It\\r\\nachieves the second best results for Motion Blur behind\\r\\nKeepTrack [48] and similar to AlphaRefine [64]. Further\\r\\nToMP-101 achieves the third best for Full Occlusion be-\\r\\nhind KeepTrack [48] and ToMP-50. Similarly it scores\\r\\nthird for Illumination Variation behind KeepTrack [48] and\\r\\nAlphaRefine [64]. We further observe, that discrimina-\\r\\ntive model prediction based methods such as TrDiMP [59],\\r\\nSuperDiMP [12], AlphaRefine [64], KeepTrack [48] and\\r\\nToMP all outperform STARK [63] on the attribute Back-\\r\\nground Clutter showing the advantage of using full training\\r\\nsamples during tracking instead of cropped templates that\\r\\nmainly cover the centered target.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                18\\r\\n\\x0c', b'                                               Robust Visual Tracking by Segmentation\\r\\n\\r\\n                                             Matthieu Paul, Martin Danelljan, Christoph Mayer, and Luc Van Gool\\r\\n\\r\\n                                                         Computer Vision Lab, ETH Zu\\xcc\\x88rich, Switzerland\\r\\n                                                   {paulma, damartin, chmayer, vangool}@vision.ee.ethz.ch\\r\\narXiv:2203.11191v1 [cs.CV] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                               Abstract. Estimating the target extent poses a fundamental challenge\\r\\n                                               in visual object tracking. Typically, trackers are box-centric and fully rely\\r\\n                                               on a bounding box to define the target in the scene. In practice, objects\\r\\n                                               often have complex shapes and are not aligned with the image axis. In\\r\\n                                               these cases, bounding boxes do not provide an accurate description of\\r\\n                                               the target and often contain a majority of background pixels.\\r\\n                                               We propose a segmentation-centric tracking pipeline that not only pro-\\r\\n                                               duces a highly accurate segmentation mask, but also works internally\\r\\n                                               with segmentation masks instead of bounding boxes. Thus, our tracker\\r\\n                                               is able to better learn a target representation that clearly differentiates\\r\\n                                               the target in the scene from background content. In order to achieve\\r\\n                                               the necessary robustness for the challenging tracking scenario, we pro-\\r\\n                                               pose a separate instance localization component that is used to condition\\r\\n                                               the segmentation decoder when producing the output mask. We infer a\\r\\n                                               bounding box from the segmentation mask and validate our tracker on\\r\\n                                               challenging tracking datasets and achieve the new state of the art on\\r\\n                                               LaSOT [16] with a success AUC score of 69.7%. Since fully evaluating\\r\\n                                               the predicted masks on tracking datasets is not possible due to the miss-\\r\\n                                               ing mask annotations, we further validate our segmentation quality on\\r\\n                                               two popular video object segmentation datasets. The code and trained\\r\\n                                               models are available at https://github.com/visionml/pytracking.\\r\\n\\r\\n\\r\\n                                         1   Introduction\\r\\n                                         Visual object tracking is the task of estimating the state of a target object for\\r\\n                                         each frame in a video sequence. The target is solely characterized by its initial\\r\\n                                         state in the video. Current approaches predominately characterize the state itself\\r\\n                                         with a bounding box. However, this only gives a very coarse representation of\\r\\n                                         the target\\xe2\\x80\\x99s state in the image. In practice, objects often have complex shapes,\\r\\n                                         undergo substantial deformations, can be highly elongated, or simply not align\\r\\n                                         well with the image axes. In such cases, the majority of the image content inside\\r\\n                                         the target\\xe2\\x80\\x99s bounding box often consists of background regions, thus providing\\r\\n                                         very limited information about the object itself. In contrast, a segmentation mask\\r\\n                                         gives a precise characterization of the object\\xe2\\x80\\x99s extent in the image (see Fig. 1\\r\\n                                         frames #1600 and #3200). Such information is vital in a variety of applications,\\r\\n                                         including video analysis, video editing, and robotics. In this work, we therefore\\r\\n                                         aim to develop an approach capable of accurately and robustly segmenting the\\r\\n                                         target object, even in the highly challenging tracking datasets [16,35].\\r\\n\\x0c\\n\\n2       Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n        # 0000    # 1600     # 3200           # 0000           # 0700           # 1400\\r\\n\\r\\n\\r\\n\\r\\nSTARK\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      STARK\\r\\n        # 0000    # 1600     # 3200           # 0000           # 0700           # 1400\\r\\nLWL\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      LWL\\r\\n        # 0000    # 1600     # 3200           # 0000           # 0700           # 1400\\r\\nRTS\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      RTS\\r\\nFig. 1. Comparison between the VOT method Stark [54], the VOS method LWL [5]\\r\\nand our proposed method on two tracking sequences from the LaSOT [16] dataset.\\r\\nThe ground-truth annotation (\\xc2\\x97) is shown in each frame for reference. Our approach\\r\\nis more robust and predicts a more accurate target representation.\\r\\n\\r\\n    While severely limiting the information achieved about the target\\xe2\\x80\\x99s state\\r\\nin the video, the aforementioned issues with the bounding box representation\\r\\ncan itself lead to inaccurate bounding box predictions, or even tracking failure.\\r\\nFig. 1 shows two typical tracking sequences. The tracking method STARK [54]\\r\\n(first row) fails to regress bounding boxes that contain the entire object (#1600,\\r\\n#1400) or even starts tracking the wrong object (#0700). Conversely, segmenta-\\r\\ntion masks are a better fit to define the target object in a scene because only pix-\\r\\nels corresponding to the target are marked as foreground. Thus, a segmentation-\\r\\ncentric tracking architecture that is designed to work internally with a segmen-\\r\\ntation mask of the target instead of a bounding box has the potential to learn\\r\\nbetter target representations because it can clearly differentiate background from\\r\\nforeground regions in the scene.\\r\\n    A few recent tracking methods [46,53] have recognized the advantage of pro-\\r\\nducing a segmentation mask instead of a bounding box as tracking output. How-\\r\\never, these trackers are typically bounding box-centric and the final segmentation\\r\\nmask is obtained by a separate box-to-mask post-processing network. Thus, these\\r\\nmethods miss the opportunity to leverage the accurate target definition of seg-\\r\\nmentation masks to learn a more accurate and robust internal representation\\r\\nof the target. In contrast, most video object segmentation method [37,5] fol-\\r\\nlow a segmentation-centric paradigm. However, these methods are not designed\\r\\nfor the challenging tracking scenarios. Typical VOS sequences consist only of a\\r\\nfew hundred frames [40] whereas multiple tracking sequence of more than ten\\r\\nthousand frames exist in tracking datasets [16]. Due to this setup, VOS meth-\\r\\nods focus on producing highly accurate segmentation masks but are sensitive to\\r\\ndistractors, substantial deformations and occlusions of the target object. Fig. 1\\r\\nshows two typical tracking sequences where the VOS method LWL [5] (second\\r\\nrow) produces a fine-grained segmentation mask of the wrong object (#3200) or\\r\\nis unable to detect only the target within a crowd (#0700, #1400).\\r\\n    We propose RTS, a unified tracking architecture capable of predicting accu-\\r\\nrate segmentation masks. To design a segmentation-centric approach, we take\\r\\ninspiration from the VOS method LWL [5]. However, to achieve robust and ac-\\r\\ncurate segmentation on tracking datasets, we propose several new components.\\r\\n\\x0c\\n\\n                                   Robust Visual Tracking by Segmentation        3\\r\\n\\r\\nIn particular, we propose an instance localization branch that is trained to pre-\\r\\ndict a target appearance model, which allows the detection of occlusions and to\\r\\nidentify the correct target even in cluttered scenes. The output of the instance\\r\\nlocalization branch is further used to condition the high dimensional mask en-\\r\\ncoding. This enables the segmentation decoder to focus on the localized target,\\r\\nleading to a more robust mask prediction. Since, our proposed method contains\\r\\na segmentation and instance memory that needs to be updated with previous\\r\\ntracking results, we design a memory management module. This module first\\r\\nassesses the prediction quality, decides whether the sample should enter into the\\r\\nmemory and triggers the tracking model if it should be updated.\\r\\nContributions Our contributions are the following: (i) We propose a unified\\r\\ntracking architecture capable of predicting robust classification scores and ac-\\r\\ncurate segmentation masks. We design separate feature spaces and memories to\\r\\nensure optimal receptive fields and update-rates for segmentation and instance\\r\\nlocalization. (ii) To produce a segmentation mask which also agrees with the\\r\\ninstance prediction, we design a fusion mechanism that further conditions the\\r\\nsegmentation decoder on the instance localization output and leads to more ro-\\r\\nbust tracking performance. (iii) We introduce an effective inference procedure\\r\\ncapable of fusing the instance localization output and mask encoding to ensure\\r\\nboth robust and accurate tracking. (iv) We perform comprehensive evaluation\\r\\nand ablation studies of the proposed tracking pipeline on multiple popular track-\\r\\ning benchmarks. Our approach achieves the new state of the art on LaSOT with\\r\\nan area-under-the-curve (AUC) score of 69.7%.\\r\\n\\r\\n\\r\\n2   Related Work\\r\\nIn this section we will revisit recent tracking and video object segmentation\\r\\nmethods and particularly focus on existing tracking methods that produce a\\r\\nsegmentation mask as output.\\r\\nVisual Object Tracking Over the years many new challenging tracking bench-\\r\\nmarks such as LaSOT [16], GOT-10k [24], and TrackingNet [36] have been pro-\\r\\nposed that accelerated the tracking research. In particular, Discriminative Corre-\\r\\nlation Filter (DCF) based, Siamese and more recently transformer based trackers\\r\\nare the most dominant paradigms in visual object tracking.\\r\\n    Visual trackers based on DCFs [6,22,15,32,12,47,60,3,14] are very popular.\\r\\nThese methods essentially solve an optimization problem to estimate the weights\\r\\nof the DCF that allow to distinguish foreground from background regions. The\\r\\nDCF is often referred to as the target appearance model and allows to lo-\\r\\ncalize the target in the video frame. More recent DCF approaches [3,14] en-\\r\\nable end-to-end training by unrolling a fixed number of the optimization itera-\\r\\ntions during offline training. Siamese tracking methods have become more and\\r\\nmore popular because they are typically end-to-end trainable, simple and fast\\r\\n[43,2,42,61,20,49,21,29,28]. These trackers aim at learning a similarity metric us-\\r\\ning only the initial video frame and its annotation that allows to clearly identify\\r\\nthe target offline. Since no online learning component is involved, these trackers\\r\\n\\x0c\\n\\n4      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\nachieve high frame rates at the cost of limited online adaptability to changes\\r\\nof the target\\xe2\\x80\\x99s appearance. Nonetheless, several methods have been proposed\\r\\nto overcome these issues [43,2,29,28]. The very recently proposed transformer\\r\\nbased trackers achieve state of the art performance on many datasets often out-\\r\\nperforming DCF or Siamese based trackers. This group of trackers typically uses\\r\\na Transformer component in order to fuse information extracted from train-\\r\\ning and test frames to produce discriminative features that allow to accurately\\r\\nlocalize and estimate the target in the scene [8,55,54,48].\\r\\nVideo Object Segmentation Semi-supervised VOS is the task of classifying\\r\\nall pixels belonging to the target in each video frame, given only the segmen-\\r\\ntation mask of the target in the initial frame. The cost of annotating accu-\\r\\nrate segmentation masks is limiting the sequence length and number of videos\\r\\ncontained in available VOS datasets. Despite the relatively small size of VOS\\r\\ndatasets compared to other computer vision problems, new benchmarks such as\\r\\nYoutube-VOS [52] and DAVIS [40] accelerated the research progress in the last\\r\\nyears. A group of methods relies on a learnt target detector [7,45,33] whereas an-\\r\\nother learns how to propagate the segmentation mask across frames [51,39,30,25].\\r\\nOther methods use feature matching techniques across one or multiple frames\\r\\nwith or without using an explicit spatio-temporal memory [9,23,44,37]. Another\\r\\nmethod employs meta-learning to tackle VOS [5]. Specifically, Bhat et al. [5] in-\\r\\ntroduce an end-to-end trainable VOS architecture employing a few-shot learner\\r\\nthat predicts a learnable labels encoding. In particular, the few-shot learner gen-\\r\\nerates and updates online the parameters of a segmentation target model that\\r\\nproduces the mask encoding used to generate the final segmentation mask.\\r\\nJoint Visual Tracking and Segmentation A group of tracking methods have\\r\\nalready identified the advantages of predicting a segmentation mask instead of a\\r\\nbounding box [53,59,46,31,50,41]. Siam-RCNN is a box-centric tracker that used\\r\\na pretrained box2seg network to predict the segmentation mask given a bound-\\r\\ning box prediction. In contrast, AlphaRefine represents a novel box2seg method\\r\\nthat has been evaluated with many recent trackers such as SuperDiMP [14]\\r\\nor SiamRPN++ [28]. Further, Zhao et al. [59] focus on generating segmentation\\r\\nmasks from bounding box annotations in videos using a spatio-temporal aggrega-\\r\\ntion module to mine consistencies of the scene across multiple frames. Conversely,\\r\\nSiamMask [50] and D3S [31] are segmentation-centric trackers that produce di-\\r\\nrectly a segmentation output mask without employing a box2seg module. In\\r\\nparticular, SiamMask [50] consists of a fully-convolutional Siamese network that\\r\\nemploys a separate branch to predict binary segmentation masks supervised via\\r\\na segmentation loss. From a high-level view the single-shot segmentation tracker\\r\\nD3S [31] is most related to our proposed method. Both methods employ two\\r\\ndedicated modules or branches; one for localization and one for segmentation.\\r\\nWhereas D3S adopts the target classification component of ATOM [12] that re-\\r\\nquires to optimize online the weights of a two-layer CNN, we simply learn online\\r\\nthe weights of a DCF similar to DiMP [3]. For segmentation, they propose a\\r\\nfeature matching technique that matches test frame features with background\\r\\nand foreground features corresponding to the initial frame. In contrast, we adopt\\r\\n\\x0c\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Robust Visual Tracking by Segmentation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5\\r\\n\\r\\n               Segmentation Memory                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Update\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Signal\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Label\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Encoder E\\xe2\\x9c\\x93\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      <latexit sha1_base64=\"PlAH+J4KN+KlAUk6ay2VBYkA7lQ=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgqiOCxgv2ANpTNdtIu3WzC7kQooX/CgxcVr/4dj/4bt20O2vpg4PHeDDPzgkQKg6777RTW1jc2t4rbpZ3dvf2D8uFRy8Sp5tDksYx1J2AGpFDQRIESOokGFgUS2sH4dua3n0AbEatHnCTgR2yoRCg4Qyt17vo9HAGyfrniVt056CrxclIhORr98ldvEPM0AoVcMmO6npugnzGNgkuYlnqpgYTxMRtC11LFIjB+Nr93Ss+sMqBhrG0ppHP190TGImMmUWA7I4Yjs+zNxP+8borhtZ8JlaQIii8WhamkGNPZ83QgNHCUE0sY18LeSvmIacbRRmQz8JY/XiWti6p3Wa091Cr1mzyNIjkhp+SceOSK1Mk9aZAm4USSZ/JK3pzEeXHenY9Fa8HJZ47JHzifP2Wcj9E=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSegmentation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Few-Shot\\r\\n   Model                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Learner A\\xe2\\x9c\\x93\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <latexit sha1_base64=\"SX+8urpQwwtaDfbK8LPe/AglH3Q=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHiqePFYwX5AG8pmO2mXbjZhdyKU0D/hwYuKV/+OR/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZeJUc2jyWMa6EzADUihookAJnUQDiwIJ7WB8N/PbT6CNiNUjThLwIzZUIhScoZU6t/0ejgBZv1xxq+4cdJV4OamQHI1++as3iHkagUIumTFdz03Qz5hGwSVMS73UQML4mA2ha6liERg/m987pWdWGdAw1rYU0rn6eyJjkTGTKLCdEcORWfZm4n9eN8Xw2s+ESlIExReLwlRSjOnseToQGjjKiSWMa2FvpXzENONoI7IZeMsfr5LWRdW7rNYeapX6TZ5GkZyQU3JOPHJF6uSeNEiTcCLJM3klb07ivDjvzseiteDkM8fkD5zPH190j80=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  Updater                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Weight\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Predictor W\\xe2\\x9c\\x93\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     <latexit sha1_base64=\"0ZbxCC53UdelsGaHgMYGUGmajTs=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69BIvgqSQiKp4KXjxWsB/QhrLZTtqlm03YnQil9E948KLi1b/j0X/jts1BWx8MPN6bYWZemEphyPO+ncLa+sbmVnG7tLO7t39QPjxqmiTTHBs8kYluh8ygFAobJEhiO9XI4lBiKxzdzfzWE2ojEvVI4xSDmA2UiARnZKV2q9elIRLrlSte1ZvDXSV+TiqQo94rf3X7Cc9iVMQlM6bjeykFE6ZJcInTUjczmDI+YgPsWKpYjCaYzO+dumdW6btRom0pcufq74kJi40Zx6HtjBkNzbI3E//zOhlFN8FEqDQjVHyxKMqkS4k7e97tC42c5NgSxrWwt7p8yDTjZCOyGfjLH6+S5kXVv6pePlxWard5GkU4gVM4Bx+uoQb3UIcGcJDwDK/w5qTOi/PufCxaC04+cwx/4Hz+AIFQj+M=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ds\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  <latexit sha1_base64=\"k6nH96m/cMqClDvpNU67v0HYhcg=\">AAAB/HicbVDLSsNAFJ3UV62v+Ni5CRbBVUmkqLgq6MJlBfuAJoTJdNoOnUzCzI1YQ/wWF25U3PofLv0bJ20W2npg4HDOvdwzJ4g5U2Db30ZpaXllda28XtnY3NreMXf32ipKJKEtEvFIdgOsKGeCtoABp91YUhwGnHaC8VXud+6pVCwSdzCJqRfioWADRjBoyTcP3BDDiGCeXme+C/QBUpX5ZtWu2VNYi8QpSBUVaPrml9uPSBJSAYRjpXqOHYOXYgmMcJpV3ETRGJMxHtKepgKHVHnpNH1mHWulbw0iqZ8Aa6r+3khxqNQkDPRknlXNe7n4n9dLYHDhpUzECVBBZocGCbcgsvIqrD6TlACfaIKJZDqrRUZYYgK6MN2BM//jRdI+rTlntfptvdq4LNooo0N0hE6Qg85RA92gJmohgh7RM3pFb8aT8WK8Gx+z0ZJR7OyjPzA+fwDXL5XB</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\xe2\\x8c\\xa7\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <latexit sha1_base64=\"CEjhwhrnOB4cECOwBNgXaMXzlK4=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1JMUvHisYD+gDWWz3bRLN5uwOxFK6F/w4EXFq3/Io//GTZuDtj4YeLw3w8y8IJHCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7nK/88S1EbF6xGnC/YiOlAgFo5hLfaTpoFpz6+4cZJV4BalBgeag+tUfxiyNuEImqTE9z03Qz6hGwSSfVfqp4QllEzriPUsVjbjxs/mtM3JmlSEJY21LIZmrvycyGhkzjQLbGVEcm2UvF//zeimGN34mVJIiV2yxKEwlwZjkj5Oh0JyhnFpCmRb2VsLGVFOGNh6bgbf88SppX9S9q/rlw2WtcVukUYYTOIVz8OAaGnAPTWgBgzE8wyu8OdJ5cd6dj0VrySlmjuEPnM8fntGOPQ==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               xs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      xm\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <latexit sha1_base64=\"iS2FGbLVU7HB5ez4hFdk3GURAhc=\">AAAB8HicbVDLSgNBEJyNrxhfUY9eBoPgKexKUPEU8OIxgnlgsoTZSScZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7glgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJ5lDnkYx0K2AGpFBQR4ESWrEGFgYSmsHoduY3n0AbEakHnMTgh2ygRF9whlZ6HHc7CGNMw2m3WHLL7hx0lXgZKZEMtW7xq9OLeBKCQi6ZMW3PjdFPmUbBJUwLncRAzPiIDaBtqWIhGD+dXzylZ1bp0X6kbSmkc/X3RMpCYyZhYDtDhkOz7M3E/7x2gv1rPxUqThAUXyzqJ5JiRGfv057QwFFOLGFcC3sr5UOmGUcbks3AW/54lTQuyt5luXJfKVVvsjTy5IScknPikStSJXekRuqEE0WeySt5c7Tz4rw7H4vWnJPNHJM/cD5/AKbIkSw=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                xf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ss\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Decoder D\\xe2\\x9c\\x93\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <latexit sha1_base64=\"qJZdQQ7DYCOLM7QkbS8IW2EfdxQ=\">AAAB8HicbVDLSgNBEJyNrxhfUY9eBoPgKexKUPEU8OIxgnlgsoTZSScZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7glgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJ5lDnkYx0K2AGpFBQR4ESWrEGFgYSmsHoduY3n0AbEakHnMTgh2ygRF9whlZ6HHc7CGNMzbRbLLlldw66SryMlEiGWrf41elFPAlBIZfMmLbnxuinTKPgEqaFTmIgZnzEBtC2VLEQjJ/OL57SM6v0aD/SthTSufp7ImWhMZMwsJ0hw6FZ9mbif147wf61nwoVJwiKLxb1E0kxorP3aU9o4CgnljCuhb2V8iHTjKMNyWbgLX+8ShoXZe+yXLmvlKo3WRp5ckJOyTnxyBWpkjtSI3XCiSLP5JW8Odp5cd6dj0VrzslmjskfOJ8/r+CRMg==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <latexit sha1_base64=\"DYVMID1Su0tImuST/hWLppZAgSo=\">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKexKUPEU8OIxgnlgEsLsZDYZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7/FgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJZrzOIhnplk8Nl0LxOgqUvBVrTkNf8qY/up35zSeujYjUA05i3g3pQIlAMIpWehz3OsjHmAbTXrHklt05yCrxMlKCDLVe8avTj1gScoVMUmPanhtjN6UaBZN8WugkhseUjeiAty1VNOSmm84vnpIzq/RJEGlbCslc/T2R0tCYSejbzpDi0Cx7M/E/r51gcN1NhYoT5IotFgWJJBiR2fukLzRnKCeWUKaFvZWwIdWUoQ3JZuAtf7xKGhdl77Jcua+UqjdZGnk4gVM4Bw+uoAp3UIM6MFDwDK/w5mjnxXl3PhatOSebOYY/cD5/AJwskSU=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <latexit sha1_base64=\"2LUbZ/Tung/IOwUZeZXeZHm3IvM=\">AAAB8HicbVBNS8NAEN3Ur1q/qh69LBbBU0lEVDwVvHisYD+wDWWznbRLN5uwOxFL6L/w4EXFq//Go//GbZuDtj4YeLw3w8y8IJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqOTR4LGPdDpgBKRQ0UKCEdqKBRYGEVjC6mfqtR9BGxOoexwn4ERsoEQrO0EoPptdFeMLMTHrlilt1Z6DLxMtJheSo98pf3X7M0wgUcsmM6Xhugn7GNAouYVLqpgYSxkdsAB1LFYvA+Nns4gk9sUqfhrG2pZDO1N8TGYuMGUeB7YwYDs2iNxX/8zophld+JlSSIig+XxSmkmJMp+/TvtDAUY4tYVwLeyvlQ6YZRxuSzcBb/HiZNM+q3kX1/O68UrvO0yiSI3JMTolHLkmN3JI6aRBOFHkmr+TN0c6L8+58zFsLTj5zSP7A+fwBqCSRLQ==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n Test Frame                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         F\\xe2\\x9c\\x93\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <latexit sha1_base64=\"IsOXO/arn2AU5sGT/2DQTPMaZ9w=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgqCOKxgv2ANpTNdtIu3WzC7kQooX/CgxcVr/4dj/4bt20O2vpg4PHeDDPzgkQKg6777RTW1jc2t4rbpZ3dvf2D8uFRy8Sp5tDksYx1J2AGpFDQRIESOokGFgUS2sH4dua3n0AbEatHnCTgR2yoRCg4Qyt17vo9HAGyfrniVt056CrxclIhORr98ldvEPM0AoVcMmO6npugnzGNgkuYlnqpgYTxMRtC11LFIjB+Nr93Ss+sMqBhrG0ppHP190TGImMmUWA7I4Yjs+zNxP+8borhtZ8JlaQIii8WhamkGNPZ83QgNHCUE0sY18LeSvmIacbRRmQz8JY/XiWti6p3Wa091Cr1mzyNIjkhp+SceOSK1Mk9aZAm4USSZ/JK3pzEeXHenY9Fa8HJZ47JHzifP2cmj9I=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      <latexit sha1_base64=\"UNy6gYkK56vySrLyPmHP5RT3wps=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgq6MFjBfsBbSib7aRdutmE3YlQQv+EBy8qXv07Hv03btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzaHJYxnrTsAMSKGgiQIldBINLAoktIPx7cxvP4E2IlaPOEnAj9hQiVBwhlbq3PV7OAJk/XLFrbpz0FXi5aRCcjT65a/eIOZpBAq5ZMZ0PTdBP2MaBZcwLfVSAwnjYzaErqWKRWD8bH7vlJ5ZZUDDWNtSSOfq74mMRcZMosB2RgxHZtmbif953RTDaz8TKkkRFF8sClNJMaaz5+lAaOAoJ5YwroW9lfIR04yjjchm4C1/vEpaF1Xvslp7qFXqN3kaRXJCTsk58cgVqZN70iBNwokkz+SVvDmJ8+K8Ox+L1oKTzxyTP3A+fwBkEo/Q</latexit>\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             xb                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Segmentation\\r\\n                Backbone B\\xe2\\x9c\\x93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 +\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <latexit sha1_base64=\"9HMnH5MJiZFydCxhN8g/AxeA0Lo=\">AAAB8HicbVDLSgNBEJyNrxhfUY9eBoPgKexKUPEU8OIxgnlgsoTZSScZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7glgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJ5lDnkYx0K2AGpFBQR4ESWrEGFgYSmsHoduY3n0AbEakHnMTgh2ygRF9whlZ6HHc7CGNMg2m3WHLL7hx0lXgZKZEMtW7xq9OLeBKCQi6ZMW3PjdFPmUbBJUwLncRAzPiIDaBtqWIhGD+dXzylZ1bp0X6kbSmkc/X3RMpCYyZhYDtDhkOz7M3E/7x2gv1rPxUqThAUXyzqJ5JiRGfv057QwFFOLGFcC3sr5UOmGUcbks3AW/54lTQuyt5luXJfKVVvsjTy5IScknPikStSJXekRuqEE0WeySt5c7Tz4rw7H4vWnJPNHJM/cD5/AJYckSE=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Model T\\xe2\\x8c\\xa7\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <latexit sha1_base64=\"xKhTZkes5gD1IVF59I2pQ/hdWu0=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqHgqePFYoa2FNpTNdtOu3WzC7kQoof/BgxcVr/4fj/4bt20O2vpg4PHeDDPzgkQKg6777RTW1jc2t4rbpZ3dvf2D8uFR28SpZrzFYhnrTkANl0LxFgqUvJNoTqNA8odgfDvzH564NiJWTZwk3I/oUIlQMIpWajf7PaRpv1xxq+4cZJV4OalAjka//NUbxCyNuEImqTFdz03Qz6hGwSSflnqp4QllYzrkXUsVjbjxs/m1U3JmlQEJY21LIZmrvycyGhkziQLbGVEcmWVvJv7ndVMMr/1MqCRFrthiUZhKgjGZvU4GQnOGcmIJZVrYWwkbUU0Z2oBsBt7yx6ukfVH1Lqu1+1qlfpOnUYQTOIVz8OAK6nAHDWgBg0d4hld4c5Tz4rw7H4vWgpPPHMMfOJ8/+TmPAA==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Mask Output\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      H\\xe2\\x9c\\x93\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      <latexit sha1_base64=\"f0ZPzGZiJxncG2gTuzU4I2EYWEI=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqHgqeOmxgv2ANpTNdtMu3WzC7kQooX/CgxcVr/4dj/4bt20O2vpg4PHeDDPzgkQKg6777RQ2Nre2d4q7pb39g8Oj8vFJ28SpZrzFYhnrbkANl0LxFgqUvJtoTqNA8k4wuZ/7nSeujYjVI04T7kd0pEQoGEUrdRuDPo450kG54lbdBcg68XJSgRzNQfmrP4xZGnGFTFJjep6boJ9RjYJJPiv1U8MTyiZ0xHuWKhpx42eLe2fkwipDEsbalkKyUH9PZDQyZhoFtjOiODar3lz8z+ulGN76mVBJilyx5aIwlQRjMn+eDIXmDOXUEsq0sLcSNqaaMrQR2Qy81Y/XSfuq6l1Xaw+1Sv0uT6MIZ3AOl+DBDdShAU1oAQMJz/AKb07ivDjvzseyteDkM6fwB87nD2o6j9Q=</latexit>\\r\\n                 <latexit sha1_base64=\"waH8la2opelqfIRL3tJJyWmgWiU=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgqevFYwX5AG8pmO2mXbjZhdyKU0D/hwYuKV/+OR/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZeJUc2jyWMa6EzADUihookAJnUQDiwIJ7WB8N/PbT6CNiNUjThLwIzZUIhScoZU6t/0ejgBZv1xxq+4cdJV4OamQHI1++as3iHkagUIumTFdz03Qz5hGwSVMS73UQML4mA2ha6liERg/m987pWdWGdAw1rYU0rn6eyJjkTGTKLCdEcORWfZm4n9eN8Xw2s+ESlIExReLwlRSjOnseToQGjjKiSWMa2FvpXzENONoI7IZeMsfr5LWRdW7rNYeapX6TZ5GkZyQU3JOPHJF6uSeNEiTcCLJM3klb07ivDjvzseiteDkM8fkD5zPH2D+j84=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Generation\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          xc\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <latexit sha1_base64=\"ENutjdbeR27zYGiwwbUE1g4Iji0=\">AAAB8HicbVDLSgNBEJyNrxhfUY9eBoPgKexKUPEU8OIxgnlgsoTZSScZMju7zPRKwpK/8OBFxat/49G/cZLsQRMLGoqqbrq7glgKg6777eTW1jc2t/LbhZ3dvf2D4uFRw0SJ5lDnkYx0K2AGpFBQR4ESWrEGFgYSmsHoduY3n0AbEakHnMTgh2ygRF9whlZ6HHc7CGNM+bRbLLlldw66SryMlEiGWrf41elFPAlBIZfMmLbnxuinTKPgEqaFTmIgZnzEBtC2VLEQjJ/OL57SM6v0aD/SthTSufp7ImWhMZMwsJ0hw6FZ9mbif147wf61nwoVJwiKLxb1E0kxorP3aU9o4CgnljCuhb2V8iHTjKMNyWbgLX+8ShoXZe+yXLmvlKo3WRp5ckJOyTnxyBWpkjtSI3XCiSLP5JW8Odp5cd6dj0VrzslmjskfOJ8/l6CRIg==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   G\\xe2\\x9c\\x93\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   <latexit sha1_base64=\"UaesNKwLEajqVI0tpttUbVk/IB4=\">AAAB7nicbVBNS8NAEN3Ur1q/qh69LBbBU0mkqHgqeNBjBfsBbSib7aRdutmE3YlQQv+EBy8qXv07Hv03btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzaHJYxnrTsAMSKGgiQIldBINLAoktIPx7cxvP4E2IlaPOEnAj9hQiVBwhlbq3PV7OAJk/XLFrbpz0FXi5aRCcjT65a/eIOZpBAq5ZMZ0PTdBP2MaBZcwLfVSAwnjYzaErqWKRWD8bH7vlJ5ZZUDDWNtSSOfq74mMRcZMosB2RgxHZtmbif953RTDaz8TKkkRFF8sClNJMaaz5+lAaOAoJ5YwroW9lfIR04yjjchm4C1/vEpaF1Xvslp7qFXqN3kaRXJCTsk58cgVqZN70iBNwokkz+SVvDmJ8+K8Ox+L1oKTzxyTP3A+fwBosI/T</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Instance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       sc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   <latexit sha1_base64=\"UybyA/02kICKOl+UyCujOjkaa4c=\">AAAB8HicbVBNS8NAEN3Ur1q/qh69LBbBU0lEVDwVvHisYD+wDWWznbRLN5uwOxFL6L/w4EXFq//Go//GbZuDtj4YeLw3w8y8IJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqOTR4LGPdDpgBKRQ0UKCEdqKBRYGEVjC6mfqtR9BGxOoexwn4ERsoEQrO0EoPptdFeMKMT3rlilt1Z6DLxMtJheSo98pf3X7M0wgUcsmM6Xhugn7GNAouYVLqpgYSxkdsAB1LFYvA+Nns4gk9sUqfhrG2pZDO1N8TGYuMGUeB7YwYDs2iNxX/8zophld+JlSSIig+XxSmkmJMp+/TvtDAUY4tYVwLeyvlQ6YZRxuSzcBb/HiZNM+q3kX1/O68UrvO0yiSI3JMTolHLkmN3JI6aRBOFHkmr+TN0c6L8+58zFsLTj5zSP7A+fwBj+SRHQ==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Model T\\xef\\xa3\\xbf\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <latexit sha1_base64=\"sP2wB1f7zTzX/PAGw75Ko8vGgfc=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69BIvgqSRSVDwVvHis0C9oQ5lsN+3SzWbZ3Qgl9E948KLi1b/j0X/jts1BWx8MPN6bYWZeKDnTxvO+ncLG5tb2TnG3tLd/cHhUPj5p6yRVhLZIwhPVDVFTzgRtGWY47UpFMQ457YST+7nfeaJKs0Q0zVTSIMaRYBEjaKzUbQ76E5QSB+WKV/UWcNeJn5MK5GgMyl/9YULSmApDOGrd8z1pggyVYYTTWamfaiqRTHBEe5YKjKkOssW9M/fCKkM3SpQtYdyF+nsiw1jraRzazhjNWK96c/E/r5ea6DbImJCpoYIsF0Upd03izp93h0xRYvjUEiSK2VtdMkaFxNiIbAb+6sfrpH1V9a+rtcdapX6Xp1GEMziHS/DhBurwAA1oAQEOz/AKb450Xpx352PZWnDymVP4A+fzB27wj9c=</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 sc\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 <latexit sha1_base64=\"UybyA/02kICKOl+UyCujOjkaa4c=\">AAAB8HicbVBNS8NAEN3Ur1q/qh69LBbBU0lEVDwVvHisYD+wDWWznbRLN5uwOxFL6L/w4EXFq//Go//GbZuDtj4YeLw3w8y8IJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqOTR4LGPdDpgBKRQ0UKCEdqKBRYGEVjC6mfqtR9BGxOoexwn4ERsoEQrO0EoPptdFeMKMT3rlilt1Z6DLxMtJheSo98pf3X7M0wgUcsmM6Xhugn7GNAouYVLqpgYSxkdsAB1LFYvA+Nns4gk9sUqfhrG2pZDO1N8TGYuMGUeB7YwYDs2iNxX/8zophld+JlSSIig+XxSmkmJMp+/TvtDAUY4tYVwLeyvlQ6YZRxuSzcBb/HiZNM+q3kX1/O68UrvO0yiSI3JMTolHLkmN3JI6aRBOFHkmr+TN0c6L8+58zFsLTj5zSP7A+fwBj+SRHQ==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Segmentation\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\xef\\xa3\\xbf\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <latexit sha1_base64=\"KQXSi9ugrQS7nhOzZArzINSX990=\">AAAB7HicbVBNS8NAEJ3Ur1q/qh69BIvgqSRS1JMUvHisYD+gDWWy3bRrN5tldyOU0P/gwYuKV/+PR/+N2zYHbX0w8Hhvhpl5oeRMG8/7dgpr6xubW8Xt0s7u3v5B+fCopZNUEdokCU9UJ0RNORO0aZjhtCMVxTjktB2Ob2d++4kqzRLxYCaSBjEOBYsYQWOlVm+MUmK/XPGq3hzuKvFzUoEcjX75qzdISBpTYQhHrbu+J02QoTKMcDot9VJNJZIxDmnXUoEx1UE2v3bqnlll4EaJsiWMO1d/T2QYaz2JQ9sZoxnpZW8m/ud1UxNdBxkTMjVUkMWiKOWuSdzZ6+6AKUoMn1iCRDF7q0tGqJAYG5DNwF/+eJW0Lqr+ZbV2X6vUb/I0inACp3AOPlxBHe6gAU0g8AjP8ApvjnBenHfnY9FacPKZY/gD5/MHExqPFA==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Mask\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Dc\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <latexit sha1_base64=\"u1I1c6T52TInSm1j7l9IIKl8RZg=\">AAAB/HicbVDLSsNAFJ3UV62v+Ni5CRbBVUmkqLgq6MJlBfuAJoTJdNoOnUzCzI1YQ/wWF25U3PofLv0bJ20W2npg4HDOvdwzJ4g5U2Db30ZpaXllda28XtnY3NreMXf32ipKJKEtEvFIdgOsKGeCtoABp91YUhwGnHaC8VXud+6pVCwSdzCJqRfioWADRjBoyTcP3BDDiGCeXme+C/QBUpL5ZtWu2VNYi8QpSBUVaPrml9uPSBJSAYRjpXqOHYOXYgmMcJpV3ETRGJMxHtKepgKHVHnpNH1mHWulbw0iqZ8Aa6r+3khxqNQkDPRknlXNe7n4n9dLYHDhpUzECVBBZocGCbcgsvIqrD6TlACfaIKJZDqrRUZYYgK6MN2BM//jRdI+rTlntfptvdq4LNooo0N0hE6Qg85RA92gJmohgh7RM3pFb8aT8WK8Gx+z0ZJR7OyjPzA+fwC+75Wx</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Instance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Model\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Model\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Predictor P\\xe2\\x9c\\x93\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              <latexit sha1_base64=\"uAPAp4KTN4ZFlGAbt9hylC/kgXE=\">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKqHgKePEYwTwgWcLspDcZMvtgplcIIT/hwYuKV3/Ho3/jJNmDJhY0FFXddHcFqZKGXPfbKaytb2xuFbdLO7t7+wflw6OmSTItsCESleh2wA0qGWODJClspxp5FChsBaO7md96Qm1kEj/SOEU/4oNYhlJwslK73uvSEIn3yhW36s7BVomXkwrkqPfKX91+IrIIYxKKG9Px3JT8CdckhcJpqZsZTLkY8QF2LI15hMafzO+dsjOr9FmYaFsxsbn6e2LCI2PGUWA7I05Ds+zNxP+8TkbhjT+RcZoRxmKxKMwUo4TNnmd9qVGQGlvChZb2ViaGXHNBNiKbgbf88SppXlS9q+rlw2WldpunUYQTOIVz8OAaanAPdWiAAAXP8ApvTuq8OO/Ox6K14OQzx/AHzucPdoqP3A==</latexit>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Updater\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Update\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Instance Memory                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Signal\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2. Overview of our entire online tracking pipeline used for inference, see Sec 3.1.\\r\\n\\r\\n\\r\\nthe few-shot learning based model prediction proposed in LWL [5] to produce\\r\\naccurate segmentation masks. Furthermore, D3S proposes to simply concatenate\\r\\nthe outputs of both modules whereas we learn a localization encoding to con-\\r\\ndition the segmentation mask decoding based on the localization information.\\r\\nCompared to D3S we update not only the instance localization but also the seg-\\r\\nmentation models and memories. Hence, our method integrates specific memory\\r\\nmanagement components.\\r\\n\\r\\n\\r\\n3      Method\\r\\n\\r\\n3.1       Overview\\r\\n\\r\\nVideo object segmentation methods can produce high quality segmentation masks\\r\\nbut are typically not robust enough for video object tracking. Robustness be-\\r\\ncomes vital for medium and long sequences, which are most prevalent in track-\\r\\ning datasets [16,35]. In such scenarios, the target object frequently undergoes\\r\\nsubstantial appearance changes, also occlusions and similarly looking objects\\r\\nare common. Hence, we propose to adapt a typical VOS approach with track-\\r\\ning components to increase its robustness. In particular, we base our approach\\r\\non the Learning What to Learn (LWL) [5] method and design a novel and\\r\\nsegmentation-centric tracking pipeline that estimates accurate object masks in-\\r\\nstead of bounding boxes. During inference a segmentation mask is typically not\\r\\nprovided in visual object tracking. Hence, we use STA [59] to generate the re-\\r\\nquired initial segmentation mask from the provided initial bounding box. An\\r\\noverview of our tracking method RTS is shown in Fig. 2. Our pipeline consists\\r\\nof a backbone network, a segmentation branch, an instance localization branch\\r\\nand a segmentation decoder. For each video frame, the backbone first extracts a\\r\\nfeature map xb . These features are further processed into segmentation features\\r\\nxs and classification features xc to serve as input of their respective branch.\\r\\n\\x0c\\n\\n6      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\nThe segmentation branch is designed to capture the details of the object with a\\r\\nhigh dimensional mask encoding whereas the instance localization branch aims\\r\\nat providing a coarser but robust score map representing the target location.\\r\\nBoth branches contain online learned components that are trained on memories\\r\\n(Ds and Dc ) that store features and predictions of past frames. The instance\\r\\nlocalization branch has two purposes: it is used to control the updating of both\\r\\nmodels and memories and it is used to condition the segmentation mask decod-\\r\\ning based on a learned score encoding produced by H\\xce\\xb8 in order to inject instance\\r\\nlocalization information. The obtained segmentation scores and the raw instance\\r\\nmodel score map are then used to generate the final segmentation mask output.\\r\\n\\r\\n\\r\\n3.2   Segmentation Branch\\r\\n\\r\\nThe architecture of the segmentation branch is adopted from LWL [5], and we\\r\\nbriefly review it here. It consists of a segmentation sample memory Ds , a label\\r\\ngenerator E\\xce\\xb8 , a weight predictor W\\xce\\xb8 , a few-shot learner A\\xce\\xb8 and a segmentation\\r\\nmodel T\\xcf\\x84 . The goal of the few-shot learner A\\xce\\xb8 is producing the parameters \\xcf\\x84 of\\r\\nthe segmentation model T\\xcf\\x84 such that the obtained mask encoding xm contains\\r\\nthe information needed to compute the final segmentation mask of the target\\r\\nobject. The label mask encodings used by the few-shot learner are predicted by\\r\\nthe label generator E\\xce\\xb8 .\\r\\n    The few-shot learner is formulated through the following optimization prob-\\r\\nlem, which is unrolled through steepest descent iterations in the network\\r\\n\\r\\n                     1      X                                            \\x01 2       \\xce\\xbbs\\r\\n         Ls (\\xcf\\x84 ) =                      W\\xce\\xb8 (ys ) \\xc2\\xb7 T\\xcf\\x84 (xs ) \\xe2\\x88\\x92 E\\xce\\xb8 (ys )         +      k\\xcf\\x84 k2 ,   (1)\\r\\n                     2                                                             2\\r\\n                         (xs ,ys )\\xe2\\x88\\x88Ds\\r\\n\\r\\nwhere Ds corresponds to the segmentation memory, xs denotes the segmentation\\r\\nfeatures, ys the segmentation masks and \\xce\\xbbs is a learnable scalar regularization\\r\\nparameter. The weight predictor W\\xce\\xb8 produces sample confidence weights for\\r\\neach spatial location in each memory sample. Applying the optimized model\\r\\nparameters \\xcf\\x84 \\xe2\\x88\\x97 within the segmentation model produces the mask encoding xm =\\r\\nT\\xcf\\x84 \\xe2\\x88\\x97 (xs ) for the segmentation features xs .\\r\\n     LWL [5] feeds the mask encoding directly into the segmentation decoder to\\r\\nproduce the segmentation mask. For long and challenging tracking sequences,\\r\\nonly relying on the mask encoding may lead to an accurate segmentation mask,\\r\\nbut often for the wrong object in the scene (see Fig 1). Since LWL [5] is only\\r\\nable to identify the target to a certain degree in challenging tracking sequences,\\r\\nwe propose to condition the mask encoding based on an instance localization\\r\\nrepresentation, described next.\\r\\n\\r\\n\\r\\n3.3   Instance Localization Branch\\r\\n\\r\\nThe previously described segmentation branch can produce accurate segmenta-\\r\\ntion masks but typically lacks the necessary robustness for tracking in medium\\r\\n\\x0c\\n\\n                                       Robust Visual Tracking by Segmentation      7\\r\\n\\r\\nor long-term sequences. Especially challenging are sequences where objects simi-\\r\\nlar to the target appear, where the target object is occluded or vanishes from the\\r\\nscene for a short time. Therefore, we propose a dedicated branch for target in-\\r\\nstance localization, in order to robustly identify the target among distractors or\\r\\nto detect occlusions. A powerful tracking paradigm that learns a target specific\\r\\nappearance model on both foreground and background information are discrim-\\r\\ninative correlation filters (DCF) [6,22,13,3]. These methods learn the weights of\\r\\na filter that differentiates foreground from background pixels represented by a\\r\\nscore map, where the maximal value corresponds to the target\\xe2\\x80\\x99s center.\\r\\n    Similar to the segmentation branch, we propose an instance localization\\r\\nbranch that consists of a sample memory Dc and a model predictor P\\xce\\xb8 . The\\r\\nlatter predicts the parameters \\xce\\xba of the instance model T\\xce\\xba . The instance model\\r\\nis trained online to produce the target score map used to localize the target\\r\\nobject. To obtain the instance model parameters \\xce\\xba we minimize the following\\r\\nloss function\\r\\n\\r\\n                               X                            \\x01 2       \\xce\\xbbc\\r\\n                 Lc (\\xce\\xba) =                  R T\\xce\\xba (xc ), yc         +      k\\xce\\xbak2 ,   (2)\\r\\n                                                                      2\\r\\n                            (xc ,yc )\\xe2\\x88\\x88Dc\\r\\n\\r\\n\\r\\nwhere Dc corresponds to the instance memory containing the classification fea-\\r\\ntures xc and the Gaussian labels yc . R denotes the robust hinge-like loss [3]\\r\\nand \\xce\\xbbc is a fixed regularization parameter. To solve the optimization problem\\r\\nwe apply the method from [3], which unrolls steepest descent iterations of the\\r\\nGauss-Newton approximation of (2) to obtain the final model parameters \\xce\\xba\\xe2\\x88\\x97 .\\r\\nThe score map can then be obtained with sc = T\\xce\\xba\\xe2\\x88\\x97 (xc ) by evaluating the target\\r\\nmodel on the classification features xc .\\r\\n\\r\\n\\r\\n\\r\\n3.4   Instance-Conditional Segmentation Decoder\\r\\n\\r\\n\\r\\nIn video object segmentation the produced mask encoding is directly fed into\\r\\nthe segmentation decoder to generate the segmentation mask. However, solely\\r\\nrelying on the mask encoding is not robust enough for the challenging tracking\\r\\nscenario, see Fig 1. Thus, we propose to integrate the instance localization in-\\r\\nformation into the segmentation decoding procedure. In particular, we condition\\r\\nthe mask encoding on a learned encoding of the instance localization score map.\\r\\n    First, we encode the raw score maps using a multi-layer Convolutional Neural\\r\\nNetwork (CNN) to learn a suitable representation. Secondly, we simply condition\\r\\nthe mask encoding with the learned representation via an element-wise addition.\\r\\nThe entire conditioning procedure can be defined as xf = xm + H\\xce\\xb8 (sc ), where\\r\\nH\\xce\\xb8 denotes the CNN encoding the scores sc , and xm the mask encoding. The\\r\\nresulting features are then fed into the segmentation decoder that produces the\\r\\nsegmentation scores of the target object.\\r\\n\\x0c\\n\\n8       Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n3.5   Jointly Learning Instance Localization and Segmentation\\r\\n\\r\\nIn this section, we describe our general training strategy and parameters. In\\r\\nparticular, we further detail the segmentation and classification losses that we\\r\\nuse for offline training.\\r\\nSegmentation Loss First, we randomly sample J frames from an annotated\\r\\nvideo sequence and sort them according to their frame ids in increasing order to\\r\\nconstruct the training sequence V = {(xjb , ysj , ycj )}J\\xe2\\x88\\x921          j        j\\r\\n                                                        j=0 , where xb = B\\xce\\xb8 (I ) are\\r\\n                                              j                             j\\r\\nthe extracted features of the video frame I using the backbone B\\xce\\xb8 , ys is the\\r\\ncorresponding segmentation mask and ysj denotes the Gaussian label at the tar-\\r\\nget\\xe2\\x80\\x99s center location. We start with entry v0 \\xe2\\x88\\x88 V and store it in the segmentation\\r\\nDs and instance memory Dc and obtain parameters \\xcf\\x84 0 and \\xce\\xba0 of the segmenta-\\r\\ntion and instance model. We use these parameters to compute the segmentation\\r\\nloss for v1 \\xe2\\x88\\x88 V. Using the predicted segmentation mask we update the segmen-\\r\\ntation model parameters to \\xcf\\x84 1 but keep the instance model parameters fixed.\\r\\nWe use this procedure since the segmentation parameters typically need to be\\r\\nupdated frequently to enable accurate segmentation. Conversely, we train the\\r\\nmodel predictor to produce instance model parameters only on a single frame\\r\\nthat generalize to multiple unseen future frames in order to ensure robust target\\r\\nlocalization. The resulting segmentation loss for the entire sequence V can thus\\r\\nbe described as follows\\r\\n                         J\\xe2\\x88\\x921     \\x12 \\x10                                    \\x13\\r\\n                         X                                        \\x01\\x11\\r\\n           Lseq\\r\\n            s (\\xce\\xb8; V) =         Ls D\\xce\\xb8 T\\xcf\\x84 j\\xe2\\x88\\x921 (xjs ) + H\\xce\\xb8 T\\xce\\xba0 (xjc ) , ysj ,      (3)\\r\\n                         j=1\\r\\n\\r\\n\\r\\nwhere xs = F\\xce\\xb8 (xb ) and xc = G\\xce\\xb8 (xb ) and Ls is the Lovasz segmentation loss [1].\\r\\nClassification Loss Instead of training our tracker only with the segmentation\\r\\nloss, we add an auxiliary loss to ensure that the instance module produces score\\r\\nmaps localizing the target via a Gaussian distribution. Generating such a score\\r\\nmap is important because we directly use it to update the segmentation and\\r\\ninstance memories and to generate the final output. As explained before, we\\r\\nuse only the first training v0 \\xe2\\x88\\x88 V to optimize the instance model parameters.\\r\\nInstead of using only the parameters corresponding to the final iteration Niter\\r\\nof the optimization method \\xce\\xba0(Niter ) explained in Sec. 3.3 we use all intermediate\\r\\nparameters \\xce\\xba0(i) to compute the loss to encourage fast convergence. The final\\r\\ntarget classification loss for the whole sequence V is defined as follows\\r\\n                               J\\xe2\\x88\\x921        Niter\\r\\n                                                                      !\\r\\n                   seq\\r\\n                               X      1 X         \\x10\\r\\n                                                            j     j\\r\\n                                                                    \\x11\\r\\n                 Lc (\\xce\\xb8; V) =                    Lc T\\xce\\xba0(i) (xc ), yc     ,       (4)\\r\\n                               j=1\\r\\n                                    Niter i=0\\r\\n\\r\\nwhere Lc is the hinge loss defined in [3]. To train our tracker we combine the\\r\\nsegmentation and classification losses using the scalar weight \\xce\\xb7 and minimize\\r\\nboth losses jointly\\r\\n\\r\\n                     Lseq           seq             seq\\r\\n                      tot (\\xce\\xb8; V) = Ls (\\xce\\xb8; V) + \\xce\\xb7 \\xc2\\xb7 Lc (\\xce\\xb8; V).                   (5)\\r\\n\\x0c\\n\\n                                    Robust Visual Tracking by Segmentation         9\\r\\n\\r\\nTraining Details We use the train sets of LaSOT [16], GOT-10k [24], Youtube-\\r\\nVOS [52] and DAVIS [40]. For VOT datasets that only provide annotated bound-\\r\\ning boxes, we use these boxes and STA [59] to generate segmentation masks and\\r\\ntreat them as ground truth annotations during training. STA [59] is trained sep-\\r\\narately on YouTube-VOS 2019 [52] and DAVIS 2017 [38]. For our model, we\\r\\nuse ResNet-50 with pre-trained MaskRCNN weights as our backbone and ini-\\r\\ntialize the segmentation model and decoder weights with the ones available from\\r\\nLWL [5]. We train for 200 epochs and sample 15\\xe2\\x80\\x99000 videos per epoch, which\\r\\ntakes 96 hours to train on a single Nvidia A100 GPU. We use the ADAM [26]\\r\\noptimizer with a learning rate decay of 0.2 at epochs 25, 115 and 160. We weight\\r\\nthe losses such that the segmentation loss is predominant but in the same range\\r\\nas the classification. We empirically choose \\xce\\xb7 = 10. Further details about training\\r\\nand the network architecture are given in the appendix.\\r\\n\\r\\n\\r\\n3.6   Inference\\r\\n\\r\\nIn this section we describe our inference approach, used during tracking.\\r\\nMemory Management and Model Updating Our tracker consists of two\\r\\ndifferent memories. The segmentation memory stores segmentation features and\\r\\npredicted segmentation masks of previous frames. In contrast, the instance mem-\\r\\nory contains classification features and Gaussian labels marking the center lo-\\r\\ncation of the target in the predicted segmentation mask of the previous video\\r\\nframe. The quality of the predicted labels directly influences the localization\\r\\nand segmentation quality in future video frames. Hence, it is crucial to avoid\\r\\ncontaminating the memories with wrong predictions not corresponding to the\\r\\nactual target. We propose the following strategy to keep the memory as clean\\r\\nas possible. (a) If the instance model is able to clearly localize the target (maxi-\\r\\nmum value in the score map larger than tsc = 0.3) and the segmentation model\\r\\nconstructs a valid segmentation mask (at least one pixel above tss = 0.5) we up-\\r\\ndate both memories with the current predictions and features. (b) If either the\\r\\ninstance localization or segmentation fail to identify the target we omit updating\\r\\nthe segmentation memory. (c) If only the segmentation mask fails to represent\\r\\nthe target but the instance model can localize it, we update the instance memory\\r\\nonly. (d) If instance localization fails we omit updating either memory. Further,\\r\\nwe trigger the few-shot learner and model predictor after 20 frames have passed\\r\\nbut only if the corresponding memory is updated.\\r\\nFinal Mask Output Generation We achieve the final segmentation mask by\\r\\nsimply thresholding the segmentation decoder output. To obtain the bounding\\r\\nbox required for standard tracking benchmarks, we simply report the smallest\\r\\naxis-aligned box that contains the entire estimated object mask.\\r\\nInference Details We set the input image resolution such that the segmenta-\\r\\ntion learner features have a resolution of 52 \\xc3\\x97 30 (stride 16), while the instance\\r\\nlearner operates of features of size 26 \\xc3\\x97 15 (stride 32). The learning rate is set to\\r\\n0.1 and 0.01 for the segmentation and instance learner respectively. We use a size\\r\\n\\x0c\\n\\n10      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\nof 32 frames for the segmentation memory and 50 frames for the instance mem-\\r\\nory. We keep the samples corresponding to the initial frame in both memories\\r\\nand replace the oldest entries if the memory is full. We update both memories\\r\\nfor the first 100 video frames and afterwards only after every 20th frame. We\\r\\nrandomly augment the sample corresponding to the initial frame with vertical\\r\\nflip, random translation and blurring.\\r\\n\\r\\n\\r\\n4     Evaluation\\r\\n\\r\\nOur approach is developed within the PyTracking [11] framework. The imple-\\r\\nmentation is done with PyTorch 1.9 with CUDA 11.1. Our model is evaluated\\r\\non a single Nvidia GTX 2080Ti GPU. Our method runs on average at 30 FPS\\r\\non LaSOT [16]. The code will be made available upon publication. Each number\\r\\ncorresponds to the average of five runs with different random seeds.\\r\\n\\r\\n\\r\\n4.1   Branch Ablation Study\\r\\n\\r\\nFor the ablation study, we analyze the impact of the instance branch on three\\r\\ndatasets and present the results in Tab. 1. First, we report the performance of\\r\\nLWL [5] since we build upon it to design our final tracking pipeline. We use\\r\\nthe network weights provided by Bhat et al. [5] and the corresponding infer-\\r\\nence settings. We input the same segmentation masks obtained from the initial\\r\\nbounding box for LWL as used for our method. We observe that LWL is not\\r\\nrobust enough for challenging tracking scenarios. The second row in Tab. 1 cor-\\r\\nresponds to our method but we omit the proposed instance branch. Hence, we\\r\\nuse the proposed inference components and settings and train the tracker as ex-\\r\\nplained in Sec. 3.5 but remove the conditioning. We observe that even without\\r\\nthe instance localization branch our tracker can achieve competitive performance\\r\\non all three datasets (e.g. +5.6% on LaSOT) but fully integrating the instance\\r\\nlocalization branch increases the performance even more (e.g. +4.4 on LaSOT).\\r\\nThus, we conclude that adapting the baseline method to the tracking domain\\r\\nimproves the tracking performance but to boost the performance and achieve\\r\\nstate-of-the-art results an additional component able to increase the tracking\\r\\nrobustness is required.\\r\\n\\r\\n\\r\\n                         Seg.  Inst. Branch   LaSOT [16]       NFS [19]        UAV123 [35]\\r\\n                        Branch Conditioning AUC P     NP    AUC P     NP     AUC P     NP\\r\\nLWL [5]                   3        -       59.7 60.6 63.3   61.5 75.1 76.9   59.7 78.8 71.4\\r\\nRTS (No Inst. Branch)     3        7       65.3 68.5 71.5   65.8 84.0 85.0   65.2 85.6 78.8\\r\\nRTS                       3        3       69.7 73.7 76.2   65.4 82.8 84.0   67.6 89.4 81.6\\r\\n\\r\\nTable 1. Comparison between our segmentation network baseline LWL and our\\r\\npipeline, with and without Instance conditioning on different VOT datasets.\\r\\n\\x0c\\n\\n                                       Robust Visual Tracking by Segmentation              11\\r\\n\\r\\nInst. Branch                  LaSOT [16]             NFS [19]              UAV123 [35]\\r\\n  Fallback      tsc    AUC       P     NP     AUC     P       NP    AUC       P     NP\\r\\n      7         0.30   69.3     73.1   75.9   65.3    82.7   84.0   66.3     87.2   80.4\\r\\n      3         0.30   69.7     73.7   76.2   65.4    82.8   84.0   67.6     89.4   81.6\\r\\n\\r\\n      3         0.20   68.6     72.3   75.0   65.3    82.7   83.9   67.0     88.7   80.7\\r\\n      3         0.30   69.7     73.7   76.2   65.4    82.8   84.0   67.6     89.4   81.6\\r\\n      3         0.40   69.1     72.7   75.6   63.3    79.7   81.7   67.1     89.1   80.7\\r\\n\\r\\nTable 2. Ablation on inference strategies. The first column analyzes the effect of using\\r\\nthe instance branch as fallback for target localization if the segmentation branch is\\r\\nunable to detect the target (max(ss ) < tss ). The second column shows the impact of\\r\\ndifferent confidence thresholds tsc .\\r\\n\\r\\n\\r\\n4.2       Inference Parameters\\r\\n\\r\\nIn this part we ablate two key aspects of our inference strategy. First, we study\\r\\nthe effect of relying on the instance branch if the segmentation decoder is unable\\r\\nto localize the target (max(ss ) < tss ). Secondly, we study different values for tsc\\r\\nthat determines whether the target is detected by the instance model, see Tab. 2.\\r\\n    We observe, that using the instance branch if the segmentation branch cannot\\r\\nidentify the target improves the tracking performance on all datasets (e.g. +1.3%\\r\\non UAV123). Furthermore, Tab. 2 shows that our tracking pipeline achieves the\\r\\nbest performance when setting tsc = 0.3 whereas smaller or larger values for\\r\\ntsc decrease the tracking accuracy. Hence, it is important to find a suitable\\r\\ntrade-off between frequently updating the model and memory to quickly adapt\\r\\nto appearance changes and updating only rarely to avoid contaminating the\\r\\nmemory and model based on wrong predictions.\\r\\n\\r\\n\\r\\n4.3       Comparison to the state of the art\\r\\n\\r\\nWe compare our approach on six VOT benchmarks and validate the segmen-\\r\\ntation masks quality on two VOS datasets because assessing the segmentation\\r\\naccuracy on tracking datasets is not possible since only bounding box annota-\\r\\ntions are provided.\\r\\nLaSOT [16] We evaluate our method on the test set of the LaSOT dataset\\r\\nwhich consists of 280 sequences with 2500 frames on average. Thus, the bench-\\r\\nmark challenges the long term adaptability and robustness of trackers. Fig. 3\\r\\nshows the success plot reporting the overlap precision OP with respect to the\\r\\noverlap threshold T . Trackers are ranked by AUC score. In addition, Tab. 3 re-\\r\\nports the precision and normalized precision for all compared methods. Our\\r\\nmethod outperforms the state-of-the-art trackers KeepTrack [34] and Stark-\\r\\nST101 [54] by a large margin (+2.6% AUC). Our method is not only as robust\\r\\nas KeepTrack (see the success plot for T < 0.2) but also estimates far more\\r\\naccurate bounding boxes than any tracker (0.8 < T < 1.0).\\r\\nGOT-10k [24] The large-scale GOT-10k dataset contains over 10.000 shorter\\r\\nsequences. Since we train our method on several datasets instead of only on\\r\\n\\x0c\\n\\n12           Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n                                     90\\r\\n                                                                 Success plot                                                                90\\r\\n                                                                                                                                                                  Precision plot\\r\\n                                     80                                                                                                      80\\r\\n                                     70                                                                                                      70\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                    Distance Precision [%]\\r\\n             Overlap Precision [%]\\r\\n                                     60            RTS [69.7]                                                                                60                                   RTS [73.7]\\r\\n                                                   KeepTrack [67.1]                                                                                                              STARK-ST101 [72.2]\\r\\n                                     50            STARK-ST101 [67.1]                                                                        50                                  STARK-ST50 [71.2]\\r\\n                                                   STARK-ST50 [66.4]                                                                                                             KeepTrack [70.2]\\r\\n                                     40            AlphaRefine [65.9]                                                                        40                                  TransT [69.0]\\r\\n                                                   TransT [64.9]                                                                                                                 AlphaRefine [68.8]\\r\\n                                     30            Siam R-CNN [64.8]                                                                         30                                  Siam R-CNN [68.4]\\r\\n                                                   TrDiMP [63.9]                                                                                                                 TrDiMP [66.3]\\r\\n                                     20            Super DiMP [63.1]                                                                         20                                  Super DiMP [65.3]\\r\\n                                                   STMTrack [60.6]                                                                                                               STMTrack [63.3]\\r\\n                                     10            PrDiMP50 [59.8]                                                                           10                                  PrDiMP50 [60.8]\\r\\n                                                   LWL [59.7]                                                                                                                    LWL [60.6]\\r\\n                                      00              0.2         0.4       0.6          0.8        1                                         00         10         20        30         40         50\\r\\n                                                                Overlap threshold                                                                         Location error threshold [pixels]\\r\\n\\r\\n\\r\\nFig. 3. Success (left) and Precision (right) plots on LaSOT [16] with other state-of-\\r\\nthe-art methods. The AUCs for all methods are ordered and reported in the legend.\\r\\nOur method outperforms all existing approaches, both in Overlap Precision (left) and\\r\\nDistance Precision (right).\\r\\n\\r\\n                                          Keep STARK Alpha            Siam   Tr Super STM Pr         DM\\r\\n                                      RTS Track ST-101 Refine TransT R-CNN DiMP DiMP Track DiMP LWL Track LTMU DiMP Ocean D3S\\r\\n                                           [34]  [54]   [53]    [8]    [46] [48] [11] [18] [14]  [5] [58] [10]  [3]  [57] [31]\\r\\nPrecision     73.7 70.2                                         72.2      68.8      69.0          68.4     66.3    65.3                           63.3   60.8     60.6     59.7     57.2       56.7      56.6   49.4\\r\\nNorm. Prec    76.2 77.2                                         76.9      73.8      73.8          72.2     73.0    72.2                           69.3   68.8     63.3     66.9     66.2       65.0      65.1   53.9\\r\\nSuccess (AUC) 69.7 67.1                                         67.1      65.9      64.9          64.8     63.9    63.1                           60.6   59.8     59.7     58.4     57.2       56.9      56.0   49.2\\r\\n\\xe2\\x88\\x86 AUC to Ours                              -       \\xe2\\x86\\x912.6         \\xe2\\x86\\x912.6     \\xe2\\x86\\x913.8       \\xe2\\x86\\x914.8         \\xe2\\x86\\x914.9      \\xe2\\x86\\x915.8 \\xe2\\x86\\x916.6 \\xe2\\x86\\x919.1 \\xe2\\x86\\x919.9 \\xe2\\x86\\x9110.0 \\xe2\\x86\\x9111.3 \\xe2\\x86\\x9112.5 \\xe2\\x86\\x9112.8 \\xe2\\x86\\x9113.7 \\xe2\\x86\\x9120.5\\r\\n\\r\\nTable 3. Comparison to the state of the art on the LaSOT [16] test set in terms of\\r\\nAUC score. The methods are ordered by AUC score.\\r\\n\\r\\n                                               RTS                   STA                 LWL                PrDiMP-50                                         DiMP-50                  SiamRPN++\\r\\n                                                                     [59]                 [5]                  [14]                                             [3]                        [28]\\r\\nSR0.50 (%)                                     94.5                 95.1                  92.4                    89.6                                           88.7                          82.8\\r\\nSR0.75 (%)                                     82.6                 85.2                  82.2                    72.8                                           68.8                           -\\r\\nAO(%)                                          85.2                 86.7                  84.6                    77.8                                           75.3                          73.0\\r\\n\\r\\nTable 4. Results on the GOT-10k validation set [24] in terms of Average Overlap\\r\\n(AO) and success rates (SR) for overlap thresholds of 0.5 and 0.75.\\r\\n\\r\\n                                               Keep STARK STARK                 Siam Alpha STM         Tr Super Pr\\r\\n                                           RTS Track ST101 ST50 STA LWL TransT R-CNN Refine Track DTT DiMP DiMP DiMP D3S\\r\\n                                                [34]  [54]  [54] [59] [5] [8]    [46] [53] [18] [55] [48] [11] [14] [31]\\r\\nPrecision     79.4 73.8                                             -              -           79.1 78.4    80.3                        80.0             78.3     76.7 78.9 73.1               73.3      70.4 66.4\\r\\nNorm. Prec    86.0 83.5                                            86.9           86.1         84.7 84.4    86.7                        85.4             85.6     85.1 85.0 83.3               83.5      81.6 76.8\\r\\nSuccess (AUC) 81.6 78.1                                            82.0           81.3         81.2 80.7    81.4                        81.2             80.5     80.3 79.6 78.4               78.1      75.8 72.8\\r\\n\\xe2\\x88\\x86 AUC to Ours                                  -      \\xe2\\x86\\x913.5         \\xe2\\x86\\x930.4           \\xe2\\x86\\x910.3 \\xe2\\x86\\x910.4 \\xe2\\x86\\x910.9 \\xe2\\x86\\x910.2                                   \\xe2\\x86\\x910.4             \\xe2\\x86\\x911.1 \\xe2\\x86\\x911.3 \\xe2\\x86\\x912.0 \\xe2\\x86\\x913.2 \\xe2\\x86\\x913.5 \\xe2\\x86\\x915.8 \\xe2\\x86\\x918.8\\r\\n\\r\\nTable 5. Comparison to the state of the art on the TrackingNet [36] test set in terms\\r\\nof AUC scores, Precision and Normalized precision.\\r\\n\\r\\nGOT-10k train, we evaluate our approach on the val set only, which consists of\\r\\n180 short videos. We compile the results in Tab. 4. Our method ranks second\\r\\nfor all metrics, falling between two segmentation oriented methods, +0.6% over\\r\\nLWL [5] and \\xe2\\x88\\x921.5% behind STA [59]. Note, that our tracker outperforms other\\r\\ntracking methods by a large margin.\\r\\nTrackingNet [36] We compare our approach on the test set of the Track-\\r\\ningNet dataset, consisting of 511 sequences. Tab. 5 shows the results obtained\\r\\nfrom the online evaluation server. Our method outperforms most of the existing\\r\\n\\x0c\\n\\n                                                 Robust Visual Tracking by Segmentation                                      13\\r\\n\\r\\n        RTS Keep        STARK              STARK Super Pr STM Siam Siam\\r\\n            Track CRACT ST101 TrDiMP TransT ST50 DiMP DiMP Track AttN R-CNN KYS DiMP LWL\\r\\n             [34]  [17]   [54]  [48]   [8]   [54] [11] [14] [18] [56]  [46]  [4] [3]  [5]\\r\\nUAV123 67.6 69.7     66.4     68.2     67.5      69.1     69.1      67.7   68.0     64.7   65.0    64.9     \\xe2\\x80\\x93     65.3    59.7\\r\\nNFS    65.4 66.4     62.5     66.2     66.2      65.7     65.2      64.8   63.5      \\xe2\\x80\\x93      \\xe2\\x80\\x93      63.9    63.5   62.0    61.5\\r\\nTable 6. Comparison with state-of-the-art on the UAV123 [35] and NFS [19] datasets\\r\\nin terms of AUC score.\\r\\n                    STARK STARK-\\r\\n                     ST-50 ST-101-                         Ocean       Fast   Alpha\\r\\n                RTS +AR     +AR          LWL       STA      Plus      Ocean   Refine       RPT     AFOD      D3S         STM\\r\\n                      [54]   [54]         [27]     [59]     [27]       [27]    [27]        [27]     [27]     [27]         [27]\\r\\nRobustness      0.845 0.817    0.789     0.798    0.824     0.842     0.803       0.777    0.869   0.795     0.769       0.574\\r\\nAccuracy        0.710 0.759    0.763     0.719    0.732     0.685     0.693       0.754    0.700   0.713     0.699       0.751\\r\\nEAO             0.506 0.505    0.497     0.463    0.510     0.491     0.461       0.482    0.530   0.472     0.439       0.308\\r\\n\\xe2\\x88\\x86 EAO to Ours    -   \\xe2\\x86\\x910.001 \\xe2\\x86\\x910.009 \\xe2\\x86\\x910.043 \\xe2\\x86\\x930.004 \\xe2\\x86\\x910.015 \\xe2\\x86\\x910.045 \\xe2\\x86\\x910.024 \\xe2\\x86\\x930.024 \\xe2\\x86\\x910.034 \\xe2\\x86\\x910.067 \\xe2\\x86\\x910.198\\r\\n\\r\\nTable 7. Results on the VOT2020-ST [27] challenge in terms of Expected Average\\r\\nOverlap (EAO), Accuracy and Robustness.\\r\\napproaches and ranks second in terms of AUC, close behind STARK-ST101 [54]\\r\\nwhich is based on a ResNet-101 backbone. Note, that we outperform STARK-\\r\\nST50 [54] that uses a ResNet-50 as backbone. We want to highlight that we\\r\\nachieve a higher precision score than other methods that produce a segmenta-\\r\\ntion mask output such as LWL [5], STA [59], Alpha-Refine [53] and D3S [31].\\r\\nUAV123 [35] The UAV dataset consists of 123 test videos that contain small\\r\\nobjects, target occlusion, and distractors. Small objects are particularly chal-\\r\\nlenging in a segmentation setup. Tab. 6 shows the achieved results in terms of\\r\\nsuccess AUC. Our method achieves competitive results on UAV123 similar to\\r\\nTrDiMP [48] or SuperDiMP [11]. Note, that it outperforms LWL [5] by a large\\r\\nmargin.\\r\\nNFS [19] The NFS dataset (30FPS version) contains 100 test videos with fast\\r\\nmotions and challenging sequences with distractors. Our method achieves an\\r\\nAUC score that is only 1% below the current best method KeepTrack [34] while\\r\\noutperforming numerous other trackers, including STARK-ST50 [54] (+0.2) Su-\\r\\nperDiMP [3] (+0.6) and PrDiMP [14] (+1.9).\\r\\nVOT 2020 [27] Finally, we evaluate our method on the VOT2020 short-term\\r\\nchallenge. It consists of 60 videos and provides segmentation mask annotations.\\r\\nFor the challenge, the multi-start protocol is used and the tracking performance\\r\\nis assessed based on accuracy and robustness. We compare with the top meth-\\r\\nods on the leader board and include more recent methods in Tab. 7. In this\\r\\nsituation, our method ranks 2nd in Robustness, thus outperforming most of the\\r\\nother methods. In particular we achieve a higher EAO score than STARK [54],\\r\\nLWL [5], AlphaRefine [53] and D3S [31].\\r\\nYouTube-VOS 2019 [52] We use the validation set of Youtube-VOS 2019 [52]\\r\\nwhich consist of 507 sequences. They contain 91 object categories out of which\\r\\n26 are unseen in the training set. The results presented in Tab. 8 were generated\\r\\nby an online server after uploading the raw results. On this benchmark we are\\r\\nnot aiming at achieving the new state of the art but rather validate the quality\\r\\nof the produced segmentation masks.\\r\\n\\x0c\\n\\n14       Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n                            YouTube-VOS 2019 [52]                DAVIS 2017 [40]\\r\\nMethod              G     Jseen  Junseen Fseen       Funseen   J &F     J        F\\r\\nRTS                79.7   77.9     75.4     82.0       83.3    80.2     77.9   82.6\\r\\nLWL [5]            81.0   79.6     76.4     83.8       84.2    81.6     79.1   84.1\\r\\nSTA [59]           80.6    -        -        -          -       -        -      -\\r\\nSTM [37]           79.2   79.6     73.0     83.6       80.6    81.8     79.2   84.3\\r\\n\\r\\nRTS (Box)          70.8   71.1     65.2     74.0       72.8    72.6     69.4   75.8\\r\\nLWL (Box) [5]       -      -        -        -          -      70.6     67.9   73.3\\r\\nSiam-RCNN [46]     67.3   68.1     61.5     70.8       68.8    70.6     66.1   75.0\\r\\nD3S [50]            -      -        -        -          -      60.8     57.8   63.8\\r\\nSiamMask [31]      52.8   60.2     45.1     58.2       47.7    56.4     54.3   58.5\\r\\nTable 8. Results on the Youtube-VOS 2019 [52] and DAVIS 2017 [40] datasets. The\\r\\ntable is split in two parts to separate methods using bounding box initialization or\\r\\nsegmentation masks initialization, in order to enable a fair comparison.\\r\\n\\r\\n\\r\\n    Hence, we use the same model weight as for VOT without further fine tuning.\\r\\nWhen using the provided segmentation masks for initialization, we observe that\\r\\nour method performs slighly worse than LWL [5] and STA [59] (-1.3 G, -0.9 G)\\r\\nbut still outperforms the VOS method STM [37] (+0.5 G). We conclude that our\\r\\nmethod can generate accurate segmentation masks. When using bounding boxes\\r\\nto predict the initialization and predict the segmentation masks, we outperform\\r\\nall other methods by a large margin. This confirms that even with our bounding-\\r\\nbox initialization strategy our method can produce accurate segmentation masks.\\r\\nDAVIS 2017 [40] Similarly, we compare our method on the validation set\\r\\nof DAVIS 2017 which contains 30 sequences without fine-tuning the model for\\r\\nthis benchmark. The results are shown in Tab. 8 and confirm the the obser-\\r\\nvation made above that our method is able to generate accurate segmentation\\r\\nmasks. Our method is competitive for the mask-initialization setup. In the box-\\r\\ninitialization however, our approach outperforms all other methods in J &F, in\\r\\nparticular the segmentation trackers like SiamMask [50] (+16.2) and D3S [31]\\r\\n(+11.8).\\r\\n\\r\\n\\r\\n5    Conclusion\\r\\nWe introduced RTS, a robust, end-to-end trainable, segmentation-driven track-\\r\\ning method that is able to generate accurate segmentation masks. Compared to\\r\\nthe traditional bounding box outputs of classical visual object trackers, segmen-\\r\\ntation masks enable a more accurate representation of the target\\xe2\\x80\\x99s shape and\\r\\nextent. The proposed instance localization branch helps increasing the robust-\\r\\nness of our tracker to enable robust tracking even for long sequences consisting\\r\\nof thousands of frames. Our method outperforms by a large margin previous\\r\\nsegmentation-driven tracking methods, and it is competitive on several VOT\\r\\nbenchmarks. In particular, we set a new state of the art for the challenging La-\\r\\nSOT [16] dataset with a success AUC of 69.7%, outperforming the previous best\\r\\nmethods by 2.6%. Competitive results on two VOS datasets confirm the high\\r\\nquality of the generated segmentation masks.\\r\\n\\x0c\\n\\n                                     Robust Visual Tracking by Segmentation           15\\r\\n\\r\\nReferences\\r\\n 1. Berman, M., Triki, A.R., Blaschko, M.B.: The lova\\xcc\\x81sz-softmax loss: A tractable\\r\\n    surrogate for the optimization of the intersection-over-union measure in neural\\r\\n    networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\\r\\n    Recognition (CVPR) (June 2018) 8\\r\\n 2. Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H.: Fully-\\r\\n    convolutional siamese networks for object tracking. In: Proceedings of the Eu-\\r\\n    ropean Conference on Computer Vision Workshops (ECCVW) (October 2016) 3,\\r\\n    4\\r\\n 3. Bhat, G., Danelljan, M., Gool, L.V., Timofte, R.: Learning discrimina-\\r\\n    tive model prediction for tracking. In: IEEE/CVF International Confer-\\r\\n    ence on Computer Vision, ICCV, Seoul, South Korea. pp. 6181\\xe2\\x80\\x936190\\r\\n    (2019). https://doi.org/10.1109/ICCV.2019.00628, https://doi.org/10.1109/\\r\\n    ICCV.2019.00628 3, 4, 7, 8, 12, 13, 19, 23\\r\\n 4. Bhat, G., Danelljan, M., Van Gool, L., Timofte, R.: Know your surroundings:\\r\\n    Exploiting scene information for object tracking. In: Proceedings of the European\\r\\n    Conference on Computer Vision (ECCV) (August 2020) 13\\r\\n 5. Bhat, G., Lawin, F.J., Danelljan, M., Robinson, A., Felsberg, M., Gool, L.V.,\\r\\n    Timofte, R.: Learning what to learn for video object segmentation. In: European\\r\\n    Conference on Computer Vision ECCV (2020), https://arxiv.org/abs/2003.\\r\\n    11540 2, 4, 5, 6, 9, 10, 12, 13, 14, 19, 21, 22, 23\\r\\n 6. Bolme, D.S., Beveridge, J.R., Draper, B.A., Lui, Y.M.: Visual object tracking using\\r\\n    adaptive correlation filters. In: CVPR (2010) 3, 7\\r\\n 7. Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taixe\\xcc\\x81, L., Cremers, D., Van Gool,\\r\\n    L.: One-shot video object segmentation. In: Proceedings of the IEEE Conference\\r\\n    on Computer Vision and Pattern Recognition. pp. 221\\xe2\\x80\\x93230 (2017) 4\\r\\n 8. Chen, X., Yan, B., Zhu, J., Wang, D., Yang, X., Lu, H.: Transformer tracking.\\r\\n    In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r\\n    Recognition (CVPR) (June 2021) 4, 12, 13, 23\\r\\n 9. Chen, Y., Pont-Tuset, J., Montes, A., Van Gool, L.: Blazingly fast video object seg-\\r\\n    mentation with pixel-wise metric learning. In: Proceedings of the IEEE Conference\\r\\n    on Computer Vision and Pattern Recognition. pp. 1189\\xe2\\x80\\x931198 (2018) 4\\r\\n10. Dai, K., Zhang, Y., Wang, D., Li, J., Lu, H., Yang, X.: High-performance long-\\r\\n    term tracking with meta-updater. In: Proceedings of the IEEE/CVF Conference\\r\\n    on Computer Vision and Pattern Recognition (CVPR) (June 2020) 12, 23\\r\\n11. Danelljan, M., Bhat, G.: PyTracking: Visual tracking library based on PyTorch.\\r\\n    https://github.com/visionml/pytracking (2019), accessed: 16/09/2019 10, 12,\\r\\n    13\\r\\n12. Danelljan, M., Bhat, G., Khan, F.S., Felsberg, M.: ATOM: accurate track-\\r\\n    ing by overlap maximization. In: IEEE Conference on Computer Vision and\\r\\n    Pattern Recognition, CVPR, Long Beach, CA, USA. pp. 4660\\xe2\\x80\\x934669 (2019).\\r\\n    https://doi.org/10.1109/CVPR.2019.00479 3, 4\\r\\n13. Danelljan, M., Bhat, G., Shahbaz Khan, F., Felsberg, M.: ECO: efficient convolu-\\r\\n    tion operators for tracking. In: Proceedings of the IEEE Conference on Computer\\r\\n    Vision and Pattern Recognition (CVPR) (June 2017) 7\\r\\n14. Danelljan, M., Gool, L.V., Timofte, R.: Probabilistic regression for visual tracking.\\r\\n    In: CVPR (2020) 3, 4, 12, 13, 23\\r\\n15. Danelljan, M., Robinson, A., Shahbaz Khan, F., Felsberg, M.: Beyond correlation\\r\\n    filters: Learning continuous convolution operators for visual tracking. In: Proceed-\\r\\n\\x0c\\n\\n16      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n    ings of the European Conference on Computer Vision (ECCV) (October 2016)\\r\\n    3\\r\\n16. Fan, H., Lin, L., Yang, F., Chu, P., Deng, G., Yu, S., Bai, H., Xu, Y., Liao, C.,\\r\\n    Ling, H.: Lasot: A high-quality benchmark for large-scale single object tracking.\\r\\n    CoRR abs/1809.07845 (2018), http://arxiv.org/abs/1809.07845 1, 2, 3, 5, 9,\\r\\n    10, 11, 12, 14, 19, 21, 22, 23, 24\\r\\n17. Fan, H., Ling, H.: Cract: Cascaded regression-align-classification for robust visual\\r\\n    tracking. arXiv preprint arXiv:2011.12483 (2020) 13\\r\\n18. Fu, Z., Liu, Q., Fu, Z., Wang, Y.: Stmtrack: Template-free visual tracking with\\r\\n    space-time memory networks. In: Proceedings of the IEEE/CVF Conference on\\r\\n    Computer Vision and Pattern Recognition (CVPR) (June 2021) 12, 13, 23\\r\\n19. Galoogahi, H.K., Fagg, A., Huang, C., Ramanan, D., Lucey, S.: Need for speed: A\\r\\n    benchmark for higher frame rate object tracking. In: ICCV (2017) 10, 11, 13, 21,\\r\\n    22, 23\\r\\n20. Guo, Q., Feng, W., Zhou, C., Huang, R., Wan, L., Wang, S.: Learning dynamic\\r\\n    siamese network for visual object tracking. In: ICCV (2017) 3\\r\\n21. He, A., Luo, C., Tian, X., Zeng, W.: Towards a better match in siamese network\\r\\n    based visual object tracker. In: ECCV workshop (2018) 3\\r\\n22. Henriques, J.F., Caseiro, R., Martins, P., Batista, J.: High-speed tracking with\\r\\n    kernelized correlation filters. IEEE Transactions on Pattern Analysis and Machine\\r\\n    Intelligence (TPAMI) 37(3), 583\\xe2\\x80\\x93596 (2015) 3, 7\\r\\n23. Hu, Y.T., Huang, J.B., Schwing, A.G.: Videomatch: Matching based video object\\r\\n    segmentation. In: Proceedings of the European Conference on Computer Vision\\r\\n    (ECCV). pp. 54\\xe2\\x80\\x9370 (2018) 4\\r\\n24. Huang, L., Zhao, X., Huang, K.: GOT-10k: A large high-diversity benchmark for\\r\\n    generic object tracking in the wild. IEEE Transactions on Pattern Analysis and\\r\\n    Machine Intelligence (2019). https://doi.org/10.1109/tpami.2019.2957464, http:\\r\\n    //dx.doi.org/10.1109/TPAMI.2019.2957464 3, 9, 11, 12, 21\\r\\n25. Khoreva, A., Benenson, R., Ilg, E., Brox, T., Schiele, B.: Lucid data dreaming for\\r\\n    object tracking. In: The DAVIS Challenge on Video Object Segmentation (2017)\\r\\n    4\\r\\n26. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio,\\r\\n    Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations,\\r\\n    ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings\\r\\n    (2015), http://arxiv.org/abs/1412.6980 9\\r\\n27. Kristan, M., Leonardis, A., Matas, J., Felsberg, M., Pflugfelder, R., Ka\\xcc\\x88ma\\xcc\\x88ra\\xcc\\x88inen,\\r\\n    J.K., Danelljan, M., Zajc, L.C\\xcc\\x8c., Lukez\\xcc\\x8cic\\xcc\\x8c, A., Drbohlav, O., He, L., Zhang, Y.,\\r\\n    Yan, S., Yang, J., Ferna\\xcc\\x81ndez, G., et al: The eighth visual object tracking vot2020\\r\\n    challenge results. In: Proceedings of the European Conference on Computer Vision\\r\\n    Workshops (ECCVW) (August 2020) 13\\r\\n28. Li, B., Wu, W., Wang, Q., Zhang, F., Xing, J., Yan, J.: Siamrpn++: Evolution of\\r\\n    siamese visual tracking with very deep networks. In: Proceedings of the IEEE/CVF\\r\\n    Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019) 3,\\r\\n    4, 12\\r\\n29. Li, B., Yan, J., Wu, W., Zhu, Z., Hu, X.: High performance visual tracking with\\r\\n    siamese region proposal network. In: Proceedings of the IEEE/CVF Conference on\\r\\n    Computer Vision and Pattern Recognition (CVPR) (June 2018) 3, 4\\r\\n30. Li, X., Change Loy, C.: Video object segmentation with joint re-identification and\\r\\n    attention-aware mask propagation. In: Proceedings of the European Conference on\\r\\n    Computer Vision (ECCV). pp. 90\\xe2\\x80\\x93105 (2018) 4\\r\\n\\x0c\\n\\n                                      Robust Visual Tracking by Segmentation            17\\r\\n\\r\\n31. Lukezic, A., Matas, J., Kristan, M.: D3s - a discriminative single shot segmentation\\r\\n    tracker. In: CVPR (2020) 4, 12, 13, 14\\r\\n32. Lukezic, A., Voj\\xc4\\xb1\\xcc\\x81r, T., Zajc, L.C., Matas, J., Kristan, M.: Discriminative correlation\\r\\n    filter tracker with channel and spatial reliability. International Journal of Computer\\r\\n    Vision (IJCV) 126(7), 671\\xe2\\x80\\x93688 (2018) 3\\r\\n33. Maninis, K.K., Caelles, S., Chen, Y., Pont-Tuset, J., Leal-Taixe\\xcc\\x81, L., Cremers, D.,\\r\\n    Van Gool, L.: Video object segmentation without temporal information. IEEE\\r\\n    Transactions on Pattern Analysis and Machine Intelligence 41(6), 1515\\xe2\\x80\\x931530\\r\\n    (2018) 4\\r\\n34. Mayer, C., Danelljan, M., Paudel, D.P., Gool, L.V.: Learning target candidate asso-\\r\\n    ciation to keep track of what not to track. In: IEEE/CVF International Conference\\r\\n    on Computer Vision, ICCV (2021), https://arxiv.org/abs/2103.16556 11, 12,\\r\\n    13, 23, 24\\r\\n35. Mueller, M., Smith, N., Ghanem, B.: A benchmark and simulator for uav tracking.\\r\\n    In: Proceedings of the European Conference on Computer Vision (ECCV) (October\\r\\n    2016) 1, 5, 10, 11, 13, 21, 22, 23\\r\\n36. Mu\\xcc\\x88ller, M., Bibi, A., Giancola, S., Al-Subaihi, S., Ghanem, B.: Trackingnet: A\\r\\n    large-scale dataset and benchmark for object tracking in the wild. In: ECCV (2018)\\r\\n    3, 12, 21\\r\\n37. Oh, S.W., Lee, J.Y., Xu, N., Kim, S.J.: Video object segmentation using space-time\\r\\n    memory networks. In: Proceedings of the IEEE/CVF International Conference on\\r\\n    Computer Vision (ICCV) (October 2019) 2, 4, 14, 21\\r\\n38. Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-\\r\\n    Hornung, A.: A benchmark dataset and evaluation methodology for video object\\r\\n    segmentation. In: Computer Vision and Pattern Recognition (2016) 9\\r\\n39. Perazzi, F., Khoreva, A., Benenson, R., Schiele, B., Sorkine-Hornung, A.: Learn-\\r\\n    ing video object segmentation from static images. In: Proceedings of the IEEE\\r\\n    Conference on Computer Vision and Pattern Recognition. pp. 2663\\xe2\\x80\\x932672 (2017) 4\\r\\n40. Pont-Tuset, J., Perazzi, F., Caelles, S., Arbela\\xcc\\x81ez, P., Sorkine-Hornung, A., Van\\r\\n    Gool, L.: The 2017 davis challenge on video object segmentation. arXiv:1704.00675\\r\\n    (2017) 2, 4, 9, 14, 21, 22\\r\\n41. Son, J., Jung, I., Park, K., Han, B.: Tracking-by-segmentation with online gradient\\r\\n    boosting decision tree. In: 2015 IEEE International Conference on Computer Vision\\r\\n    (ICCV). pp. 3056\\xe2\\x80\\x933064 (2015). https://doi.org/10.1109/ICCV.2015.350 4\\r\\n42. Tao, R., Gavves, E., Smeulders, A.W.M.: Siamese instance search for tracking. In:\\r\\n    CVPR (2016) 3\\r\\n43. Valmadre, J., Bertinetto, L., Henriques, J., Vedaldi, A., Torr, P.H.S.: End-to-end\\r\\n    representation learning for correlation filter based tracking. In: Proceedings of the\\r\\n    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\r\\n    (July 2017) 3, 4\\r\\n44. Voigtlaender, P., Chai, Y., Schroff, F., Adam, H., Leibe, B., Chen, L.C.: Feelvos:\\r\\n    Fast end-to-end embedding learning for video object segmentation. In: Proceedings\\r\\n    of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9481\\xe2\\x80\\x93\\r\\n    9490 (2019) 4\\r\\n45. Voigtlaender, P., Leibe, B.: Online adaptation of convolutional neural networks for\\r\\n    video object segmentation. In: BMVC (2017) 4\\r\\n46. Voigtlaender, P., Luiten, J., Torr, P.H., Leibe, B.: Siam R-CNN: Visual track-\\r\\n    ing by re-detection. In: IEEE/CVF Conference on Computer Vision and Pattern\\r\\n    Recognition (CVPR) (June 2020) 2, 4, 12, 13, 14, 23\\r\\n\\x0c\\n\\n18      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n47. Wang, G., Luo, C., Sun, X., Xiong, Z., Zeng, W.: Tracking by instance detec-\\r\\n    tion: A meta-learning approach. In: Proceedings of the IEEE/CVF Conference on\\r\\n    Computer Vision and Pattern Recognition (CVPR) (June 2020) 3\\r\\n48. Wang, N., Zhou, W., Wang, J., Li, H.: Transformer meets tracker: Exploiting\\r\\n    temporal context for robust visual tracking. In: Proceedings of the IEEE/CVF\\r\\n    Conference on Computer Vision and Pattern Recognition (CVPR) (June 2021) 4,\\r\\n    12, 13, 23\\r\\n49. Wang, Q., Teng, Z., Xing, J., Gao, J., Hu, W., Maybank, S.J.: Learning attentions:\\r\\n    Residual attentional siamese network for high performance online visual tracking.\\r\\n    In: CVPR (2018) 3\\r\\n50. Wang, Q., Zhang, L., Bertinetto, L., Hu, W., Torr, P.H.: Fast online object tracking\\r\\n    and segmentation: A unifying approach. In: Proceedings of the IEEE conference\\r\\n    on computer vision and pattern recognition (2019) 4, 14\\r\\n51. Wug Oh, S., Lee, J.Y., Sunkavalli, K., Joo Kim, S.: Fast video object segmentation\\r\\n    by reference-guided mask propagation. In: Proceedings of the IEEE Conference on\\r\\n    Computer Vision and Pattern Recognition. pp. 7376\\xe2\\x80\\x937385 (2018) 4\\r\\n52. Xu, N., Yang, L., Fan, Y., Yue, D., Liang, Y., Yang, J., Huang, T.: Youtube-vos:\\r\\n    A large-scale video object segmentation benchmark (2018) 4, 9, 13, 14, 21, 22\\r\\n53. Yan, B., Wang, D., Lu, H., Yang, X.: Alpha-refine: Boosting tracking performance\\r\\n    by precise bounding box estimation. In: CVPR (2021) 2, 4, 12, 13, 23\\r\\n54. Yan, B., Peng, H., Fu, J., Wang, D., Lu, H.: Learning spatio-temporal transformer\\r\\n    for visual tracking. In: Proceedings of the IEEE/CVF International Conference on\\r\\n    Computer Vision (ICCV). pp. 10448\\xe2\\x80\\x9310457 (October 2021) 2, 4, 11, 12, 13, 23, 24\\r\\n55. Yu, B., Tang, M., Zheng, L., Zhu, G., Wang, J., Feng, H., Feng, X., Lu, H.:\\r\\n    High-performance discriminative tracking with transformers. In: Proceedings of\\r\\n    the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 9856\\xe2\\x80\\x93\\r\\n    9865 (October 2021) 4, 12\\r\\n56. Yu, Y., Xiong, Y., Huang, W., Scott, M.R.: Deformable siamese attention net-\\r\\n    works for visual object tracking. In: Proceedings of the IEEE/CVF Conference on\\r\\n    Computer Vision and Pattern Recognition (CVPR) (June 2020) 13\\r\\n57. Zhang, Z., Peng, H., Fu, J., Li, B., Hu, W.: Ocean: Object-aware anchor-free track-\\r\\n    ing. In: Proceedings of the European Conference on Computer Vision (ECCV)\\r\\n    (August 2020) 12\\r\\n58. Zhang, Z., Zhong, B., Zhang, S., Tang, Z., Liu, X., Zhang, Z.: Distractor-aware\\r\\n    fast tracking via dynamic convolutions and mot philosophy. In: Proceedings of\\r\\n    the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\r\\n    (June 2021) 12\\r\\n59. Zhao, B., Bhat, G., Danelljan, M., Gool, L.V., Timofte, R.: Generating masks from\\r\\n    boxes by mining spatio-temporal consistencies in videos. In: IEEE/CVF Interna-\\r\\n    tional Conference on Computer Vision, ICCV (2021), https://arxiv.org/abs/\\r\\n    2101.02196 4, 5, 9, 12, 13, 14, 21\\r\\n60. Zheng, L., Tang, M., Chen, Y., Wang, J., Lu, H.: Learning feature embeddings for\\r\\n    discriminant model based tracking. In: Proceedings of the European Conference\\r\\n    on Computer Vision (ECCV) (August 2020) 3\\r\\n61. Zhu, Z., Wang, Q., Bo, L., Wu, W., Yan, J., Hu, W.: Distractor-aware siamese\\r\\n    networks for visual object tracking. In: ECCV (2018) 3\\r\\n\\x0c\\n\\n                                  Robust Visual Tracking by Segmentation       19\\r\\n\\r\\nAppendix\\r\\nIn this Appendix, we provide further details on various aspects of our tracking\\r\\npipeline. First, we provide additional architectural and inference details in Sec-\\r\\ntions A and B. Second, we provide additional ablation studies, in particular on\\r\\nthe loss weighting parameter \\xce\\xb7 on different benchmarks to show the importance\\r\\nof the auxiliary instance localization loss in Section C. Then, we provide success\\r\\nplots for different VOT benchmarks as well as a detailed analysis of our results\\r\\non LaSOT [16] by comparing our approach against the other state-of-the-art\\r\\nmethods for all the dataset attributes in Section D. Finally, we provide some\\r\\nadditional visual comparison to other trackers in Section E.\\r\\n\\r\\nA    Additional Architecture details\\r\\nClassification Scores Encoder H\\xce\\xb8 First, we describe in Figure A1 the archi-\\r\\ntecture of the Classification Scores Encoder H\\xce\\xb8 . It takes as input the H \\xc3\\x97 W -\\r\\ndimensional scores predicted by the Instance Localization (Classification) branch\\r\\nand outputs a 16 channels deep representation of those scores. The score encoder\\r\\nconsists of a convolutional layer followed by a max-pool layer with stride one and\\r\\ntwo residual blocks. The output of the residual blocks has 64 channels. Thus, the\\r\\nfinal convolutional layer reduces the number of channels of the output to 16 to\\r\\nmatch the encoded scores with the mask encoding. All the convolutional layers\\r\\nuse (3 \\xc3\\x97 3) kernels with a stride of one to preserve the spatial size of the input\\r\\nclassification scores.\\r\\nSegmentation Decoder D\\xce\\xb8 The segmentation decoder has the same struc-\\r\\nture has in LWL [5]. Together with the backbone it shows a U-Net structure and\\r\\nmainly consists of four decoder blocks. It takes as input the extracted ResNet-50\\r\\nbackbone features and the combined encoding xf from both the instance localiza-\\r\\ntion branch (H\\xce\\xb8 (sc )) and the segmentation branch (xm ), with xf = xm + H\\xce\\xb8 (sc ).\\r\\nSince the encoded instance localization scores have a lower spatial resolution\\r\\nthan the mask encoding xm , we upscale the encoded instance localization scores\\r\\nusing a bilinear interpolation before adding it with the mask encoding xm . We\\r\\nrefer the reader to [5] for more details about the decoder structure.\\r\\nSegmentation Branch We use the same architectures for the feature extractor\\r\\nF\\xce\\xb8 , the label encoder E\\xce\\xb8 , the weight predictor W\\xce\\xb8 , the few-shot learner A\\xce\\xb8 and\\r\\nthe segmentation model T\\xcf\\x84 as proposed in LWL [5]. Hence, we refer the reader\\r\\nto [5] for more details.\\r\\nInstance Localization Branch We use the same architectures for the feature\\r\\nextractor G\\xce\\xb8 , the model predictor P\\xce\\xb8 and the instance model T\\xce\\xba as proposed in\\r\\nDiMP [3]. Hence, we refer the reader to [3] for more details.\\r\\n\\r\\nB    Additional Inference details\\r\\nSearch region selection The backbone does not extract features on the full\\r\\nimage. Instead, we sample a smaller image patch for extraction, which is centered\\r\\n\\x0c\\n\\n20      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n               Conv + ReLU\\r\\n         (Kernel Size = 3, Stride = 1)           ResBlock                         ReLU\\r\\n\\r\\n\\r\\n                  ResBlock\\r\\n                                                                                  +\\r\\n                                                                                 Conv\\r\\n                  ResBlock\\r\\n                                                                      (Kernel Size = 3, Stride = 1)\\r\\n\\r\\n                                                             Conv\\r\\n                MAX POOL                          (Kernel Size = 3, Stride = 1)\\r\\n         (Kernel Size = 3, Stride = 1)\\r\\n                                                                            Conv + ReLU\\r\\n               Conv + ReLU                                            (Kernel Size = 3, Stride = 1)\\r\\n         (Kernel Size = 3, Stride = 1)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                           Fig. A1. Classification Scores Encoder H\\xce\\xb8 .\\r\\n\\r\\n\\r\\nat the current target location and 6 times larger than the current estimated\\r\\ntarget size, when it does not exceed the size of the image. The estimation of the\\r\\ntarget state (position and size) is therefore crucial to ensure an optimal crop. In\\r\\nmost situations, the segmentation output is used to determine the target state\\r\\nsince it has a high accuracy. The target center is computed as the center of\\r\\nmass of the predicted per-pixel segmentation probability scores. The target size\\r\\nis computed as the variance of the segmentation probability scores.\\r\\n    If the segmentation branch cannot find the target (as described in the main\\r\\npaper), but the instance branch still outputs a high enough confidence score, we\\r\\nuse it to update the target position. This is particularly important in sequences\\r\\nwhere the target is becoming too small for some time, but we still can track the\\r\\ntarget position.\\r\\n    When both branch cannot find the target, the internal state of the tracker is\\r\\nnot updated. We upscale the search area based on the previous 60 valid predicted\\r\\nscales. This is helpful in situations where the size of the object shrinks although\\r\\nits size does not change. This typically happens during occlusions or if the target\\r\\ngoes out of the frame partially on completely.\\r\\n\\r\\n\\r\\nC    Additional Ablations\\r\\n\\r\\nIn this section, we provide additional ablation studies related to our method, first\\r\\non the weighting of the segmentation and classification losses used for training,\\r\\n\\x0c\\n\\n                                          Robust Visual Tracking by Segmentation              21\\r\\n\\r\\n         LaSOT [16]        GOT-10k [24]     TrackingNet [36]     NFS [19]       UAV123 [35]\\r\\n \\xce\\xb7          AUC               AO                 AUC              AUC             AUC\\r\\n0.0         67.7               84.0               81.2                63.7         64.7\\r\\n0.4         69.8               84.0               81.4                66.2         67.4\\r\\n10          69.7               85.2               81.6                65.4         67.6\\r\\n\\r\\nTable A1. Ablation on the classification vs. segmentation loss weighting on different\\r\\ndatasets in terms of AUC (area-under-the-curve) and AO (average overlap)\\r\\n\\r\\n\\r\\n                                 YouTube-VOS 2019 [52]                     DAVIS 2017 [40]\\r\\nMethod                 G       Jseen  Junseen Fseen          Funseen     J &F     J        F\\r\\nRTS                   79.7     77.9       75.4    82.0         83.3      80.2      77.9   82.6\\r\\nRTS (YT-FT)           80.3     78.8       76.2    82.9         83.5      80.3      77.7   82.9\\r\\nLWL [5]               81.0     79.6       76.4    83.8         84.2      81.6      79.1   84.1\\r\\nSTA [59]              80.6      -          -       -            -         -         -      -\\r\\nSTM [37]              79.2     79.6       73.0    83.6         80.6      81.8      79.2   84.3\\r\\n\\r\\nTable A2. Results on the Youtube-VOS 2019 [52] and DAVIS 2017 [40] datasets with\\r\\na fined tuned model and inference parameters refered as RTS (YT-FT).\\r\\n\\r\\n\\r\\nsecond on the parameters that might make a difference specifically for VOS\\r\\nbenchmarks like Youtube-VOS [52].\\r\\nWeighting segmentation and classification losses For this ablation, we\\r\\nstudy the weighting of the segmentation loss Ls and the instance localization\\r\\nloss Lc in the total loss Ltot used to train our model and its influence on the\\r\\noverall performance during tracking. We recall that\\r\\n\\r\\n                                      Ltot = Ls + \\xce\\xb7 \\xc2\\xb7 Lc .                                    (6)\\r\\nTable A1 shows the results when training the tracker with three different values\\r\\nof \\xce\\xb7 on five VOT datasets. First we examine the case where we omit the aux-\\r\\niliary instance localization loss (\\xce\\xb7 = 0.0), which means that the whole pipeline\\r\\nis trained for segmentation and the instance branch is not trained to produce\\r\\nspecifically accurate localization scores. We observe that for this setting leads to\\r\\nthe lowest performance on all tested datasets, often by a large margin. Secondly,\\r\\nwe test a dominant segmentation loss (\\xce\\xb7 = 0.4) because the segmentation branch\\r\\nneeds to be trained for a more complex task than the instance branch. We see a\\r\\nperformance gain for almost all datasets. Thus, employing the auxiliary loss to\\r\\ntrain the instance localization branch helps to improve the tracking performance.\\r\\nWe observed that using the auxiliary loss leads to localization scores generated\\r\\nduring inference that are sharper, cleaner and localize the center of the target\\r\\nmore accurately. Finally, we put an even higher weight on the classification term\\r\\n(\\xce\\xb7 = 10). This setup leads to an even more accurate localization and leads to\\r\\nthe on average best results. Thus, we set \\xce\\xb7 = 10 to train our tracking pipeline.\\r\\nFinetuning on Youtube-VOS [16] In this section, we analyze whether we\\r\\ncan gear our pipeline towards VOS benchmarks. To do that, we take our model\\r\\nand inference parameters and modify them slightly. On the one hand, the model\\r\\n\\x0c\\n\\n22                              Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n                        90\\r\\n                                                    Success plot                                                                            90\\r\\n                                                                                                                                                         Normalized Precision plot\\r\\n                        80                                                                                                                  80\\r\\n                        70                                                                                                                  70\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                   Distance Precision [%]\\r\\nOverlap Precision [%]\\r\\n                               RTS [69.7]                                                                                                                                         KeepTrack [77.2]\\r\\n                        60     KeepTrack [67.1]                                                                                             60                                    STARK-ST101 [76.9]\\r\\n                               STARK-ST101 [67.1]                                                                                                                                 STARK-ST50 [76.3]\\r\\n                        50     STARK-ST50 [66.4]                                                                                            50                                    RTS [76.2]\\r\\n                               AlphaRefine [65.9]                                                                                                                                 AlphaRefine [73.8]\\r\\n                               TransT [64.9]                                                                                                                                      TransT [73.8]\\r\\n                        40     Siam R-CNN [64.8]\\r\\n                               TrDiMP [63.9]\\r\\n                                                                                                                                            40                                    TrDiMP [73.0]\\r\\n                                                                                                                                                                                  Siam R-CNN [72.2]\\r\\n                               Super DiMP [63.1]                                                                                                                                  Super DiMP [72.2]\\r\\n                        30     STMTrack [60.6]                                                                                              30                                    STMTrack [69.3]\\r\\n                               PrDiMP50 [59.8]                                                                                                                                    PrDiMP50 [68.8]\\r\\n                               LWL [59.7]                                                                                                                                         DMTrack [66.9]\\r\\n                        20     DMTrack [58.4]                                                                                               20                                    LTMU [66.5]\\r\\n                               TACT [57.5]                                                                                                                                        TACT [66.0]\\r\\n                        10     LTMU [57.2]                                                                                                  10                                    Ocean [65.1]\\r\\n                               DiMP50 [56.9]                                                                                                                                      DiMP50 [65.0]\\r\\n                               Ocean [56.0]                                                                                                                                       LWL [63.3]\\r\\n                         00          0.2            0.4                             0.6   0.8        1                                       00      0.1        0.2       0.3         0.4              0.5\\r\\n                                                Overlap threshold                                                                                          Location error threshold\\r\\n                                                                                   90\\r\\n                                                                                                  Precision plot\\r\\n                                                                                   80\\r\\n                                                                                   70\\r\\n                                                          Distance Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                                                    RTS [73.7]\\r\\n                                                                                   60                                               STARK-ST101 [72.2]\\r\\n                                                                                                                                    STARK-ST50 [71.2]\\r\\n                                                                                   50                                               KeepTrack [70.2]\\r\\n                                                                                                                                    TransT [69.0]\\r\\n                                                                                                                                    AlphaRefine [68.8]\\r\\n                                                                                   40                                               Siam R-CNN [68.4]\\r\\n                                                                                                                                    TrDiMP [66.3]\\r\\n                                                                                                                                    Super DiMP [65.3]\\r\\n                                                                                   30                                               STMTrack [63.3]\\r\\n                                                                                                                                    PrDiMP50 [60.8]\\r\\n                                                                                                                                    TACT [60.7]\\r\\n                                                                                   20                                               LWL [60.6]\\r\\n                                                                                                                                    DMTrack [59.7]\\r\\n                                                                                   10                                               LTMU [57.2]\\r\\n                                                                                                                                    DiMP50 [56.7]\\r\\n                                                                                                                                    Ocean [56.6]\\r\\n                                                                                    00    10        20        30                             40      50\\r\\n                                                                                          Location error threshold [pixels]\\r\\n\\r\\nFig. A2. Success, precision and normalized precision plots on LaSOT [16]. Our ap-\\r\\nproach outperforms all other methods by a large margin in AUC, reported in the\\r\\nlegend.\\r\\n\\r\\n\\r\\nis fined-tuned for 50 epochs using Youtube-VOS [52] only for both training and\\r\\nvalidation. We also increase the initialization phase from 100 to 200 frames, and\\r\\nremove the relative target scale change limit from one frame to the next (in our\\r\\nmodel, we limit that scale change to 20% for increased robustness).\\r\\n     The results are presented in Table A2 for Youtube-VOS [52] and Davis [40].\\r\\nWe observe that the performances between both of our models stay very close\\r\\nfor Davis but that the finetuned model is getting closer to the baseline LWL [5]\\r\\nfor Youtube-VOS. The more frequent updates seem to help and not restricting\\r\\nthe scale change of objects from one frame to the next seems to play a role, since\\r\\nwe get an improvement of 0.6 in G score.\\r\\n\\r\\n\\r\\nD                             Additional Evaluation results\\r\\n\\r\\nIn this section we provide additional plots of our approach on different bench-\\r\\nmarks and a attribute analysis on LaSOT [16].\\r\\nSuccess plots for LaSOT [16], NFS [19] and UAV123 [35] We provide\\r\\nin Figure A2 all the plots for the metrics we report for LaSOT [16] in the paper:\\r\\n\\x0c\\n\\n                                                                                   Robust Visual Tracking by Segmentation                                                                    23\\r\\n\\r\\n                        100\\r\\n                                                   Success plot                                                               100\\r\\n                                                                                                                                                               Success plot\\r\\n\\r\\n                         80                                                                                                    80\\r\\nOverlap Precision [%]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                      Overlap Precision [%]\\r\\n                               KeepTrack [69.7]\\r\\n                         60    TransT [69.1]                                                                                   60\\r\\n                               PrDiMP50 [68.0]\\r\\n                               Super DiMP [67.7]                                                                                           KeepTrack [66.4]\\r\\n                               RTS [67.6]                                                                                                  TrDiMP [66.2]\\r\\n                         40    TrDiMP [67.5]                                                                                   40          RTS [65.4]\\r\\n                               STMTrack [65.7]                                                                                             TransT [65.3]\\r\\n                               DiMP50 [65.3]                                                                                               Super DiMP [64.8]\\r\\n                               SiamRPN++ [65.2]                                                                                            PrDiMP50 [63.5]\\r\\n                               ATOM [64.2]                                                                                                 DiMP50 [61.9]\\r\\n                         20    DaSiamRPN [57.7]                                                                                20          ATOM [58.4]\\r\\n                               UPDT [54.5]                                                                                                 UPDT [53.6]\\r\\n                               ECO [53.2]                                                                                                  CCOT [48.8]\\r\\n                               CCOT [51.3]                                                                                                 ECO [46.6]\\r\\n                          00       0.2             0.4      0.6          0.8         1                                          00             0.2             0.4          0.6     0.8       1\\r\\n                                              Overlap threshold                                                                                           Overlap threshold\\r\\n\\r\\nFig. A3. Success plots on the UAV123 [35] (left) and NFS [19] (right) datasets in terms\\r\\nof overall AUC score, reported in the legend.\\r\\n\\r\\n                                 Illumination Partial             Motion Camera         Background Viewpoint Scale     Full     Fast                 Low        Aspect\\r\\n                                   Variation Occlusion Deformation Blur Motion Rotation   Clutter   Change Variation Occlusion Motion Out-of-View Resolution Ratio Change Total\\r\\nLTMU [10]                            56.5          54.0   57.2    55.8    61.6    55.1      49.9       56.7                         57.1      49.9      44.0         52.7    51.4    55.1   57.2\\r\\nLWL [5]                              65.3          56.4   61.6    59.1    64.7    57.4      53.1       58.1                         59.3      48.7      46.5         51.5    48.7    57.9   59.7\\r\\nPrDiMP50 [14]                        63.7          56.9   60.8    57.9    64.2    58.1      54.3       59.2                         59.4      51.3      48.4         55.3    53.5    58.6   59.8\\r\\nSTMTrack [18]                        65.2          57.1   64.0    55.3    63.3    60.1      54.1       58.2                         60.6      47.8      42.4         51.9    50.3    58.8   60.6\\r\\nSuperDiMP [3]                        67.8          59.7   63.4    62.0    68.0    61.4      57.3       63.4                         62.9      54.1      50.7         59.0    56.4    61.6   63.1\\r\\nTrDiMP [48]                          67.5          61.1   64.4    62.4    68.1    62.4      58.9       62.8                         63.4      56.4      53.0         60.7    58.1    62.3   63.9\\r\\nSiam R-CNN [46]                      64.6          62.2   65.2    63.1    68.2    64.1      54.2       65.3                         64.5      55.3      51.5         62.2    57.1    63.4   64.8\\r\\nTransT [8]                           65.2          62.0   67.0    63.0    67.2    64.3      57.9       61.7                         64.6      55.3      51.0         58.2    56.4    63.2   64.9\\r\\nAlphaRefine [53]                     69.4          62.3   66.3    65.2    70.0    63.9      58.8       63.1                         65.4      57.4      53.6         61.1    58.6    64.1   65.3\\r\\nKeepTrack Fast [34]                  70.1          63.8   66.2    65.0    70.7    65.1      60.1       67.6                         66.6      59.2      57.1         63.4    62.0    65.6   66.8\\r\\nKeepTrack [34]                       69.7          64.1   67.0    66.7    71.0    65.3      61.2       66.9                         66.8      60.1      57.7         64.1    62.0    65.9   67.1\\r\\nSTARK-ST101 [54]                     67.5          65.1   68.3    64.5    69.5    66.6      57.4       68.8                         66.8      58.9      54.2         63.3    59.6    65.6   67.1\\r\\nRTS                                  68.7          66.9   71.6    67.7    74.4    67.9      61.4       69.7                         69.3      60.5      53.8         66.3    62.7    68.2   69.7\\r\\n\\r\\n\\r\\n\\r\\nTable A3. LaSOT [16] attribute-based analysis. Each column corresponds to the re-\\r\\nsults computed on all sequences in the dataset with the corresponding attribute. Our\\r\\nmethod outperforms all others in 12 out of 14 attributes.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSuccess, Normalized Precision and Precision plots. For completeness, we provide\\r\\nthe success plots for NFS [19] and UAV123 [35] in Figure A3.\\r\\nAttribute analysis on LaSOT [16] In this section, we focus on the dataset\\r\\nsequences attributes. We compare our approach to numerous other trackers, and\\r\\nprovide the detailed results in Table A3. Furthermore, we highlight the strength\\r\\nof our approach in Figure A4 by focusing the comparison only to the two current\\r\\nstate-of-the-art methods KeepTrack [34] and STARK-ST101 [54].\\r\\n    There are 14 attributes provided for LaSOT [16] sequences, representing dif-\\r\\nferent kind of challenges the tracker has to deal with in different situations. Com-\\r\\npared to existing trackers, our method achieves better AUC scores in 12 out of 14\\r\\nattributes. In particular, we outperform KeepTrack [34] and STARK-ST101 [54]\\r\\nby a large margin for the following attributes: Camera Motion (+3.4% and\\r\\n+3.9%), Background Clutter (+0.2% and +4.0%), Scale Variation (+2.5% and\\r\\n2.5%), Deformation (+4.6% and +3.3%) and Aspect Ratio Change (+2.3% and\\r\\n+2.6%). Our method is only outperformed on two attributes by KeepTrack [34]\\r\\nand KeepTrack Fast [34] for Fast Motion (-3.9% and -3.3%) and for Illumination\\r\\nVariation (-1.0% and -1.4%).\\r\\n\\x0c\\n\\n24      Paul, Danelljan, Mayer, Van Gool\\r\\n\\r\\n                                   Camera Motion\\r\\n                                   Motion Blur\\r\\n                          Rotation             Deformation\\r\\n                                         70\\r\\n                   Background                          Partial\\r\\n                     Clutter             60          Occlusion\\r\\n\\r\\n                  Viewpoint                 50              Illumination\\r\\n                   Change                                      Variation\\r\\n\\r\\n                     Scale                                  Aspect\\r\\n                    Variation                                Ratio\\r\\n                                                            Change\\r\\n                           Full                         Low\\r\\n                         Occlusion                   Resolution\\r\\n                                      Fast\\r\\n                                     Motion Out-of-View\\r\\n                       RTS       KeepTrack           STARK-ST101\\r\\n\\r\\n                  Fig. A4. Attributes comparison on LaSOT [16].\\r\\n\\r\\n\\r\\nE    Additional Content\\r\\n\\r\\nFigure A5 shows additional visual results compared to other state-of-the-art\\r\\ntrackers on 6 different sequences of LaSOT [16]. For more content, we refer the\\r\\nreader to: https://github.com/visionml/pytracking.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. A5. Qualitative results on LaSOT [16] of our approach compared to the previous\\r\\nstate-of-the-art methods KeepTrack [34] and STARK-ST101 [54]. As they do not pro-\\r\\nduce segmentation masks, we represent ours as a red overlay and print for all methods\\r\\nthe predicted bounding boxes with the following color code:\\r\\n\\xc2\\x97 KeepTrack \\xc2\\x97 STARK-ST101 \\xc2\\x97 RTS\\r\\n\\x0c', b'                                             Improved Sampling-to-Counting Reductions in\\r\\n                                             High-Dimensional Expanders and Faster Parallel\\r\\n                                                       Determinantal Sampling\\r\\narXiv:2203.11190v1 [cs.DS] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             Nima Anari1 , Callum Burgess1 , Kevin Tian1 , and Thuy-Duong Vuong1\\r\\n                                            1 Stanford University, {anari,callumb,kjtian,tdvuong}@stanford.edu\\r\\n\\r\\n\\r\\n\\r\\n                                                                                     Abstract\\r\\n                                             We study parallel sampling algorithms for classes of distributions defined via determi-\\r\\n                                         nants: symmetric, nonsymmetric, and partition-constrained determinantal point processes.\\r\\n                                         For these distributions, counting, a.k.a. computing the partition function, can be reduced to\\r\\n                                         a simple determinant computation which is highly parallelizable; Csanky proved it is in NC.\\r\\n                                         However, parallel counting does not automatically translate to parallel sampling, as the classic\\r\\n                                         reductions between sampling and counting are inherently sequential. Despite this, we show\\r\\n                                         that for all the aforementioned determinant-based distributions, a roughly quadratic parallel\\r\\n                                         speedup over sequential sampling can be achieved. If the distribution is supported on subsets\\r\\n                                         of size k of a ground set, we show how to approximately produce a sample in O    e (k 12 +c ) time\\r\\n                                         with polynomially many processors for any c > 0.   \\xe2\\x88\\x9a In the special case of symmetric determi-\\r\\n                                         nantal point processes, our bound improves to O e ( k) and we show how to sample exactly in\\r\\n                                         this case.\\r\\n                                             We obtain our results via a generic sampling-to-counting reduction that uses approximate\\r\\n                                         rejection sampling. As our main technical contribution, we show that whenever a distribution\\r\\n                                         satisfies a certain form of high-dimensional expansion called entropic independence, approx-\\r\\n                                         imate rejection sampling can achieve a roughly quadratic speedup in sampling via counting.\\r\\n                                         Various forms of high-dimensional expansion, including the notion of entropic independence\\r\\n                                         we use in this work, have been the source of major breakthroughs in sampling algorithms\\r\\n                                         in recent years; thus we expect our framework to prove useful in the future for distributions\\r\\n                                         beyond those defined by determinants.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                         1\\r\\n\\x0c\\n\\n1 Introduction\\r\\nSampling and counting are intimately connected computational problems. For many classes\\r\\nof distributions defined by weight functions \\xc2\\xb5 : X \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 , where typically the space X is\\r\\nexponential-sized, the problems of approximately sampling x \\xe2\\x88\\x88 X with P [ x] \\xe2\\x88\\x9d \\xc2\\xb5( x) and ap-\\r\\nproximately computing the partition function \\xe2\\x88\\x91 x \\xe2\\x88\\x88 X \\xc2\\xb5( x) are polynomial-time reducible to each\\r\\nother [JVV86]. However, this equivalence appears to break down for complexity classes below\\r\\nP. For example, there is no known polylogarithmic-overhead parallel reduction between approx-\\r\\nimate counting and approximate sampling.\\r\\nMotivated by the mysterious relationship between sampling and counting in the parallel algo-\\r\\nrithms world, Anari, Hu, Saberi, and Schild [Ana+20], based on the earlier work of Teng [Ten95],\\r\\nraised the question of designing fast parallel sampling algorithms for several classes of distribu-\\r\\ntions where counting, even exactly, was possible in polylogarithmic time and polynomial work,\\r\\ni.e., in the class NC. The distributions in this challenge set all enjoy fast parallel counting algo-\\r\\nrithms because their partition functions can be written as determinants and determinants are\\r\\ncomputable in NC [Csa75]. Anari, Hu, Saberi, and Schild [Ana+20] solved one of these chal-\\r\\nlenges, and showed how to sample random arborescences in RNC, completing the earlier work\\r\\nof Teng [Ten95] on random spanning trees. However, the algorithms in these two works are\\r\\nhighly tailored to the random spanning tree and random arborescence distributions, and they do\\r\\nnot provide any general recipe for parallel sampling from other distributions.\\r\\nIn this work, we study a general framework to improve the parallel efficiency of sampling-to-\\r\\ncounting reductions. We build on the success of a recent trend in the analysis of random walks\\r\\nand sampling algorithms, where combinatorial distributions are analyzed through the lens of\\r\\nhigh-dimensional expanders [Ana+19; AL20; ALO20]. We show that under one notion of high-\\r\\ndimensional expansion, namely entropic independence [Ana+21b], the sampling-to-counting re-\\r\\nduction can be made roughly quadratically faster using parallelization. We formally define\\r\\nentropic independence in Section 3, see Definition 20.\\r\\nIn our setup, we consider combinatorial distributions defined on size k sets of a ground set of\\r\\nelements [n], which we denote by an unnormalized density\\r\\n                                         \\x12 \\x13\\r\\n                                           [n ]\\r\\n                                      \\xc2\\xb5:        \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 .\\r\\n                                            k\\r\\n\\r\\nWe remark that the choice of ([nk ]) is standard in the high-dimensional expanders literature, and\\r\\nmany other domains such as the popular product spaces, can be naturally transformed into ([nk ])\\r\\n[ALO20]. Our access to this distribution is through an oracle that can answer counting queries.\\r\\nGiven any1 set T \\xe2\\x8a\\x86 [n], the oracle returns\\r\\n                                     \\x1a          \\x12 \\x13         \\x1b\\r\\n                                                 [n ]\\r\\n                                 \\xe2\\x88\\x91 \\xc2\\xb5(S) S \\xe2\\x88\\x88 k , T \\xe2\\x8a\\x86 S .\\r\\nOur goal is to use the oracle and output a random set S that approximately follows P [S] \\xe2\\x88\\x9d \\xc2\\xb5(S).\\r\\nThe classical reduction from sampling to counting proceeds by picking the k elements of S one\\r\\nat a time. In each step, conditioned on all previously chosen elements, marginals PS\\xe2\\x88\\xbc\\xc2\\xb5 [i \\xe2\\x88\\x88 S |\\r\\nprevious choices] of all remaining elements in the ground set are computed and a new element\\r\\n  1 Note that by querying sets T of size exactly k, a counting query can also return the value of \\xc2\\xb5 on any desired set.\\r\\n\\r\\n\\r\\n\\r\\n                                                          2\\r\\n\\x0c\\n\\nis picked randomly with probability proportional to the conditional marginals. In each step,\\r\\nmarginals can be computed via parallel calls to the counting oracle. However, this procedure\\r\\nis inherently sequential as the choice of each element affects the conditional marginals in future\\r\\niterations. A parallel implementation of this reduction takes time \\xe2\\x84\\xa6(k). The main question we\\r\\naddress is:\\r\\n      For which \\xc2\\xb5 is there a faster parallel reduction from sampling to counting?\\r\\nOur main result establishes that for distributions \\xc2\\xb5 which are good high-dimensional expanders,\\r\\nmeasured in terms of the notion of entropic independence [Ana+21b], the sampling-to-counting\\r\\nreduction can be sped up roughly quadratically. Throughout (e.g. in Theorem 1), we use O      e (\\xc2\\xb7)\\r\\nto hide logarithmic factors in n and failure probabilities; these factors primarily come from the\\r\\nparallel complexity of linear algebra (e.g. evaluating determinants and partition functions).\\r\\n\\r\\nTheorem 1 (Main, informal, see Theorem 33). Let \\xc2\\xb5 : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be O(1)-entropically independent,\\r\\nand assume that we have access to a counting oracle for \\xc2\\xb5. For any constant c > 0 and any \\xc7\\xab \\xe2\\x88\\x88 (0, 1),\\r\\n   \\x10\\xe2\\x88\\x9aexists\\r\\nthere    \\x10 \\x11an  algorithm that can sample from a distribution within total variation distance \\xc7\\xab of \\xc2\\xb5 in\\r\\n             c\\x11\\r\\nOe    k\\xc2\\xb7  k\\r\\n                 parallel time using (n/\\xc7\\xab)O(1/c) machines in the PRAM model of computation.\\r\\n          \\xc7\\xab\\r\\n\\r\\nWe remark that various notions of high-dimensional expansion and in particular entropic in-\\r\\ndependence have proven useful in the analysis of Markov chains and sequential sampling algo-\\r\\nrithms [Ana+21b; Ana+21c], but this is the first work to relate these notions to parallel algorithms.\\r\\nWe also note that entropic independence is not a binary property of a distribution, but rather ev-\\r\\nery distribution \\xc2\\xb5 has some parameter of entropic independence \\xe2\\x88\\x88 [1, k] and the above result\\r\\napplies for all distributions whose entropic independence parameter is O(1). See Theorem 33 in\\r\\nSection 6 for details. This is also the exact condition that implies fast mixing of Markov chains\\r\\nand a polynomial runtime for sequential sampling algorithms [Ana+21b].\\r\\nRemark 2 (Beyond determinantal distributions). In this work, we explore the applications of The-\\r\\norem 1 to distributions defined via determinants defined in the next section; this is due to the\\r\\nfact that partition functions of such distributions can be computed in NC, which gives us a fast\\r\\nparallel counting oracle. However, we believe Theorem 1 may find applications beyond determi-\\r\\nnantal distributions in the future. As an example, for distributions whose partition functions do\\r\\nnot have roots in certain regions of the complex plane, Barvinok [Bar18] devised efficient deter-\\r\\nministic approximate counting algorithms, which have been refined by subsequent works [PR17].\\r\\nThese counting algorithms have the potential to be parallelized, as they involve enumerating a\\r\\nsmall number of combinatorial structures, unlike the more involved and inherently sequential\\r\\nMarkov Chain Monte Carlo methods. Recent works [Ali+21; CLV21] have shown that absence\\r\\nof roots in the complex plane implies forms of high-dimensional expansion, including entropic\\r\\nindependence, paving the way for the application of Theorem 1 to such distributions.\\r\\n\\r\\n1.1 Determinantal distributions\\r\\nIn this work, we consider applications of Theorem 1 to various distributions \\xc2\\xb5 that are defined\\r\\nbased on determinants. Prior progress on designing parallel sampling algorithms for problems\\r\\nthat enjoy determinant-based counting has been very limited. Teng [Ten95] showed how to sim-\\r\\nulate random walks on a graph in parallel, which combined with the classic algorithm of Aldous\\r\\n[Ald90] and Broder [Bro89] yielded RNC algorithms for sampling spanning trees of a graph.\\r\\nAnari, Hu, Saberi, and Schild [Ana+20] extended this to sampling arborescences, a.k.a. directed\\r\\n\\r\\n\\r\\n\\r\\n                                                  3\\r\\n\\x0c\\n\\nspanning trees, of directed graphs. In this work we tackle a much larger class of problems that\\r\\nenjoy determinant-based counting, namely variants of determinantal point processes.\\r\\nDeterminantal point processes (DPPs) have found many applications, such as data summa-\\r\\nrization [Gon+14; LB12], recommender systems [GPK16; Wil+18], neural network compression\\r\\n[MS15], kernel approximation [LJS16], multi-modal output generation [Elf+19], and randomized\\r\\nnumerical linear algebra [DM21]. Formally, a DPP on a set of items [n] = {1, . . . , n} is a probabil-\\r\\nity distribution over subsets Y \\xe2\\x8a\\x86 [n] defined via an n \\xc3\\x97 n matrix L where probabilities are given\\r\\n(proportionally) by principal minors: P[Y ] \\xe2\\x88\\x9d det( LY,Y ).\\r\\nNote that for the distribution to be well-defined, all principal minors of L have to be \\xe2\\x89\\xa5 0. For\\r\\nsymmetric L (L = L\\xe2\\x8a\\xba ), having nonnegative principle minors is equivalent to L being positive\\r\\nsemi-definite (PSD). Symmetric DPPs, where L = L\\xe2\\x8a\\xba is a PSD matrix, have received the most\\r\\nattention in the literature.\\r\\nDefinition 3 (Symmetric DPP). Given a symmetric n \\xc3\\x97 n matrix L \\x17 0, the symmetric DPP\\r\\ndefined by L is the probability distribution over subsets Y \\xe2\\x8a\\x86 [n], where P [Y ] \\xe2\\x88\\x9d det( LY,Y ).\\r\\nBeyond the (symmetric) determinantal point proceses defined above, our work provides sam-\\r\\npling algorithms for a variety of discrete distributions related to determinants, which serve dif-\\r\\nferent roles in modeling applications. In the remainder of the section, we will outline each family\\r\\nof distributions.\\r\\nRecently, [Bru18; Gar+19; Gar+20] initiated the study of non-symmetric DPPs in applications\\r\\nand argued for their use because of their increased modeling power. Non-symmetric DPPs are\\r\\ncharacterized by a non-symmetric positive-definite matrix L, i.e., a matrix L where L + L\\xe2\\x8a\\xba \\x17\\r\\n0. Symmetric DPPs necessarily exhibit strong forms of negative dependence [BBL09], which\\r\\nare unrealistic in some applications; non-symmetric DPPs on the other hand, can have positive\\r\\ncorrelations. As an example application, a good recommender system for online shopping should\\r\\nmodel complementary items, such as tablets and tablet pens, as having positive correlation; non-\\r\\nsymmetric DPPs can model such positive interactions.\\r\\nDefinition 4. A matrix L \\xe2\\x88\\x88 R n\\xc3\\x97n is non-symmetric positive semidefinite (nPSD) if L + L\\xe2\\x8a\\xba \\x17 0.\\r\\nDefinition 5 (Non-symmetric DPP). Given an nPSD n \\xc3\\x97 n matrix L, the non-symmetric DPP\\r\\ndefined by it is the probability distribution over subsets Y \\xe2\\x8a\\x86 [n] given by P [Y ] \\xe2\\x88\\x9d det( LY,Y ).\\r\\nA related and more commonly used model related to DPPs, is a k-DPP, where we constrain the\\r\\ncardinality of the sampled set Y to be exactly k. In many applications restricting to sets of a\\r\\npredetermined size is more desirable [KT12b].\\r\\nDefinition 6 (k-DPP). Given a PSD or nPSD matrix L, the k-DPP defined by it is the distribution\\r\\nof the corresponding determinantal point process restricted to only k-sized sets.\\r\\nA natural generalization of simple cardinality constraints on DPPs are DPPs under partition\\r\\nconstraints [Cel+16]. Partition constraints arise naturally when there is an inherent labeling or\\r\\ngrouping of the ground set items that is not captured by the DPP kernel itself. More concretely,\\r\\nsuppose the ground set [n] is partitioned into disjoint sets [n] = V1 \\xe2\\x88\\xaa V2 \\xe2\\x88\\xaa \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xe2\\x88\\xaa Vr , and we want\\r\\nto produce a subset S with c1 items from V1 , c2 items from V2 and so on. We define Partition-DPP\\r\\nas the corresponding conditioning of the DPP under these constraints on S. Celis, Deshpande,\\r\\nKathuria, Straszak, and Vishnoi [Cel+16] established that efficiently sampling and counting from\\r\\nPartition-DPPs is possible when the number of constraints is O(1) and that counting is #P-hard\\r\\nwhen the number of constraints is unbounded \\xe2\\x80\\x93 it includes as a special case the problem of\\r\\n\\r\\n                                                  4\\r\\n\\x0c\\n\\ncomputing mixed discriminants. In this paper, we will only study Partition-DPPs when the\\r\\nensemble matrix L is symmetric PSD and the number of constraints is O(1). Alimohammadi,\\r\\nAnari, Shiragur, and Vuong [Ali+21] showed that local Markov chains can be used to sample\\r\\nfrom these Partition-DPPs.\\r\\nDefinition 7 (Partition-DPP). Given a symmetric n \\xc3\\x97 n matrix L \\x17 0 and a partitioning of [n] =\\r\\nV1 \\xe2\\x88\\xaa V2 \\xe2\\x88\\xaa \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xe2\\x88\\xaa Vr into r = O(1) partitions together with c1 , . . . , cr \\xe2\\x88\\x88 Z \\xe2\\x89\\xa50 , the Partition-DPP is the\\r\\ndistribution of the DPP defined by L restricted to sets S that have |S \\xe2\\x88\\xa9 Vi | = ci for all i.\\r\\nIn this work, we establish as corollaries of Theorem 1, a roughly quadratic parallel speedup in\\r\\nsampling from all of the aforementioned distributions. A crucial part of our algorithm relies on\\r\\nthe existence of highly parallel counting oracles for these models. For example, for unconstrained\\r\\nDPPs, the partition function can be written as\\r\\n\\r\\n                                            \\xe2\\x88\\x91 det( LS,S ) = det( L + I ),\\r\\n                                             S\\r\\n\\r\\nand this can be computed in NC [Csa75]. For k-DPPs and Partition-DPPs, the partition function\\r\\ncan be computed via polynomial interpolation [Cel+16], which is again highly parallelizable\\r\\n(by, e.g., solving linear systems of equations involving Vandermonde matrices). The entropic\\r\\nindependence of all the determinantal distributions discusses in this work was established by\\r\\nAlimohammadi, Anari, Shiragur, and Vuong [Ali+21] and Anari, Jain, Koehler, Pham, and Vuong\\r\\n[Ana+21b].\\r\\nTheorem 8 (Sampling from non-symmetric DPPs). Let L be a n \\xc3\\x97 n non-symmetric PSD matrix,\\r\\n\\xc7\\xab \\xe2\\x88\\x88 (0, 1), and k \\xe2\\x88\\x88 [n].\\r\\n\\r\\n   1. Let \\xc2\\xb5k : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be the k-DPP defined by L. For any constant c > 0, there exists an algorithm\\r\\n                                                                                  \\x10\\xe2\\x88\\x9a        \\x11\\r\\n      to approximately sample from within \\xc7\\xab total variation distance of \\xc2\\xb5k in Oe     k( k )c parallel time\\r\\n                                                                                                \\xc7\\xab\\r\\n      using (n/\\xc7\\xab)O(1/c) machines.\\r\\n   2. Let \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be the DPP defined by L. For any constant c > 0,\\xe2\\x88\\x9athere exists\\r\\n                                                                                    \\x01   an algorithm to\\r\\n                                                                          e\\r\\n      approximately sample from within \\xc7\\xab total variation distance of \\xc2\\xb5 in O n( n\\xc7\\xab )c parallel time using\\r\\n      (n/\\xc7\\xab)O(1/c) machines.\\r\\nTheorem 9 (Sampling from partition-DPPs). Let L be a n \\xc3\\x97 n symmetric PSD matrix. Let r = O(1),\\r\\nand let V1 \\xe2\\x88\\xaa \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xe2\\x88\\xaa Vr = [n] be a partition of [n] together with integers t1 , . . . , tr . Let k = \\xe2\\x88\\x91i\\xe2\\x88\\x88[r ] ti . Let\\r\\n\\xc2\\xb5 L;V,t : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be the DPP with partition constraints defined by\\r\\n                                                                 r\\r\\n                                    \\xc2\\xb5 L;V,t (S) \\xe2\\x88\\x9d det( LS,S ) \\xc2\\xb7 \\xe2\\x88\\x8f 1 [|S \\xe2\\x88\\xa9 Vi | = ti ].\\r\\n                                                                i=1\\r\\n\\r\\nFor any constant c > \\x10   0, there exists an algorithm to approximately sample from within \\xc7\\xab total variation\\r\\n                          \\xe2\\x88\\x9a k \\x11\\r\\n                       e\\r\\ndistance of \\xc2\\xb5 L;V,t in O     k( )c parallel time using (n/\\xc7\\xab)O(1/c) machines.\\r\\n                                \\xc7\\xab\\r\\n\\r\\nIn the case of symmetric DPPs\\xe2\\x88\\x9aand symmetric k-DPPs, we are able to improve Theorem 1 to\\r\\n                             e ( k). Our algorithms below have a small chance \\xce\\xb4 of failure, but\\r\\nobtain a parallel runtime of O\\r\\nconditioned on success they sample exactly from the desired distribution; this is desirable, as we\\r\\ncan repeat the algorithm in the case of failure, to sample exactly from the desired distribution.\\r\\nTheorem 10 is proven in Section 4.\\r\\n\\r\\n\\r\\n                                                            5\\r\\n\\x0c\\n\\nTheorem 10 (Sampling from symmetric DPPs). Let L be a n \\xc3\\x97 n symmetric PSD matrix, k \\xe2\\x88\\x88 [n], and\\r\\n\\xce\\xb4 \\xe2\\x88\\x88 (0, 1).\\r\\n\\r\\n   1. Let \\xc2\\xb5k : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be the k-DPP defined by L. There exists an algorithm that with probability\\r\\n                                             \\xe2\\x88\\x9a\\r\\n                                          e ( k) parallel time using poly(n) \\xc2\\xb7 log k machines.\\r\\n      \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 \\xce\\xb4, exactly samples from \\xc2\\xb5k in O                                        \\xce\\xb4\\r\\n\\r\\n   2. Let \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be the DPP defined\\r\\n                                           \\xe2\\x88\\x9a by L. There exists an algorithm, nthat with probability\\r\\n                                         e\\r\\n      \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 \\xce\\xb4, exactly samples from \\xc2\\xb5 in O( n) parallel time using poly(n) \\xc2\\xb7 log \\xce\\xb4 machines.\\r\\nWe are also able to refine our results about DPPs so that the runtime is expressed in terms of\\r\\ntypical sizes of the sets S in the support, as measured by eigenvalues or traces of the matrix L.\\r\\nWe leave the details to Section 5, where we prove Theorem 28.\\r\\n\\r\\n1.2 Techniques and algorithms\\r\\nThroughout, we heavily use the fact that for all distributions \\xc2\\xb5 that we study in this paper, the\\r\\nmarginals P S\\xe2\\x88\\xbc\\xc2\\xb5 [i \\xe2\\x88\\x88 S] can be computed in NC, and that the distributions \\xc2\\xb5 are self-reducible \\xe2\\x80\\x94\\r\\nby conditioning on element inclusion, we obtain another distribution in the same family of DPP\\r\\nvariants. These two properties alone are the basis of the most classical (inherently sequential)\\r\\nalgorithm for sampling from DPPs, which we describe below.\\r\\n\\r\\nfor i = 1, . . . , k do\\r\\n    Compute the marginals of \\xc2\\xb5 conditioned on elements x1 , . . . , xi\\xe2\\x88\\x921 .\\r\\n    Sample an element outside x1 , . . . , xi\\xe2\\x88\\x921 with probability proportional to the computed\\r\\n     marginals. Call the sampled element xi .\\r\\nreturn { x1 , . . . , xk }.\\r\\n\\r\\nWe show that rejection sampling can be used to speed up this algorithm. Roughly speaking, we\\r\\ncompute marginals of \\xc2\\xb5, and sample a batch of elements x1 , . . . , x\\xe2\\x84\\x93 i.i.d. from these marginals.\\r\\nWe then use rejection sampling to accept or reject the batch to make sure any set { x1 , . . . , x\\xe2\\x84\\x93 }\\r\\nis selected with probability given by the \\xe2\\x84\\x93-order marginals \\xe2\\x88\\x9d PS\\xe2\\x88\\xbc\\xc2\\xb5 [{ x1 , . . . , x\\xe2\\x84\\x93 } \\xe2\\x8a\\x86 S]. Once we\\r\\nhave a batch of elements successfully accepted, we continue sampling the next batch from the\\r\\ndistribution conditioned on including this batch. A high-level description of this algorithm can\\r\\nbe seen in Algorithm 1. Our innovation is to implement the batch sampling step highlighted via\\r\\n(*) by i.i.d. sampling from marginals and performing a correction based on rejection sampling.\\r\\n\\r\\nAlgorithm 1: Batched sampling\\r\\nInput: \\xc2\\xb5 : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50\\r\\nk0 \\xe2\\x86\\x90 k\\r\\n\\xc2\\xb5 ( 0) \\xe2\\x86\\x90 \\xc2\\xb5             \\xe2\\x88\\x9a\\r\\nfor i = 0, 1, . . . , 2 k do\\r\\n                                           \\xe2\\x88\\x9a\\r\\n      (*): Sample Ti \\xe2\\x88\\xbc \\xc2\\xb5(i) with | Ti | = \\xe2\\x8c\\x88 ki \\xe2\\x8c\\x89\\r\\n      Update \\xc2\\xb5(i+1) \\xe2\\x86\\x90 \\xc2\\xb5(i) (\\xc2\\xb7 | Ti )\\r\\n      Update ki+1 = ki \\xe2\\x88\\x92 | Ti |\\r\\n               S\\r\\nreturn T := i Ti .\\r\\n\\r\\nClearly, the sizes of the batches we sample dictate the parallel runtime\\r\\n                                                                  \\xe2\\x88\\x9a      of this algorithm. Even\\r\\nfor symmetric DPPs, there is a natural barrier at batch size \\xe2\\x84\\x93 \\xe2\\x89\\x83 k. Consider L to be the Gram\\r\\n\\r\\n\\r\\n                                                   6\\r\\n\\x0c\\n\\nmatrix of vectors {e1 , e1 , . . . , ek , ek } (where every standard basis vector ei \\xe2\\x88\\x88 R n is repeated twice,\\r\\nso k = n2 ). The marginals of this DPP are uniform, but because of the Birthday Paradox, any\\r\\n              \\xe2\\x88\\x9a\\r\\nsample of \\xe2\\x89\\xab k elements contains a pair of identical vectors       \\xe2\\x88\\x9a    with high probability, resulting in a\\r\\nDPP weight of 0. Hence, we must set the batch size \\xe2\\x84\\x93 . k to have a good acceptance probability.\\r\\nA significant difficulty that arises for DPP variants beyond symmetric DPPs is the lack of negative\\r\\ndependence. This not only poses an analysis challenge, but presents an algorithmic difficulty as\\r\\nwell. Roughly speaking, due to the lack of negative dependence, the acceptance probabilities\\r\\nused in symmetric DPPs for rejection sampling have to be scaled down in other cases by a factor\\r\\nof \\xe2\\x89\\x83 2\\xe2\\x84\\x93 ; otherwise we would sometimes have to accept with probability > 1. We overcome\\r\\nthis challenge by replacing rejection sampling with approximate rejection sampling, where we\\r\\nallow the acceptance probabilities to go above 1 on a small subset of the event space, and if we\\r\\nsee any batch of this kind we declare the algorithm has failed. Our main insight is that such\\r\\nbad batches of elements must consist of large groups of highly correlated elements. On the other\\r\\nhand, we quantify limits on correlations in all of our models using the recently established notion\\r\\nof entropic independence [Ana+21b]. Intuitively, we prove that correlations in our model must\\r\\n                                                            1\\r\\nbe limited to small groups of elements and a batch of \\xe2\\x89\\x83 k 2 \\xe2\\x88\\x92\\xc7\\xab elements will, with high probability\\r\\nnot contain more than one element from the same group of highly correlated elements. We\\r\\nformalize this approach to prove Theorems 8 and 9 in Section 6. Finally, we give an example\\r\\nshowing that this sub-polynomial overhead in the parallel depth is likely to be necessary for our\\r\\nbatched rejection sampling approach in Section 7.\\r\\n\\r\\n1.3 Further related work\\r\\nThe prior works of [Ten95; Ana+20] study the problems of sampling spanning trees and arbores-\\r\\ncences from graphs in parallel. Spanning trees are a special case of DPPs, but there are specialized\\r\\nalgorithms for sampling from spanning trees and arborescences [Ald90; Bro89] that were paral-\\r\\nlelized in prior works; as far as we know the random-walk-based algorithms of Aldous [Ald90]\\r\\nand Broder [Bro89] have no counterpart for general DPPs.\\r\\nBeyond determinant-based distributions, Feng, Hayes, and Yin [FHY21], and more recently Liu\\r\\nand Yin [LY21], showed how to efficiently parallelize a popular class of Metropolis Markov chains\\r\\nand obtain nearly optimal parallelism for several graphical models such as the hardcore and Ising\\r\\nmodels, as well as proper colorings. While their results are stated more generally for arbitrary\\r\\nMetropolis chains satisfying certain Lipschitz conditions on log-densities, they do not directly\\r\\napply to our models. While there are efficient Markov chains for sampling from DPPs [AOR16;\\r\\nHS19; Ali+21], the Metropolis versions of these Markov chains (where a rejection filter is applied)\\r\\ndo not have a nearly-linear mixing time; in fact, even for the simplest case of symmetric DPPs,\\r\\nthey have at least a quadratic mixing time, O(nk) in the case of symmetric DPPs (this is not to be\\r\\nconfused with non-Metropolis versions of these chains which do have linear mixing time). The\\r\\nresults of Feng, Hayes, and Yin [FHY21] can only achieve a linear speedup, i.e., a reduction by a\\r\\nfactor of n, for Metropolis chains, which is moot by the existence of O\\xcc\\x83(k) parallel time sampling\\r\\nalgorithms for k-DPPs.\\r\\n\\r\\n1.4 Acknowledgements\\r\\nNima Anari and Thuy-Duong Vuong are supported by NSF CAREER Award CCF-2045354, a\\r\\nSloan Research Fellowship, and a Google Faculty Research Award. Callum Burgess was sup-\\r\\nported by a Stanford CURIS Fellowship. Kevin Tian was supported by a Google Ph.D. Fellowship\\r\\n\\r\\n                                                     7\\r\\n\\x0c\\n\\nand a Simons-Berkeley VMware Research Fellowship.\\r\\n\\r\\n\\r\\n2 Overview of approach\\r\\nIn this section, we give a technical outline of how we prove our main results. We also provide a\\r\\nroadmap of the rest of the paper. All preliminaries can be found in Section 3.\\r\\n\\r\\nSymmetric DPPs. In Section 4, we prove our most basic result, Theorem 10 (sampling symmet-\\r\\nric DPPs), as an introduction to our techniques and to demonstrate how we apply Algorithm 1.\\r\\nConveniently, symmetric DPPs exhibit strong negative dependence properties which the rest of\\r\\n                               \\xe2\\x88\\x9a Theorem 10, we directly bound the acceptance probability of\\r\\nour applications do not. To prove\\r\\nAlgorithm 1 for batch size \\xe2\\x84\\x93 \\xe2\\x89\\x83 k. Specifically, we show that for such a batch size, directly apply-\\r\\n                                                                        2\\r\\ning negative dependence bounds the acceptance probability by exp(\\xe2\\x88\\x92 \\xe2\\x84\\x93k ). By a simple recursion\\r\\n                                                                    \\xe2\\x88\\x9a\\r\\nargument to bound the overall parallel depth of our sampler by O( k), we obtain Theorem 10.\\r\\n\\r\\nBounded symmetric DPPs. In Section 5, we give improved sampling guarantees in terms of\\r\\nparallel depth for DPPs with kernel matrices exhibiting bounded spectral structure by proving\\r\\nTheorem 28. The two types of spectral bounds we consider are parameterized by the kernel\\r\\nmatrix trace and largest eigenvalue. To obtain the first result, we apply a concentration bound\\r\\nfrom [PP13] to show that the size of the sample concentrates tightly around its mean, which by\\r\\nlinearity of expectation is the trace. This implies by a strategy outlined in Remark 12 that with\\r\\nhigh probability, the parallel depth is bounded by a function of the trace, via our algorithm in\\r\\nTheorem 10.\\r\\nFor our other result, we appeal to an alternative analysis of our rejection sampling, which uses\\r\\nproperties of negatively-correlated distributions. We use Algorithm 4, a modification of Algo-\\r\\nrithm 1 based on directly manipulating the kernel matrix, as our base method. We show that\\r\\nif the kernel matrix is scaled down by a factor \\xce\\xb1 so its largest eigenvalue is bounded by \\xe2\\x89\\x88 \\xe2\\x88\\x9a1n ,\\r\\nwe can achieve polylogarithmic parallel depth for each run of Algorithm 4. We also prove a\\r\\ncharacterization of this scaling down procedure as randomly dropping elements from a sample\\r\\nfrom the original DPP, so that each element is kept with probability \\xce\\xb1. By setting \\xce\\xb1 \\xe2\\x89\\x88 \\xce\\xbb (1K)\\xe2\\x88\\x9an ,\\r\\n                                                                                        max\\r\\nstandard binomial concentration bounds the number of calls to this \\xe2\\x80\\x9cscaled-down sampler\\xe2\\x80\\x9d and\\r\\nyields the other half of Theorem 28.\\r\\n\\r\\nEntropic independence. In Section 6, we provide a meta-result (Theorem 33, a formal restate-\\r\\nment of Theorem 1) used to derive Theorems 8 and 9 as corollaries. Theorem 33 shows that for\\r\\nany constant-entropically independent distribution supported on subsets of size k, we can reduce\\r\\n                                                                  e (k 12 +c ) with high probability.\\r\\nsampling to marginal computations at a parallel depth overhead of O\\r\\nHere, c is any constant, which parameterizes the (polynomial) number of machines used.\\r\\nTo demonstrate Theorem 33, we use the assumed entropic independence property to derive\\r\\nconcentration bounds on the acceptance probability in Algorithm 3 applied to our distribution.\\r\\nAs a first step towards this goal, we use entropic independence to demonstrate that up to parallel\\r\\n               1\\r\\ndepth \\xe2\\x84\\x93 \\xe2\\x89\\x88 k 2 \\xe2\\x88\\x92c , the KL divergence between our target distribution (the \\xe2\\x84\\x93-marginals) and our\\r\\nproposal distribution (the product distribution on 1-marginals) is bounded.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 8\\r\\n\\x0c\\n\\nThis KL divergence bound does not suffice for our overall scheme; intuitively, it provides an \\xe2\\x80\\x9cav-\\r\\nerage case\\xe2\\x80\\x9d bound on the log-acceptance probability of Algorithm 3, whereas\\xe2\\x88\\x9a we would like to\\r\\nargue a high probability bound, since we need to union bound over at least k stages of rejection\\r\\nsampling. To simplify our concentration argument, we begin by assuming without loss of gener-\\r\\nality that our distribution has roughly even 1-marginals, by using a subdivision process used in\\r\\n[AD20; Ana+21a]. We then use comparison inequalities between KL divergences and (exponen-\\r\\ntiated) Renyi divergences for nearly-uniform distributions to bound moments associated with\\r\\nour rejection sampler\\xe2\\x80\\x99s acceptance probabilities. Finally, we use these moment bounds to show\\r\\nthat over a high-probability set of outcomes (in the sense of Algorithm 3), the log-acceptance\\r\\nprobability is a submartingale, which yields concentration via Markov\\xe2\\x80\\x99s inequality.\\r\\n\\r\\nHard instance. It is natural to ask: can we improve the subpolynomial overhead in Theorem 33\\r\\n(and hence, Theorems 8 and 9) to a smaller overhead, e.g. polylogarithmic? In Section 7, we\\r\\ngive a hard example showing that the subpolynomial overhead may be inherent to rejection\\r\\nsampling strategies, at least in the full generality of entropically independent distributions. In\\r\\nparticular, our hard instance pairs indices in [n], randomly chooses 2k of these pairs, and then\\r\\nincludes both elements in each of the 2k selected pairs. This distribution is a k-nonsymmetric\\r\\n                                                                                      \\x14      \\x15\\r\\n                                                           n                            0 \\xe2\\x88\\x921\\r\\nDPP defined by a block-diagonal matrix L composed of 2 2 \\xc3\\x97 2 blocks, all equal to              . To\\r\\n                                                                                        1 0\\r\\ndemonstrate hardness, we first argue that to succeed with good probability on a polynomially-\\r\\nbounded number of parallel machines, our rejection sampler must have the property that it is\\r\\nlikely only a constant number of \\xe2\\x80\\x9cduplicates\\xe2\\x80\\x9d are encountered when drawing samples from\\r\\nthe product distribution. We then show using a Birthday Paradox-like\\r\\n                                                                  \\xe2\\x88\\x9a       analysis that under this\\r\\nrestriction, our batch size \\xe2\\x84\\x93 must be polynomially smaller than k, yielding our claim.\\r\\n\\r\\n\\r\\n3 Preliminaries\\r\\nIn this section, we provide preliminaries for the rest of the paper.\\r\\nWe use [n] to denote the set {1, . . . , n}. For a set S, (Sk) denotes the family of subsets of size k.\\r\\n\\r\\nFor a distribution \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 and T \\xe2\\x8a\\x86 [n], define \\xc2\\xb5(\\xc2\\xb7 | T ) to be the distribution on 2[n]\\\\ T\\r\\ndefined by \\xc2\\xb5( F | T ) \\xe2\\x88\\x9d \\xc2\\xb5( F \\xe2\\x88\\xaa T ). We will sometimes use the shorthand \\xc2\\xb5| T .\\r\\n\\r\\nFor density function \\xc2\\xb5 : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 , the generating polynomial of \\xc2\\xb5 is the multivariate k-\\r\\nhomogeneous polynomial defined as follows:\\r\\n\\r\\n                                    g\\xc2\\xb5 ( z1 , . . . , z n ) =     \\xe2\\x88\\x91 \\xc2\\xb5( S ) \\xe2\\x88\\x8f zi .\\r\\n                                                                S \\xe2\\x88\\x88([nk ])   i\\xe2\\x88\\x88S\\r\\n\\r\\n\\r\\n\\r\\n3.1 Determinantal point processes\\r\\nA DPP on n items defines a probability distribution over subsets Y \\xe2\\x8a\\x86 [n]. It is parameterized by\\r\\na matrix L \\xe2\\x88\\x88 R n\\xc3\\x97n : P L [Y ] \\xe2\\x88\\x9d det( LY ), where LY is the principal submatrix whose columns and\\r\\nrows are indexed by Y. We call L the ensemble matrix. We define the marginal kernel K of P L by\\r\\n\\r\\n                           K = L ( I + L ) \\xe2\\x88\\x921 = I \\xe2\\x88\\x92 ( I + L ) \\xe2\\x88\\x921 = ( L \\xe2\\x88\\x921 + I ) \\xe2\\x88\\x921 .                      (1)\\r\\n\\r\\n\\r\\n\\r\\n                                                            9\\r\\n\\x0c\\n\\nThen, det(K A ) = P L [ A \\xe2\\x8a\\x86 Y ] (a proof can be found in [KT12a]). This also implies K \\x16 I for\\r\\nsymmetric K. Conversely,\\r\\n\\r\\n                           L = K ( I \\xe2\\x88\\x92 K ) \\xe2\\x88\\x921 = ( I \\xe2\\x88\\x92 K ) \\xe2\\x88\\x921 \\xe2\\x88\\x92 I = ( K \\xe2\\x88\\x921 \\xe2\\x88\\x92 I ) \\xe2\\x88\\x921 .                    (2)\\r\\n\\r\\nGiven a cardinality constraint k, the k-DPP parameterized by L is a distribution over subsets Y\\r\\n                                     det( L )\\r\\nof size k, defined by P kL [Y ] = \\xe2\\x88\\x91 \\xe2\\x80\\xb2 detY( L \\xe2\\x80\\xb2 ) . To ensure that P L defines a probability distribution,\\r\\n                                    |Y |= k   Y\\r\\nall principal minors of L must be non-negative: det( LS ) \\xe2\\x89\\xa5 0. Matrices that satisfy this property\\r\\nare called P0 -matrices [Fan89, Definition 1]. Any nonsymmetric (or symmetric) PSD matrix is\\r\\nautomatically a P0 -matrix [Gar+19, Lemma 1].\\r\\nConsider a matrix L \\xe2\\x88\\x88 R n\\xc3\\x97n , partition V1 \\xe2\\x88\\xaa \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xe2\\x88\\xaa Vr = [n] of [n], and tuple {ci }ri=1 of integers. The\\r\\nDPP with partition constraint (Partition-DPP) \\xc2\\xb5 L;V,c : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 is defined by\\r\\n\\r\\n                                \\xc2\\xb5 L;V,c (S) \\xe2\\x88\\x9d 1[\\xe2\\x88\\x80i : |S \\xe2\\x88\\xa9 Vi | = ci ] det( LS,S )\\r\\n\\r\\nFor any Y \\xe2\\x8a\\x86 [n], if we condition the distribution P L (P kL resp.) on the event that items in Y are\\r\\nincluded in the sample, we still get a DPP ((k \\xe2\\x88\\x92 |Y |)-DPP resp.); the new ensemble matrix is\\r\\n                                                 \\xe2\\x88\\x921\\r\\ngiven by the Schur complement LY = LY\\xcc\\x83 \\xe2\\x88\\x92 LY\\xcc\\x83,Y LY,Y LY,Y\\xcc\\x83 where Y\\xcc\\x83 = [n] \\\\ Y.\\r\\nFor Partition-DPPs, a similar statement holds. Conditioning \\xc2\\xb5 L;V,c on Y being included in the set\\r\\nresults in a Partition-DPP \\xc2\\xb5 LY ;V \\xe2\\x80\\xb2 ,c\\xe2\\x80\\xb2 with ensemble matrix LY and partition V1\\xe2\\x80\\xb2 \\xe2\\x88\\xaa \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xe2\\x88\\xaa Vr\\xe2\\x80\\xb2 = [n] \\\\ Y\\r\\nwith Vi\\xe2\\x80\\xb2 = Vi \\\\ Y, and c\\xe2\\x80\\xb2i = ci \\xe2\\x88\\x92 |Vi \\xe2\\x88\\xa9 Y |.\\r\\nProposition 11. Suppose \\xc2\\xb5 is one of the following distributions.\\r\\n   1. k-DPP: \\xc2\\xb5(S) \\xe2\\x88\\x9d 1[|Y | = k] det( LS ).\\r\\n   2. DPP: \\xc2\\xb5(S) \\xe2\\x88\\x9d det( LS ).\\r\\n   3. Partition-DPP: \\xc2\\xb5V,c (S) \\xe2\\x88\\x9d 1[\\xe2\\x88\\x80i \\xe2\\x88\\x88 [r] : |S \\xe2\\x88\\xa9 Vi | = ci ] det( LS ) with r = O(1).\\r\\n                                                         e (1)-parallel time using poly(n) machines.\\r\\nThere are algorithms that perform the following tasks in O\\r\\n   1. Given S \\xe2\\x8a\\x86 T \\xe2\\x8a\\x86 [n], exactly computes the conditional probabilities \\xc2\\xb5( T | S).\\r\\n   2. Given S \\xe2\\x8a\\x86 [n] and integer t \\xe2\\x88\\x88 [n], exactly computes P T \\xe2\\x88\\xbc\\xc2\\xb5 [| T | = t].\\r\\n\\r\\nProof of Proposition 11. First, note that DPPs and k-DPPs correspond to Partition-DPPs with 0\\r\\nand 1 partition constraints respectively. Thus, we only need to show the claim for \\xc2\\xb5 being\\r\\na Partition-DPP with O(1) constraints. Computing the marginals is equivalent to computing\\r\\nthe partition functions of \\xc2\\xb5 and of \\xc2\\xb5 conditioned on subsets. As shown in [Cel+17, Theorem\\r\\n1.1], computing the partition function is equivalent to computing the coefficients of a certain\\r\\nunivariate polynomial which can be evaluated efficiently given access to g\\xc2\\xb5 . Evaluating g\\xc2\\xb5 at\\r\\n                                                                                             e (1)-parallel\\r\\n(z1 , . . . , zn ) is equivalent to computing det( L + diag(zi )ni=1 ), which can be done in O\\r\\ntime [Ber84].\\r\\n\\r\\nRemark 12. To sample from a DPP \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 , we can first in constant parallel-time compute\\r\\nthe distribution H on [n] defined by P H [k] := P S\\xe2\\x88\\xbc\\xc2\\xb5 [|S| = k] and sample the cardinality of the set\\r\\nk from H, and then sample S from \\xc2\\xb5k using the results of this paper.\\r\\n\\r\\n\\r\\n\\r\\n                                                       10\\r\\n\\x0c\\n\\n3.2 Real-stable polynomials and negative correlation\\r\\nIn this section, we define various polynomial properties and their relationships.\\r\\n                                                                 \\x08\\r\\nDefinition 13. Consider an open half-plane H\\xce\\xb8 = e\\xe2\\x88\\x92i\\xce\\xb8 z Im(z) > 0 \\xe2\\x8a\\x86 C. We say a polynomial\\r\\ng(z1 , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , zn ) \\xe2\\x88\\x88 C [z1 , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , zn ] real-stable if g has real coefficients and does not have any root in\\r\\nH0n . In particular, the zero polynomial is real-stable.\\r\\nWe say that distribution \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 is strongly Rayleigh if and only if its generating polyno-\\r\\nmial is real stable [see BBL09]. If \\xc2\\xb5 is strongly Rayleigh then for any set F \\xe2\\x8a\\x86 [n], the conditional\\r\\ndistribution \\xc2\\xb5(\\xc2\\xb7 | F ) is also strongly Rayleigh. A useful property of strongly Rayleigh polynomi-\\r\\nals is that they satisfy negative correlation, defined in the following.\\r\\nLemma 14 (Negative correlation). Suppose \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 is strongly Rayleigh. For any set T\\r\\n                                      P S \\xe2\\x88\\xbc \\xc2\\xb5 [ T \\xe2\\x8a\\x86 S ] \\xe2\\x89\\xa4 \\xe2\\x88\\x8f P S \\xe2\\x88\\xbc \\xc2\\xb5 [i \\xe2\\x88\\x88 S ].\\r\\n                                                           i\\xe2\\x88\\x88 T\\r\\n\\r\\n\\r\\nLemma 15. Let L be a symmetric PSD matrix. The following distributions are strongly Rayleigh.\\r\\n\\r\\n   1. \\xc2\\xb5k : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 , the k-DPP defined by L.\\r\\n\\r\\n   2. \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 , the DPP defined by L.\\r\\nCorollary 16. Let K \\xe2\\x88\\x88 R n\\xc3\\x97n satisfy 0 \\x16 K \\x16 I. For any set T \\xe2\\x8a\\x86 [n],\\r\\n                                                 det(K T ) \\xe2\\x89\\xa4 \\xe2\\x88\\x8f Ki,i\\r\\n                                                               i\\xe2\\x88\\x88 T\\r\\n\\r\\nProof. Consider the DPP \\xc2\\xb5 with kernel matrix K. Apply Lemma 14 and note that det(K T ) =\\r\\nP S\\xe2\\x88\\xbc\\xc2\\xb5 [ T \\xe2\\x8a\\x86 S] and Ki,i = PS\\xe2\\x88\\xbc\\xc2\\xb5 [i \\xe2\\x88\\x88 S].\\r\\n\\r\\n\\r\\n                                                                                                          \\xe2\\x88\\x9a\\r\\nLemma 17. Let \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be a strongly Rayleigh distribution. Suppose E S\\xe2\\x88\\xbc\\xc2\\xb5 [|S|] \\xe2\\x89\\xa4                       n. For\\r\\n\\xc7\\xab \\xe2\\x88\\x88 (0, 14 ), there exists an absolute constant c > 0 such that\\r\\n                                              \"        r          #\\r\\n                                                                1\\r\\n                                         P S\\xe2\\x88\\xbc\\xc2\\xb5 |S| \\xe2\\x89\\xa5 c n log        \\xe2\\x89\\xa4\\xc7\\xab\\r\\n                                                                \\xc7\\xab\\r\\n\\r\\nand                                         \\x14                             \\x15\\r\\n                                                                        1\\r\\n                                    P S\\xe2\\x88\\xbc\\xc2\\xb5       |S| \\xe2\\x89\\xa5 c E S\\xe2\\x88\\xbc\\xc2\\xb5 [|S|] log     \\xe2\\x89\\xa4\\xc7\\xab\\r\\n                                                                        \\xc7\\xab\\r\\n\\r\\nProof. Let f (S) = |S| correspond to the 1-Lipschitz (with respect to the Hamming metric) func-\\r\\ntion of taking the magnitude of a sample S from \\xc2\\xb5, and let f indicate the value E S\\xe2\\x88\\xbc\\xc2\\xb5 [ f ].\\r\\nBy applying [PP13, Theorem 3.2], it follows that\\r\\n                                                                      \\x12                 \\x13\\r\\n                                                                              \\xe2\\x88\\x92 a2\\r\\n                                 P S\\xe2\\x88\\xbc\\xc2\\xb5 [ f \\xe2\\x88\\x92 f > a] \\xe2\\x89\\xa4 3 exp                               .\\r\\n                                                                          16( a + 2 f )\\r\\n                                      q                                                                   q\\r\\n                                                                          2\\r\\nFor the first statement, let a = 10       n log 1\\xc7\\xab , and note exp(\\xe2\\x88\\x92 16( aa+2 f\\xc2\\xaf) ) \\xe2\\x89\\xa4 3\\xc7\\xab and a + f\\xc2\\xaf \\xe2\\x89\\xa4 11       n log 1\\xc7\\xab .\\r\\n                                                       2\\r\\nFor the second, let a = f\\xc2\\xaf log 1\\xc7\\xab and note exp(\\xe2\\x88\\x92 16( aa+2 f\\xc2\\xaf) ) \\xe2\\x89\\xa4 3\\xc7\\xab and a + f\\xc2\\xaf \\xe2\\x89\\xa4 2 f\\xc2\\xaf log 1\\xc7\\xab .\\r\\n\\r\\n                                                          11\\r\\n\\x0c\\n\\n3.3 Fractional log-concavity and entropic independence\\r\\nWe recall the notions of fractional log-concavity [Ali+21] and entropic independence [Ana+21b].\\r\\n\\r\\nDefinition 18 ([Ali+21]). A probability distribution \\xc2\\xb5 : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 is \\xce\\xb1-fractionally-log-concave if\\r\\ng\\xc2\\xb5 (z1\\xce\\xb1 , . . . , z\\xce\\xb1n ) is log-concave for z1 , . . . , zn \\xe2\\x88\\x88 R n\\xe2\\x89\\xa50 . If \\xce\\xb1 = 1, we say \\xc2\\xb5 is log-concave.\\r\\nTo define entropic independence we need the definition of the \\xe2\\x80\\x9cdown\\xe2\\x80\\x9d operator. In brief, Dk\\xe2\\x86\\x92\\xe2\\x84\\x93\\r\\ntransitions from a set S of size k to a uniformly random subset of size \\xe2\\x84\\x93.\\r\\n                                                                                           ([n ])\\xc3\\x97([n\\xe2\\x84\\x93 ])\\r\\nDefinition 19 (Down operator). For \\xe2\\x84\\x93 \\xe2\\x89\\xa4 k define the row-stochastic matrix Dk\\xe2\\x86\\x92\\xe2\\x84\\x93 \\xe2\\x88\\x88 R \\xe2\\x89\\xa5k0                      by\\r\\n                                             (\\r\\n                                               0    if T 6\\xe2\\x8a\\x86 S\\r\\n                               Dk\\xe2\\x86\\x92\\xe2\\x84\\x93 (S, T ) = 1\\r\\n                                                k   otherwise.\\r\\n                                                        ( \\xe2\\x84\\x93)\\r\\n\\r\\nNote that for a distribution \\xc2\\xb5 on size-k sets, \\xc2\\xb5Dk\\xe2\\x86\\x92\\xe2\\x84\\x93 will be a distribution on size-\\xe2\\x84\\x93 sets. In\\r\\nparticular, \\xc2\\xb5Dk\\xe2\\x86\\x921 will be the vector of normalized marginals of \\xc2\\xb5: { 1k P [i \\xe2\\x88\\x88 S]}i\\xe2\\x88\\x88[n] .\\r\\n\\r\\nDefinition 20. A probability distribution \\xc2\\xb5 on ([nk ]) is said to be \\xce\\xb11 -entropically independent, for\\r\\n\\xce\\xb1 \\xe2\\x88\\x88 (0, 1], if for all probability distributions \\xce\\xbd on ([nk ]),\\r\\n                                                                 1\\r\\n                                D KL (\\xce\\xbdDk\\xe2\\x86\\x921 k \\xc2\\xb5Dk\\xe2\\x86\\x921 ) \\xe2\\x89\\xa4             D KL (\\xce\\xbd k \\xc2\\xb5).\\r\\n                                                                 \\xce\\xb1k\\r\\n\\r\\nLemma 21 ([Ana+21b], Theorem 4). If \\xc2\\xb5 is \\xce\\xb1-FLC then \\xc2\\xb5 and all conditional distributions of \\xc2\\xb5, i.e.\\r\\n\\xc2\\xb5(\\xc2\\xb7 | S) for any S \\xe2\\x8a\\x86 [n], are 1\\xce\\xb1 -entropically independent.\\r\\nLemma 22 ([Ali+21]). The following distributions are \\xce\\xb1-FLC for \\xce\\xb1 = \\xe2\\x84\\xa6(1) and k \\xe2\\x88\\x88 [n].\\r\\n   1. k-DPP and DPP defined by nonsymmetric PSD L \\xe2\\x88\\x88 R n\\xc3\\x97n .\\r\\n   2. Partition-DPP defined by symmetric PSD L \\xe2\\x88\\x88 R n\\xc3\\x97n and a partition {Vi }ri=1 with r = O(1).\\r\\n\\r\\n3.4 Rejection sampling\\r\\n                                                                                                     \\xc2\\xb5( x)\\r\\nConsider distributions \\xc2\\xb5, \\xce\\xbd over the same domain, and parameter C such that maxx \\xe2\\x88\\x88supp(\\xce\\xbd) \\xce\\xbd( x ) \\xe2\\x89\\xa4\\r\\nC. Assuming sample access to \\xce\\xbd, we can also sample from \\xc2\\xb5 via rejection sampling as follows.\\r\\n\\r\\nAlgorithm 2: Rejection sampling\\r\\n                                                      \\xc2\\xb5( x)\\r\\nInput: parameter C > 0 such that maxx \\xe2\\x88\\x88supp(\\xce\\xbd) \\xce\\xbd( x ) \\xe2\\x89\\xa4 C.\\r\\nSample x \\xe2\\x88\\xbc \\xce\\xbd.\\r\\n                                     \\xc2\\xb5( x)\\r\\nAccept and output x with probability C\\xce\\xbd( x ) .\\r\\n\\r\\n\\r\\nWhen Algorithm 2 succeeds, its output distribution is exactly \\xc2\\xb5, since\\r\\n                                                               \\xc2\\xb5( x)\\r\\n                            P [Algorithm 2 outputs x] \\xe2\\x88\\x9d               \\xce\\xbd( x ) \\xe2\\x88\\x9d \\xc2\\xb5 ( x ).\\r\\n                                                               C\\xce\\xbd( x)\\r\\nAlgorithm 2 succeeds with probability\\r\\n                                                      \\xc2\\xb5( x)           1\\r\\n                                     P [accept] = \\xe2\\x88\\x91           \\xce\\xbd( x ) = .\\r\\n                                                    x C\\xce\\xbd( x )         C\\r\\n\\r\\n                                                      12\\r\\n\\x0c\\n\\nFor any \\xce\\xb4 \\xe2\\x88\\x88 (0, 1), by running C log \\xce\\xb4\\xe2\\x88\\x921 copies of the algorithm in parallel and taking the first\\r\\naccepted copy, we can boost the acceptance rate to 1 \\xe2\\x88\\x92 \\xce\\xb4, stated formally in the following.\\r\\nProposition 23. There is an algorithm that with probability 1 \\xe2\\x88\\x92 \\xce\\xb4, outputs a sample from \\xc2\\xb5 in the same\\r\\nasymptotic parallel time as required to sample from \\xce\\xbd, using O(Cpoly(n) log 1\\xce\\xb4 ) machines.\\r\\nWe consider the following modification of Algorithm 2 when we have a weaker assumption that\\r\\n                               \\xc2\\xb5( x)\\r\\nfor some \\xe2\\x84\\xa6 \\xe2\\x8a\\x86 supp(\\xce\\xbd) : maxx \\xe2\\x88\\x88\\xe2\\x84\\xa6 \\xce\\xbd( x ) \\xe2\\x89\\xa4 C where \\xe2\\x88\\x91 x \\xe2\\x88\\x88\\xe2\\x84\\xa6 \\xc2\\xb5( x) \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 \\xc7\\xab for \\xc7\\xab \\xe2\\x88\\x88 [0, 1).\\r\\n\\r\\nAlgorithm 3: Modified rejection sampling\\r\\n                                                                     \\xc2\\xb5( x)\\r\\nInput: \\xe2\\x84\\xa6 \\xe2\\x8a\\x86 supp(\\xce\\xbd), parameter C > 0 such that max x \\xe2\\x88\\x88\\xe2\\x84\\xa6 \\xce\\xbd( x ) \\xe2\\x89\\xa4 C\\r\\nSample x \\xe2\\x88\\xbc \\xce\\xbd.\\r\\n                                                \\xc2\\xb5( x)\\r\\nIf x \\xe2\\x88\\x88 \\xe2\\x84\\xa6, accept and output x with probability C\\xc2\\xb5( x ) .\\r\\n\\r\\n\\r\\nThe guarantees of Algorithm 3 follow immediately from Proposition 24 and the fact that the\\r\\nrestriction of \\xc2\\xb5 to \\xe2\\x84\\xa6 has total variation at most \\xc7\\xab from \\xc2\\xb5. In particular, we will use Proposition 24\\r\\nwith \\xce\\xb4 = \\xc7\\xab and output an arbitrary sample when it fails to accept a sample.\\r\\nProposition 24. There is an algorithm that outputs a sample from \\xc2\\xb5\\xcc\\x83 in the same asymptotic parallel time\\r\\nas required to sample from \\xce\\xbd, using O(Cpoly(n) log 1\\xc7\\xab ) machines, where dTV (\\xc2\\xb5\\xcc\\x83, \\xc2\\xb5) = O(\\xc7\\xab).\\r\\n\\r\\n3.5 Divergences\\r\\nLet q, p be distributions over the same finite ground set [n]. We define the KL divergence and,\\r\\nfor \\xce\\xbb \\xe2\\x89\\xa5 1, the \\xce\\xbb-divergence between q and p as follows:\\r\\n                                               \\x14     \\x15             \\x12 \\x13\\r\\n                                                   q                 qi\\r\\n                           DKL (qk p) := Eq log        = \\xe2\\x88\\x91 qi log        ,\\r\\n                                                   p     i\\xe2\\x88\\x88[n ]\\r\\n                                                                     pi\\r\\n                                               \"\\x12 \\x13 #\\r\\n                                                 q \\xce\\xbb\\r\\n                            D\\xce\\xbb ( qk p) : = E p          = \\xe2\\x88\\x91 q\\xce\\xbbi p1i \\xe2\\x88\\x92\\xce\\xbb .\\r\\n                                                 p         i\\xe2\\x88\\x88[n ]\\r\\n\\r\\nWe remark that our definition of D\\xce\\xbb is (up to a constant scalar multiplication) the exponential of\\r\\nthe standard Renyi divergence of order \\xce\\xbb. These divergences exhibit the following useful bound.\\r\\nLemma 25. Let q, p be distributions over [n]. Suppose for some C \\xe2\\x89\\xa5 1 and S \\xe2\\x8a\\x86 [n]: pi \\xe2\\x89\\xa4 Cn for all i \\xe2\\x88\\x88 [n]\\r\\n          1\\r\\nand pi \\xe2\\x89\\xa5 Cn for all i \\xe2\\x88\\x88 S. Then, for any \\xce\\xbb \\xe2\\x89\\xa5 1, if S = [n]\\r\\n                                        \\x10                                       \\x11\\r\\n                    D \\xce\\xbb (q k p) \\xe2\\x89\\xa4 C \\xce\\xbb\\xe2\\x88\\x921 1 + n\\xce\\xbb\\xe2\\x88\\x921 \\xce\\xbb(\\xce\\xbb \\xe2\\x88\\x92 1)(D KL (q k p) + log C ) .\\r\\n\\r\\nMore generally,\\r\\n                         \\x12        \\x13 \\xce\\xbb \\xe2\\x88\\x921          \\x10                                        \\x11\\r\\n                             qi\\r\\n                  \\xe2\\x88\\x91 qi                     \\xe2\\x89\\xa4 C \\xce\\xbb\\xe2\\x88\\x921 1 + n\\xce\\xbb\\xe2\\x88\\x921 \\xce\\xbb(\\xce\\xbb \\xe2\\x88\\x92 1)(D KL (q k p) + log C ) .\\r\\n                  i\\xe2\\x88\\x88S\\r\\n                             pi\\r\\n\\r\\nProof. We first prove the inequality for the case S = [n] and C = 1. Clearly,\\r\\n                                         \\x10                                  \\x11\\r\\n                         f \\xe2\\x80\\xb2 (r) = n\\xce\\xbb\\xe2\\x88\\x921 \\xce\\xbb r\\xce\\xbb\\xe2\\x88\\x921 \\xe2\\x88\\x92 (\\xce\\xbb \\xe2\\x88\\x92 1)(1 + log r + log n)\\r\\n\\r\\n                                                           13\\r\\n\\x0c\\n\\nand                                                        \\x12         \\x13\\r\\n                                                                   1\\r\\n                                   f \\xe2\\x80\\xb2\\xe2\\x80\\xb2 (r) = n\\xce\\xbb\\xe2\\x88\\x921 \\xce\\xbb(\\xce\\xbb \\xe2\\x88\\x92 1) r\\xce\\xbb\\xe2\\x88\\x922 \\xe2\\x88\\x92     \\xe2\\x89\\xa4 0.\\r\\n                                                                   r\\r\\nThus f is concave and                                                !        \\x12 \\x13\\r\\n                                  1 n                     1 n                  1    1\\r\\n                                  n i\\xe2\\x88\\x91                    n i\\xe2\\x88\\x91\\r\\n                                        f (qi ) \\xe2\\x89\\xa4 f             qi        = f     =\\r\\n                                     =1                      =1\\r\\n                                                                               n    n\\r\\nwhich is equivalent to\\r\\n                              n                                                  n\\r\\n                             \\xe2\\x88\\x91 q\\xce\\xbbi n\\xce\\xbb\\xe2\\x88\\x921 \\xe2\\x89\\xa4 1 + n\\xce\\xbb\\xe2\\x88\\x921 \\xce\\xbb(\\xce\\xbb \\xe2\\x88\\x92 1) \\xe2\\x88\\x91 qi log(nqi ).\\r\\n                             i=1                                               i=1\\r\\n\\r\\nThe case C > 1 then follows from\\r\\n                                    n\\r\\n                   D \\xce\\xbb (q k p) = \\xe2\\x88\\x91 q\\xce\\xbbi p1i \\xe2\\x88\\x92\\xce\\xbb\\r\\n                                   i=1\\r\\n                                           n\\r\\n                              \\xe2\\x89\\xa4 C \\xce\\xbb\\xe2\\x88\\x921 \\xe2\\x88\\x91 q\\xce\\xbbi n\\xce\\xbb\\xe2\\x88\\x921\\r\\n                                          i=1\\r\\n                                                                                          !\\r\\n                                                                           n\\r\\n                              \\xe2\\x89\\xa4 C \\xce\\xbb\\xe2\\x88\\x921 1 + n\\xce\\xbb\\xe2\\x88\\x921 \\xce\\xbb(\\xce\\xbb \\xe2\\x88\\x92 1) \\xe2\\x88\\x91 qi log(nqi )\\r\\n                                                                           i=1\\r\\n                                                                                                     !!\\r\\n                                                                                 n\\r\\n                                                                                        qi\\r\\n                              \\xe2\\x89\\xa4 C \\xce\\xbb \\xe2\\x88\\x92 1 1 + n \\xce\\xbb \\xe2\\x88\\x92 1 \\xce\\xbb ( \\xce\\xbb \\xe2\\x88\\x92 1)                 \\xe2\\x88\\x91 qi log pi + log C        .\\r\\n                                                                               i=1\\r\\n\\r\\nThe case S 6= [n] follows by noticing\\r\\n                                                \\x12        \\x13 \\xce\\xbb \\xe2\\x88\\x921       n\\r\\n                                                    qi\\r\\n                                         \\xe2\\x88\\x91 qi                     \\xe2\\x89\\xa4 \\xe2\\x88\\x91 q\\xce\\xbbi p1i \\xe2\\x88\\x92\\xce\\xbb .\\r\\n                                         i\\xe2\\x88\\x88S\\r\\n                                                    pi               i=1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n4 Symmetric DPP\\r\\nHere, we prove our basic result, Theorem 10. We provide a strengthening for DPPs satisfying\\r\\nnontrivial spectral bounds, Theorem 28, in Section 5. We first state helper bounds used in the\\r\\nproof.\\r\\n\\r\\nLemma 26. Suppose \\xc2\\xb5 : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 is negatively correlated. Let \\xc2\\xb5t = \\xc2\\xb5Dk\\xe2\\x86\\x92t and pi = P\\xc2\\xb5 [i \\xe2\\x88\\x88 S].\\r\\nThen                                                     \\x12 2\\x13\\r\\n                                       \\xc2\\xb5t ( T )           t\\r\\n                                                pi \\xe2\\x89\\xa4 exp      .\\r\\n                                    t! \\xe2\\x88\\x8f i\\xe2\\x88\\x88 T k            k\\r\\n\\r\\nProof. Note that\\r\\n                          \\x12 \\x13 \\xe2\\x88\\x921\\r\\n                           k                                     t!\\r\\n               \\xc2\\xb5t ( T ) =        P S\\xe2\\x88\\xbc\\xc2\\xb5 [ T \\xe2\\x8a\\x86 S] =                               P S \\xe2\\x88\\xbc \\xc2\\xb5 [ T \\xe2\\x8a\\x86 S ].\\r\\n                           t                      k ( k \\xe2\\x88\\x92 1) \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ( k \\xe2\\x88\\x92 t + 1)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                             14\\r\\n\\x0c\\n\\nThus, by negative correlation,\\r\\n\\r\\n                          \\xc2\\xb5t ( T )                        kt                P S\\xe2\\x88\\xbc\\xc2\\xb5 [ T \\xe2\\x8a\\x86 S]\\r\\n                                     pi =\\r\\n                        t! \\xe2\\x88\\x8f i\\xe2\\x88\\x88 T k         k ( k \\xe2\\x88\\x92 1) \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ( k \\xe2\\x88\\x92 t + 1) \\xe2\\x88\\x8f i \\xe2\\x88\\x88 T P S \\xe2\\x88\\xbc \\xc2\\xb5 [ i \\xe2\\x88\\x88 S ]\\r\\n                                                      kt\\r\\n                                       \\xe2\\x89\\xa4\\r\\n                                        k ( k \\xe2\\x88\\x92 1) \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 ( k \\xe2\\x88\\x92 t + 1)\\r\\n                                           t \\xe2\\x88\\x921 \\x12         \\x13 ! \\xe2\\x88\\x921       \\x12 2\\x13\\r\\n                                                        i               t\\r\\n                                       = \\xe2\\x88\\x8f 1\\xe2\\x88\\x92                    \\xe2\\x89\\xa4 exp\\r\\n                                           i=1\\r\\n                                                       k                 k\\r\\n\\r\\n                                                                                 \\x01           \\x102      \\x11\\r\\n                                                  \\xe2\\x88\\x921                                          t \\xe2\\x88\\x92t\\r\\nwhere we used the facts that 1 \\xe2\\x88\\x92 x \\xe2\\x89\\xa5 e\\xe2\\x88\\x922x and \\xe2\\x88\\x8f ti= 1 exp\\r\\n                                                                            2i\\r\\n                                                                            k        = exp      k        .\\r\\n                                                                           \\xe2\\x88\\x9a\\r\\nProposition 27. If step (*) takes O(\\xcf\\x84 )-parallel time, Algorithm 1 takes O( k \\xc2\\xb7 \\xcf\\x84 )-parallel time.\\r\\n\\r\\nProof. Note that\\r\\n                                                                     \\x13      \\x12p\\r\\n                                               p                  p1 2\\r\\n                         k i+1 = k i \\xe2\\x88\\x92 \\xe2\\x8c\\x88 k i \\xe2\\x8c\\x89 \\xe2\\x89\\xa4 k i \\xe2\\x88\\x92 k i \\xe2\\x89\\xa4  ki \\xe2\\x88\\x92     .\\r\\n                                                                   2\\r\\n         p        \\xe2\\x88\\x9a                                  \\xe2\\x88\\x9a       \\xe2\\x88\\x9a       \\xe2\\x88\\x9a\\r\\nand thus ki+1 \\xe2\\x89\\xa4 ki \\xe2\\x88\\x92 12 . Hence, since t \\xe2\\x89\\xa5 2 k implies kt \\xe2\\x89\\xa4 k0 \\xe2\\x88\\x92 2t \\xe2\\x89\\xa4 0, the algorithm\\r\\n                \\xe2\\x88\\x9a                               \\xe2\\x88\\x9a\\r\\nterminates in O( k) iterations, and takes O( k) parallel time.\\r\\n\\r\\nProof of Theorem 10. We first consider the case of sampling k-DPPs. By Lemma 15, when \\xc2\\xb5 is a\\r\\nk-DPP defined by symmetric PSD ensemble matrix L, \\xc2\\xb5 and the conditionals of \\xc2\\xb5 are real-stable.\\r\\nIn particular, all \\xc2\\xb5(i) as defined in Algorithm 1 are real-stable and hence strongly Rayleigh.\\r\\nConsider some loop i, and let \\xc2\\xb5 \\xe2\\x89\\xa1 \\xc2\\xb5(i) . We use Lemma 26 to implement step (*). In particular, we\\r\\nfirst compute the marginals pi = P \\xc2\\xb5 [i \\xe2\\x88\\x88 S] in O e (1)-parallel time. Next, let \\xce\\xbd be the distribution\\r\\n                                            t\\r\\nover ordered tuples (i1 , . . . , it ) \\xe2\\x88\\x88 [n] with\\r\\n                                                                       t\\r\\n                                                                           pir\\r\\n                                              \\xce\\xbd({i1 , . . . , it }) = \\xe2\\x88\\x8f        .\\r\\n                                                                      r =1\\r\\n                                                                            k\\r\\n\\r\\n                                                                                                             \\xc2\\xb5 ({i ,...,i })\\r\\nWe can identify \\xc2\\xb5t with the distribution \\xc2\\xb5\\xe2\\x88\\x97t over [n]t where \\xc2\\xb5\\xe2\\x88\\x97t ({i1 , . . . , it }) = 1    t\\r\\n                                                                                               . Let\\r\\n                                             \\xe2\\x88\\x9a                                            t!\\r\\n \\xe2\\x80\\xb2     \\xce\\xb4\\r\\n\\xce\\xb4 = \\xe2\\x88\\x9a , where Algorithm 1 takes at most 2 k iterations by Proposition 27. We run the rejection\\r\\n      2 k\\r\\nsampling algorithm in Proposition 23, which succeeds with probability 1 \\xe2\\x88\\x92 \\xce\\xb4\\xe2\\x80\\xb2 , to sample from \\xc2\\xb5\\xe2\\x88\\x97t\\r\\n                                       2                    \\xe2\\x88\\x9a\\r\\ngiven samples from \\xce\\xbd, with C \\xe2\\x89\\xa4 exp( tk ) = O(1) since t = \\xe2\\x8c\\x88 k\\xe2\\x8c\\x89. Clearly obtaining a sample from\\r\\n\\xc2\\xb5\\xe2\\x88\\x97t yields a sample from \\xc2\\xb5t by forgetting the ordering on the elements.\\r\\n                                           e\\r\\n        \\xe2\\x88\\x9a iteration i of Algorithm 1 takes O(1)-parallel time. By Proposition 27, the\\r\\nHence, each                                                                           \\xe2\\x88\\x9a algorithm\\r\\ntakes O( k)-parallel time. By a union bound, the success probability is at least 1 \\xe2\\x88\\x92 2 k\\xce\\xb4\\xe2\\x80\\xb2 = 1 \\xe2\\x88\\x92 \\xce\\xb4.\\r\\nThe number of machines used is the same as in Proposition 23, that is, O(poly(n) log \\xce\\xb4k ).\\r\\nThe result for DPPs immediately follows from Remark 12.\\r\\n\\r\\n\\r\\n5 Refined guarantees for bounded symmetric DPPs\\r\\nFor symmetric PSD ensemble matrices L with non-trivial eigenvalue or trace bounds, we give the\\r\\nfollowing refined result improving upon Theorem 10 in various interesting parameter regimes.\\r\\n\\r\\n                                                             15\\r\\n\\x0c\\n\\nTheorem 28. Let L be a n \\xc3\\x97 n symmetric PSD matrix and \\xc7\\xab \\xe2\\x88\\x88 (0, 1). Let \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be the DPP\\r\\ndefined by L. Let K = L( I + L)\\xe2\\x88\\x921 \\x16 I be the kernel of L. There exists an algorithm to approximately\\r\\nsample from within \\xc7\\xab total variation distance of \\xc2\\xb5 in\\r\\n                                     \\x12     \\x1aq                       \\x1b\\x13\\r\\n                                                                 \\xe2\\x88\\x9a\\r\\n                                   e\\r\\n                                   O min        Tr(K ), \\xce\\xbbmax (K ) n\\r\\n\\r\\nparallel time using poly(n)( 1\\xc7\\xab )o(1) machines.\\r\\nWe will use the following \\xe2\\x80\\x9cfiltered\\xe2\\x80\\x9d variant of Algorithm 1.\\r\\n\\r\\nAlgorithm 4: Filtering\\r\\n                  [n]\\r\\n         \\xe2\\x88\\x9a \\xe2\\x88\\x92\\xc2\\xb51 : 2 \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 with kernel K, \\xce\\xbbmax (K ) \\xe2\\x89\\xa4 \\xce\\xbb.\\r\\nInput: DPP\\r\\n\\xce\\xb1 \\xe2\\x86\\x90 (\\xce\\xbb n)\\r\\nif \\xce\\xb1 > 1 then\\r\\n    (1): Sample S \\xe2\\x88\\xbc \\xc2\\xb5 and return S.\\r\\nS\\xe2\\x88\\x921 , K (0) , L(0) \\xe2\\x86\\x90 \\xe2\\x88\\x85, K, L\\r\\nfor i = 0, 1, . . . , R do\\r\\n    (2): Sample Ti \\xe2\\x88\\xbc DPP with kernel K\\xcc\\x83 (i) := \\xce\\xb1K (i)\\r\\n    Update Si \\xe2\\x86\\x90 Si\\xe2\\x88\\x921 \\xe2\\x88\\xaa Ti\\r\\n    Update L(i+1) \\xe2\\x86\\x90 ((1 \\xe2\\x88\\x92 \\xce\\xb1) L(i) ) Ti (where ( L) T is the ensemble matrix corresponding to the\\r\\n      DPP with ensemble matrix L, conditioned on including T; see Section 3.1)\\r\\n    Update K (i+1) \\xe2\\x86\\x90 I \\xe2\\x88\\x92 ( I + L(i+1) )\\xe2\\x88\\x921\\r\\nOutput SR\\r\\n\\r\\nWe prove Theorem 28 in this section. Our first step is to show that for R = \\xce\\x98(\\xce\\xb1\\xe2\\x88\\x921 log n\\xc7\\xab ), the\\r\\noutput distribution of Algorithm 4 is within \\xc7\\xab of the target distribution \\xc2\\xb5.\\r\\nWe require the following helper claims. The first shows that randomly independently dropping\\r\\nelements of a sample from a DPP \\xc2\\xb5 is equivalent to scaling the kernel matrix.\\r\\nProposition 29. Let \\xc2\\xb5 be a DPP with kernel K. Let \\xc2\\xb5\\xe2\\x80\\xb2 be the DPP with kernel K \\xe2\\x80\\xb2 := \\xce\\xb1K. Let \\xce\\xbd be the\\r\\ndistribution obtained by first sampling U \\xe2\\x88\\xbc \\xc2\\xb5, then outputting S \\xe2\\x8a\\x86 U with probability \\xce\\xb1|S| (1 \\xe2\\x88\\x92 \\xce\\xb1)|S| , i.e.\\r\\n\\r\\n                                   \\xce\\xbd(S) = \\xe2\\x88\\x91 \\xc2\\xb5(U )\\xce\\xb1|S| (1 \\xe2\\x88\\x92 \\xce\\xb1)|U |\\xe2\\x88\\x92|S| .\\r\\n                                            U \\xe2\\x8a\\x87S\\r\\n\\r\\nThen, \\xc2\\xb5\\xe2\\x80\\xb2 and \\xce\\xbd are identical.\\r\\n\\r\\nProof. Given a set A, we have P S\\xe2\\x88\\xbc\\xc2\\xb5\\xe2\\x80\\xb2 [ A \\xe2\\x8a\\x86 S] = det((\\xce\\xb1K ) A ) = \\xce\\xb1| A| det(K A ) = \\xce\\xb1| A| \\xe2\\x88\\x91U \\xe2\\x8a\\x87 A \\xc2\\xb5(U ). On\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                    16\\r\\n\\x0c\\n\\nthe other hand, we have\\r\\n                            P S \\xe2\\x88\\xbc \\xce\\xbd [ A \\xe2\\x8a\\x86 S ] = \\xe2\\x88\\x91 \\xce\\xbd( S )\\r\\n                                                         S\\xe2\\x8a\\x87 A\\r\\n\\r\\n                                                     =     \\xe2\\x88\\x91       \\xc2\\xb5(U )\\xce\\xb1|S| (1 \\xe2\\x88\\x92 \\xce\\xb1)|U |\\xe2\\x88\\x92|S|\\r\\n                                                         U \\xe2\\x8a\\x87S\\xe2\\x8a\\x87 A\\r\\n\\r\\n                                                     = \\xce\\xb1| A|       \\xe2\\x88\\x91       \\xc2\\xb5(U )\\xce\\xb1|S|\\xe2\\x88\\x92| A| (1 \\xe2\\x88\\x92 \\xce\\xb1)|U |\\xe2\\x88\\x92|S|\\r\\n                                                                 U \\xe2\\x8a\\x87S\\xe2\\x8a\\x87 A\\r\\n                                                          | A|                                   \\xe2\\x80\\xb2                       \\xe2\\x80\\xb2\\r\\n                                                     =\\xce\\xb1          \\xe2\\x88\\x91 \\xc2\\xb5 (U ) \\xe2\\x80\\xb2 \\xe2\\x88\\x91               \\xce\\xb1|S | (1 \\xe2\\x88\\x92 \\xce\\xb1)|U \\\\ A|\\xe2\\x88\\x92|S |\\r\\n                                                                 U\\xe2\\x8a\\x87A             S \\xe2\\x8a\\x86U \\\\ A\\r\\n                                                          | A|\\r\\n                                                     =\\xce\\xb1          \\xe2\\x88\\x91 \\xc2\\xb5 (U ).\\r\\n                                                                 U\\xe2\\x8a\\x87A\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProposition 30. Consider the setup of Algorithm 4. Suppose \\xce\\xb1 \\xe2\\x89\\xa4 1. Let P i denote the distribution of Si .\\r\\nFix \\xc7\\xab > 0. For i = \\xe2\\x84\\xa6(\\xce\\xb1\\xe2\\x88\\x921 log n\\xc7\\xab ) for a sufficiently large constant,\\r\\n                                                                 dTV (P i , \\xc2\\xb5) \\xe2\\x89\\xa4 \\xc7\\xab.\\r\\n\\r\\nProof. Let \\xc2\\xb5(i) be the DPP with ensemble matrix L(i) (and kernel matrix K (i) ), and let \\xce\\xbd(i) be the\\r\\nDPP with kernel matrix \\xce\\xb1K (i) . We will prove by induction that for all i,\\r\\n                          P i [Si ] = \\xe2\\x88\\x91 \\xc2\\xb5(0) (U )(1 \\xe2\\x88\\x92 \\xce\\xb1)(i+1)(|U |\\xe2\\x88\\x92|Si|) (1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1)i+1 )|Si | .\\r\\n                                          U \\xe2\\x8a\\x87 Si\\r\\n\\r\\nThe base case i = 0 follows from Proposition 29. Now, supposing the induction hypothesis holds\\r\\nfor some i \\xe2\\x88\\x92 1, we show that it also holds for i. In the following, let S0 be the set sampled in the\\r\\nfirst iteration of Algorithm 4, and let P i [Si | S0 ] denote the probability we observe Si conditioned\\r\\non the value of S0 . The induction hypothesis then yields the probability we observe Si \\\\ S0 in the\\r\\nnext i \\xe2\\x88\\x92 1 iterations, with the starting matrix L(1) \\xe2\\x86\\x90 ((1 \\xe2\\x88\\x92 \\xce\\xb1) L)S0 as follows:\\r\\n                P i [Si | S0 ] = \\xe2\\x88\\x91 \\xc2\\xb5(1) (U \\\\ S0 )(1 \\xe2\\x88\\x92 \\xce\\xb1)i(|U \\\\S0 |\\xe2\\x88\\x92|Si \\\\S0 |) (1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1)i )|Si \\\\S0 | .\\r\\n                                        U \\xe2\\x8a\\x87 Si\\r\\n\\r\\nHence, we have\\r\\n        P i [Si ] =    \\xe2\\x88\\x91 P i [ S i | S0 ] P 0 [ S0 ]\\r\\n                      S0 \\xe2\\x8a\\x86 S i\\r\\n                                                                                                                                   !\\r\\n                                            ( 1)                           i(|U \\\\ S0 |\\xe2\\x88\\x92|Si \\\\ S0 |)                i | S i \\\\ S0 |\\r\\n                =      \\xe2\\x88\\x91             \\xe2\\x88\\x91 \\xc2\\xb5           (U \\\\ S0 )(1 \\xe2\\x88\\x92 \\xce\\xb1)                                  (1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1 ) )                   P 0 [ S0 ]\\r\\n                      S0 \\xe2\\x8a\\x86 S i   U \\xe2\\x8a\\x87 Si\\r\\n\\r\\n                =         \\xe2\\x88\\x91          \\xc2\\xb5(1) (U \\\\ S0 )P0 [S0 ](1 \\xe2\\x88\\x92 \\xce\\xb1)i(|U |\\xe2\\x88\\x92|Si |) (1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1)i )|Si \\\\S0 |\\r\\n                      U \\xe2\\x8a\\x87 S i \\xe2\\x8a\\x87 S0\\r\\n\\r\\n                =         \\xe2\\x88\\x91          \\xc2\\xb5(0) (U )(1 \\xe2\\x88\\x92 \\xce\\xb1)|U |\\xe2\\x88\\x92|S0 | \\xce\\xb1|S0 | (1 \\xe2\\x88\\x92 \\xce\\xb1)i(|U |\\xe2\\x88\\x92|Si |) (1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1)i )|Si \\\\S0 |\\r\\n                      U \\xe2\\x8a\\x87 S i \\xe2\\x8a\\x87 S0\\r\\n\\r\\n                = \\xe2\\x88\\x91 \\xc2\\xb5(0) (U )(1 \\xe2\\x88\\x92 \\xce\\xb1)(i+1)(|U |\\xe2\\x88\\x92|Si|) \\xe2\\x88\\x91 (1 \\xe2\\x88\\x92 \\xce\\xb1)|Si |\\xe2\\x88\\x92|S0 | (1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1)i )|Si \\\\S0 | \\xce\\xb1|S0 |\\r\\n                      U \\xe2\\x8a\\x87 Si                                                 S0 \\xe2\\x8a\\x86 S i\\r\\n                                                    \\x10                           \\x11 | Si |\\r\\n                = \\xe2\\x88\\x91 \\xc2\\xb5(0) (U )(1 \\xe2\\x88\\x92 \\xce\\xb1)(i+1)(|U |\\xe2\\x88\\x92|Si|) \\xce\\xb1 + (1 \\xe2\\x88\\x92 \\xce\\xb1)(1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1)i )\\r\\n                      U \\xe2\\x8a\\x87 Si\\r\\n\\r\\n                = \\xe2\\x88\\x91 \\xc2\\xb5(0) (U )(1 \\xe2\\x88\\x92 \\xce\\xb1)(i+1)(|U |\\xe2\\x88\\x92|Si|) (1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1)i+1 )|Si |\\r\\n                      U \\xe2\\x8a\\x87 Si\\r\\n\\r\\n\\r\\n\\r\\n                                                                           17\\r\\n\\x0c\\n\\nwhere the third equality uses Proposition 29 and the definition of L1 = ((1 \\xe2\\x88\\x92 \\xce\\xb1) L)S0 to derive\\r\\n\\r\\n            \\xc2\\xb5 ( 1 ) ( U \\\\ S 0 ) P 0 [ S 0 ] = \\xc2\\xb5 ( 1 ) ( U \\\\ S 0 ) \\xe2\\x88\\x91 ( 1 \\xe2\\x88\\x92 \\xce\\xb1 ) | V \\\\ S0 | \\xce\\xb1 | S0 | \\xc2\\xb5 ( 0 ) ( V )\\r\\n                                                                        V \\xe2\\x8a\\x87 S0\\r\\n\\r\\n                                                  ( 1 \\xe2\\x88\\x92 \\xce\\xb1 ) | U \\\\ S0 | \\xc2\\xb5 ( 0 ) ( U )\\r\\n                                          =                       | V \\\\ S   |   ( 0) ( V ) \\xe2\\x88\\x91\\r\\n                                                                                                 ( 1 \\xe2\\x88\\x92 \\xce\\xb1 ) | V \\\\ S0 | \\xce\\xb1 | S0 | \\xc2\\xb5 ( 0 ) ( V )\\r\\n                                              \\xe2\\x88\\x91 V \\xe2\\x8a\\x87 S0 ( 1 \\xe2\\x88\\x92  \\xce\\xb1 )         0   \\xc2\\xb5           V \\xe2\\x8a\\x87 S0\\r\\n\\r\\n                                          = ( 1 \\xe2\\x88\\x92 \\xce\\xb1 ) | U \\\\ S0 | \\xce\\xb1 | S0 | \\xc2\\xb5 ( 0 ) ( U ) .\\r\\n                                                                                                           log n\\r\\nThus the induction hypothesis holds for all i. By taking i = \\xe2\\x84\\xa6( \\xce\\xb1 \\xc7\\xab ), and only considering the\\r\\nsummand corresponding to U = Si , we have\\r\\n                                                      \\x10     \\x10 \\xc7\\xab \\x11\\x11n\\r\\n                                          i +1 | Si |\\r\\n            P i [Si ] \\xe2\\x89\\xa5 \\xc2\\xb5(Si )(1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1) ) \\xe2\\x89\\xa5 \\xc2\\xb5(Si ) 1 \\xe2\\x88\\x92 O         \\xe2\\x89\\xa5 \\xc2\\xb5(Si )(1 \\xe2\\x88\\x92 \\xc7\\xab),\\r\\n                                                              n\\r\\nand hence,\\r\\n\\r\\n                  dTV (P i [\\xc2\\xb7], \\xc2\\xb5) =               \\xe2\\x88\\x91                (\\xc2\\xb5(Si ) \\xe2\\x88\\x92 P i [Si ]) \\xe2\\x89\\xa4               \\xe2\\x88\\x91                 \\xc7\\xab\\xc2\\xb5(Si ) \\xe2\\x89\\xa4 \\xc7\\xab.\\r\\n                                          Si :P i [Si ]\\xe2\\x89\\xa4 \\xc2\\xb5 ( Si )                                Si :P i [Si ]\\xe2\\x89\\xa4 \\xc2\\xb5 ( Si )\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nNext, we show that each step of the for loop can be implemented in constant parallel time.\\r\\nLemma 31. Let \\xc2\\xb5 : 2[n] \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be a DPP with marginal kernel K. If \\xce\\xbbmax (K ) \\xe2\\x89\\xa4 \\xe2\\x88\\x9a1n then we can sample\\r\\nfrom a distribution \\xc7\\xab-away in total variation distance from \\xc2\\xb5 in O       e (1)-time using O(poly(n)( 1 )o(1) )\\r\\n                                                                                                      \\xc7\\xab\\r\\nmachines.\\r\\n                   q\\r\\nProof. Let s = c n log \\xc7\\xab1\\xe2\\x80\\xb2 for c as in Lemma 17. Let \\xe2\\x84\\xa6 := {S \\xe2\\x8a\\x86 [n] | |S| \\xe2\\x89\\xa4 s} . Let pi := Ki,i =\\r\\nP S\\xe2\\x88\\xbc\\xc2\\xb5 [i \\xe2\\x88\\x88 S]. Let \\xce\\xbd be the distribution obtained by independently sampling independent bi \\xe2\\x88\\xbc\\r\\nBer( pi ) for all i \\xe2\\x88\\x88 [n], and outputting T = {i | bi = 1} . By Lemma 17, \\xe2\\x88\\x91S\\xe2\\x88\\x88\\xe2\\x84\\xa6 \\xc2\\xb5(S) \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 \\xc7\\xab\\xe2\\x80\\xb2 .\\r\\nMoreover, for fixed T \\xe2\\x88\\x88 \\xe2\\x84\\xa6, we have\\r\\n                                                ! \\xe2\\x88\\x921                                               ! \\xe2\\x88\\x921\\r\\n    \\xc2\\xb5( T )       det L T\\r\\n              det( I + L) \\xe2\\x88\\x8f\\r\\n            =                    pi \\xe2\\x88\\x8f (1 \\xe2\\x88\\x92 pi )      = det( L T ) det( I \\xe2\\x88\\x92 K ) \\xe2\\x88\\x8f Ki,i \\xe2\\x88\\x8f(1 \\xe2\\x88\\x92 Ki,i )      ,\\r\\n    \\xce\\xbd( T )                  i\\xe2\\x88\\x88 T    i6 \\xe2\\x88\\x88 T                                       i\\xe2\\x88\\x88 T  i6 \\xe2\\x88\\x88 T\\r\\n\\r\\nwhere we use I + L = ( I \\xe2\\x88\\x92 K )\\xe2\\x88\\x921 . By applying Corollary 16 to I \\xe2\\x88\\x92 K, we have\\r\\n\\r\\n                                       det( I \\xe2\\x88\\x92 K ) \\xe2\\x89\\xa4 \\xe2\\x88\\x8f (1 \\xe2\\x88\\x92 Ki,i ) \\xe2\\x89\\xa4 \\xe2\\x88\\x8f(1 \\xe2\\x88\\x92 Ki,i ),\\r\\n                                                                    i\\xe2\\x88\\x88[n ]                  i6 \\xe2\\x88\\x88 T\\r\\n\\r\\nso it suffices to show\\r\\n                                             \\x12 \\x13 o ( 1)                       \\x12 \\x13 o ( 1)\\r\\n                                              1                      \\xc2\\xb5( T )    1\\r\\n                                det( L T ) \\xe2\\x89\\xa4\\r\\n                                              \\xc7\\xab         \\xe2\\x88\\x8f    Ki,i =\\xe2\\x87\\x92\\r\\n                                                                     \\xce\\xbd( T )\\r\\n                                                                            \\xe2\\x89\\xa4\\r\\n                                                                               \\xc7\\xab\\r\\n                                                                                         ,\\r\\n                                                        i\\xe2\\x88\\x88 T\\r\\n\\r\\nat which point we can apply Proposition 24. Let K = UDU \\xe2\\x8a\\xba where U \\xe2\\x88\\x88 R n\\xc3\\x97n is an orthonormal\\r\\nbasis of eigenvectors of K, and D = diag({\\xce\\xbbi }i\\xe2\\x88\\x88[n] ), where \\xce\\xbb1 \\xe2\\x89\\xa5 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xe2\\x89\\xa5 \\xce\\xbbn are the eigenvalues of\\r\\nK. By (2), we can write                                     !\\r\\n                                            \\x1a          \\x1b\\r\\n                                                 \\xce\\xbbi\\r\\n                                L = Udiag                     U\\xe2\\x8a\\xba.\\r\\n                                               1 \\xe2\\x88\\x92 \\xce\\xbbi i\\xe2\\x88\\x88[n]\\r\\n\\r\\n                                                                             18\\r\\n\\x0c\\n\\nThus, by applying the Cauchy-Binet formula twice,\\r\\n                                                                 \\x1a              \\x1b             !               !\\r\\n                                                                         \\xce\\xbbi\\r\\n                 det( L T ) = det UT,[n]diag                                                      U[\\xe2\\x8a\\xban],T )\\r\\n                                                                       1 \\xe2\\x88\\x92 \\xce\\xbbi        i\\xe2\\x88\\x88[n ]\\r\\n                                                                                              !\\r\\n                                                                              \\xce\\xbbi\\r\\n                             =         \\xe2\\x88\\x91              det(\\xce\\x9b T,S )         \\xe2\\x88\\x8f 1 \\xe2\\x88\\x92 \\xce\\xbbi                 det(\\xce\\x9b\\xe2\\x8a\\xbaS,T )\\r\\n                                 S \\xe2\\x8a\\x86[n ],|S |=| T |                       i\\xe2\\x88\\x88S\\r\\n                                             r              !                                                     !\\r\\n                                                     1\\r\\n                             \\xe2\\x89\\xa4 exp c             log                    \\xe2\\x88\\x91            det(\\xce\\x9b T,S )         \\xe2\\x88\\x8f \\xce\\xbbi         det(\\xce\\x9b\\xe2\\x8a\\xbaS,T )\\r\\n                                                     \\xc7\\xab          S \\xe2\\x8a\\x86[n ],|S |=| T |                        i\\xe2\\x88\\x88S\\r\\n                                             r              !\\r\\n                                                        1\\r\\n                             = exp c log                        det(K T )\\r\\n                                                        \\xc7\\xab\\r\\n                                             r              !\\r\\n                                                     1\\r\\n                             \\xe2\\x89\\xa4 exp c             log\\r\\n                                                     \\xc7\\xab          \\xe2\\x88\\x8f Ki,i\\r\\n                                                                i\\xe2\\x88\\x88 T\\r\\n\\r\\nwhere in the first inequality, we used\\r\\n                                         \\x12                  \\x13|S|        \\x12                 \\x13s                      r           !\\r\\n                                                 1                              1                                         1\\r\\n                   \\xe2\\x88\\x8f (1 \\xe2\\x88\\x92 \\xce\\xbb i ) \\xe2\\x89\\xa5            1\\xe2\\x88\\x92 \\xe2\\x88\\x9a\\r\\n                                                  n\\r\\n                                                                   \\xe2\\x89\\xa5        1\\xe2\\x88\\x92 \\xe2\\x88\\x9a\\r\\n                                                                                 n\\r\\n                                                                                                  \\xe2\\x89\\xa5 exp \\xe2\\x88\\x92c            log\\r\\n                                                                                                                          \\xc7\\xab\\r\\n                   i\\xe2\\x88\\x88S\\r\\n\\r\\nand in the last inequality, we used Corollary 16. Thus by Proposition 24, we can sample from \\xc2\\xb5\\xcc\\x83\\r\\nthat is \\xc7\\xab-away from \\xc2\\xb5 in O(1) parallel time using O(( 1\\xc7\\xab )o(1) poly(n)) machines, by setting \\xce\\xb4 \\xe2\\x86\\x90 2\\xc7\\xab\\r\\nand adjusting the definition of \\xc7\\xab in this proof by a constant.\\r\\n\\r\\nProposition 32. Consider the setup of Algorithm 4. Suppose \\xce\\xb1 \\xe2\\x89\\xa4 1. We have \\xce\\xbbmax (K (i) ) \\xe2\\x89\\xa4 \\xce\\xbb for all i,\\r\\n                                                                                                  e (1) parallel\\r\\nso that \\xce\\xbbmax (K\\xcc\\x83 (i) ) \\xe2\\x89\\xa4 \\xe2\\x88\\x9a1n . Consequently, each iteration of the for loop can be implemented in O\\r\\ntime using O(poly(n)( 1\\xc7\\xab )o(1) ) machines, up to total variation distance \\xc7\\xab.\\r\\n\\r\\nProof. We inductively show that \\xce\\xbbmax (K (i) ) \\xe2\\x89\\xa4 \\xce\\xbb for all i. The base case i = 0 directly follows\\r\\nfrom the input assumption. Now let us assume that \\xce\\xbbmax (K (i) ) \\xe2\\x89\\xa4 \\xce\\xbb for some i \\xe2\\x89\\xa5 0. We will show\\r\\nthat \\xce\\xbbmax (K (i+1) ) \\xe2\\x89\\xa4 \\xce\\xbb follows. Let S be the index set of Li and S\\xcc\\x83 = S \\\\ Ti where Ti was sampled.\\r\\nThen,\\r\\n                      L(i+1) = ((1 \\xe2\\x88\\x92 \\xce\\xb1) L(i) ) Ti = (1 \\xe2\\x88\\x92 \\xce\\xb1) LS\\xcc\\x83 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xb1) LS\\xcc\\x83,Ti L\\xe2\\x88\\x92  1\\r\\n                                                                                  Ti ,Ti L Ti ,S\\xcc\\x83 .\\r\\n                           \\x10      \\x11 \\xe2\\x88\\x921\\r\\n                      ( i)   ( i)        ( i)                              ( i)        ( i)\\r\\nSince L(i) is PSD, LS\\xcc\\x83,T L Ti ,Ti      L T ,S\\xcc\\x83 \\x17 0. Thus L(i+1) \\x16 (1 \\xe2\\x88\\x92 \\xce\\xb1) LS\\xcc\\x83 \\x16 LS\\xcc\\x83 .\\r\\n                         i                       i\\r\\n\\r\\n\\r\\nLet \\xce\\x9b be the set of the eigenvalues of K (i) . Due to (2), the eigenvalues of L(i) are given by\\r\\n{ 1\\xe2\\x88\\x92\\xce\\xbb \\xce\\xbb | \\xce\\xbb \\xe2\\x88\\x88 \\xce\\x9b}. As 1\\xe2\\x88\\x92\\xce\\xbb \\xce\\xbb is strictly increasing in the range [0, 1], and the largest eigenvalue of\\r\\nL(i+1) is dominated by the largest eigenvalue of L(i) (since restrictions to index sets can only\\r\\ndecrease quadratic forms), we have the first desired conclusion. The second conclusion follows\\r\\nfrom Lemma 31 as the eigenvalue bound is satisfied.\\r\\n\\r\\nFinally, we are ready to prove Theorem 28.\\r\\n\\r\\nProof of Theorem 28. The bound involving Tr(K ) follows from a similar\\r\\n                                                                    \\x08 argument as in Remark 12.\\r\\nNote that Tr(K ) = E S\\xe2\\x88\\xbc\\xc2\\xb5 [|S|], and that by Lemma 17, the set \\xe2\\x84\\xa6 := S \\xe2\\x8a\\x86 [n] |S| \\xe2\\x89\\xa4 Tr(K ) log 2\\xc7\\xab\\r\\nhas \\xc2\\xb5(\\xe2\\x84\\xa6) \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 2\\xc7\\xab . When drawing k from H (the distribution on cardinality values), if k \\xe2\\x89\\xa4\\r\\n\\r\\n                                                                         19\\r\\n\\x0c\\n\\nTr(K ) log 2\\xc7\\xab , we use Theorem 10 to approximately sample from within 2\\xc7\\xab of \\xc2\\xb5k , else we output an\\r\\narbitrary subset. By the triangle inequality, the output\\xe2\\x80\\x99s distribution is within 2\\xc7\\xab + 2\\xc7\\xab = \\xc7\\xab of \\xc2\\xb5. The\\r\\nalgorithm runs in the stated parallel time depending on Tr(K ) using the number of machines as\\r\\nTheorem 10.\\r\\nNow we focus on the bound involving \\xce\\xbbmax (K ). If \\xce\\xb1 > 1 then the conclusion follows            \\xe2\\x88\\x9a from\\r\\nLemma 31 applied to step (1). Else, suppose \\xce\\xb1 \\xe2\\x89\\xa4 1. We run Algorithm 4 with R = O(\\xce\\xbb n log n\\xc7\\xab )\\r\\nsuch that Proposition 30 guarantees that if we can run the algorithm correctly, the output has\\r\\ntotal variation 2\\xc7\\xab . Let \\xc7\\xab\\xe2\\x80\\xb2 = R\\xc7\\xab . Let \\xce\\xbd(i) be the target distribution of Ti in ith step of the for loop.\\r\\nBy Proposition 32, we can modify step (2) to sample from \\xce\\xbd\\xcc\\x82(i) that is \\xc7\\xab\\xe2\\x80\\xb2 -away from \\xce\\xbd(i) in TV-\\r\\ndistance in Oe (1) time using O(poly(n)( 1 )o(1) ) machines. Hence, by the triangle inequality, the\\r\\n                                             \\xc7\\xab\\r\\noutput of the algorithm is 2\\xc7\\xab away from the output if we were given exact sample access to each\\r\\n\\xce\\xbd(i) . Combining with the approximation error of Proposition 30 yields the conclusion.\\r\\n\\r\\n\\r\\n6 Entropic independence\\r\\nIn this section, we prove the following main result. We will use Theorem 33 to derive our\\r\\nsamplers for various entropically independent distributions, namely Theorems 8 and 9, which\\r\\nimmediately follow from combining Remark 12, Lemma 21, Lemma 22, and Theorem 33.\\r\\n\\r\\nTheorem 33. Let \\xc2\\xb5 : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be such that all its conditional distributions are \\xce\\xb11 -entropically inde-\\r\\npendent with \\xce\\xb1 = \\xe2\\x84\\xa6(1). Suppose we can compute marginals P \\xc2\\xb5 [i | S] for S \\xe2\\x8a\\x86 [n] and i 6\\xe2\\x88\\x88 S in O        e ( 1)\\r\\nparallel time. For any constant c > 0 and any \\xc7\\xab \\xe2\\x88\\x88 (0, 1), there exists an algorithm that uses O to sample\\r\\nfrom a distribution within total variation distance \\xc7\\xab of \\xc2\\xb5 in\\r\\n                                                \\x12\\xe2\\x88\\x9a \\x12 \\x13c \\x13\\r\\n                                              e          k\\r\\n                                             O      k\\xc2\\xb7\\r\\n                                                         \\xc7\\xab\\r\\n                                \\xe2\\x88\\x921 )\\r\\nparallel time using ( n\\xc7\\xab )O(c          machines.\\r\\n\\r\\n6.1 Isotropic transformation\\r\\nWe first reduce to the case of near-isotropic distributions. Similarly to [AD20; Ana+21a], we say a\\r\\ndistribution \\xc2\\xb5 : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 is isotropic if for all i \\xe2\\x88\\x88 [n], the marginal P S\\xe2\\x88\\xbc\\xc2\\xb5 [i \\xe2\\x88\\x88 S] is nk . Prior work\\r\\n[AD20] introduced the following subdivision process transforming an arbitrary \\xc2\\xb5 : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50\\r\\nto a nearly-isotropic \\xc2\\xb5\\xe2\\x80\\xb2 : (Uk ) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 , while preserving entropic independence.\\r\\nDefinition 34. Let \\xc2\\xb5 : (nk) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be an arbitrary probability distribution, and assume that we\\r\\nhave access to the marginals p1 , . . . , pn of the distribution with p1 + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + pn = k and pi =\\r\\n                                                                 n\\r\\nP S\\xe2\\x88\\xbc\\xc2\\xb5 [i \\xe2\\x88\\x88 S] for all i. For a parameter \\xce\\xb2 \\xe2\\x88\\x88 (0, 1), let ti := \\xe2\\x8c\\x88 \\xce\\xb2k pi \\xe2\\x8c\\x89. We create a new distribution out\\r\\nof \\xc2\\xb5 as follows: for each i \\xe2\\x88\\x88 [n], create ti copies of element i and let the collection of all copies be\\r\\n                               S\\r\\nthe new ground set: U := ni=1 {i( j) } j\\xe2\\x88\\x88[ti ] . Define the following distribution \\xc2\\xb5iso : (Uk ) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 :\\r\\n                                                  \\x10n                           o\\x11        \\xc2\\xb5({i1 , . . . , ik })\\r\\n                                            iso         (j )            (j )\\r\\n                                        \\xc2\\xb5              i1 1 , . . . , i k k         :=                         .\\r\\n                                                                                            t1 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 t k\\r\\n\\r\\nWe call \\xc2\\xb5iso the isotropic transformation of \\xc2\\xb5.\\r\\n\\r\\n\\r\\n\\r\\n                                                                               20\\r\\n\\x0c\\n\\nAnother way we can think of \\xc2\\xb5iso is that to produce a sample from it, we can first generate\\r\\n                                                            (j )\\r\\na sample {i1 , . . . , ik } from \\xc2\\xb5, and then choose a copy imm for each element im in the sample,\\r\\nuniformly at random. We recall that subdivision preserves entropic independence.\\r\\nProposition 35 ([Ana+21a, Proposition 19]). If \\xc2\\xb5 is \\xce\\xb11 -entropically-independent, then so is \\xc2\\xb5iso .\\r\\nThe following generalizes [Ana+21a, Proposition 24], which summarizes useful properties of \\xc2\\xb5iso .\\r\\nProposition 36. Let \\xc2\\xb5 : (nk) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 , and let \\xc2\\xb5iso : (Uk ) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be the subdivided distribution from\\r\\n                                     p\\r\\nDefinition 34 for some \\xce\\xb2. Let C = 1 + \\xce\\xb2. The following hold for \\xc2\\xb5iso .\\r\\n   1. Marginal upper bound: For all i( j) \\xe2\\x88\\x88 U, the marginal P S\\xe2\\x88\\xbc\\xc2\\xb5iso [i ( j) \\xe2\\x88\\x88 S] \\xe2\\x89\\xa4 C |Uk | .\\r\\n                                                      \\xe2\\x88\\x9a\\r\\n                                                        \\xce\\xb2k\\r\\n   2. Marginal lower bound: If pi := PS\\xe2\\x88\\xbc\\xc2\\xb5 [i \\xe2\\x88\\x88 S] \\xe2\\x89\\xa5 n , then for all j \\xe2\\x88\\x88 [ti ], P S\\xe2\\x88\\xbc\\xc2\\xb5iso [i ( j) \\xe2\\x88\\x88 S] \\xe2\\x89\\xa5 C |kU | .\\r\\n                               \\x1a             \\xe2\\x88\\x9a             \\x1b\\r\\n                                   ( j )        \\xce\\xb2k\\r\\n      Furthermore, letting R := i        pi \\xe2\\x89\\xa5 n , j \\xe2\\x88\\x88 [ti ] then for any \\xe2\\x84\\x93 \\xe2\\x89\\xa4 k\\r\\n\\r\\n                                                                         p\\r\\n                                                  \\xe2\\x88\\x91 \\xc2\\xb5iso\\r\\n                                                     \\xe2\\x84\\x93 (S) \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92             \\xce\\xb2 \\xe2\\x84\\x93.\\r\\n                                                S \\xe2\\x88\\x88( R\\xe2\\x84\\x93 )\\r\\n\\r\\n\\r\\n\\r\\n   3. Bounded ground set size: n\\xce\\xb2\\xe2\\x88\\x921 \\xe2\\x89\\xa4 |U | \\xe2\\x89\\xa4 n(1 + \\xce\\xb2\\xe2\\x88\\x921 ).\\r\\n\\r\\nProof. First, we check the cardinality of the new ground set U:\\r\\n                   n                   n       n \\x12       \\x13\\r\\n             \\xe2\\x88\\x921        n                             n          n n\\r\\n          n\\xce\\xb2 = \\xe2\\x88\\x91\\r\\n                      k\\xce\\xb2\\r\\n                         p i \\xe2\\x89\\xa4 |U | = \\xe2\\x88\\x91 t i \\xe2\\x89\\xa4 \\xe2\\x88\\x91 1 +\\r\\n                                                    k\\xce\\xb2\\r\\n                                                       pi = n +\\r\\n                                                                k\\xce\\xb2 \\xe2\\x88\\x91 pi = n (1 + \\xce\\xb2 \\xe2\\x88\\x921 ).\\r\\n                  i=1                 i=1     i=1                  i=1\\r\\n\\r\\n\\r\\nNext, we check that for any i ( j) , the marginal probabilities P S\\xe2\\x88\\xbc\\xc2\\xb5iso [i ( j) \\xe2\\x88\\x88 S] are at most |Ck\\r\\n                                                                                                   U|\\r\\n                                                                                                      . In\\r\\nthe following calculation, we interpret the sampling from \\xc2\\xb5iso as first sampling from \\xc2\\xb5, and then\\r\\nchoosing a copy j \\xe2\\x88\\x88 [ti ] for each element. This yields\\r\\n\\r\\n   P S\\xe2\\x88\\xbc\\xc2\\xb5iso [i ( j) \\xe2\\x88\\x88 S] = \\xe2\\x88\\x91 P [we chose copy j | we sampled S from \\xc2\\xb5] \\xc2\\xb7 P [we sampled S from \\xc2\\xb5]\\r\\n                        S\\xe2\\x88\\x8bi\\r\\n                            1            1           1\\r\\n                      =\\xe2\\x88\\x91      \\xc2\\xb7 \\xc2\\xb5 ( S ) = \\xe2\\x88\\x91 \\xc2\\xb5 ( S ) = \\xc2\\xb7 P S \\xe2\\x88\\xbc \\xc2\\xb5 [i \\xe2\\x88\\x88 S ].\\r\\n                           t\\r\\n                        S\\xe2\\x88\\x8bi i\\r\\n                                         t i S\\xe2\\x88\\x8bi     t i\\r\\n\\r\\n\\r\\n          n            n\\r\\nSince 1 + \\xce\\xb2k pi \\xe2\\x89\\xa5 ti \\xe2\\x89\\xa5 \\xce\\xb2k pi , we obtain\\r\\n\\r\\n              k               pi                              \\xce\\xb2k    k ( \\xce\\xb2 + 1)    k ( \\xce\\xb2 + 1)   Ck\\r\\n                       =       n     \\xe2\\x89\\xa4 P S\\xe2\\x88\\xbc\\xc2\\xb5iso [i( j) \\xe2\\x88\\x88 S] \\xe2\\x89\\xa4    =              \\xe2\\x89\\xa4            \\xe2\\x89\\xa4      .\\r\\n        kp\\xe2\\x88\\x92 1\\r\\n          i + n\\xce\\xb2\\r\\n                 \\xe2\\x88\\x921        1 + \\xce\\xb2k pi                          n             \\xe2\\x88\\x92\\r\\n                                                                   n (1 + \\xce\\xb2 ) 1       |U |     |U |\\r\\n\\r\\nThe latter inequality shows\\xe2\\x88\\x9athe marginal upper bound. Next, to show the marginal lower bound,\\r\\n                                 \\xce\\xb2k\\r\\nsuppose P \\xc2\\xb5 [i \\xe2\\x88\\x88 S] = pi \\xe2\\x89\\xa5      n . Then for all j \\xe2\\x88\\x88 [ti ],\\r\\n\\r\\n                                                        k                    k                     k\\r\\n                     P S\\xe2\\x88\\xbc\\xc2\\xb5iso [i ( j) \\xe2\\x88\\x88 S] \\xe2\\x89\\xa5                     \\xe2\\x89\\xa5                  p        \\xe2\\x89\\xa5          .\\r\\n                                               kp\\xe2\\x88\\x92 1\\r\\n                                                 i + n\\xce\\xb2\\r\\n                                                        \\xe2\\x88\\x921           n\\xce\\xb2\\xe2\\x88\\x921 (1 +          \\xce\\xb2)       C |U |\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            21\\r\\n\\x0c\\n\\n                              \\x1a              \\xe2\\x88\\x9a \\x1b\\r\\n                                              \\xce\\xb2k\\r\\nFinally, letting R\\xcc\\x84 :=            i   pi \\xe2\\x89\\xa5   n          \\xe2\\x8a\\x86 [ n ],\\r\\n\\r\\n                                                                                                                                            \\xe2\\x84\\x93 pi      p\\r\\n \\xe2\\x88\\x91 \\xc2\\xb5iso\\r\\n    \\xe2\\x84\\x93 ( S ) = \\xe2\\x88\\x91 \\xc2\\xb5\\xe2\\x84\\x93 ( S\\xcc\\x84 ) = 1 \\xe2\\x88\\x92                       \\xe2\\x88\\x91                   \\xc2\\xb5\\xe2\\x84\\x93 (S\\xcc\\x84) \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 \\xe2\\x88\\x91                \\xe2\\x88\\x91              \\xc2\\xb5\\xe2\\x84\\x93 (S\\xcc\\x84) = 1 \\xe2\\x88\\x92 \\xe2\\x88\\x91\\r\\n                                                                                                                                             k\\r\\n                                                                                                                                                 \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 \\xce\\xb2 \\xe2\\x84\\x93.\\r\\nS \\xe2\\x88\\x88( R\\xe2\\x84\\x93 )       S\\xcc\\x84 \\xe2\\x88\\x88( R\\xcc\\x84\\xe2\\x84\\x93 )                  S\\xcc\\x84 \\xe2\\x8a\\x86([n\\xe2\\x84\\x93 ]): S\\xcc\\x846 \\xe2\\x8a\\x86 ( R\\xcc\\x84\\xe2\\x84\\x93 )                      i6 \\xe2\\x88\\x88 R\\xcc\\x84 S\\xcc\\x84 \\xe2\\x8a\\x86([n ]) :i\\xe2\\x88\\x88S\\xcc\\x84\\r\\n                                                                                                           \\xe2\\x84\\x93\\r\\n                                                                                                                                    i6 \\xe2\\x88\\x88 R\\xcc\\x84\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThe following is a simple consequence of the data processing inequality.\\r\\nRemark 37. For any \\xe2\\x84\\x93 \\xe2\\x88\\x88 [k], suppose algorithm A can sample from within total variation distance\\r\\n\\xc7\\xab of \\xc2\\xb5iso\\r\\n      \\xe2\\x84\\x93 . Then A can also be used to sample from within total variation distance \\xc7\\xab of \\xc2\\xb5\\xe2\\x84\\x93 using\\r\\nthe same amount of (parallel) time and machines.\\r\\n\\r\\n6.2 KL divergence bound\\r\\nThroughout this section and Section 6.3, let \\xe2\\x84\\x93 \\xe2\\x88\\x88 [k]. We begin by proving a bound on the KL\\r\\ndivergence between conditional marginals from an observed set.\\r\\n\\r\\nLemma 38. Let S \\xe2\\x88\\x88 ([nt ]) for t \\xe2\\x89\\xa4 12 k. Let \\xc2\\xb5t+1|S : [n] \\\\ S \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 be the marginal distribution of elements\\r\\nin S\\xe2\\x80\\xb2 \\xe2\\x88\\xbc \\xc2\\xb5t+1 conditioned on S \\xe2\\x8a\\x82 S\\xe2\\x80\\xb2 , namely \\xc2\\xb5t+1|S := \\xc2\\xb5(\\xc2\\xb7 | S) D(k\\xe2\\x88\\x92t)\\xe2\\x86\\x921. Then,\\r\\n                               \\x12             \\x13             \\x12                 \\x13\\r\\n                                          1         2                1          2t\\r\\n                         DKL \\xc2\\xb5t+1|S k p \\xe2\\x89\\xa4              log                     + .                       (3)\\r\\n                                          k        \\xce\\xb1k        P T \\xe2\\x88\\xbc\\xc2\\xb5 [S \\xe2\\x8a\\x82 T ]    k\\r\\n\\r\\nProof. Throughout this proof, fix the set S \\xe2\\x88\\x88 ([nt ]), and let AS denote the left-hand side of (3). Let\\r\\nqi := P T \\xe2\\x88\\xbc\\xc2\\xb5 [i \\xe2\\x88\\x88 T | S \\xe2\\x8a\\x86 T ], and note that qi = 1 for all i \\xe2\\x88\\x88 S. Moreover, we have\\r\\n                                                        \\x12           \\x13\\r\\n                                              qi          qi     k\\r\\n                                  AS = \\xe2\\x88\\x91            log      \\xc2\\xb7\\r\\n                                       i6 \\xe2\\x88\\x88S\\r\\n                                             k\\xe2\\x88\\x92t          pi k \\xe2\\x88\\x92 t\\r\\n                                                           \\x12 \\x13\\r\\n                                           k        qi       qi            k\\r\\n                                        k \\xe2\\x88\\x92 t i\\xe2\\x88\\x91\\r\\n                                     =                 log         + log                            (4)\\r\\n                                                6\\xe2\\x88\\x88S\\r\\n                                                     k       p i         k \\xe2\\x88\\x92 t\\r\\n                                                           \\x12 \\x13\\r\\n                                           k        qi       qi      2t\\r\\n                                     \\xe2\\x89\\xa4         \\xe2\\x88\\x91\\r\\n                                        k \\xe2\\x88\\x92 t i6 \\xe2\\x88\\x88S k\\r\\n                                                       log\\r\\n                                                             pi\\r\\n                                                                   + .\\r\\n                                                                      k\\r\\n                                                                                         q\\r\\nThe first equation used that by definition, \\xc2\\xb5t+1|S = k\\xe2\\x88\\x92Sct where qSc restricts q to Sc := [n] \\\\ S; the\\r\\nonly inequality used log(1 + c) \\xe2\\x89\\xa4 c for all c \\xe2\\x89\\xa5 0 and t \\xe2\\x89\\xa4 21 k. We note that for\\r\\n                                               \\x12 \\x13\\r\\n                                           qi    qi          1    1\\r\\n                               BS := \\xe2\\x88\\x91 log           + \\xe2\\x88\\x91 log ,\\r\\n                                     i6 \\xe2\\x88\\x88S\\r\\n                                           k     pi      i\\xe2\\x88\\x88S\\r\\n                                                             k    pi\\r\\n\\r\\nwe have by t \\xe2\\x89\\xa4 21 k that                                                    \\x12        \\x13\\r\\n                                                      q                         qi           2t        2t\\r\\n                                         AS \\xe2\\x89\\xa4 2 \\xe2\\x88\\x91 i log                                  +      \\xe2\\x89\\xa4 2BS + ,                                             (5)\\r\\n                                                i6 \\xe2\\x88\\x88S\\r\\n                                                      k                         pi           k         k\\r\\n\\r\\nsince log p1i \\xe2\\x89\\xa5 0 for all i \\xe2\\x88\\x88 [n]. We next give an interpretation of the quantity BS . Let \\xc2\\xb5S be the\\r\\ndistribution of T \\xe2\\x88\\xbc \\xc2\\xb5 conditioned on S \\xe2\\x8a\\x82 T, so that\\r\\n                                                  (\\r\\n                                                    1\\r\\n                                                        i\\xe2\\x88\\x88S\\r\\n                                       \\xc2\\xb5S Dk\\xe2\\x86\\x921 = qki            .\\r\\n                                                     k  i 6 \\xe2\\x88\\x88 S\\r\\n\\r\\n                                                                                22\\r\\n\\x0c\\n\\nNotice that BS is defined to be DKL (\\xc2\\xb5S Dk\\xe2\\x86\\x921 k\\xc2\\xb5Dk\\xe2\\x86\\x921 ) (since \\xc2\\xb5Dk\\xe2\\x86\\x921 = 1k p), which we can control\\r\\nby entropic independence of \\xc2\\xb5. In particular,\\r\\n\\r\\n                                                    1\\r\\n                BS = DKL (\\xc2\\xb5S Dk\\xe2\\x86\\x921 k\\xc2\\xb5Dk\\xe2\\x86\\x921 ) \\xe2\\x89\\xa4           DKL (\\xc2\\xb5S k\\xc2\\xb5)\\r\\n                                                    \\xce\\xb1k\\r\\n                                                    1                      \\xc2\\xb5S ( T )\\r\\n                                                  =\\r\\n                                                    \\xce\\xb1k  \\xe2\\x88\\x91     \\xc2\\xb5S ( T ) log\\r\\n                                                                           \\xc2\\xb5( T )\\r\\n                                                         [n ]\\r\\n                                                            T \\xe2\\x88\\x88( k )\\r\\n                                                             S\\xe2\\x8a\\x82 T\\r\\n                                                                                        \\x12                            \\x13\\r\\n                                                       1                                         \\xc2\\xb5( T )       1\\r\\n                                                  =\\r\\n                                                       \\xce\\xb1k      \\xe2\\x88\\x91         \\xc2\\xb5S ( T ) log                      \\xc2\\xb7\\r\\n                                                                                            P T \\xe2\\x88\\xbc\\xc2\\xb5 [S \\xe2\\x8a\\x82 T ] \\xc2\\xb5( T )\\r\\n                                                            T \\xe2\\x88\\x88([nk ])\\r\\n                                                             S\\xe2\\x8a\\x82 T\\r\\n                                                                    \\x12                        \\x13\\r\\n                                                    1                            1\\r\\n                                                  =    log                                       .\\r\\n                                                    \\xce\\xb1k                   P T \\xe2\\x88\\xbc\\xc2\\xb5 [S \\xe2\\x8a\\x82 T ]\\r\\n\\r\\nCombining the above display with (5) completes the proof.\\r\\n\\r\\nBy averaging Lemma 38 over \\xc2\\xb5S (the conditional distribution of T \\xe2\\x88\\xbc \\xc2\\xb5 on S \\xe2\\x8a\\x82 T), we immediately\\r\\nobtain the following corollary.\\r\\nCorollary 39. Let t \\xe2\\x89\\xa4 12 k. Then following the notation of Lemma 38,\\r\\n                                            \\x12                  \\x13     \\x12      \\x12 \\x13     \\x13\\r\\n                                                           1       2t 1      2n\\r\\n                    \\xe2\\x88\\x91      \\xc2\\xb5D k\\xe2\\x86\\x92t ( S )D KL   \\xc2\\xb5 t + 1| S k\\r\\n                                                           k\\r\\n                                                             p   \\xe2\\x89\\xa4\\r\\n                                                                   k \\xce\\xb1\\r\\n                                                                        log\\r\\n                                                                              k\\r\\n                                                                                + 1   .\\r\\n                      [n ]\\r\\n                      S \\xe2\\x88\\x88( t )\\r\\n\\r\\n\\r\\nProof. It suffices to apply Lemma 38, and the calculation\\r\\n                                 \\x12                     \\x13                                                       !\\r\\n                                             1                                                         1\\r\\n          \\xe2\\x88\\x91 \\xc2\\xb5Dk\\xe2\\x86\\x92t (S) log            P T \\xe2\\x88\\xbc\\xc2\\xb5 [S \\xe2\\x8a\\x82 T ]\\r\\n                                                           =     \\xe2\\x88\\x91 \\xc2\\xb5Dk\\xe2\\x86\\x92t (S) log   \\xc2\\xb5Dk\\xe2\\x86\\x92t (S)(kt)\\r\\n        S \\xe2\\x88\\x88([nt ])                                             S \\xe2\\x88\\x88([nt ])\\r\\n                                                                                 \\x12              \\x13\\r\\n                                                                                       1                1\\r\\n                                                           = \\xe2\\x88\\x91 \\xc2\\xb5Dk\\xe2\\x86\\x92t (S) log                      + log k\\r\\n                                                                                   \\xc2\\xb5D k\\xe2\\x86\\x92t ( S )        (t)\\r\\n                                                             S \\xe2\\x88\\x88([nt ])\\r\\n                                                                            \\x12    \\x13\\r\\n                                                                    ( n)      2n\\r\\n                                                           \\xe2\\x89\\xa4 log kt \\xe2\\x89\\xa4 t log        .\\r\\n                                                                    (t)        k\\r\\n\\r\\nThe first equality used P T \\xe2\\x88\\xbc\\xc2\\xb5 [S \\xe2\\x8a\\x82 T ] = \\xc2\\xb5Dk\\xe2\\x86\\x92t (S)(kt), and the last line used that the negative\\r\\nentropy of a distribution supported on N elements is bounded by log N.\\r\\n\\r\\nFinally, we use Corollary 39 to derive a KL divergence bound between the distributions \\xc2\\xb5\\xe2\\x84\\x93 and \\xc2\\xb5\\xe2\\x80\\xb2\\xe2\\x84\\x93 ,\\r\\nrespectively the target and proposal distributions encountered in our rejection sampling scheme.\\r\\nLemma 40. Let \\xc2\\xb5\\xe2\\x80\\xb2j be the distribution of the set formed by j independent draws from 1k p. Let \\xe2\\x84\\x93 \\xe2\\x89\\xa4 12 k.\\r\\nThen,                                             \\x12       \\x12 \\x13       \\x13\\r\\n                                         \\xe2\\x80\\xb2     \\xe2\\x84\\x932 1        2n\\r\\n                              DKL (\\xc2\\xb5\\xe2\\x84\\x93 k\\xc2\\xb5\\xe2\\x84\\x93 ) \\xe2\\x89\\xa4         log        +1 .\\r\\n                                                k \\xce\\xb1         k\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                23\\r\\n\\x0c\\n\\nProof. For any j \\xe2\\x88\\x88 [\\xe2\\x84\\x93], following the notation of Lemma 38,\\r\\n                                                                                                   \\x12               \\x13\\r\\n                                                                                                              1\\r\\n                DKL (\\xc2\\xb5 j k\\xc2\\xb5\\xe2\\x80\\xb2j ) \\xe2\\x88\\x92 DKL (\\xc2\\xb5 j\\xe2\\x88\\x921 k\\xc2\\xb5\\xe2\\x80\\xb2j\\xe2\\x88\\x921 ) =      \\xe2\\x88\\x91 \\xc2\\xb5Dk\\xe2\\x86\\x92( j\\xe2\\x88\\x921) (S)DKL                       \\xc2\\xb5 j|S k p\\r\\n                                                                                                              k\\r\\n                                                          S \\xe2\\x88\\x88( j[\\xe2\\x88\\x92n ]1)\\r\\n                                                                          \\x12           \\x12        \\x13       \\x13\\r\\n                                                       2( j \\xe2\\x88\\x92 1)              1           2n\\r\\n                                                     \\xe2\\x89\\xa4                          log                +1 .\\r\\n                                                            k                 \\xce\\xb1            k\\r\\n\\r\\nIn the first line, we used the chain rule of KL divergence, and the second line used Corollary 39.\\r\\nFinally, the conclusion follows by telescoping the above display for 1 \\xe2\\x89\\xa4 j \\xe2\\x89\\xa4 \\xe2\\x84\\x93.\\r\\n\\r\\nLemma 40 bounds the KL divergence between \\xc2\\xb5\\xe2\\x84\\x93 and \\xc2\\xb5\\xe2\\x80\\xb2\\xe2\\x84\\x93 , which can be thought of as an average\\r\\nlog-acceptance probability for our rejection sampling scheme. For constant \\xce\\xb11 , this bound suggests\\r\\n                      \\xe2\\x88\\x9a\\r\\nthat we can take \\xe2\\x84\\x93 \\xe2\\x89\\x88 k and obtain an efficient sampler for \\xe2\\x84\\x93-marginals; however, it is only an\\r\\naverage bound. We make this intuition rigorous in Section 6.3, where we use the tools from this\\r\\nsection to give concentration bounds on the acceptance probability of rejection sampling.\\r\\n\\r\\n6.3 Concentration of acceptance probability\\r\\nIn this section, assume that we have already performed the transformation in Proposition 36\\r\\nparameterized by some \\xce\\xb2, and obtained a distribution \\xce\\xbdiso : (Uk ) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 and a set R \\xe2\\x8a\\x86 U of\\r\\nelements with lower bounds on marginals as given by Proposition 36. Let \\xce\\xbd := \\xce\\xbdiso and let \\xce\\xbd\\xe2\\x80\\xb2 be\\r\\ndefined analogously to Section 6.2. Our goal is to sample from within \\xc7\\xab total variation of \\xce\\xbd\\xe2\\x84\\x93 for\\r\\na suitably chosen \\xe2\\x84\\x93, which also implies that we can sample from within \\xc7\\xab of \\xc2\\xb5\\xe2\\x84\\x93 (see Remark 37).\\r\\nOur algorithm will be the modified rejection sampler of Algorithm 3.\\r\\nTo use Algorithm 3 with P = \\xce\\xbdDk\\xe2\\x86\\x92\\xe2\\x84\\x93 and Q the \\xe2\\x84\\x93-wise product distribution drawing from 1k p, we\\r\\nfirst define a relevant high-probability set \\xe2\\x84\\xa6 on our state space X := (Uk ). Our set \\xe2\\x84\\xa6 will be a\\r\\nsubset of the following set, for some \\xce\\xb5 > 0 we will choose later:\\r\\n                                  \\x1a     \\x12 \\x13                               \\x1b\\r\\n                           \\xe2\\x84\\xa6e \\xce\\xb5 := S \\xe2\\x88\\x88 U | \\xce\\xbd| T | ( T ) \\xe2\\x89\\xa5 \\xce\\xb5| T | , \\xe2\\x88\\x80 T \\xe2\\x8a\\x86 S .\\r\\n                                          \\xe2\\x84\\x93\\r\\n\\r\\nIn other words, \\xe2\\x84\\xa6   e \\xce\\xb5 contains all sets S such that all subsets T \\xe2\\x8a\\x82 S are relatively well-represented\\r\\naccording to \\xce\\xbd| T | ( T ). We begin with an observation lower bounding the measure of \\xe2\\x84\\xa6    e \\xce\\xb5.\\r\\n                             1\\r\\nLemma 41. For any 0 \\xe2\\x89\\xa4 \\xce\\xb5 \\xe2\\x89\\xa4 2 |U |\\xe2\\x84\\x93 ,\\r\\n                                              \\xe2\\x88\\x91 \\xce\\xbd\\xe2\\x84\\x93 (S) \\xe2\\x89\\xa4 2 |U |\\xe2\\x84\\x93\\xce\\xb5.\\r\\n                                                e\\xce\\xb5\\r\\n                                             S6\\xe2\\x88\\x88\\xe2\\x84\\xa6\\r\\n\\r\\n\\r\\nProof. Let C := \\xe2\\x88\\xaa\\xe2\\x84\\x93t=1 {T \\xe2\\x88\\x88 (Ut ) | \\xce\\xbdt ( T ) \\xe2\\x89\\xa4 \\xce\\xb5t }. For any S 6\\xe2\\x88\\x88 \\xe2\\x84\\xa6e \\xce\\xb5 , we say T \\xe2\\x88\\x88 C is a \\xe2\\x80\\x9ccertificate\\xe2\\x80\\x9d of S\\r\\nif T \\xe2\\x8a\\x82 S; every S \\xe2\\x88\\x88 \\xe2\\x84\\xa6 e c\\xce\\xb5 has at least one certificate, so there is a map M : \\xe2\\x84\\xa6       e c\\xce\\xb5 \\xe2\\x86\\x92 C . Moreover, for\\r\\n                 e c                               e c\\r\\nsome T \\xe2\\x88\\x88 C , let \\xe2\\x84\\xa6\\xce\\xb5 ( T ) be the set of all S \\xe2\\x88\\x88 \\xe2\\x84\\xa6\\xce\\xb5 such that M(S) = T. Then since\\r\\n                                                                          \\x12 \\x13\\r\\n                                          1                                 \\xe2\\x84\\x93\\r\\n                         \\xce\\xbdt ( T ) = \\xe2\\x88\\x91 \\xe2\\x84\\x93 \\xce\\xbd\\xe2\\x84\\x93 (S) =\\xe2\\x87\\x92         \\xe2\\x88\\x91     \\xce\\xbd\\xe2\\x84\\x93 (S) \\xe2\\x89\\xa4       \\xce\\xbdt ( T ),\\r\\n                                    S\\xe2\\x8a\\x87 T ( )              ec\\r\\n                                                                            t\\r\\n                                         t                   S\\xe2\\x88\\x88\\xe2\\x84\\xa6\\xce\\xb5 ( T )\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                          24\\r\\n\\x0c\\n\\nsumming over all T \\xe2\\x88\\x88 C yields\\r\\n                                               \\x12 \\x13\\r\\n                                                 \\xe2\\x84\\x93\\r\\n                               \\xe2\\x88\\x91 \\xce\\xbd\\xe2\\x84\\x93 (S) \\xe2\\x89\\xa4 \\xe2\\x88\\x91 t \\xce\\xbd|T | (T )\\r\\n                                e c\\xce\\xb5\\r\\n                              S\\xe2\\x88\\x88\\xe2\\x84\\xa6         T \\xe2\\x88\\x88C\\r\\n                                                      \\x12 \\x13\\r\\n                                                       \\xe2\\x84\\x93\\r\\n                                        = \\xe2\\x88\\x91        \\xe2\\x88\\x91 t \\xce\\xbdt (T )\\r\\n                                          1\\xe2\\x89\\xa4 t\\xe2\\x89\\xa4\\xe2\\x84\\x93    U\\r\\n                                                        T \\xe2\\x88\\x88( t )\\r\\n                                                       \\xce\\xbdt ( T )\\xe2\\x89\\xa4 \\xce\\xb5 t\\r\\n                                                                         |U |\\xe2\\x84\\x93\\xce\\xb5\\r\\n                                          \\xe2\\x89\\xa4     \\xe2\\x88\\x91 (|U |\\xe2\\x84\\x93\\xce\\xb5)t \\xe2\\x89\\xa4 1 \\xe2\\x88\\x92 |U |\\xe2\\x84\\x93\\xce\\xb5 \\xe2\\x89\\xa4 2 |U |\\xe2\\x84\\x93\\xce\\xb5.\\r\\n                                              1\\xe2\\x89\\xa4 t\\xe2\\x89\\xa4\\xe2\\x84\\x93\\r\\n\\r\\n\\r\\nThe last line used the approximations (\\xe2\\x84\\x93t ) \\xe2\\x89\\xa4 \\xe2\\x84\\x93t , (|Ut |) \\xe2\\x89\\xa4 |U |t .\\r\\n\\r\\n                                                                                    e \\xce\\xb5 captures at least\\r\\nFor the remainder of the section we will specifically use \\xce\\xb5 = 32 |\\xc7\\xabU |\\xe2\\x84\\x93 , such that \\xe2\\x84\\xa6\\r\\n      \\xc7\\xab                          U                                                  e \\xce\\xb5 for simplicity.\\r\\na 1 \\xe2\\x88\\x92 fraction of the mass of ( ) according to \\xce\\xbd\\xe2\\x84\\x93 . We will also drop \\xce\\xb5 from \\xe2\\x84\\xa6\\r\\n      16                              \\xe2\\x84\\x93\\r\\n                                                        e have a polynomially bounded acceptance\\r\\nOur next goal is to show that almost all of the sets in \\xe2\\x84\\xa6\\r\\nprobability when the proposal is given by independent draws from 1k p. Consider iteratively\\r\\nbuilding a set St for all 1 \\xe2\\x89\\xa4 t \\xe2\\x89\\xa4 \\xe2\\x84\\x93, where St is a random variable formed by St\\xe2\\x88\\x921 \\xe2\\x88\\xaa {it } for it \\xe2\\x88\\xbc 1k p.\\r\\nIn particular, we use it to denote the tth draw from 1k p in this process. For parameters \\xcf\\x84, \\xce\\xb3 \\xe2\\x89\\xa5 0 to\\r\\nbe defined later, iteratively define the random variables:\\r\\n\\r\\n                      Yt+1 := Yt exp (\\xe2\\x88\\x86t+1 ) ,\\r\\n                                \\xef\\xa3\\xb1     \\x12              \\x13\\r\\n                                \\xef\\xa3\\xb2\\xce\\xb3 log \\xce\\xbdt+1|St (it+1) \\xe2\\x88\\x92 \\xcf\\x84               \\xce\\xbdt (St ) \\xe2\\x89\\xa5 \\xce\\xb5t and it+1 \\xe2\\x88\\x88 R\\r\\n                                          1\\r\\n                      \\xe2\\x88\\x86t +1 : =           k p i t +1\\r\\n                                \\xef\\xa3\\xb3\\r\\n                                  \\xe2\\x88\\x92\\xe2\\x88\\x9e                                     otherwise\\r\\n              p\\r\\nwith C = 1 + \\xce\\xb2 and R as defined in Proposition 36. Also by Proposition 36, pi \\xe2\\x89\\xa4 |Ck\\r\\n                                                                                 U|\\r\\n                                                                                    for all\\r\\ni \\xe2\\x88\\x88 U. We use the convention exp(\\xe2\\x88\\x92\\xe2\\x88\\x9e) = 0.\\r\\nWe next prove that Yt+1 is a submartingale for appropriate parameter choices.\\r\\n                                                p        n                o\\r\\nLemma 42. Let St = T have \\xce\\xbdt ( T ) \\xe2\\x89\\xa5 \\xce\\xb5t . Assume \\xce\\xb2 \\xe2\\x89\\xa4 min 3\\xce\\xb3 1\\r\\n                                                              , \\xce\\xb1kt log 1\\xce\\xb5 . Then,\\r\\n                                          \\x12                 \\x13\\r\\n                          \\xce\\xb3                   12t     1\\r\\n                 \\xcf\\x84 \\xe2\\x89\\xa5 |U | \\xce\\xb3 ( 1 + \\xce\\xb3 ) \\xc2\\xb7           log             =\\xe2\\x87\\x92 Ei\\xe2\\x88\\xbc\\xce\\xbdt+1|St [Yt+1 | St = T ] \\xe2\\x89\\xa4 Yt .\\r\\n                                              \\xce\\xb1k      \\xce\\xb5\\r\\n\\r\\nProof. If Yt = 0 then Yt+1 = 0 by definition. In the following, assume Yt > 0. By the definition of\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 25\\r\\n\\x0c\\n\\nYt+1 = Yt exp(\\xe2\\x88\\x86t+1 ), and since \\xce\\xbdt ( T ) \\xe2\\x89\\xa5 \\xce\\xb5t , we have\\r\\n                          \\x14                \\x15\\r\\n                            Yt+1\\r\\n               E i\\xe2\\x88\\xbc\\xce\\xbdt+1|T         | St = T\\r\\n                             Yt\\r\\n                                         \"                       !\\xce\\xb3             #\\r\\n                                                  \\xce\\xbdt+1| T (i )\\r\\n               = exp (\\xe2\\x88\\x92\\xcf\\x84 ) Ei\\xe2\\x88\\xbc\\xce\\xbdt+1|T 1 i\\xe2\\x88\\x88 R            1\\r\\n                                                                      | St = T\\r\\n                                                       k p i\\r\\n                                                             !\\xce\\xb3\\r\\n                                                \\xce\\xbdt+1| T (i )\\r\\n               = exp (\\xe2\\x88\\x92\\xcf\\x84 ) \\xe2\\x88\\x91 \\xce\\xbdt+1|T (i)            1\\r\\n                               i\\xe2\\x88\\x88 R                k pi\\r\\n                                    \\x12                        \\x12        \\x12             \\x13         \\x13\\x13\\r\\n                                 \\xce\\xb3           \\xce\\xb3                                   1\\r\\n               \\xe2\\x89\\xa4 exp (\\xe2\\x88\\x92\\xcf\\x84 ) C 1 + |U | \\xce\\xb3(1 + \\xce\\xb3) DKL \\xce\\xbdt+1|T k p + log C\\r\\n                                                                                 k\\r\\n                                             \\x12                             \\x12                  \\x13\\x13\\r\\n                                       p                   \\xce\\xb3                 4t       1 p\\r\\n               \\xe2\\x89\\xa4 exp (\\xe2\\x88\\x92\\xcf\\x84 ) (1 + 2 \\xce\\xb2\\xce\\xb3) 1 + |U | \\xce\\xb3(1 + \\xce\\xb3) \\xc2\\xb7                        log + \\xce\\xb2         .\\r\\n                                                                             \\xce\\xb1k       \\xce\\xb5\\r\\n                                                                             p\\r\\nThe second-to-last inequality used Lemma 25 with C = 1 + \\xce\\xb2, and the last inequality used\\r\\nLemma 38, which shows that since \\xce\\xbdt ( T ) \\xe2\\x89\\xa5 \\xce\\xb5t ,\\r\\n                           \\x12            \\x13               \\x12                    \\x13\\r\\n                                      1        2                    1              2t\\r\\n                     DKL \\xce\\xbdt+1|T k p \\xe2\\x89\\xa4             log                           +\\r\\n                                      k        \\xce\\xb1k          P S\\xe2\\x88\\xbc\\xce\\xbd [ T \\xe2\\x8a\\x82 S]          k\\r\\n                                                                        !\\r\\n                                               2                 1           2t      4t    1\\r\\n                                           =      log                k\\r\\n                                                                           +     \\xe2\\x89\\xa4      log ,\\r\\n                                               \\xce\\xb1k           \\xce\\xbdt ( T )( )       k     \\xce\\xb1k     \\xce\\xb5\\r\\n                                                                            t\\r\\n                      p          p\\r\\nas well as log(1 +        \\xce\\xb2) \\xe2\\x89\\xa4       \\xce\\xb2 and\\r\\n\\r\\n                                             (1 + x)\\xce\\xb3 \\xe2\\x89\\xa4 ex\\xce\\xb3 \\xe2\\x89\\xa4 1 + 2x\\xce\\xb3\\r\\n            p\\r\\nfor x\\xce\\xb3 :=       \\xce\\xb2\\xce\\xb3 \\xe2\\x89\\xa4 13 . The conclusion follows from\\r\\n                                      \\x12                         \\x12                   \\x13\\x13\\r\\n                             p                     \\xce\\xb3               4t      1 p\\r\\n                       (1 + 2 \\xce\\xb2\\xce\\xb3) 1 + |U | \\xce\\xb3(1 + \\xce\\xb3) \\xc2\\xb7                  log + \\xce\\xb2\\r\\n                                                                  \\xce\\xb1k       \\xce\\xb5\\r\\n                                                             \\x12             \\x13\\r\\n                                p                               5t       1         p\\r\\n                        \\xe2\\x89\\xa4 1 + 2 \\xce\\xb2\\xce\\xb3 + |U |\\xce\\xb3 \\xce\\xb3(1 + \\xce\\xb3) \\xc2\\xb7               log      (1 + 2 \\xce\\xb2\\xce\\xb3)\\r\\n                                                                \\xce\\xb1k       \\xce\\xb5\\r\\n                                \\x12              \\x13                        \\x12          \\x13\\x12      \\x13\\r\\n                                    2t       1          \\xce\\xb3                 5t     1       2\\r\\n                        \\xe2\\x89\\xa4 1+\\xce\\xb3           log       + |U | \\xce\\xb3 ( 1 + \\xce\\xb3 ) \\xc2\\xb7       log      1+\\r\\n                                    \\xce\\xb1k       \\xce\\xb5                            \\xce\\xb1k     \\xce\\xb5       3\\r\\n                                                   \\x12            \\x13\\r\\n                                                     12t      1\\r\\n                        \\xe2\\x89\\xa4 1 + |U | \\xce\\xb3 \\xce\\xb3 ( 1 + \\xce\\xb3 ) \\xc2\\xb7       log       \\xe2\\x89\\xa4 1 + \\xcf\\x84 \\xe2\\x89\\xa4 exp(\\xcf\\x84 ).\\r\\n                                                     \\xce\\xb1k       \\xce\\xb5\\r\\n\\r\\n\\r\\n                                                              n         o\\r\\nNow, applying Lemma 42 with the definition of \\xe2\\x84\\xa6   e \\xe2\\x80\\xb2 := \\xe2\\x84\\xa6e \\xe2\\x88\\xa9 S \\xe2\\x88\\x88 ( R) allows us to obtain a\\r\\n                                                                      \\xe2\\x84\\x93\\r\\nhigh-probability bound on the acceptance probability of our rejection sampling scheme.\\r\\nLemma 43. Let B \\xe2\\x89\\xa5 1. For sufficiently small \\xc7\\xab \\xe2\\x88\\x88 (0, 1), and \\xe2\\x84\\x93 \\xe2\\x88\\x88 [k] satisfying\\r\\n\\r\\n                                                      16\\r\\n                                                           \\x01 B3\\r\\n                                               12\\xe2\\x84\\x932    \\xc7\\xab          log 1\\xce\\xb5\\r\\n                                                                           \\xe2\\x89\\xa4 1,\\r\\n                                                      \\xce\\xb1k\\r\\n\\r\\n\\r\\n\\r\\n                                                             26\\r\\n\\x0c\\n\\nand supposing our choices of parameters satisfy\\r\\n                                                                  \\x1a                    \\x1b\\r\\n                                                 p                     1 1      1\\r\\n                                                     \\xce\\xb2 \\xe2\\x89\\xa4 min            ,   log            ,\\r\\n                                                                      3\\xce\\xb3 \\xce\\xb1k     \\xce\\xb5\\r\\nwe have                                              \\x14                               \\x15\\r\\n                                                         \\xce\\xbd\\xe2\\x84\\x93 (S\\xe2\\x84\\x93 )       B        e \\xe2\\x80\\xb2   \\xc7\\xab\\r\\n                                          P S\\xe2\\x84\\x93 \\xe2\\x88\\xbc\\xce\\xbd\\xe2\\x84\\x93        \\xe2\\x80\\xb2       \\xe2\\x89\\xa5 |U | | S \\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6 \\xe2\\x89\\xa4 .\\r\\n                                                         \\xce\\xbd\\xe2\\x84\\x93 (S\\xe2\\x84\\x93 )                      8\\r\\n\\r\\nProof. Throughout this proof, we will assume\\r\\n                                                           2 log 16\\r\\n                                                                  \\xc7\\xab        log 16\\r\\n                                                                                \\xc7\\xab\\r\\n                                                  \\xce\\xb3=                  , \\xcf\\x84=        .\\r\\n                                                           B log |U |        \\xe2\\x84\\x93\\r\\nWe first observe that our parameter choices indeed satisfy the condition on \\xcf\\x84 used in Lemma 42:\\r\\n                                  \\x12 \\x13 B3                          \\x12           \\x13\\r\\n             \\xce\\xb3 ( 1 + \\xce\\xb3 ) |U | \\xce\\xb3    16                         \\xce\\xb3     12\\xe2\\x84\\x93     1       \\xe2\\x84\\x93\\r\\n                      16\\r\\n                          \\x01     \\xe2\\x89\\xa4        =\\xe2\\x87\\x92 \\xce\\xb3 ( 1 + \\xce\\xb3 ) | U |   \\xc2\\xb7       log     \\xc2\\xb7        \\xe2\\x89\\xa4 1.\\r\\n                 log \\xc7\\xab              \\xc7\\xab                               \\xce\\xb1k      \\xce\\xb5     log 16\\r\\n                                                                                       \\xc7\\xab\\r\\n\\r\\n\\r\\nIn the following we denote \\xc2\\xb5\\xcc\\x82 j to be the joint distribution of {i1 , i2 , . . . , i j } where i1 \\xe2\\x88\\xbc \\xce\\xbd1 , i2 \\xe2\\x88\\xbc\\r\\n\\xce\\xbd2|S1 ={i1 } , and so on. In other words, if S\\xe2\\x84\\x93 is the unordered set of {i1 , i2 , . . . , i\\xe2\\x84\\x93 }, we have \\xce\\xbd\\xe2\\x84\\x93 (S\\xe2\\x84\\x93 ) =\\r\\n\\xe2\\x84\\x93! \\xc2\\xb7 \\xc2\\xb5\\xcc\\x82 ({i1 , i2 , . . . , i\\xe2\\x84\\x93 }). We similarly define \\xc2\\xb5\\xcc\\x82\\xe2\\x80\\xb2j so that \\xce\\xbd\\xe2\\x84\\x93\\xe2\\x80\\xb2 (S\\xe2\\x84\\x93 ) = \\xe2\\x84\\x93! \\xc2\\xb7 \\xc2\\xb5\\xcc\\x82\\xe2\\x80\\xb2\\xe2\\x84\\x93 ({i1 , i2 , . . . , i\\xe2\\x84\\x93 }). For S\\xe2\\x84\\x93 \\xe2\\x88\\x88 (U\\xe2\\x84\\x93 ),\\r\\nand some realization {i1 , i2 , . . . , i\\xe2\\x84\\x93 } whose unordered set is S\\xe2\\x84\\x93 , cancelling a factor of \\xe2\\x84\\x93! yields\\r\\n\\r\\n                                    \\xce\\xbd\\xe2\\x84\\x93 (S\\xe2\\x84\\x93 )  \\xc2\\xb5\\xcc\\x82 ({i , i2 , . . . , i\\xe2\\x84\\x93 })        \\xce\\xbdj|S j\\xe2\\x88\\x921 ={i1 ,i2 ,...,i j\\xe2\\x88\\x921} ({i j })\\r\\n                     L ( S\\xe2\\x84\\x93 ) : =    \\xe2\\x80\\xb2       = \\xe2\\x80\\xb2\\xe2\\x84\\x93 1                         =\\xe2\\x88\\x8f                  1\\r\\n                                                                                                                                (6)\\r\\n                                    \\xce\\xbd\\xe2\\x84\\x93 (S\\xe2\\x84\\x93 )  \\xc2\\xb5\\xcc\\x82\\xe2\\x84\\x93 ({i1 , i2 , . . . , i\\xe2\\x84\\x93 })  j\\xe2\\x88\\x88\\xe2\\x84\\x93                k pi j\\r\\n\\r\\nwhere \\xce\\xbd\\xe2\\x84\\x93\\xe2\\x80\\xb2 is the distribution of the unordered set corresponding to \\xe2\\x84\\x93 draws from 1k p. Next, we\\r\\napply Lemma 42 which yields a submartingale property on Y\\xe2\\x84\\x93 . Letting 1 S \\xe2\\x88\\x88\\xe2\\x84\\xa6                      e \\xe2\\x80\\xb2 be the 0-1 valued\\r\\n                                                                                              \\xe2\\x84\\x93\\r\\nindicator function of the event S\\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6           e , we compute\\r\\n                                                   \\xe2\\x80\\xb2\\r\\n                                           h                  i\\r\\n           1 = Y0 \\xe2\\x89\\xa5 E {i1 ,i2 ,...,i\\xe2\\x84\\x93 }\\xe2\\x88\\xbc\\xc2\\xb5\\xcc\\x82\\xe2\\x84\\x93 Y\\xe2\\x84\\x93 \\xc2\\xb7 1 S\\xe2\\x84\\x93 \\xe2\\x88\\x88\\xe2\\x84\\xa6  e\\xe2\\x80\\xb2\\r\\n                                                                       h                                  i\\r\\n                  = P S\\xe2\\x84\\x93 \\xe2\\x88\\xbc\\xce\\xbd\\xe2\\x84\\x93 [S\\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6      e \\xe2\\x80\\xb2 ] \\xc2\\xb7 E {i ,i ,...,i }\\xe2\\x88\\xbc\\xc2\\xb5\\xcc\\x82 exp (\\xe2\\x88\\x92\\xe2\\x84\\x93\\xcf\\x84 + \\xce\\xb3 log L(S\\xe2\\x84\\x93 )) | S\\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6 e\\xe2\\x80\\xb2 .\\r\\n                                                      1 2       \\xe2\\x84\\x93    \\xe2\\x84\\x93\\r\\n\\r\\n\\r\\n\\r\\nIn the last two expressions, S\\xe2\\x84\\x93 denotes the unordered set {i1 , i2 , . . . , i\\xe2\\x84\\x93 }. The first inequality used\\r\\nthe fact that Y\\xe2\\x84\\x93 is always nonnegative, and whenever S\\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6                      e \\xe2\\x80\\xb2 we can apply Lemma 42 to all\\r\\nsubsets in the stages of its construction. The second line follows since whenever S\\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6                   e \\xe2\\x80\\xb2 , we are\\r\\nalways in the first case in the definition of \\xe2\\x88\\x86t+1 , and then we can apply (6). Hence, for any B \\xe2\\x89\\xa5 0,\\r\\n                                                                    h                           i\\r\\n                   P S\\xe2\\x84\\x93 \\xe2\\x88\\xbc\\xce\\xbd\\xe2\\x84\\x93 [S\\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6                                                          e \\xe2\\x80\\xb2 \\xe2\\x89\\xa4 exp (\\xe2\\x84\\x93\\xcf\\x84 )\\r\\n                                       e \\xe2\\x80\\xb2 ] \\xc2\\xb7 E {i ,i ,...,i }\\xe2\\x88\\xbc\\xc2\\xb5\\xcc\\x82 exp (\\xce\\xb3 log L(S\\xe2\\x84\\x93 )) | S\\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6\\r\\n                                                   1 2       \\xe2\\x84\\x93    \\xe2\\x84\\x93\\r\\n                                     h                                         i\\r\\n           =\\xe2\\x87\\x92 P {i1 ,i2 ,...,i\\xe2\\x84\\x93 }\\xe2\\x88\\xbc\\xc2\\xb5\\xcc\\x82\\xe2\\x84\\x93 log L(S\\xe2\\x84\\x93 ) \\xe2\\x89\\xa5 B log |U | | S\\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6      e \\xe2\\x80\\xb2 \\xe2\\x89\\xa4 2 exp (\\xe2\\x84\\x93\\xcf\\x84 \\xe2\\x88\\x92 \\xce\\xb3B log |U |) ,\\r\\n\\r\\nwhere the last line used Markov\\xe2\\x80\\x99s inequality, and that \\xe2\\x84\\xa6    e \\xe2\\x80\\xb2 captures at least half the mass of \\xce\\xbd\\xe2\\x84\\x93 .\\r\\nHowever, every permutation giving rise to the unordered set S\\xe2\\x84\\x93 is equally likely under \\xc2\\xb5\\xcc\\x82\\xe2\\x84\\x93 , so by\\r\\naggregating permutations, this can be rewritten as the desired\\r\\n                         h                          i\\r\\n                                                 e \\xe2\\x80\\xb2 \\xe2\\x89\\xa4 2 exp (\\xe2\\x84\\x93\\xcf\\x84 \\xe2\\x88\\x92 \\xce\\xb3B log |U |) = \\xc7\\xab .\\r\\n                 P S\\xe2\\x84\\x93 \\xe2\\x88\\xbc\\xce\\xbd\\xe2\\x84\\x93 L(S\\xe2\\x84\\x93 ) \\xe2\\x89\\xa5 |U | B | S\\xe2\\x84\\x93 \\xe2\\x88\\x88 \\xe2\\x84\\xa6\\r\\n                                                                                  8\\r\\n\\r\\n\\r\\n\\r\\n                                                                    27\\r\\n\\x0c\\n\\nFinally, we combine Lemma 41 and Lemma 43 to prove our main result.\\r\\nLemma 44. Let B \\xe2\\x89\\xa5 1, and for a sufficiently small constant below, suppose\\r\\n                                              \\x12              \\x13\\r\\n                                        2          \\xce\\xb1k      3\\r\\n                                       \\xe2\\x84\\x93 =O             \\xc2\\xb7 \\xc7\\xabB .\\r\\n                                                 log n\\xc7\\xab\\r\\n\\r\\nThere is a parallel algorithm using O((nk2 \\xc7\\xab\\xe2\\x88\\x922 ) B log 1\\xc7\\xab ) machines which runs in O(1) time and returns a\\r\\ndraw from a distribution within total variation distance 2\\xc7\\xab of \\xc2\\xb5Dk\\xe2\\x86\\x92\\xe2\\x84\\x93 .\\r\\n\\r\\nProof. Without loss of generality, we can assume n is at least a sufficiently large constant, else the\\r\\nstandard sequential sampler has parallel depth Oe (1). We set\\r\\n                                    \\x1a             \\x1b         (                     )\\r\\n                 p        \\xc7\\xab            1     1 1                1      1 B log n\\r\\n                   \\xce\\xb2 :=      \\xe2\\x89\\xa4 min        log ,     = min         log ,             ,\\r\\n                         32k           \\xce\\xb1k    \\xce\\xb5 3\\xce\\xb3              \\xce\\xb1k      \\xce\\xb5 6 log 16\\r\\n                                                                                \\xc7\\xab\\r\\n\\r\\nwhich clearly satisfies the assumption of Lemma 42 for sufficiently large n. Set \\xce\\xb5 = 32 |\\xc7\\xabU |\\xe2\\x84\\x93 . Com-\\r\\nbining Proposition 36 and Lemma 41 and using a union bound, we have\\r\\n                                   \\x12       \\x12\\x1a    \\x12 \\x13\\x1b\\x13\\x13\\r\\n            \\xe2\\x80\\xb2                                     R                p                        \\xc7\\xab\\r\\n                             e )) \\xe2\\x88\\x92 1 \\xe2\\x88\\x92 \\xce\\xbd\\xe2\\x84\\x93\\r\\n          e ) \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xce\\xbd\\xe2\\x84\\x93 (\\xe2\\x84\\xa6\\r\\n      \\xce\\xbd\\xe2\\x84\\x93 (\\xe2\\x84\\xa6                                   S\\xe2\\x88\\x88           \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 2 \\xce\\xb2k \\xe2\\x88\\x92 2 |U |\\xe2\\x84\\x93\\xce\\xb5 \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 .\\r\\n                                                  \\xe2\\x84\\x93                                         8\\r\\nBy Proposition 36, |U | \\xe2\\x89\\xa4 2n\\xce\\xb2\\xe2\\x88\\x921 = O(nk2 \\xc7\\xab\\xe2\\x88\\x922 ) and\\r\\n                                        \\x12            \\x13    \\x10\\r\\n                                  1            |U |\\xe2\\x84\\x93           n\\x11\\r\\n                              log = O log              = O log\\r\\n                                  \\xce\\xb5              \\xc7\\xab             \\xc7\\xab\\r\\n\\r\\nThus, this setting of \\xe2\\x84\\x93 and \\xce\\xb2 satisfies the assumption of Lemma 43. Hence, the subset \\xe2\\x84\\xa6 \\xe2\\x8a\\x82 \\xe2\\x84\\xa6        e\\xe2\\x80\\xb2\\r\\n                                                                             \\xc7\\xab\\r\\nwhich satisfies the conclusion of Lemma 43 has measure at least 1 \\xe2\\x88\\x92 4 according to \\xce\\xbd\\xe2\\x84\\x93 . Using\\r\\nAlgorithm 3 and Remark 37, we can sample from within total variation 2\\xc7\\xab from \\xce\\xbdDk\\xe2\\x86\\x92\\xe2\\x84\\x93 and from\\r\\n\\xc2\\xb5Dk\\xe2\\x86\\x92\\xe2\\x84\\x93 in O e (1)-time using O(|U | B log 1 ) = O((nk2 \\xc7\\xab\\xe2\\x88\\x922 ) B log 1 ) machines. We note that to imple-\\r\\n                                         \\xce\\xb5                        \\xc7\\xab\\r\\nment our modified rejection sampling, it suffices to check that the likelihood ratio is bounded,\\r\\nwhich will certainly be the case for all elements in \\xe2\\x84\\xa6 e \\xe2\\x80\\xb2 , and if there are other sets with bounded\\r\\nlikelihood ratio this only improves the total variation distance guarantee.\\r\\n\\r\\n\\r\\n\\r\\n6.4 Proof of Theorem 33\\r\\nIn this section, we combine the isotropic transformation of Section 6.1, the parallel sampler of\\r\\nLemma 44, and the recursive strategy of Proposition 27 to prove Theorem 33.\\r\\n\\r\\nProof of Theorem 33. Since we can always sample in O      e (k) parallel time, the statement is nontrivial\\r\\n              1       \\xe2\\x80\\xb2   \\xc7\\xab\\r\\nonly for c \\xe2\\x89\\xa4 2 . Set \\xc7\\xab \\xe2\\x86\\x90 k . As in Proposition 27, it suffices to repeatedly sample from \\xc2\\xb5Dk\\xe2\\x86\\x92\\xe2\\x84\\x93 for\\r\\nsome choice of \\xe2\\x84\\x93 respecting the bound in Lemma 44, within total variation \\xc7\\xab\\xe2\\x80\\xb2 . We then condition\\r\\non this set, and then repeat. By the coupling characterization of total variation, the resulting\\r\\ndistribution will be at total variation \\xc7\\xab from \\xc2\\xb5, since this process will clearly terminate within k\\r\\nrounds. It is straightforward to see this will terminate in\\r\\n                                      \\xef\\xa3\\xab                  \\xef\\xa3\\xb6\\r\\n                                        v\\r\\n                                        u        k\\r\\n                                      \\xef\\xa3\\xacu                 \\xef\\xa3\\xb7\\r\\n                                    O \\xef\\xa3\\xadt               3 \\xef\\xa3\\xb8 iterations\\r\\n                                              \\xce\\xb1       \\xe2\\x80\\xb2B\\r\\n                                            log n \\xc2\\xb7 \\xc7\\xab\\r\\n                                                \\xc7\\xab\\xe2\\x80\\xb2\\r\\n\\r\\n\\r\\n\\r\\n                                                     28\\r\\n\\x0c\\n\\nby a variation of the proof of Proposition 27 and the maximum allowable \\xe2\\x84\\x93 in Lemma 44. Setting\\r\\nB = 3c gives the desired bound on the number of iterations.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n7 Hard instance for rejection sampling\\r\\nIn this section, we give a simple hard instance of a fractionally log-concave distribution, which\\r\\ndemonstrates that the dependence on k in Theorem 33 may be inherent to our rejection sampling\\r\\nstrategy. In particular, it is natural to hope that we can improve Theorem 33 to obtain a parallel\\r\\n          \\xe2\\x88\\x9a                                  1\\r\\ndepth of k \\xc2\\xb7 polylog(k), as opposed to k 2 +c for a constant c. Here, we give an example which\\r\\nsuggests that new algorithmic techniques may be necessary to obtain this improvement.\\r\\n\\r\\nOur hard distribution \\xc2\\xb5 : ([nk ]) \\xe2\\x86\\x92 R \\xe2\\x89\\xa50 will be defined as follows. Let n and k be even, and\\r\\nconsider a partition of the ground set [n] into pairs Si := (2i \\xe2\\x88\\x92 1, 2i ) for all i \\xe2\\x88\\x88 [ n2 ]. Then, the\\r\\ndistribution \\xc2\\xb5 is uniformly supported on sets of the form\\r\\n                                                         \\x12\\x02 n \\x03\\x13\\r\\n                                         [\\r\\n                                  S :=           Si , where S\\xe2\\x80\\xb2 \\xe2\\x88\\x88   2\\r\\n                                                                   k\\r\\n                                                                       .                            (7)\\r\\n                                         i\\xe2\\x88\\x88S \\xe2\\x80\\xb2                     2\\r\\n\\r\\nIn other words, \\xc2\\xb5 randomly chooses 2k indices between 1 and n2 , and takes the k elements formed\\r\\nby including the pairs corresponding to those indices. It is known that \\xc2\\xb5 is \\xe2\\x84\\xa6(1)-FLC (see\\r\\n[Ana+21a]). To simplify notation, we will assume that k = o(n) and \\xe2\\x84\\x93 = o(k). We will also\\r\\nassume there is a constant B such that we have access to n B parallel machines. Following the\\r\\nguarantees of Algorithm 3 in Proposition 24, if we are willing to tolerate a total variation distance\\r\\nof \\xce\\xb4 from \\xc2\\xb5\\xe2\\x84\\x93 , we need to show that with probability at least 1 \\xe2\\x88\\x92 \\xce\\xb4, S \\xe2\\x88\\xbc \\xc2\\xb5\\xe2\\x84\\x93 satisfies\\r\\n\\r\\n                                                  \\xc2\\xb5\\xe2\\x84\\x93 (S)\\r\\n                                                          \\xe2\\x89\\xa4 nB.                                     (8)\\r\\n                                                  \\xc2\\xb5\\xe2\\x80\\xb2\\xe2\\x84\\x93 (S)\\r\\n\\r\\nHere and throughout the following discussion, \\xc2\\xb5\\xe2\\x80\\xb2\\xe2\\x84\\x93 (S) = n\\xe2\\x84\\x93!\\xe2\\x84\\x93 is the probability S is formed by \\xe2\\x84\\x93\\r\\nindependent draws from the uniform distribution on [n]. In particular, clearly the 1-marginal\\r\\ndistribution of \\xc2\\xb5 is uniform, so this is the proposal distribution used by rejection sampling.\\r\\nOur argument on the tightness of our rejection sampling proceeds as follows. Say that a set\\r\\nS \\xe2\\x88\\x88 ([n\\xe2\\x84\\x93 ]) has t \\xe2\\x80\\x9cduplicates\\xe2\\x80\\x9d if amongst the elements of S, there are exactly t pairs of elements\\r\\nbelonging to the same Si . For example, for \\xe2\\x84\\x93 = 4 we say the set {1, 2, 3, 5} contains 1 duplicate,\\r\\nthe pair (1, 2). We first show that for a set S to satisfy (8), it cannot contain more than t = O( B)\\r\\nduplicates. We then show that this limitation, along with attaining a failure probability \\xce\\xb4 inverse-\\r\\n                                            1\\r\\npolynomial in k, forces us to choose \\xe2\\x84\\x93 = k 2 \\xe2\\x88\\x92c for a constant c > 0 which may depend on B.\\r\\n\\r\\nHow many duplicates can we afford? Suppose S \\xe2\\x88\\x88 ([n\\xe2\\x84\\x93 ]) contains t duplicates. Each permuta-\\r\\ntion of S is equally likely to be observed by either of the following processes starting from T0 = \\xe2\\x88\\x85\\r\\n(we use Ti to denote an ordered set, and Si to denote its unordered counterpart, for all i \\xe2\\x88\\x88 [\\xe2\\x84\\x93]).\\r\\n   1. For i \\xe2\\x88\\x88 [\\xe2\\x84\\x93], draw j \\xe2\\x88\\x88 [n] uniformly at random and add it to Ti\\xe2\\x88\\x921 to form Ti .\\r\\n   2. For i \\xe2\\x88\\x88 [\\xe2\\x84\\x93], draw j \\xe2\\x88\\x88 [n] according to the marginal distribution of \\xc2\\xb5\\xe2\\x84\\x93 conditioned on\\r\\n      including Ti\\xe2\\x88\\x921 and add it to Ti\\xe2\\x88\\x921 to form Ti .\\r\\n\\r\\n\\r\\n                                                       29\\r\\n\\x0c\\n\\n                  \\xc2\\xb5 (S)\\r\\nHence, to bound \\xc2\\xb5\\xe2\\x80\\xb2\\xe2\\x84\\x93 (S) as needed by (8), it suffices to fix a permutation T\\xe2\\x84\\x93 of S\\xe2\\x84\\x93 = S and bound\\r\\n                    \\xe2\\x84\\x93\\r\\nthe ratios of the probabilities T\\xe2\\x84\\x93 is observed according to each of the above processes. Clearly it\\r\\nis observed with probability n\\xe2\\x88\\x92\\xe2\\x84\\x93 according to the first process above, so satisfying (8) means the\\r\\nprobability T\\xe2\\x84\\x93 is observed by the second must be at most n B\\xe2\\x88\\x92\\xe2\\x84\\x93 .\\r\\nIt is straightforward to see that the probability\\r\\n                                          \\x01       we observe each second element in a duplicate\\r\\n                                        1\\r\\npair in the relevant round i \\xe2\\x88\\x88 [\\xe2\\x84\\x93] is \\xce\\x98 k . On the other hand, the probability of observing each\\r\\n                              \\x01\\r\\nsingleton in its round is \\xce\\x98 n1 . For k = o(n), this shows that to meet (8) we must have\\r\\n                                         \\x12 \\x12 \\x13\\x13\\xe2\\x84\\x93\\xe2\\x88\\x92t \\x12 \\x12 \\x13\\x13t\\r\\n                                            1         1\\r\\n                                          \\xce\\x98         \\xce\\x98      \\xe2\\x89\\xa4 n B\\xe2\\x88\\x92\\xe2\\x84\\x93 .\\r\\n                                            n         k\\r\\n\\r\\nThis shows that we must have t at most a constant (depending on B).\\r\\n\\r\\nProbability of t duplicates. Let t be a constant. Recall that the distribution \\xc2\\xb5 is uniform over all\\r\\nsets of the form (7), and a sample from \\xc2\\xb5\\xe2\\x84\\x93 = Dk\\xe2\\x86\\x92\\xe2\\x84\\x93 \\xc2\\xb5 is formed by sampling a set S \\xe2\\x88\\xbc \\xc2\\xb5 and then\\r\\nrandomly selecting one of the (k\\xe2\\x84\\x93) subsets of S. Hence, it suffices to fix some S of the form (7),\\r\\nand bound the probability that this downsampling process results in a subset with t duplicates.\\r\\nBy symmetry of \\xc2\\xb5, we lose no generality by only considering the set S = \\xe2\\x88\\xaai\\xe2\\x88\\x88[ k ] Si .\\r\\n                                                                                      2\\r\\n\\r\\nNow, for a constant t, the number of subsets S of size \\xe2\\x84\\x93 with exactly t duplicates is\\r\\n                                                \\x12k\\x13 \\x12 k     \\x13\\r\\n                                                 2 \\xc2\\xb7  2 \\xe2\\x88\\x92t    \\xc2\\xb7 2\\xe2\\x84\\x93\\xe2\\x88\\x922t .\\r\\n                                                 t   \\xe2\\x84\\x93 \\xe2\\x88\\x92 2t\\r\\n\\r\\nThe first term corresponds to choosing which t sets Si will be fully included, the second corre-\\r\\nsponds to choosing which sets the remaining \\xe2\\x84\\x93 \\xe2\\x88\\x92 2t elements come from, and the third is because\\r\\nfor each of the non-duplicated sets we have two options. Hence, the probability a draw from \\xc2\\xb5\\xe2\\x84\\x93\\r\\nhas exactly t duplicates for constant t scales as\\r\\n                  k       k\\r\\n                           2 \\xe2\\x88\\x92 t ) \\xc2\\xb7 2\\xe2\\x84\\x93\\xe2\\x88\\x922t\\r\\n                                              \\x12 \\x12 \\x13\\x13 \\xe2\\x88\\x92\\xe2\\x84\\x93              \\x12 \\x12 \\x13\\x13\\xe2\\x84\\x93\\xe2\\x88\\x922t\\r\\n                ( 2t ) \\xc2\\xb7 (\\xe2\\x84\\x93\\xe2\\x88\\x92  2t                 k                 t     k\\r\\n                                             = \\xce\\x98        \\xc2\\xb7 (\\xce\\x98 (k)) \\xc2\\xb7 \\xce\\x98             \\xc2\\xb7 2\\xe2\\x84\\x93\\xe2\\x88\\x922t\\r\\n                              (k\\xe2\\x84\\x93)               \\xe2\\x84\\x93                       \\xe2\\x84\\x93\\r\\n                                              \\x12 \\x12 \\x13\\x132t                \\x12 \\x12 2 \\x13\\x13t\\r\\n                                                 \\xe2\\x84\\x93               t        \\xe2\\x84\\x93\\r\\n                                             = \\xce\\x98       \\xc2\\xb7 (\\xce\\x98 ( k)) = \\xce\\x98           .\\r\\n                                                 k                         k\\r\\n\\r\\nIn other words, to guarantee that a draw from \\xc2\\xb5\\xe2\\x84\\x93 contains less than t duplicates with probability\\r\\nat least 1 \\xe2\\x88\\x92 \\xce\\xb4, we need to ensure that\\r\\n                                     \\x12       \\x12 2 \\x13\\x13t              \\x10\\xe2\\x88\\x9a 1 \\x11\\r\\n                                              \\xe2\\x84\\x93\\r\\n                                         \\xce\\x98           \\xe2\\x89\\xa4 \\xce\\xb4 =\\xe2\\x87\\x92 \\xe2\\x84\\x93 = O   k\\xce\\xb4 2t .\\r\\n                                               k\\r\\n\\r\\nFor \\xce\\xb4 scaling inverse-polynomially in k (which is necessary to perform a union bound over the\\r\\n                                                                         1\\r\\npoly(k) iterations of rejection sampling), this shows we must take \\xe2\\x84\\x93 \\xe2\\x89\\xa4 k 2 \\xe2\\x88\\x92c for some constant c\\r\\nwhich depends on our budget constant B from the earlier discussion.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                          30\\r\\n\\x0c\\n\\nReferences\\r\\n[AD20]    Nima Anari and Micha\\xc5\\x82 Derezin\\xcc\\x81ski. \\xe2\\x80\\x9cIsotropy and Log-Concave Polynomials: Accel-\\r\\n          erated Sampling and High-Precision Counting of Matroid Bases\\xe2\\x80\\x9d. In: Proceedings of\\r\\n          the 61st Annual Symposium on Foundations of Computer Science. 2020.\\r\\n[AL20]    Vedat Levi Alev and Lap Chi Lau. \\xe2\\x80\\x9cImproved analysis of higher order random walks\\r\\n          and applications\\xe2\\x80\\x9d. In: Proceedings of the 52nd Annual ACM SIGACT Symposium on\\r\\n          Theory of Computing. 2020, pp. 1198\\xe2\\x80\\x931211.\\r\\n[Ald90]   David J Aldous. \\xe2\\x80\\x9cThe random walk construction of uniform spanning trees and\\r\\n          uniform labelled trees\\xe2\\x80\\x9d. In: SIAM Journal on Discrete Mathematics 3.4 (1990), pp. 450\\xe2\\x80\\x93\\r\\n          465.\\r\\n[Ali+21]  Yeganeh Alimohammadi, Nima Anari, Kirankumar Shiragur, and Thuy-Duong Vuong.\\r\\n          \\xe2\\x80\\x9cFractionally Log-Concave and Sector-Stable Polynomials: Counting Planar Match-\\r\\n          ings and More\\xe2\\x80\\x9d. In: arXiv preprint arXiv:2102.02708 (2021).\\r\\n[ALO20]   Nima Anari, Kuikui Liu, and Shayan Oveis Gharan. \\xe2\\x80\\x9cSpectral Independence in High-\\r\\n          Dimensional Expanders and Applications to the Hardcore Model\\xe2\\x80\\x9d. In: Proceedings of\\r\\n          the 61st IEEE Annual Symposium on Foundations of Computer Science. IEEE Computer\\r\\n          Society, 2020.\\r\\n[Ana+19]  Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant. \\xe2\\x80\\x9cLog-concave\\r\\n          polynomials II: high-dimensional walks and an FPRAS for counting bases of a ma-\\r\\n          troid\\xe2\\x80\\x9d. In: Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Com-\\r\\n          puting. 2019, pp. 1\\xe2\\x80\\x9312.\\r\\n[Ana+20]  Nima Anari, Nathan Hu, Amin Saberi, and Aaron Schild. \\xe2\\x80\\x9cSampling Arborescences\\r\\n          in Parallel\\xe2\\x80\\x9d. In: arXiv preprint arXiv:2012.09502 (2020).\\r\\n[Ana+21a] Nima Anari, Michal Derezinski, Thuy-Duong Vuong, and Elizabeth Yang. \\xe2\\x80\\x9cDomain\\r\\n          Sparsification of Discrete Distributions using Entropic Independence\\xe2\\x80\\x9d. In: CoRR\\r\\n          abs/2109.06442 (2021).\\r\\n[Ana+21b] Nima Anari, Vishesh Jain, Frederic Koehler, Huy Tuan Pham, and Thuy-Duong\\r\\n          Vuong. \\xe2\\x80\\x9cEntropic Independence I: Modified Log-Sobolev Inequalities for Fraction-\\r\\n          ally Log-Concave Distributions and High-Temperature Ising Models\\xe2\\x80\\x9d. In: CoRR abs/2106.04105\\r\\n          (2021). arXiv: 2106.04105.\\r\\n[Ana+21c] Nima Anari, Vishesh Jain, Frederic Koehler, Huy Tuan Pham, and Thuy-Duong\\r\\n          Vuong. \\xe2\\x80\\x9cEntropic Independence II: Optimal Sampling and Concentration via Re-\\r\\n          stricted Modified Log-Sobolev Inequalities\\xe2\\x80\\x9d. In: arXiv preprint arXiv:2111.03247 (2021).\\r\\n[AOR16]   Nima Anari, Shayan Oveis Gharan, and Alireza Rezaei. \\xe2\\x80\\x9cMonte Carlo Markov chain\\r\\n          algorithms for sampling strongly Rayleigh distributions and determinantal point\\r\\n          processes\\xe2\\x80\\x9d. In: Conference on Learning Theory. PMLR. 2016, pp. 103\\xe2\\x80\\x93115.\\r\\n[Bar18]   Alexander Barvinok. \\xe2\\x80\\x9cApproximating real-rooted and stable polynomials, with com-\\r\\n          binatorial applications\\xe2\\x80\\x9d. In: arXiv preprint arXiv:1806.07404 (2018).\\r\\n[BBL09]   Julius Borcea, Petter Br\\xc3\\xa4nd\\xc3\\xa9n, and Thomas Liggett. \\xe2\\x80\\x9cNegative dependence and the\\r\\n          geometry of polynomials\\xe2\\x80\\x9d. In: Journal of the American Mathematical Society 22.2 (2009),\\r\\n          pp. 521\\xe2\\x80\\x93567.\\r\\n[Ber84]   Stuart J. Berkowitz. \\xe2\\x80\\x9cOn computing the determinant in small parallel time using a\\r\\n          small number of processors\\xe2\\x80\\x9d. In: Information Processing Letters 18.3 (1984), pp. 147\\xe2\\x80\\x93\\r\\n          150.\\r\\n[Bro89]   Andrei Z Broder. \\xe2\\x80\\x9cGenerating random spanning trees\\xe2\\x80\\x9d. In: FOCS. Vol. 89. Citeseer.\\r\\n          1989, pp. 442\\xe2\\x80\\x93447.\\r\\n\\r\\n\\r\\n\\r\\n                                            31\\r\\n\\x0c\\n\\n[Bru18]    Victor-Emmanuel Brunel. \\xe2\\x80\\x9cLearning Signed Determinantal Point Processes through\\r\\n           the Principal Minor Assignment Problem\\xe2\\x80\\x9d. In: Advances in Neural Information Pro-\\r\\n           cessing Systems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-\\r\\n           Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc., 2018, pp. 7365\\xe2\\x80\\x937374.\\r\\n[Cel+16]   L Elisa Celis, Amit Deshpande, Tarun Kathuria, Damian Straszak, and Nisheeth K\\r\\n           Vishnoi. \\xe2\\x80\\x9cOn the complexity of constrained determinantal point processes\\xe2\\x80\\x9d. In: arXiv\\r\\n           preprint arXiv:1608.00554 (2016).\\r\\n[Cel+17]   L. Elisa Celis, Amit Deshpande, Tarun Kathuria, Damian Straszak, and Nisheeth\\r\\n           K. Vishnoi. \\xe2\\x80\\x9cOn the Complexity of Constrained Determinantal Point Processes\\xe2\\x80\\x9d. In:\\r\\n           APPROX-RANDOM. 2017.\\r\\n[CLV21]    Zongchen Chen, Kuikui Liu, and Eric Vigoda. \\xe2\\x80\\x9cSpectral independence via stability\\r\\n           and applications to holant-type problems\\xe2\\x80\\x9d. In: arXiv preprint arXiv:2106.03366 (2021).\\r\\n[Csa75]    Laszlo Csanky. \\xe2\\x80\\x9cFast parallel matrix inversion algorithms\\xe2\\x80\\x9d. In: 16th Annual Sympo-\\r\\n           sium on Foundations of Computer Science (sfcs 1975). IEEE. 1975, pp. 11\\xe2\\x80\\x9312.\\r\\n[DM21]     Micha\\xc5\\x82 Derezinski and Michael W Mahoney. \\xe2\\x80\\x9cDeterminantal point processes in ran-\\r\\n           domized numerical linear algebra\\xe2\\x80\\x9d. In: Notices of the American Mathematical Society\\r\\n           68.1 (2021), pp. 34\\xe2\\x80\\x9345.\\r\\n[Elf+19]   Mohamed Elfeki, Camille Couprie, Morgane Riviere, and Mohamed Elhoseiny. \\xe2\\x80\\x9cGDPP:\\r\\n           Learning diverse generations using determinantal point processes\\xe2\\x80\\x9d. In: International\\r\\n           Conference on Machine Learning. PMLR. 2019, pp. 1774\\xe2\\x80\\x931783.\\r\\n[Fan89]    Li Fang. \\xe2\\x80\\x9cOn the spectra of P- and P0-matrices\\xe2\\x80\\x9d. In: Linear Algebra and its Applications\\r\\n           119 (1989), pp. 1\\xe2\\x80\\x9325. issn: 0024-3795. doi: https://doi.org/10.1016/0024-3795(89)90065-7.\\r\\n[FHY21]    Weiming Feng, Thomas P Hayes, and Yitong Yin. \\xe2\\x80\\x9cDistributed metropolis sampler\\r\\n           with optimal parallelism\\xe2\\x80\\x9d. In: Proceedings of the 2021 ACM-SIAM Symposium on Dis-\\r\\n           crete Algorithms (SODA). SIAM. 2021, pp. 2121\\xe2\\x80\\x932140.\\r\\n[Gar+19]   Mike Gartrell, Victor-Emmanuel Brunel, Elvis Dohmatob, and Syrine Krichene. \\xe2\\x80\\x9cLearn-\\r\\n           ing Nonsymmetric Determinantal Point Processes\\xe2\\x80\\x9d. In: ArXiv abs/1905.12962 (2019).\\r\\n[Gar+20]   Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, and Victor-Emmanuel\\r\\n           Brunel. Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Pro-\\r\\n           cesses. 2020. arXiv: 2006.09862 [cs.LG].\\r\\n[Gon+14]   Boqing Gong, Wei-lun Chao, Kristen Grauman, and Fei Sha. Large-Margin Determi-\\r\\n           nantal Point Processes. 2014. arXiv: 1411.1537 [stat.ML].\\r\\n[GPK16]    Mike Gartrell, Ulrich Paquet, and Noam Koenigstein. \\xe2\\x80\\x9cBayesian Low-Rank Determi-\\r\\n           nantal Point Processes\\xe2\\x80\\x9d. In: Proceedings of the 10th ACM Conference on Recommender\\r\\n           Systems. RecSys \\xe2\\x80\\x9916. Boston, Massachusetts, USA: Association for Computing Ma-\\r\\n           chinery, 2016, pp. 349\\xe2\\x80\\x93356. isbn: 9781450340359. doi: 10.1145/2959100.2959178.\\r\\n[HS19]     Jonathan Hermon and Justin Salez. \\xe2\\x80\\x9cModified log-Sobolev inequalities for strong-\\r\\n           Rayleigh measures\\xe2\\x80\\x9d. In: arXiv preprint arXiv:1902.02775 (2019).\\r\\n[JVV86]    Mark R Jerrum, Leslie G Valiant, and Vijay V Vazirani. \\xe2\\x80\\x9cRandom generation of com-\\r\\n           binatorial structures from a uniform distribution\\xe2\\x80\\x9d. In: Theoretical computer science 43\\r\\n           (1986), pp. 169\\xe2\\x80\\x93188.\\r\\n[KT12a]    Alex Kulesza and Ben Taskar. \\xe2\\x80\\x9cDeterminantal Point Processes for Machine Learn-\\r\\n           ing\\xe2\\x80\\x9d. In: Found. Trends Mach. Learn. 5.2-3 (2012), pp. 123\\xe2\\x80\\x93286.\\r\\n[KT12b]    Alex Kulesza and Ben Taskar. k-DPPs: Fixed-Size Determinantal Point Processes. 2012.\\r\\n[LB12]     Hui Lin and Jeff Bilmes. \\xe2\\x80\\x9cLearning Mixtures of Submodular Shells with Application\\r\\n           to Document Summarization\\xe2\\x80\\x9d. In: Uncertainty in Artificial Intelligence - Proceedings of\\r\\n           the 28th Conference, UAI 2012 (Oct. 2012).\\r\\n\\r\\n\\r\\n\\r\\n                                            32\\r\\n\\x0c\\n\\n[LJS16]    Chengtao Li, Stefanie Jegelka, and Suvrit Sra. \\xe2\\x80\\x9cFast DPP Sampling for Nystr\\xc3\\xb6m with\\r\\n           Application to Kernel Methods\\xe2\\x80\\x9d. In: CoRR abs/1603.06052 (2016). arXiv: 1603.06052.\\r\\n[LY21]     Hongyang Liu and Yitong Yin. \\xe2\\x80\\x9cSimple Parallel Algorithms for Single-Site Dynam-\\r\\n           ics\\xe2\\x80\\x9d. In: arXiv preprint arXiv:2111.04044 (2021).\\r\\n[MS15]     Zelda Mariet and Suvrit Sra. \\xe2\\x80\\x9cFixed-point algorithms for determinantal point pro-\\r\\n           cesses\\xe2\\x80\\x9d. In: CoRR, abs/1508.00792 (2015).\\r\\n[PP13]     Robin Pemantle and Yuval Peres. Concentration of Lipschitz functionals of determinantal\\r\\n           and other strong Rayleigh measures. 2013. arXiv: 1108.0687 [math.PR].\\r\\n[PR17]     Viresh Patel and Guus Regts. \\xe2\\x80\\x9cDeterministic polynomial-time approximation algo-\\r\\n           rithms for partition functions and graph polynomials\\xe2\\x80\\x9d. In: SIAM Journal on Comput-\\r\\n           ing 46.6 (2017), pp. 1893\\xe2\\x80\\x931919.\\r\\n[Ten95]    Shang-Hua Teng. \\xe2\\x80\\x9cIndependent sets versus perfect matchings\\xe2\\x80\\x9d. In: Theoretical Com-\\r\\n           puter Science 145.1-2 (1995), pp. 381\\xe2\\x80\\x93390.\\r\\n[Wil+18]   Mark Wilhelm, Ajith Ramanathan, Alexander Bonomo, Sagar Jain, Ed H. Chi, and\\r\\n           Jennifer Gillenwater. \\xe2\\x80\\x9cPractical Diversified Recommendations on YouTube with De-\\r\\n           terminantal Point Processes\\xe2\\x80\\x9d. In: Proceedings of the 27th ACM International Conference\\r\\n           on Information and Knowledge Management. CIKM \\xe2\\x80\\x9918. Torino, Italy: Association for\\r\\n           Computing Machinery, 2018, pp. 2165\\xe2\\x80\\x932173. isbn: 9781450360142. doi: 10.1145/3269206.3272018.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                           33\\r\\n\\x0c', b'                                                                                                                                                ACL\\xe2\\x80\\x992022 findings\\r\\n\\r\\n\\r\\n                                         Relevant CommonSense Subgraphs for \"What if...\" Procedural Reasoning\\r\\n\\r\\n\\r\\n                                                              Chen Zheng                                   Parisa Kordjamshidi\\r\\n                                                         Michigan State University                       Michigan State University\\r\\n                                                          zhengc12@msu.edu                                kordjams@msu.edu\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                              Abstract                                   Procedural Text:\\r\\n                                                                                                         1. A plant produces a seed.\\r\\n                                                                                                         2. The seed falls to the ground.\\r\\n                                             We study the challenge of learning causal rea-              3. The seed is buried.\\r\\n                                             soning over procedural text to answer \"What                 4. The seed germinates.\\r\\n                                                                                                         5. A plant grows.\\r\\narXiv:2203.11187v1 [cs.CL] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             if...\" questions when external commonsense                  6. The plant produces flowers.\\r\\n                                                                                                         7. The flowers produce more seeds\\r\\n                                             knowledge is required. We propose a novel\\r\\n                                                                                                         Questions and Answers:\\r\\n                                             multi-hop graph reasoning model to 1) effi-                 1. suppose plants will produce more seeds\\r\\n                                             ciently extract a commonsense subgraph with                 happens, how will it affect less plants.\\r\\n                                                                                                         (A) More (B) Less (C) No effect\\r\\n                                             the most relevant information from a large                  2. suppose the soil is rich in nutrients happens,\\r\\n                                                                                                         how will it affect more seeds are produced.\\r\\n                                             knowledge graph; 2) predict the causal answer               (A) More (B) Less (C) No effect\\r\\n                                             by reasoning over the representations obtained              3. suppose The sun comes out happens, how\\r\\n                                                                                                         will it affect less plants.\\r\\n                                             from the commonsense subgraph and the con-                  (A) More (B) Less (C) No effect\\r\\n                                             textual interactions between the questions and\\r\\n                                             context. We evaluate our model on WIQA               Figure 1: WIQA contains procedural text, and different\\r\\n                                             benchmark and achieve state-of-the-art perfor-       types of questions. The bold choices are the answers.\\r\\n                                             mance compared to the recent models.\\r\\n                                                                                                  relatedto, soil) and (soil, relatedto, seed) derived\\r\\n                                         1   Introduction                                         from ConceptNet, we can build an explicit reason-\\r\\n                                         In recent years, large-scale pre-trained language        ing chain and choose an explainable answer.\\r\\n                                         models (LMs) have made a breakthrough progress              Two challenges exist in procedural text reason-\\r\\n                                         and demonstrate a high performance in many NLP           ing and using external KBs. The first challenge is\\r\\n                                         tasks, including procedural text reasoning (Tandon       effectively extracting the most relevant external in-\\r\\n                                         et al., 2019; Rajagopal et al., 2020). There is a        formation and reducing the noise from the KB. The\\r\\n                                         large amount of knowledge that is stored implicitly      second challenge is reasoning over the extracted\\r\\n                                         in language models that help in solving various          knowledge. Several works enhance the QA model\\r\\n                                         NLP tasks (Devlin et al., 2019b). When we rea-           with commonsense knowledge (Lin et al., 2019; Lv\\r\\n                                         son over text, sometimes, the knowledge contained        et al., 2020). However, the noisy knowledge from\\r\\n                                         in a given text is sufficient to predict the answer,     KG will seriously mislead the QA model in pre-\\r\\n                                         as it is shown in the question 1 of Figure 1. This       dicting the answer. Moreover, using KBs is often\\r\\n                                         knowledge is directly encoded and used by LMs            investigated in the tasks that perform QA directly\\r\\n                                         models (Tandon et al., 2019). However, there are         over KB itself, such as CommonsenseQA (Talmor\\r\\n                                         many cases in which the required knowledge is not        et al., 2019), etc. There are less sophisticated tech-\\r\\n                                         included in the procedural text itself. For example,     niques proposed for using external knowledge ex-\\r\\n                                         for the question 2 in Figure 1, the information about    plicitly (i.e. not through training LMs) in reading\\r\\n                                         the \\xe2\\x80\\x9cnutrient\\xe2\\x80\\x9d on the seeds does not exist in the pro-   comprehension for aiding QA over text. REM-\\r\\n                                         cedural text. Therefore, the external commonsense        Net (Huang et al., 2021) is the only work that uses\\r\\n                                         knowledge is required.                                   commonsense for WIQA and uses a memory net-\\r\\n                                            There are several existing resources that contain     work to extract the external triplets to solve the first\\r\\n                                         world knowledge and commonsense. Examples are            challenge. However, this work has no reasoning\\r\\n                                         knowledge graphs (KGs) like ConceptNet (Speer            process over the extracted knowledge and uses a\\r\\n                                         et al., 2017) and ATOMIC (Sap et al., 2019). Look-       simple multi-head attention operator to predict the\\r\\n                                         ing back at the question 2, we observe that through      answer. EIGEN (Madaan et al., 2020) constructs an\\r\\n                                         providing the external knowledge triplets (nutrient,     influence graph to find the chain of reasoning given\\r\\n\\x0c\\n\\n                               A. KG-attention Triplet Selection                                                      B. Multi-hop Reasoning\\r\\n\\r\\n                                  Triplet 1           (2)                        (2)                        (3)                           (4)               (4)\\r\\n          \\x02    \\x06\\x07 \\x0e               Triplet 2\\r\\n              \\x04\\x07\\x0e\\r\\n                         (1)\\r\\n                                          \\xe2\\x80\\xa6\\r\\n                                                   KG Attention           Relevant Triplets                                       Multi-Hop Graph Answer Prediction\\r\\n                                   CLS n\\r\\n                                  Triplet                                                                                             Encoder\\r\\n                                                                                                                          \\x05\\x03\\x02\\x04 \\x01    Relational Graph\\r\\n               (1)                                                         [CLS ;Triplet i]                                    \\x08    Representation\\r\\n                               [CLS ;Triplet 1]                            [CLS ;Triplet j]\\r\\n      Open Information         [CLS ;Triplet 2]                                   \\xe2\\x80\\xa6\\r\\n        Extraction                    \\xe2\\x80\\xa6                                    [CLS ; Triplet x]     Highly Relevant Commonsense\\r\\n                               [CLS ; Triplet n]                                                                                          (4)\\r\\n                                                                                   concat        Subgraph Construction\\r\\n               (1)                                                                                                             Text Interaction Encoder\\r\\n                                     CLS                                                                                           Question + document\\r\\n        Question+                                  concat    Pretrain A            MLP                                             Contextual Interaction\\r\\n                         LM\\r\\n        Document                 Ques token rep                  (I)\\r\\n                         (2)\\r\\n                                 Doc token rep\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 2: MRRG Model is composed of Candidate Triplet Extraction, KG Attention, Commonsense Subgraph\\r\\nConstruction, Text encoder with contextual interaction, Graph Reasoning, and Answer prediction modules.\\r\\n\\r\\nprocedural text. However, EIGEN cannot deal with                                               sentences C = {s1 , . . . , sn }, and a large knowl-\\r\\nthe challenge when the required knowledge is not                                               edge graph KG.\\r\\nin the given document.                                                                            Figure 2 shows the proposed architecture. (1)\\x01\\r\\n   To solve these two challenges, we propose a                                                 We extract the entities from question and context\\r\\nMulti-hop Reasoning network over Relevant Com-                                                 in preprocessing step and use them to retrieve the\\r\\nmonSense SubGraphs (MRRG) for casual reason-                                                   set of candidate triples from the ConceptNet. (2)\\r\\ning over procedural Text. Our motivation is to                                                 We train the KG Attention module to extract the\\r\\neffectively and efficiently extract the most relevant                                          most relevant triplets given the procedure and ques-\\r\\ninformation from a large KG to help procedural                                                 tion and reduce the noisy concepts from candidate\\r\\nreasoning. First, we extract the entities, retrieve re-                                        triplets. (3) We augment the commonsense sub-\\r\\nlated external triplets from KG, and learn to extract                                          graph based on the relevant triplets. (4) We train\\r\\nthe most relevant triplets to a given the procedure                                            a model that uses two components, the common-\\r\\nand question input by a novel KG attention mecha-                                              sense subgraph as a relational graph network and\\r\\nnism. Then, we construct a commonsense subgraph                                                a text encoder including question and document to\\r\\nbased on the extracted KG triplets in a pipeline. We                                           do procedural reasoning. Below, we describe the\\r\\nuse the extracted subgraphs as a part of end-to-end                                            details of each module.\\r\\nQA model to help in filling the knowledge gaps\\r\\n                                                                                               2.2   Candidate Triplet Extraction from KG\\r\\nin the procedure and performing multi-hop reason-\\r\\ning. The final model predicts the causal answer                                                Given the input q and C, we extract the contextual\\r\\nby reasoning over the contextual interaction repre-                                            entities (concepts) by a open Information Extrac-\\r\\nsentations over the question and the document and                                              tion (OpenIE) model (Stanovsky et al., 2018). For\\r\\nlearning graph representations over the KB sub-                                                each extracted entity tin , we retrieve the relational\\r\\ngraphs. We evaluate our MRRG on the \\xe2\\x80\\x9cwhat if\\xe2\\x80\\x9d                                                  triplets t = (tin , r, tout ) from KG, where tout is\\r\\nWIQA benchmark. MRRG model achieves SOTA                                                       the concept taken from ConceptNet and r is a se-\\r\\nand brings significant improvements compared to                                                mantic relation type. We then apply a pre-trained\\r\\nthe existing baselines.                                                                        Language Model, RoBERTa, to obtain the represen-\\r\\n   The contributions of our work are: 1) We train a                                            tation of each triplet: E t = fLM ([tin , r, tout ]) \\xe2\\x88\\x88\\r\\nseparate module that extracts the relevant parts of                                            R3\\xc3\\x97d , where fLM denotes the language model op-\\r\\nthe KB given the procedure and question to avoid                                               eration and the triplets are given as a sequence of\\r\\nthe noisy and inefficient usage of the information in                                          concepts and relations to the LM.\\r\\nlarge KBs. 2) We design an end-to-end model that\\r\\n                                                                                               2.3   KG Attention\\r\\nuses the extracted QA-dependent KB as a subgraph\\r\\nto guide the reasoning over the procedural text                                                The KG attention module is shown in Figure 2-A\\r\\nto answer the questions. 3) Our MRRG achieves                                                  and Figure 3. We concatenate q and C to form Q =\\r\\nSOTA on the WIQA benchmark.                                                                    [[CLS]; q; [SEP ]; C], where [CLS] and [SEP] are\\r\\n                                                                                               special tokens in the LMs tokenizer process (Liu\\r\\n2     Model Description                                                                        et al., 2019). We use RoBERTa to obtain the list of\\r\\n                                                                                               token representations E[CLS] , Eq , and EC . E[CLS]\\r\\n2.1     Problem Formulation and Overview                                                       is the summary representation of the question and\\r\\nFormally, the problem is to predict an answer a                                                paragraph, Eq is the list of the question tokens\\r\\nfrom a set of pre-defined answers given input ques-                                            embeddings, and EC is the list of the paragraph\\r\\ntion q, a document C which is composed of several                                              tokens embeddings output of Roberta.\\r\\n\\x0c\\n\\n   Given triplet E t that is generated based                                                                        messages from its direct neighbors and relational\\r\\non the triplet extraction described in Section                                                                      semantic edges. The (l + 1)-th layer node represen-\\r\\n2.2, we build a context-triplet pair Ezt =                                                                                  (l+1)\\r\\n                                                                                                                    tation hi     is updated based on the neighborhood\\r\\n             t ; E t ; E t ], where E t is the represen-\\r\\n[E[CLS] ; Ein     r       out              in                                                                       node representations hlj from the l-layer multiplied\\r\\ntation of the head entity from text, Eout       t is the rep-                                                                                            (l)          (l)\\r\\n                                                                                                                    by the relational matrices Wr1 , . . . , Wr|R| . The\\r\\nresentation of the tail entity from KG, and Ert is                                                                                      (l+1)\\r\\nthe representation of the relation. Afterwards, we                                                                  representation hi  is computed as follows:\\r\\ncompute context-triplet pair attention and a soft-                                                                    (l+1)\\r\\n                                                                                                                                 X X 1\\r\\n                                                                                                                                                (l) (l)    (l) (l)\\r\\n                                                                                                                     hi     = \\xcf\\x83(           r | Wr hj + W0 hi ),\\r\\nmax layer to output the Context-Triplet pairwise                                                                                      r\\r\\n                                                                                                                                        |N i\\r\\n                                                                                                                                   r\\xe2\\x88\\x88R j\\xe2\\x88\\x88Ni\\r\\nimportance Score CT S. The process is computed\\r\\n                                exp(M LP (E t ))                                                                    where \\xcf\\x83 denotes a non-linear activation function,\\r\\nas follows: CT St = Pm exp(M LPz(E t )) .\\r\\n                               j=1               z                                                                  Nir represents a set that includes neighbor indices\\r\\n   Then we choose the top-k relevant triplets                                                                       of node i under semantic relation r. Finally, we ob-\\r\\nwith the top CT S scores and then use the rele-                                                                     tain the EGs after several hops of message passing.\\r\\nvant triplets to construct the subgraph. For each                                                                   (II) Text Contextual Interaction Encoder: We\\r\\nselected triplet, we obtain the triplet represen-                                                                   have obtained the contextual token represen-\\r\\ntation E 0t = [Ein      0t , E t , E 0t ] \\xe2\\x88\\x88 R3\\xc3\\x97d , where\\r\\n                               r    out                                                                             tations E[CLS] , Eq , and EC in the KG\\r\\n  0t = f ([CT S \\xc2\\xb7 E t ; CT S \\xc2\\xb7 E t ]) and E 0t =\\r\\nEin       in          t      in          t    r        out                                                          attention module that described in Section\\r\\nfout ([CT St \\xc2\\xb7 Eoutt ; CT S \\xc2\\xb7 E t ]). Notice that f\\r\\n                                  t     r                  in                                                       2.3. Followed by Seo et al., we utilize Bi-\\r\\nand fout are MLP layers, [; ] is the concatenation,                                                                 DAF style contextual interaction module to\\r\\nand [\\xc2\\xb7] is the scalar product.                                                                                      feed Eq and EC to Context-to-Question Atten-\\r\\n                                       KG Attention                                             Training Strategy   tion EC\\xe2\\x86\\x92q = sof tmax(sim(EqT , EC ))Eq and\\r\\n      \\x01\\x06\\x05\\x03\\x04\\x07\\x08\\r\\n                    (1)\\r\\n                              Triplet 1\\r\\n                              Triplet 2\\r\\n                                              KG Attention\\r\\n                                                  (2)\\r\\n                                                                  Relevant Triplets\\r\\n                                                                          (2)\\r\\n                                                                                            Answer Prediction       Question-to-Context Attention Eq\\xe2\\x86\\x92C to obtain the\\r\\n        \\x02\\x04\\x08                          \\xe2\\x80\\xa6\\r\\n                               CLS\\r\\n                              Triplet n\\r\\n                                                                                                                    contextual interaction between question and con-\\r\\n        (1)                                                        [CLS ;Triplet i]\\r\\n\\r\\n Open Information\\r\\n   Extraction\\r\\n                          [CLS ;Triplet 1]\\r\\n                          [CLS ;Triplet 2]\\r\\n                                 \\xe2\\x80\\xa6\\r\\n                                                                   [CLS ;Triplet j]\\r\\n                                                                          \\xe2\\x80\\xa6\\r\\n                                                                   [CLS ; Triplet x]\\r\\n                                                                                       concat\\r\\n                                                                                                                    text. Then we use LSTM to obtain the hidden\\r\\n                          [CLS ; Triplet n]\\r\\n         (1)                                                                                                        state representations: Fq\\xe2\\x86\\x92C = LST M (Eq\\xe2\\x86\\x92C ),\\r\\n                                 CLS\\r\\n   Question+\\r\\n   Document\\r\\n                    LM\\r\\n                    (2)\\r\\n                            Ques token rep\\r\\n                            Doc token rep\\r\\n                                                         concat\\r\\n                                                                                                                    and FC\\xe2\\x86\\x92q = LST M (EC\\xe2\\x86\\x92q ).\\r\\n                                                                                                                    2.6    Answer Prediction\\r\\nFigure 3: The architecture of training the KG Attention\\r\\nmodule.                                                                                                             We concatenate E[CLS] , Fq\\xe2\\x86\\x92C , FC\\xe2\\x86\\x92q , and the com-\\r\\n                                                                                                                                                               0\\r\\n                                                                                                                    pact subgraph representation EGs obtained from\\r\\n2.4        Commonsense Subgraph Construction                                                                        attentive pooling, and use it as the final represen-\\r\\n                                                                                                                                                                     0\\r\\nWe construct the subgraph Gs based on the relevant                                                                  tation: F = [E[CLS] ; Fq\\xe2\\x86\\x92C ; FC\\xe2\\x86\\x92q ; EGs ]. Then we\\r\\n                                                                                                                                                \\x01\\r\\n\\r\\n\\r\\ntriplets from KG attention for each question and                                                                    utilize a classifier MLP (F ) to predict the answer.\\r\\nanswer pair. We add more edges to the subgraph                                                                      Our MRRG has two separate training modules used\\r\\nas follows: Two entities in the triplets will have an                                                               in a pipeline for triplet selection and procedural rea-\\r\\nedge if a relation r in the KG exists between them.                                                                 soning.\\r\\nThe assumption is that the augmented common-                                                                         (I) Training KG Attention for Triplet Selection:\\r\\nsense subgraph will contain the reasoning paths.                                                                    Figure 3 and the left block of Figure 2 show the\\r\\nWe use Ein 0t and E 0t for the KG subgraph initial                                                                  same triplet selection model. The architecture of\\r\\n                    out\\r\\nnode representation h(0) which is used in RGCN                                                                      Figure 2.B is taken and 3 extra MLP layers added\\r\\nformulation in Section 2.5.                                                                                         to it for training as shown in Figure 3. The MLP is\\r\\n                                                                                                                    applied on the concatenation of the concatenation\\r\\n2.5        Procedural Reasoning                                                                                     of [E[CLS] ; Eq ; EC ; E10t ; . . . ; Ek0t ] to predict the an-\\r\\nProcedural Reasoning composes of two parts:                                                                         swer. We use the cross-entropy as the loss function\\r\\nMulti-Hop Graph Reasoning and Text Contextual                                                                       to train the model.\\r\\nInteraction Encoder.                                                                                                  (II) Training End-to-End MRRG: After pre-\\r\\n(I) Multi-Hop Graph Reasoning: this is the Graph                                                                    training the KG attention, we keep the learned pa-\\r\\nReasoning part of Figure 2-B. Given the subgraph                                                                    rameters and extract the most relevant concepts\\r\\nGs , we use RGCN (Schlichtkrull et al., 2018) to                                                                    and construct the multi-relational commonsense\\r\\nlearn the representations of the relational graph.                                                                  subgraph Gs . We combine subgraph representa-\\r\\nRGCN learns graph representations by aggregating                                                                    tion and text interaction representation as input\\r\\n\\x0c\\n\\nto train the answer prediction module by cross-         base language model uses the rules as a regulariza-\\r\\nentropy loss.                                           tion term during training to impose the consistency\\r\\n                                                        between the answers of multiple questions.\\r\\n3     Experiments and Results                           RGN (Zheng and Kordjamshidi, 2021) is the re-\\r\\n                                                        cent SOTA baseline that utilizes a gating net-\\r\\nWe implemented our MRRG framework using Py-\\r\\n                                                        work (Zheng et al., 2020) to effectively filter out the\\r\\nTorch 1 . We use a pre-trained RoBERTa (Liu et al.,\\r\\n                                                        key entities and relationships in the given document\\r\\n2019) to encode the contextual information in the\\r\\n                                                        and learns the contextual representations to predict\\r\\ninput. The maximum number of triplets is 50 and\\r\\n                                                        the answer. RGN does not consider the external\\r\\nthe maximum number of nodes in the graph is 100.\\r\\n                                                        knowledge for procedural reasoning challenges.\\r\\nFurther details of hyper-parameters of the graph\\r\\n                                                        REM-Net (Huang et al., 2021) proposes a recur-\\r\\nare shown in Table 3. The maximum number of\\r\\n                                                        sive erasure memory network to find out the causal\\r\\nwords for the paragraph context is 256. For the\\r\\n                                                        evidence. Specifically, REM-Net refines the evi-\\r\\ngraph construction module, we utilize open Infor-\\r\\n                                                        dence by a recursive memory mechanism and then\\r\\nmation Extraction model (Stanovsky et al., 2018)\\r\\n                                                        uses a generative model to predict the causal an-\\r\\nfrom AllenNLP2 to extract the entities. The max-\\r\\n                                                        swer. REM-Net is the only work that uses external\\r\\nimum number of hops for the graph module is 3.\\r\\n                                                        knowledge for WIQA. REM-Net uses the external\\r\\nThe learning rate is 1e \\xe2\\x88\\x92 5. The model is optimized\\r\\n                                                        knowledge by training an attention mechanism that\\r\\nusing Adam optimizer (Kingma and Ba, 2015).\\r\\n                                                        considers the KG triplet representations for finding\\r\\n3.1    Datasets                                         the answer. It does not explicitly select the most\\r\\n                                                        relevant triplets as we do, and the graph reasoning\\r\\nWIQA is a large dataset for \\xe2\\x80\\x9cwhat if\\xe2\\x80\\x9d causal rea-       is not exploited for finding the chain of reasoning.\\r\\nsoning. WIQA contains three types of questions:\\r\\n                                                         Models                                     in-para   out-of-para   no-effect   Test V1 Acc\\r\\n1) the questions can be directly answered based on       Majority                                    45.46      49.47         55.0         30.66\\r\\n                                                         Polarity                                    76.31      53.59         27.0         39.43\\r\\nthe text, called in-paragraph questions. 2) the ques-    Adaboost (Freund and Schapire, 1995)\\r\\n                                                         emphDecomp-Attn (Parikh et al., 2016)\\r\\n                                                                                                     49.41\\r\\n                                                                                                     56.31\\r\\n                                                                                                                36.61\\r\\n                                                                                                                48.56\\r\\n                                                                                                                             48.42\\r\\n                                                                                                                             73.42\\r\\n                                                                                                                                           43.93\\r\\n                                                                                                                                           59.48\\r\\ntions require external knowledge to be answered,         BERT (no para) (Devlin et al., 2019a)\\r\\n                                                         BERT (Tandon et al., 2019)\\r\\n                                                                                                     60.32\\r\\n                                                                                                     79.68\\r\\n                                                                                                                43.74\\r\\n                                                                                                                56.13\\r\\n                                                                                                                             84.18\\r\\n                                                                                                                             89.38\\r\\n                                                                                                                                           62.41\\r\\n                                                                                                                                           73.80\\r\\n                                                         RoBERTa (Tandon et al., 2019)               74.55      61.29        89.47         74.77\\r\\ncalled out-of-paragraph questions, and 3) irrele-        EIGEN (Madaan et al., 2020)                 73.58      64.04        90.84         76.92\\r\\n                                                         REM-Net (Huang et al., 2021)                75.67      67.98        87.65         77.56\\r\\nvant causes and effects, called no-effect questions.     Logic-Guided (Asai and Hajishirzi, 2020)      -           -            -          78.50\\r\\n                                                         RoBERTa+KG-attention Triplet Selection      72.21      64.60        89.13         75.22\\r\\nWIQA contains 29808 training samples, 6894 de-           MRRG (RoBERTa-base)\\r\\n                                                         Human\\r\\n                                                                                                     79.85\\r\\n                                                                                                       -\\r\\n                                                                                                                69.93\\r\\n                                                                                                                   -\\r\\n                                                                                                                             91.02\\r\\n                                                                                                                                -\\r\\n                                                                                                                                           80.06\\r\\n                                                                                                                                           96.33\\r\\nvelopment samples, 3993 test samples (test V1),\\r\\nand 3003 test samples (test V2).                        Table 1: Model Comparisons on WIQA test V1 dataset.\\r\\n\\r\\n3.2    Baseline Description                             3.3      Results\\r\\nWe briefly describe the most recent baselines that      Table 1 and Table 2 show the performance of\\r\\nuse the Transformer-based language model as the         MRRG on the WIQA task compared to other base-\\r\\nbackbone. We separately fine-tune the BERT and          lines on two different test sets V1 and V2. First,\\r\\nRoBERTa as the first two baselines.                     Both tables show that our proposed KG Attention\\r\\nEIGEN (Madaan et al., 2020) is a baseline that          triplet selection model outperforms the RoBERTa\\r\\nbuilds an event influence graph based on a doc-         and has 3.3% improvement on the out-of-para cat-\\r\\nument and leverages LMs to create the chain of          egory. Second, our MRRG achieves SOTA results\\r\\nreasoning to predict the answer. However, EIGEN         compared to all baseline models. MRRG achieves\\r\\ndoes not use any external knowledge to solve the        the SOTA on both in-para, out-of-para, and no-\\r\\nproblem.                                                effect questions in WIQA V1 and V2.\\r\\nLogic-Guided (Asai and Hajishirzi, 2020) is a            Models                                     in-para   out-of-para   no-effect   Test v2 Acc\\r\\n                                                         Random                                      33.33      33.33        33.33         33.33\\r\\nbaseline that combines neural networks and logic         Majority                                    00.00      00.00        100.0         41.80\\r\\n                                                         BERT                                        70.57      58.54        91.08         74.26\\r\\nrules. Specifically, the Logic-Guided model uses         RoBERTa\\r\\n                                                         REM-Net\\r\\n                                                                                                     70.69\\r\\n                                                                                                     70.94\\r\\n                                                                                                                60.20\\r\\n                                                                                                                63.22\\r\\n                                                                                                                             91.11\\r\\n                                                                                                                             91.24\\r\\n                                                                                                                                           75.34\\r\\n                                                                                                                                           76.29\\r\\n                                                         REM-Net (RoBERTa-large)                     76.23      69.13        92.35         80.09\\r\\nlogic rules including symmetry and transitivity          QUARTET (RoBERTa-large)                     74.49      65.65        95.30         82.07\\r\\n                                                         (Rajagopal et al., 2020)\\r\\nrules to augment the training data. Moreover, the        RGN (Zheng and Kordjamshidi, 2021)          75.91       66.15        92.12        79.95\\r\\n                                                         RoBERTa+KG Attention Triplet Selection      70.02       62.30        91.23        75.86\\r\\n                                                         MRRG (RoBERTa-base)                         76.80       67.83        92.28        80.39\\r\\n  1                                                      MRRG (RoBERTa-large)                        78.82       71.10        93.53        82.95\\r\\n    Our code is available at https://github.com/         Human                                         -           -            -          96.30\\r\\nHLR/MRRG.\\r\\n  2\\r\\n    https://demo.allennlp.org/\\r\\nopen-information-extraction.                            Table 2: Model Comparisons on WIQA test V2 dataset.\\r\\n\\x0c\\n\\n                      Question and Document Content                            RoBERTa   +Interaction   Incorporating Triplets        +KG         +Graph\\r\\n                                                                                                                                      Attention\\r\\n Question: suppose more fruit is produced happens,\\r\\n           how will it affect MORE plants?\\r\\n Content: [\\xe2\\x80\\x9cThe seed germinates.\\xe2\\x80\\x9d, \\xe2\\x80\\x9cThe plant grows.\\xe2\\x80\\x9d, \\xe2\\x80\\x9cThe plant flowers.\\xe2\\x80\\x9d,      X           \\xe2\\x88\\x9a          (fruit, createdby, plant)       \\xe2\\x88\\x9a          \\xe2\\x88\\x9a      Model     # hop = 1   # hop = 2   # hop = 3\\r\\n           \\xe2\\x80\\x9cProduces fruit.\\xe2\\x80\\x9d, \\xe2\\x80\\x9cThe fruit releases seeds.\\xe2\\x80\\x9d\\r\\n Gold Answer: More\\r\\n Question: suppose the soil is rich in nutrients happens,                                                                                                  BERT      71.6%       62.5%       59.5%\\r\\n           how will it affect more seeds are produced.                                                  (nutrient, relatedto, soil)\\r\\n Content: [\\xe2\\x80\\x9cA plant produces a seed\\xe2\\x80\\x9d, \\xe2\\x80\\x9cThe seed falls to the ground\\xe2\\x80\\x9d, \\xe2\\x80\\x9cThe        X           X          (soil, relatedto, seed)         \\xe2\\x88\\x9a          \\xe2\\x88\\x9a\\r\\n           seed is buried\\xe2\\x80\\x9d, \\xe2\\x80\\x9cThe seed germinates\\xe2\\x80\\x9d, \\xe2\\x80\\x9cA plant grows\\xe2\\x80\\x9d, \\xe2\\x80\\x9cThe\\r\\n           plant produces flowers\\xe2\\x80\\x9d, \\xe2\\x80\\x9cThe flowers produce more seeds.\\xe2\\x80\\x9d]\\r\\n                                                                                                                                                           RoBERTa   73.5%       63.9%       61.1%\\r\\n Gold Answer: More\\r\\n Question: suppose more land available happens,                                                         (igneous rock, isa, rock)                          EIGEN     78.8%       63.5%       68.3%\\r\\n           how will it affect less igneous rock forming.                                                (land, relatedto, rock)\\r\\n Content: [\\xe2\\x80\\x9cDifferent kinds of rocks melt into magma\\xe2\\x80\\x9d, \\xe2\\x80\\x9cMagma cools in            X           X         (land, relatedto, surface)       X          \\xe2\\x88\\x9a\\r\\n the crust\\xe2\\x80\\x9d, \\xe2\\x80\\x9cMagma goes to the surface and becomes lava\\xe2\\x80\\x9d, \\xe2\\x80\\x9cLava cools\\xe2\\x80\\x9d,                                (surface, relatedto,\\r\\n           \\xe2\\x80\\x9cCooled magma and lava become igneous rock.\\xe2\\x80\\x9d]                                                igneous rock)                                      MRRG      81.0%       72.3%       70.4%\\r\\n Gold Answer: Less\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 4: Left: Case study of the MRRG Framework. \\xe2\\x80\\x9c+interaction\\xe2\\x80\\x9d means adding the contextual interaction\\r\\nmodule. \\xe2\\x80\\x9cKG ATTN\\xe2\\x80\\x9d means adding the KG Attention Triplet Selection module. \\xe2\\x80\\x99X\\xe2\\x80\\x99 indicates the model failed\\r\\nto predict the correct answer and \\xe2\\x80\\x9cX\\xe2\\x80\\x9d means the prediction was successful with the included module. Right:\\r\\nComparing the results over different number of hops.\\r\\n\\r\\n4      Analysis                                                                                              reasoning with multiple hops due to the relational\\r\\n                                                                                                             graph reasoning and the effectiveness of the ex-\\r\\n4.1       Effects of Using External Knowledge                                                                tracted commonsense subgraph. We study some\\r\\nIn the WIQA, all the baseline models achieve sig-                                                            cases to analyze the multi-hop reasoning and the\\r\\nnificantly lower accuracy in the out-of-para than                                                            reasoning chains. In the third case in Figure 4,\\r\\nin-para and no-effect categories. MRRG achieves                                                              the extracted relevant triplets (land, relatedto, sur-\\r\\nSOTA in the out-of-para category because of using                                                            face), (surface, relatedto, igneous rock) construct a\\r\\nthe highly relevant commonsense subgraphs and                                                                two-hop reasoning chain \\xe2\\x80\\x9cland\\xe2\\x86\\x92surface\\xe2\\x86\\x92igneous\\r\\nthe combination of reasoning over text interaction                                                           rock\\xe2\\x80\\x9d that helps MRRG to find the correct answer.\\r\\nand the graph reasoning modules. As is shown in\\r\\ntable 2, the advantage of the MRRG model is re-                                                              4.3          Ablation Study\\r\\nflected on out-of-para questions. MRRG improves                                                              Table 3 shows the ablation study results of MRRG\\r\\n4.61% over REM-Net. Notice that REM-Net is                                                                   using WIQA. Firstly, we remove the commonsense\\r\\nthe only model that utilizes external knowledge on                                                           subgraph and graph network. The accuracy de-\\r\\nWIQA. Figure 4 shows a case in which the \\xe2\\x80\\x9csoil\\xe2\\x80\\x9d                                                              creases 3.4% compared to MRRG. Second, we\\r\\nand \\xe2\\x80\\x9cnutrient\\xe2\\x80\\x9d only appear in the question and do                                                            remove the contextual interaction module and the\\r\\nnot exist in the text. The baseline models fail to                                                           accuracy decreases 1.3%. In an additional exper-\\r\\nanswer this out-of-para question due to missing                                                              iment, we use the KG attention triplet selection\\r\\nexternal knowledge. However, our model predicts                                                              module to directly predict the answer without the\\r\\nthe correct answer by explicitly incorporating the                                                           pipeline of constructing the subgraph and using the\\r\\n(nutrient, relatedto, soil), (soil, relatedto, seed) that                                                    graph reasoning module. We show the result as\\r\\nconnects the critical information between the ques-                                                          KG Attention Triplet Selection in Table 3. The re-\\r\\ntion and document.                                                                                           sult shows that removing the triplet selection mod-\\r\\n            Ablation                       Model                                Dev Acc                      ule decreases the accuracy by 1.8%. In the same\\r\\n            Text only                 RoBERTa-base                              75.51%                       table 3, we report results about the impact of in-\\r\\n            Text only             + contextual interaction                      76.85%\\r\\n            Text only           KG Attention Triplet Selection                  77.39%                       cluding the relation types in the RGCN graph and\\r\\n                                    - semantic relation                         78.31%\\r\\n                                       GNN dim=50                               79.18%                       the influence of changing the dimensionality of the\\r\\n          Text+Graph                  GNN dim=100                               80.30%\\r\\n                                      GNN dim=200                               79.88%\\r\\n                                                                                                             node representations in the model.\\r\\n\\r\\nTable 3: Ablation and hyper-para. choices on WIQA.                                                           5         Conclusion\\r\\n\\xe2\\x80\\x9cGNN dim\\xe2\\x80\\x9d is the dimension of graph representation.\\r\\n                                                                                                             We propose MRRG model for using external knowl-\\r\\n                                                                                                             edge graph in reasoning over procedural text. Our\\r\\n4.2       Relational Reasoning and Multi-Hops                                                                model extracts a relevant subgraph for each ques-\\r\\nBoth in-para and out-of-para question types require                                                          tion from the KG and uses that knowledge subgraph\\r\\nmultiple hops of reasoning to find the answer in                                                             for answering the question. The extracted subgraph\\r\\nthe WIQA. As shown in the right side of Figure 4,                                                            includes the reasoning path for answering the ques-\\r\\nthe MRRG model accuracy improved 2% for 1                                                                    tion and helps multi-hop reasoning to predict an\\r\\nhop, 8% for 2 hops, and 2% for 3 hops compared                                                               explainable answer. We evaluate MRRG on the\\r\\nto EIGEN. MRRG made a sharp improvement in                                                                   WIQA and achieve SOTA performance.\\r\\n\\x0c\\n\\nReferences                                                 Ankur Parikh, Oscar T\\xc3\\xa4ckstr\\xc3\\xb6m, Dipanjan Das, and\\r\\n                                                             Jakob Uszkoreit. 2016. A decomposable attention\\r\\nAkari Asai and Hannaneh Hajishirzi. 2020. Logic-             model for natural language inference. In Proceed-\\r\\n  guided data augmentation and regularization for con-       ings of the 2016 Conference on Empirical Methods\\r\\n  sistent question answering. In Proceedings of the          in Natural Language Processing, pages 2249\\xe2\\x80\\x932255,\\r\\n  58th Annual Meeting of the Association for Compu-          Austin, Texas. Association for Computational Lin-\\r\\n  tational Linguistics, pages 5642\\xe2\\x80\\x935650. Association         guistics.\\r\\n  for Computational Linguistics.\\r\\n                                                           Dheeraj Rajagopal, Niket Tandon, Peter Clark, Bha-\\r\\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina          vana Dalvi, and Eduard Hovy. 2020. What-if I ask\\r\\n   Toutanova. 2019a. Bert: Pre-training of deep bidi-        you to explain: Explaining the effects of perturba-\\r\\n   rectional transformers for language understanding.        tions in procedural text. In Findings of the Associ-\\r\\n   In NAACL-HLT.                                             ation for Computational Linguistics: EMNLP 2020,\\r\\n                                                             pages 3345\\xe2\\x80\\x933355, Online. Association for Computa-\\r\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and                tional Linguistics.\\r\\n   Kristina Toutanova. 2019b. BERT: Pre-training of\\r\\n   deep bidirectional transformers for language under-     Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-\\r\\n   standing. In Proceedings of the 2019 Conference          dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\\r\\n   of the North American Chapter of the Association         Brendan Roof, Noah A Smith, and Yejin Choi. 2019.\\r\\n   for Computational Linguistics: Human Language            Atomic: An atlas of machine commonsense for if-\\r\\n  Technologies, Volume 1 (Long and Short Papers),           then reasoning. In Proceedings of the AAAI Con-\\r\\n   pages 4171\\xe2\\x80\\x934186, Minneapolis, Minnesota. Associ-         ference on Artificial Intelligence, volume 33, pages\\r\\n   ation for Computational Linguistics.                     3027\\xe2\\x80\\x933035.\\r\\nY. Freund and R. Schapire. 1995. A decision-theoretic      Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,\\r\\n   generalization of on-line learning and an application     Rianne Van Den Berg, Ivan Titov, and Max Welling.\\r\\n   to boosting. In EuroCOLT.                                 2018. Modeling relational data with graph convolu-\\r\\n                                                             tional networks. In European semantic web confer-\\r\\nYinya Huang, Meng Fang, Xunlin Zhan, Qingxing Cao,           ence, pages 593\\xe2\\x80\\x93607. Springer.\\r\\n  Xiaodan Liang, and Liang Lin. 2021. Rem-net: Re-\\r\\n  cursive erasure memory network for commonsense           Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\r\\n  evidence refinement. In AAAI.                              Hannaneh Hajishirzi. 2017. Bidirectional attention\\r\\n                                                             flow for machine comprehension. In ICLR.\\r\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\\r\\n  method for stochastic optimization. In ICLR.             Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.\\r\\n                                                             Conceptnet 5.5: An open multilingual graph of gen-\\r\\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xi-            eral knowledge. In Proceedings of the AAAI Confer-\\r\\n   ang Ren. 2019. KagNet: Knowledge-aware graph              ence on Artificial Intelligence, volume 31.\\r\\n   networks for commonsense reasoning. In Proceed-\\r\\n   ings of the 2019 Conference on Empirical Methods        Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer,\\r\\n   in Natural Language Processing and the 9th Inter-         and Ido Dagan. 2018. Supervised open information\\r\\n   national Joint Conference on Natural Language Pro-        extraction. In Proceedings of the 2018 Conference\\r\\n   cessing (EMNLP-IJCNLP), pages 2829\\xe2\\x80\\x932839, Hong             of the North American Chapter of the Association\\r\\n   Kong, China. Association for Computational Lin-           for Computational Linguistics: Human Language\\r\\n   guistics.                                                 Technologies, Volume 1 (Long Papers), pages 885\\xe2\\x80\\x93\\r\\n                                                             895, New Orleans, Louisiana. Association for Com-\\r\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-          putational Linguistics.\\r\\n  dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\r\\n  Luke Zettlemoyer, and Veselin Stoyanov. 2019.            Alon Talmor, Jonathan Herzig, Nicholas Lourie, and\\r\\n  Roberta: A robustly optimized bert pretraining ap-         Jonathan Berant. 2019. CommonsenseQA: A ques-\\r\\n  proach. arXiv preprint arXiv:1907.11692.                   tion answering challenge targeting commonsense\\r\\n                                                             knowledge. In Proceedings of the 2019 Conference\\r\\nShangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang,               of the North American Chapter of the Association\\r\\n  Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang,             for Computational Linguistics: Human Language\\r\\n  Guihong Cao, and Songlin Hu. 2020. Graph-based             Technologies, Volume 1 (Long and Short Papers),\\r\\n  reasoning over heterogeneous external knowledge            pages 4149\\xe2\\x80\\x934158, Minneapolis, Minnesota. Associ-\\r\\n  for commonsense question answering. In Proceed-            ation for Computational Linguistics.\\r\\n  ings of the AAAI Conference on Artificial Intelli-\\r\\n  gence, volume 34, pages 8449\\xe2\\x80\\x938456.                       Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-\\r\\n                                                             ter Clark, and Antoine Bosselut. 2019. WIQA: A\\r\\nAman Madaan, Dheeraj Rajagopal, Yiming Yang, Ab-             dataset for \\xe2\\x80\\x9cwhat if...\\xe2\\x80\\x9d reasoning over procedural\\r\\n hilasha Ravichander, Eduard Hovy, and Shrimai               text. In Proceedings of the 2019 Conference on\\r\\n Prabhumoye. 2020. Eigen: Event influence gen-               Empirical Methods in Natural Language Processing\\r\\n eration using pre-trained language models. arXiv            and the 9th International Joint Conference on Natu-\\r\\n preprint arXiv:2010.11764.                                  ral Language Processing (EMNLP-IJCNLP), pages\\r\\n\\x0c\\n\\n  6076\\xe2\\x80\\x936085, Hong Kong, China. Association for\\r\\n  Computational Linguistics.\\r\\nChen Zheng, Quan Guo, and Parisa Kordjamshidi.\\r\\n  2020. Cross-modality relevance for reasoning on\\r\\n  language and vision. In Proceedings of the 58th An-\\r\\n  nual Meeting of the Association for Computational\\r\\n  Linguistics, pages 7642\\xe2\\x80\\x937651. Association for Com-\\r\\n  putational Linguistics.\\r\\nChen Zheng and Parisa Kordjamshidi. 2021. Rela-\\r\\n  tional gating for \\xe2\\x80\\x9dwhat if\\xe2\\x80\\x9d reasoning. In Proceed-\\r\\n  ings of the Thirtieth International Joint Conference\\r\\n  on Artificial Intelligence, IJCAI-21, pages 4015\\xe2\\x80\\x93\\r\\n  4022. International Joint Conferences on Artificial\\r\\n  Intelligence Organization. Main Track.\\r\\n\\x0c', b'                                                  A discontinuous Galerkin spectral element method for a nonconservative\\r\\n                                                                compressible multicomponent flow model\\r\\n\\r\\n                                                                                     Re\\xcc\\x81mi Abgralla , Pratik Rai,b,c,\\xe2\\x88\\x97, Florent Renacc,\\xe2\\x88\\x97\\r\\n\\r\\n                                                                                     a Institute of Mathematics, University of Zu\\xcc\\x88rich, Switzerland\\r\\n\\r\\n                                                                           b CMAP, E\\xcc\\x81cole Polytechnique, Route de Saclay, 91128 Palaiseau Cedex, France\\r\\n\\r\\n                                                                                c DAAA, ONERA, Universite\\xcc\\x81 Paris Saclay F-92322 Cha\\xcc\\x82tillon, France\\r\\narXiv:2203.11184v1 [math.NA] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                              Abstract\\r\\n                                                      In this work, we propose an accurate, robust (the solution remains in the set of states), and stable discretization of a\\r\\n                                                  nonconservative model for the simulation of compressible multicomponent flows with shocks and material interfaces.\\r\\n                                                  We consider the gamma-based model by Shyue [J. Comput. Phys., 142 (1998), 208\\xe2\\x80\\x93242] where each component\\r\\n                                                  follows a stiffened gas equation of state (EOS). We here extend the framework proposed in Renac [J. Comput. Phys.,\\r\\n                                                  382 (2019), 1\\xe2\\x80\\x9326] and Coquel et al. [J. Comput. Phys. 431 (2021), 110135] for the discretization of hyperbolic\\r\\n                                                  systems, with both fluxes and nonconservative products, to unstructured meshes with curved elements in multiple\\r\\n                                                  space dimensions. The framework relies on a high-order discontinuous Galerkin spectral element method (DGSEM)\\r\\n                                                  using collocation of quadrature and interpolation points as proposed by Gassner [SIAM J. Sci. Comput., 35 (2013)]\\r\\n                                                  in the case of hyperbolic conservation laws. We modify the integrals over discretization elements where we replace\\r\\n                                                  the physical fluxes and nonconservative products by two-point numerical fluctuations. The contributions of this\\r\\n                                                  work are threefold. First, we analyze the semi-discrete DGSEM discretization of general hyperbolic systems with\\r\\n                                                  conservative and nonconservative terms and derive the conditions to obtain a scheme that is high-order accurate, free-\\r\\n                                                  stream preserving, and entropy stable when excluding material interfaces. Second, we design a three-point scheme\\r\\n                                                  with a HLLC solver for the gamma-based model that does not require a root-finding algorithm for the approximation\\r\\n                                                  of the nonconservative products. The scheme is proved to be robust and entropy stable for convex entropies, to\\r\\n                                                  preserve uniform profiles of pressure and velocity across material interfaces (material interface preservation), and to\\r\\n                                                  satisfy a discrete minimum principle on the specific entropy and maximum principles on the parameters of the EOS.\\r\\n                                                  Third, the HLLC solver is applied at interfaces in the DGSEM scheme, while we consider two kinds of fluctuations in\\r\\n                                                  the integrals over discretization elements: the former is entropy conservative (EC), while the latter preserves material\\r\\n                                                  interfaces (CP). Time integration is performed using high-order strong-stability preserving Runge-Kutta schemes.\\r\\n                                                  The fully discrete scheme is shown to preserve material interfaces with CP fluctuations. Under a given condition on\\r\\n                                                  the time step, both EC and CP fluctuations ensure that the cell-averaged solution remains in the set of states; satisfy\\r\\n                                                  a minimum principle on any convex entropy and maximum principles on the EOS parameters. These results allow\\r\\n                                                  to use existing limiters in order to restore positivity, and discrete maximum principles of degrees-of-freedom within\\r\\n                                                  elements. Numerical experiments in one and two space dimensions on flows with discontinuous solutions support\\r\\n                                                  the conclusions of our analysis and highlight stability, robustness and accuracy of the DGSEM scheme with either\\r\\n                                                  CP, or EC fluctuations, while the scheme with CP fluctuations is shown to offer better resolution capabilities.\\r\\n\\r\\n                                              Keywords. Compressible multicomponent flows, Nonconservative hyperbolic systems, Discontinuous Galerkin method,\\r\\n                                           Summation-by-parts, Material interface capturing, Entropy stability, High-order accuracy\\r\\n\\r\\n                                               AMS subject classifications. 65M12, 65M70, 76T10\\r\\n\\r\\n\\r\\n                                           1. Introduction\\r\\n\\r\\n                                               The discussion in this paper focuses on the approximation in multiple space dimensions of a compressible multicomponent\\r\\n                                           flow model in nonconservative form. We consider a gamma-based model [53] for a mixture with a stiffened gas equation of state\\r\\n                                           (EOS) approximating components including both gas and compressible liquids (hereafter referred to as the SG-gamma model).\\r\\n                                           The model is written in quasi-conservative form [1] to preserve velocity and pressure profiles across material interfaces separating\\r\\n\\r\\n\\r\\n                                              \\xe2\\x88\\x97 Corresponding author\\xe2\\x80\\x99s email addresses: pratik.rai@polytechnique.edu (Pratik Rai), florent.renac@onera.fr (Florent Renac)\\r\\n\\x0c\\n\\ndifferent components. The model approximates mixture quantities and presents the main advantage of being independent of the\\r\\nnumber of components. We are here interested in high-order, robust (i.e., preserving the solution in the set of admissible states),\\r\\nand entropy stable simulations of flows with shocks, material interfaces, and complex interactions triggering small scale flow\\r\\nphenomena. Numerical approximation of multicomponent and multiphase flows based on interface capturing methods has been\\r\\nthe subject of numerous works (see, e.g. [36, 51] and references therein).\\r\\n    We discretize the SG-gamma model using the discontinuous Galerkin spectral element method (DGSEM) with Gauss-Lobatto\\r\\nquadrature rules [39]. The DGSEM uses diagonal norm summation-by-parts (SBP) operators [24] and, in the case of hyperbolic\\r\\nconservation laws, falls into the general framework of conservative elementwise flux differencing schemes [21]. In this framework,\\r\\nusing entropy conservative (EC) numerical fluxes from Tadmor [55], semi-discrete EC finite-difference and spectral collocation\\r\\nschemes have been derived in [21, 24]. The framework has been extended to nonconservative hyperbolic systems on Cartesian\\r\\nmeshes in [47, 14] by using EC numerical fluctuations from [9]. In both frameworks, a semi-discrete entropy inequality may be\\r\\nobtained by replacing physical fluxes and nonconservative products with EC numerical fluxes or fluctuations within discretization\\r\\nelements, while using entropy stable ones at interfaces between elements. The design of the latter relies either on adding upwind-\\r\\ntype dissipation [33] to EC numerical fluxes and fluctuations [7, 22, 17, 25, 47, 14], or on designing approximate Riemann solvers\\r\\n[18, 49, 46]. Note that the SBP operators take into account the numerical quadrature when approximating integrals compared to\\r\\nother schemes that require their exact evaluation to achieve entropy stability [34, 30, 31].\\r\\n     Here, we extend the framework proposed in [47] to multidimensional unstructured meshes with curved elements by using\\r\\ntensor multiplication of quadrature rules and function basis [39] that satisfy geometric conservation laws (the so-called metric\\r\\nidentities [38]) at the discrete level. This framework has been recently applied to the approximation with a well-balanced DGSEM\\r\\nof balance laws with geometric source terms on multidimensional high-order meshes in [63]. We here rather focus on specific\\r\\nproperties of discretizations of nonconservative multicomponent flows: preservation of material interfaces, discrete conservation\\r\\nof physical fluxes, maximum principles on purely transported quantities such as the EOS parameters, and entropy stability. The\\r\\nlatter property presents some difficulties due to the form of the entropy associated to SG-gamma model. First, as a model for the\\r\\nmixture, the properties of the individual components, such as mass and void fractions, are not known in general which prevents the\\r\\nevaluation of the entropy variables necessary for the derivation of EC fluctuations from the Castro et al. condition [9]. Then, the\\r\\nentropy is not convex as is often the case in phase transition models [29] which will restrict the entropy stability across shocks not\\r\\ninteracting with material fronts. We here circumvent these difficulties by considering a specific entropy that allows the evaluation\\r\\nof the entropy variables. This entropy satisfies a Gibbs relation and thus defines a complete EOS [42] and is concave with respect\\r\\nto two thermodynamic intensive properties of the mixture, so entropy stability can be ensured when excluding material interfaces.\\r\\nLet us stress that material interfaces do not require the scheme to be entropy stable, but rather a consistent approximation of the\\r\\nenergy equations to ensure pressure equilibrium [1]. We thus also design material interface preserving (CP) fluctuations to preserve\\r\\nat the discrete level pressure and velocity fields across such interfaces.\\r\\n     We then design a HLLC solver [58, 60] for the SG-gamma model that does not require a root-finding algorithm to evaluate\\r\\nthe nonconservative product in contrast to other schemes [19, 11, 56]. We analyze the properties of a three-point scheme using\\r\\nthe HLLC solver and prove that the scheme is robust and entropy stable for convex entropies defining a complete EOS, preserves\\r\\nuniform profiles of pressure and velocity across material interfaces, and satisfies a discrete minimum principle on the specific\\r\\nentropy and maximum principles on the parameters of the EOS. We then apply the HLLC solver at mesh interfaces in the DGSEM\\r\\nscheme and analyze the properties of the fully discrete scheme with an explicit first-order Euler time integration. We derive\\r\\nconditions on the time step so that the cell-averaged solution is a convex combination of degrees-of-freedom (DOFs) and updates\\r\\nof three-point schemes, as a result the scheme inherits the properties of the three-point scheme. In particular, the DGSEM scheme\\r\\nsatisfies a minimum principle on the entropy irrespective of the fluctuations (EC or CP) that are used in the discretization elements.\\r\\nAs a consequence, the DGSEM with CP fluctuations within elements and the HLLC solver at interfaces is able to handle shocks and\\r\\nto preserve material interfaces. Time integration is performed using high-order strong-stability preserving Runge-Kutta schemes\\r\\nthat are convex combinations of explicit Euler schemes, while linear scaling limiters [65, 66] are applied at the end of each stage\\r\\nto impose positivity and maximum principles at all DOFs within discretization elements.\\r\\n    This paper is organized as follows. The SG-gamma model and the entropy pair are described in section 2. We then introduce\\r\\nthe semi-discrete DGSEM scheme on multidimensional and high-order unstructured meshes in section 3. In section 4, we derive\\r\\nCP and EC numerical fluxes for the SG-gamma model, and in section 5, we propose the HLLC approximate Riemann solver\\r\\nand analyze its properties. We recall the main properties of the fully-discrete DGSEM scheme in section 6. A posteriori limiters\\r\\nare described in section 7. The results are assessed by numerical experiments in one and two space dimensions in section 8 and\\r\\nconcluding remarks about this work are given in section 9.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                  2\\r\\n\\x0c\\n\\n2. The SG-gamma model\\r\\n\\r\\n2.1. Governing equations and thermodynamic model\\r\\n     In this work, we consider the gamma-based compressible multicomponent flow model where each component is assumed to\\r\\nbe a stiffened gas [53] and refer to it as the SG-gamma model. The main interest in this model is that the number of unknowns is\\r\\nindependent of the number of components. We are here interested in high-order approximations of the associated Cauchy problem\\r\\nin d space dimensions for flows with n s components:\\r\\n\\r\\n\\r\\n                                                     \\xe2\\x88\\x82t u + \\xe2\\x88\\x87x \\xc2\\xb7 f(u) + c(u)\\xe2\\x88\\x87x u = 0,                                x \\xe2\\x88\\x88 Rd , t > 0,                                 (1a)\\r\\n                                                                                      u(x, 0) = u0 (x), x \\xe2\\x88\\x88 Rd ,                                                     (1b)\\r\\n                                                                \\x10                      \\x11\\r\\nwhere x = (x1 , . . . , xd ) are the spatial coordinates, f(u) = f1 (u), . . . , fd (u) , c(u)\\xe2\\x88\\x87x u = di=1 ci (u)\\xe2\\x88\\x82 xi u, and\\r\\n                                                                                                    P\\r\\n\\r\\n\\r\\n                                                  \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac \\xcf\\x81 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                   \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac \\xcf\\x81v>\\r\\n                                                  \\xef\\xa3\\xab \\xef\\xa3\\xb6                         \\xef\\xa3\\xab                    \\xef\\xa3\\xb6                       \\xef\\xa3\\xab                          \\xef\\xa3\\xb6\\r\\n                                                                                                   \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                     \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                      \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac \\xcf\\x81v \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                  \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac \\xcf\\x81vv + pI \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                                            >\\r\\n                                                    \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                     \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                  \\xef\\xa3\\xb7                       \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                      \\xef\\xa3\\xb7\\r\\n                                                                                                                               \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                           u = \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xcf\\x81E \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 ,          f(u) = \\xef\\xa3\\xac\\xef\\xa3\\xac(\\xcf\\x81E + p)v \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 ,>\\xef\\xa3\\xb7\\r\\n                                                                                                               c(u)\\xe2\\x88\\x87x u = \\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 ,                                 (2)\\r\\n                                                        \\xef\\xa3\\xac \\xef\\xa3\\xb7                         \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                                          \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                  \\xef\\xa3\\xb7\\r\\n                                                          \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac \\xce\\x93 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                          0                                       \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac v \\xc2\\xb7 \\xe2\\x88\\x87x \\xce\\x93 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                        \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                       \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac            \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                           \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                \\xef\\xa3\\xb7\\r\\n                                                                                        \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac            \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                            \\xef\\xa3\\xad\\xce\\xa0\\xef\\xa3\\xb8                           \\xef\\xa3\\xad  0           \\xef\\xa3\\xb8                             \\xef\\xa3\\xadv \\xc2\\xb7 \\xe2\\x88\\x87 \\xce\\xa0\\xef\\xa3\\xb8\\r\\n                                                                                                                                              x\\r\\n\\r\\n\\r\\nrepresent the vector of state variables, the physical fluxes and the nonconservative products1 , respectively. The mixture density,\\r\\nmomentum, total energy, and internal energy are defined as\\r\\n\\r\\n                                         ns                             ns                                                                        ns\\r\\n                                         X                              X                               1                                         X\\r\\n                                   \\xcf\\x81=          \\xce\\xb1i \\xcf\\x81i ,          \\xcf\\x81v =           \\xce\\xb1i \\xcf\\x81i vi ,      \\xcf\\x81E = \\xcf\\x81e + \\xcf\\x81v \\xc2\\xb7 v,                      \\xcf\\x81e =              \\xce\\xb1i \\xcf\\x81i ei ,\\r\\n                                         i=1                             i=1\\r\\n                                                                                                        2                                         i=1\\r\\n\\r\\nwhere \\xcf\\x81i , vi , and ei represent the density, the velocity vector and the specific internal energy of the ith component. The model\\r\\nassumes immiscible phases and thus imposes a saturation condition on the void fractions \\xce\\xb1i :\\r\\n                                                                                        ns\\r\\n                                                                                        X\\r\\n                                                                                               \\xce\\xb1i = 1.                                                                (3)\\r\\n                                                                                         i=1\\r\\n\\r\\n\\r\\n     The partial pressures are related to partial densities and specific internal energies through the stiffened gas EOS:\\r\\n\\r\\n                                                                                                                         p\\xe2\\x88\\x9ei\\r\\n                                      pi (\\xcf\\x81i , ei ) = (\\xce\\xb3i \\xe2\\x88\\x92 1)\\xcf\\x81i ei \\xe2\\x88\\x92 \\xce\\xb3i p\\xe2\\x88\\x9ei ,                    ei = Cvi T i +             ,        i = 1, . . . , n s ,            (4)\\r\\n                                                                                                                          \\xcf\\x81i\\r\\nwhere \\xce\\xb3i = Cp i /Cvi > 1 is the ratio of specific heats, T i is the temperature of the species, and p\\xe2\\x88\\x9ei > 0 is a pressure-like constant.\\r\\nObserve that when p\\xe2\\x88\\x9ei = 0 in (4) we recover the polytropic EOS. Assuming thermal equilibrium of the species, T i (\\xcf\\x81i , ei ) = T (\\xcf\\x81, e),\\r\\n1 6 i 6 n s , the EOS for the mixture is indeed conveniently defined by [53]\\r\\n\\r\\n                                                                                                                             ns\\r\\n                                                    p + \\xce\\xb3p\\xe2\\x88\\x9e                                                                  X\\r\\n                                                            = p\\xce\\x93 + \\xce\\xa0 = \\xcf\\x81e,                           \\xcf\\x81e = \\xcf\\x81Cv T +                   \\xce\\xb1i p\\xe2\\x88\\x9ei ,                          (5)\\r\\n                                                    (\\xce\\xb3 \\xe2\\x88\\x92 1)                                                                   i=1\\r\\n\\r\\nwhere Cv = ni=1   Yi Cvi denotes the specific heat at constant volume of the mixture, the Yi = \\xce\\xb1i\\xcf\\x81\\xcf\\x81i are the mass fractions of the\\r\\n              Ps\\r\\nspecies, and the EOS parameters \\xce\\x93 and \\xce\\xa0 are defined by\\r\\n\\r\\n                                                                   ns                                               ns\\r\\n                                                             1    X      \\xce\\xb1i                                  \\xce\\xb3p\\xe2\\x88\\x9e   X   \\xce\\xb1i \\xce\\xb3i p\\xe2\\x88\\x9ei\\r\\n                                                 \\xce\\x93=             =            ,                    \\xce\\xa0=             =               .                                    (6)\\r\\n                                                            \\xce\\xb3\\xe2\\x88\\x921   i=1\\r\\n                                                                      \\xce\\xb3 i \\xe2\\x88\\x921                                 \\xce\\xb3\\xe2\\x88\\x921   i=1\\r\\n                                                                                                                       \\xce\\xb3i \\xe2\\x88\\x92 1\\r\\n\\r\\n\\r\\n\\r\\n                                                                           i jk = ck (u)i j with the ck (u) in R\\r\\n   1 The components of the third-order tensor in (1a) read c(u)                                                 neq \\xc3\\x97neq , so the ith component of c(u)\\xe2\\x88\\x87u reads\\r\\nPneq Pd\\r\\n j=1 k=1 c(u)i jk \\xe2\\x88\\x82 xk u j . Likewise, for n = (n1 , . . . , nd ) , we have c(u)n = i=1 ni ci (u).\\r\\n                                                                 >                    Pd\\r\\n\\r\\n\\r\\n\\r\\n                                                                                                3\\r\\n\\x0c\\n\\n    Hyperbolicity of the SG-gamma model requires that the solutions to (1) belong to the set of states\\r\\n                                          n                                                 o\\r\\n                                     \\xe2\\x84\\xa6GM = u \\xe2\\x88\\x88 Rneq : \\xcf\\x81 > 0, v \\xe2\\x88\\x88 Rd , \\xcf\\x81e > p\\xe2\\x88\\x9e , \\xce\\x93 > 0, \\xce\\xa0 > 0 ,                                         (7)\\r\\n                                                          \\x10                   \\x11\\r\\nwith neq = d + 4. The matrix-valued function         i=1 ni fi (u) + ci (u)\\r\\n                                                             0\\r\\n                                                 Pd\\r\\n                                                                                  in Rneq \\xc3\\x97neq admits real eigenvalues\\r\\n\\r\\n                               \\xce\\xbb1 (u) = v \\xc2\\xb7 n \\xe2\\x88\\x92 c,    \\xce\\xbb2 (u) = \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 = \\xce\\xbbneq \\xe2\\x88\\x921 (u) = v \\xc2\\xb7 n,          \\xce\\xbbneq (u) = v \\xc2\\xb7 n + c,             (8)\\r\\n\\r\\nfor all unit vector n = (n1 , . . . , nd ), where c = \\xce\\xb3(\\xce\\xb3 \\xe2\\x88\\x92 1)(\\xcf\\x81e \\xe2\\x88\\x92 p\\xe2\\x88\\x9e )/\\xcf\\x81 is the speed of sound of the mixture. Here, {\\xce\\xbbi }26i6neq \\xe2\\x88\\x921 are\\r\\n                                                     p\\r\\n\\r\\nassociated to linearly degenerate (LD) fields, while \\xce\\xbb1 and \\xce\\xbbneq are associated to genuinely nonlinear (GNL) fields. Observe that\\r\\n(1a) is not strictly hyperbolic as the eigenvalues associated to the LD fields are not distinct.\\r\\n\\r\\n\\r\\n2.2. Entropy pair\\r\\n    Solutions to (1) may develop discontinuities and (1a) has to be understood in the sense of distributions where we look for weak\\r\\nsolutions. Weak solutions are not necessarily unique and (1) must be supplemented with further admissibility conditions to select\\r\\nthe physical solution. We here focus on entropy inequalities\\r\\n\\r\\n                                               \\xe2\\x88\\x82t \\xce\\xb7(u) + \\xe2\\x88\\x87x \\xc2\\xb7 q(u) 6 0,                 x \\xe2\\x88\\x88 Rd , t > 0,                                (9)\\r\\n\\r\\nfor some convex entropy \\xe2\\x80\\x93 entropy flux pair (\\xce\\xb7(u), q(u)). One common way to derive such pair consists in considering partial\\r\\nentropies of the species si (\\xcf\\x81i , \\xce\\xb8) = \\xe2\\x88\\x92Cvi ln \\xce\\xb8 + (\\xce\\xb3i \\xe2\\x88\\x92 1) ln \\xcf\\x81i , where \\xce\\xb8 = 1/T is the inverse of the temperature, that satisfies a Gibbs\\r\\n                                                                 \\x01\\r\\nrelation\\r\\n\\r\\n                                                                                   pi\\r\\n                                                              T dsi = dei \\xe2\\x88\\x92            d\\xcf\\x81i ,                                          (10)\\r\\n                                                                                   \\xcf\\x812i\\r\\n\\r\\nso the mixture entropy reads\\r\\n\\r\\n                                               ns\\r\\n                                               X                          ns\\r\\n                                                                          X\\r\\n                                                     Yi si = \\xe2\\x88\\x92Cv ln \\xce\\xb8 \\xe2\\x88\\x92            Yi (\\xce\\xb3i \\xe2\\x88\\x92 1)Cvi ln \\xcf\\x81i .\\r\\n                                               i=1                        i=1\\r\\n\\r\\n    Unfortunately, we cannot use this entropy to derive EC fluxes in section 4 because they would require to evaluate the Yi which\\r\\nis not possible from u in (2). Hence, here we consider an alternative pair\\r\\n\\r\\n                                                         \\xce\\xb7(u) = \\xe2\\x88\\x92\\xcf\\x81s,          q(u) = \\xe2\\x88\\x92\\xcf\\x81sv,                                            (11)\\r\\n\\r\\nwhere, upon introducing \\xcf\\x84 = \\xcf\\x811 the covolume of the mixture, the specific entropy reads\\r\\n\\r\\n                                                          p + p\\xe2\\x88\\x9e (6) \\x12 \\x12       \\xce\\xa0 \\x13 1\\r\\n                                                                !                                  \\x13\\r\\n                               s(\\xcf\\x84, e, Cv , \\xce\\x93, \\xce\\xa0) = Cv ln         = Cv ln e \\xe2\\x88\\x92     \\xcf\\x84 +   ln \\xcf\\x84 \\xe2\\x88\\x92 ln \\xce\\x93  .                                (12)\\r\\n                                                            \\xcf\\x81\\xce\\xb3                \\xce\\x93+1     \\xce\\x93\\r\\n   The rationale for considering this entropy is as follows. First, we will see in section 4.2 that the EC fluctuations can be explicitly\\r\\ncomputed without knowledge of the individual mass fractions. Then, for smooth solutions of (1a), we have\\r\\n\\r\\n                             \\xe2\\x88\\x82t Cv + v \\xc2\\xb7 \\xe2\\x88\\x87x Cv = 0,     \\xe2\\x88\\x82t \\xcf\\x84 + v \\xc2\\xb7 \\xe2\\x88\\x87x \\xcf\\x84 = \\xcf\\x84\\xe2\\x88\\x87x \\xc2\\xb7 v,             \\xe2\\x88\\x82t e + v \\xc2\\xb7 \\xe2\\x88\\x87x e = \\xe2\\x88\\x92\\xcf\\x84p\\xe2\\x88\\x87x \\xc2\\xb7 v,\\r\\n\\r\\nhence\\r\\n\\r\\n                                                                                                                          !\\r\\n                            \\x10               \\x11      \\x10               \\x11                                    1   p\\xe2\\x88\\x9e       p      (5)\\r\\n      \\xe2\\x88\\x82t s + v \\xc2\\xb7 \\xe2\\x88\\x87x s = \\xe2\\x88\\x82\\xcf\\x84 s \\xe2\\x88\\x82t \\xcf\\x84 + v \\xc2\\xb7 \\xe2\\x88\\x87x \\xcf\\x84 + \\xe2\\x88\\x82e s \\xe2\\x88\\x82t e + v \\xc2\\xb7 \\xe2\\x88\\x87x e = \\xcf\\x84\\xe2\\x88\\x87x \\xc2\\xb7 v(\\xe2\\x88\\x82\\xcf\\x84 s \\xe2\\x88\\x92 p\\xe2\\x88\\x82e s) = Cv \\xe2\\x88\\x87x \\xc2\\xb7 v   \\xe2\\x88\\x92      \\xe2\\x88\\x92          = 0,\\r\\n                                                                                                        \\xce\\x93 \\xcf\\x81e \\xe2\\x88\\x92 p\\xe2\\x88\\x9e \\xcf\\x81e \\xe2\\x88\\x92 p\\xe2\\x88\\x9e\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                          4\\r\\n\\x0c\\n\\nso the mixture entropy is conserved, i.e., (9) is an equality. Moreover, (\\xcf\\x84, e) 7\\xe2\\x86\\x92 s(\\xcf\\x84, e, Cv , \\xce\\x93, \\xce\\xa0) is obviously strictly concave in\\r\\n\\xe2\\x84\\xa6GM and from the first equality in the above relation, we conclude that the inequality in (9) makes sense even if \\xce\\xb7(u) is not strictly\\r\\nconvex which is often the case in phase transition models [29]. Finally, differentiating (12) while fixing Cv , \\xce\\x93 and \\xce\\xa0, gives\\r\\n\\r\\n                                                                                        \\xcf\\x81Cv\\r\\n                                                        !                         !                    !\\r\\n                                             dp      d\\xcf\\x81 (5)    (\\xce\\xb3 \\xe2\\x88\\x92 1)d(\\xcf\\x81e)    d\\xcf\\x81                  p\\r\\n                               ds = Cv            \\xe2\\x88\\x92\\xce\\xb3      = Cv              \\xe2\\x88\\x92\\xce\\xb3      =         de \\xe2\\x88\\x92 2 d\\xcf\\x81 ,                        (13)\\r\\n                                           p + p\\xe2\\x88\\x9e     \\xcf\\x81           p + p\\xe2\\x88\\x9e        \\xcf\\x81     \\xcf\\x81e \\xe2\\x88\\x92 p\\xe2\\x88\\x9e     \\xcf\\x81\\r\\n\\r\\nso the entropy satisfies a Gibbs relation similar to (10) but with a different temperature T\\xcc\\x83 = \\xcf\\x81C1 v (\\xcf\\x81e \\xe2\\x88\\x92 p\\xe2\\x88\\x9e ) instead of T in (5).\\r\\nThe entropy hence defines a complete EOS with both pressure and temperature [42]. This latter observation will be important in\\r\\nsection 5 to prove entropy stability of the HLLC solver through the existence of local minimum entropy principles (see proof of\\r\\nLemma 5.1).\\r\\n    We end this section by deriving the entropy variables associated to the entropy \\xce\\xb7(u) in (11) when considering pure phases. For\\r\\npure phases, we have d\\xce\\xb3 = d\\xce\\xa0 = 0 so we obtain\\r\\n\\r\\n                                 (12)        d(p + p\\xe2\\x88\\x9e )                (5)              d\\xcf\\x81e\\r\\n            d\\xce\\xb7 = \\xe2\\x88\\x92\\xcf\\x81ds \\xe2\\x88\\x92 sd\\xcf\\x81 = \\xe2\\x88\\x92\\xcf\\x81Cv                      + \\xce\\xb3Cv d\\xcf\\x81 \\xe2\\x88\\x92 sd\\xcf\\x81 = \\xe2\\x88\\x92\\xcf\\x81Cv (\\xce\\xb3 \\xe2\\x88\\x92 1)        + \\xce\\xb3Cv d\\xcf\\x81 \\xe2\\x88\\x92 sd\\xcf\\x81\\r\\n                                              p + p\\xe2\\x88\\x9e                                  p + p\\xe2\\x88\\x9e\\r\\n                                                                            (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv \\x12                 v\\xc2\\xb7v \\x13\\r\\n                                                                       = \\xe2\\x88\\x92\\xcf\\x81             d\\xcf\\x81E \\xe2\\x88\\x92 v \\xc2\\xb7 d\\xcf\\x81v +     d\\xcf\\x81 + \\xce\\xb3Cv d\\xcf\\x81 \\xe2\\x88\\x92 sd\\xcf\\x81\\r\\n                                                                             p + p\\xe2\\x88\\x9e                      2\\r\\n                                                                       (15)\\r\\n                                                                                              \\x12            v \\xc2\\xb7 v\\x13\\r\\n                                                                        = \\xe2\\x88\\x92\\xce\\xb6d\\xcf\\x81E + \\xce\\xb6v \\xc2\\xb7 d\\xcf\\x81v + \\xce\\xb3Cv \\xe2\\x88\\x92 s \\xe2\\x88\\x92 \\xce\\xb6          d\\xcf\\x81,\\r\\n                                                                                                            2\\r\\n\\r\\nand we thus obtain the following expression of the entropy variables\\r\\n\\r\\n                                                            \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xce\\xb3Cv \\xe2\\x88\\x92 s \\xe2\\x88\\x92 \\xce\\xb6 v\\xc2\\xb7v\\r\\n                                                            \\xef\\xa3\\xab                  \\xef\\xa3\\xb6\\r\\n                                                                               \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                             2 \\xef\\xa3\\xb7\\r\\n                                                                          \\xce\\xb6v\\r\\n                                                              \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac               \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                  \\xe2\\x88\\x82\\r\\n                                                                \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac               \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                         \\xcf\\x91(u) =     \\xce\\xb7(u) = \\xef\\xa3\\xac\\xef\\xa3\\xac             \\xe2\\x88\\x92\\xce\\xb6                       \\xe2\\x88\\x80u \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM ,                    (14)\\r\\n                                                                  \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac               \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                 \\xe2\\x88\\x82u\\r\\n                                                                                      \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                          0\\r\\n                                                                    \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                      \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb8\\r\\n                                                                        \\xef\\xa3\\xad 0\\r\\n\\r\\nwhere\\r\\n                                                                                (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv \\xcf\\x81\\r\\n                                                                           \\xce\\xb6=               .                                    (15)\\r\\n                                                                                  p + p\\xe2\\x88\\x9e\\r\\n\\r\\n\\r\\n3. The discontinuous Galerkin spectral element method (DGSEM)\\r\\n\\r\\n     In this section, we recall the DGSEM framework [14, 39, 47] which is used to discretize the Cauchy problem (1). Here, the\\r\\nspace domain \\xe2\\x84\\xa6 = Rd is discretized using a mesh \\xe2\\x84\\xa6h consisting of nonoverlapping and nonempty cells \\xce\\xba (quadrangles for d = 2\\r\\nand hexahedra for d = 3) forming a partition of \\xe2\\x84\\xa6. By Eh we denote the set of interfaces in \\xe2\\x84\\xa6h . For the sake of clarity, we introduce\\r\\nthe DGSEM in two space dimensions d = 2, as the extension to d = 3 is straightforward while its derivation for d = 1 can be found\\r\\nin [14, 48].\\r\\n\\r\\n\\r\\n3.1. Numerical approximation and function space\\r\\n     Let us consider the reference element I2 = [\\xe2\\x88\\x921, 1]2 with coordinates \\xce\\xbe = (\\xce\\xbe, \\xce\\xb7) and the reference edge I = [\\xe2\\x88\\x921, 1], the functions\\r\\nx\\xce\\xba (\\xce\\xbe) and xe (\\xce\\xbe) map reference to physical element and edge, respectively. The approximate solution of (1) is sought in the function\\r\\nspace of piecewise polynomials\\r\\n                                                      n                                               o\\r\\n                                                 Vhp = \\xcf\\x86 \\xe2\\x88\\x88 L2 (\\xe2\\x84\\xa6h ) : \\xcf\\x86\\xce\\xba \\xe2\\x97\\xa6 x\\xce\\xba (\\xce\\xbe) \\xe2\\x88\\x88 Q p (I2 ), \\xe2\\x88\\x80\\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h ,\\r\\n\\r\\nwhere Q p (I2 ) denotes the space of polynomials over the reference element I2 formed by the tensor product of polynomials of degree\\r\\nat most p in each direction. The approximate solution reads\\r\\n\\r\\n                                                             p\\r\\n                                                             X\\r\\n                                              uh (x, t) :=            \\xcf\\x86i\\xce\\xbaj (x)Ui\\xce\\xbaj (t) \\xe2\\x88\\x80x \\xe2\\x88\\x88 \\xce\\xba,   \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h ,   t > 0,               (16)\\r\\n                                                             i, j=0\\r\\n\\r\\n\\r\\n                                                                                   5\\r\\n\\x0c\\n\\n                                    \\xe2\\x80\\xa2               \\xe2\\x80\\xa2\\r\\n                                                                                \\xe2\\x80\\xa2             e\\r\\n                                                                                              \\xe2\\x80\\xa2\\r\\n                                        \\xe2\\x80\\xa2         xi\\xce\\xbaj \\xe2\\x80\\xa2                                            u+h\\r\\n                                                                                \\xe2\\x80\\xa2               \\xe2\\x80\\xa2\\r\\n                                                                                          u\\xe2\\x88\\x92h\\r\\n                                                               \\xce\\xba=\\xce\\xba          \\xe2\\x88\\x92                                                   \\xce\\xbae+\\r\\n\\r\\n                                            \\xe2\\x80\\xa2              \\xe2\\x80\\xa2                         \\xe2\\x80\\xa2     xke \\xe2\\x80\\xa2\\r\\n                                                                                                                  nke\\r\\n                                                    \\xe2\\x80\\xa2          \\xe2\\x80\\xa2                      \\xe2\\x80\\xa2           \\xe2\\x80\\xa2\\r\\n\\r\\n                                                                                                                                            ij\\r\\nFigure 1: Notations for the mesh in two space dimensions: cell \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h with quadrature points x\\xce\\xba (bullets \\xe2\\x80\\xa2), edge e \\xe2\\x88\\x88 \\xe2\\x88\\x82\\xce\\xba with quadrature point xke ,\\r\\nassociated unit outward normal nke , and traces of the approximate solution u\\xc2\\xb1h on e; adjacent cell \\xce\\xbae+ sharing edge e.\\r\\n\\r\\n\\r\\n\\r\\nwhere {\\xcf\\x86i\\xce\\xbaj }06i, j6p constitutes a basis of Vhp restricted onto \\xce\\xba, with dimension (p + 1)2 , and {Ui\\xce\\xbaj }06i, j6p are the DOFs. Let `06k6p\\r\\ndenote the Lagrange interpolation polynomials associated to the Gauss-Lobatto nodes over I: \\xe2\\x88\\x921 = \\xce\\xbe0 < \\xce\\xbe1 < \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 < \\xce\\xbe p = 1. We\\r\\ndefine the basis functions as the tensor products of these polynomials:\\r\\n\\r\\n                                                        \\xcf\\x86i\\xce\\xbaj (x) = \\xcf\\x86i j (x\\xce\\xba (\\xce\\xbe)) = `i (\\xce\\xbe)` j (\\xce\\xb7),                       0 6 i, j 6 p,                          (17)\\r\\n\\r\\nwhich satisfy the following cardinality relation\\r\\n\\r\\n                                            0 0\\r\\n                                   \\xcf\\x86i\\xce\\xbaj (xi\\xce\\xba j ) = \\xcf\\x86i\\xce\\xbaj (x\\xce\\xba (\\xce\\xbei0 j0 )) = `i (\\xce\\xbei0 )` j (\\xce\\xb7 j0 ) = \\xce\\xb4ii0 \\xce\\xb4 j j0 ,                         0 6 i, i0 , j, j0 6 p,   (18)\\r\\n\\r\\nwhere \\xce\\xb4ii0 is the Kronecker delta. The DOFs are therefore point values of the solution: Ui\\xce\\xbaj (t) = uh (xi\\xce\\xbaj , t). Likewise, the elements\\r\\n\\xce\\xba are interpolated on the same grid of quadrature points as the numerical solution, i.e., x\\xce\\xba (\\xce\\xbe) = 06i, j6p `i (\\xce\\xbe)` j (\\xce\\xb7)xi\\xce\\xbaj and xe (\\xce\\xbe) =\\r\\n                                                                                                   P\\r\\n\\r\\n  k=0 `i (\\xce\\xbe)xe (see Figure 1).\\r\\nPp           k\\r\\n\\r\\n\\r\\n    The integrals over elements and interfaces are approximated by using Gauss-Lobatto quadrature rules where the quadrature\\r\\nand interpolation points are collocated:\\r\\n\\r\\n                                        Z                          p\\r\\n                                                                   X                                      Z                           p\\r\\n                                                                                                                                      X\\r\\n                                                f (x)dV \\xe2\\x89\\x88                   \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j f (xi\\xce\\xbaj ),               f (x)dS \\xe2\\x89\\x88                 \\xcf\\x89k Jek f (xke ),   (19)\\r\\n                                            \\xce\\xba                      i, j=0                                     e                       k=0\\r\\n\\r\\n\\r\\nwhere \\xcf\\x89i , \\xcf\\x89 j > 0 are the quadrature weights and J\\xce\\xbai j = J\\xce\\xba (xi\\xce\\xbaj ) = |\\xe2\\x88\\x87\\xce\\xbe x\\xce\\xba (\\xce\\xbei j )|, and Jek = Je (xke ) = |\\xe2\\x88\\x87\\xce\\xbe xe (\\xce\\xbek )|. The cell-averaged solution\\r\\nthus reads\\r\\n\\r\\n                                                                            p\\r\\n                                                                                              J\\xce\\xbai j i j\\r\\n                                                                                                                        Z\\r\\n                                                                            X                               1\\r\\n                                                        hui\\xce\\xba (t) :=                  \\xcf\\x89i \\xcf\\x89 j        U (t) \\xe2\\x89\\x88                      uh (x, t)dV,                   (20)\\r\\n                                                                            i, j=0\\r\\n                                                                                              |\\xce\\xba| \\xce\\xba        |\\xce\\xba|              \\xce\\xba\\r\\n\\r\\n\\r\\nwhere |\\xce\\xba| =     i, j=0 \\xcf\\x89i \\xcf\\x89 j J\\xce\\xba is the volume of the cell \\xce\\xba.\\r\\n              Pp               ij\\r\\n\\r\\n\\r\\n    We also introduce the discrete difference matrix with entries\\r\\n\\r\\n                                                                            Dik = `k0 (\\xce\\xbei ),            0 6 i, k 6 p,                                          (21)\\r\\n\\r\\n                         l=0 `l \\xe2\\x89\\xa1 1 implies\\r\\n                      Pp\\r\\nwhere the property\\r\\n\\r\\n                                                                             p\\r\\n                                                                             X\\r\\n                                                                                     Dkl = 0          \\xe2\\x88\\x800 6 k 6 p.                                              (22)\\r\\n                                                                             l=0\\r\\n\\r\\n    The discrete difference matrix is known to satisfy the SBP property [39]:\\r\\n\\r\\n                                                        \\xcf\\x89k Dkl + \\xcf\\x89l Dlk = \\xce\\xb4kp \\xce\\xb4lp \\xe2\\x88\\x92 \\xce\\xb4k0 \\xce\\xb4l0                             \\xe2\\x88\\x800 6 k, l 6 p.                         (23)\\r\\n\\r\\n                                                                                                    6\\r\\n\\x0c\\n\\n     Finally, the discretization is assumed to satisfy the following metric identities [38]\\r\\n\\r\\n                                                        p\\r\\n                                                        X\\r\\n                                                              Dik J\\xce\\xbak j \\xe2\\x88\\x87x \\xce\\xbe(xk\\xce\\xba j ) + D jk J\\xce\\xbaik \\xe2\\x88\\x87x \\xce\\xb7(xik\\xce\\xba ) = 0   \\xe2\\x88\\x800 6 i, j 6 p,                                      (24)\\r\\n                                                        k=0\\r\\n\\r\\nand volume and edge metric terms are related by\\r\\n\\r\\n\\r\\n  J\\xce\\xbap j \\xe2\\x88\\x87\\xce\\xbe(x\\xce\\xbap j ) = Je (x\\xce\\xbap j )ne (x\\xce\\xbap j ),       J\\xce\\xba0 j \\xe2\\x88\\x87\\xce\\xbe(x0\\xce\\xba j ) = \\xe2\\x88\\x92Je (x0\\xce\\xba j )ne (x0\\xce\\xba j ),              \\xce\\xba ) = Je (x\\xce\\xba )ne (x\\xce\\xba ),\\r\\n                                                                                                   J\\xce\\xbaip \\xe2\\x88\\x87\\xce\\xb7(xip         ip      ip\\r\\n                                                                                                                                               \\xce\\xba ) = \\xe2\\x88\\x92Je (x\\xce\\xba )ne (x\\xce\\xba ), (25)\\r\\n                                                                                                                                      J\\xce\\xbai0 \\xe2\\x88\\x87\\xce\\xb7(xi0          i0      i0\\r\\n\\r\\n\\r\\n\\r\\nfor 0 6 i, j 6 p, where ne denotes the unit normal to e in \\xe2\\x88\\x82\\xce\\xba pointing outward from \\xce\\xba (see Figure 1).\\r\\n\\r\\n\\r\\n3.2. Semi-discrete form\\r\\n   Following [23, 14, 47], we multiply (1a) with a test function vh \\xe2\\x88\\x88 Vhp and perform double integration-by-parts to get the\\r\\nsemi-discrete weak form of (1a): find uh in (Vhp )neq such that\\r\\n\\r\\n               XZ                                                    XZ\\r\\n                                                                        v\\xe2\\x88\\x92h D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92h , u+h , ne ) + v+h D+ (u\\xe2\\x88\\x92h , u+h , ne )dS = 0 \\xe2\\x88\\x80vh \\xe2\\x88\\x88 Vhp ,\\r\\n                             \\x10                                 \\x11\\r\\n                           vh \\xe2\\x88\\x82t uh + \\xe2\\x88\\x87x \\xc2\\xb7 f(uh ) + c(uh )\\xe2\\x88\\x87x uh dV +\\r\\n               \\xce\\xba\\xe2\\x88\\x88\\xe2\\x84\\xa6h    \\xce\\xba                                                             e\\xe2\\x88\\x88Eh    e\\r\\n\\r\\n\\r\\nwhere u\\xc2\\xb1h (x, t) = lim\\xce\\xb5\\xe2\\x86\\x930 uh (x \\xc2\\xb1 \\xce\\xb5ne (x), t) are the traces of uh at x on a given cell interface e \\xe2\\x88\\x88 Eh (see Figure 1) and D\\xc2\\xb1 (\\xc2\\xb7, \\xc2\\xb7, \\xc2\\xb7) are the\\r\\nnumerical fluctuations that are applied at the interfaces and are considered under the form\\r\\n\\r\\n\\r\\n                                                         D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) = h(u\\xe2\\x88\\x92 , u+ , n) \\xe2\\x88\\x92 f(u\\xe2\\x88\\x92 ) \\xc2\\xb7 n + d\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n),                                           (26a)\\r\\n                                                         D+ (u\\xe2\\x88\\x92 , u+ , n) = f(u+ ) \\xc2\\xb7 n \\xe2\\x88\\x92 h(u\\xe2\\x88\\x92 , u+ , n) + d+ (u\\xe2\\x88\\x92 , u+ , n),                                           (26b)\\r\\n\\r\\nto allow proper discretizations of each term in (1a), they satisfy the consistency relations\\r\\n\\r\\n                                                              h(u, u, n) = f(u) \\xc2\\xb7 n,             d\\xc2\\xb1 (u, u, n) = 0 \\xe2\\x88\\x80u \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM ,                                           (27)\\r\\n\\r\\nand will be introduced in section 5.\\r\\n     We then substitute vh for the Lagrange interpolation polynomials (17) and consider the quadrature rules (19). Using the discrete\\r\\ndifference matrix (21), the semi-discrete problem reads: find uh in (Vhp )neq such that\\r\\n\\r\\n                                                             p \\x12\\r\\n                                       d ij                 X       \\x10                         \\x11                  \\x10                       \\x11           \\x13\\r\\n                        \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j      U\\xce\\xba + \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j      Dik f(Uk\\xce\\xba j ) + c(Ui\\xce\\xbaj )Uk\\xce\\xba j \\xe2\\x88\\x87x \\xce\\xbe(\\xce\\xbei j ) + D jk f(Uik\\xce\\xba ) + c(Ui\\xce\\xbaj )Uik\\xce\\xba \\xe2\\x88\\x87x \\xce\\xb7(\\xce\\xbei j )\\r\\n                                       dt                   k=0\\r\\n                                                    p\\r\\n                                                                                                                                                                       (28)\\r\\n                                                   XX\\r\\n                                                              \\xcf\\x86i\\xce\\xbaj (xke )\\xcf\\x89k Jek D\\xe2\\x88\\x92 Ui\\xce\\xbaj , u+h (xke , t), nke \\xe2\\x88\\x80t > 0,\\r\\n                                                                                  \\x10                         \\x11\\r\\n                                               +                                                                          \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h ,    0 6 i, j 6 p,\\r\\n                                                   e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\n\\r\\nwhere by (18) \\xcf\\x86i\\xce\\xbaj (xke ) = 1 if xi\\xce\\xbaj = xke and \\xcf\\x86i\\xce\\xbaj (xke ) = 0 else.\\r\\n     The initial condition (1b) is projected onto the function space:\\r\\n\\r\\n                                                                  Ui\\xce\\xbaj (0) = u0 (x\\xce\\xbai j ) \\xe2\\x88\\x80\\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h ,          0 6 i, j 6 p.                                              (29)\\r\\n\\r\\n    The integral over elements \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h in (28) should be modified where we replace the physical fluxes and nonconservative\\r\\nproducts with numerical fluctuations of the form [9, 10, 14, 47]\\r\\n\\r\\n\\r\\n                                                        D\\xe2\\x88\\x92X (u\\xe2\\x88\\x92 , u+ , n) = hX (u\\xe2\\x88\\x92 , u+ , n) \\xe2\\x88\\x92 f(u\\xe2\\x88\\x92 ) \\xc2\\xb7 n + d\\xe2\\x88\\x92X (u\\xe2\\x88\\x92 , u+ , n),                                        (30a)\\r\\n                                                        D+X (u\\xe2\\x88\\x92 , u+ , n) = f(u+ ) \\xc2\\xb7 n \\xe2\\x88\\x92 hX (u\\xe2\\x88\\x92 , u+ , n) + d+X (u\\xe2\\x88\\x92 , u+ , n),                                        (30b)\\r\\n\\r\\n\\r\\n                                                                                                  7\\r\\n\\x0c\\n\\nwhere the subscript X will refer to either ec or cp to denote either entropy conservative or contact preserving numerical fluctuations,\\r\\nrespectively, that will be introduced in section 4. The modified scheme now reads\\r\\n\\r\\n                                                    p\\r\\n                                    d ij           X   \\x10                                                                         \\x11\\r\\n                     \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j      U\\xce\\xba + \\xcf\\x89i \\xcf\\x89 j       Dik D\\xcc\\x83X (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) + D jk D\\xcc\\x83X (Ui\\xce\\xbaj , Uik\\xce\\xba , ni( j,k) )\\r\\n                                    dt             k=0\\r\\n                                                 p\\r\\n                                                                                                                                                                             (31)\\r\\n                                                XX\\r\\n                                                               \\xcf\\x86i\\xce\\xbaj (xke )\\xcf\\x89k Jek D\\xe2\\x88\\x92 Ui\\xce\\xbaj , u+h (xke , t), nke\\r\\n                                                                                    \\x10                           \\x11\\r\\n                                            +                                                                         \\xe2\\x88\\x80t > 0,         \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h ,   0 6 i, j 6 p,\\r\\n                                                e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\nwhere\\r\\n\\r\\n\\r\\n                                 D\\xcc\\x83X (u\\xe2\\x88\\x92 , u+ , n) := D\\xe2\\x88\\x92X (u\\xe2\\x88\\x92 , u+ , n) \\xe2\\x88\\x92 D+X (u+ , u\\xe2\\x88\\x92 , n),                                                                                (32a)\\r\\n                                                        (30)\\r\\n                                                        = hX (u\\xe2\\x88\\x92 , u+ , n) + hX (u+ , u\\xe2\\x88\\x92 , n) + d\\xe2\\x88\\x92X (u\\xe2\\x88\\x92 , u+ , n) \\xe2\\x88\\x92 d+X (u+ , u\\xe2\\x88\\x92 , n),                                      (32b)\\r\\n                                                        (22)\\r\\n\\r\\n\\r\\nand\\r\\n\\r\\n                                               1 \\x10 ij                               \\x11                               1 \\x10 ij                              \\x11\\r\\n                                  n(i,k) j =      J\\xce\\xba \\xe2\\x88\\x87x \\xce\\xbe(\\xce\\xbei j ) + J\\xce\\xbak j \\xe2\\x88\\x87x \\xce\\xbe(\\xce\\xbek j ) ,             ni( j,k) =          J\\xce\\xba \\xe2\\x88\\x87x \\xce\\xb7(\\xce\\xbei j ) + J\\xce\\xbaik \\xe2\\x88\\x87x \\xce\\xb7(\\xce\\xbeik )                      (33)\\r\\n                                               2                                                                    2\\r\\nmust be introduced to keep conservation of the physical fluxes [64] and preserve uniform states.\\r\\n      The numerical flux and fluctuations in (30) satisfy the consistency conditions\\r\\n\\r\\n                                                     hX (u, u, n) = f(u) \\xc2\\xb7 n,                d\\xc2\\xb1X (u, u, n) = 0            \\xe2\\x88\\x80u \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM ,                                         (34)\\r\\n\\r\\nand entropy conservative (EC) fluctuations D\\xc2\\xb1ec (\\xc2\\xb7, \\xc2\\xb7, \\xc2\\xb7) satisfy [9]\\r\\n\\r\\n                                      \\xcf\\x91(u\\xe2\\x88\\x92 )> D\\xe2\\x88\\x92ec (u\\xe2\\x88\\x92 , u+ , n) + \\xcf\\x91(u+ )> D+ec (u\\xe2\\x88\\x92 , u+ , n) = ~q(u)\\x7f \\xc2\\xb7 n                              \\xe2\\x88\\x80u\\xc2\\xb1 \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM ,                          (35)\\r\\n\\r\\nwhere ~vh \\x7f = v+h \\xe2\\x88\\x92 v\\xe2\\x88\\x92h and \\xcf\\x91(u) := \\xe2\\x88\\x87u \\xce\\xb7(u) denotes the entropy variables. Additionally, the interface fluctuations (26) in (28) are\\r\\nassumed to be entropy stable:\\r\\n\\r\\n                                       \\xcf\\x91(u\\xe2\\x88\\x92 )> D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) + \\xcf\\x91(u+ )> D+ (u\\xe2\\x88\\x92 , u+ , n) > ~q(u)\\x7f \\xc2\\xb7 n                                \\xe2\\x88\\x80u\\xc2\\xb1 \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM .                           (36)\\r\\n\\r\\n     The theorem below summarizes the main properties of the semi-discrete scheme (31) for the discretization of general systems\\r\\nof the form (1a).\\r\\nTheorem 3.1. Let D\\xc2\\xb1X (\\xc2\\xb7, \\xc2\\xb7, \\xc2\\xb7) in (30) be consistent fluctuations with d\\xc2\\xb1X (\\xc2\\xb7, \\xc2\\xb7, \\xc2\\xb7) in (32) satisfying\\r\\n                                                                       d\\xc2\\xb1X (u\\xe2\\x88\\x92 , u+ , n) = C\\xc2\\xb1 (u\\xe2\\x88\\x92 , u+ , n)~u\\x7f,                                                             (37a)\\r\\n                                                                                    +             +             +                       +\\r\\n                                                               C(u , u , n) := C (u , u , n) + C (u , u , n),\\r\\n                                                                              \\xe2\\x88\\x92                        \\xe2\\x88\\x92                     \\xe2\\x88\\x92    \\xe2\\x88\\x92\\r\\n                                                                                                                                                                            (37b)\\r\\n                                                           +\\r\\n                                                C(u , u , n) + C(u+ , u\\xe2\\x88\\x92 , n) = c(u\\xe2\\x88\\x92 ) + c(u+ ) n,\\r\\n                                                                               \\x10               \\x11\\r\\n                                                    \\xe2\\x88\\x92\\r\\n                                                                                                                                                                            (37c)\\r\\n                                                                            C(u, u, n) = c(u)n,                                                                             (37d)\\r\\n\\r\\nwhere n = (n1 , . . . , nd )> , c(u)n = i=1 ni ci (u), and ~u\\x7f = u+ \\xe2\\x88\\x92 u\\xe2\\x88\\x92 , and let D\\xe2\\x88\\x92 (\\xc2\\xb7, \\xc2\\xb7, \\xc2\\xb7) in (28) be consistent (27) fluctuations. Then,\\r\\n                                               Pd\\r\\nthe semi-discrete scheme (31) has the following properties:\\r\\n\\r\\n   (i) it is a high-order accurate approximation of smooth enough solutions to (1);\\r\\n  (ii) it preserves uniform states (free-stream preservation);\\r\\n  (iii) the cell-averaged solution (20) satisfies the following cell-averaged semi-discrete scheme\\r\\n                                              p\\r\\n                         d                X             \\x10                                                        \\x11\\r\\n                   |\\xce\\xba|      huh i\\xce\\xba (t) +          \\xcf\\x89i \\xcf\\x89 j Dik c(Ui\\xce\\xbaj )n(i,k) j Uk\\xce\\xba j + D jk c(Ui\\xce\\xbaj )ni( j,k) Uik\\xce\\xba\\r\\n                         dt              i, j,k=0\\r\\n                                             p\\r\\n                                                                                                                                                                             (38)\\r\\n                                            XX                    \\x12 \\x10                                                                         \\x11\\x13\\r\\n                                                                   h u\\xe2\\x88\\x92h (xke , t), u+h (xke , t), nke + d\\xe2\\x88\\x92 u\\xe2\\x88\\x92h (xke , t), u+h (xke , t), nke\\r\\n                                                                                                      \\x11    \\x10\\r\\n                                        +               \\xcf\\x89k Jek                                                                                         \\xe2\\x88\\x80t > 0,   \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h ,\\r\\n                                            e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\n                                                                                              8\\r\\n\\x0c\\n\\n        which ensures a discretely conservative approximation of the physical fluxes in (1a);\\r\\n  (iv) if the fluctuations DX (\\xc2\\xb7, \\xc2\\xb7, \\xc2\\xb7) in the volume integral are further assumed to be EC (35) and the fluctuations at interfaces\\r\\n       D(\\xc2\\xb7, \\xc2\\xb7, \\xc2\\xb7) are entropy stable (36) for a convex entropy pair (\\xce\\xb7(u), q(u)), then the following semi-discrete entropy inequality\\r\\n       holds:\\r\\n                                                        p\\r\\n                                     d            XX\\r\\n                                                           \\xcf\\x89k Jek Q u\\xe2\\x88\\x92h (xke , t), u+h (xke , t), nke 6 0 \\xe2\\x88\\x80t > 0, \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h ,\\r\\n                                                                   \\x10                                 \\x11\\r\\n                                 |\\xce\\xba| h\\xce\\xb7(uh )i\\xce\\xba +                                                                                (39)\\r\\n                                    dt            e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\n        with the consistent and conservative entropy flux\\r\\n\\r\\n                                                                   1\\x10 \\xe2\\x88\\x92                  1                          1\\r\\n                                         Q(u\\xe2\\x88\\x92 , u+ , n) =            q(u ) + q(u+ ) \\xc2\\xb7 n + \\xcf\\x91(u\\xe2\\x88\\x92 )> D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) \\xe2\\x88\\x92 \\xcf\\x91(u+ )> D+ (u\\xe2\\x88\\x92 , u+ , n).\\r\\n                                                                                   \\x11\\r\\n                                                                                                                                                                                                     (40)\\r\\n                                                                   2                     2                          2\\r\\n\\r\\nProof. Preliminary: By the metric identities (24) and (22), the metric terms in (33) satisfy\\r\\n\\r\\n                            p                                             p\\r\\n                            X                                          1 X \\x10 ij                                 \\x11      \\x10                                 \\x11\\r\\n                                  Dik n(i,k) j + D jk ni( j,k) =             Dik J\\xce\\xba \\xe2\\x88\\x87\\xce\\xbe(xi\\xce\\xbaj ) + J\\xce\\xbak j \\xe2\\x88\\x87\\xce\\xbe(xk\\xce\\xba j ) + D jk J\\xce\\xbai j \\xe2\\x88\\x87\\xce\\xb7(xi\\xce\\xbaj ) + J\\xce\\xbak j \\xe2\\x88\\x87\\xce\\xb7(xik\\xce\\xba ) = 0.                                       (41)\\r\\n                            k=0\\r\\n                                                                       2 k=0\\r\\n\\r\\n     High-order accuracy: It is sufficient to prove that the volume integral in (31) is a high-order approximation of \\xe2\\x88\\x87 \\xc2\\xb7 f(u) + c(u)\\xe2\\x88\\x87u\\r\\nat (xi\\xce\\xbaj , t) for smooth enough solutions. High-order accuracy of the conservative term \\xe2\\x88\\x87 \\xc2\\xb7 f(u) has been proved in [12, 45] and the\\r\\nhigh-order accuracy of c(u)\\xe2\\x88\\x87u in one space dimension has been proved in [47, Th. 3.2]. Since we are using tensor products of\\r\\none-dimensional operators, the proof of accuracy follows by considering each space dimension independently.\\r\\n    Free-stream preservation: Let us assume that Ui\\xce\\xbaj = U in the space residuals for all \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h and 0 6 i, j 6 p, then by\\r\\nconsistency: D\\xe2\\x88\\x92 (U, U, nke ) = 0 and D\\xcc\\x83X (u\\xe2\\x88\\x92 , u+ , n) = 2f(U) \\xc2\\xb7 n. Further using (41), (31) becomes\\r\\n\\r\\n                                                                                   p\\r\\n                                                                   d ij           X                                                      d\\r\\n                                                0 = \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j      U\\xce\\xba + \\xcf\\x89i \\xcf\\x89 j     2f(U)(Dik n(i,k) j + D jk ni( j,k) ) = \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j Ui\\xce\\xbaj .\\r\\n                                                                   dt             k=0\\r\\n                                                                                                                                         dt\\r\\n\\r\\n    Cell-averaged semi-discrete scheme: Summing up (31) over 0 6 i, j 6 p gives\\r\\n\\r\\n                                                                p          p                p\\r\\n                                               d               X          X           XX\\r\\n                                                                                               \\xcf\\x89k Jek D\\xe2\\x88\\x92 u\\xe2\\x88\\x92h (xke , t), u+h (xke , t), nke = 0,\\r\\n                                                                                                        \\x10                                 \\x11\\r\\n                                         |\\xce\\xba|      huh i\\xce\\xba (t) +     \\xcf\\x89jAj +     \\xcf\\x89i Bi +\\r\\n                                               dt              j=0        i=0         e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\nwhere from (32), we have\\r\\n\\r\\n\\r\\n        p\\r\\n        X                                                   p\\r\\n                                                            X\\r\\n                                                                    \\xcf\\x89i Dik hX (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) + hX (Uk\\xce\\xba j , Ui\\xce\\xbaj , n(i,k) j ) + d\\xe2\\x88\\x92 (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) \\xe2\\x88\\x92 d+ (Uk\\xce\\xba j , Ui\\xce\\xbaj , n(i,k) j )\\r\\n                                                                          \\x10                                                                                                                              \\x11\\r\\n Aj =           \\xcf\\x89i Dik D\\xcc\\x83X (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) =\\r\\n        i,k=0                                               i,k=0\\r\\n        p\\r\\n        X                                                   p\\r\\n                                                            X\\r\\n                                                                    \\xcf\\x89 j D jk hX (Ui\\xce\\xbaj , Uik\\xce\\xba , ni( j,k) ) + hX (Uik\\xce\\xba , Ui\\xce\\xbaj , ni( j,k) ) + d\\xe2\\x88\\x92 (Ui\\xce\\xbaj , Uik\\xce\\xba , ni( j,k) ) \\xe2\\x88\\x92 d+ (Uik\\xce\\xba , Ui\\xce\\xbaj , ni( j,k) )\\r\\n                                                                            \\x10                                                                                                                          \\x11\\r\\n Bi =           \\xcf\\x89 j D jk D\\xcc\\x83X (Ui\\xce\\xbaj , Uik\\xce\\xba , ni( j,k) ) =\\r\\n        j,k=0                                               j,k=0\\r\\n\\r\\n\\r\\n    Let us consider the first term. Using (25), we have\\r\\n\\r\\n\\r\\n                  (23)\\r\\n           A j = Je (x\\xce\\xbap j )f(U\\xce\\xbap j ) \\xc2\\xb7 ne (x\\xce\\xbap j ) + Je (x0\\xce\\xba j )f(U0\\xce\\xba j ) \\xc2\\xb7 ne (x0\\xce\\xba j )\\r\\n                  (34)\\r\\n                      p\\r\\n                      X\\r\\n                  +           \\xcf\\x89i Dik hX (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) \\xe2\\x88\\x92 \\xcf\\x89k Dki hX (Uk\\xce\\xba j , Ui\\xce\\xbaj , n(i,k) j ) + \\xcf\\x89i Dik d\\xe2\\x88\\x92 (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) + \\xcf\\x89k Dki d+ (Uk\\xce\\xba j , Ui\\xce\\xbaj , n(i,k) j )\\r\\n                      i,k=0\\r\\n                                                                                                            p\\r\\n                                                                                                            X\\r\\n                    i\\xe2\\x86\\x94k\\r\\n                      = Je (x\\xce\\xbap j )f(U\\xce\\xbap j ) \\xc2\\xb7 ne (x\\xce\\xbap j ) + Je (x0\\xce\\xba j )f(U0\\xce\\xba j ) \\xc2\\xb7 ne (x0\\xce\\xba j ) +                   \\xcf\\x89i Dik C(Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j )(Uk\\xce\\xba j \\xe2\\x88\\x92 Ui\\xce\\xbaj )\\r\\n                  (37a,b)\\r\\n                                                                                                            i,k=0\\r\\n                   (23)              \\x10                                                         \\x11                \\x10                                                    \\x11\\r\\n                    =     Je (x\\xce\\xbap j ) f(U\\xce\\xbap j ) \\xc2\\xb7 ne (x\\xce\\xbap j ) \\xe2\\x88\\x92 c(U\\xce\\xbap j )ne (x\\xce\\xbap j )U\\xce\\xbap j          + Je (x0\\xce\\xba j ) f(U0\\xce\\xba j ) \\xc2\\xb7 ne (x0\\xce\\xba j ) \\xe2\\x88\\x92 c(U0\\xce\\xba j )ne (x0\\xce\\xba j )U0\\xce\\xba j\\r\\n                  (37d)\\r\\n                      p\\r\\n                      X\\r\\n                  +           \\xcf\\x89i Dik C(Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j )Uk\\xce\\xba j + \\xcf\\x89k Dki C(Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j )Ui\\xce\\xbaj\\r\\n                      i,k=0\\r\\n\\r\\n                                                                                                        9\\r\\n\\x0c\\n\\n                   i\\xe2\\x86\\x94k\\r\\n                                \\x10                                                   \\x11             \\x10                                                    \\x11\\r\\n                   = Je (x\\xce\\xbap j ) f(U\\xce\\xbap j ) \\xc2\\xb7 ne (x\\xce\\xbap j ) \\xe2\\x88\\x92 c(U\\xce\\xbap j )ne (x\\xce\\xbap j )U\\xce\\xbap j + Je (x0\\xce\\xba j ) f(U0\\xce\\xba j ) \\xc2\\xb7 ne (x0\\xce\\xba j ) \\xe2\\x88\\x92 c(U0\\xce\\xba j )ne (x0\\xce\\xba j )U0\\xce\\xba j\\r\\n                  (37c)\\r\\n                      p\\r\\n                      X             \\x10                    \\x11\\r\\n                  +           \\xcf\\x89i Dik c(Ui\\xce\\xbaj ) + c(Uk\\xce\\xba j ) n(i,k) j Uk\\xce\\xba j\\r\\n                      i,k=0\\r\\n                                                                                                    p\\r\\n                                                                                                    X\\r\\n                   (23)\\r\\n                    = Je (x\\xce\\xbap j )f(U\\xce\\xbap j ) \\xc2\\xb7 ne (x\\xce\\xbap j ) + Je (x0\\xce\\xba j )f(U0\\xce\\xba j ) \\xc2\\xb7 ne (x0\\xce\\xba j ) +             \\xcf\\x89i Dik c(Ui\\xce\\xbaj )n(i,k) j (Uk\\xce\\xba j \\xe2\\x88\\x92 Ui\\xce\\xbaj ),\\r\\n                  (37d)\\r\\n                                                                                                    i,k=0\\r\\n\\r\\n\\r\\nwhere i \\xe2\\x86\\x94 k indicates an inversion of indices i and k in some of the terms. Likewise, we have\\r\\n                                                                                                                    p\\r\\n                                                                                                                    X\\r\\n                                               \\xce\\xba )f(U\\xce\\xba ) \\xc2\\xb7 ne (x\\xce\\xba ) + Je (x\\xce\\xba )f(U\\xce\\xba ) \\xc2\\xb7 ne (x\\xce\\xba ) +\\r\\n                                     Bi = Je (xip    ip         ip         i0    i0         i0\\r\\n                                                                                                                            \\xcf\\x89 j D jk c(Ui\\xce\\xbaj )ni( j,k) (Uik\\xce\\xba \\xe2\\x88\\x92 Ui\\xce\\xbaj ).\\r\\n                                                                                                                    j,k=0\\r\\n\\r\\n\\r\\n    From (41) we deduce\\r\\n                      p\\r\\n                      X                     p\\r\\n                                            X                p\\r\\n                                                            XX                                      p\\r\\n                                                                                                    X\\r\\n                                                                               \\x10             \\x11              \\x10                                                       \\x11\\r\\n                             \\xcf\\x89jAj +               \\xcf\\x89i Bi =              \\xcf\\x89k Jek h u\\xe2\\x88\\x92h (xke , t) nke +   \\xcf\\x89i \\xcf\\x89 j Dik c(Ui\\xce\\xbaj )n(i,k) j Uk\\xce\\xba j + D jk c(Ui\\xce\\xbaj )ni( j,k) Uik\\xce\\xba ,\\r\\n                      j=0                   i=0             e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0                                i, j,k=0\\r\\n\\r\\n\\r\\nand we get (38).\\r\\n   Entropy stability: Let us now consider EC fluxes in the volume integral in the semi-discrete DGSEM scheme (31). Left\\r\\nmultiply (31) by \\xcf\\x91i\\xce\\xbaj = \\xcf\\x91(ui\\xce\\xbaj ) and sum up over 0 6 i, j 6 p to get\\r\\n\\r\\n                                                   p            p                 p\\r\\n                                    d             X            X            XX\\r\\n                                                                                     \\xcf\\x89k Jek \\xcf\\x91 u\\xe2\\x88\\x92h (xke , t) \\xc2\\xb7 D\\xe2\\x88\\x92 u\\xe2\\x88\\x92h (xke , t), u+h (xke , t), nke = 0,\\r\\n                                                                                             \\x10             \\x11    \\x10                                 \\x11\\r\\n                              |\\xce\\xba|      h\\xce\\xb7i\\xce\\xba (t) +     \\xcf\\x89 jC j +     \\xcf\\x89i E i +\\r\\n                                    dt            j=0          i=0          e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\nwhere\\r\\n\\r\\n\\r\\n        p\\r\\n        X                                                                                                       p\\r\\n                                                                                                                X\\r\\n                \\xcf\\x89i Dik \\xcf\\x91i\\xce\\xbaj \\xc2\\xb7 D\\xe2\\x88\\x92ec (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) \\xe2\\x88\\x92 D+ec (Uk\\xce\\xba j , Ui\\xce\\xbaj , n(i,k) j ) ,                       \\xcf\\x89 j D jk \\xcf\\x91i\\xce\\xbaj \\xc2\\xb7 D\\xe2\\x88\\x92ec (Ui\\xce\\xbaj , Uik\\xce\\xba , ni( j,k) ) \\xe2\\x88\\x92 D+ec (Uik\\xce\\xba , Ui\\xce\\xbaj , ni( j,k) ) .\\r\\n                             \\x10                                                                 \\x11                                       \\x10                                                               \\x11\\r\\n Cj =                                                                                                 Ei =\\r\\n        i,k=0                                                                                                   j,k=0\\r\\n\\r\\n\\r\\n    Using (35), we have\\r\\n\\r\\n\\r\\n                          p\\r\\n                          X             \\x12                                                                                                                   \\x13\\r\\n                                                                                                                           \\x10                    \\x11\\r\\n                Cj =              \\xcf\\x89i Dik \\xcf\\x91i\\xce\\xbaj \\xc2\\xb7 D\\xe2\\x88\\x92ec (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) + \\xcf\\x91k\\xce\\xba j \\xc2\\xb7 D\\xe2\\x88\\x92ec (Uk\\xce\\xba j , Ui\\xce\\xbaj , n(i,k) j ) \\xe2\\x88\\x92 q(Ui\\xce\\xbaj ) \\xe2\\x88\\x92 q(Uk\\xce\\xba j ) \\xc2\\xb7 n(i,k) j\\r\\n                          i,k=0\\r\\n                      (23)\\r\\n                      = Je (x\\xce\\xbap j )q(U\\xce\\xbap j ) \\xc2\\xb7 ne (x\\xce\\xbap j ) + Je (x0\\xce\\xba j )q(U0\\xce\\xba j ) \\xc2\\xb7 ne (x0\\xce\\xba j )\\r\\n                      (34)\\r\\n                          p\\r\\n                          X                                                                                                       \\x10                                  \\x11\\r\\n                      +           \\xcf\\x89i Dik \\xcf\\x91i\\xce\\xbaj \\xc2\\xb7 D\\xe2\\x88\\x92ec (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) \\xe2\\x88\\x92 \\xcf\\x89k Dki \\xcf\\x91k\\xce\\xba j \\xc2\\xb7 D\\xe2\\x88\\x92ec (Uk\\xce\\xba j , Ui\\xce\\xbaj , n(i,k) j ) \\xe2\\x88\\x92 \\xcf\\x89i Dik q(Ui\\xce\\xbaj ) + \\xcf\\x89k Dki q(Uk\\xce\\xba j ) \\xc2\\xb7 n(i,k) j\\r\\n                          i,k=0\\r\\n                                                                                                       p\\r\\n                                                                                                       X\\r\\n                      i\\xe2\\x86\\x94k\\r\\n                      = Je (x\\xce\\xbap j )q(U\\xce\\xbap j ) \\xc2\\xb7 ne (x\\xce\\xbap j ) + Je (x0\\xce\\xba j )q(U0\\xce\\xba j ) \\xc2\\xb7 ne (x0\\xce\\xba j ) \\xe2\\x88\\x92              \\xcf\\x89i Dik J\\xce\\xbak j q(Ui\\xce\\xbaj )\\xe2\\x88\\x87\\xce\\xbe(xk\\xce\\xba j ).\\r\\n                      (22)\\r\\n                                                                                                       i,k=0\\r\\n\\r\\n\\r\\n    Likewise\\r\\n\\r\\n                                                                                                                           p\\r\\n                                                                                                                           X\\r\\n                                                   \\xce\\xba )q(U\\xce\\xba ) \\xc2\\xb7 ne (x\\xce\\xba ) + Je (x\\xce\\xba )q(U\\xce\\xba ) \\xc2\\xb7 ne (x\\xce\\xba ) \\xe2\\x88\\x92\\r\\n                                         Ei = Je (xip    ip         ip         i0    i0         i0\\r\\n                                                                                                                                   \\xcf\\x89 j D jk J\\xce\\xbaik q(Ui\\xce\\xbaj )\\xe2\\x88\\x87\\xce\\xb7(xik\\xce\\xba ),\\r\\n                                                                                                                           j,k=0\\r\\n\\r\\nand again using the metric identities (24), we get\\r\\n\\r\\n                                                                p\\r\\n                                            d             XX             \\x12                                                                      \\x13\\r\\n                                                                   \\xcf\\x89k Jek \\xcf\\x91(xke ) \\xc2\\xb7 D\\xe2\\x88\\x92 u\\xe2\\x88\\x92h (xke , t), u+h (xke , t), nke + q u\\xe2\\x88\\x92h (xke , t) \\xc2\\xb7 nke = 0.\\r\\n                                                                                      \\x10                                 \\x11   \\x10             \\x11\\r\\n                                      |\\xce\\xba|      h\\xce\\xb7i\\xce\\xba (t) +\\r\\n                                            dt            e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\n                                                                                                 10\\r\\n\\x0c\\n\\n    Then, using (40) we have\\r\\n\\r\\n                                                                      1                1                       1\\r\\n                     \\xcf\\x91\\xe2\\x88\\x92 \\xc2\\xb7 D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) + q\\xe2\\x88\\x92 \\xc2\\xb7 n = Q(u\\xe2\\x88\\x92 , u+ , n) + (q\\xe2\\x88\\x92 \\xe2\\x88\\x92 q+ ) \\xc2\\xb7 n + \\xcf\\x91\\xe2\\x88\\x92 \\xc2\\xb7 D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) + \\xcf\\x91+ \\xc2\\xb7 D+ (u\\xe2\\x88\\x92 , u+ , n)\\r\\n                                                                      2                2                       2\\r\\nand since the sum of the three last terms are non-negative by (36), we obtain the desired entropy inequality (39).\\r\\n\\r\\nRemark 3.1. Fluctuations with the following expressions fall into the category (37a):\\r\\n\\r\\n                                                                                1\\x10\\r\\n                                                       C\\xc2\\xb1 (u\\xe2\\x88\\x92 , u+ , n) =\\r\\n                                                                                                         \\x11\\r\\n                                                                                  \\xce\\xb1c(u\\xc2\\xb1 ) + (1 \\xe2\\x88\\x92 \\xce\\xb1)c(u\\xe2\\x88\\x93 ) n,            0 6 \\xce\\xb1 6 1,\\r\\n                                                                                2\\r\\nwhich belong to the Volpert path family of schemes [61] and correspond to the skew-symmetric splitting \\xce\\xb1c\\xe2\\x88\\x87u + (1 \\xe2\\x88\\x92 \\xce\\xb1)(\\xe2\\x88\\x87 \\xc2\\xb7 (cu) \\xe2\\x88\\x92\\r\\n(\\xe2\\x88\\x87 \\xc2\\xb7 c)u) of the nonconservative product. Relations (37b) and (37d) indeed correspond to the consistency condition of the Volpert\\r\\npath family of schemes [61], while (37c) is only necessary to get (38) which will be useful to prove Theorem 6.1.\\r\\n\\r\\n   In the next two sections, we describe the fluctuations we use in the DGSEM scheme (31) for the discretization of the SG-\\r\\ngamma model (1) and (2). In section 4, we first focus on designing CP and EC fluctuations that are applied in the volume integral.\\r\\nWe then design in section 5 a HLLC approximate Riemann solver for (1a) that is applied at interfaces.\\r\\n\\r\\n\\r\\n4. Numerical fluctuations for the volume integrals\\r\\n\\r\\n    In this section we focus on numerical fluctuations (30) for the scheme (31) that will be applied in the volume integrals. We\\r\\nwill make use of the Leibniz identities, which we recall here: let a+ , a\\xe2\\x88\\x92 , b+ , b\\xe2\\x88\\x92 , c+ , c\\xe2\\x88\\x92 in R and have finite values, then we have\\r\\n                                                                                                       \\x10           \\x11\\r\\n                                                       ~ab\\x7f = a~b\\x7f + b~a\\x7f,                    ~abc\\x7f = a b~c\\x7f + c~b\\x7f + bc~a\\x7f,                                                         (42)\\r\\n\\r\\n               a+ + a\\xe2\\x88\\x92\\r\\nwhere a =              is the arithmetic mean and ~a\\x7f = a+ \\xe2\\x88\\x92 a\\xe2\\x88\\x92 the jump.\\r\\n                  2\\r\\n\\r\\n4.1. Contact preserving numerical fluxes\\r\\n     Here, we focus on deriving conditions that will ensure that the numerical fluxes maintain uniform pressure and velocity\\r\\nprofiles across an isolated material interface. To this purpose, we introduce CP fluctuations in (30), with index cp, where\\r\\nhcp (u\\xe2\\x88\\x92 , u+ , n) = (h\\xcf\\x81cp , h\\xcf\\x81v>   \\xcf\\x81E\\r\\n                             cp , hcp , 0, 0) is the vector of numerical fluxes for the conservative equations of mass, momentum and\\r\\n                                             >\\r\\n                         +\\r\\nenergy, and dcp (u , u , n) = (0, 0, 0, d\\xce\\x93\\xc2\\xb1 , d\\xce\\xa0\\xc2\\xb1 )> is the vector for the fluctuations for the nonconservative products in (1a).\\r\\n                 \\xc2\\xb1  \\xe2\\x88\\x92\\r\\n\\r\\n\\r\\n     For the sake of brevity, we here focus on volume fluctuations only, though the same relations may be derived for the interface\\r\\nfluctuations by following the same lines (see section 5.4.2). Instead of using the symmetrizer in (32), we assume that the numerical\\r\\nfluxes are symmetric: hX (u\\xe2\\x88\\x92 , u+ , n) = hcp (u+ , u\\xe2\\x88\\x92 , n) without loss of generality (they will be, see proposition 4.1). Let us represent\\r\\nthe conserved and nonconserved quantities in (1a) using A \\xe2\\x88\\x88 {\\xcf\\x81, \\xcf\\x81v, \\xcf\\x81E} and B \\xe2\\x88\\x88 {\\xce\\x93, \\xce\\xa0}, respectively. The DGSEM scheme for (1a)\\r\\nnow reads\\r\\n\\r\\n\\r\\n                                    p                                                                                    p\\r\\n                   d ij            X                                                                            \\x11 XX\\r\\n                                                                                                                            \\xcf\\x86i\\xce\\xbaj (xke )\\xcf\\x89k Jek D\\xe2\\x88\\x92A Ui\\xce\\xbaj , u+h (xke , t), nke\\r\\n                                       \\x10                                                                                                         \\x10                          \\x11\\r\\n    \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j      A\\xce\\xba + 2\\xcf\\x89i \\xcf\\x89 j            A\\r\\n                                         Dik hcp (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) + D jk hcp\\r\\n                                                                                    A\\r\\n                                                                                       (Ui\\xce\\xbaj , Uik\\xce\\xba , ni(k, j) ) +                                                                  (43a)\\r\\n                   dt              k=0                                                                             e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n                                   p \\x12\\r\\n                   d ij           X       \\x10                                                                 \\x11      \\x10                                                                \\x11\\x13\\r\\n    \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j      B\\xce\\xba + \\xcf\\x89i \\xcf\\x89 j           B,\\xe2\\x88\\x92\\r\\n                                       Dik dcp  (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) \\xe2\\x88\\x92 dcp\\r\\n                                                                              B,+\\r\\n                                                                                  (Uk\\xce\\xba j , Ui\\xce\\xbaj , n(i,k) j ) + D jk dcp\\r\\n                                                                                                                     B,\\xe2\\x88\\x92\\r\\n                                                                                                                         (Ui\\xce\\xbaj , Uik\\xce\\xba , ni(k, j) ) \\xe2\\x88\\x92 dcp\\r\\n                                                                                                                                                      B,+\\r\\n                                                                                                                                                          (Uik\\xce\\xba , Ui\\xce\\xbaj , ni(k, j) )\\r\\n                   dt             k=0\\r\\n                                p\\r\\n                               XX\\r\\n                                          \\xcf\\x86i\\xce\\xbaj (xke )\\xcf\\x89k Jek D\\xe2\\x88\\x92B Ui\\xce\\xbaj , u+h (xke , t), nke\\r\\n                                                               \\x10                          \\x11\\r\\n                           +                                                                                                                                                       (43b)\\r\\n                               e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\n\\r\\n    Now let us suppose that the initial condition consists of a material interface with uniform velocity, v = (u, v)> , and pressure, p,\\r\\nand states \\xcf\\x81L , \\xce\\x93L and \\xcf\\x80L in \\xe2\\x84\\xa6L and \\xcf\\x81R , \\xce\\x93R and \\xcf\\x80R in \\xe2\\x84\\xa6R with \\xe2\\x84\\xa6L \\xe2\\x88\\xaa \\xe2\\x84\\xa6R = \\xe2\\x84\\xa6, then so do the DOFs. We now derive conditions for the\\r\\nnumerical fluxes (30) to preserve the uniform states in time.\\r\\n\\r\\n                                                                                                 11\\r\\n\\x0c\\n\\n     We, first, focus on the velocity state and impose the semi-discrete scheme (43) to satisfy a discrete counterpart to the differential\\r\\nrelation \\xcf\\x81dv = d\\xcf\\x81v \\xe2\\x88\\x92 vd\\xcf\\x81 = 0. Ignoring the interface fluxes, \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j \\xcf\\x81i\\xce\\xbaj dt vi\\xce\\xbaj = 0 requires\\r\\n\\r\\n                      p\\r\\n                      X\\r\\n                         (h\\xcf\\x81v                           \\xcf\\x81                                \\xcf\\x81v                           \\xcf\\x81\\r\\n                        \\x10                                                                                                                         \\x11\\r\\n             \\xcf\\x89i \\xcf\\x89 j        cp (U\\xce\\xba , U\\xce\\xba , n(i,k) j ) \\xe2\\x88\\x92 vhcp (U\\xce\\xba , U\\xce\\xba , n(i,k) j ))Dik + (hcp (U\\xce\\xba , U\\xce\\xba , ni(k, j) ) \\xe2\\x88\\x92 vhcp (U\\xce\\xba , U\\xce\\xba , ni(k, j ))D jk = 0,\\r\\n                                ij   kj                      ij   kj                          ij   ik                      ij   ik\\r\\n\\r\\n                      k=0\\r\\n\\r\\nand a sufficient condition reads\\r\\n\\r\\n                                               h\\xcf\\x81v       +               + \\xcf\\x81         +               +\\r\\n                                                cp (u , u , n) = v\\xcc\\x83(u , u )hcp (u , u , n) + p\\xcc\\x83(u , u , n)\\r\\n                                                     \\xe2\\x88\\x92               \\xe2\\x88\\x92           \\xe2\\x88\\x92               \\xe2\\x88\\x92\\r\\n                                                                                                                                             \\xe2\\x88\\x80u\\xc2\\xb1 \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM ,           (44)\\r\\n\\r\\nwhere v\\xcc\\x83 and p\\xcc\\x83 are any consistent discretizations of the velocity vector and pressure. Similarly, a semi-discrete equation for the\\r\\npressure (5) can be obtained by using \\xce\\x93dp = d\\xcf\\x81E \\xe2\\x88\\x92 ( 21 v \\xc2\\xb7 v)d\\xcf\\x81 \\xe2\\x88\\x92 pd\\xce\\x93 \\xe2\\x88\\x92 d\\xce\\xa0 from (6), and again ignoring surface contributions in\\r\\n(43), \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j \\xce\\x93i\\xce\\xbaj dt p(Ui\\xce\\xbaj ) = 0 requires\\r\\n\\r\\n                            p\\r\\n                            X\\r\\n                                  Dik 2h\\xcf\\x81E                               \\xcf\\x81                                                        +\\r\\n                                                                                                     \\x10                                                    \\x11\\r\\n                  \\xcf\\x89i \\xcf\\x89 j                cp (U\\xce\\xba , U\\xce\\xba , n(i,k) j ) \\xe2\\x88\\x92 v \\xc2\\xb7 vhcp (U\\xce\\xba , U\\xce\\xba , n(i,k) j ) \\xe2\\x88\\x92 p d\\xce\\x93 (U\\xce\\xba , U\\xce\\xba , n(i,k) j ) \\xe2\\x88\\x92 d\\xce\\x93 (U\\xce\\xba , U\\xce\\xba , n(i,k) j )\\r\\n                                             ij   kj                          ij   kj                  \\xe2\\x88\\x92   ij   kj                    kj   ij\\r\\n\\r\\n                            k=0\\r\\n                                                                                                      !\\r\\n                                  \\xe2\\x88\\x92 d\\xce\\xa0\\xe2\\x88\\x92 (Ui\\xce\\xbaj , Uk\\xce\\xba j , n(i,k) j ) + d\\xce\\xa0+ (Uk\\xce\\xba j , Ui\\xce\\xbaj , n(i,k) j )\\r\\n\\r\\n                             +D jk 2h\\xcf\\x81E                                      \\xcf\\x81                                                 +\\r\\n                                                                                                   \\x10                                                  \\x11\\r\\n                                     cp (U\\xce\\xba , U\\xce\\xba , ni(k, j) ) \\xe2\\x88\\x92 v \\xc2\\xb7 vhcp (U\\xce\\xba , U\\xce\\xba , ni( j,k) ) \\xe2\\x88\\x92 p d\\xce\\x93 (U\\xce\\xba , U\\xce\\xba , ni(k, j) ) \\xe2\\x88\\x92 d\\xce\\x93 (U\\xce\\xba , U\\xce\\xba , ni(k, j) )\\r\\n                                             ij     ik                               ij      ik      \\xe2\\x88\\x92  ij   ik                    ik   ij\\r\\n\\r\\n                                                                                             !\\r\\n                              \\xe2\\x88\\x92 d\\xce\\xa0\\xe2\\x88\\x92 (Ui\\xce\\xbaj , Uik\\xce\\xba , ni(k, j) ) + d\\xce\\xa0+ (Uik\\xce\\xba , Ui\\xce\\xbaj , ni(k, j) ) = 0,\\r\\n\\r\\n                                                                P p \\x10 \\xcf\\x81E i j   v\\xc2\\xb7v \\xcf\\x81\\r\\n                                                                                        \\x11\\r\\nand subtracting the trivial quantity 2\\xcf\\x89i \\xcf\\x89 j                     k=0 f (U\\xce\\xba ) \\xe2\\x88\\x92 2 f (U\\xce\\xba ) \\xc2\\xb7 (Dik n(i,k) j + D jk ni( j,k) ) = 0, from (41), a sufficient condition\\r\\n                                                                                     ij\\r\\n\\r\\nreads\\r\\n\\r\\n\\r\\n                                                       v \\xc2\\xb7 v\\x10 \\xcf\\x81 \\xe2\\x88\\x92 +\\r\\n   h\\xcf\\x81E       +\\r\\n                                                                                        \\x11\\r\\n    cp (u , u , n) \\xe2\\x88\\x92 (\\xcf\\x81E + p)v \\xc2\\xb7 n =                         hcp (u , u , n) \\xe2\\x88\\x92 \\xcf\\x81\\xe2\\x88\\x92 v \\xc2\\xb7 n\\r\\n         \\xe2\\x88\\x92              \\xe2\\x88\\x92\\r\\n                                                        2\\r\\n                                                                            p\\x10 \\xe2\\x88\\x92 \\xe2\\x88\\x92 +                             \\x11 1\\x10\\r\\n                                                                               d\\xce\\x93 (u , u , n) \\xe2\\x88\\x92 d\\xce\\x93+ (u+ , u\\xe2\\x88\\x92 , n) + d\\xce\\xa0\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) \\xe2\\x88\\x92 d\\xce\\xa0+ (u+ , u\\xe2\\x88\\x92 , n) .\\r\\n                                                                                                                                                         \\x11\\r\\n                                                                         +                                                                                         (45)\\r\\n                                                                            2                                      2\\r\\n\\r\\n     We can now propose CP fluxes for the volume integral.\\r\\n\\r\\nProposition 4.1. Numerical fluxes of the form (30) where\\r\\n\\r\\n                                                     \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac \\xcf\\x81 v \\xc2\\xb7 n \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                     \\xef\\xa3\\xab                        \\xef\\xa3\\xb6                                                           \\xef\\xa3\\xab                \\xef\\xa3\\xb6\\r\\n                                                                                                                                          \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                         \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xcf\\x81 v v \\xc2\\xb7 n + pn\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                       \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                    \\xef\\xa3\\xb7                                                             \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac            \\xef\\xa3\\xb7\\r\\n                                                                                                                                              \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                                                                               1\\r\\n                              hcp (u\\xe2\\x88\\x92 , u+ , n) = \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac(\\xcf\\x81E + p)v \\xc2\\xb7 n\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 ,                                  d\\xc2\\xb1cp (u\\xe2\\x88\\x92 , u+ , n) = v\\xc2\\xb1 \\xc2\\xb7 n \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 ,               (46)\\r\\n                                                           \\xef\\xa3\\xac                  \\xef\\xa3\\xb7                                                                 \\xef\\xa3\\xac          \\xef\\xa3\\xb7\\r\\n                                                           \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                                              2                \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac        \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                             \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad  0            \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb8                                                              \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad ~\\xce\\x93\\x7f   \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                   0                                                                                   ~\\xce\\xa0\\x7f\\xef\\xa3\\xb8\\r\\n\\r\\npreserve the uniform pressure and velocity fields across contact discontinuities and material interfaces for the SG-gamma model\\r\\n(1a) and (2), with the mixture EOS (5).\\r\\n\\r\\nProof. Checking that condition (44) holds for (46) is direct, wile using the mixture EOS (5), we get\\r\\n\\r\\n\\r\\n     p\\x10 \\xe2\\x88\\x92 \\xe2\\x88\\x92 +                            \\x11 1\\x10                                    \\x11 v \\xc2\\xb7 n\\x10          \\x11 (42) v \\xc2\\xb7 n \\x12        v\\xc2\\xb7v    \\x13\\r\\n       d\\xce\\x93 (u , u , n) \\xe2\\x88\\x92 d\\xce\\x93+ (u+ , u\\xe2\\x88\\x92 , n) + d\\xce\\xa0\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) \\xe2\\x88\\x92 d\\xce\\xa0+ (u+ , u\\xe2\\x88\\x92 , n) =      p~\\xce\\x93\\x7f + ~\\xce\\xa0\\x7f =             ~\\xcf\\x81E\\x7f \\xe2\\x88\\x92     ~\\xcf\\x81\\x7f ,                                 (47)\\r\\n     2                                     2                                         2                      2             2\\r\\n\\r\\nso (45) holds as well.\\r\\n\\r\\nRemark 4.1. The CP numerical fluxes (46) are similar to the one proposed in [37]. Here we have modified the contributions\\r\\ntowards the energy equation to satisfy (45).\\r\\n\\r\\n                                                                                                      12\\r\\n\\x0c\\n\\n4.2. Entropy conservative numerical fluxes\\r\\n    We, now, propose EC fluxes for the SG-gamma model that are applied to the modified volume integral in (31) and, according\\r\\nto Theorem 3.1, these numerical fluxes will contribute to the entropy stability of the numerical scheme.\\r\\n\\r\\nProposition 4.2. Consider the entropy pair (11) with (12), then fluctuations of the form (30) with\\r\\n\\r\\n                                                         \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xcf\\x81\\xcc\\x82v \\xc2\\xb7 n\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                         \\xef\\xa3\\xab                 \\xef\\xa3\\xb6                                    \\xef\\xa3\\xab                \\xef\\xa3\\xb6\\r\\n                                                                                                                \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                           \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac h\\xcf\\x81v \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                                          \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                              \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac ec \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                  hec (u\\xe2\\x88\\x92 , u+ , n) = \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac h\\xcf\\x81E             \\xef\\xa3\\xb7\\xef\\xa3\\xb7 , d\\xc2\\xb1 (u\\xe2\\x88\\x92 , u+ , n) = 1 v\\xc2\\xb1 \\xc2\\xb7 n \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 ,\\r\\n                                                                                                                     \\xef\\xa3\\xac           \\xef\\xa3\\xb7\\r\\n                                                                                                                                                                        (48)\\r\\n                                                                \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac ec \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7     ec\\r\\n                                                                                                   2                 \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac ~\\xce\\x93\\x7f \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                                                                     \\xef\\xa3\\xac\\r\\n                                                                                                                     \\xef\\xa3\\xac\\r\\n                                                                  \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac 0 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                                            \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad       \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                    \\xef\\xa3\\xad 0 \\xef\\xa3\\xb8                                                  ~\\xce\\xa0\\x7f\\xef\\xa3\\xb8\\r\\n\\r\\nwhere\\r\\n                                             \\xef\\xa3\\xab                          \\xef\\xa3\\xb6                              \\xef\\xa3\\xabV                     \\xef\\xa3\\xb6             \\xef\\xa3\\xab           !\\xef\\xa3\\xb6\\r\\n                                                                                                                  v\\xe2\\x88\\x92 \\xc2\\xb7 v+ \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                              !                                        \\xef\\xa3\\xac\\xef\\xa3\\xac C !\\r\\n                                             \\xef\\xa3\\xac\\xef\\xa3\\xac            Cv           \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                                                                 \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac       Cv \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n        h\\xcf\\x81v       +\\r\\n                                                                                h\\xcf\\x81E       +                v\\r\\n         ec (u , u , n) = \\xcf\\x81\\xcc\\x82(v \\xc2\\xb7 n)v + \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad\\xcf\\x81(\\xce\\xb3 \\xe2\\x88\\x92 1)                               ec (u , u , n) = \\xef\\xa3\\xac             +             \\xef\\xa3\\xb7 \\xcf\\x81\\xcc\\x82v \\xc2\\xb7 n + \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad\\xcf\\x81(\\xce\\xb3 \\xe2\\x88\\x92 1)\\r\\n              \\xe2\\x88\\x92                                                                       \\xe2\\x88\\x92           \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\r\\n                                                                \\xe2\\x88\\x92 p\\xe2\\x88\\x9e \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb8 n,                                                                                \\xef\\xa3\\xb7\\xef\\xa3\\xb7 v \\xc2\\xb7 n,\\r\\n                                       \\xef\\xa3\\xac\\xef\\xa3\\xac                                                                                                     \\xef\\xa3\\xac\\r\\n                                                           \\xce\\xb6                                        \\xef\\xa3\\xac\\xef\\xa3\\xad     \\xce\\xb6         2 \\xef\\xa3\\xb7\\xef\\xa3\\xb8                             \\xce\\xb6 \\xef\\xa3\\xb8\\r\\n\\r\\n                   \\xce\\xa0\\r\\nand C\\xce\\xb6v = \\xce\\x93\\xcf\\x81 (p + \\xce\\x93+1 ) from (15), are EC in the sense (35) when excluding material interfaces, i.e., ~\\xce\\x93\\x7f \\xe2\\x89\\xa1 ~\\xce\\xa0\\x7f \\xe2\\x89\\xa1 0.\\r\\n\\r\\nProof. As we consider pure phases, the system (1a) is conservative and (35) reduces to the Tadmor condition [55]\\r\\n\\r\\n                                                     hec (u\\xe2\\x88\\x92 , u+ , n) \\xc2\\xb7 ~\\xcf\\x91\\x7f \\xe2\\x88\\x92 ~\\xcf\\x88(u)\\x7f \\xc2\\xb7 n = 0 \\xe2\\x88\\x80u\\xc2\\xb1 \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM ,\\r\\n\\r\\nwhere the entropy variables \\xcf\\x91 are defined in (14), and \\xcf\\x88 \\xe2\\x89\\xa1 f> \\xcf\\x91 \\xe2\\x88\\x92 q is the entropy potential and reads\\r\\n               (14)\\r\\n                                                    \\x12            v \\xc2\\xb7 v\\x13\\r\\n          \\xcf\\x88(u) = \\xe2\\x88\\x92\\xce\\xb6(\\xcf\\x81E + p)v + \\xce\\xb6v(\\xcf\\x81v \\xc2\\xb7 v + p) + \\xce\\xb3Cv \\xe2\\x88\\x92 s \\xe2\\x88\\x92 \\xce\\xb6             \\xcf\\x81v + \\xcf\\x81sv = (\\xce\\xb3Cv \\xe2\\x88\\x92 \\xce\\xb6e)\\xcf\\x81v = (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv \\xcf\\x81 \\xe2\\x88\\x92 p\\xe2\\x88\\x9e \\xce\\xb6 v,\\r\\n                                                                                                                    \\x01\\r\\n                                                                  2\\r\\nso\\r\\n                                                                    (42) \\x10                    \\x11\\r\\n                                                         ~\\xcf\\x88(u)\\x7f \\xc2\\xb7 n = (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv ~\\xcf\\x81v\\x7f \\xe2\\x88\\x92 p\\xe2\\x88\\x9e ~\\xce\\xb6v\\x7f \\xc2\\xb7 n.                                                                     (49)\\r\\n\\r\\n     Using the definition of \\xce\\xb6 in (15), the entropy of the mixture (12) may be reformulated as\\r\\n\\r\\n                                             p + p\\xe2\\x88\\x9e\\r\\n                                                    !                                    \\x10         \\x11\\r\\n                                  s = Cv ln           = \\xe2\\x88\\x92Cv ln \\xce\\xb6 \\xe2\\x88\\x92 (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv ln \\xcf\\x81 + Cv ln (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv ,                                                                   (50)\\r\\n                                                \\xcf\\x81 \\xce\\xb3\\r\\n\\r\\n\\r\\n\\r\\nthen we have\\r\\n                                                                                                                            \\xef\\xa3\\xab\\xef\\xa3\\xab                                          \\xef\\xa3\\xb6\\r\\n                                                                                                             \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac Cv v\\xe2\\x88\\x92 \\xc2\\xb7 v+ \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                   \\xef\\xa3\\xab                                   \\xef\\xa3\\xb6                                        \\xef\\xa3\\xb6\\r\\n                                        v\\xc2\\xb7v                          (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv \\xcf\\x81                                               \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 \\xcf\\x81\\xcc\\x82 + (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv \\xcf\\x81\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 v \\xc2\\xb7 n\\r\\n                                                                                                                                                      \\xef\\xa3\\xb7\\r\\nh>ec (u\\xe2\\x88\\x92 , u+ , n)~\\xcf\\x91(u)\\x7f = ~\\xce\\xb3Cv \\xe2\\x88\\x92 s \\xe2\\x88\\x92 \\xce\\xb6     \\x7f\\xcf\\x81\\xcc\\x82v \\xc2\\xb7 n + \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xadv \\xc2\\xb7 v\\xcf\\x81\\xcc\\x82 +                                                 +\\r\\n                                                          \\xef\\xa3\\xac\\xef\\xa3\\xac                             \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                                 \\xe2\\x88\\x92 p\\xe2\\x88\\x9e \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb8 ~\\xce\\xb6v\\x7f \\xc2\\xb7 n \\xe2\\x88\\x92 ~\\xce\\xb6\\x7f \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad\\r\\n                                                   2                                     \\xce\\xb6                                       \\xce\\xb6\\xcc\\x82       2                  \\xce\\xb6\\r\\n                                                                                                                                 \\xef\\xa3\\xb8                    \\xef\\xa3\\xb7\\xef\\xa3\\xb8\\r\\n                                 \\xef\\xa3\\xab                           \\xef\\xa3\\xb6                       \\xef\\xa3\\xab                              \\xef\\xa3\\xb6\\r\\n                           (50) \\xef\\xa3\\xac  ~\\xce\\xb6\\x7f               ~\\xcf\\x81\\x7f \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                                    (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv \\xcf\\x81\\r\\n                            = \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad         + (\\xce\\xb3 \\xe2\\x88\\x92 1)         \\xef\\xa3\\xb7\\xef\\xa3\\xb8 Cv \\xcf\\x81\\xcc\\x82v \\xc2\\xb7 n + ~\\xce\\xb6v\\x7f \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xadv \\xc2\\xb7 v\\xcf\\x81\\xcc\\x82 +\\r\\n                                 \\xef\\xa3\\xac                           \\xef\\xa3\\xb7                       \\xef\\xa3\\xac\\r\\n                                                                                     \\xef\\xa3\\xac                              \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                                                            \\xe2\\x88\\x92 p\\xe2\\x88\\x9e \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb8 \\xc2\\xb7 n\\r\\n                           (42)     \\xce\\xb6\\xcc\\x82                \\xcf\\x81\\xcc\\x82                                             \\xce\\xb6\\r\\n                                    \\xef\\xa3\\xab\\xef\\xa3\\xab                                          \\xef\\xa3\\xb6\\r\\n                                    \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac Cv v\\xe2\\x88\\x92 \\xc2\\xb7 v+ \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                         \\xef\\xa3\\xb6\\r\\n                                                         \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 \\xcf\\x81\\xcc\\x82 + (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv \\xcf\\x81\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 v \\xc2\\xb7 n\\r\\n                                                                                \\xef\\xa3\\xb7\\r\\n                           \\xe2\\x88\\x92 ~\\xce\\xb6\\x7f \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad\\xef\\xa3\\xac     +\\r\\n                                          \\xce\\xb6\\xcc\\x82       2 \\xef\\xa3\\xb8                   \\xce\\xb6      \\xef\\xa3\\xb8\\xef\\xa3\\xb7\\r\\n                              \\x10                                      \\x11\\r\\n                           = (\\xce\\xb3 \\xe2\\x88\\x92 1)Cv ~\\xcf\\x81v\\x7f \\xe2\\x88\\x92 p\\xe2\\x88\\x9e ~\\xce\\xb6v\\x7f \\xc2\\xb7 n,\\r\\n\\r\\nwhich cancels out with (49), so the proof is complete.\\r\\n\\r\\n\\r\\n5. An HLLC Riemann solver for the cell interfaces\\r\\n\\r\\n    We now look for two-point numerical fluctuations at interfaces and design an HLLC approximate Riemann solver for the SG-\\r\\ngamma model (1a) and analyze its properties. For the sake of generality, we assume that the entropy \\xce\\xb7(u) in (9) is convex, which\\r\\nexcludes material interfaces.\\r\\n\\r\\n                                                                                     13\\r\\n\\x0c\\n\\n5.1. One-dimensional Riemann problem\\r\\n    We are interested in approximating solutions to the following Riemann problem in a given unit direction n in Rd :\\r\\n\\r\\n                                                                          \\xe2\\x88\\x82t u + \\xe2\\x88\\x82 x f(u) \\xc2\\xb7 n + cn (u)\\xe2\\x88\\x82 x u = 0,                                     (51a)\\r\\n\\r\\nwhere x = x \\xc2\\xb7 n and cn (u) = c(u)n =\\r\\n                                                           Pd\\r\\n                                                             i=1 ni ci (u), together with initial data\\r\\n\\r\\n                                                                                       \\xef\\xa3\\xb2 uL ,     x < 0,\\r\\n                                                                                       \\xef\\xa3\\xb1\\r\\n                                                                              u0 (x) = \\xef\\xa3\\xb4\\r\\n                                                                                       \\xef\\xa3\\xb4\\r\\n                                                                                       \\xef\\xa3\\xb3 uR ,                                                        (51b)\\r\\n                                                                                                  x > 0.\\r\\n\\r\\n    By W( xt , uL , uR , n) we denote the exact entropy weak solution to (51) for t > 0. Following [28], we integrate (51a) over the\\r\\ncontrol volume [\\xe2\\x88\\x92 2h , 2h ] \\xc3\\x97 [0, \\xe2\\x88\\x86t] with h > 0 and \\xe2\\x88\\x86t > 0 the space and time steps, respectively. Using (51b), we obtain\\r\\n                         Z h                \\x12 x                                                          Z \\xe2\\x88\\x86t Z h\\r\\n                                 2\\r\\n                                                             \\x13    h                                             2\\r\\n                                                ; uL , uR , n dx \\xe2\\x88\\x92 (uL + uR ) + \\xe2\\x88\\x86t f(uR ) \\xe2\\x88\\x92 f(uL ) \\xc2\\xb7 n +          cn (u)\\xe2\\x88\\x82 x udxdt = 0.\\r\\n                                                                                                  \\x01\\r\\n                                      W                                                                                                               (52)\\r\\n                               \\xe2\\x88\\x92 h2          \\xe2\\x88\\x86t                   2                                       0     h\\r\\n                                                                                                               \\xe2\\x88\\x922\\r\\n\\r\\n\\r\\n     Note that both \\xce\\x93 and \\xce\\xa0 are continuous across shocks and discontinuous across the intermediate contact wave. Let us introduce\\r\\nthe two last components e\\xce\\x93 = (0, 0, 0, 1, 0)> and e\\xce\\xa0 = (0, 0, 0, 0, 1)> of the canonical basis in Rneq , and define u := v \\xc2\\xb7 n. By (52)\\r\\nand (2) we have\\r\\n         Z \\xe2\\x88\\x86t Z h                                                                                  Z h\\r\\n                    2                                   h                                              2\\r\\n                                                                                                                \\x12 x                \\x13\\r\\n                        cn (u)\\xe2\\x88\\x82 x udxdt =                 (uL + uR ) \\xe2\\x88\\x92 \\xe2\\x88\\x86t f(uR ) \\xe2\\x88\\x92 f(uL ) \\xc2\\xb7 n \\xe2\\x88\\x92                       ; uL , uR , n dx\\r\\n                                                                                         \\x01\\r\\n                                                                                                            W\\r\\n          0     \\xe2\\x88\\x92 h2                                    2                                            \\xe2\\x88\\x92 h2        \\xe2\\x88\\x86t\\r\\n                                                                                                            !                   !\\r\\n                                                      h               h                  h                                    h\\r\\n                                                    =   (\\xce\\x93L + \\xce\\x93R )e\\xce\\x93 + (\\xce\\xa0L + \\xce\\xa0R )e\\xce\\xa0 \\xe2\\x88\\x92      \\xe2\\x88\\x92 u? \\xe2\\x88\\x86t (\\xce\\x93R e\\xce\\x93 + \\xce\\xa0R e\\xce\\xa0 ) \\xe2\\x88\\x92 u? \\xe2\\x88\\x86t +     (\\xce\\x93L e\\xce\\x93 + \\xce\\xa0L e\\xce\\xa0 )\\r\\n                                                      2               2                  2                                    2\\r\\n                                                    = \\xe2\\x88\\x86tu? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 ,\\r\\n                                                           \\x10                          \\x11\\r\\n\\r\\n\\r\\nwhere uL , u? , and uR are the normal velocity components in the left state, the star region, and the right states, respectively. The\\r\\nintegral form for (1a) thus reads\\r\\n                    Z h\\r\\n               1          2\\r\\n                                       \\x12 x                 \\x13      h\\r\\n                                                                     (uL + uR ) + f(uR ) \\xe2\\x88\\x92 f(uL ) \\xc2\\xb7 n + u? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 = 0.\\r\\n                                                                                 \\x10               \\x11        \\x10                           \\x11\\r\\n                                W             ; uL , uR , n dx \\xe2\\x88\\x92                                                                                      (53)\\r\\n               \\xe2\\x88\\x86t       \\xe2\\x88\\x92 h2               \\xe2\\x88\\x86t                    2\\xe2\\x88\\x86t\\r\\n\\r\\n    Likewise integrating (9) in the direction n over the control volume [\\xe2\\x88\\x92 h2 , 2h ] \\xc3\\x97 [0, \\xe2\\x88\\x86t] gives\\r\\n                                           Z h                              \\x13!\\r\\n                                      1        2\\r\\n                                                           \\x12 x                       h \\x10               \\x11\\r\\n                                                    \\xce\\xb7 W        ; uL , uR , n dxdt \\xe2\\x88\\x92     \\xce\\xb7(uL ) + \\xce\\xb7(uR ) + q(uR ) \\xc2\\xb7 n \\xe2\\x88\\x92 q(uL ) \\xc2\\xb7 n 6 0.                (54)\\r\\n                                      \\xe2\\x88\\x86t     \\xe2\\x88\\x92 h2           \\xe2\\x88\\x86t                      2\\xe2\\x88\\x86t\\r\\n\\r\\n\\r\\n5.2. Three-point schemes and the Godunov method\\r\\n     It will be convenient for the analysis of the HLLC solver to consider one-dimensional three-point numerical schemes in\\r\\nfluctuation form [43]\\r\\n                                                  \\xe2\\x88\\x86t \\x10 \\xe2\\x88\\x92 n n\\r\\n                                                      D (U j , U j+1 , n) + D+ (Unj\\xe2\\x88\\x921 , Unj , n) = 0,\\r\\n                                                                                                \\x11\\r\\n                                     Un+1\\r\\n                                       j  \\xe2\\x88\\x92 Unj +                                                                      (55)\\r\\n                                                   h\\r\\nwhere D\\xc2\\xb1 (\\xc2\\xb7, \\xc2\\xb7, \\xc2\\xb7) are assumed to be consistent (27). Here, Unj approximates the cell-averaged solution in the jth cell of size h at time\\r\\nt(n) = n\\xe2\\x88\\x86t. In the Godunov method, the fluctuations are defined by solving exact Riemann problems (51) centered at every interface\\r\\n j + 12 of coordinate x j+ 1 , between cells j and j + 1 (see Figure 2), with Unj and Unj+1 as the left and right initial data, respectively.\\r\\n                             2\\r\\nThe solution at time tn+1 in the jth cell is defined as the cell-average of the exact solution at t(n+1) = t(n) + \\xe2\\x88\\x86t:\\r\\n                         \\xef\\xa3\\xab                                                                                 \\xef\\xa3\\xb6\\r\\n                         \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xacZ x j                                  Z x 1                            \\x13 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                      1                     \\x12 x              \\x13          j+ 2\\r\\n                                                                               \\x12 x\\r\\n             Un+1  = \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                      ; Un , Un , n dx +                 ; Unj , Unj+1 , n dx\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 ,\\r\\n                           \\xef\\xa3\\xac                                                                               \\xef\\xa3\\xb7\\r\\n                                        W                                    W                                                                       (56)\\r\\n               j\\r\\n                      h\\xef\\xa3\\xad x 1                 \\xe2\\x88\\x86t j\\xe2\\x88\\x921 j                xj         \\xe2\\x88\\x86t                         \\xef\\xa3\\xb8\\r\\n                               j\\xe2\\x88\\x92 2\\r\\n                                    \\xef\\xa3\\xab                                                                                                            \\xef\\xa3\\xb6\\r\\n                               \\xe2\\x88\\x86t\\r\\n                                    \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac               Z xj                                                     Z x 1                      \\x13 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                          h         1           \\x12 x              \\x13          h          1          j+ 2\\r\\n                                                                                                                         \\x12 x\\r\\n                   = Unj \\xe2\\x88\\x92                   Un \\xe2\\x88\\x92                   ; Un , Un , n dx +          Un \\xe2\\x88\\x92                         ; Un , Un , n dx\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 ,\\r\\n                                      \\xef\\xa3\\xac                                                                                                          \\xef\\xa3\\xb7\\r\\n                                      \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac                    W                                                         W\\r\\n                                h \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad 2\\xe2\\x88\\x86t j \\xe2\\x88\\x86t x 1               \\xe2\\x88\\x86t j\\xe2\\x88\\x921 j                  2\\xe2\\x88\\x86t j \\xe2\\x88\\x86t x j                   \\xe2\\x88\\x86t j j+1               \\xef\\xa3\\xb8\\r\\n                                                                   j\\xe2\\x88\\x92 2\\r\\n\\r\\n\\r\\n                                                                                           14\\r\\n\\x0c\\n\\nwith x j = x j\\xe2\\x88\\x92 1 + h2 = x j+ 1 \\xe2\\x88\\x92 h2 (see Figure 2), and takes the form (55) with the fluctuations defined as\\r\\n               2               2\\r\\n\\r\\n                                                         Z 0\\r\\n                                            h \\xe2\\x88\\x92 1                \\x12 x              \\x13\\r\\n                       D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) =      u \\xe2\\x88\\x92           W       ; u\\xe2\\x88\\x92 , u+ , n dx                                                                (57a)\\r\\n                                          2\\xe2\\x88\\x86t        \\xe2\\x88\\x86t \\xe2\\x88\\x92 h2      \\xe2\\x88\\x86t\\r\\n                                        = f W(0; u\\xe2\\x88\\x92 , u+ , n) \\xc2\\xb7 n \\xe2\\x88\\x92 f(u\\xe2\\x88\\x92 ) \\xc2\\xb7 n + min(u? , 0) (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 ,\\r\\n                                           \\x10                 \\x11                              \\x10                           \\x11\\r\\n\\r\\n                                                         Z h\\r\\n                                            h + 1          2\\r\\n                                                                 \\x12 x              \\x13\\r\\n                     D+ (u\\xe2\\x88\\x92 , u+ , n) : =      u \\xe2\\x88\\x92           W       ; u\\xe2\\x88\\x92 , u+ , n dx                                                                (57b)\\r\\n                                          2\\xe2\\x88\\x86t        \\xe2\\x88\\x86t 0         \\xe2\\x88\\x86t\\r\\n                                             +\\r\\n                                        = f(u ) \\xc2\\xb7 n \\xe2\\x88\\x92 f W(0; u , u+ , n) \\xc2\\xb7 n + max(u? , 0) (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 .\\r\\n                                                       \\x10                 \\x11                  \\x10                            \\x11\\r\\n                                                                \\xe2\\x88\\x92\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                            h\\r\\n                                                                  x j\\xe2\\x88\\x92 12   xj    x j+ 21\\r\\n                                                                                                             x\\r\\n                                                           j\\xe2\\x88\\x921               j              j+1\\r\\n\\r\\n                                           Figure 2: Notations for the mesh used for the three-point scheme (55).\\r\\n\\r\\n\\r\\n\\r\\n    Note that D\\xc2\\xb1 (\\xc2\\xb7, \\xc2\\xb7, \\xc2\\xb7) in (57) satisfy the following path-conservation property [43]\\r\\n\\r\\n                            D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) + D+ (u\\xe2\\x88\\x92 , u+ , n) = f(u+ ) \\xc2\\xb7 n \\xe2\\x88\\x92 f(u\\xe2\\x88\\x92 ) \\xc2\\xb7 n + u? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + u? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\xa0 ,                       (58)\\r\\n\\r\\nfor a path \\xcf\\x86 : [0, 1] \\xc3\\x97 \\xe2\\x84\\xa6GM \\xc3\\x97 \\xe2\\x84\\xa6GM \\xe2\\x86\\x92 \\xe2\\x84\\xa6GM such that\\r\\n                          Z 1 \\x10                                                                    Z 1 \\x10\\r\\n          u? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L ) =                                                        u? (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L ) =\\r\\n                                              \\x11                                                                       \\x11\\r\\n                              u \\xcf\\x86(s; uL , uR ) \\xe2\\x88\\x82 s \\xcf\\x86(s; uL , uR ) \\xc2\\xb7 e\\xce\\x93 ds,                            u \\xcf\\x86(s; uL , uR ) \\xe2\\x88\\x82 s \\xcf\\x86(s; uL , uR ) \\xc2\\xb7 e\\xce\\xa0 ds.\\r\\n                               0                                                                     0\\r\\n\\r\\n\\r\\n    The interface Riemann problems are assumed to be noninteracting through the definition of a half CFL condition:\\r\\n                                                            \\xe2\\x88\\x86t                               1\\r\\n                                                               max |\\xce\\xbb|max (Unj , Unj+1 , n) 6 ,                                                       (59)\\r\\n                                                            h j\\xe2\\x88\\x88Z                            2\\r\\n\\r\\nwhere |\\xce\\xbb|max (uL , uR , n) is an upper bound of the absolute value of the signal speeds in the Riemann problem (51).\\r\\n    Finally, invoking a Jensen\\xe2\\x80\\x99s inequality in (56) for any convex entropy function in (11), we obtain the following discrete entropy\\r\\ninequality [28] consistent with (9):\\r\\n                                                                    \\xe2\\x88\\x86t \\x10                                        \\x11\\r\\n                                                 j ) 6 \\xce\\xb7(U j ) \\xe2\\x88\\x92\\r\\n                                              \\xce\\xb7(Un+1                     Q(Unj , Unj+1 , n) \\xe2\\x88\\x92 Q(Unj\\xe2\\x88\\x921 , Unj , n) ,                                    (60)\\r\\n                                                                    h\\r\\n\\r\\nwith the consistent entropy flux Q(u\\xe2\\x88\\x92 , u+ , n) = q W(0; u\\xe2\\x88\\x92 , u+ , n) \\xc2\\xb7 n.\\r\\n                                                                     \\x01\\r\\n\\r\\n\\r\\n5.3. HLLC Riemann solver\\r\\n    The HLLC solver [60, 2] is a simple solver [8] with four uniform states separated by simple discontinuities, see Figure 3:\\r\\n                                                                 \\xef\\xa3\\xb1\\r\\n                                                                 \\xef\\xa3\\xb4\\r\\n                                                                 \\xef\\xa3\\xb4\\r\\n                                                                 \\xef\\xa3\\xb4 uL , xt < sL ,\\r\\n                                                                 \\xef\\xa3\\xb2 u?L , sL < xt < s? ,\\r\\n                                                \\x12x           \\x13 \\xef\\xa3\\xb4 \\xef\\xa3\\xb4\\r\\n                                                                 \\xef\\xa3\\xb4\\r\\n                                         W HLLC ; uL , uR , n = \\xef\\xa3\\xb4\\r\\n                                                                 \\xef\\xa3\\xb4\\r\\n                                                                                                                               (61)\\r\\n                                                 t               \\xef\\xa3\\xb4\\r\\n                                                                 \\xef\\xa3\\xb4\\r\\n                                                                 \\xef\\xa3\\xb4 u?R , s? < xt < sR ,\\r\\n                                                                 \\xef\\xa3\\xb3 u R , sR < t ,\\r\\n                                                                 \\xef\\xa3\\xb4\\r\\n                                                                 \\xef\\xa3\\xb4\\r\\n                                                                 \\xef\\xa3\\xb4             x\\r\\n\\r\\n\\r\\n\\r\\nwhere the wave s? approximates the speed of the intermediate contact wave. In contrast to [28], we here require an approximate\\r\\nconsistency of the HLLC solver (61) with the integral form (53). Integrating (61) over [\\xe2\\x88\\x92 h2 , 2h ] at time \\xe2\\x88\\x86t gives\\r\\n                   Z h               \\x12 x                             !                                                   !\\r\\n                       2\\r\\n                                                      \\x13     h                                                          h\\r\\n                            W HLLC       ; uL , uR , n dx =   \\xe2\\x88\\x92 sR \\xe2\\x88\\x86t uR + \\xe2\\x88\\x86t(sR \\xe2\\x88\\x92 s? )u?R + \\xe2\\x88\\x86t(s? \\xe2\\x88\\x92 sL )u?L + sL \\xe2\\x88\\x86t +     uL ,                       (62)\\r\\n                     \\xe2\\x88\\x92 h2             \\xe2\\x88\\x86t                    2                                                          2\\r\\n                                                                             15\\r\\n\\x0c\\n\\n                                                                        t\\r\\n\\r\\n                                  sL                               \\xe2\\x88\\x86t                     s\\xe2\\x88\\x97                  sR\\r\\n\\r\\n\\r\\n                                                            u\\xe2\\x88\\x97L                            u\\xe2\\x88\\x97R\\r\\n\\r\\n                                        uL                                                               uR\\r\\n\\r\\n\\r\\n\\r\\n                                                 \\xce\\x93L , \\xce\\xa0L                                       \\xce\\x93R , \\xce\\xa0R\\r\\n\\r\\n                                                                                                                                x\\r\\n                  \\xe2\\x88\\x92 h2        sL \\xe2\\x88\\x86t                                     0                                      sR \\xe2\\x88\\x86t        h\\r\\n                                                                                                                            2\\r\\n\\r\\n                                                Figure 3: Wave pattern of the HLLC solver (61).\\r\\n\\r\\n\\r\\n\\r\\nand using (53) with W HLLC and s? in place of W and u? , we obtain\\r\\n\\r\\n              sR (u?R \\xe2\\x88\\x92 uR ) + s? (u?L \\xe2\\x88\\x92 u?R ) + sL (uL \\xe2\\x88\\x92 u?L ) + f(uR ) \\xc2\\xb7 n \\xe2\\x88\\x92 f(uL ) \\xc2\\xb7 n + s? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 = 0.\\r\\n                                                                                              \\x10                           \\x11\\r\\n                                                                                                                                    (63)\\r\\n\\r\\n    The component on mass conservation in the above relation gives\\r\\n\\r\\n                                         \\xcf\\x81R (sR \\xe2\\x88\\x92 uR ) + \\xcf\\x81L (uL \\xe2\\x88\\x92 sL ) = \\xcf\\x81?R (sR \\xe2\\x88\\x92 s? ) + \\xcf\\x81?L (s? \\xe2\\x88\\x92 sL ),\\r\\n\\r\\nwhich is satisfied by further requiring the half-consistency conditions [3] which give\\r\\n\\r\\n                            QL = \\xcf\\x81L (uL \\xe2\\x88\\x92 sL ) = \\xcf\\x81?L (s? \\xe2\\x88\\x92 sL ) > 0,             QR = \\xcf\\x81R (sR \\xe2\\x88\\x92 uR ) = \\xcf\\x81?R (sR \\xe2\\x88\\x92 s? ) > 0,           (64)\\r\\n\\r\\nand will be referred to as the mass fluxes [59]. Note that (64) will be shown to be important for the proof of entropy stability\\r\\nin section 5.4.1 are also usually invoked through the satisfaction of some jump relations across the sL and sR waves [60, 2] (see\\r\\nbelow).\\r\\n    From the approximate global consistency relation (63) we also deduce\\r\\n\\r\\n                                         QR (u?R \\xe2\\x88\\x92 uR ) + QL (u?L \\xe2\\x88\\x92 uL ) = pL \\xe2\\x88\\x92 pR ,\\r\\n                                      QR (v\\xe2\\x8a\\xa5R \\xe2\\x88\\x92 v?\\xe2\\x8a\\xa5              ?\\xe2\\x8a\\xa5\\r\\n                                                 R ) + QL (vL \\xe2\\x88\\x92 vL ) = 0,\\r\\n                                                            \\xe2\\x8a\\xa5\\r\\n\\r\\n                                       QR (ER? \\xe2\\x88\\x92 ER ) + QL (E L? \\xe2\\x88\\x92 E L ) = pL uL \\xe2\\x88\\x92 pR uR ,\\r\\n                                          sR (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93?R ) + sL (\\xce\\x93?L \\xe2\\x88\\x92 \\xce\\x93L ) = s? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93?R ) + s? (\\xce\\x93?L \\xe2\\x88\\x92 \\xce\\x93L ),\\r\\n                                        sR (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0?R ) + sL (\\xce\\xa0?L \\xe2\\x88\\x92 \\xce\\xa0L ) = s? (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0?R ) + s? (\\xce\\xa0?L \\xe2\\x88\\x92 \\xce\\xa0L ),\\r\\n\\r\\n\\r\\n\\r\\nwhere v\\xe2\\x8a\\xa5 = v \\xe2\\x88\\x92 un denotes the velocity component perpendicular to n. The second and two latter conditions impose\\r\\n\\r\\n                              v?\\xe2\\x8a\\xa5\\r\\n                               L = vL ,\\r\\n                                    \\xe2\\x8a\\xa5\\r\\n                                               v?\\xe2\\x8a\\xa5\\r\\n                                                R = vR ,\\r\\n                                                     \\xe2\\x8a\\xa5\\r\\n                                                              \\xce\\x93?L = \\xce\\x93L ,         \\xce\\x93?R = \\xce\\x93R ,      \\xce\\xa0?L = \\xce\\xa0L ,   \\xce\\xa0?R = \\xce\\xa0R ,            (65)\\r\\n\\r\\nmeaning that v\\xe2\\x8a\\xa5 , \\xce\\x93, and \\xce\\xa0 are continuous across shocks and may be discontinuous across s? in agreement with the fact that v\\xe2\\x8a\\xa5 , \\xce\\x93,\\r\\nand \\xce\\xa0 are associated to LD fields. The remaining unknowns can be computed by imposing the Rankine-Hugoniot relations across\\r\\nsL and sR\\r\\n\\r\\n                                       f?L \\xe2\\x88\\x92 f(uL ) \\xc2\\xb7 n = sL (u?L \\xe2\\x88\\x92 uL ),        f?R \\xe2\\x88\\x92 f(uR ) \\xc2\\xb7 n = sR (u?R \\xe2\\x88\\x92 uR ),                 (66)\\r\\n\\r\\n\\r\\n                                                                            16\\r\\n\\x0c\\n\\n        ?\\r\\nwhere fX=L,R means that we are considering p?X as an unknown instead of evaluating the pressure via the EOS (5) and u?X . This leads\\r\\nto the following definition of the velocity in the star regions:\\r\\n\\r\\n                                                                p?L \\xe2\\x88\\x92 pL                            p?R \\xe2\\x88\\x92 pR\\r\\n                                                   u?L = uL \\xe2\\x88\\x92            ,        u?R = uR +                 ,                                 (67)\\r\\n                                                                   QL                                  QR\\r\\nand to the following jump relation across the s? wave:\\r\\n\\r\\n                                            fR? \\xe2\\x88\\x92 fL? = s? u?R \\xe2\\x88\\x92 u?L \\xe2\\x88\\x92 (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 \\xe2\\x88\\x92 (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 .\\r\\n                                                          \\x10                                       \\x11\\r\\n\\r\\n    Further imposing continuity of the velocity and the pressure across the intermediate wave, u?L = u?R = s? and p?L = p?R = p? ,\\r\\ngives the expression for the pressure and velocity in the star region:\\r\\n\\r\\n                                         QL pR + QR pL + QR QL (uL \\xe2\\x88\\x92 uR )                          QL uL + QR uR + pL \\xe2\\x88\\x92 pR\\r\\n                                  p? =                                    ,               s? =                             .                   (68)\\r\\n                                                    Q L + QR                                              Q L + QR\\r\\n    The other states are then directly obtained:\\r\\n\\r\\n\\r\\n                                                      s? \\xe2\\x88\\x92 u L\\r\\n                                                                    !                                                               !\\r\\n                uL \\xe2\\x88\\x92 sL                                          pL                           1                                  pL\\r\\n        \\xcf\\x81?L =            \\xcf\\x81L ,   e?L = eL + (s? \\xe2\\x88\\x92 uL )          \\xe2\\x88\\x92      ,           E L? = e?L + v?L \\xc2\\xb7 v?L = E L + (s? \\xe2\\x88\\x92 uL ) s? \\xe2\\x88\\x92      ,       (69a)\\r\\n                s? \\xe2\\x88\\x92 s L                                 2       QL                           2                                  QL\\r\\n                                                      s? \\xe2\\x88\\x92 uR\\r\\n                                                                    !                                                               !\\r\\n                sR \\xe2\\x88\\x92 u R                                         pR                           1                                  pR\\r\\n        \\xcf\\x81?R =            \\xcf\\x81R ,   e?R = eR + (s? \\xe2\\x88\\x92 uR )          +       ,          ER? = e?R + v?R \\xc2\\xb7 v?R = ER + (s? \\xe2\\x88\\x92 uR ) s? +         .      (69b)\\r\\n                sR \\xe2\\x88\\x92 s?                                  2       QR                           2                                  QR\\r\\n\\r\\n    As for the Godunov method in (57), we now define the fluctuations through\\r\\n\\r\\n                                                                              Z 0\\r\\n                                                               h       1                           \\x12 x                \\x13\\r\\n                                         D\\xe2\\x88\\x92 (uL , uR , n) =       uL \\xe2\\x88\\x92                    W HLLC         ; uL , uR , n dx,                    (70a)\\r\\n                                                              2\\xe2\\x88\\x86t      \\xe2\\x88\\x86t         \\xe2\\x88\\x92 h2              \\xe2\\x88\\x86t\\r\\n                                                                              Z h\\r\\n                                                               h       1              2\\r\\n                                                                                                   \\x12 x                \\x13\\r\\n                                         D+ (uL , uR , n) =       uR \\xe2\\x88\\x92                    W HLLC         ; uL , uR , n dx,                    (70b)\\r\\n                                                              2\\xe2\\x88\\x86t      \\xe2\\x88\\x86t         0                 \\xe2\\x88\\x86t\\r\\n\\r\\nand plugging (61) into (70) gives\\r\\n                                          \\xef\\xa3\\xb1\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4 0,                                                                               0 < sL ,\\r\\n                                          \\xef\\xa3\\xb2 sL (u?L \\xe2\\x88\\x92 uL ),                                                                  s L < 0 < s? ,\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                       D (uL , uR , n) = \\xef\\xa3\\xb4\\r\\n                        \\xe2\\x88\\x92\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                                                                                                                              (71a)\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4 f?R \\xe2\\x88\\x92 f(uL ) \\xc2\\xb7 n + s? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + s? (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 ,                           s? < 0 < sR ,\\r\\n                                          \\xef\\xa3\\xb3 f(uR ) \\xc2\\xb7 n \\xe2\\x88\\x92 f(uL ) \\xc2\\xb7 n + s? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + s? (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 ,                    sR < 0,\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb1\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4 f(uR ) \\xc2\\xb7 n \\xe2\\x88\\x92 f(uL ) \\xc2\\xb7 n + s? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + s? (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 ,                    0 < sL ,\\r\\n                                          \\xef\\xa3\\xb2 f(uR ) \\xc2\\xb7 n \\xe2\\x88\\x92 f?R + s? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + s? (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0 ,                           s L < 0 < s? ,\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                       D+ (uL , uR , n) = \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                                                                                                                              (71b)\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4 sR (uR \\xe2\\x88\\x92 u?R ),                                                                  s? < 0 < sR ,\\r\\n                                                                                                                             sR < 0.\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb3 0,\\r\\n                                          \\xef\\xa3\\xb4\\r\\n                                          \\xef\\xa3\\xb4\\r\\n\\r\\n    Note that by construction the numerical fluxes (71) satisfy the relation\\r\\n\\r\\n                          D\\xe2\\x88\\x92 (uL , uR , n) + D+ (uL , uR , n) = f(uR ) \\xc2\\xb7 n \\xe2\\x88\\x92 f(uL ) \\xc2\\xb7 n + s? (\\xce\\x93R \\xe2\\x88\\x92 \\xce\\x93L )e\\xce\\x93 + s\\xe2\\x88\\x97 (\\xce\\xa0R \\xe2\\x88\\x92 \\xce\\xa0L )e\\xce\\xa0\\r\\n\\r\\nsimilar to the path-conservation property (58) [43]. Note that for (58) to hold, one needs s? = u? which would require a root-finding\\r\\nalgorithm to evaluate u? . We here follow another strategy where we approximate u? by s? in (68), this is justified by the fact that\\r\\nthe nonconservative product is here associated to a LD field.\\r\\n    Finally, we define the updated cell-averaged solution as\\r\\n                                  1 xj                                        1 x j+ 12\\r\\n                                    Z             \\x12 x                  \\x13       Z               \\x12 x                  \\x13\\r\\n                         U(n+1) =         W  HLLC\\r\\n                                                      ; U n\\r\\n                                                              , U n\\r\\n                                                                    , n  dx +           W HLLC     ; Unj , Unj+1 , n dx,                       (72)\\r\\n                           j\\r\\n                                  h x 1            \\xe2\\x88\\x86t     j\\xe2\\x88\\x921     j\\r\\n                                                                              h xj              \\xe2\\x88\\x86t\\r\\n                                            j\\xe2\\x88\\x92 2\\r\\n\\r\\n\\r\\n\\r\\nso the HLLC solver may be recast into the three-point scheme form (55) with (71).\\r\\n\\r\\n                                                                             17\\r\\n\\x0c\\n\\n5.4. Properties of the HLLC solver\\r\\n     In this section, we analyse the properties of the numerical scheme (55) using fluxes (71), where the time step \\xe2\\x88\\x86t > 0 is assumed\\r\\nto satisfy the CFL condition\\r\\n                                           \\xe2\\x88\\x86t       \\x12                                         \\x13 1\\r\\n                                               max sR (Unj , Unj+1 , n) , sL (Unj , Unj+1 , n) 6 ,                                (73)\\r\\n                                            h j\\xe2\\x88\\x88Z                                               2\\r\\nwhere the wave speeds sL and sR are defined in section 5.4.4.\\r\\n\\r\\n\\r\\n5.4.1. Discrete entropy inequality\\r\\n     We are here interested in the nonlinear stability of the scheme (55) and follow [3] to use the local entropy minimum principles\\r\\nand first prove in Theorem 5.1 the entropy inequality in integral form (60) for the HLLC solution (61). We then prove in Lemma 5.1\\r\\nthat the local entropy minimum principles hold for the intermediate states.\\r\\n\\r\\nTheorem 5.1. Suppose that condition (73) on the time step holds and that the intermediate states in the HLLC solver (61), satisfy\\r\\nu?L , u?R \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM together with the following local minimum entropy principles\\r\\n\\r\\n                                                           s(u?L ) > s(uL ),        s(u?R ) > s(uR ),                                              (74)\\r\\n\\r\\nfor the specific entropy (12). Then, the three-point scheme (55) satisfies an entropy inequality (60) with the consistent numerical\\r\\nflux                                                          Z xj\\r\\n                                                            1               \\x12 x                  \\x13!     h \\x10 n\\x11\\r\\n                         Q(Unj\\xe2\\x88\\x921 , Unj , n) = q(Unj ) \\xc2\\xb7 n +        \\xce\\xb7 W HLLC     ; Unj\\xe2\\x88\\x921 , Unj , n dx \\xe2\\x88\\x92     \\xce\\xb7 Uj .              (75)\\r\\n                                                            \\xe2\\x88\\x86t x 1           \\xe2\\x88\\x86t                        2\\xe2\\x88\\x86t\\r\\n                                                                      j\\xe2\\x88\\x92 2\\r\\n\\r\\n\\r\\n\\r\\nProof. We first prove the entropy inequality in integral form (54) using (62), therefore, we have\\r\\n Z h                 \\x12 x              \\x13!                 !                                                                !\\r\\n     2                                                 h                                                        h\\r\\n          \\xce\\xb7 W HLLC       ; uL , uR , n dx = sL \\xe2\\x88\\x86t +        \\xce\\xb7(uL ) + (s? \\xe2\\x88\\x92 sL )\\xe2\\x88\\x86t\\xce\\xb7(u?L ) + (sR \\xe2\\x88\\x92 s? )\\xe2\\x88\\x86t\\xce\\xb7(u?R ) +    \\xe2\\x88\\x92 sR \\xe2\\x88\\x86t \\xce\\xb7(uR ),\\r\\n   \\xe2\\x88\\x92 h2               \\xe2\\x88\\x86t                               2                                                        2\\r\\n                                          (11) h \\x10\\r\\n                                                  \\xce\\xb7(uL ) + \\xce\\xb7(uR ) \\xe2\\x88\\x92 \\xe2\\x88\\x86t sL \\xcf\\x81L s(uL ) + (s? \\xe2\\x88\\x92 sL )\\xcf\\x81?L s(u?L ) \\xe2\\x88\\x92 \\xe2\\x88\\x86t (sR \\xe2\\x88\\x92 s? )\\xcf\\x81?R s(u?R ) \\xe2\\x88\\x92 sR \\xcf\\x81R s(uR ) ,\\r\\n                                                                  \\x11      \\x10                                 \\x11    \\x10                                    \\x11\\r\\n                                           =\\r\\n                                               2\\r\\n                                          (74) h \\x10\\r\\n                                                  \\xce\\xb7(uL ) + \\xce\\xb7(uR ) \\xe2\\x88\\x92 \\xe2\\x88\\x86ts(uL ) sL \\xcf\\x81L + (s? \\xe2\\x88\\x92 sL )\\xcf\\x81?L \\xe2\\x88\\x92 \\xe2\\x88\\x86t (sR \\xe2\\x88\\x92 s? )\\xcf\\x81?R \\xe2\\x88\\x92 sR \\xcf\\x81R s(uR ),\\r\\n                                                                  \\x11           \\x10                     \\x11        \\x10                   \\x11\\r\\n                                           6\\r\\n                                               2\\r\\n                                              h\\x10                \\x11      \\x10                         \\x11\\r\\n                                          = \\xce\\xb7(uL ) + \\xce\\xb7(uR ) \\xe2\\x88\\x92 \\xe2\\x88\\x86t \\xcf\\x81R vR s(uR ) \\xe2\\x88\\x92 \\xcf\\x81L vL s(uL ) \\xc2\\xb7 n,\\r\\n                                              2\\r\\nwhere we have used the half-consistency conditions (64). As a consequence, setting uL = Unj and uR = Unj+1 , the numerical flux\\r\\n(75) satisfies\\r\\n                                                         1 x j+ 12                               \\x13!\\r\\n                                                          Z                 \\x12 x                         h\\r\\n                      Q(Unj , Unj+1 , n) 6 q(Unj ) \\xc2\\xb7 n \\xe2\\x88\\x92           \\xce\\xb7 W HLLC     ; Unj , Unj+1 , n dx +     \\xce\\xb7(Unj ),        (76)\\r\\n                                                         h xj                \\xe2\\x88\\x86t                        2\\xe2\\x88\\x86t\\r\\n\\r\\nand using (72) we have the following relation through Jensen\\xe2\\x80\\x99s inequality for the convex entropy function (11)\\r\\n\\r\\n                        1 xj                                     \\x13!         1 x j+ 21                             \\x13!\\r\\n                          Z                   \\x12 x                             Z                \\x12 x\\r\\n            \\xce\\xb7(U j ) 6\\r\\n               n+1\\r\\n                                  \\xce\\xb7 W   HLLC\\r\\n                                                  ; U , U , n dx +\\r\\n                                                       n     n\\r\\n                                                                                      \\xce\\xb7 W HLLC\\r\\n                                                                                                    ; U , U , n dx,\\r\\n                                                                                                         n   n\\r\\n                        h x 1                  \\xe2\\x88\\x86t j\\xe2\\x88\\x921 j                     h xj                 \\xe2\\x88\\x86t j j+1\\r\\n                             j\\xe2\\x88\\x92 2\\r\\n\\r\\n                    (75) \\xe2\\x88\\x86t                                                         \\xe2\\x88\\x86t\\r\\n                                                                                !                                                      !\\r\\n                                                                    h                                                        h\\r\\n                     6        Q(Unj\\xe2\\x88\\x921 , Unj , n) \\xe2\\x88\\x92 q(Unj ) \\xc2\\xb7 n +      \\xce\\xb7(Unj )) +        \\xe2\\x88\\x92Q(Unj , Unj+1 , n) + q(Unj ) \\xc2\\xb7 n +     \\xce\\xb7(Unj ) ,\\r\\n                    (76) h                                        2\\xe2\\x88\\x86t                h                                      2\\xe2\\x88\\x86t\\r\\n                                  \\xe2\\x88\\x86t \\x10                                        \\x11\\r\\n                    = \\xce\\xb7(Unj ) \\xe2\\x88\\x92        Q(Unj , Unj+1 , n) \\xe2\\x88\\x92 Q(Unj\\xe2\\x88\\x921 , Unj , n) .\\r\\n                                  h\\r\\n\\r\\n\\r\\nLemma 5.1. There exist wave speed estimates sL and sR large enough that: (i) bound the minimum and maximum wave speeds in\\r\\nthe exact entropy weak solution of the Riemann problem (51), (ii) satisfy the interlacing condition sL < s? < sR , (iii) ensure that\\r\\nthe local minimum entropy principles (74) hold.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                               18\\r\\n\\x0c\\n\\nProof. The first result (i) is obvious under the assumption that the wave speeds are finite in the exact Riemann solution. Then,\\r\\n(ii) and (iii) are consequences of [3, Prop. 3.2] and we now show that the required assumptions on the half domains separated by\\r\\n x\\r\\n t\\r\\n   = s? hold, see [3, after Prop. 3.2]. This is justified here because the EOS (5) does not change accross the extreme waves sL and\\r\\nsR , hence xt = s? therefore separates domains with one unique equivalent \\xe2\\x80\\x9cpure phase\\xe2\\x80\\x9d and its associated physical entropy (12)\\r\\nwhich is strictly convex and satisfies the Gibbs principle (13). The required conditions across the extreme waves are: first, the\\r\\nhalf-consistency relations hold through (64); then, the following quantities must be invariant across the sL and sR waves [3, \\xc2\\xa7 4.2]:\\r\\n                                            Q2X       Q2X               (p?X )2        p2\\r\\n                                    p?X +    ? = pX +     ,     e?X \\xe2\\x88\\x92           = eX \\xe2\\x88\\x92 X2 ,       X = L, R.\\r\\n                                            \\xcf\\x81X        \\xcf\\x81X                2QX  2\\r\\n                                                                                      2QX\\r\\n\\r\\n    The first relation is a direct consequence of (67), with u?L = u?R = s? :\\r\\n                                            p?L = pL + QL (uL \\xe2\\x88\\x92 s? ),        p?R = pR + QR (s? \\xe2\\x88\\x92 uR ),\\r\\n\\r\\nand (64). For the second relation, we inject the above relations in the expression of e?X=L,R in (69) to get\\r\\n                                            pX \\xe2\\x88\\x92 p?X \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac pX \\xe2\\x88\\x92 p?X pX \\xe2\\x88\\x92 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7     (p? )2 \\xe2\\x88\\x92 p2X\\r\\n                                                       \\xef\\xa3\\xab                 \\xef\\xa3\\xb6\\r\\n                                e?X \\xe2\\x88\\x92 pX =             \\xef\\xa3\\xac\\xef\\xa3\\xad        \\xe2\\x88\\x92       \\xef\\xa3\\xb7\\xef\\xa3\\xb8 = \\xe2\\x88\\x92 X           , X = L, R,\\r\\n                                               QX         2QX      QX             2QX\\r\\n\\r\\nwhich concludes the proof.\\r\\n\\r\\n    We finally link the discrete entropy inequality (54) to the entropy stable character of the numerical fluctuations [9], so that\\r\\nthe HLLC fluxes can be used at the interfaces in the DGSEM scheme to prove the semi-discrete entropy inequality established in\\r\\nTheorem 3.1.\\r\\nCorollary 5.1. Let a three-point scheme of the form (55) to discretize (1a). Then, the entropy inequality (60) implies the entropy\\r\\nstability of the numerical fluxes in the sense of (36).\\r\\n\\r\\nProof. The proof relies on similar arguments as the ones used in [5, Lemma 2.8] in the conservative setting. Let Unj\\xe2\\x88\\x921 = Unj = u\\xe2\\x88\\x92\\r\\nand Unj+1 = u+ , then from (55), we obtain Un+1\\r\\n                                            j   = u\\xe2\\x88\\x92 \\xe2\\x88\\x92 \\xe2\\x88\\x86th D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) and (60) gives\\r\\n                                         \\xe2\\x88\\x86t                              \\xe2\\x88\\x86t \\x10\\r\\n                                                          !\\r\\n                                 \\xce\\xb7 u\\xe2\\x88\\x92 \\xe2\\x88\\x92 D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) 6 \\xce\\xb7(u\\xe2\\x88\\x92 ) \\xe2\\x88\\x92           Q(u\\xe2\\x88\\x92 , u+ , n) \\xe2\\x88\\x92 q(u\\xe2\\x88\\x92 ) \\xc2\\xb7 n .\\r\\n                                                                                                         \\x11\\r\\n                                         h                                h\\r\\nLikewise, using Unj\\xe2\\x88\\x921 = u\\xe2\\x88\\x92 and Unj = Unj+1 = u+ , we get Un+1\\r\\n                                                           j  = u+ \\xe2\\x88\\x92 D+ (u\\xe2\\x88\\x92 , u+ , n) and\\r\\n                                        \\xe2\\x88\\x86t                           \\xe2\\x88\\x86t \\x10 +\\r\\n                                                         !\\r\\n                               \\xce\\xb7 u+ \\xe2\\x88\\x92 D+ (u\\xe2\\x88\\x92 , u+ , n) 6 \\xce\\xb7(u+ ) \\xe2\\x88\\x92        q(u ) \\xc2\\xb7 n \\xe2\\x88\\x92 Q(u\\xe2\\x88\\x92 , u+ , n) .\\r\\n                                                                                                   \\x11\\r\\n                                         h                           h\\r\\nSumming both equations and letting \\xe2\\x88\\x86t \\xe2\\x86\\x92 0+ with fixed h, we have up to O(\\xe2\\x88\\x86t):\\r\\n                   \\xe2\\x88\\x86t                                     \\xe2\\x88\\x86t                                             \\xe2\\x88\\x86t \\x10 +\\r\\n                      \\xcf\\x91(u\\xe2\\x88\\x92 ) \\xc2\\xb7 D\\xe2\\x88\\x92 (u\\xe2\\x88\\x92 , u+ , n) + \\xce\\xb7(u+ ) \\xe2\\x88\\x92 \\xcf\\x91(u+ ) \\xc2\\xb7 D+ (u\\xe2\\x88\\x92 , u+ , n) 6 \\xce\\xb7(u\\xe2\\x88\\x92 ) + \\xce\\xb7(u+ ) \\xe2\\x88\\x92\\r\\n                                                                                                                           \\x11\\r\\n          \\xce\\xb7(u\\xe2\\x88\\x92 ) \\xe2\\x88\\x92                                                                                           q(u ) \\xe2\\x88\\x92 q(u\\xe2\\x88\\x92 ) \\xc2\\xb7 n,\\r\\n                   h                                      h                                              h\\r\\nand simplifying terms gives (36).\\r\\n\\r\\n    Finally, as an immediate consequence of the local minimum entropy principles (74) and the definition of the updated solution\\r\\n(72), the HLLC solver also satisfies a discrete minimum principle on the specific physical entropy s(u):\\r\\n                                                             \\x10                             \\x11\\r\\n                                                    j ) > min s(U j\\xe2\\x88\\x921 ), s(U j ), s(U j+1 ) .\\r\\n                                                 s(Un+1          n          n        n\\r\\n                                                                                                                                   (77)\\r\\n\\r\\n5.4.2. Preservation of material interfaces and pure phases\\r\\n    We, first, prove that the three-point scheme (55), with numerical fluxes (71), preserves material interfaces [1] by following the\\r\\nsame analysis as in section 4.1. Let us assume that the left and right states satisfy vL = vR = v and pL = pR = p, then (68) gives\\r\\np? = p and s? = u, respectively. As a result, the discrete requirements dv\\xcf\\x81 = vd\\xcf\\x81 and d\\xcf\\x81E = ( 12 v \\xc2\\xb7 v)d\\xcf\\x81 + pd\\xce\\x93 + d\\xce\\xa0 applied to the\\r\\nthree-point scheme (55) impose\\r\\n                                                                   v\\xc2\\xb7v \\xc2\\xb1\\r\\n                                             D\\xc2\\xb1\\xcf\\x81u = uD\\xc2\\xb1\\xcf\\x81 , D\\xc2\\xb1\\xcf\\x81E =       D + pD\\xc2\\xb1\\xce\\x93 + D\\xc2\\xb1\\xce\\xa0 ,                                         (78)\\r\\n                                                                    2 \\xcf\\x81\\r\\nand are obviously satisfied. Now assume that \\xce\\x93L = \\xce\\x93R and \\xce\\xa0L = \\xce\\xa0R , then D\\xc2\\xb1\\xce\\x93 = 0 and D\\xc2\\xb1\\xce\\xa0 = 0 and the fluxes reduce to the\\r\\nconservative HLLC solver for the Euler equations so pure phases are preserved.\\r\\n\\r\\n                                                                        19\\r\\n\\x0c\\n\\n5.4.3. Positivity of the solution\\r\\n    Since the updated solution (72) is the cell-average of the superposition of approximate Riemann solutions from the HLLC\\r\\nsolver (61), and assuming that the left and right states are positive, it is sufficient to prove positivity of the intermediate states in the\\r\\n                                                                                                                         ?\\r\\nRiemann solution [20]. Note that the intermediate states should be evaluated from the intermediate fluxes [2], fX=L,R          in (66) since\\r\\nwe are imposing the pressure via (68), and not evaluating it from the EOS (5), see section 5.3.\\r\\n     The proof for positivity of density in the star region can be directly stated following its definition in (69) since sL < uL , uR < sR\\r\\nfrom the wave estimates in section 5.4.4, and the interlacing property sL < s? < sR in Lemma 5.1. According to (7) hyperbolicity\\r\\nof (1a) and positivity of the solution in the star region require satisfying \\xcf\\x81?X e?X > p\\xe2\\x88\\x9eX , X = L, R, which gives for the left intermediate\\r\\nstate:\\r\\n                  \\xef\\xa3\\xac\\xef\\xa3\\xac ? (s? )2 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7                                                                      (s? )2 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                  \\xef\\xa3\\xab             \\xef\\xa3\\xb6                 \\xef\\xa3\\xab                                                             \\xef\\xa3\\xb6\\r\\n               ?\\xef\\xa3\\xac                         (69) ? \\xef\\xa3\\xac        ?  ?             ?             pL\\r\\n              \\xcf\\x81L \\xef\\xa3\\xad\\xef\\xa3\\xacE L \\xe2\\x88\\x92                  \\xe2\\x87\\x94 \\xcf\\x81L \\xef\\xa3\\xad\\xef\\xa3\\xacE L + s (s \\xe2\\x88\\x92 uL ) \\xe2\\x88\\x92 (s \\xe2\\x88\\x92 uL )\\r\\n                                \\xef\\xa3\\xb8\\xef\\xa3\\xb7 > p\\xe2\\x88\\x9eL (64)                                                        \\xe2\\x88\\x92          \\xef\\xa3\\xb8\\xef\\xa3\\xb7 > p\\xe2\\x88\\x9eL\\r\\n                                                  \\xef\\xa3\\xac\\r\\n                                                  \\xef\\xa3\\xac\\r\\n                           2                                                       \\xcf\\x81L (sL \\xe2\\x88\\x92 uL )         2\\r\\n                                                        (uL \\xe2\\x88\\x92 s? )2\\r\\n                                                  \\xef\\xa3\\xab                                             \\xef\\xa3\\xb6\\r\\n                                                                                       pL\\r\\n                                           \\xe2\\x87\\x94\\xcf\\x81?L \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xadeL +              \\xe2\\x88\\x92 (s? \\xe2\\x88\\x92 uL )                 \\xef\\xa3\\xb7\\xef\\xa3\\xb8 > p\\xe2\\x88\\x9eL\\r\\n                                                  \\xef\\xa3\\xac\\xef\\xa3\\xac                                            \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                             2                    \\xcf\\x81L (uL \\xe2\\x88\\x92 sL )\\r\\n                                                            pL + \\xce\\xb3L p\\xe2\\x88\\x9eL       uL \\xe2\\x88\\x92 sL \\xcf\\x81L (uL \\xe2\\x88\\x92 s? )2               s? \\xe2\\x88\\x92 uL\\r\\n                                                        !                !             !                                   !\\r\\n                                           (5) uL \\xe2\\x88\\x92 sL\\r\\n                                          \\xe2\\x87\\x94 ?                              + ?                             \\xe2\\x88\\x92 ?               pL \\xe2\\x88\\x92 p\\xe2\\x88\\x9eL > 0\\r\\n                                          (64) s \\xe2\\x88\\x92 sL         \\xce\\xb3L \\xe2\\x88\\x92 1          s \\xe2\\x88\\x92 sL               2               s \\xe2\\x88\\x92 sL\\r\\n                                       s? >sL             pL + \\xce\\xb3L p\\xe2\\x88\\x9eL                 \\xcf\\x81L\\r\\n                                                                       !\\r\\n                                        \\xe2\\x87\\x94 (uL \\xe2\\x88\\x92 sL )                     + (uL \\xe2\\x88\\x92 sL ) \\xcf\\x832 + \\xcf\\x83pL \\xe2\\x88\\x92 (s? \\xe2\\x88\\x92 sL )p\\xe2\\x88\\x9eL > 0\\r\\n                                                             \\xce\\xb3L \\xe2\\x88\\x92 1                    2\\r\\n                                                        \\xcf\\x81L 2                                   pL + p\\xe2\\x88\\x9eL\\r\\n                                                                                                            !\\r\\n                                           \\xe2\\x87\\x94(uL \\xe2\\x88\\x92 sL ) \\xcf\\x83 + (pL + p\\xe2\\x88\\x9eL )\\xcf\\x83 + (uL \\xe2\\x88\\x92 sL )                              > 0,\\r\\n                                                         2                                          \\xce\\xb3L \\xe2\\x88\\x92 1\\r\\n\\r\\nwhere \\xcf\\x83 = uL \\xe2\\x88\\x92 s? . The sign for this inequality holds for all \\xcf\\x83 \\xe2\\x88\\x88 R if the discriminant D of the above quadratic equation is\\r\\nnegative:\\r\\n                                                                                 pL + p\\xe2\\x88\\x9eL\\r\\n                                                                                           !\\r\\n                                             D = (pL + p\\xe2\\x88\\x9eL )2 \\xe2\\x88\\x92 2\\xcf\\x81L (uL \\xe2\\x88\\x92 sL )2\\r\\n                                                                                   \\xce\\xb3L \\xe2\\x88\\x92 1\\r\\n\\r\\n    Using \\xce\\xb3L (pL + p\\xe2\\x88\\x9eL ) = \\xcf\\x81L c2L , D < 0 implies sL < uL \\xe2\\x88\\x92 (\\xce\\xb3L \\xe2\\x88\\x92 1)/2\\xce\\xb3L cL which is satisfied by the wave speed estimates in\\r\\n                                                                  p\\r\\n\\r\\nsection 5.4.4 since \\xce\\xb32\\xce\\xb3\\r\\n                     L \\xe2\\x88\\x921\\r\\n                        L\\r\\n                          < 1. A similar result holds for the right intermediate state. Note that the above relations are the same as in\\r\\n[2] with pX + p\\xe2\\x88\\x9eX instead of pX .\\r\\n     Finally, let prove positivity of \\xce\\x93 and \\xce\\xa0 through a discrete maximum principle. The discrete equation for X \\xe2\\x88\\x88 {\\xce\\x93, \\xce\\xa0} in (55) with\\r\\nfluctuations (71) reads\\r\\n                                  \\xe2\\x88\\x86t \\x12                 \\x10             \\x11                   \\x10             \\x11\\x13\\r\\n                    X n+1\\r\\n                      j   = X n\\r\\n                              j \\xe2\\x88\\x92      min(s \\xe2\\x88\\x97\\r\\n                                                1\\r\\n                                             j+ 2\\r\\n                                                  , 0)   X n\\r\\n                                                           j+1 \\xe2\\x88\\x92 X n\\r\\n                                                                   j   + max(s \\xe2\\x88\\x97\\r\\n                                                                                  1\\r\\n                                                                               j\\xe2\\x88\\x92 2\\r\\n                                                                                    , 0)   X n\\r\\n                                                                                             j\\xe2\\x88\\x921 \\xe2\\x88\\x92 X n\\r\\n                                                                                                     j\\r\\n                                   h\\r\\n                                  \\xe2\\x88\\x86t \\x12                                   \\x13!       \\xe2\\x88\\x86t                           \\xe2\\x88\\x86t\\r\\n                          = 1\\xe2\\x88\\x92         max(s\\xe2\\x88\\x97j\\xe2\\x88\\x92 1 , 0) \\xe2\\x88\\x92 min(s\\xe2\\x88\\x97j+ 1 , 0) X nj \\xe2\\x88\\x92        min(s\\xe2\\x88\\x97j+ 1 , 0)X nj+1 +    max(s\\xe2\\x88\\x97j\\xe2\\x88\\x92 1 , 0)X nj\\xe2\\x88\\x921 (79)\\r\\n                                  h             2                   2              h             2             h           2\\r\\n\\r\\n\\r\\n\\r\\nwhich shows that X n+1\\r\\n                   j\\r\\n                                                       n\\r\\n                       is a convex combination of the Xi= j\\xc2\\xb11, j under the CFL condition (73).\\r\\n\\r\\n\\r\\n\\r\\n5.4.4. Wave speed estimates\\r\\n     The fan of waves of the HLLC solver must contain the fan of waves of the exact Riemann problem (51). This is in particular\\r\\nrequired to ensure the local entropy minimum principles (74), see [3, Prop. 3.2]. Direct wave speed estimates have been proposed\\r\\nthat do not require to solve the exact Riemann problem [57, 5]. Note that the usual estimate S R = \\xe2\\x88\\x92S L = max(|uL | + cL , |uR | + cR )\\r\\nmay be wrong due to the Lax entropy condition across a shock. On the other hand, the time steps can be affected by overestimating\\r\\nwave speed estimates through (73). We propose the following wave speeds estimates\\r\\n\\r\\n                                                        sL = uL \\xe2\\x88\\x92 c\\xcc\\x83L ,        sR = uR + c\\xcc\\x83R ,                                              (80)\\r\\n\\r\\nwhere uX = vX \\xc2\\xb7 n, X = L, R, and\\r\\n                       \\xef\\xa3\\xb1                      \\x12                      \\x13                   \\xef\\xa3\\xb1                     \\x12                      \\x13\\r\\n                                      \\xce\\xb3+1                                                              \\xce\\xb3+1\\r\\n                              =     +           pR \\xe2\\x88\\x92pL\\r\\n                                                       +          ,     ,                      =     +           pL \\xe2\\x88\\x92pR\\r\\n                                                                                                                        +          ,    ,\\r\\n                       \\xef\\xa3\\xb4                                                                 \\xef\\xa3\\xb4\\r\\n                         c\\xcc\\x83     c         max            u   \\xe2\\x88\\x92 u    0                      c\\xcc\\x83    c         max            u   \\xe2\\x88\\x92 u    0\\r\\n                       \\xef\\xa3\\xb4\\r\\n                       \\xef\\xa3\\xb4                                                                 \\xef\\xa3\\xb4\\r\\n                                                                                         \\xef\\xa3\\xb4\\r\\n                            L     L                        L    R                            R     R                        L    R\\r\\n                                              \\x12 \\xcf\\x81R cR                                                          \\x12 \\xcf\\x81L cL\\r\\n                       \\xef\\xa3\\xb4\\r\\n                       \\xef\\xa3\\xb4               2\\r\\n                                                                                         \\xef\\xa3\\xb4\\r\\n                                                                                         \\xef\\xa3\\xb4              2\\r\\n          if pR > pL : \\xef\\xa3\\xb4                                                          else : \\xef\\xa3\\xb4                                                  (81)\\r\\n                       \\xef\\xa3\\xb2                                              \\x13                  \\xef\\xa3\\xb2                                            \\x13\\r\\n                                      \\xce\\xb3+1                                                              \\xce\\xb3+1\\r\\n                       \\xef\\xa3\\xb3 c\\xcc\\x83R = cR + 2 max \\xcf\\x81L c\\xcc\\x83L + uL \\xe2\\x88\\x92 uR , 0 ,                         \\xef\\xa3\\xb3 c\\xcc\\x83L = cL + 2 max \\xcf\\x81R c\\xcc\\x83R + uL \\xe2\\x88\\x92 uR , 0 ,\\r\\n                       \\xef\\xa3\\xb4\\r\\n                       \\xef\\xa3\\xb4\\r\\n                       \\xef\\xa3\\xb4                        p  \\xe2\\x88\\x92p\\r\\n                                                  L R                                    \\xef\\xa3\\xb4\\r\\n                                                                                         \\xef\\xa3\\xb4\\r\\n                                                                                         \\xef\\xa3\\xb4                       p  \\xe2\\x88\\x92p\\r\\n                                                                                                                   R L\\r\\n                       \\xef\\xa3\\xb4                                                                 \\xef\\xa3\\xb4\\r\\n\\r\\n                                                                          20\\r\\n\\x0c\\n\\nand \\xce\\xb3 = max(\\xce\\xb3L , \\xce\\xb3R ), cX = \\xce\\xb3(pX + p\\xe2\\x88\\x9eX )/\\xcf\\x81X for X = L, R. These estimates will bound the wave speeds in the exact Riemann\\r\\n                               p\\r\\n\\r\\nsolution in the case of pure phases [5]. Then, the above definition of \\xce\\xb3 will allow to bound the signal speeds in the case of polytropic\\r\\ngases, p\\xe2\\x88\\x9eL = p\\xe2\\x88\\x9eR = 0 [41]. Though, it is difficult to guarantee such properties in the general case when \\xce\\x93L , \\xce\\x93R and \\xce\\xa0L , \\xce\\xa0r ,\\r\\nthese estimates proved to be robust in the present numerical experiments.\\r\\n\\r\\n\\r\\n6. Properties of the DGSEM scheme\\r\\n\\r\\n    We recall here the main properties of the DGSEM scheme proposed in this work for the discretization of the SG-gamma model\\r\\n(1) with a stiffened gas EOS (5).\\r\\n\\r\\n\\r\\n6.1. Semi-discrete scheme\\r\\n    The semi-discrete scheme (31) with the EC fluxes (48) in the volume integral and the HLLC flux (71) at interfaces satisfy the\\r\\nsemi-discrete entropy inequality, see Theorem 3.1. In contrast, the scheme with the CP fluxes (46) in the volume integral, along\\r\\nwith the HLLC solver at the interfaces, preserves uniform pressure and velocity profiles across material interfaces.\\r\\n\\r\\n\\r\\n6.2. Fully discrete scheme\\r\\n     We restrict ourselves to the use of a one-step first-order explicit time discretization for both CP and EC numerical fluctuations\\r\\nin the volume integral. High-order time integration will be done by using a strong-stability preserving explicit Runge-Kutta method\\r\\nfrom [52] that is a convex combination of forward Euler steps and thus keeps the properties of the first-order in time scheme under\\r\\nsome condition on the time step. The fully discrete DGSEM scheme reads\\r\\n\\r\\n                                                  Ui\\xce\\xbaj,n+1 \\xe2\\x88\\x92 Ui\\xce\\xbaj,n\\r\\n                                   \\xcf\\x89i \\xcf\\x89 j J\\xce\\xbai j                     + Ri\\xce\\xbaj (uh(n) ) = 0   \\xe2\\x88\\x80\\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h , 0 6 i, j 6 p, n > 0,                                (82)\\r\\n                                                        \\xe2\\x88\\x86t(n)\\r\\n\\r\\n                                                                      h = uh (\\xc2\\xb7, t ), and the vector of space residuals R\\xce\\xba (\\xc2\\xb7) is defined from\\r\\nwhere \\xe2\\x88\\x86t(n) = t(n+1) \\xe2\\x88\\x92 t(n) is the time step, Ui\\xce\\xbaj,n = Ui\\xce\\xbaj (t(n) ), u(n)         (n)                                    ij\\r\\n\\r\\n(31). The following theorem summarizes the properties of the above scheme with fluctuations (30) and (26). These properties are\\r\\nindependent of the EC or CP character of the volume fluctuations and therefore hold for both EC and CP fluxes for the SG-gamma\\r\\nmodel (2), though the minimum entropy principle holds when excluding material interfaces. Likewise, any other interface flux that\\r\\nsatisfy properties (i) to (iv) below can be used in place of the HLLC solver.\\r\\n     The derivation of the CFL condition will rely on the work in [6, Lemma 3.4] that proves that there exist pseudo-equilibrium\\r\\nstates u?,n                                        ?,n\\r\\n        \\xce\\xba in \\xe2\\x84\\xa6GM and finite wave speed estimates \\xce\\xbb\\xce\\xba > 0 such that\\r\\n\\r\\n\\r\\n                       p                                                                                                   \\x12\\r\\n                      XX                                                                                                                          \\x11\\x13\\r\\n                                 \\xcf\\x89k Jek hRus u?,n                                  \\xce\\xbb?,n                              |\\xce\\xbb|max u?,n\\r\\n                                            \\x10                      \\x11                                                \\x10\\r\\n                                              \\xce\\xba , uh (xe , t ), ne = 0,                                                      \\xce\\xba , uh (xe , t ), ne ,\\r\\n                                                   \\xe2\\x88\\x92 k (n)       k                                                                \\xe2\\x88\\x92 k (n)       k\\r\\n                                                                                    \\xce\\xba >              max                                                (83)\\r\\n                                                                                                  06k6p, e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba\\r\\n                      e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\n                                                    \\xce\\xbb?,n\\r\\nwhere hRus (u\\xe2\\x88\\x92 , u+ , n) = 21 f(u\\xe2\\x88\\x92 ) + f(u+ ) \\xc2\\xb7 n \\xe2\\x88\\x92 \\xce\\xba2 (u+ \\xe2\\x88\\x92 u\\xe2\\x88\\x92 ) is the Rusanov flux and \\xce\\xbb?,n\\r\\n                             \\x10               \\x11\\r\\n                                                                                                    \\xce\\xba   bounds the fans of waves in the exact\\r\\nRiemann problems (51) with n = nke and left and right states u?,n   \\xce\\xba   and u\\xe2\\x88\\x92h (xke , t(n) ), see (84). Note that other conditions adapted to\\r\\nunstructured meshes may be used [49].\\r\\n\\r\\nTheorem 6.1. Let us consider the DGSEM scheme (82) with consistent fluctuations (30) of the form (37) in the volume integrals\\r\\n                                                                                                             j\\xe2\\x88\\x88Z \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM ; (ii) satisfies\\r\\nand consistent interface fluctuations (26) such that the associated three-point scheme (55): (i) is robust: Un>0\\r\\na discrete entropy inequality (60) with consistent numerical flux; (iii) satisfies a discrete minimum principle (77) on the specific\\r\\nphysical entropy s(u); (iv) satisfies discrete maximum principles (79) on \\xce\\x93 and \\xce\\xa0. Then, the updated cell-averaged solution hu(n+1)\\r\\n                                                                                                                                h    i\\xce\\xba\\r\\nis a convex combination of DOFs at time t(n) and updates of three-point schemes under the following conditions on the time step:\\r\\n\\r\\n\\r\\n                                                                \\xcf\\x89k Jek                                                                    1\\r\\n                                                                                                                   , \\xce\\xbb?,n , \\xce\\xbb?,n\\r\\n                                                                                 \\x10                                               \\x11\\r\\n                                 \\xe2\\x88\\x86t(n) max max                                max snL              , snR              \\xce\\xba\\xe2\\x88\\x92     \\xce\\xba+\\r\\n                                                                                                                                   6            ,      (84a)\\r\\n                                        e\\xe2\\x88\\x88Eh 06k6p \\xcf\\x89\\xcc\\x83k min J\\xce\\xba\\xc2\\xb1 (xke ))                    j+ 21            j\\xe2\\x88\\x92 12                       p(p + 1)\\r\\n                                                    p\\r\\n                                                   X     \\xcf\\x89k \\x10                                                      \\x11\\r\\n                            \\xe2\\x88\\x86t(n) max max                        \\xcf\\x89 j Dki vk\\xce\\xba j,n \\xc2\\xb7 n(i,k) j + \\xcf\\x89i Dk j vik,n\\r\\n                                                                                                       \\xce\\xba \\xc2\\xb7 ni( j,k) 6 1,                               (84b)\\r\\n                                                   k=0 \\xcf\\x89\\xcc\\x83i j J\\xce\\xba\\r\\n                                     \\xce\\xba\\xe2\\x88\\x88\\xe2\\x84\\xa6h 06i, j6p            ij\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                  21\\r\\n\\x0c\\n\\n                                                                                               \\xcf\\x89\\xcf\\x89                         \\xcf\\x890\\r\\nwhere \\xce\\xbb?,n\\r\\n        \\xce\\xba is defined in (83), \\xcf\\x89\\xcc\\x83i j = \\xcf\\x89i \\xcf\\x89 j when 0 < i, j < p, \\xcf\\x89\\xcc\\x83i j = 2 else, and \\xcf\\x89\\xcc\\x83k = \\xcf\\x89k when 0 < k < p, \\xcf\\x89\\xcc\\x830 = \\xcf\\x89\\xcc\\x83 p = 2 = p(p+1) .\\r\\n                                                                        i j                                                     1\\r\\n\\r\\nAssuming that the DOFs U\\xce\\xba are in \\xe2\\x84\\xa6GM for all \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h and 0 6 i, j 6 p, the DGSEM scheme (82) guarantees positivity of the\\r\\n                           i j,n\\r\\n\\r\\ncell-averaged solution:\\r\\n                                                           h i\\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6GM\\r\\n                                                        hun+1             \\xe2\\x88\\x80\\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h ,\\r\\n\\r\\ntogether with a minimum principle on the specific entropy when excluding material interfaces\\r\\n\\r\\n                                 i\\xce\\xba ) > min s(Ui\\xce\\xbaj,n ) : 0 6 i, j 6 p \\xe2\\x88\\xaa s u+h (xke , t(n) ) : e \\xe2\\x88\\x88 \\xe2\\x88\\x82\\xce\\xba, 0 6 k 6 p ,\\r\\n                                           n                         o n \\x10                 \\x11                   o\\r\\n                       s(hu(n+1)\\r\\n                           h\\r\\n\\r\\n\\r\\nand maximum principles on the EOS parameters Y in {\\xce\\x93, \\xce\\xa0}:\\r\\n\\r\\n                                                                    S\\xce\\xba (Yh(n) ) = {Y\\xce\\xbai j,n : 0 6 i, j 6 p} \\xe2\\x88\\xaa Yh+ (xke , t(n) ) : e \\xe2\\x88\\x88 \\xe2\\x88\\x82\\xce\\xba, 0 6 k 6 p \\xe2\\x88\\x80\\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h .\\r\\n                                                                                                            n                                     o\\r\\n     min S\\xce\\xba (Yh(n) ) 6 hYh(n+1) i\\xce\\xba 6 max S\\xce\\xba (Yh(n) ),\\r\\n\\r\\nProof. From (38) and (82), the cell-averaged discrete scheme for the d + 2 first components of (1a), Y in {\\xcf\\x81, \\xcf\\x81v, \\xcf\\x81E}, reads\\r\\n                                                                                  p\\r\\n                                                                       \\xe2\\x88\\x86t(n) X X\\r\\n                                                                                                   u\\xe2\\x88\\x92h (xke , t(n) ), u+h (xke , t(n) ), nke ,\\r\\n                                                                                                  \\x10                                         \\x11\\r\\n                                          hYh(n+1) i\\xce\\xba = hYh(n) i\\xce\\xba \\xe2\\x88\\x92                  \\xcf\\x89k Jek hHLLC\\r\\n                                                                                             Y\\r\\n                                                                        |\\xce\\xba| e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\n                 +                +\\r\\n        Y (u , u , n) = DY (u , u , n) + fY (u ) \\xc2\\xb7 n is the numerical flux in conservation form associated to the HLLC fluctuations\\r\\n             \\xe2\\x88\\x92             \\xe2\\x88\\x92 \\xe2\\x88\\x92                 \\xe2\\x88\\x92\\r\\nwhere hHLLC\\r\\n(71). Following [6, Sec. 4.2], we add \\xe2\\x88\\x86t|\\xce\\xba| times the flux balance in (83) to the above relation and decompose hYh(n) i\\xce\\xba in (20) to get\\r\\n                                         (n)\\r\\n\\r\\n\\r\\n\\r\\n                                                p\\xe2\\x88\\x921                             p\\r\\n                                                X             J\\xce\\xbai j i j,n X X             J\\xce\\xba (xke ) \\xe2\\x88\\x92 k (n)\\r\\n                                hYh(n+1) i\\xce\\xba =         \\xcf\\x89i \\xcf\\x89 j       Y\\xce\\xba +            \\xcf\\x890 \\xcf\\x89\\xcc\\x83k             uh (xe , t )\\r\\n                                                i, j=1\\r\\n                                                              |\\xce\\xba|         e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n                                                                                            |\\xce\\xba|\\r\\n                                                       \\xe2\\x88\\x86t(n) \\xcf\\x89k Jek \\x12 HLLC \\x10 \\xe2\\x88\\x92 k (n) + k (n) k \\x11                    Rus ?,n\\r\\n                                                                                                                       \\x10                      \\x11\\x13!\\r\\n                                                \\xe2\\x88\\x92                       h   u  (x  , t ), u  (x   , t   ), n    \\xe2\\x88\\x92 h     u\\xce\\xba  , u\\xe2\\x88\\x92 k (n)\\r\\n                                                                                                                                (x , t ), n k\\r\\n                                                                                                                                                 .\\r\\n                                                     \\xcf\\x890 \\xcf\\x89\\xcc\\x83k J\\xce\\xba (xke ) Y       h e          h e              e       Y          h e          e\\r\\n\\r\\n\\r\\n\\r\\n    As a consequence, the first d + 2 components of hu(n+1)      h   i\\xce\\xba are convex combination of quantities in \\xe2\\x84\\xa6GM under (84a) which\\r\\nconfirms positivity of h\\xcf\\x81(n+1)\\r\\n                         h     i \\xce\\xba and \\xcf\\x81e(hu(n+1)\\r\\n                                            h     i \\xce\\xba ) by concavity of \\xcf\\x81e(u) = \\xcf\\x81E \\xe2\\x88\\x92 \\xcf\\x81v\\xc2\\xb7\\xcf\\x81v\\r\\n                                                                                      2\\xcf\\x81\\r\\n                                                                                           . Excluding material interfaces, we also get the\\r\\nminimum entropy principle.\\r\\n    Likewise, the two last components Y in {\\xce\\x93, \\xce\\xa0} satisfy\\r\\n                                     \\xef\\xa3\\xab p                                                                               p                                                          \\xef\\xa3\\xb6\\r\\n                            \\xe2\\x88\\x86t(n) \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac X                                                                      \\x11 XX\\r\\n                                                                                                                          \\xcf\\x89k Jek D\\xe2\\x88\\x92Y u\\xe2\\x88\\x92h (xke , t(n) ), u+h (xke , t(n) ), nke \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb8.\\r\\n                                                \\x10                                                                                   \\x10                                         \\x11\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n   hYh(n+1) i\\xce\\xba =hYh(n) i\\xce\\xba \\xe2\\x88\\x92          \\xef\\xa3\\xac\\xef\\xa3\\xad   \\xcf\\x89i \\xcf\\x89 j Dik vi\\xce\\xbaj,n \\xc2\\xb7 n(i,k) j Y\\xce\\xbak j,n + D jk vi\\xce\\xbaj,n \\xc2\\xb7 ni( j,k) Y\\xce\\xbaik,n +\\r\\n                             |\\xce\\xba| i, j,k=0                                                                        e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n\\r\\n\\r\\n     Inverting indices i and k, then j and k in the volume integral, then using consistency of the interface fluctuations (27) to add\\r\\nthe trivial quantity\\r\\n                                                     p\\r\\n                                          \\xe2\\x88\\x86t(n) X X\\r\\n                                                        \\xcf\\x89k Jek D+Y u\\xe2\\x88\\x92h (xke , t(n) ), u\\xe2\\x88\\x92h (xke , t(n) ), nke = 0,\\r\\n                                                                  \\x10                                         \\x11\\r\\n                                        \\xe2\\x88\\x92\\r\\n                                           |\\xce\\xba| e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n                                                                                          ij\\r\\n                                                                                           J\\xce\\xba                            J\\xce\\xba (xke ) \\xe2\\x88\\x92 k (n)\\r\\nand finally using the convex decomposition hYh(n) i\\xce\\xba =                        i, j=0 \\xcf\\x89\\xcc\\x83i j |\\xce\\xba| Y\\xce\\xba +           k=0 \\xcf\\x89\\xcc\\x830 \\xcf\\x89\\xcc\\x83k |\\xce\\xba| Yh (xe , t ) from (20) we get\\r\\n                                                                           Pp                   i j,n P      Pp\\r\\n                                                                                                        e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba\\r\\n\\r\\n\\r\\n\\r\\n                   p\\r\\n                                    \\xef\\xa3\\xab                   p\\r\\n                                                                                                                              \\xef\\xa3\\xb6\\r\\n                   X       J\\xce\\xbai j \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac        \\xe2\\x88\\x86t(n) X \\xcf\\x89k \\xcf\\x89 j                                   \\xcf\\x89i \\xcf\\x89k                           \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n   hYh(n+1) i\\xce\\xba =        \\xcf\\x89\\xcc\\x83i j       \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad1 \\xe2\\x88\\x92 i j                       Dki vk\\xce\\xba j,n \\xc2\\xb7 n(i,k) j +          Dk j vik,n\\r\\n                                                                                                              \\xce\\xba   \\xc2\\xb7 n i( j,k) \\xef\\xa3\\xb7\\r\\n                                                                                                                                \\xef\\xa3\\xb7\\xef\\xa3\\xb7 Y i j,n\\r\\n                                                                                                                                 \\xef\\xa3\\xb8 \\xce\\xba\\r\\n                  i, j=0\\r\\n                           |\\xce\\xba|                J \\xce\\xba      k=0\\r\\n                                                             \\xcf\\x89\\xcc\\x83  i j                           \\xcf\\x89\\xcc\\x83 i j\\r\\n\\r\\n                         p\\r\\n                                                                           \\xe2\\x88\\x86t(n) \\xcf\\x89k Jek \\x12 \\xe2\\x88\\x92 \\x10 \\xe2\\x88\\x92 k (n) + k (n) k \\x11\\r\\n                                                     \\xef\\xa3\\xab                                                                                                                            \\xef\\xa3\\xb6\\r\\n                  XX                     J\\xce\\xba (xke ) \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac + k (n)                                                                                    + \\xe2\\x88\\x92 k (n)\\r\\n                                                                                                                                                    \\x10                           \\x11\\x13\\xef\\xa3\\xb7\\r\\n                +          \\xcf\\x89\\xcc\\x830 \\xcf\\x89\\xcc\\x83k                   \\xef\\xa3\\xad\\xef\\xa3\\xacYh (xe , t ) \\xe2\\x88\\x92                        D     u   (x  , t   ), u   (x        , t    ), n  + D   u  (x , t ), u    ,\\r\\n                                                                                                                                                                   \\xe2\\x88\\x92 k (n)\\r\\n                                                                                                                                                                    (x   t ), nk \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb8 ,\\r\\n                  e\\xe2\\x88\\x88\\xe2\\x88\\x82\\xce\\xba k=0\\r\\n                                           |\\xce\\xba|                           \\xcf\\x89\\xcc\\x83 0  \\xcf\\x89\\xcc\\x83 k J\\xce\\xba (xk)\\r\\n                                                                                         e\\r\\n                                                                                               Y      h e            h e                     e    Y    h e         h e         e \\xef\\xa3\\xb7\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nand invoking the metric identities in (41), hYh(n+1) i\\xce\\xba is indeed a convex combination of the DOFs associated to Yh(n) and updates of\\r\\nthree-point scheme (55) under (84). Positivity and the minimum and maximum principles follow directly.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                          22\\r\\n\\x0c\\n\\n7. A posteriori limiters\\r\\n\\r\\n    Properties of the discrete DGSEM scheme in Theorem 6.1 hold for the cell-averaged solution and a posteriori limiters are\\r\\napplied at the end of each Runge-Kutta stage to extend these properties to all DOFs within elements. Here we describe these\\r\\nlimiters for (82) that are similar to the ones proposed in [66, 65, 14, 62]. The basic principle consists in limiting DOFs at time t(n+1)\\r\\nthrough a linear scaling around the cell-average (20):\\r\\n                                    i j,n+1\\r\\n                                                \\x10                     \\x11\\r\\n                                  U\\xcc\\x83\\xce\\xba       = \\xce\\xb8\\xce\\xba Ui\\xce\\xbaj,n+1 \\xe2\\x88\\x92 hu(n+1)\\r\\n                                                              h     i\\xce\\xba + hu(n+1)\\r\\n                                                                           h     i\\xce\\xba \\xe2\\x88\\x800 6 i, j 6 p, \\xce\\xba \\xe2\\x88\\x88 \\xe2\\x84\\xa6h ,\\r\\n\\r\\nwhere 0 6 \\xce\\xb8\\xce\\xba 6 1 is the limiter coefficient. We here apply successive limiters (\\xce\\xb8\\xce\\xba\\xcf\\x81 , \\xce\\xb8\\xce\\xba\\xcf\\x81e , \\xce\\xb8\\xce\\xba\\xce\\x93 , \\xce\\xb8\\xce\\xba\\xce\\xa0 ) on:\\r\\n\\r\\n    \\xe2\\x80\\xa2 mixture density:\\r\\n                                                                                                          \\xef\\xa3\\xab                          \\xef\\xa3\\xb6\\r\\n                                                                                                          \\xef\\xa3\\xac\\xef\\xa3\\xac h\\xcf\\x81(n+1) i\\xce\\xba \\xe2\\x88\\x92 \\x0f          \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                        \\xcf\\x81\\xcc\\x83i\\xce\\xbaj,n+1 = \\xce\\xb8\\xce\\xba\\xcf\\x81 \\xcf\\x81i\\xce\\xbaj,n+1 \\xe2\\x88\\x92 h\\xcf\\x81(n+1)                                  \\xce\\xb8\\xce\\xba\\xcf\\x81 = min \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad\\r\\n                                         \\x10                         \\x11\\r\\n                                                           i\\xce\\xba          + h\\xcf\\x81(n+1) i\\xce\\xba ,                          h\\r\\n                                                                                                                               , 1\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb8 ,            \\xcf\\x81min = min \\xcf\\x81i\\xce\\xbaj,n+1 ;       (85)\\r\\n                                                                                                         \\xef\\xa3\\xac\\r\\n                                                     h                     h                                                                         \\xce\\xba\\r\\n                                                                                                             h\\xcf\\x81 (n+1)\\r\\n                                                                                                                h     i \\xe2\\x88\\x92\\xcf\\x81\\r\\n                                                                                                                      \\xce\\xba\\r\\n                                                                                                                           min\\r\\n                                                                                                                              \\xce\\xba\\r\\n                                                                                                                                                            06i, j6p\\r\\n\\r\\n\\r\\n    \\xe2\\x80\\xa2 EOS parameters \\xce\\x93 and \\xce\\xa0:\\r\\n                                                                                                   \\xef\\xa3\\xab (n+1)                                       \\xef\\xa3\\xb6\\r\\n                                 \\x10                       \\x11                                         \\xef\\xa3\\xac\\xef\\xa3\\xac hY   i\\xce\\xba \\xe2\\x88\\x92 mY            MY \\xe2\\x88\\x92 hYh(n+1) i\\xce\\xba   \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n              Y\\xcc\\x83\\xce\\xbai j,n+1 = \\xce\\xb8\\xce\\xbaY    Y\\xce\\xbai j,n+1 \\xe2\\x88\\x92 hYh(n+1) i\\xce\\xba + hYh(n+1) i\\xce\\xba ,         \\xce\\xb8\\xce\\xbaY = min \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad         h\\r\\n                                                                                                                      ,                    , 1\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb8 ,              Y \\xe2\\x88\\x88 {\\xce\\x93, \\xce\\xa0},   (86)\\r\\n                                                                                               \\xef\\xa3\\xac\\r\\n                                                                                                       (n+1)      min     max      (n+1)\\r\\n                                                                                                    hYh      i \\xe2\\x88\\x92Y\\r\\n                                                                                                               \\xce\\xba      \\xce\\xba Y     \\xe2\\x88\\x92 hY\\xce\\xba      i      h       \\xce\\xba\\r\\n\\r\\n\\r\\n       where Y\\xce\\xbamin = min06i, j6p Y\\xce\\xbai j,n+1 , Y\\xce\\xbamax = max06i, j6p Y\\xce\\xbai j,n+1 , and\\r\\n\\r\\n                                                    1                                   1                                 \\xce\\xb3i p\\xe2\\x88\\x9ei                              \\xce\\xb3i p\\xe2\\x88\\x9ei\\r\\n                                 m\\xce\\x93 = min                ,       M\\xce\\x93 = max                      ,     m\\xce\\xa0 = min                         ,       M\\xce\\xa0 = max                 ;\\r\\n                                        16i6n s \\xce\\xb3i \\xe2\\x88\\x92 1                     16i6n s \\xce\\xb3i \\xe2\\x88\\x92 1                      16i6n s \\xce\\xb3i \\xe2\\x88\\x92 1                         16i6n s \\xce\\xb3i \\xe2\\x88\\x92 1\\r\\n\\r\\n\\r\\n    \\xe2\\x80\\xa2 mixture total internal energy:\\r\\n\\r\\n                                                  Y\\xcc\\x83\\xce\\xbai j,n+1 = \\xce\\xb8\\xce\\xba\\xcf\\x81e Y\\xce\\xbai j,n+1 \\xe2\\x88\\x92 hYh(n+1) i\\xce\\xba + hYh(n+1) i\\xce\\xba ,\\r\\n                                                                   \\x10                       \\x11\\r\\n                                                                                                                          Y \\xe2\\x88\\x88 {\\xcf\\x81, \\xcf\\x81v, \\xcf\\x81E},                                      (87)\\r\\n\\r\\n       where \\xcf\\x81e(u) = \\xcf\\x81E \\xe2\\x88\\x92 \\xcf\\x81v\\xc2\\xb7\\xcf\\x81v\\r\\n                           2\\xcf\\x81\\r\\n                                and\\r\\n                                                                            \\xef\\xa3\\xab           \\xef\\xa3\\xab                                         \\xef\\xa3\\xb6       \\xef\\xa3\\xb6\\r\\n                                                                            \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac         \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac \\xcf\\x81e(huh in+1 ) \\xe2\\x88\\x92 p\\xcc\\x83i\\xe2\\x88\\x9ej,n+1 \\xe2\\x88\\x92 \\x0f \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\r\\n                                                                                                    \\xce\\xba\\r\\n                                                             \\xce\\xb8\\xce\\xba\\xcf\\x81e = min \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad min \\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xac\\xef\\xa3\\xad                            \\xce\\xba         \\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 , 1\\xef\\xa3\\xb7\\xef\\xa3\\xb7\\xef\\xa3\\xb7 .                                        (88)\\r\\n                                                                               06i, j6p      \\xcf\\x81e(huh i\\xce\\xban+1 ) \\xe2\\x88\\x92 \\xcf\\x81ei\\xce\\xbaj,n+1 \\xef\\xa3\\xb8 \\xef\\xa3\\xb8\\r\\n                                                                                                                                     \\xef\\xa3\\xb7\\r\\n\\r\\n    Note that in (85) and (88), 0 < \\x0f \\x1c 1 is a small parameter, which we set as \\x0f = 10\\xe2\\x88\\x928 in our numerical tests. The limiters (85)\\r\\nand (87) thus guarantee that \\xcf\\x81\\xcc\\x8306i\\r\\n                               \\xce\\xba\\r\\n                                   j6p,n+1\\r\\n                                                    \\xcb\\x9c \\xce\\xba06i j6p,n+1 > p\\xcc\\x8306i\\r\\n                                           > 0 and \\xcf\\x81e                  \\xe2\\x88\\x9e\\xce\\xba\\r\\n                                                                           j6p,n+1\\r\\n                                                                                   , respectively. Recalling (6), we have mY 6 Y 6 MY , with\\r\\nY in {\\xce\\x93, \\xce\\xa0}, formally which may be viewed as relaxed maximum principles. The limiters in (86) thus impose similar maximum\\r\\nprinciples:\\r\\n                                           m\\xce\\x93 \\xe2\\x89\\xa4 \\xce\\x93\\xcc\\x8306k6p,n+1\\r\\n                                                  j            6 M\\xce\\x93 , m\\xce\\xa0 6 \\xce\\xa0\\xcc\\x8306k6p,n+1j        6 M\\xce\\xa0 .\\r\\n\\r\\n\\r\\n8. Numerical experiments\\r\\n\\r\\n     We now perform numerical tests on the DGSEM scheme for the SG-gamma model (1)-(2) where the space discretization is\\r\\ndefined in (31) and where the three-stage third-order strong stability-preserving Runge-Kutta scheme by Shu and Osher [52] is\\r\\nused for the time discretization. The time step is evaluated from the CFL condition (84a) and the limiter, introduced in section 7, is\\r\\napplied at the end of each stage. We consider numerical tests from [13, 15, 14, 35, 16]. The scheme was implemented in the CFD\\r\\ncode Aghora developed at ONERA [50]. We recall that here we have proposed two DGSEM schemes for the SG-gamma model\\r\\nthat differ in the fluctuations (32) in the volume integral: a CP scheme with (46); a semi-discrete entropy stable scheme with EC\\r\\nfluxes (48). We will compare the performances of both schemes in this section. All numerical tests are performed at fourth-order\\r\\naccuracy, p = 3, in space unless stated otherwise.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                            23\\r\\n\\x0c\\n\\n8.1. Advection of a density wave\\r\\n    We begin by validating the high-order accuracy of the scheme (31) by advecting a density wave in a uniform flow in the domain\\r\\n\\xe2\\x84\\xa6 = [0, 1]2 , discretized with unstructured meshes with fourth-order curved elements (see Figure 4(a)), with periodic conditions\\r\\nand initial condition u0 (x):\\r\\n                               1 1                                    1\\r\\n                   \\xce\\xb110 (x) =    + sin 4\\xcf\\x80(x + y),       \\xcf\\x810 (x) = 1 +     sin 2\\xcf\\x80(x + y),   u0 (x) = v0 (x) = 1,   p0 (x) = 1,\\r\\n                               2 4                                    2\\r\\n\\r\\nalong with the EOS parameters Cv1 = Cv2 = 1, \\xce\\xb31 = 1.4, p\\xe2\\x88\\x9e1 = 0, \\xce\\xb32 = 3, p\\xe2\\x88\\x9e2 = 2. In this case, the density and EOS parameters\\r\\nare purely convected in uniform velocity and pressure fields. Norms on the mixture density error, eh = \\xcf\\x81h \\xe2\\x88\\x92 \\xcf\\x81, under h- and\\r\\np-refinements are displayed in Table 1 with either CP fluxes, or EC fluxes in the volume integral. We observe that as the mesh is\\r\\nrefined the expected p + 1 order of convergence is recovered by both schemes, but lower error levels are obtained with the CP flux.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                    (a)                                                          (b)\\r\\n\\r\\nFigure 4: Examples of meshes used for the numerical tests: (a) square mesh h = 1/4 with 16 fourth-order curved quadrangles with representation\\r\\nof the nodes; (b) example of unstructured mesh in the range 2.5 6 x 6 5 with 1580 elements and initial positions of the shock and bubble (see\\r\\nsection 8.3).\\r\\n\\r\\n\\r\\n\\r\\n8.2. Riemann problems\\r\\n    We first consider the advection of an isolated material discontinuity to assess the contact preservation property of the present\\r\\nscheme. The computational domain for this test is chosen to be \\xe2\\x84\\xa6h = [\\xe2\\x88\\x920.5, 0.5] with 100 elements. The Riemann initial data are\\r\\n\\r\\n                                                                                  x < 0,\\r\\n                                                           \\xef\\xa3\\xb1\\r\\n                                                           \\xef\\xa3\\xb2 (0.375, 2, 1, 1),\\r\\n                                          (\\xce\\xb11 , \\xcf\\x81, u, p) = \\xef\\xa3\\xb4\\r\\n                                                           \\xef\\xa3\\xb4\\r\\n                                                           \\xef\\xa3\\xb3 (0.146342, 1, 1, 1), x > 0,\\r\\n\\r\\nwith Cv1 = 1, Cv2 = 2, \\xce\\xb31 = 1.4, \\xce\\xb32 = 1.5, p\\xe2\\x88\\x9e1 = p\\xe2\\x88\\x9e2 = 0. Results at time t = 0.2 are displayed in Figure 5. As expected, compu-\\r\\ntations with CP fluxes preserve uniform profiles of pressure and velocity across the material interface, while spurious oscillations\\r\\noccur when using EC fluxes. In both cases, the interface is well captured with low amplitude oscillations in the \\xcf\\x81 and \\xce\\x93 profiles.\\r\\n    We now consider a gas-gas shock-interface interaction problem which was originally proposed in [40]. Here a shock wave\\r\\nin helium gas travels at Mach 8.96 and interacts with an helium-air interface. The computational domain \\xe2\\x84\\xa6h = [\\xe2\\x88\\x921, 1] with 100\\r\\nelements and the initial condition is as follows:\\r\\n\\r\\n                                                       (0, 0.386, 26.59, 100), x < \\xe2\\x88\\x920.8,\\r\\n                                                     \\xef\\xa3\\xb1\\r\\n                                                     \\xef\\xa3\\xb4\\r\\n                                                     \\xef\\xa3\\xb4\\r\\n                                                     \\xef\\xa3\\xb4\\r\\n                                    (\\xce\\xb11 , \\xcf\\x81, u, p) = \\xef\\xa3\\xb4                         \\xe2\\x88\\x920.8 < x < \\xe2\\x88\\x920.2,\\r\\n                                                     \\xef\\xa3\\xb4\\r\\n                                                       (0, 0.1, \\xe2\\x88\\x920.5, 1),\\r\\n                                                     \\xef\\xa3\\xb2\\r\\n                                                                               x > \\xe2\\x88\\x920.2,\\r\\n                                                     \\xef\\xa3\\xb4\\r\\n                                                     \\xef\\xa3\\xb4\\r\\n                                                     \\xef\\xa3\\xb3 (1, 1.0, \\xe2\\x88\\x920.5, 1),\\r\\n                                                     \\xef\\xa3\\xb4\\r\\n\\r\\n\\r\\nwith Cv1 = 1, Cv2 = 2.5, \\xce\\xb31 = 1.4, \\xce\\xb32 = 5/3, p\\xe2\\x88\\x9e1 = p\\xe2\\x88\\x9e2 = 0. The results are displayed in Figure 6, where the solution shows two\\r\\nshocks, one traveling left and the other traveling right, with a right traveling material interface in between. As a result, the shocks\\r\\nand interface are well captured, while spurious oscillations of small amplitude occur in the pressure and velocity fields.\\r\\n\\r\\n                                                                      24\\r\\n\\x0c\\n\\n                                  p    h         keh kL1 (\\xe2\\x84\\xa6h )    O1     keh kL2 (\\xe2\\x84\\xa6h )    O2     keh kL\\xe2\\x88\\x9e (\\xe2\\x84\\xa6h )   O\\xe2\\x88\\x9e\\r\\n                                       1/8       3.76E-01          \\xe2\\x80\\x93     4.16E-01         \\xe2\\x80\\x93      6.25E-01          \\xe2\\x80\\x93\\r\\n                                       1/16      1.82E-01        1.05    2.04E-01        1.03    3.69E-01        0.76\\r\\n                                  1\\r\\n                                       1/32      5.47E-02        1.73    6.11E-02        1.74    1.08E-01        1.77\\r\\n                                       1/64      1.43E-02        1.93    1.59E-02        1.94    2.56E-02        2.08\\r\\n                                       1/8       1.17E-02          \\xe2\\x80\\x93     1.41E-02         \\xe2\\x80\\x93      3.32E-02          \\xe2\\x80\\x93\\r\\n                                       1/16      1.06E-03        3.46    1.23E-03        3.52    3.64E-03        3.19\\r\\n                          CP      2\\r\\n                                       1/32      9.64E-05        3.45    1.17E-04        3.40    4.82E-04        2.90\\r\\n                                       1/64      1.01E-05        3.25    1.30E-05        3.16    6.50E-05        2.90\\r\\n                                       1/8       3.51E-04          \\xe2\\x80\\x93     4.68E-04         \\xe2\\x80\\x93      3.18E-03          \\xe2\\x80\\x93\\r\\n                                       1/16      1.73E-05        4.35    2.65E-05        4.14    1.98E-04        4.01\\r\\n                                  3\\r\\n                                       1/32      1.05E-06        4.04    1.66E-06        4.00    1.20E-05        4.04\\r\\n                                       1/64      6.73E-08        3.96    1.05E-07        3.98    7.54E-07        3.99\\r\\n                                       1/8       3.72E-01          \\xe2\\x80\\x93     4.11E-01         \\xe2\\x80\\x93      6.24E-01          \\xe2\\x80\\x93\\r\\n                                       1/16      1.78E-01        1.06    2.05E-01        1.01    4.29E-01        0.54\\r\\n                                  1\\r\\n                                       1/32      5.62E-02        1.66    7.14E-02        1.52    2.12E-01        1.02\\r\\n                                       1/64      1.52E-02        1.89    1.99E-02        1.84    6.81E-02        1.64\\r\\n                                       1/8       2.78E-02          \\xe2\\x80\\x93     3.48E-02         \\xe2\\x80\\x93      9.99E-02          \\xe2\\x80\\x93\\r\\n                                       1/16      3.95E-03        2.81    5.23E-03        2.73    1.76E-02        2.50\\r\\n                          EC      2\\r\\n                                       1/32      3.56E-04        3.47    4.55E-04        3.52    1.56E-03        3.50\\r\\n                                       1/64      2.91E-05        3.61    3.77E-05        3.59    1.41E-04        3.46\\r\\n                                       1/8       4.58E-03          \\xe2\\x80\\x93     5.78E-03         \\xe2\\x80\\x93      1.62E-02          \\xe2\\x80\\x93\\r\\n                                       1/16      2.35E-04        4.28    3.10E-04        4.22    1.10E-03        3.88\\r\\n                                  3\\r\\n                                       1/32      8.53E-06        4.78    1.21E-05        4.68    6.62E-05        4.06\\r\\n                                       1/64      2.91E-07        4.87    4.63E-07        4.71    4.01E-06        4.05\\r\\n\\r\\nTable 1: Advection of a density wave: results using either CP (top) numerical fluxes (46), or EC (bottom) numerical fluxes (48) in the volume\\r\\nintegral. Norms of the error on density under p- and h-refinements and associated orders of convergence at time t = 2.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFigure 5: Advection of an isolated material interface: fourth-order accurate simulations obtained on a mesh with 100 elements, and using either\\r\\nCP (top), or EC (bottom) fluxes in the volume integral. Approximate results (symbols) are shown at t = 0.2 and are compared to the exact solution\\r\\n(lines).\\r\\n\\r\\n\\r\\n                                                                        25\\r\\n\\x0c\\n\\nFigure 6: Shock-material interface interaction: fourth-order accurate, p = 3, simulations obtained on a mesh with 100 elements, and using either\\r\\nCP (top), or EC (bottom) fluxes in the volume integral. Approximate results (symbols) are shown at t = 0.07 and are compared to the exact solution\\r\\n(lines).\\r\\n\\r\\n\\r\\n\\r\\n     The last test case concerns a gas-water shock-interface interaction problem and simulates an underwater explosion, where the\\r\\ninitial condition consists of a material interface separating highly compressed air to the left and water at atmospheric pressure to\\r\\nthe right. The computational domain is \\xe2\\x84\\xa6h = [\\xe2\\x88\\x925, 5] with 100 elements, and the initial data are given as\\r\\n\\r\\n                                                                                         x < 0,\\r\\n                                                         \\xef\\xa3\\xb1\\r\\n                                                         \\xef\\xa3\\xb2 (1, 1.241, 0, 2.753),\\r\\n                                        (\\xce\\xb11 , \\xcf\\x81, u, p) = \\xef\\xa3\\xb4\\r\\n                                                         \\xef\\xa3\\xb4\\r\\n                                                         \\xef\\xa3\\xb3 (0, 0.991, 0, 3.059 \\xc3\\x97 10\\xe2\\x88\\x924 ), x > 0,\\r\\n\\r\\nwith Cv1 = 1.2, Cv2 = 0.073037, \\xce\\xb31 = 1.4, \\xce\\xb32 = 5.5, p\\xe2\\x88\\x9e1 = 0, and p\\xe2\\x88\\x9e2 = 1.505. The results, in Figure 7, show a right traveling\\r\\nshock, a right traveling contact wave, a right advected material interface and a left rarefaction wave. We observe small oscillations\\r\\non the velocity and pressure profiles even though the shock is of large amplitude, and the shock and material interface are well\\r\\ncaptured.\\r\\n\\r\\n\\r\\n8.3. Shock wave-helium bubble interaction\\r\\n    We now consider the interaction of a shock with a helium bubble, which was experimentally investigated in [27] and used to\\r\\nassess numerical schemes for multiphase and multicomponent flows [14, 26, 32, 35, 36, 49, 44]. The test involves a stationary\\r\\nhelium bubble (\\xce\\xb31 = 1.648 and Cv1 = 6.0598) which is surrounded by air (\\xce\\xb32 = 1.4 and Cv2 = 1.7857) and interacts with a left\\r\\nmoving Mach 1.22 shock. The computational domain \\xe2\\x84\\xa6h = [0.0, 6.5] \\xc3\\x97 [0, 1.78] is discretized with an unstructured mesh with\\r\\n433016 elements (see Figure 4(b)). The helium bubble of unit diameter is centered at x = 3.5 and y = 0.89 and the left traveling\\r\\nshock is located at x = 4. Periodic boundary conditions are imposed on the top and bottom boundaries, while non-reflective\\r\\nconditions are applied at the left and right boundaries. The initial data are made nondimensional with the initial bubble diameter,\\r\\ndensity, temperature and sound speed of air in the pre-shock region.\\r\\n     We first test the ability of both schemes to preserve material interfaces and consider the advection of the bubble only on a mesh\\r\\nwith 64 \\xc3\\x97 64 fourth-order curved elements (see Figure 4(a)). We thus remove the shock wave, impose a uniform velocity field\\r\\nv0 (x) = (1, 0)> and reduce the size of the bubble which is now initially centered at (0.5, 0.5). Figure 8 shows the bubble at time\\r\\nt = 76.19\\xc2\\xb5s corresponding to a transport of the bubble over a unit distance. The scheme with the CP fluxes captures the interface\\r\\nsharply and preserves the uniform velocity and pressure profiles across the interface, while spurious oscillations are observed with\\r\\nthe EC flux.\\r\\n     We now consider the shock-bubble interaction. Figure 9 shows the deformation of the bubble at several physical times as\\r\\nthe left traveling shock passes through it and represents contours of void fraction of the helium bubble, \\xce\\xb11 , and mixture pressure,\\r\\n\\r\\n                                                                       26\\r\\n\\x0c\\n\\nFigure 7: Gas-water shock-interface interaction problem: fourth-order accurate, p = 3, simulations obtained on a mesh with 100 elements, and\\r\\nusing either CP (top), or EC (bottom) fluxes in the volume integral. Approximate results (symbols) are shown at t = 1 and are compared to the\\r\\nexact solution (lines).\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                    27\\r\\n\\x0c\\n\\n                                                  (a) t = 0                      (b) t = 76.19\\xc2\\xb5s, y = 0.5\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            (c) t = 76.19\\xc2\\xb5s, CP                    (d) t = 76.19\\xc2\\xb5s, EC\\r\\n\\r\\nFigure 8: Advection of a helium bubble in air: (a) initial condition, and results obtained with fourth-order accuracy (p = 3) in space with either\\r\\nCP fluxes, or EC fluxes: (b) absolute error levels (logarithmic scale) on the pressure and velocity distributions along y = 0.5 obtained with either\\r\\nCP (black symbols), or EC (red symbols) fluctuations; (c,d) numerical Schlieren \\xcf\\x86 = exp(|\\xe2\\x88\\x87\\xcf\\x81|/|\\xe2\\x88\\x87\\xcf\\x81|max ).\\r\\n\\r\\n\\r\\n\\r\\np, together with numerical Schlieren. We observe that the scheme with CP fluxes allows a better and sharper resolution of the\\r\\nbubble interface for all physical times and is able to accurately capture the shock and bubble dynamics. The bubble interface\\r\\ndevelops vortices after interacting with shock due to a Kelvin-Helmholtz instability. Once again, results with the EC fluxes show\\r\\nsome spurious oscillations at the material interface before and after the interaction in contrast to CP fluxes, while both CP and EC\\r\\nschemes show good resolutions of the shock.\\r\\n\\r\\n\\r\\n8.4. Strong shock wave-hydrogen bubble interaction\\r\\n     We finally consider an interaction problem of a strong M = 2 shock in air with a hydrogen bubble that has been numerically\\r\\ninvestigated in [4, 54]. Compared to section 8.3, these conditions result in faster shock and bubble dynamics. The computational\\r\\ndomain for this test is \\xe2\\x84\\xa6h = [0, 22.5] \\xc3\\x97 [0, 7.5] and is discretized with an unstructured mesh with 154622 elements. The hydrogen\\r\\nbubble (\\xce\\xb31 = 1.41 and Cv1 = 7.424) is initially centered at x = 4 and y = 0, and the right traveling shock located at x = 7 in air\\r\\n(\\xce\\xb32 = 1.353 and Cv2 = 0.523). The initial data is made nondimensional with the pre-shock density, velocity and temperature of the\\r\\nair and a length scale of 1mm. We impose symmetry conditions at the top and bottom boundaries, along with supersonic inflow\\r\\ncondition at the left boundary and nonreflecting conditions at the right boundary.\\r\\n    Figure 10 shows the deformation of the bubble as the shock passes through it, where we plot contours of the void fraction\\r\\nof the hydrogen bubble, \\xce\\xb11 , mixture pressure pressure, p, and the numerical Schlieren. Here, we once again observe that the\\r\\nnumerical scheme is able to resolve the bubble interface well along with the shock. The oscillations at the interface are due to the\\r\\nKelvin-Helmholtz instability and they were also observed in [4]. Using CP fluxes maintains sharp resolution of the interface while\\r\\nalso proving to well capture shocks.\\r\\n\\r\\n\\r\\n9. Concluding remarks\\r\\n\\r\\n   In this work, we propose a high-order, robust and entropy stable discretization of the nonconservative multicomponent SG-\\r\\ngamma model [53]. The space discretization of this system relies on the DGSEM framework [47, 14] based on the modification of\\r\\n\\r\\n                                                                        28\\r\\n\\x0c\\n\\nt = 32\\xc2\\xb5s\\r\\nt = 102\\xc2\\xb5s\\r\\nt = 427\\xc2\\xb5s\\r\\nt = 674\\xc2\\xb5s\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                      CP fluxes                            EC fluxes                           CP fluxes                            EC fluxes\\r\\n\\r\\n        Figure 9: Interaction of a M = 1.22 shock in air with a helium bubble: fourth order accurate in space, p = 3, numerical simulations obtained at\\r\\n        different times using either CP, or EC fluxes in the volume integral, on an unstructured mesh with 433016 elements. For each snapshot the left\\r\\n        image shows the helium void fraction contours (color levels) and the pressure contours (lines), while the right image shows the numerical Schlieren\\r\\n        of the density \\xcf\\x86 = exp |\\xe2\\x88\\x87\\xcf\\x81|/|\\xe2\\x88\\x87\\xcf\\x81max |.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                               29\\r\\n\\x0c\\n\\nt = 1.5\\xc2\\xb5s\\r\\nt = 2.5\\xc2\\xb5s\\r\\nt = 4.0\\xc2\\xb5s\\r\\nt = 8.8\\xc2\\xb5s\\r\\nt = 13.6\\xc2\\xb5s\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                30\\r\\n                       CP fluxes                           EC fluxes                            CP fluxes                            EC fluxes\\r\\n\\r\\n         Figure 10: Interaction of a M = 2 shock in air with a Hydrogen bubble: fourth-order accurate in space, p = 3, numerical solutions obtained at\\r\\n         different times using either CP, or EC numerical fluxes in the volume integral, on an unstructured mesh with 154622 elements. For each snapshot\\r\\n         the left image shows the helium void fraction contours (color levels) and the pressure contours (lines), while the right image shows the numerical\\r\\n         Schlieren of the density \\xcf\\x86 = exp |\\xe2\\x88\\x87\\xcf\\x81|/|\\xe2\\x88\\x87\\xcf\\x81max |.\\r\\n\\x0c\\n\\nthe integral over discretization elements where we replace the physical fluxes and nonconservative products by two-point numerical\\r\\nfluctuations. We first extend this framework to multidimensional unstructured grids with curved elements and derive conditions\\r\\nunder which the semi-discrete scheme is high-order accurate, free-stream preserving and entropy stable. We then design a robust,\\r\\nentropy stable, material interface preserving, and maximum principles preserving HLLC solver for the SG-gamma model that does\\r\\nnot require a root-finding algorithm to evaluate the nonconservative product. The HLLC solver is then used as interface fluctuations\\r\\nin the DGSEM scheme, while we consider either EC, or CP fluctuations in the integrals over discretization elements. The DGSEM\\r\\nscheme is shown to either satisfy a semi-discrete entropy inequality with EC fluctuations (when excluding material interfaces), or\\r\\nto preserve material interfaces with CP fluctuations.\\r\\n    We then analyze the fully discrete DGSEM scheme with a forward Euler time discretization. We derive conditions on the\\r\\ntime step to guaranty that the cell-averaged solution remains in the set of states and satisfies minimum principles on entropy\\r\\nand maximum principles on EOS parameters with either EC, or CP fluctuations. We use a posteriori scaling limiters [65, 66] to\\r\\nextend these properties to all DOFs within elements, while a strong-stability preserving Runge-Kutta scheme [52] is used for the\\r\\nhigh-order time integration and keeps properties of the first-order in time scheme.\\r\\n    High-order accurate numerical simulations of flows in one and two space dimensions with discontinuous solutions and complex\\r\\nwave interactions confirm robustness, stability and accuracy of the present scheme with either EC, or CP fluctuations, and the\\r\\nscheme with CP fluxes present better resolution capabilities.\\r\\n\\r\\n\\r\\nReferences\\r\\n [1] R. Abgrall, How to prevent pressure oscillations in multicomponent flow calculations: a quasi conservative approach, J. Comput. Phys., 125\\r\\n     (1996), pp. 150\\xe2\\x80\\x93160.\\r\\n [2] P. Batten, N. Clarke, C. Lambert, and D. M. Causon, On the choice of wavespeeds for the HLLC Riemann solver, J. Sci. Comput., 18 (1997),\\r\\n     pp. 1553\\xe2\\x80\\x931570.\\r\\n [3] C. Berthon, B. Dubroca, and A. Sangam, A local entropy minimum principle for deriving entropy preserving schemes, SIAM J. Numer.\\r\\n     Anal., 50 (2012), pp. 468\\xe2\\x80\\x93491.\\r\\n [4] G. Billet, V. Giovangigli, and G. De Gassowski, Impact of volume viscosity on a shock\\xe2\\x80\\x93hydrogen-bubble interaction, Combust. Theory\\r\\n     Model., 12 (2008), pp. 221\\xe2\\x80\\x93248.\\r\\n [5] F. Bouchut, Nonlinear stability of finite Volume Methods for hyperbolic conservation laws: And Well-Balanced schemes for sources, Springer\\r\\n     Science & Business Media, 2004.\\r\\n [6] V. Carlier and F. Renac, Invariant domain preserving high-order spectral discontinuous approximations of hyperbolic systems,\\r\\n     arXiv:2203.05452 [math.NA], (2022).\\r\\n [7] M. H. Carpenter, T. C. Fisher, E. J. Nielsen, and S. H. Frankel, Entropy stable spectral collocation schemes for the Navier\\xe2\\x80\\x93Stokes equations:\\r\\n     Discontinuous interfaces, SIAM J. Sci. Comput., 36 (2014), pp. B835\\xe2\\x80\\x93B867.\\r\\n [8] M. J. Castro, T. M. de Luna, and C. Pare\\xcc\\x81s, Well-balanced schemes and path-conservative numerical methods, in Handbook of Numer. Anal.,\\r\\n     vol. 18, Elsevier, 2017, pp. 131\\xe2\\x80\\x93175.\\r\\n [9] M. J. Castro, U. S. Fjordholm, S. Mishra, and C. Pare\\xcc\\x81s, Entropy conservative and entropy stable schemes for nonconservative hyperbolic\\r\\n     systems, SIAM J. Numer. Anal., 51 (2013), pp. 1371\\xe2\\x80\\x931391.\\r\\n[10] M. J. Castro, J. Gallardo, and C. Pare\\xcc\\x81s, High order finite volume schemes based on reconstruction of states for solving hyperbolic systems\\r\\n     with nonconservative products. applications to shallow-water systems, Math. Comput., 75 (2006), pp. 1103\\xe2\\x80\\x931134.\\r\\n[11] Castro D\\xc4\\xb1\\xcc\\x81az, Manuel Jesu\\xcc\\x81s, Ferna\\xcc\\x81ndez-Nieto, Enrique Domingo, Morales de Luna, Toma\\xcc\\x81s, Narbona-Reina, Gladys, and Pare\\xcc\\x81s, Carlos, A\\r\\n     hllc scheme for nonconservative hyperbolic problems. application to turbidity currents with sediment transport, ESAIM: M2AN, 47 (2013),\\r\\n     pp. 1\\xe2\\x80\\x9332.\\r\\n[12] T. Chen and C.-W. Shu, Entropy stable high order discontinuous Galerkin methods with suitable quadrature rules for hyperbolic conservation\\r\\n     laws, J. Comput. Phys., 345 (2017), pp. 427\\xe2\\x80\\x93461.\\r\\n[13] J. Cheng, F. Zhang, and T. Liu, A discontinuous Galerkin method for the simulation of compressible gas-gas and gas-water two-medium\\r\\n     flows, J. Computat. Phys., 403 (2020), p. 109059.\\r\\n[14] F. Coquel, C. Marmignon, P. Rai, and F. Renac, An entropy stable high-order discontinuous Galerkin spectral element method for the Baer-\\r\\n     Nunziato two-phase flow model, J. Comput. Phys., 431 (2021), p. 110135.\\r\\n[15] V. Coralic and T. Colonius, Finite-volume WENO scheme for viscous compressible multicomponent flows, J. Computat. Phys., 274 (2014),\\r\\n     pp. 95\\xe2\\x80\\x93121.\\r\\n[16] M. T. H. de Frahan, S. Varadan, and E. Johnsen, A new limiting procedure for discontinuous Galerkin methods applied to compressible\\r\\n     multiphase flows with shocks and interfaces, J. Comput. Phys., 280 (2015), pp. 489\\xe2\\x80\\x93509.\\r\\n[17] D. Derigs, A. R. Winters, G. J. Gassner, and S. Walch, A novel averaging technique for discrete entropy-stable dissipation operators for\\r\\n     ideal MHD, J. Comput. Phys., 330 (2017), pp. 624\\xe2\\x80\\x93632.\\r\\n[18] B. Despres, Entropy inequality for high order discontinuous Galerkin approximation of Euler equations, in Hyperbolic Problems: Theory,\\r\\n     Numerics, Applications, M. Fey and R. Jeltsch, eds., Basel, 1999, Birkha\\xcc\\x88user Basel, pp. 225\\xe2\\x80\\x93231.\\r\\n[19] M. Dumbser and D. S. Balsara, A new efficient formulation of the hllem riemann solver for general conservative and non-conservative\\r\\n     hyperbolic systems, J. Comput. Phys., 304 (2016), pp. 275\\xe2\\x80\\x93319.\\r\\n[20] B. Einfeldt, C.-D. Munz, P. L. Roe, and B. Sjo\\xcc\\x88green, On godunov-type methods near low densities, J. Comput. Phys., 92 (1991), pp. 273\\xe2\\x80\\x93295.\\r\\n[21] T. C. Fisher and M. H. Carpenter, High-order entropy stable finite difference schemes for nonlinear conservation laws: Finite domains, J.\\r\\n     Comput. Phys., 252 (2013), pp. 518\\xe2\\x80\\x93557.\\r\\n\\r\\n                                                                       31\\r\\n\\x0c\\n\\n[22] U. S. Fjordholm, S. Mishra, and E. Tadmor, Arbitrarily high-order accurate entropy stable essentially nonoscillatory schemes for systems of\\r\\n     conservation laws, SIAM J. Numer. Anal., 50 (2012), pp. 544\\xe2\\x80\\x93573.\\r\\n[23] E. Franquet and V. Perrier, Runge\\xe2\\x80\\x93Kutta discontinuous Galerkin method for the approximation of Baer and Nunziato type multiphase\\r\\n     models, J. Comput. Phys., 231 (2012), pp. 4096\\xe2\\x80\\x934141.\\r\\n[24] G. J. Gassner, A skew-symmetric discontinuous Galerkin spectral element discretization and its relation to SBP-SAT finite difference methods,\\r\\n     SIAM J. Sci. Comput., 35 (2013), pp. A1233\\xe2\\x80\\x93A1253.\\r\\n[25] G. J. Gassner, A. R. Winters, and D. A. Kopriva, Split form nodal discontinuous Galerkin schemes with summation-by-parts property for\\r\\n     the compressible Euler equations, J. Comput. Phys., 327 (2016), pp. 39\\xe2\\x80\\x9366.\\r\\n[26] J. Giordano and Y. Burtschell, Richtmyer-Meshkov instability induced by shock-bubble interaction: Numerical and analytical studies with\\r\\n     experimental validation, Phys. Fluids, 18 (2006), p. 036102.\\r\\n[27] J. F. Haas and B. Sturtevant, Interaction of weak shock waves with cylindrical and spherical gas inhomogeneities, J. Fluid Mech., 181\\r\\n     (1987), pp. 41\\xe2\\x80\\x9376.\\r\\n[28] A. Harten, P. D. Lax, and B. V. Leer, On upstream differencing and Godunov-type schemes for hyperbolic conservation laws, SIAM review,\\r\\n     25 (1983), pp. 35\\xe2\\x80\\x9361.\\r\\n[29] Helluy, Philippe and Seguin, Nicolas, Relaxation models of phase transition flows, ESAIM: M2AN, 40 (2006), pp. 331\\xe2\\x80\\x93352.\\r\\n[30] A. Hiltebrand and S. Mishra, Entropy stable shock capturing space\\xe2\\x80\\x93time discontinuous Galerkin schemes for systems of conservation laws,\\r\\n     Numer. Math., 126 (2014), pp. 103\\xe2\\x80\\x93151.\\r\\n[31] A. Hiltebrand, S. Mishra, and C. Pare\\xcc\\x81s, Entropy-stable space\\xe2\\x80\\x93time DG schemes for non-conservative hyperbolic systems, ESAIM: M2AN,\\r\\n     52 (2018), pp. 995\\xe2\\x80\\x931022.\\r\\n[32] X. Y. Hu, B. Khoo, N. A. Adams, and F. L. Huang, A conservative interface method for compressible flows, J. Comput. Phys., 219 (2006),\\r\\n     pp. 553\\xe2\\x80\\x93578.\\r\\n[33] F. Ismail and P. L. Roe, Affordable, entropy-consistent Euler flux functions ii: Entropy production at shocks, J. Comput. Phys., 228 (2009),\\r\\n     pp. 5410\\xe2\\x80\\x935436.\\r\\n[34] G. S. Jiang and C.-W. Shu, On a cell entropy inequality for discontinuous Galerkin methods, Math. Comput., 62 (1994), pp. 531\\xe2\\x80\\x93538.\\r\\n[35] E. Johnsen and T. Colonius, Implementation of WENO schemes in compressible multicomponent flow problems, J. Comput. Phys., 219 (2006),\\r\\n     pp. 715\\xe2\\x80\\x93732.\\r\\n[36] S. Kawai and H. Terashima, A high-resolution scheme for compressible multicomponent flows with shock waves, Int. J. Numer. Methods.\\r\\n     Fluids, 66 (2011), pp. 1207\\xe2\\x80\\x931225.\\r\\n[37] C. A. Kennedy and A. Gruber, Reduced aliasing formulations of the convective terms within the navier\\xe2\\x80\\x93stokes equations for a compressible\\r\\n     fluid, J. Comput. Phys., 227 (2008), pp. 1676\\xe2\\x80\\x931700.\\r\\n[38] D. A. Kopriva, Metric identities and the discontinuous spectral element method on curvilinear meshes., J. Sci. Comput., 26 (2006), pp. 302\\xe2\\x80\\x93\\r\\n     327.\\r\\n[39] D. A. Kopriva and G. Gassner, On the quadrature and weak form choices in collocation type discontinuous Galerkin spectral element\\r\\n     methods, J. Sci. Comput., 44 (2010), pp. 136\\xe2\\x80\\x93155.\\r\\n[40] T. G. Liu, B. C. Khoo, and K. S. Yeo, Ghost fluid method for strong shock impacting on material interface, J. Comput. Phys., 190 (2003),\\r\\n     pp. 651\\xe2\\x80\\x93681.\\r\\n[41] C. Marmignon, F. Naddei, and F. Renac, Energy relaxation approximation for the compressible multicomponent flows in thermal nonequilib-\\r\\n     rium, arXiv:2103.03731 [math.NA], (2021).\\r\\n[42] R. Menikoff and B. J. Plohr, The riemann problem for fluid flow of real materials, Rev. Mod. Phys., 61 (1989), pp. 75\\xe2\\x80\\x93130.\\r\\n[43] C. Pare\\xcc\\x81s, Numerical methods for nonconservative hyperbolic systems: a theoretical framework., SIAM J. Numer. Anal., 44 (2006), pp. 300\\xe2\\x80\\x93\\r\\n     321.\\r\\n[44] J. J. Quirk and S. Karni, On the dynamics of a shock\\xe2\\x80\\x93bubble interaction, J. Fluid Mech., 318 (1996), pp. 129\\xe2\\x80\\x93163.\\r\\n[45] H. Ranocha, Comparison of some entropy conservative numerical fluxes for the Euler equations, J. Sci. Comput., 76 (2018), pp. 216\\xe2\\x80\\x93242.\\r\\n[46] F. Renac, A robust high-order discontinuous Galerkin method with large time steps for the compressible Euler equations, Commun. Math.\\r\\n     Sci., 15 (2017), pp. 813\\xe2\\x80\\x93837.\\r\\n[47]          , Entropy stable DGSEM for nonlinear hyperbolic systems in nonconservative form with application to two-phase flows, J. Comput.\\r\\n     Phys., 382 (2019), pp. 1\\xe2\\x80\\x9326.\\r\\n[48]          , Entropy stable, positive DGSEM with sharp resolution of material interfaces for a 4 \\xc3\\x97 4 two-phase flow system: a legacy from\\r\\n     three-point schemes, arXiv preprint arXiv:2001.05710, (2019).\\r\\n[49]          , Entropy stable, robust and high-order DGSEM for the compressible multicomponent Euler equations, J. Comput. Phys., (2021),\\r\\n     p. 110584.\\r\\n[50] F. Renac, M. de la Llave Plata, E. Martin, J. B. Chapelier, and V. Couaillier, Aghora: A High-Order DG Solver for Turbulent Flow\\r\\n     Simulations, Springer International Publishing, Cham, 2015, pp. 315\\xe2\\x80\\x93335.\\r\\n[51] R. Saurel and C. Pantano, Diffuse-interface capturing methods for compressible two-phase flows, Annu. Rev. Fluid Mech., 50 (2018),\\r\\n     pp. 105\\xe2\\x80\\x93130.\\r\\n[52] C.-W. Shu and S. Osher, Efficient implementation of essentially non-oscillatory shock-capturing schemes, J. Comput. Phys., 77 (1988),\\r\\n     pp. 439\\xe2\\x80\\x93471.\\r\\n[53] K.-M. Shyue, An efficient shock-capturing algorithm for compressible multicomponent problems, J. Comput. Phys., 142 (1998), pp. 208\\xe2\\x80\\x93242.\\r\\n[54] B. Sjo\\xcc\\x88green and H. C. Yee, Grid convergence of high order methods for multiscale complex unsteady viscous compressible flows, J. Comput.\\r\\n     Phys., 185 (2003), pp. 1\\xe2\\x80\\x9326.\\r\\n[55] E. Tadmor, The numerical viscosity of entropy stable schemes for systems of conservation laws. I, Math. Comput., 49 (1987), pp. 91\\xe2\\x80\\x93103.\\r\\n[56] S. A. Tokareva and E. F. Toro, HLLC-type Riemann solver for the Baer\\xe2\\x80\\x93Nunziato equations of compressible two-phase flow, J. Comput.\\r\\n     Phys., 229 (2010), pp. 3573\\xe2\\x80\\x933604.\\r\\n[57] E. Toro, L. Mu\\xcc\\x88ller, and A. Siviglia, Bounds for wave speeds in the Riemann problem: Direct theoretical estimates, Comput. Fluids, 209\\r\\n     (2020), p. 104640.\\r\\n\\r\\n                                                                       32\\r\\n\\x0c\\n\\n[58] E. F. Toro, Riemann-problem-based techniques for computing reactive two-phased flows, in Numer. Combustion, Springer, 1989, pp. 472\\xe2\\x80\\x93\\r\\n     481.\\r\\n[59]        , Riemann solvers and numerical methods for fluid dynamics: a practical introduction, Springer Science & Business Media, 2013.\\r\\n[60] E. F. Toro, M. Spruce, and W. Speares, Restoration of the contact surface in the hll-riemann solver, Shock waves, 4 (1994), pp. 25\\xe2\\x80\\x9334.\\r\\n[61] A. Volpert, The spaces BV and quasilinear equations, Math. USSR Sbornik, 115 (1967), pp. 255\\xe2\\x80\\x93302.\\r\\n[62] C. Wang, X. Zhang, C.-W. Shu, and J. Ning, Robust high order discontinuous Galerkin schemes for two-dimensional gaseous detonations, J.\\r\\n     Comput. Phys., 231 (2012), pp. 653\\xe2\\x80\\x93665.\\r\\n[63] M. Waruszewski, J. E. Kozdon, L. C. Wilcox, T. H. Gibson, and F. X. Giraldo, Entropy stable discontinuous galerkin methods for balance\\r\\n     laws in non-conservative form: Applications to euler with gravity, 2021.\\r\\n[64] N. Wintermeyer, A. R. Winters, G. J. Gassner, and D. A. Kopriva, An entropy stable nodal discontinuous Galerkin method for the two\\r\\n     dimensional shallow water equations on unstructured curvilinear meshes with discontinuous bathymetry, J. Comput. Phys., 340 (2017),\\r\\n     pp. 200\\xe2\\x80\\x93242.\\r\\n[65] X. Zhang and C. Shu, On positivity-preserving high order discontinuous Galerkin schemes for compressible Euler equations on rectangular\\r\\n     meshes, J. Comput. Phys., 229 (2010), pp. 8918\\xe2\\x80\\x938934.\\r\\n[66] X. Zhang and C.-W. Shu, On maximum-principle-satisfying high order schemes for scalar conservation laws, J. Comput. Phys., 229 (2010),\\r\\n     pp. 3091\\xe2\\x80\\x933120.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                    33\\r\\n\\x0c', b'              Masked Discrimination for\\r\\n      Self-Supervised Learning on Point Clouds\\r\\n\\r\\n                       Haotian Liu      Mu Cai      Yong Jae Lee\\r\\n\\r\\n                           University of Wisconsin\\xe2\\x80\\x93Madison\\r\\n                        {lht,mucai,yongjaelee}@cs.wisc.edu\\r\\n\\r\\n        Abstract. Masked autoencoding has achieved great success for self-\\r\\n        supervised learning in the image and language domains. However, mask\\r\\n        based pretraining has yet to show benefits for point cloud understanding,\\r\\n        likely due to standard backbones like PointNet being unable to properly\\r\\n        handle the training versus testing distribution mismatch introduced by\\r\\n        masking during training. In this paper, we bridge this gap by proposing\\r\\n        a discriminative mask pretraining Transformer framework, MaskPoint,\\r\\n        for point clouds. Our key idea is to represent the point cloud as discrete\\r\\n        occupancy values (1 if part of the point cloud; 0 if not), and perform sim-\\r\\n        ple binary classification between masked object points and sampled noise\\r\\n        points as the proxy task. In this way, our approach is robust to the point\\r\\n        sampling variance in point clouds, and facilitates learning rich represen-\\r\\n        tations. We evaluate our pretrained models across several downstream\\r\\n        tasks, including 3D shape classification, segmentation, and real-word ob-\\r\\n        ject detection, and demonstrate state-of-the-art results while achieving\\r\\n        a significant pretraining speedup (e.g., 4.1\\xc3\\x97 on ScanNet) compared to\\r\\n        the prior state-of-the-art Transformer baseline.1\\r\\n\\r\\n\\r\\n1     Introduction\\r\\nLearning rich feature representations without human supervision, also known\\r\\nas self-supervised learning, has made tremendous strides in recent years. We\\r\\nnow have methods in NLP [41,12,40] and computer vision [21,5,20,8,2] that can\\r\\nproduce stronger features than those learned on labeled datasets.\\r\\n    In particular, masked autoencoding, whose task is to reconstruct the masked\\r\\ndata from the unmasked input (e.g., predicting the masked word in a sentence\\r\\nor masked patch in an image, based on surrounding unmasked context) is the\\r\\ndominant self-supervised learning approach for text understanding [12,26,27,63]\\r\\nand has recently shown great promise in image understanding [2,20] as well.\\r\\nCuriously, for point cloud data, masked autoencoding has not yet been able\\r\\nto produce convincing results [53,65,62]. Self-supervised learning would be ex-\\r\\ntremely beneficial for point cloud data, as obtaining high-quality annotations\\r\\nis both hard and expensive, especially for real-world scans. At the same time,\\r\\nmasked autoencoding should also be a good fit for point cloud data, since each\\r\\npoint (or group of points) can easily be masked or unmasked.\\r\\n    We hypothesize that the primary reason why masked autoencoding has thus\\r\\nfar not worked well for point cloud data is because standard point cloud back-\\r\\n1\\r\\n    Code will be publicly available at https://github.com/haotian-liu/MaskPoint.\\r\\n\\x0c\\n\\n2       Liu et al.\\r\\n\\r\\n                            Unmasked Points                  real   fake\\r\\n                                              Transformer\\r\\n                                                encoder     Transformer\\r\\n                                                              decoder\\r\\n\\r\\n\\r\\n\\r\\n                                                                    noise\\r\\n                             Masked Points\\r\\n\\r\\nFig. 1: Main Idea. We randomly partition the point cloud into masked and\\r\\nunmasked sets. We only feed the visible portion of the point cloud into the\\r\\nencoder. Then, a set of real query points are sampled from the masked points,\\r\\nand a set of fake query points are randomly sampled from 3D space. We train\\r\\nthe decoder so that it distinguishes between the real and fake points. After pre-\\r\\ntraining, we discard the decoder and use the encoder for downstream tasks.\\r\\n\\r\\n\\r\\nbones are unable to properly handle the distribution mismatch between train-\\r\\ning and testing data introduced by masking. Specifically, PointNet type back-\\r\\nbones [39,37,38] leverage local aggregation layers that operate over local neigh-\\r\\nborhoods (e.g., k-nearest neighbors) of each point. The extent of the local neigh-\\r\\nborhoods can change drastically with the introduction of masking, creating a\\r\\ndiscrepancy between the distribution of local neighborhoods seen on masked\\r\\ntraining scenes versus unmasked test scenes.\\r\\n    Transformers [52], on the other hand, can perform self-attention (a form of\\r\\naggregation) on either all or selective portions of the input data. This means\\r\\nthat it has the ability to only process the unmasked portions of the scene in\\r\\nthe training data, without being impacted by the masked scene portions. This\\r\\nproperty suggests that Transformers could be an ideal backbone choice for self-\\r\\nsupervised masked autoencoding for point clouds.\\r\\n    For image understanding, the state-of-the-art masked autoencoding Trans-\\r\\nformer approach MAE [20] masks out a large random subset of image patches,\\r\\napplies the Transformer encoder to the unmasked patches, and trains a small\\r\\nTransformer decoder that takes in the positional encodings of the masked patches\\r\\nto reconstruct their original pixel values. However, this approach cannot be di-\\r\\nrectly applied to point cloud data, because the raw representation of each 3D\\r\\npoint is its spatial xyz location. Thus, training the decoder to predict the xyz co-\\r\\nordinates of a masked point would be trivial, since its positional encoding would\\r\\nleak the correct answer. In this case, the network would simply take a shortcut\\r\\nand not learn meaningful features.\\r\\n    To address this, we propose a simple binary point classification objective as a\\r\\nnew pretext task for point cloud masked autoencoding. We first group points into\\r\\nlocal neighborhoods, and then mask out a large random subset of those groups.\\r\\nThe Transformer encoder takes in the unmasked point groups, and encodes each\\r\\ngroup through self-attention with the other groups. The Transformer decoder\\r\\ntakes in a set of real query points and fake query points, where the real queries\\r\\nare sampled from the masked points, while the fake queries are randomly sampled\\r\\nfrom the full 3D space. We then perform cross attention between the decoder\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds       3\\r\\n\\r\\nqueries and encoder outputs. Finally, we apply a binary classification head to\\r\\nthe decoder\\xe2\\x80\\x99s outputs and require it to distinguish between the real and fake\\r\\nqueries. We find this design to be simple yet effective, as it creates a difficult\\r\\nand meaningful pretext task that requires the network to deduce the shape of\\r\\nthe object from only a small amount of visible point groups.\\r\\n    Importantly, we find that a much higher masking ratio (e.g., 90%) is required\\r\\nfor point cloud data compared to the image domain (75% in [20]). We identify\\r\\ntwo possible reasons. First, the additional depth dimension in point clouds can\\r\\nhelp disambiguate object instances (i.e., make the task easier) \\xe2\\x80\\x93 with the same\\r\\nmasking ratio, a 3D patch in point cloud data is less likely to overlap multiple\\r\\nobjects than a 2D patch in images. Second, local point groups often already\\r\\ncontain unique categorical information, e.g. a wheel-shaped local point patch can\\r\\nyield a high probability of a car-shaped object. These two observations demand\\r\\nstronger masking ratios in order for the pretext task to be hard enough to make\\r\\nthe model learn useful feature representations.\\r\\n    Among existing self-supervised point cloud approaches, Point-BERT [65] is\\r\\nthe most related. It trains a discrete Variational AutoEncoder (dVAE) [43] to\\r\\nencode the input point cloud into discrete point token representations, and per-\\r\\nforms BERT-style pretraining over them. To aid training, it uses point patch\\r\\nmixing augmentation, together with an auxiliary MoCo [21] loss. However, the\\r\\ndependency on a pretrained dVAE together with other auxiliary techniques, cre-\\r\\nates a significant computational overhead in pretraining \\xe2\\x80\\x93 our experiments show\\r\\nthat its pre-training is significantly slower (e.g., 4.1\\xc3\\x97 on ScanNet [9]) than ours\\r\\neven without taking into account the training time of the dVAE module. The\\r\\nlarge speedup is also due to our design of having a high masking rate and the\\r\\nTransformer encoder processing only the unmasked points.\\r\\n    In sum, our main contributions are: (1) A novel masked point classification\\r\\nTransformer, MaskPoint, for self-supervised learning on point clouds. (2) Our\\r\\napproach is simple and effective, achieving state-of-the-art performance on a va-\\r\\nriety of downstream tasks, including object classification on ModelNet40 [58] /\\r\\nScanObjectNN [51], part segmentation on ShapeNetPart [64], object detection\\r\\non ScanNet [9], and few-shot object classification on ModelNet40 [58]. (3) No-\\r\\ntably, for the first time, we show that a standard Transformer architecture can\\r\\noutperform sophisticatedly designed point cloud backbones.\\r\\n\\r\\n2   Related Work\\r\\nTransformers. Transformers were first proposed to model long-term dependen-\\r\\ncies in sequential data [52], and have achieved great success in natural language\\r\\nprocessing [52,12,41]. More recently, they have also shown promising performance\\r\\non various computer vision tasks, including image classification [14,50,46], ob-\\r\\nject detection [3], semantic segmentation [54], image generation [25], and multi-\\r\\nmodal learning [40]. There have also been attempts to adopt transformers to 3D\\r\\npoint cloud data. PCT [19] and Point Transformer [71] propose new attention\\r\\nmechanisms for point cloud feature aggregation. 3DETR [34] uses Transfomer\\r\\nblocks and the parallel decoding strategy from DETR [3] for 3D object detec-\\r\\n\\x0c\\n\\n4      Liu et al.\\r\\n\\r\\ntion. However, it is still hard to get promising performance using the standard\\r\\nTransformer. For example, in 3D object detection, there is a large performance\\r\\ngap between 3DETR [34] and state-of-the-art point based [69] and convolution\\r\\nbased [10] methods. In this paper, we propose a novel masked autoencoding\\r\\nTransformer for self-supervised learning on point clouds.\\r\\nSelf-supervised Learning. Self-supervised learning (SSL) aims to learn mean-\\r\\ningful representations from the data itself, to better serve downstream tasks. Tra-\\r\\nditional methods typically rely on pretext tasks, such as image rotation predic-\\r\\ntion [18], image colorization [67], and solving jigsaw puzzles [35]. Recent methods\\r\\nbased on contrastive learning (e.g., MoCo [21], SimCLR [5], SimSiam [8]) have\\r\\nachieved great success in the image domain, sometimes producing even better\\r\\ndownstream performance compared to supervised pretraining on ImageNet [11].\\r\\n    Self-supervised learning has also begun to be explored for point cloud data.\\r\\nPretext methods include deformation reconstruction [1], geometric structure pre-\\r\\ndiction [47], and orientation estimation [36]. Contrastive learning approaches in-\\r\\nclude PointContrast [60], which learns corresponding points from different cam-\\r\\nera views, and DepthContrast [68], which learns representations by comparing\\r\\ntransformations of a 3D point cloud/voxel. OcCo [53] learns an autoencoder\\r\\nto reconstruct the scene from the occluded input. However, due to the sampling\\r\\nvariance of the underlying 3D shapes, explicitly reconstructing the original point\\r\\ncloud will inevitably capture such variance. In this paper, we explore a simple\\r\\nbut effective discriminative classification pretext task to learn representations\\r\\nthat are robust to the sampling variance.\\r\\nMask based Pretraining. Masking out content has been used in various ways\\r\\nto improve model robustness including as a regularizer [45,17], data augmenta-\\r\\ntion [13,44,72], and self-supervised learning [7,12,20]. For self-supervised learn-\\r\\ning, the key idea is to train the model to predict the masked content based on its\\r\\nsurrounding context. The most successful approaches are built upon the Trans-\\r\\nformer [52], due in part to its token-based representation and ability to model\\r\\nlong-range dependencies.\\r\\n    In masked language modeling, BERT [12] and its variants [26,27] achieve\\r\\nstate-of-the-art performance across nearly all NLP downstream tasks by predict-\\r\\ning masked tokens during pretraining. Masked image modeling works [2,20] adopt\\r\\na similar idea for image pretraining. BEiT [2] maps image patches into discrete\\r\\ntokens, then masks a small portion of patches, and feeds the remaining visible\\r\\npatches into the Transformer to reconstruct the tokens of the masked patches.\\r\\nInstead of reconstructing tokens, the recent Masked AutoEncoder (MAE) [20]\\r\\nreconstructs the masked patches at the pixel level, and with a much higher mask\\r\\nratio of \\xe2\\x89\\xa5 70%. Following works try to predict high-level visual features such as\\r\\nHoG [56], or improve the representation capability of the encoder by aligning\\r\\nthe feature from both visible patches and masked patches [7]. To our knowl-\\r\\nedge, the only self-supervised mask modeling Transformer approach for point\\r\\nclouds is Point-BERT [65], which adopts a similar idea as BEiT [2]. However, to\\r\\nobtain satisfactory performance, it requires a pretrained dVAE and other auxil-\\r\\niary techniques (e.g., a momentum encoder [21]), which slow down training. We\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds       5\\r\\n\\r\\npropose a simple and effective masked autoencoding Transformer approach for\\r\\npoint clouds, which largely accelerates training (4.1\\xc3\\x97 faster than Point-BERT)\\r\\nwhile achieving state-of-the-art performance for various downstream tasks.\\r\\n\\r\\n3     Approach\\r\\nThe goal is to learn semantic feature representations without human supervision\\r\\nthat can perform well on downstream point cloud recognition tasks. We motivate\\r\\nour self-supervised learning design with a qualitative example. Fig. 1 \\xe2\\x80\\x9cUnmasked\\r\\nPoints\\xe2\\x80\\x9d shows a point cloud with a large portion (90%) of its points masked out.\\r\\nStill, based on our prior semantic understanding of the world, we as humans are\\r\\nable to say a number of things about it: (1) it might be an airplane; (2) if so, it\\r\\nshould consist of the head, body, tail, and wing; and even (3) roughly where these\\r\\nparts should be present. In other words, because we already know what airplanes\\r\\nare, we can recover the missing information from the small visible subset of the\\r\\npoint cloud. In a similar way, training a model to recover information about the\\r\\nmasked portion of the point cloud given the visible portion could force the model\\r\\nto learn object semantics.\\r\\n    However, even as humans, it can be difficult or impossible to precisely re-\\r\\nconstruct all missing points, since there are several ambiguous factors; e.g., the\\r\\nprecise thickness of the wings or the precise length of the airplane. If we are\\r\\ninstead given a sampled 3D point in space, and are asked to answer whether it\\r\\nlikely belongs to the object or not, we would be more confident in our answer.\\r\\nThis discriminative point classification task is much less ambiguous than the\\r\\nreconstruction task, yet still requires a deep understanding of object semantics\\r\\nin order to deduce the masked points from the small number of visible points.\\r\\n\\r\\n\\r\\n3.1   Masked Point Discrimination\\r\\n\\r\\nOur approach works as follows. We randomly partition each input point cloud\\r\\nP \\xe2\\x88\\x88 RN \\xc3\\x973 into two groups: masked M and unmasked U. We use the Transformer\\r\\nencoder to model the correlation between the sparsely-distributed unmasked\\r\\ntokens U via self-attention. Ideally, the resulting encoded latent representation\\r\\ntokens L should not only model the relationship between the unmasked points\\r\\nU, but also recover the latent distribution of masked points M, so as to perform\\r\\nwell on the pretraining task. We next sample a set of real query points Qreal\\r\\nand a set of fake query points Qf ake . The real query points are sampled from\\r\\nthe masked point set M, while the fake query points are randomly sampled from\\r\\nthe full 3D space. We then perform cross attention between each decoder query\\r\\nq \\xe2\\x88\\x88 {Qreal , Qf ake } and the encoder outputs: CA(q, L), to model the relationship\\r\\nbetween the masked query point and the unmasked points. Finally, we apply a\\r\\nbinary classification head to the decoder\\xe2\\x80\\x99s outputs and require it to distinguish\\r\\nbetween the real and fake queries.\\r\\n    We show in our experiments that our approach is both simple and effective,\\r\\nas it creates a pretext task that is difficult and meaningful enough for the model\\r\\nto learn rich semantic point cloud representations.\\r\\n\\x0c\\n\\n6       Liu et al.\\r\\n\\r\\n         Input Points         Grouping & Masking             unmasked\\r\\n                                                             masked\\r\\n                                                                          Point Queries          Discriminative Decoder\\r\\n                                                                                           xyz\\r\\n\\r\\n                            FPS                    Masking                                fake\\r\\n                                                                                          real\\r\\n                                                                                                        transformer\\r\\n                                                                                                          decoder\\r\\n\\r\\n                             Masked Encoder                                     \\xe2\\x80\\xa6\\r\\n\\r\\n                                                              transformer                              Downstream Tasks\\r\\n                                  Point Patchification          encoder                           Classification   Few-Shot\\r\\n\\r\\n                        \\xe2\\x80\\xa6              PointNet                                 \\xe2\\x80\\xa6                 Segmentation     Detection\\r\\n         Visible Point Groups                                  Visible Tokens\\r\\n\\r\\n\\r\\n\\r\\nFig. 2: MaskPoint architecture. We first uniformly sample point groups from\\r\\nthe point cloud, and partition them to masked and unmasked. We patchify the\\r\\nvisible point groups to token embeddings with PointNet and feed these visible\\r\\ntokens into the encoder. Then, a set of real query points are sampled from\\r\\nthe masked points, and a set of fake query points are randomly sampled from\\r\\n3D space. We train the decoder so that it distinguishes between the real and\\r\\nfake points. After pre-training, we discard the decoder and use the encoder for\\r\\ndownstream tasks. See Sec. 3.1 for details.\\r\\n\\r\\n\\r\\nDiscarding Ambiguous Points. Since we sample fake query points uniformly\\r\\nat random over the entire space, there will be some points that fall close to the\\r\\nobject\\xe2\\x80\\x99s surface. Such points can cause training difficulties since their target label\\r\\nis \\xe2\\x80\\x98fake\\xe2\\x80\\x99 even though they are on the object. In preliminary experiments, we find\\r\\nthat such ambiguous points can lead to vanishing gradients in the early stages of\\r\\ntraining. Thus, to stabilize training, we simply remove all fake points p\\xcc\\x82 \\xe2\\x88\\x88 Qf ake\\r\\nwhose euclidean distance is less than \\xce\\xb3 to any object (masked or unmasked) point\\r\\npi \\xe2\\x88\\x88 P: mini ||p\\xcc\\x82\\xe2\\x88\\x92pi ||2 < \\xce\\xb3. To address the size variance of the input point cloud,\\r\\n\\xce\\xb3 is dynamically selected per point cloud P: P\\xcc\\x82 = FPS(P), \\xce\\xb3 = minj\\xcc\\xb8=i ||P\\xcc\\x82i \\xe2\\x88\\x92P\\xcc\\x82j ||2 .\\r\\n\\r\\n3D Point Patchification. Feeding every single point into the Transformer\\r\\nencoder can yield an unacceptable cost due to the quadratic complexity of self-\\r\\nattention operators. Following [65,14], we adopt a patch embedding strategy that\\r\\nconverts input point clouds into 3D point patches.\\r\\n    Given the input point cloud P \\xe2\\x88\\x88 RN \\xc3\\x973 , S points {pi }Si=1 are sampled as\\r\\npatch centers using farthest point sampling [39]. We then gather the k nearest\\r\\nneighbors for each patch center to generate a set of 3D point patches {gi }Si=1 . A\\r\\nPointNet [38] is then applied to encode each 3D point patch gi \\xe2\\x88\\x88 Rk\\xc3\\x973 to a fea-\\r\\nture embedding fi \\xe2\\x88\\x88 Rd . In this way, we obtain S tokens and their corresponding\\r\\nfeatures {fi }Si=1 and center coordinates {pi }Si=1 .\\r\\n    Note that our patchification strategy can generate overlapping patches (e.g.,\\r\\nif two point centers are sampled to be close to each other). Although a prior\\r\\nstudy [59] showed that such overlapping patches can stabilize training in vision\\r\\ntransformers, in our case, they could undesirably leak information that would\\r\\nallow the network to take a shortcut solution; e.g., a portion of a masked patch\\r\\nbeing part of a neighboring unmasked patch. In practice, we set a very high\\r\\n\\x0c\\n\\n                                                                Masked Discrimination for Self-Supervised Learning on Point Clouds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                7\\r\\n\\r\\nmasking ratio (e.g., 90%), and consequently such overlap rarely happens and\\r\\ndoes not hinder training.\\r\\n\\r\\nTransformer Architecture. Our network architecture is shown in Fig. 2. We\\r\\nadopt the standard Transformer encoder [52] as the encoding backbone, where\\r\\neach Transformer encoder block consists of a multi-head self-attention (MSA)\\r\\nlayer and a feed forward network (FFN). As noted in Section 3.1, we construct\\r\\n                                                                            N \\xc3\\x973\\r\\npatch-wise features {fi }M         i=1 from the input point cloud P \\xe2\\x88\\x88 R          . Following [65],\\r\\n                                                             M\\r\\nwe apply the MLP positional embedding {posi }i=1 to the patch features {fi }M                 i=1 .\\r\\nThen, the class token E[s], which will be used for downstream classification\\r\\ntasks, is stacked to the top of the patch features {fi }M           i=1 ; i.e., the input to the\\r\\nTransformer encoder is I0 = {E[s], f1 + pos1 , f2 + pos2 , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , fM + posM }. Af-\\r\\nter n Transformer blocks, we get the feature embedding for each group In =\\r\\n                              n\\r\\n{En [s], f1n , f2n , \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , fM }.\\r\\n    During the decoding stage, Nq real query points Qreal and Nq fake query\\r\\npoints Qf ake are sampled. We pass the encoder output In and its positional\\r\\n                                                                                           Q 2N\\r\\nembedding {posi }M           i=1 , Qreal , Qf ake and their positional embedding {posi }i=1\\r\\ninto a one-layer Transformer decoder. Cross attention is only performed between\\r\\nthe queries and encoder keys/values, but not between different queries. Finally,\\r\\nthe decoder output goes through an MLP classification head, which is trained\\r\\nwith the binary focal loss [29], since there can be a large imbalance between\\r\\npositive and negative samples.\\r\\n    For downstream tasks, the point patchification module and Transformer en-\\r\\ncoder will be used with their pretrained weights as initialization.\\r\\n\\r\\nAn Information Theoretic Perspective. Here, we provide an information\\r\\ntheoretic perspective to our self-supervised learning objective, using mutual in-\\r\\nformation. The mutual information between random variables X and Y , I(X; Y ),\\r\\nmeasures the amount of information that can be gained about random variable\\r\\nX from the knowledge about the other random variable Y .\\r\\n    Ideally, we would like the model to learn a rich feature representation of\\r\\nthe point cloud: the latent representation L from our encoder E should contain\\r\\nenough information to recover the original point cloud P, i.e., we would like to\\r\\nmaximize the mutual information I(P; L). However, directly estimating I(P; L)\\r\\nis hard since we need to know the exact probability distribution of P (P|L).\\r\\nFollowing [6], we instead use auxiliary distribution Q to approximate it:\\r\\n\\r\\n      \\\\begin {aligned} I(\\\\mathcal {P}; \\\\mathcal {L}) &=-H(\\\\mathcal {P} | \\\\mathcal {L}) + H(\\\\mathcal {P}) \\\\\\\\ &=\\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P} | \\\\mathcal {L})}[\\\\log P(p^{\\\\prime } | x)]]+H(\\\\mathcal {P}) \\\\\\\\ &=\\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\underbrace {D_{\\\\mathrm {KL}}(P(\\\\cdot | x) \\\\| Q(\\\\cdot | x))}_{\\\\geq 0} + \\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P}|\\\\mathcal {L})}[\\\\log Q(p^{\\\\prime } | x)]]+H(\\\\mathcal {P}) \\\\\\\\ & \\\\geq \\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P}|\\\\mathcal {L})}[\\\\log Q(p^{\\\\prime } | x)]]+H(\\\\mathcal {P}) \\\\end {aligned} \\r\\n\\r\\n\\r\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (1)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nLemma 3.1. For random variables X, Y and function f (x, y) under suitable\\r\\nregularity conditions: Ex\\xe2\\x88\\xbcX,y\\xe2\\x88\\xbcY |x [f (x, y)] = Ex\\xe2\\x88\\xbcX,y\\xe2\\x88\\xbcY |x,x\\xe2\\x80\\xb2 \\xe2\\x88\\xbcX|y [f (x\\xe2\\x80\\xb2 , y)].\\r\\n\\x0c\\n\\n8                  Liu et al.\\r\\n\\r\\n    Therefore, we can define a variational lower bound, LI (Q, L), of the mutual\\r\\ninformation, I(P; L):\\r\\n                                        \\\\begin {aligned} L_{I}(Q, \\\\mathcal {L}) &=\\\\mathbb {E}_{p \\\\sim P(\\\\mathcal {P}), x \\\\sim \\\\mathcal {L}}[\\\\log Q(p|x)] + H(\\\\mathcal {P}) \\\\\\\\ &=\\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P}|\\\\mathcal {L})}[\\\\log Q(p^{\\\\prime } | x)]] + H(\\\\mathcal {P}) \\\\\\\\ & \\\\leq I(\\\\mathcal {P}; \\\\mathcal {L}) \\\\end {aligned} \\r\\n                                                                                                                                                                                                                                                                                                                                                                                    (2)\\r\\n\\r\\n\\r\\nTherefore, we have:\\r\\n      \\\\max I(\\\\mathcal {P}; \\\\mathcal {L}) \\\\iff \\\\max L_{I}(Q, \\\\mathcal {L}) \\\\iff \\\\max \\\\mathbb {E}_{x \\\\sim \\\\mathcal {L}}[\\\\mathbb {E}_{p^{\\\\prime } \\\\sim P(\\\\mathcal {P}|\\\\mathcal {L})}[\\\\log Q(p^{\\\\prime } | x)]]  (3)\\r\\n    Previous works use the Chamfer distance to approximate such auxiliary func-\\r\\ntion Q, but it has the disadvantage of being sensitive to point sampling variance\\r\\n(discussed in detail in Sec. 3.2). Thus, we instead represent the point cloud distri-\\r\\nbution with occupancy values within the tightest 3D bounding box of the point\\r\\ncloud: B \\xe2\\x88\\x88 {x, y, z, o}L , where (x, y, z) \\xe2\\x88\\x88 R3 , o \\xe2\\x88\\x88 {0, 1}, and L is the number\\r\\nof densely sampled points. We let the output of Q denote the continuous dis-\\r\\ntribution of the occupancy value o\\xcc\\x82, where Q(\\xc2\\xb7) \\xe2\\x88\\x88 [0, 1]. In our implementation,\\r\\nas discussed in Sec. 3.1, we construct a set of real query points and fake query\\r\\npoints, assign them with the corresponding occupancy labels, and optimize the\\r\\nprobability outputs from the model with a binary classification objective.\\r\\n\\r\\n3.2           Why not reconstruction, as in MAE?\\r\\nIn this section, we delve into the details on why a reconstruction objective (i.e.,\\r\\nreconstructing the original point cloud from the unmasked points) as used in the\\r\\nrelated Masked AutoEncoder (MAE) [20] approach for images would not work\\r\\nfor our point cloud setting.\\r\\n    First, in MAE, the self-supervised learning task is to reconstruct the masked\\r\\npatches, based on the input image\\xe2\\x80\\x99s unmasked (visible) patches. Specifically,\\r\\ngiven the 2D spatial position for each masked image patch query, the objective is\\r\\nto generate its RGB pixel values. In our case, the analogue would be to generate\\r\\nthe spatial xyz values for a masked 3D point patch query \\xe2\\x80\\x93 which would be\\r\\ntrivial for the model since the query already contains the corresponding spatial\\r\\ninformation. Such a trivial solution will result in perfect zero-loss, and prevent\\r\\nthe model from learning meaningful feature representations.\\r\\n    Another issue with the reconstruction objective for point clouds is that there\\r\\nwill be point sampling variance. Specifically, the true 3D shape of the object will\\r\\nbe a continuous surface, but a point cloud will a discrete sampling of it. Suppose\\r\\nwe sample two such point clouds, and denote the first set as the \\xe2\\x80\\x9cground truth\\xe2\\x80\\x9d\\r\\ntarget, and the second set as the prediction of a model. Although both sets\\r\\nreflect the same geometric shape of the object, the Chamfer distance (which can\\r\\nbe used to measure the shape difference between the two point sets) between\\r\\nthem is non-zero (i.e., there would be a loss). Thus, minimizing the Chamfer\\r\\ndistance would force the model to generate predictions that exactly match the\\r\\nfirst set. And since the first set is just one sampling of the true underlying\\r\\ndistribution, this can be an unnecessarily difficult optimization problem that\\r\\nleads to suboptimal model performance.\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds      9\\r\\n\\r\\n4     Experiments\\r\\n\\r\\nWe evaluate the pre-trained representation learned by the proposed model on\\r\\na variety of point cloud downstream tasks, including object classification, part\\r\\nsegmentation, object detection, and few-shot object classification. We also visu-\\r\\nalize the reconstruction results from masked point clouds, to qualitatively study\\r\\nthe effect of our pretraining. Finally, we perform ablation studies on masking\\r\\nstrategies and decoder designs.\\r\\n\\r\\nPretraining Datasets. (1) ShapeNet [4] has 50,000 unique 3D models from\\r\\n55 common object categories, and is used as our pre-training dataset for object\\r\\nclassification, part segmentation, and few-shot classification. For ShapeNet pre-\\r\\ntraining, we sample 1024 points from each 3D model as the inputs. We follow\\r\\n[65] to sample 64 point groups, each containing 32 points.\\r\\n    (2) We also use single-view depth map videos from the popular ScanNet [9]\\r\\ndataset, which contains around 2.5 million RGBD scans. We do not use its RGB\\r\\ninformation in this paper. We adopt similar pre-processing steps as DepthCon-\\r\\ntrast [68], but we generate a smaller subset of the dataset than in [68], which we\\r\\ncall \\xe2\\x80\\x98ScanNet-Medium\\xe2\\x80\\x99, to accelerate pretraining. ScanNet-Medium is generated\\r\\nby sampling every 10-th frame from ScanNet, resulting in \\xe2\\x88\\xbc25k samples. We use\\r\\nScanNet-Medium (only geometry information) as the pre-training dataset for 3D\\r\\nobject detection. For pretraining, we sample 20k points from each 3D scene scan\\r\\nas the input. We follow [34] to sample 2048 groups, each containing 64 points.\\r\\n\\r\\nTransformer Encoder. We construct a 12-layer standard Transformer en-\\r\\ncoder, named PointViT, for point cloud understanding. Following Point-BERT\\r\\n[65], we set the hidden dimension of each encoder block to 384, number of heads\\r\\nto 6, FFN expansion ratio to 4, and drop rate of stochastic depth [23] to 0.1.\\r\\n\\r\\nTransformer Decoder. We use a single-layer Transformer decoder for pre-\\r\\ntraining. The configuration of the attention block is identical to the encoder.\\r\\n\\r\\nTraining Details. Following [65], we pretrain with the AdamW [33] optimizer\\r\\nwith a weight decay of 0.05, and a learning rate of 5 \\xc3\\x97 10\\xe2\\x88\\x924 decayed with the\\r\\ncosine schedule. The model is trained for 300 epochs with a batch size of 128,\\r\\nwith random scaling and translation data augmentation. Only for ModelNet40\\r\\nexperiments, we also pretrain with a MoCo loss [21], following [65]. However,\\r\\nwe do not use it for any other datasets, unlike [65], which always uses it. For\\r\\nfinetuning and additional training details, please see supp.\\r\\n\\r\\n\\r\\n4.1   3D Object Classification\\r\\n\\r\\nDatasets. We compare the performance of object classification on two datasets:\\r\\nthe synthetic ModelNet40 [58], and real-world ScanObjectNN [51]. ModelNet40\\r\\n[58] consists of 12,311 CAD models from 40 classes. We follow the official data\\r\\nsplitting scheme in [58]. We evaluate the overall accuracy (OA) over all test\\r\\nsamples. ScanObjectNN [51] is a more challenging point cloud benchmark that\\r\\n\\x0c\\n\\n10     Liu et al.\\r\\n\\r\\nMethod                 SSL #point OA                                 OA\\r\\n                                             Method\\r\\nPointNet [38]               1k    89.2                          OBJ BG PB\\r\\nPointNet++ [39]             1k    90.7       PointNet [38]      79.2 73.3 68.0\\r\\nPointCNN [28]               1k    92.2       PointNet++ [39]    84.3 82.3 77.9\\r\\nSpiderCNN [61]              1k    92.4       PointCNN [28]      85.5 86.1 78.5\\r\\nPointWeb [70]               1k    92.3       SpiderCNN [61]     79.5 77.1 73.7\\r\\nPointConv [57]              1k    92.5       DGCNN [55]         86.2 82.8 78.1\\r\\nDGCNN [55]                  1k    92.9       BGA-DGCNN [51]      \\xe2\\x80\\x93    \\xe2\\x80\\x93 79.7\\r\\nKPConv [48]                 1k    92.9       BGA-PN++ [51]       \\xe2\\x80\\x93    \\xe2\\x80\\x93 80.2\\r\\nDensePoint [30]             1k    93.2       PointViT           80.6 79.9 77.2\\r\\nPosPool [32]                5k    93.2       PointViT-OcCo [53] 85.5 84.9 78.8\\r\\nRSCNN [31]                  5k    93.6       Point-BERT [65]    88.1 87.4 83.1\\r\\n[T] Point Trans. [16]       1k    92.8       MaskPoint (Ours) 89.3 88.1 84.3\\r\\n[T] Point Trans. [71]        \\xe2\\x80\\x93    93.7       Table 2: Shape Classification\\r\\n[T] PCT [19]                1k    93.2       on ScanObjectNN [51]. OBJ:\\r\\n[ST] PointViT               1k    91.4       object-only; BG: with background;\\r\\n[ST] PointViT-OcCo [53] \\xe2\\x9c\\x93   1k    92.1\\r\\n                                             PB: BG with manual perturbation.\\r\\n[ST] Point-BERT [65]    \\xe2\\x9c\\x93   1k    93.2\\r\\n[ST] MaskPoint (Ours)   \\xe2\\x9c\\x93   1k 93.8\\r\\n                                                                        mIoU\\r\\nTable 1: Shape Classification on              Method\\r\\n                                                                    cat.    ins.\\r\\nModelNet40 [58]. With a standard              PointNet [38]         80.4   83.7\\r\\nTransformer backbone, our approach            PointNet++ [39]       81.9   85.1\\r\\nsignificantly outperforms training-from-      DGCNN [55]            82.3   85.2\\r\\nscratch baselines and SOTA pretrain-          PointViT              83.4   85.1\\r\\ning methods. It even outperforms Point-       PointViT-OcCo [53]    83.4   85.1\\r\\nTransformer [71], which uses an attention     Point-BERT [65]       84.1   85.6\\r\\noperator specifically designed for point      MaskPoint (Ours)      84.4   86.0\\r\\nclouds. *SSL: Self-supervised pretrain-      Table 3: Part Segmentation on\\r\\ning. [T]: Transformer-based networks         ShapeNetPart [64]. Our method\\r\\nwith special designs for point clouds.       also works well on dense prediction\\r\\n[ST]: Standard Transformer network.          tasks like segmentation.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nconsists of 2902 unique objects in 15 categories collected from noisy real-world\\r\\nscans. It has three splits: OBJ (object only), BG (with background), PB (with\\r\\nbackground and manually added perturbations). We evaluate the overall accu-\\r\\nracy (OA) over all test samples on all three splits.\\r\\n\\r\\nModelNet40 Results. Table 1 shows ModelNet40 [58] results. With 1k points,\\r\\nour approach achieves a significant 2.4% OA improvement compared to training\\r\\nfrom scratch (PointViT). It also brings a 1.7% gain over OcCo [53] pretraining,\\r\\nand 0.6% gain over Point-BERT [65] pretraining. The significant improvement\\r\\nover the baselines indicates the effectiveness of our pre-training method. Notably,\\r\\nfor the first time, with 1k points, a standard vision transformer architecture pro-\\r\\nduces competitive performance compared to sophisticatedly designed attention\\r\\noperators from PointTransformer [71] (93.8% vs 93.7%).\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds    11\\r\\n\\r\\n                                   5-way                10-way\\r\\n         Method\\r\\n                             10-shot    20-shot    10-shot    20-shot\\r\\n         DGCNN [53]         91.8 \\xc2\\xb1 3.7 93.4 \\xc2\\xb1 3.2 86.3 \\xc2\\xb1 6.2 90.9 \\xc2\\xb1 5.1\\r\\n         DGCNN-OcCo [53] 91.9 \\xc2\\xb1 3.3 93.9 \\xc2\\xb1 3.2 86.4 \\xc2\\xb1 5.4 91.3 \\xc2\\xb1 4.6\\r\\n         PointViT           87.8 \\xc2\\xb1 5.3 93.3 \\xc2\\xb1 4.3 84.6 \\xc2\\xb1 5.5 89.4 \\xc2\\xb1 6.3\\r\\n         PointViT-OcCo [53] 94.0 \\xc2\\xb1 3.6 95.9 \\xc2\\xb1 2.3 89.4 \\xc2\\xb1 5.1 92.4 \\xc2\\xb1 4.6\\r\\n         Point-BERT [65]    94.6 \\xc2\\xb1 3.1 96.3 \\xc2\\xb1 2.7 91.0 \\xc2\\xb1 5.4 92.7 \\xc2\\xb1 5.1\\r\\n         MaskPoint (Ours) 95.0 \\xc2\\xb1 3.7 97.2 \\xc2\\xb1 1.7 91.4 \\xc2\\xb1 4.0 93.4 \\xc2\\xb1 3.5\\r\\n          Table 4: Few-shot classfication on ModelNet40 [58].\\r\\n\\r\\n\\r\\n\\r\\nScanObjectNN Results. We next conduct experiments using the real-world\\r\\nscan dataset ScanObjectNN [51]. Table 2 shows the results. Our approach achieves\\r\\nSOTA performance on all three splits. On the hardest PB split, our approach\\r\\nachieves a large 7.1% OA improvement compared to training from scratch (Point-\\r\\nViT). It achieves a 5.5% gain over OcCo [53] pretraining, and a 1.2% gain over\\r\\nPoint-BERT [65] pretraining. The large improvement over the baselines high-\\r\\nlights the transferability of our model\\xe2\\x80\\x99s self-supervised representation, as there\\r\\nis a significant domain gap between the clean synthetic ShapeNet [4] dataset\\r\\nused for pretraining and the noisy real-world ScanObjectNN [51] dataset.\\r\\n    We believe the performance gain over OcCo [53] and Point-BERT [65] is\\r\\nmainly because of our discriminative pretext task. OcCo suffers from the sam-\\r\\npling variance issue (Sec. 3.2) as it uses a reconstruction-based objective in\\r\\npretraining. Compared to Point-BERT, we do not use the point patch mixing\\r\\ntechnique, which mixes two different point clouds. This could introduce unneces-\\r\\nsary noise and domain shifts to pretraining and harm downstream performance.\\r\\n\\r\\n4.2   3D Part Segmentation\\r\\nDataset. ShapeNetPart [64] consists of 16,880 models from 16 shape categories,\\r\\nwith 14,006 models for training and 2,874 for testing. It contains 50 different\\r\\nparts in total, and the number of parts for each category is between 2 and 6.\\r\\nWe use the sampled point sets produced by [39] for a fair comparison with prior\\r\\nwork. We report per-category mean IoU (cat. mIoU) and mean IoU averaged\\r\\nover all test instances (ins. mIoU).\\r\\nResults. Table 3 shows the results (per-category IoU is in supp). Our approach\\r\\noutperforms the training from the scratch (PointViT) and OcCo-pretraining\\r\\nbaselines by 1.0%/0.9% in cat./ins. mIoU. It also produces a 0.3%/0.4% gain\\r\\ncompared to Point-BERT. Thanks to our dense discriminative pretraining ob-\\r\\njective, in which we densely classify points over the 3D space, we are able to\\r\\nobtain good performance when scaling to dense prediction tasks.\\r\\n\\r\\n4.3   Few-shot Classification\\r\\nWe conduct few-shot classification experiments on ModelNet40 [58], following\\r\\nthe settings in [65]. The standard experiment setting is the \\xe2\\x80\\x9cK-way N -shot\\xe2\\x80\\x9d\\r\\n\\x0c\\n\\n12     Liu et al.\\r\\n\\r\\nconfiguration, where K classes are first randomly selected, and then N + 20\\r\\nobjects are sampled for each class. We train the model on K\\xc3\\x97N samples (support\\r\\nset), and evaluate on the remaining K \\xc3\\x97 20 samples (query set). We compare our\\r\\napproach with OcCo and Point-BERT, which are the current state-of-the-art.\\r\\n    We perform experiments with 4 settings, where for each setting, we run the\\r\\ntrain/evaluation on 10 different sampled splits, and report the mean and std over\\r\\nthe 10 runs. Table 4 shows the results. Our approach achieves the best perfor-\\r\\nmance for all settings. It demonstrate an absolute gain of 7.2%/3.8%/4.6%/2.5%\\r\\nover the PointViT training from the scratch baseline. When comparing to pre-\\r\\ntraining baselines, it outperforms OcCo by 1.0%/1.3%/1.5%/1.0%, and outper-\\r\\nforms Point-BERT by 0.4%/0.9%/0.4%/0.7%. It also clearly outperforms the\\r\\nDGCNN baselines. Our state-of-the-art performance on few-shot classification\\r\\nfurther demonstrates the effectiveness of our pretraining approach.\\r\\n\\r\\n4.4   3D Object Detection\\r\\nOur most closely related work, Point-BERT [65] showed experiments only on\\r\\nobject-level classification and segmentation tasks. In this paper, we evaluate\\r\\na model\\xe2\\x80\\x99s pretrained representation on a more challenging scene-level down-\\r\\nstream task: 3D object detection on ScanNetV2 [9], which consists of real-world\\r\\nrichly-annotated 3D reconstructions of indoor scenes. It comprises 1201 training\\r\\nscenes, 312 validation scenes and 100 hidden test scenes. Axis-aligned bounding\\r\\nbox labels are provided for 18 object categories. For this experiment, we adopt\\r\\n3DETR [34] as the downstream model for both our method and Point-BERT.\\r\\n3DETR is an end-to-end transformer-based 3D object detection pipeline. Dur-\\r\\ning finetuning, the input point cloud is first downsampled to 2048 points via\\r\\na VoteNet-style Set Aggregation (SA) layer [37,39], which then goes through\\r\\n3-layer self-attention blocks. The decoder is composed of 8-layer cross-attention\\r\\nblocks. For a fair comparison with the 3DETR train-from-scratch baseline, we\\r\\nstrictly follow its architecture of SA layer and encoder during pretraining, whose\\r\\nweights are transferred during finetuning. Our pretraining dataset is ScanNet\\r\\nMedium, as described in Sec. 4.\\r\\n    Table 5 shows that our method surpasses the 3DETR train-from-scratch\\r\\nbaseline by a large margin (+1.3mAP@0.25 and +2.7mAP@0.50). Interestingly,\\r\\nPoint-BERT brings nearly no improvement compared to training from scratch.\\r\\nThe low mask rate and discrete tokens learned from dVAE may impede Point-\\r\\nBERT from learning meaningful representations for detection. Also, the 3DETR\\r\\npaper [34] found that increasing the number of encoding layers in 3DETR brings\\r\\nonly a small benefit to its detection performance. Here we increase the num-\\r\\nber of layers from 3 to 12, which leads to a large performance improvement\\r\\n(+2.1mAP@0.25 and +4.1mAP@0.50) for our approach compared to training\\r\\nfrom scratch. This result demonstrates that by pre-training on a large unlabeled\\r\\ndataset, we can afford to increase the model\\xe2\\x80\\x99s encoder capacity to learn richer\\r\\nrepresentations. Finally, note that we also include VoteNet based methods at\\r\\nthe top of Table 5 as a reference, but the numbers are not directly comparable\\r\\nas they are using a different (non Transformer-based) detector.\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds      13\\r\\n\\r\\n       Methods                         SSL   Pretrained Input   AP25   AP50\\r\\n       VoteNet [37]                                  -          58.6   33.5\\r\\n       STRL [24]                       \\xe2\\x9c\\x93           Geo          59.5   38.4\\r\\n       Implicit Autoencoder [62]       \\xe2\\x9c\\x93           Geo          61.5   39.8\\r\\n       RandomRooms [42]                \\xe2\\x9c\\x93           Geo          61.3   36.2\\r\\n       PointContrast [60]              \\xe2\\x9c\\x93           Geo          59.2   38.0\\r\\n       DepthContrast [68]              \\xe2\\x9c\\x93           Geo          61.3    \\xe2\\x80\\x93\\r\\n       DepthContrast [68]              \\xe2\\x9c\\x93       Geo + RGB        64.0   42.9\\r\\n       3DETR [34]                                    -          62.1   37.9\\r\\n       Point-BERT [65]                 \\xe2\\x9c\\x93           Geo          61.0   38.3\\r\\n       MaskPoint (Ours)                \\xe2\\x9c\\x93           Geo          63.4   40.6\\r\\n       MaskPoint (Ours, 12 Enc)        \\xe2\\x9c\\x93           Geo          64.2   42.0\\r\\nTable 5: 3D object detection results on ScanNet validation set. The backbone of\\r\\nour pretraining model and Point-BERT [65] is 3DETR [34]. All other methods\\r\\nuse VoteNet [38] as the finetuning backbone. Only geometry information is fed\\r\\ninto the downstream task. \\xe2\\x80\\x9cInput\\xe2\\x80\\x9d column denotes input type for the pretraining\\r\\nstage. \\xe2\\x80\\x9cGeo\\xe2\\x80\\x9d denotes geometry information. Note that DepthContrast (Geo +\\r\\nRGB) model uses a heavier backbone (PointNet 3x) for downstream tasks.\\r\\n\\r\\n\\r\\n        Original   Masked     Recon.         Original     Masked       Recon.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3: Reconstruction results. By reformulating reconstruction as a discrim-\\r\\ninative occupancy classification task, we achieve a similar learning objective to\\r\\ngenerative reconstruction while being robust to point sampling variance. Even\\r\\nwith a high 90% mask ratio, our approach recovers the overall shape of the\\r\\noriginal point cloud, without overfitting.\\r\\n\\r\\n\\r\\n4.5   Qualitative Reconstructions\\r\\nAlthough our model is not trained with the reconstruction objective, we can still\\r\\nreconstruct the point cloud with our decoder by classifying the densely sampled\\r\\npoints from the point cloud\\xe2\\x80\\x99s full 3D bounding box space: Prec = {x|D(x|L) =\\r\\n1}. Fig. 3 shows the reconstruction results with masking ratio of 90%. Even with\\r\\nsuch a large masking rate, our model is able to reconstruct the overall shape of\\r\\nthe original point cloud, without overfitting. Fig. 3 (bottom right) shows how our\\r\\nmodel uses its learned prior to perform reasonable reconstructions: a shoe shelf\\r\\nwhere the stand on the right is masked. We hypothesize that it first recognizes\\r\\nit as a shoe shelf (without any labels), and learns that shoe shelves are usually\\r\\nsymmetric from left-to-right, to reconstruct the right part.\\r\\n\\x0c\\n\\n14       Liu et al.\\r\\n\\r\\n      ratio   OA           ratio   OA           # queries   OA       # dec.   OA\\r\\n      0.25    83.2         0.25    82.4             64      83.7       1      84.3\\r\\n      0.50    83.7         0.50    83.8            256      84.3       3      83.7\\r\\n      0.75    84.1         0.75    83.7           1024      83.9       6      83.9\\r\\n      0.90    84.3         0.90    84.1\\r\\n\\r\\n(a) Mask rate (random) (b) Mask rate (block)   (c) # dec. queries   (d) # dec. layers\\r\\n\\r\\nTable 6: Ablations on ScanObjectNN [51] (PB split). Our findings are: a larger\\r\\nmasking ratio generally yields better performance; random masking is slightly\\r\\nbetter than block masking; 256-query provides a good balance between informa-\\r\\ntion and noise; a thin decoder ensures rich feature representation in the encoder\\r\\nand benefits downstream performance.\\r\\n\\r\\n\\r\\n\\r\\n4.6    Ablation Studies\\r\\nMasking Strategy. We show the influence of different masking strategies in\\r\\nTable 6a, 6b. First, we observe that a higher masking ratio generally yields\\r\\nbetter performance, regardless of sampling type. This matches our intuition that\\r\\na higher masking ratio creates a harder and more meaningful pretraining task.\\r\\nFurther, with a higher masking ratio, random masking is slightly better than\\r\\nblock masking. Therefore, we use a high mask rate of 90% with random masking.\\r\\nPretraining Decoder Design. We study the design of the pretraining de-\\r\\ncoder in Table 6c, 6d, by varying the number of decoder queries and layers. The\\r\\nnumber of decoder queries influence the balance between the classification of\\r\\nthe real points and fake points. We find that 256-query is the sweet spot, where\\r\\nmore queries could introduce too much noise, and fewer queries could result in\\r\\ninsufficient training information.\\r\\n    The modeling power of the decoder affects the effectiveness of the pretraining:\\r\\nideally, we want the encoder to only encode the features, while the decoder only\\r\\nprojects the features to the pre-training objective. Any imbalance in either way\\r\\ncan harm the model\\xe2\\x80\\x99s performance on downstream tasks. We find that a single-\\r\\nlayer decoder is sufficient for performing our proposed point discrimination task,\\r\\nand having more decoder layers harms the model\\xe2\\x80\\x99s performance.\\r\\n\\r\\n\\r\\n5     Conclusion\\r\\nWe proposed a discriminative masked point cloud pretraining framework, which\\r\\nfacilitates a variety of downstream tasks while significantly reducing the pre-\\r\\ntraining time compared to the prior Transformer-based state-of-the-art method.\\r\\nWe adopted occupancy values to represent the point cloud, forming a simpler\\r\\nyet effective binary pretraining objective function. Extensive experiments on 3D\\r\\nshape classification, detection, and segmentation demonstrated the strong per-\\r\\nformance of our approach. Currently, we randomly mask local point groups to\\r\\npartition the point cloud into masked and unmasked sets. It could be interesting\\r\\nto explore ways to instead learn how to mask the points. We hope our research\\r\\ncan raise more attention to mask based self-supervised learning on point clouds.\\r\\n\\x0c\\n\\n           Masked Discrimination for Self-Supervised Learning on Point Clouds                                 15\\r\\n\\r\\nSupplementary Material\\r\\nA       More results\\r\\nShapeNetPart In Table 7, we compare the categorical mIoU on ShapeNet-\\r\\nPart with other methods. With a PointViT backbone, we get the highest class\\r\\nmIoU at 84.4% and the highest instance mIoU at 86.0%, outperforming pre-\\r\\nvious self-supervised learning approaches (OcCo [53] and Point-BERT [65]). It\\r\\nalso outperforms standard train-from-scratch point cloud backbones like Point-\\r\\nNet++ [39] and DGCNN [55]. For all categories, our method either has the\\r\\nhighest accuracy or is among the best. Thanks to our dense discriminative pre-\\r\\ntraining objective, in which we densely classify points over the 3D space, we are\\r\\nable to obtain good performance when scaling to dense prediction tasks like part\\r\\nsegmentation.\\r\\n\\r\\n\\r\\nMethods             cls. ins. aero bag cap car chair earp. guit. knif. lamp lapt. mot. mug pist. rock. skt. table\\r\\nPointNet [38]       80.4 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6\\r\\nPN++ [39]           81.9 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6\\r\\nDGCNN [55]          82.3 85.2 84.0 83.4 86.7 77.8 90.6 74.7 91.2 87.5 82.8 95.7 66.3 94.9 81.1 63.5 74.5 82.6\\r\\nPointViT         83.4 85.1 82.9 85.4 87.7 78.8 90.5 80.8 91.1 87.7 85.3 95.6 73.9 94.9 83.5 61.2 74.9 80.6\\r\\nOcCo [53]        83.4 85.1 83.3 85.2 88.3 79.9 90.7 74.1 91.9 87.6 84.7 95.4 75.5 94.4 84.1 63.1 75.7 80.8\\r\\nPN-BERT [65]     84.1 85.6 84.3 84.8 88.0 79.8 91.0 81.7 91.6 87.9 85.2 95.6 75.6 94.7 84.3 63.4 76.3 81.5\\r\\nMaskPoint (Ours) 84.4 86.0 84.2 85.6 88.1 80.3 91.2 79.5 91.9 87.8 86.2 95.3 76.9 95.0 85.3 64.4 76.9 81.8\\r\\nTable 7: Part segmentation results on ShapeNetPart [64]. Bold and underline\\r\\nnumbers denote best and second best performance, respectively.\\r\\n\\r\\n\\r\\nScanNet Table 8 reports per-class average precision on 18 classes of Scan-\\r\\nNetV2 with a 0.25 box IoU threshold. Relying on purely geometric information,\\r\\nour method exceeds 3DETR [34] in detecting objects like curtain, garbagebin,\\r\\ntable, desk, etc, where geometry is a strong cue for recognition. These results\\r\\nindicate that our mask based discriminative pretraining framework is effective\\r\\nin learning strong geometric representations. More importantly, our model out-\\r\\nperforms 3DETR on classes where it has relatively low AP, e.g., picture, door,\\r\\ncurtain, refrigerator, etc, which demonstrates the usefulness of pretraining: with\\r\\nthe pretrained knowledge relevant to those hard classes, the model is able to\\r\\nmake more accurate predictions than the training from the scratch baseline.\\r\\n\\r\\n\\r\\nModel           AP25 cab. bed cha. sofa tab. door win. boo. pic. cou. desk cur. ref. sho. toi. sink bat. gar.\\r\\n3DETR [34] 62.2 50.2 87.0 86.0 87.1 61.6 46.6 40.1 54.5 9.1 62.8 69.5 48.4 50.9 68.4 97.9 67.6 85.9 45.8\\r\\nOurs       63.4 51.8 82.5 85.9 86.8 69.8 50.9 36.9 47.3 10.7 59.6 76.3 65.9 55.6 66.4 99.1 61.5 83.7 49.8\\r\\nOurs (12\\xc3\\x97) 64.2 49.5 81.0 87.2 86.3 65.2 51.3 42.6 56.7 16.2 56.8 73.8 59.6 56.0 77.0 97.8 66.6 85.0 47.7\\r\\nTable 8: 3D object detection scores per category on the ScanNetV2 dataset,\\r\\nevaluated with bbox mIoU 0.25. Ours (12\\xc3\\x97): 12 encoder blocks.\\r\\n\\x0c\\n\\n16          Liu et al.\\r\\n\\r\\n Original    Masked p>0.01          p>0.025   p>0.05   p>0.075   p>0.10   p>0.20   p>0.30   p>0.40   p>0.50\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  low                                                                                                  high\\r\\n\\r\\n\\r\\nFig. 4: Reconstruction results. We densely perform the discriminative occu-\\r\\npancy classification task in 3D space, and visualize the predicted occupancy\\r\\nprobability. By varying the confidence threshold p\\xcc\\x82, we show that our model is\\r\\nable to predict a continuous probability distribution of the occupancy function.\\r\\n\\r\\n                         Original                 Masked                     Recon            high\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                                              low\\r\\n\\r\\n\\r\\nFig. 5: A closer look at occupancy distribution. Although there are no\\r\\npoints present in both red and purple regions of the masked point cloud, the\\r\\nreconstructed probability distribution correctly reflects that of the original point\\r\\ncloud: a lower occupancy in red region, and a higher occupancy in purple region.\\r\\n\\r\\n\\r\\n\\r\\nMore reconstruction visualizations We densely perform the discriminative\\r\\noccupancy classification task in 3D space, and visualize in Fig. 4 the predicted\\r\\noccupancy probability. In different columns, we vary the occupancy threshold \\xcf\\x84 ,\\r\\nand only show the points with occupancy probability prediction that is higher\\r\\nthan the given threshold. We can see that our model is able to output a continu-\\r\\nous probability distribution of the occupancy function, even if it is only trained\\r\\nwith discrete occupancy values from the sampled points.\\r\\n    When we take a closer look at the occupancy distribution, we find several\\r\\ninteresting clues on how the model is modeling the probability distribution im-\\r\\npressively well. We show our findings in Fig. 5. There are no points present in\\r\\nboth red and purple regions of the masked point cloud, while in the original point\\r\\ncloud, there are points present in the purple region, and no points are in the red\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds               17\\r\\n\\r\\nregion. In the reconstructed probability distribution, the model predicts a low\\r\\noccupancy probability in the red region, and a high occupancy in the purple\\r\\nregion.\\r\\n    We find such predictions align with how a human might understand the\\r\\nscene. First, although there are no points in the purple region of the masked\\r\\npoint cloud, given the partial view of the top-left region of the desk top, and\\r\\nthe regions where the desk legs are present, it is very likely that there are points\\r\\npresent in the purple region (top-right region of the desk top). As for the red\\r\\nregion, the model\\xe2\\x80\\x99s prediction can be interpreted as follows: usually desk tops\\r\\nare rectangle-shaped; however, there do exist desks whose surface shrinks inside\\r\\nthe region where the person sits. Given that there is not a decisive evidence\\r\\nthat indicates how this particular desk instance is shaped, the model produces\\r\\npredictions with probability around 0.7, which is lower than other regions that\\r\\nare more certain (yellow points in Fig. 5 \\xe2\\x80\\x9cRecon\\xe2\\x80\\x9d, with p>0.9).\\r\\n    These two intriguing and encouraging visualizations suggest that our pre-\\r\\ntrained model is capable of modeling a continuous occupancy probability distri-\\r\\nbution, and it has learned a deep understanding of the input scene.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  (a) Training from scratch   (b) Pretrained on ShapeNet   (c) Finetuned on ModelNet40\\r\\n\\r\\nFig. 6: t-SNE visualization of the encoder features for ModelNet40 under three\\r\\nsettings: (a) training from scratch, (b) pretrained on ShapeNet, and (c) finetuned\\r\\non ModelNet40.\\r\\n\\r\\nt-SNE visualizations We show the t-SNE visualizations of the extracted fea-\\r\\nture vectors from our approach in Fig. 6. We use the class token from the encoder\\r\\noutput as the high dimensional feature representation for t-SNE. Three setting\\r\\nare adopted here: (a) training from scratch, (b) pretraining on ShapeNet [4], and\\r\\n(c) finetuning on ModelNet40 [58].\\r\\n    When training the ModelNet40 classification model from scratch, the result-\\r\\ning features from different categories become heavily entangled, which can leads\\r\\nto less interpretable and robust predictions for new test-time inputs. In con-\\r\\ntrast, when pretraining the model on ShapeNet using our proposed MaskPoint,\\r\\nthe features are much more distinguishable from each other. Furthermore, after\\r\\nfinetuning on ModelNet40, the projected features from different classes become\\r\\nclearly separable from each other, which indicates the effectiveness of our ap-\\r\\nproach. Interestingly, the feature clusters in our approach are quite tight. Such\\r\\n\\x0c\\n\\n18     Liu et al.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n         (a)              (b)              (c)            (d)           (e)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n         (f)              (g)              (h)             (i)           (j)\\r\\n\\r\\nFig. 7: Qualitative results of 3D object detection on ScanNetV2 [9]. We\\r\\nshow ground truth in green and predictions in red bounding boxes.\\r\\n\\r\\n\\r\\nfeature layout indicates that we can learn a more compact and disjoint decision\\r\\nboundary, which has been evaluated to be critical in machine learning applica-\\r\\ntions such mixup [66,49] and uncertainty estimation in deep learning [15].\\r\\n3D object detection visualizations We show 3D object detection visualiza-\\r\\ntions of ScanNetV2 in Figure 7 with green ground truth bounding boxes and red\\r\\npredicted bounding boxes. Our model is capable of precisely localizing the obe-\\r\\njct (Fig. 7b and Fig. 7j). Results indicate that our masked based discriminative\\r\\npretraining can not only produce high-quality bounding boxes for the previously\\r\\nannotated objects, but also discover objects that are not annotated. For exam-\\r\\nple, in Fig. 7c, our model produces the a bounding box for the bookshelf in the\\r\\nlower region; in Fig. 7f, it correctly locates the sofa in the center of the room.\\r\\n\\r\\n\\r\\nB     Additional Implementation Details\\r\\nB.1   Pretraining\\r\\nTransformer Encoder. We follow the standard Transformer design in [52,65]\\r\\nto construct our point cloud Transformer backbone, PointViT. It consists of a\\r\\nlinear stack of 12 Transformer blocks, where each Transformer block contains\\r\\na multi-head self-attention (MHSA) layer and a feed-forward network (FFN).\\r\\nLayerNorm (LN) is adopted in both layers. Following [65], we use the MLP-\\r\\nbased positional embedding. We set the Transformer hidden dimension to 384,\\r\\nMHSA head number to 6, expansion rate of FFN to 4, stochastic drop path [23]\\r\\nrate to 0.1.\\r\\nFeed-forward network (FFN). Following [65], we use a two-layer MLP with\\r\\nReLU and dropout as the feed-forward network. Dropout rate is set to 0.1.\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds    19\\r\\n\\r\\n\\r\\n      Module                 Block     Cin   Cout   k   Nout     Cmiddle\\r\\n      Positional Embed.      MLP        3    384                   128\\r\\n      Point Classify Head    MLP       384    2                     64\\r\\n      Classification Head    MLP       768   Ncls                512, 256\\r\\n                             MLP       387   384                  384\\xc3\\x974\\r\\n                            DGCNN      384   512    4    128\\r\\n                            DGCNN      512   384    4    128\\r\\n                            DGCNN      384   512    4    256\\r\\n      Segmentation Head     DGCNN      512   384    4    256\\r\\n                            DGCNN      384   512    4    512\\r\\n                            DGCNN      512   384    4    512\\r\\n                            DGCNN      384   512    4   2048\\r\\n                            DGCNN      512   384    4   2048\\r\\nTable 9: Detailed module design of MaskPoint. Cin /Cout denotes the in-\\r\\nput/output channels, Cmiddle denotes the hidden channels of MLP modules,\\r\\nNout denotes the cardinality of the output point/feature set, k is the number of\\r\\nneighbors used in the k-NN operator.\\r\\n\\r\\n\\r\\nPositional Embeddings. Following [65], we use a two-layer MLP with GELU [22]\\r\\nas the positional embedding module. All Transformer modules share the same\\r\\npositional embedding MLP module. Detailed configuration is shown in Table 9.\\r\\n\\r\\nPoint Classification Head. We use a simple two-layer MLP with GELU [22]\\r\\nfor the point classification pretext task in pretraining. We use the binary focal\\r\\nloss [29] to balance the information from positive and negative samples. Detailed\\r\\nconfiguration is shown in Table 9.\\r\\n\\r\\nScanNet-Medium Pretraining Note that for ScanNet-Medium pretraining,\\r\\nwe use the encoder with 3 Transformer blocks, where each block still consists\\r\\nof a MHSA layer and a FFN layer. LN and MLP positional embedding are also\\r\\nutilized in the encoder. Following the downstream architecture of 3DETR [34],\\r\\nwe set the hidden dimension to be 256, the number of MHSA heads to be 4, and\\r\\nDropout rate to be 0.1 for the Transformer. The hidden dimension is set to be\\r\\n128 for the FFN layer.\\r\\n    For other settings such as positional embedding and classification head, the\\r\\nsetting is exactly the same as the ShapeNet pretraining setting.\\r\\n\\r\\n\\r\\nB.2   Finetuning\\r\\n\\r\\nClassification We use a three-layer MLP with dropout for the classification\\r\\nhead. The input feature to the classification head consists of two parts from the\\r\\nTransformer encoder: (1) the CLS token; (2) the max-pooled feature of other\\r\\noutput features. These two features are concatenated together and fed into the\\r\\nclassification head. Detailed configuration is shown in Table 9.\\r\\n\\x0c\\n\\n20      Liu et al.\\r\\n\\r\\n config              value                   config                   value\\r\\n epochs              300                     epochs                    300\\r\\n optimizer           AdamW                   optimizer              AdamW\\r\\n learning rate       5e-4                    learning rate             5e-4\\r\\n weight decay        5e-2                    weight decay              5e-2\\r\\n LR schedule         cosine decay            LR schedule          cosine decay\\r\\n warmup epochs       3                       warmup epochs              10\\r\\n augmentation        Scale/Translate\\r\\n                                             augmentation        Scale/Translate\\r\\n batch size          128\\r\\n                                             batch size          32(cls), 16(seg)\\r\\n # points            1024                    # points          1024(cls), 2048(seg)\\r\\n # patches           64                      # patches          64(cls), 128(seg)\\r\\n patch size          32                      patch size                 32\\r\\n mask ratio          0.90\\r\\n mask type           random                 Table 11: Finetuning setting on\\r\\nTable 10: Pretraining setting on            classification (cls) and segmentation\\r\\nShapeNet [4].                               (seg).\\r\\n\\r\\nPart Segmentation The standard Transformer only has a single-scale fea-\\r\\nture output, which is not suitable for common head designs for dense prediction\\r\\ntasks like segmentation. Following [65], after getting the feature outputs from\\r\\nthe Transformer encoder, we perform segmentation in two steps: (1) generating\\r\\na multi-scale feature pyramid from the Transformer encoder outputs; (2) apply-\\r\\ning a standard feature propagation head for point cloud segmentation on the\\r\\ngenerated multi-scale feature maps to generate dense predictions.\\r\\n    We obtain the feature maps f{4,8,12} \\xe2\\x88\\x88 RN3 \\xc3\\x97d from the 4th, 8th, 12th layer,\\r\\nand our goal is to convert them to a feature pyramid with different cardinality\\r\\nN{0,1,2,3} , where N0 is the cardinality of the original point cloud P, and N{1,2,3}\\r\\nare the desired cardinality of the feature maps at different scales; in our case,\\r\\nN{0,1,2,3} = {2048, 512, 256, 128}.\\r\\n    First, we use furthest point sampling (FPS) to downsample the original point\\r\\ncloud P0 to different resolutions: P{1,2,3} \\xe2\\x88\\x88 RN{1,2,3} \\xc3\\x973 , then a feature propaga-\\r\\ntion module is used to upsample the feature maps f{4,8,12} to the corresponding\\r\\n               up\\r\\ncardinality f{4,8,12} \\xe2\\x88\\x88 RN{1,2,3} \\xc3\\x97d .\\r\\n    After obtaining the multi-scale feature maps, we then apply the DGCNN\\r\\n                                                                              up\\r\\nmodule to propagate the features through different scales, f\\xcb\\x864 = DGCNN(f{4,8,12}     ).\\r\\n                                                            \\xcb\\x86\\r\\nAnother feature propagation layer is then applied on f4 for upsampling to the\\r\\nhighest resolution f\\xcb\\x860 \\xe2\\x88\\x88 RN0 \\xc3\\x97d .\\r\\n    Finally, we apply a pointwise MLP classifier on the features at the highest\\r\\nresolution f\\xcb\\x860 to obtain the segmentation results. Detailed configuration is shown\\r\\nin Table 9.\\r\\n\\r\\n3D Object Detection We strictly follow the setting of the original 3DETR [34]\\r\\nmodel as the downstream 3D object detector. The points are first donwsampled\\r\\nto 2048 points using a Set-Aggregation (SA) layer. The encoder is composed\\r\\nof 3 standard Transformer blocks. The decoder is comprised of 8 Transformer\\r\\nblocks using cross attention. During finetuning, only the weights of the SA layer\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds            21\\r\\n\\r\\nand the encoder are transferred to the downstream tasks. The finetuning epoch\\r\\nnumber is 1080, the optimizer is AdamW with learning rate of 5 \\xc3\\x97 10\\xe2\\x88\\x924 and\\r\\nweight decay of 0.1, the batch size is 8.\\r\\n\\r\\n\\r\\nReferences\\r\\n\\r\\n 1. Achituve, I., Maron, H., Chechik, G.: Self-supervised learning for domain adap-\\r\\n    tation on point clouds. In: Proceedings of the IEEE/CVF Winter Conference on\\r\\n    Applications of Computer Vision. pp. 123\\xe2\\x80\\x93133 (2021)\\r\\n 2. Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv\\r\\n    preprint arXiv:2106.08254 (2021)\\r\\n 3. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\\r\\n    to-end object detection with transformers. In: European conference on computer\\r\\n    vision. pp. 213\\xe2\\x80\\x93229. Springer (2020)\\r\\n 4. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,\\r\\n    Savarese, S., Savva, M., Song, S., Su, H., et al.: Shapenet: An information-rich\\r\\n    3d model repository. arXiv preprint arXiv:1512.03012 (2015)\\r\\n 5. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-\\r\\n    trastive learning of visual representations. In: III, H.D., Singh, A. (eds.) Proceed-\\r\\n    ings of the 37th International Conference on Machine Learning. Proceedings of\\r\\n    Machine Learning Research, vol. 119, pp. 1597\\xe2\\x80\\x931607. PMLR (13\\xe2\\x80\\x9318 Jul 2020),\\r\\n    https://proceedings.mlr.press/v119/chen20j.html\\r\\n 6. Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.: Info-\\r\\n    gan: Interpretable representation learning by information maximizing generative\\r\\n    adversarial nets. Advances in neural information processing systems 29 (2016)\\r\\n 7. Chen, X., Ding, M., Wang, X., Xin, Y., Mo, S., Wang, Y., Han, S., Luo, P., Zeng, G.,\\r\\n    Wang, J.: Context autoencoder for self-supervised representation learning. arXiv\\r\\n    preprint arXiv:2202.03026 (2022)\\r\\n 8. Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings\\r\\n    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\\r\\n    15750\\xe2\\x80\\x9315758 (2021)\\r\\n 9. Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nie\\xc3\\x9fner, M.: Scannet:\\r\\n    Richly-annotated 3d reconstructions of indoor scenes. In: Proceedings of the IEEE\\r\\n    conference on computer vision and pattern recognition. pp. 5828\\xe2\\x80\\x935839 (2017)\\r\\n10. Danila Rukhovich, Anna Vorontsova, A.K.: Fcaf3d: Fully convolutional anchor-free\\r\\n    3d object detection. arXiv preprint arXiv:2112.00322 (2021)\\r\\n11. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-\\r\\n    scale hierarchical image database. In: 2009 IEEE conference on computer vision\\r\\n    and pattern recognition. pp. 248\\xe2\\x80\\x93255. Ieee (2009)\\r\\n12. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\\r\\n    bidirectional transformers for language understanding. In: Proceedings of the 2019\\r\\n    Conference of the North American Chapter of the Association for Computational\\r\\n    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\\r\\n    pp. 4171\\xe2\\x80\\x934186. Association for Computational Linguistics, Minneapolis, Minnesota\\r\\n    (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology.org/\\r\\n    N19-1423\\r\\n13. DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net-\\r\\n    works with cutout. arXiv preprint arXiv:1708.04552 (2017)\\r\\n\\x0c\\n\\n22      Liu et al.\\r\\n\\r\\n14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\\r\\n    T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\\r\\n    An image is worth 16x16 words: Transformers for image recognition at scale. ICLR\\r\\n    (2021)\\r\\n15. Du, X., Wang, X., Gozum, G., Li, Y.: Unknown-aware object detection: Learning\\r\\n    what you don\\xe2\\x80\\x99t know from videos in the wild. Proceedings of the IEEE/CVF\\r\\n    Conference on Computer Vision and Pattern Recognition (2022)\\r\\n16. Engel, N., Belagiannis, V., Dietmayer, K.: Point transformer. IEEE Access 9,\\r\\n    134826\\xe2\\x80\\x93134840 (2021)\\r\\n17. Ghiasi, G., Lin, T.Y., Le, Q.V.: Dropblock: A regularization method for convolu-\\r\\n    tional networks. In: NeurIPS (2018)\\r\\n18. Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by pre-\\r\\n    dicting image rotations. In: International Conference on Learning Representations\\r\\n    (2018), https://openreview.net/forum?id=S1v4N2l0-\\r\\n19. Guo, M.H., Cai, J.X., Liu, Z.N., Mu, T.J., Martin, R.R., Hu, S.M.: Pct: Point\\r\\n    cloud transformer. Computational Visual Media 7(2), 187\\xe2\\x80\\x93199 (2021)\\r\\n20. He, K., Chen, X., Xie, S., Li, Y., Doll\\xc2\\xb4ar, P., Girshick, R.: Masked autoencoders\\r\\n    are scalable vision learners. arXiv preprint arXiv:2111.06377 (2021)\\r\\n21. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised\\r\\n    visual representation learning. In: Proceedings of the IEEE/CVF conference on\\r\\n    computer vision and pattern recognition. pp. 9729\\xe2\\x80\\x939738 (2020)\\r\\n22. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint\\r\\n    arXiv:1606.08415 (2016)\\r\\n23. Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with\\r\\n    stochastic depth. In: European conference on computer vision. pp. 646\\xe2\\x80\\x93661.\\r\\n    Springer (2016)\\r\\n24. Huang, S., Xie, Y., Zhu, S.C., Zhu, Y.: Spatio-temporal self-supervised represen-\\r\\n    tation learning for 3d point clouds. arXiv preprint arXiv:2109.00179 (2021)\\r\\n25. Jiang, Y., Chang, S., Wang, Z.: Transgan: Two pure transformers can make one\\r\\n    strong gan, and that can scale up. Advances in Neural Information Processing\\r\\n    Systems 34 (2021)\\r\\n26. Joshi, M., Chen, D., Liu, Y., Weld, D.S., Zettlemoyer, L., Levy, O.: Spanbert:\\r\\n    Improving pre-training by representing and predicting spans. Transactions of the\\r\\n    Association for Computational Linguistics 8, 64\\xe2\\x80\\x9377 (2020)\\r\\n27. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: A\\r\\n    lite bert for self-supervised learning of language representations. In: International\\r\\n    Conference on Learning Representations (2020), https://openreview.net/forum?\\r\\n    id=H1eA7AEtvS\\r\\n28. Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B.: Pointcnn: Convolution on x-\\r\\n    transformed points. Advances in neural information processing systems 31 (2018)\\r\\n29. Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollar, P.: Focal loss for dense object\\r\\n    detection. In: ICCV (2017)\\r\\n30. Liu, Y., Fan, B., Meng, G., Lu, J., Xiang, S., Pan, C.: Densepoint: Learning densely\\r\\n    contextual representation for efficient point cloud processing. In: Proceedings of the\\r\\n    IEEE/CVF International Conference on Computer Vision. pp. 5239\\xe2\\x80\\x935248 (2019)\\r\\n31. Liu, Y., Fan, B., Xiang, S., Pan, C.: Relation-shape convolutional neural network\\r\\n    for point cloud analysis. In: Proceedings of the IEEE/CVF Conference on Com-\\r\\n    puter Vision and Pattern Recognition. pp. 8895\\xe2\\x80\\x938904 (2019)\\r\\n32. Liu, Z., Hu, H., Cao, Y., Zhang, Z., Tong, X.: A closer look at local aggregation\\r\\n    operators in point cloud analysis. In: European Conference on Computer Vision.\\r\\n    pp. 326\\xe2\\x80\\x93342. Springer (2020)\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds           23\\r\\n\\r\\n33. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\\r\\n    arXiv:1711.05101 (2017)\\r\\n34. Misra, I., Girdhar, R., Joulin, A.: An End-to-End Transformer Model for 3D Object\\r\\n    Detection. In: ICCV (2021)\\r\\n35. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving\\r\\n    jigsaw puzzles. In: European conference on computer vision. pp. 69\\xe2\\x80\\x9384. Springer\\r\\n    (2016)\\r\\n36. Poursaeed, O., Jiang, T., Qiao, H., Xu, N., Kim, V.G.: Self-supervised learning of\\r\\n    point clouds via orientation estimation. In: 2020 International Conference on 3D\\r\\n    Vision (3DV). pp. 1018\\xe2\\x80\\x931028. IEEE (2020)\\r\\n37. Qi, C.R., Litany, O., He, K., Guibas, L.J.: Deep hough voting for 3d object de-\\r\\n    tection in point clouds. In: Proceedings of the IEEE International Conference on\\r\\n    Computer Vision (2019)\\r\\n38. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets\\r\\n    for 3d classification and segmentation. In: Proceedings of the IEEE conference on\\r\\n    computer vision and pattern recognition. pp. 652\\xe2\\x80\\x93660 (2017)\\r\\n39. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learn-\\r\\n    ing on point sets in a metric space. arXiv preprint arXiv:1706.02413 (2017)\\r\\n40. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\\r\\n    Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from\\r\\n    natural language supervision. In: International Conference on Machine Learning.\\r\\n    pp. 8748\\xe2\\x80\\x938763. PMLR (2021)\\r\\n41. Radford, A., Sutskever, I.: Improving language understanding by generative pre-\\r\\n    training. In: arxiv (2018)\\r\\n42. Rao, Y., Liu, B., Wei, Y., Lu, J., Hsieh, C.J., Zhou, J.: Randomrooms: Unsu-\\r\\n    pervised pre-training from synthetic shapes and randomized layouts for 3d object\\r\\n    detection. In: Proceedings of the IEEE/CVF International Conference on Com-\\r\\n    puter Vision. pp. 3283\\xe2\\x80\\x933292 (2021)\\r\\n43. Rolfe, J.T.: Discrete variational autoencoders. In: ICLR (2017)\\r\\n44. Singh, K.K., Lee, Y.J.: Hide-and-seek: Forcing a network to be meticulous for\\r\\n    weakly-supervised object and action localization. In: 2017 IEEE international con-\\r\\n    ference on computer vision (ICCV). pp. 3544\\xe2\\x80\\x933553. IEEE (2017)\\r\\n45. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:\\r\\n    Dropout: A simple way to prevent neural networks from overfitting. Journal of\\r\\n    Machine Learning Research 15(56), 1929\\xe2\\x80\\x931958 (2014), http://jmlr.org/papers/\\r\\n    v15/srivastava14a.html\\r\\n46. Steiner, A., Kolesnikov, A., , Zhai, X., Wightman, R., Uszkoreit, J., Beyer, L.: How\\r\\n    to train your vit? data, augmentation, and regularization in vision transformers.\\r\\n    arXiv preprint arXiv:2106.10270 (2021)\\r\\n47. Thabet, A., Alwassel, H., Ghanem, B.: Self-supervised learning of local features\\r\\n    in 3d point clouds. In: Proceedings of the IEEE/CVF Conference on Computer\\r\\n    Vision and Pattern Recognition Workshops. pp. 938\\xe2\\x80\\x93939 (2020)\\r\\n48. Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.J.:\\r\\n    Kpconv: Flexible and deformable convolution for point clouds. In: Proceedings of\\r\\n    the IEEE/CVF international conference on computer vision. pp. 6411\\xe2\\x80\\x936420 (2019)\\r\\n49. Thulasidasan, S., Chennupati, G., Bilmes, J.A., Bhattacharya, T., Michalak, S.: On\\r\\n    mixup training: Improved calibration and predictive uncertainty for deep neural\\r\\n    networks. Advances in Neural Information Processing Systems 32 (2019)\\r\\n50. Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T.,\\r\\n    Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., Dosovitskiy, A.: Mlp-\\r\\n    mixer: An all-mlp architecture for vision. arXiv preprint arXiv:2105.01601 (2021)\\r\\n\\x0c\\n\\n24      Liu et al.\\r\\n\\r\\n51. Uy, M.A., Pham, Q.H., Hua, B.S., Nguyen, T., Yeung, S.K.: Revisiting point cloud\\r\\n    classification: A new benchmark dataset and classification model on real-world\\r\\n    data. In: Proceedings of the IEEE/CVF international conference on computer vi-\\r\\n    sion. pp. 1588\\xe2\\x80\\x931597 (2019)\\r\\n52. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\\r\\n    Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg,\\r\\n    U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R.\\r\\n    (eds.) Advances in Neural Information Processing Systems. vol. 30. Curran\\r\\n    Associates, Inc. (2017), https://proceedings.neurips.cc/paper/2017/file/\\r\\n    3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\\r\\n53. Wang, H., Liu, Q., Yue, X., Lasenby, J., Kusner, M.J.: Unsupervised point cloud\\r\\n    pre-training via occlusion completion. In: ICCV (2021)\\r\\n54. Wang, H., Zhu, Y., Adam, H., Yuille, A., Chen, L.C.: MaX-DeepLab: End-to-end\\r\\n    panoptic segmentation with mask transformers. In: CVPR (2021)\\r\\n55. Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M.: Dynamic\\r\\n    graph cnn for learning on point clouds. Acm Transactions On Graphics (tog) 38(5),\\r\\n    1\\xe2\\x80\\x9312 (2019)\\r\\n56. Wei, C., Fan, H., Xie, S., Wu, C.Y., Yuille, A., Feichtenhofer, C.: Masked feature\\r\\n    prediction for self-supervised visual pre-training. arXiv preprint arXiv:2112.09133\\r\\n    (2021)\\r\\n57. Wu, W., Qi, Z., Fuxin, L.: Pointconv: Deep convolutional networks on 3d point\\r\\n    clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\\r\\n    Pattern Recognition. pp. 9621\\xe2\\x80\\x939630 (2019)\\r\\n58. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets: A\\r\\n    deep representation for volumetric shapes. In: Proceedings of the IEEE conference\\r\\n    on computer vision and pattern recognition. pp. 1912\\xe2\\x80\\x931920 (2015)\\r\\n59. Xiao, T., Dollar, P., Singh, M., Mintun, E., Darrell, T., Girshick, R.: Early convo-\\r\\n    lutions help transformers see better. Advances in Neural Information Processing\\r\\n    Systems 34 (2021)\\r\\n60. Xie, S., Gu, J., Guo, D., Qi, C.R., Guibas, L., Litany, O.: Pointcontrast: Unsuper-\\r\\n    vised pre-training for 3d point cloud understanding. In: European conference on\\r\\n    computer vision. pp. 574\\xe2\\x80\\x93591. Springer (2020)\\r\\n61. Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y.: Spidercnn: Deep learning on point sets\\r\\n    with parameterized convolutional filters. In: Proceedings of the European Confer-\\r\\n    ence on Computer Vision (ECCV). pp. 87\\xe2\\x80\\x93102 (2018)\\r\\n62. Yan, S., Yang, Z., Li, H., Guan, L., Kang, H., Hua, G., Huang, Q.: Implicit au-\\r\\n    toencoder for point cloud self-supervised representation learning. arXiv preprint\\r\\n    arXiv:2201.00785 (2022)\\r\\n63. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet:\\r\\n    Generalized autoregressive pretraining for language understanding. Advances in\\r\\n    neural information processing systems 32 (2019)\\r\\n64. Yi, L., Kim, V.G., Ceylan, D., Shen, I.C., Yan, M., Su, H., Lu, C., Huang, Q.,\\r\\n    Sheffer, A., Guibas, L.: A scalable active framework for region annotation in 3d\\r\\n    shape collections. ACM Transactions on Graphics (ToG) 35(6), 1\\xe2\\x80\\x9312 (2016)\\r\\n65. Yu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., Lu, J.: Point-bert: Pre-\\r\\n    training 3d point cloud transformers with masked point modeling. arXiv preprint\\r\\n    arXiv:2111.14819 (2021)\\r\\n66. Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization\\r\\n    strategy to train strong classifiers with localizable features. In: ICCV (2019)\\r\\n67. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: European confer-\\r\\n    ence on computer vision. pp. 649\\xe2\\x80\\x93666. Springer (2016)\\r\\n\\x0c\\n\\n        Masked Discrimination for Self-Supervised Learning on Point Clouds            25\\r\\n\\r\\n68. Zhang, Z., Girdhar, R., Joulin, A., Misra, I.: Self-supervised pretraining of 3d\\r\\n    features on any point-cloud. In: Proceedings of the IEEE/CVF International Con-\\r\\n    ference on Computer Vision (ICCV). pp. 10252\\xe2\\x80\\x9310263 (October 2021)\\r\\n69. Zhang, Z., Sun, B., Yang, H., Huang, Q.: H3dnet: 3d object detection using hybrid\\r\\n    geometric primitives. In: European Conference on Computer Vision. pp. 311\\xe2\\x80\\x93329.\\r\\n    Springer (2020)\\r\\n70. Zhao, H., Jiang, L., Fu, C.W., Jia, J.: Pointweb: Enhancing local neighborhood\\r\\n    features for point cloud processing. In: Proceedings of the IEEE/CVF conference\\r\\n    on computer vision and pattern recognition. pp. 5565\\xe2\\x80\\x935573 (2019)\\r\\n71. Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: Proceed-\\r\\n    ings of the IEEE/CVF International Conference on Computer Vision. pp. 16259\\xe2\\x80\\x93\\r\\n    16268 (2021)\\r\\n72. Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data augmenta-\\r\\n    tion. arXiv preprint arXiv:1708.04896 (2017)\\r\\n\\x0c', b'                                          Prepared for submission to JHEP\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          Renormalization of gluonic leading-twist Operators in\\r\\n                                          covariant Gauges\\r\\narXiv:2203.11181v1 [hep-ph] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                          Giulio Falcioni,a Franz Herzoga\\r\\n                                          a\\r\\n                                              Higgs Centre for Theoretical Physics, School of Physics and Astronomy, The University of Edin-\\r\\n                                              burgh, Edinburgh EH9 3FD, Scotland, UK\\r\\n\\r\\n                                              E-mail: giulio.falcioni@ed.ac.uk, fherzog@ed.ac.uk\\r\\n\\r\\n                                          Abstract: We provide the all-loop structure of gauge-variant operators required for the\\r\\n                                          renormalisation of Green\\xe2\\x80\\x99s functions with insertions of twist-two operators in Yang-Mills\\r\\n                                          theory. Using this structure we work out an explicit basis valid up to 4-loop order for\\r\\n                                          an arbitrary compact simple gauge group. To achieve this we employ a generalised gauge\\r\\n                                          symmetry, originally proposed by Dixon and Taylor, which arises after adding to the Yang-\\r\\n                                          Mills Lagrangian also operators proportional to its equation of motion. Promoting this\\r\\n                                          symmetry to a generalised BRST symmetry allows to generate the ghost operator from\\r\\n                                          a single exact operator in the BRST-generalised sense. We show that our construction\\r\\n                                          complies with the theorems by Joglekar and Lee. We further establish the existence of a\\r\\n                                          generalised anti-BRST symmetry which we employ to derive non-trivial relations among the\\r\\n                                          anomalous dimension matrices of ghost and equation-of-motion operators. For the purpose\\r\\n                                          of demonstration we employ the formalism to compute the N = 2, 4 Mellin moments of the\\r\\n                                          gluonic splitting function up to 4 loops and its N = 6 Mellin moment up to 3 loops, where\\r\\n                                          we also take advantage of additional simplifications of the background field formalism.\\r\\n\\x0c\\n\\nContents\\r\\n\\r\\n1 Introduction                                              2\\r\\n\\r\\n2 Background                                                4\\r\\n  2.1 Yang-Mills Lagrangian                                 4\\r\\n  2.2 Gauge-fixing, Ghosts and BRST                         4\\r\\n  2.3 Gluonic twist-2 operators                             6\\r\\n\\r\\n3 EOM operators and generalised Gauge Symmetry              7\\r\\n  3.1 General formalism                                     7\\r\\n  3.2 Generalised Gauge symmetry                            8\\r\\n  3.3 Explicit construction up to four loops                9\\r\\n\\r\\n4 Ghost operators and generalised BRST symmetry             14\\r\\n  4.1 Generalised BRST symmetry                             14\\r\\n  4.2 Compatibility with the Theorems of Joglekar and Lee   15\\r\\n  4.3 Ghost operators up to four loops                      17\\r\\n  4.4 Generalised anti-BRST symmetry                        18\\r\\n\\r\\n5 Operator Bases Construction for fixed N                   20\\r\\n  5.1 N = 2 operators                                       21\\r\\n  5.2 N = 4 operators                                       21\\r\\n  5.3 N = 6 operators                                       23\\r\\n  5.4 Operators of higher N                                 26\\r\\n\\r\\n6 Background-field formulation                              27\\r\\n  6.1 Bases of operators up to four loops                   29\\r\\n\\r\\n7 Calculations and results                                  30\\r\\n  7.1 Mixing with EOM and Ghost Operators                   31\\r\\n  7.2 Renormalisation of physical operators                 35\\r\\n\\r\\n8 Conclusions                                               37\\r\\n\\r\\nA Computing anomalous dimension in QCD                      39\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                       \\xe2\\x80\\x931\\xe2\\x80\\x93\\r\\n\\x0c\\n\\n1   Introduction\\r\\n\\r\\nThe increasing precision with which particle collisions are being measured at the Large\\r\\nHadron Collider have pushed theoretical predictions in QCD to the next-to-next-to-next-\\r\\nto-leading order (N3LO) in perturbative QCD. First calculations at this order were com-\\r\\npleted for Higgs boson production [1\\xe2\\x80\\x936] and the Drell-Yan process [4, 7]. Even 2 \\xe2\\x86\\x92 2\\r\\nreactions may become feasible at this order in the not-too-far future as first results for\\r\\n3-loop dijet production amplitudes have become available [8, 9]. One of the dominant re-\\r\\nmaining theoretical uncertainties associated to such N3LO calculations is now related to\\r\\nthe lack of knowledge of the 4-loop splitting functions, which determine the evolution of\\r\\nthe parton densities at the relevant perturbative order, and which are known completely\\r\\nonly up to three loops [10\\xe2\\x80\\x9328].\\r\\n     Substantial efforts to improve this situation have already been made. In the non-singlet\\r\\nsector, after pioneering calculations of lower moments [29], numerical approximations for\\r\\nthe 4-loop splitting functions are now known to high accuracy. An analytic reconstruction\\r\\nwas achieved in the limit of leading number of colours [30] and for subleading corrections\\r\\nin the number of quark flavors, nf , [31]; the leading nf -contributions were known already\\r\\nfor some time [32] to all orders in perturbation theory. Even some low N -moments at\\r\\nfive loops [33] are already known. In the singlet sector instead only a handful of lower\\r\\nmoments have been calculated so far at the four-loop level [34] and this input was found\\r\\nto be insufficient for reliable numerical approximations.\\r\\n     The reason for why calculations in the non-singlet sector are so much more advanced\\r\\nthan those in the singlet sector is not only due to more numerous and complex Feynman\\r\\ndiagrams but also due to a more powerful framework. This framework is based on the\\r\\noperator product expansion (OPE)[35, 36], which allows the extraction of Mellin moments\\r\\nfrom the anomalous dimensions of leading-twist light-cone operators. While the renormali-\\r\\nsation of such operators entering in the non-singlet sector is relatively straight forward, the\\r\\nrenormalisation of the corresponding gluonic operators in the off-shell formalism (likely the\\r\\nmost promising framework to allow for progress at four loops at this time) is non-trivial due\\r\\nto the mixing into unphysical operators. The mixing into these operators arises from sub-\\r\\ngraphs with external gluons and ghosts which contain the insertion of the singlet operator;\\r\\nschematic examples appearing at four loops are depicted in figure 1.\\r\\n     An explicit basis for these unphysical operators, valid up to the two-loop level, was\\r\\nworked out by Dixon and Taylor already almost fifty years ago [37] and was employed in\\r\\nthe computation of one-loop Mellin moments in the Feynman gauge. Hamberg and Van\\r\\nNeerven about twenty years later managed to successfully employ the same framework to\\r\\nperform calculations at the 2-loop order in dimensional regularisation [13]. This calculation\\r\\nwas also repeated very recently [38]. It is interesting to note that the calculation by\\r\\nHamberg and Van Neerven managed to successfully resolve a number of conflicting results\\r\\n[10\\xe2\\x80\\x9312] which were present at that time because of negligence of the mixing into unphysical\\r\\noperators.\\r\\n     Nevertheless the structure of the basis proposed by Dixon and Taylor remained some-\\r\\nwhat mysterious. This was pointed out in particular by Collins [39], who argued that the\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            \\xe2\\x80\\x932\\xe2\\x80\\x93\\r\\n\\x0c\\n\\n        (a)                       (b)                        (c)                       (d)\\r\\n\\r\\nFigure 1. The grey blobs denote examples of multi-loop multi-gluon subgraphs contributing at the\\r\\nfour loop order, which contain insertions of the gauge invariant operator, and whose UV-divergences\\r\\nlead to mixing with unphysical operators under renormalisation.\\r\\n\\r\\n\\r\\nunphysical operators appeared to be in conflict with a general theorem which states that\\r\\nthe unphysical operators should be either proportional to the equation of motion (EOM)\\r\\nor BRST-exact. This theorem was in part first conjectured by Kluberg-Stern and Zu-\\r\\nber [40, 41] before it was proven by Joglekar and Lee [42\\xe2\\x80\\x9344]. Another proof based on\\r\\ncohomology theory was later provided by Henneaux [45].\\r\\n     In the present paper we revisit the problem. More concretely we reinterpret the basis\\r\\nof Dixon and Taylor as being build up from EOM and ghost operators. The former are\\r\\nproportional to the EOM of the gauge invariant part of the Yang-Mills Lagrangian. Using\\r\\nthis notion we are able to write down the general form of the EOM operators at arbitrary\\r\\nloop orders. As was noted already by Dixon and Taylor the combined Lagrangian consisting\\r\\nof the Yang-Mills Lagrangian and their EOM operators is invariant under a generalised\\r\\ngauge symmetry. This generalised gauge symmetry can then be promoted to a generalised\\r\\nBRST symmetry. We prove that the generalised BRST transformation is nilpotent and\\r\\nthat the ghost operator can be constructed from a single BRST-exact operator in the\\r\\ngeneralised BRST sense. While the generalised BRST symmetry was in part pointed out\\r\\nalready by Hamberg and Van Neerven in [13], it was not clearly spelt out how to use it to\\r\\nconstruct the ghost Lagrangian. We apply the formalism to work out the explicit form of\\r\\nEOM and ghost operators up to four-loop order. We also show that the basis is in accord\\r\\nwith the theorem of Joglekar and Lee.\\r\\n     To further simplify calculations in the OPE framework we explore two further sym-\\r\\nmetry principles. We observe that the ghost term of the unphysical operator can also be\\r\\ngenerated from a generalised anti-BRST symmetry [46\\xe2\\x80\\x9349] and a corresponding generalised\\r\\nanti-BRST-exact operator. In particular we employ this alternative formulation to derive\\r\\na set of nontrivial identities among the anomalous dimensions of the EOM and ghost op-\\r\\nerators. We also make use of the background field formalism [40, 41, 50\\xe2\\x80\\x9352, 52, 53, 53, 54].\\r\\nBackground field gauge invariance allows one to reduce by one the maximum number of\\r\\nloops at which the anomalous dimensions of the EOM and ghost operators are required -\\r\\nthereby yielding another welcome simplification for the calculation of unphysical countert-\\r\\nerms. For the purpose of demonstration we will employ the formalism to re-calculate the\\r\\nN = 2 and N = 4 moments of the purely gluonic contributions in the singlet sector up to\\r\\n4 loops and the N = 6 moment up to three loops.\\r\\n     In the following we give a brief outline of the paper. In section 2 we summarise\\r\\nour conventions and review some of the relevant background material. The construction\\r\\nof EOM operators and the generalised gauge symmetry is discussed in section 3. The\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              \\xe2\\x80\\x933\\xe2\\x80\\x93\\r\\n\\x0c\\n\\nconstruction of ghost operators and the generalised BRST and anti-BRST invariance is\\r\\ndiscussed in section 4. There also the compatibility of our construction with the theorems\\r\\nof Joglekar and Lee is shown. The concepts are employed to build an independent basis\\r\\nof gauge-variant operators in section 5 for various fixed values of N . The background-field\\r\\nformulation is presented in section 6 and employed in section 7 for the computation of\\r\\nMellin moments up to 4-loop order. We conclude in section 8.\\r\\n\\r\\n2     Background\\r\\n\\r\\n2.1    Yang-Mills Lagrangian\\r\\nIn the following we summarise our conventions for the Yang Mills Lagrangian. We define\\r\\nthe field strength tensor as\\r\\n                             a\\r\\n                            F\\xc2\\xb5\\xce\\xbd = \\xe2\\x88\\x82\\xc2\\xb5 Aa\\xce\\xbd \\xe2\\x88\\x92 \\xe2\\x88\\x82\\xce\\xbd Aa\\xc2\\xb5 + gf abc Ab\\xc2\\xb5 Ac\\xce\\xbd ,                   (2.1)\\r\\n\\r\\nsuch that the gauge invariant part of the Yang-Mills action is given by\\r\\n                              Z\\r\\n                                                      1\\r\\n                        S0 = dd x L0 ,        L0 = \\xe2\\x88\\x92 Fa\\xc2\\xb5\\xce\\xbd F\\xc2\\xb5\\xce\\xbd  a\\r\\n                                                                  .                    (2.2)\\r\\n                                                      4\\r\\nLet us further define the covariant derivative in the adjoint representation,\\r\\n\\r\\n                                  D\\xc2\\xb5ac = \\xe2\\x88\\x82\\xc2\\xb5 \\xce\\xb4ac + gf abc Ab\\xc2\\xb5 .                         (2.3)\\r\\n\\r\\nWith this definition the EOM of the Yang-Mills Lagrangian is written compactly as\\r\\n                                     \\xce\\xb4S0\\r\\n                                          = (D\\xce\\xbd F \\xce\\xbd\\xc2\\xb5 )a .                              (2.4)\\r\\n                                     \\xce\\xb4A\\xc2\\xb5a\\r\\nThe action S0 is of course invariant under infinitesimal gauge transformations\\r\\n\\r\\n                       Aa\\xc2\\xb5 \\xe2\\x86\\x92 Aa\\xc2\\xb5 + \\xce\\xb4\\xcf\\x89 Aa\\xc2\\xb5 ,   with    \\xce\\xb4\\xcf\\x89 Aa\\xc2\\xb5 = (D\\xc2\\xb5 \\xcf\\x89)a .               (2.5)\\r\\n\\r\\n2.2    Gauge-fixing, Ghosts and BRST\\r\\nThe gauge invariance is broken by the gauge fixing (GF) and ghost (G) terms, the latter\\r\\nbeing required to cancel unphysical degrees of freedom of the gauge field. For the commonly\\r\\nused choice of the linear covariant gauge the gauge-fixing and ghost contributions to the\\r\\nLagrangian are\\r\\n                                        1\\r\\n                           LGF+G = \\xe2\\x88\\x92 (\\xe2\\x88\\x82 \\xc2\\xb5 Aa\\xc2\\xb5 )2 \\xe2\\x88\\x92 ca \\xe2\\x88\\x82 \\xc2\\xb5 D\\xc2\\xb5ab cb ,                    (2.6)\\r\\n                                        2\\xce\\xbe\\r\\nwhere ca and c\\xcc\\x84a are respectively the ghost and anti-ghost fields. The complete gauge-fixed\\r\\nYang-Mills action is then given by\\r\\n                           Z\\r\\n                       S = dd x L ,        with     L = L0 + LGF+G ,                   (2.7)\\r\\n\\r\\nand its EOM is given by\\r\\n                       \\xce\\xb4S         \\xce\\xbd\\xc2\\xb5   1 \\xc2\\xb5 \\xce\\xbd a      abc \\xc2\\xb5 b c\\r\\n                         \\xc2\\xb5 = (D\\xce\\xbd F )a + \\xe2\\x88\\x82 \\xe2\\x88\\x82 A\\xce\\xbd \\xe2\\x88\\x92 gf    (\\xe2\\x88\\x82 c )c .                       (2.8)\\r\\n                      \\xce\\xb4Aa              \\xce\\xbe\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              \\xe2\\x80\\x934\\xe2\\x80\\x93\\r\\n\\x0c\\n\\nWhile eq. (2.6) breaks gauge invariance it does remain invariant under nilpotent BRST\\r\\ntransformations. This feature becomes most transparent after the introduction of an aux-\\r\\niliary field ba (x), also known as the Nakanishi-Lautrup field [55, 56]. In this formulation\\r\\nthe Lagrangian is written as\\r\\n                                                 \\xce\\xbe\\r\\n                            LGF+G = \\xe2\\x88\\x92ba \\xe2\\x88\\x82 \\xc2\\xb5 Aa\\xc2\\xb5 + (ba )2 \\xe2\\x88\\x92 ca \\xe2\\x88\\x82 \\xc2\\xb5 D\\xc2\\xb5ab cb .                    (2.9)\\r\\n                                                 2\\r\\nEq. (2.9) can be seen to be equivalent to eq. (2.6) after substituting the solution of the EOM\\r\\nba = 1\\xce\\xbe \\xe2\\x88\\x82 \\xc2\\xb5 Aa\\xc2\\xb5 . The BRST variation leaving this Lagrangian invariant [57, 58] is defined as\\r\\n\\r\\n                                         \\xce\\xb4BRST (\\xe2\\x80\\xa2) \\xe2\\x89\\xa1 \\xce\\xb8 s(\\xe2\\x80\\xa2) ,                                 (2.10)\\r\\n\\r\\nwhere \\xce\\xb8 is a Grassmann number, s denotes the BRST operator, which being nilpotent\\r\\nsatisfies s2 (\\xe2\\x80\\xa2) = 0, and whose action on the fields is given by\\r\\n                                                         g\\r\\n              sba = 0 ,      sAa\\xc2\\xb5 = (D\\xc2\\xb5 c)a ,     sca = \\xe2\\x88\\x92 f abc cb cc ,       sca = \\xe2\\x88\\x92ba .     (2.11)\\r\\n                                                         2\\r\\nThe BRST invariance of eq. (2.9) can be made manifest by writing it in BRST-exact form,\\r\\nthat is as the BRST variation of an ancestor operator:\\r\\n                                                                \\x02                \\x03\\r\\n                  LGF+G = sOancestor ,            Oancestor = ca \\xe2\\x88\\x82 \\xc2\\xb5 Aa\\xc2\\xb5 \\xe2\\x88\\x92 12 \\xce\\xbeba .           (2.12)\\r\\n\\r\\nAn interpretation of the BRST symmetry is that it corresponds to a certain subclass of\\r\\ngauge transformations, where the parameter \\xcf\\x89(x) of the gauge transformation is identified\\r\\nwith the ghost field times a Grassmann number. There exists in fact a second such symme-\\r\\ntry in the gauge-fixed Lagrangian where the role of the ghost field in the BRST variations is\\r\\nreplaced with that of the anti-ghost field. This leads to the so-called anti-BRST symmetry\\r\\n[46\\xe2\\x80\\x9349]. To discuss this symmetry we first introduce another auxiliary field:\\r\\n\\r\\n                                        b\\xcc\\x84a = \\xe2\\x88\\x92ba + gf abc c\\xcc\\x84b cc                             (2.13)\\r\\n\\r\\nThe anti-BRST variation is then given by\\r\\n\\r\\n                                         \\xce\\xb4BRST (\\xe2\\x80\\xa2) \\xe2\\x89\\xa1 \\xce\\xb8\\xcc\\x84 s\\xcc\\x84(\\xe2\\x80\\xa2) ,                               (2.14)\\r\\n\\r\\nwith \\xce\\xb8\\xcc\\x84 another Grassmann number and\\r\\n                                                           g\\r\\n              s\\xcc\\x84b\\xcc\\x84a = 0 ,    s\\xcc\\x84Aa\\xc2\\xb5 = (D\\xc2\\xb5 c\\xcc\\x84)a ,   s\\xcc\\x84c\\xcc\\x84a = \\xe2\\x88\\x92 f abc c\\xcc\\x84b c\\xcc\\x84c ,   s\\xcc\\x84ca = \\xe2\\x88\\x92b\\xcc\\x84a .   (2.15)\\r\\n                                                           2\\r\\nThe BRST and anti-BRST variations fulfil the following consistency condition:\\r\\n\\r\\n                                 ss(\\xe2\\x80\\xa2) = s\\xcc\\x84s\\xcc\\x84(\\xe2\\x80\\xa2) = 0 = s\\xcc\\x84s(\\xe2\\x80\\xa2) + ss\\xcc\\x84(\\xe2\\x80\\xa2)                        (2.16)\\r\\n\\r\\nSimilarly to eq. (2.12) the anti-BRST symmetry of eq. (2.9) can be made manifest by\\r\\nwriting it in anti-BRST exact form, that is as the anti-BRST variation of another ancestor\\r\\noperator:\\r\\n                                                          \\x02                \\x03\\r\\n                   LGF+G = s\\xcc\\x84 Oancestor ,   Oancestor = ca 21 \\xce\\xbeba \\xe2\\x88\\x92 \\xe2\\x88\\x82 \\xc2\\xb5 Aa\\xc2\\xb5 .       (2.17)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                  \\xe2\\x80\\x935\\xe2\\x80\\x93\\r\\n\\x0c\\n\\n2.3     Gluonic twist-2 operators\\r\\nLet us now consider the extension of the Yang-Mills Lagrangian to include also a general\\r\\ngauge invariant gluonic twist-2 spin-N operator\\r\\n                               1 \\x02 a1                a     aN\\xe2\\x88\\x921 aN\\xe2\\x88\\x921 ;\\xc2\\xb5 \\x03\\r\\n                  )\\r\\n           O\\xc2\\xb5(N1 ...\\xc2\\xb5  (x) =     S F \\xc2\\xb5\\xc2\\xb51 D\\xc2\\xb5a12a2 ...D\\xc2\\xb5N\\xe2\\x88\\x922\\r\\n                                                       N\\xe2\\x88\\x921     F       \\xc2\\xb5N + traceless ,                        (2.18)\\r\\n                     N\\r\\n                               2\\r\\nwhere we have indicated\\r\\n\\r\\n      \\xe2\\x80\\xa2 the sum over all permutations of \\xc2\\xb51 , ..., \\xc2\\xb5N via the operation S,\\r\\n                                                                        (N )\\r\\n      \\xe2\\x80\\xa2 and the presence of further terms which make O\\xc2\\xb51 ...\\xc2\\xb5N (x) traceless, i.e. the sum\\r\\n        vanishes when any two of its Lorentz indices are contracted, by the term \\xe2\\x80\\x98+traceless\\xe2\\x80\\x99.\\r\\n\\r\\nA well known trick to simplify this expression is to contract it with N identical light-like\\r\\nvectors which we denote by \\xe2\\x88\\x86\\xc2\\xb5 and which satisfy \\xe2\\x88\\x86.\\xe2\\x88\\x86 = 0. It is then conventional to\\r\\nintroduce the notation\\r\\n\\r\\n          F \\xc2\\xb5;a = \\xe2\\x88\\x86\\xce\\xbd F \\xc2\\xb5\\xce\\xbd;a ,        Aa = \\xe2\\x88\\x86\\xc2\\xb5 A\\xc2\\xb5;a ,         D = \\xe2\\x88\\x86\\xc2\\xb5 D \\xc2\\xb5 ,              \\xe2\\x88\\x82 = \\xe2\\x88\\x86\\xc2\\xb5 \\xe2\\x88\\x82 \\xc2\\xb5 .             (2.19)\\r\\n\\r\\nUsing this notation we then define the scalarised version of eq. (2.18):\\r\\n\\r\\n                        (N )                                  1 \\x02              \\x03\\r\\n                                       )\\r\\n                       O1 (x) = O\\xc2\\xb5(N1 ...\\xc2\\xb5  (x)\\xe2\\x88\\x86 \\xc2\\xb51\\r\\n                                                    ...\\xe2\\x88\\x86 \\xc2\\xb5N\\r\\n                                                            =   Tr F\\xce\\xbd D N \\xe2\\x88\\x922 \\xce\\xbd\\r\\n                                                                            F    .                             (2.20)\\r\\n                                          N\\r\\n                                                              2\\r\\n                                              (N )\\r\\nIt is well known that the operator O1 (x) when inserted into general Green\\xe2\\x80\\x99s functions\\r\\nmixes with non-physical operators under renormalisation. A basis for these non-physical\\r\\noperators will be constructed in the following sections consisting of two kinds of operators,\\r\\nnamely operators proportional to the EOM, defined in eq. (2.4), and operators containing\\r\\n                                             (N )                         (N )        (N )\\r\\nghosts (G). We therefore include besides O1 (x) also the operators OEOM and OG (x).\\r\\nThe complete Lagrangian is then given by\\r\\n                                                        (N )    (N )           (N )        (N )\\r\\n                 L\\xcc\\x83(A, c\\xcc\\x84, c; g, \\xce\\xbe) = L0 + LGF+G + C1          O1 (x) + OEOM + OG (x) ,                        (2.21)\\r\\n          (N )                                                  (N )                                          (N )\\r\\nwhere C1 is the Wilson coefficient associated to O1 (x). The mass dimension of O1 (x),\\r\\n  (N )               (N )\\r\\nOEOM and OG (x) equals the dimension of space-time, d, this is achieved by defining \\xe2\\x88\\x86\\r\\nto carry a mass dimension of 2/N \\xe2\\x88\\x92 1.\\r\\n     Let us now briefly discuss the renormalisation of L\\xcc\\x83, which in eq. (2.21) was defined\\r\\nin terms of physical or equivalently renormalised fields and couplings. The counterterms\\r\\nrequired to make finite all correlators of the fields Aa\\xc2\\xb5 , ca and c\\xcc\\x84a at distinct positions can\\r\\nbe readily generated by replacing the fields and couplings with their bare counterparts in\\r\\nL\\xcc\\x83(Ab , c\\xcc\\x84b , cb ; gb , \\xce\\xbe b ) with\\r\\n                        1/2\\r\\n         Ab;\\xc2\\xb5         \\xc2\\xb5\\r\\n          a (x) = Z3 Aa (x) ,                    c\\xcc\\x84ba (x) = Zc1/2 c\\xcc\\x84ba (x) ,      cba (x) = Zc1/2 cba (x) ,\\r\\n                 gb = \\xc2\\xb5\\xc7\\xab gZg ,                        \\xce\\xbe b = \\xce\\xbeZ3 .                                              (2.22)\\r\\n                                                                                                         (N )\\r\\nThis replacement is not sufficient to renormalise correlators with an insertion of O1 . For\\r\\nthis purpose it is convenient to introduce the vector notation O   ~ (N ) = (O(N ) , ..., On(N ) ),\\r\\n                                                                               1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                     \\xe2\\x80\\x936\\xe2\\x80\\x93\\r\\n\\x0c\\n\\n         (N )                                                                                       (N )\\r\\nwith O1 defined in eq. (2.20) and the remaining components Oi>1 , to which we asso-\\r\\nciate Wilson coefficients Ci , forming a basis of operators spanning the space of EOM and\\r\\n                   (N )          (N )\\r\\nghost operators OEOM and OG . The required counterterms are obtained by taking into\\r\\naccount the mixing of the operators under renormalisation. This is achieved by making\\r\\nthe replacement\\r\\n                                       (N ),b    (N ) (N )\\r\\n                                      Ci      = Zji Cj .\\r\\nThe bare Lagrangian then takes the form\\r\\n                                                                                              X       (N )    (N )   (N ),b\\r\\n L\\xcc\\x83(Ab , c\\xcc\\x84b , cb ; Cib , g b , \\xce\\xbe b ) = L0 (Ab ; gb ) + LGF+G (Ab , c\\xcc\\x84b , cb ; gb , \\xce\\xbe b ) +         Ci       Zij Oj           (2.23)\\r\\n                                                                                              i,j\\r\\n\\r\\n                 (N ),b\\r\\nwhere the Oi     denote the operators written in terms of bare couplings and fields. It is\\r\\nwell known [40\\xe2\\x80\\x9345] that the structure of Zij is block triangular in that the physical operator\\r\\n (N )                (N )\\r\\nO1 may mix into Oi>1 but not vice versa. This is discussed further in section 4.2.\\r\\n\\r\\n3     EOM operators and generalised Gauge Symmetry\\r\\n\\r\\nFor the sake of keeping the notation as light as possible we will in the following discuss\\r\\nsymmetry properties of the Lagrangian L\\xcc\\x83 at the level of renormalised fields and parameters.\\r\\nNote that all of these properties can be directly translated to the bare Lagrangian given\\r\\nthat it has the same functional form.\\r\\n\\r\\n3.1     General formalism\\r\\nIn this section we will elucidate the general structure of the EOM operator. It is well\\r\\nknown that Green\\xe2\\x80\\x99s functions are not invariant under field redefinitions\\r\\n\\r\\n                                    Aa\\xc2\\xb5 \\xe2\\x86\\x92 Aa\\xc2\\xb5 + G\\xc2\\xb5a (A\\xce\\xb1 , \\xe2\\x88\\x82\\xce\\xb1 A\\xce\\xb2 , \\xe2\\x88\\x82\\xce\\xb1 \\xe2\\x88\\x82\\xce\\xb2 A\\xcf\\x81 , ...) ,\\r\\n\\r\\nwhere G is a general local, i.e. polynomial, function of the gauge field A and its derivatives.\\r\\nTo leading order in G the variation of the Yang-Mills action in eq. (2.2) can then be written\\r\\nas follows:\\r\\n             Z                        Z\\r\\n                      \\xce\\xb4S0\\r\\n       \\xce\\xb4S0 = dd x \\xc2\\xb5          G\\xc2\\xb5a (x) = dd x (D\\xce\\xbd F \\xce\\xbd\\xc2\\xb5 )a G\\xc2\\xb5a (A\\xce\\xb1 , \\xe2\\x88\\x82\\xce\\xb1 A\\xce\\xb2 , \\xe2\\x88\\x82\\xce\\xb1 \\xe2\\x88\\x82\\xce\\xb2 A\\xcf\\x81 , ...) . (3.1)\\r\\n                    \\xce\\xb4Aa (x)\\r\\n\\r\\nFor a general form of the function G, as we shall see later, this is actually the most general\\r\\n                                  (N )\\r\\nsuch EOM operator into which O1 can mix under renormalisation, leading us to write\\r\\n                                  (N )\\r\\n                               OEOM = (D\\xce\\xbd F \\xce\\xbd\\xc2\\xb5 )a G\\xc2\\xb5a (A\\xce\\xb1 , \\xe2\\x88\\x82\\xce\\xb1 A\\xce\\xb2 , \\xe2\\x88\\x82\\xce\\xb1 \\xe2\\x88\\x82\\xce\\xb2 A\\xcf\\x81 , ...) .                                          (3.2)\\r\\n\\r\\nA number of constraints on the structure of this EOM operator derive from the overall\\r\\n                                           (N )\\r\\nmass dimension and the twist-2 nature of O1 . This implies that G\\xc2\\xb5a must be N -linear\\r\\nin \\xe2\\x88\\x86 and for the mass dimensions to work out the total number of As and \\xe2\\x88\\x82s entering in\\r\\nevery monomial of G must then equal N \\xe2\\x88\\x92 1. It follows that G itself must be proportional\\r\\nto \\xe2\\x88\\x86 and that every single A or \\xe2\\x88\\x82 entering in G must itself also be contracted with \\xe2\\x88\\x86.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            \\xe2\\x80\\x937\\xe2\\x80\\x93\\r\\n\\x0c\\n\\nThese considerations therefore pin down the general structure of the EOM operator to be\\r\\nas follows:\\r\\n                            (N )\\r\\n                           OEOM = (D.F )a G a (A, \\xe2\\x88\\x82A, \\xe2\\x88\\x82 2 A, ...) ,               (3.3)\\r\\nwhere G\\xc2\\xb5a = \\xe2\\x88\\x86\\xc2\\xb5 G a . By expanding G over all possible monomials which satisfy the power\\r\\ncounting constraints we then obtain\\r\\n            \\xe2\\x88\\x9e\\r\\n            X                                                            X\\r\\n (N )              (N ),k              (N ),k                                                        \\x01           \\x01\\r\\nOEOM =            OEOM      with     OEOM = gk\\xe2\\x88\\x921 (D \\xc2\\xb7 F )a                      Cia;a 1 ..ak\\r\\n                                                                                  1 ..ik\\r\\n                                                                                             \\xe2\\x88\\x82 i1 Aa1 .. \\xe2\\x88\\x82 ik Aak .\\r\\n            k=1                                                     i1 +..+ik\\r\\n                                                                    =N \\xe2\\x88\\x92k\\xe2\\x88\\x921\\r\\n                                                                                                             (3.4)\\r\\n\\r\\nHere the coefficients Cia;a   1 ..ak\\r\\n                          1 ..ik\\r\\n                                     are in general color-dependent coupling constants which can\\r\\nbe further decomposed into some basis of group-invariant color structures. Let us for\\r\\nexample consider the case k = 2, whose general decomposition1 can be written as\\r\\n                   (N ),2\\r\\n                                             X                             \\x01        \\x01\\r\\n                 OEOM = g (D \\xc2\\xb7 F )a               \\xce\\xbai1 i2 f a a1 a2 \\xe2\\x88\\x82 i1 Aa1 \\xe2\\x88\\x82 i2 Aa2 ,      (3.5)\\r\\n                                                i1 +i2\\r\\n                                                =N \\xe2\\x88\\x923\\r\\n\\r\\nwhere the \\xce\\xbaij s are Wilson coefficients, to be discussed further below. Let us also remark\\r\\nthat there exists a general constraint on the C-coefficients which derives from the fact\\r\\nthat the operators are colour singlets. For general k, the coefficients obey the following\\r\\ninvariance relation\\r\\n\\r\\n                      Cib;a 1 ..ak b a x\\r\\n                         1 i2 ..ik\\r\\n                                   f     + Cia;b..a k\\r\\n                                             1 i2 ..ik\\r\\n                                                       f b a1 x + .. + Cia;a1 a2 ..b b ak x\\r\\n                                                                         1 i2 ..ik\\r\\n                                                                                    f       = 0.             (3.6)\\r\\n\\r\\nWe now study the symmetry properties of the lagrangian in eq. (2.21).\\r\\n\\r\\n3.2     Generalised Gauge symmetry\\r\\n                                                                  (N )\\r\\nWhile gauge transformations leave both L0 and O1 invariant, the same can not be said\\r\\nabout the general EOM operator. To cancel its variation we will now contruct a generalised\\r\\ngauge transformation,\\r\\n                              Aa\\xc2\\xb5 \\xe2\\x86\\x92 Aa\\xc2\\xb5 + \\xce\\xb4\\xcf\\x89 Aa\\xc2\\xb5 + \\xce\\xb4\\xcf\\x89\\xe2\\x88\\x86 Aa\\xc2\\xb5 ,                         (3.7)\\r\\n                                                                                             (N )\\r\\nwhere \\xce\\xb4\\xcf\\x89\\xe2\\x88\\x86 is multi-linear in \\xe2\\x88\\x86 and is such that the gauge variation of OEOM is cancelled by\\r\\nthe generalised gauge variation of L0 , i.e. \\xce\\xb4\\xcf\\x89\\xe2\\x88\\x86 L0 . This leads to\\r\\n                                                                    (N )\\r\\n                                    (D.F )a\\xc2\\xb5 \\xce\\xb4\\xcf\\x89\\xe2\\x88\\x86 A\\xc2\\xb5a (x) + \\xce\\xb4\\xcf\\x89 OEOM = 0 ,                                     (3.8)\\r\\n\\r\\nand combined with eq. (3.3) it then follows that the generalised gauge variation satisfies:\\r\\n\\r\\n                                     \\xce\\xb4\\xcf\\x89\\xe2\\x88\\x86 Aa\\xc2\\xb5 + \\xce\\xb4\\xcf\\x89 G\\xc2\\xb5a \\xe2\\x88\\x92 g f abc G\\xc2\\xb5b \\xcf\\x89 c = 0 .                                (3.9)\\r\\n  1\\r\\n      Note we ignore here the fully symmetric rank 3 tensor dabc as it can not appear in Yang-Mills theory.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         \\xe2\\x80\\x938\\xe2\\x80\\x93\\r\\n\\x0c\\n\\nUsing eqs. (3.3) and (3.4) we then find the following general solution:\\r\\n                       \\xe2\\x88\\x9e\\r\\n                       X X                          \\x01                \\x01 X a;a\\xcf\\x83(1) ..a\\xcf\\x83(k)\\r\\n       \\xce\\xb4\\xcf\\x89\\xe2\\x88\\x86 Aa\\xc2\\xb5 = \\xe2\\x88\\x92\\xe2\\x88\\x86\\xc2\\xb5                        \\xe2\\x88\\x82 i1 Aa1 .. \\xe2\\x88\\x82 ik +1 \\xcf\\x89 ak    Ci\\xcf\\x83(1) ..i\\xcf\\x83(k)\\r\\n                       k=1 i1 +..+ik                                 \\xcf\\x83\\xe2\\x88\\x88Zk\\r\\n                            N \\xe2\\x88\\x92k\\xe2\\x88\\x921\\r\\n                                                                  k\\r\\n                                                                                                       !\\r\\n                         X                  \\x01       \\x01 i +1 a \\x01 X                       im + ik+1 + 1\\r\\n                                      i1 a1   ik ak\\r\\n             + g\\xe2\\x88\\x86\\xc2\\xb5                   \\xe2\\x88\\x82 A .. \\xe2\\x88\\x82 A       \\xe2\\x88\\x82 k+1\\r\\n                                                            \\xcf\\x89 k+1\\r\\n\\r\\n                      i1 +..+ik+1                                               m=1\\r\\n                                                                                            im\\r\\n                        N \\xe2\\x88\\x92k\\xe2\\x88\\x922\\r\\n                                 a;a ..am\\xe2\\x88\\x921 bam+1 ..ak b am ak+1\\r\\n                            \\xc3\\x97 Ci1 ..i1m+i k+1 +1..ik\\r\\n                                                      f                                                (3.10)\\r\\n\\r\\nwhere we have used eq. (3.6) and symmetry to cancel all terms which contain \\xcf\\x89 without\\r\\nderivatives. By collecting terms of identical field content and powers of g we can bring\\r\\neq. (3.10) into the following form\\r\\n                      \\xe2\\x88\\x9e\\r\\n                      X               X                              \\x01               \\x01            \\x01\\r\\n      \\xce\\xb4\\xcf\\x89\\xe2\\x88\\x86 Aa\\xc2\\xb5 = \\xe2\\x88\\x92\\xe2\\x88\\x86\\xc2\\xb5         gk\\xe2\\x88\\x921                e a;a1 .. ak \\xe2\\x88\\x82 i1 Aa1 .. \\xe2\\x88\\x82 ik\\xe2\\x88\\x921 Aak\\xe2\\x88\\x921 \\xe2\\x88\\x82 ik +1 \\xcf\\x89 ak .\\r\\n                                                C                                                      (3.11)\\r\\n                                                  i1 .. ik\\r\\n                      k=1          i1 +..+ik\\r\\n                                   =N \\xe2\\x88\\x92k+1\\r\\n\\r\\nThe Ce a;a1 .. ak can be extracted from the building blocks of C a;a1 .. ak once a color basis has\\r\\n      i1 .. ik                                                  i1 .. ik\\r\\nbeen specified. We will construct an explicit solution valid up to four loops in the next\\r\\nsubsection.\\r\\n\\r\\n3.3     Explicit construction up to four loops\\r\\nThe loop order puts stringent constraints on the type of the EOM operators actually\\r\\n                                                                                           (N )\\r\\nrequired. The quantity from which we wish to extract the anomalous dimension of O1\\r\\n                                                                                   (N )\\r\\nis naturally the gluon 2-point 1PI correlator with an insertion of the operator O1 :\\r\\n                              Z\\r\\n                 (N ) \\xc2\\xb5\\xce\\xbd                                            (N )\\r\\n              (\\xce\\x931;gg )ab (Q) = dd x dd z eiQ.x h0|T {A\\xc2\\xb5a (x)A\\xce\\xbdb (0)O1 (z)}|0i1PI .      (3.12)\\r\\n\\r\\nAt one-loop there are no subdivergences and we only require counterterms with two external\\r\\n                                                       (N )        (N ),1\\r\\ngluons. We thus only need the one-loop mixing of O1 into OEOM , as this is the only\\r\\nEOM operator contributing to the two gluon vertex. At two loops we then require two-loop\\r\\n             (N )       (N ),1                          (N )        (N ),2\\r\\nmixing of O1 into OEOM , and one-loop mixing of O1 into OEOM , given that at two\\r\\nloops we can have one-loop subgraphs with three external gluons. This reasoning can be\\r\\ncontinued at higher loop orders leading to more EOM operators. Diagrams with subgraphs\\r\\nhighlighting this pattern are shown in table 1 and the corresponding loop numbers, from\\r\\nwhich we require certain EOMs, are also summarised again in table 2. We can therefore\\r\\nignore terms in eq. (3.3) from k = 4 onwards leading to the following set of EOM operators\\r\\nrequired up to 4 loops:\\r\\n                                   (N ),1\\r\\n                              OEOM = \\xce\\xb7 (D.F )a \\xe2\\x88\\x82 N \\xe2\\x88\\x922 Aa                                               (3.13)\\r\\n\\r\\n\\r\\n                                   (N ),2\\r\\n                                                         X\\r\\n                              OEOM = g(D.F )a                       (\\xe2\\x88\\x82 A )(\\xe2\\x88\\x82 j Ac )\\r\\n                                                                 abc i b\\r\\n                                                                Cij                                    (3.14)\\r\\n                                                         i+j=\\r\\n                                                         N \\xe2\\x88\\x923\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                        \\xe2\\x80\\x939\\xe2\\x80\\x93\\r\\n\\x0c\\n\\n               (N ),\\xe2\\x89\\xa41                       (N ),\\xe2\\x89\\xa42                        (N ),\\xe2\\x89\\xa43              (N ),\\xe2\\x89\\xa44\\r\\n L           OEOM                         OEOM                          OEOM                    OEOM\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n 1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n 2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n 3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n 4\\r\\n\\r\\n\\r\\n\\r\\nTable 1. In the Lth row the table gives examples of diagrams contributing to the L-loop contri-\\r\\n           (N )                                                                       (N ),k\\r\\nbution to \\xce\\x93gg . Subgraphs whose UV-counterterms require the various EOM operators OEOM are\\r\\nhighlighted with dashed boxes.\\r\\n\\r\\n                  (N ),3\\r\\n                                         X\\r\\n                OEOM = g2 (D.F )a                abcd\\r\\n                                                Cijk  (\\xe2\\x88\\x82 i Ab )(\\xe2\\x88\\x82 j Ac )(\\xe2\\x88\\x82 k Ad )                          (3.15)\\r\\n                                        i+j+k\\r\\n                                        =N \\xe2\\x88\\x924\\r\\n                  (N ),4\\r\\n                                          X\\r\\n                OEOM = g3 (D.F )a                  abcde\\r\\n                                                  Cijkl  (\\xe2\\x88\\x82 i Ab )(\\xe2\\x88\\x82 j Ac )(\\xe2\\x88\\x82 k Ad )(\\xe2\\x88\\x82 l Ae )              (3.16)\\r\\n                                        i+j+k+l\\r\\n                                         =N \\xe2\\x88\\x925\\r\\n\\r\\nLet us now discuss the color decomposition of the C-coefficients. While at rank two and\\r\\nthree possible color structures are limited to \\xce\\xb4ab and f abc , color decompositions for operators\\r\\nof higher rank are in general non-trivial, in particular when keeping the color gauge group\\r\\ngeneral as we do here. However the fact that we only require counterterms valid up to\\r\\ncertain loop orders imposes strong contstraints and allows us to identify the following color\\r\\ndecompositions:\\r\\n                              abc\\r\\n                             Cij  = f abc \\xce\\xbaij                                                              (3.17)\\r\\n                            abcd                  (1)           (2)             (3)\\r\\n                           Cijk  = (f f )abcd \\xce\\xbaijk + dabcd\\r\\n                                                      4    \\xce\\xbaijk + dabcd\\r\\n                                                                   d\\r\\n                                                                   4f\\r\\n                                                                        \\xce\\xba\\r\\n                                                                      f ijk\\r\\n                                                                                                           (3.18)\\r\\n                            abcde                   (1)               (2)\\r\\n                           Cijkl  = (f f f )abcde \\xce\\xbaijkl + dabcde\\r\\n                                                           4f    \\xce\\xbaijkl ,                                   (3.19)\\r\\n\\r\\nwhere the different color structures are defined as\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                  \\xe2\\x80\\x93 10 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                                       (N ),1    (N ),2       (N ),3     (N ),4\\r\\n                               L      OEOM      OEOM        OEOM       OEOM\\r\\n                                1       1         0            0          0\\r\\n                                2       2         1            0          0\\r\\n                                3       3         2            1          0\\r\\n                                4       4         3            2          1\\r\\n\\r\\n                                                                                  (N )          (N ),k\\r\\nTable 2. The table summarizes the loop orders for which the mixing of O1                 into OEOM is required,\\r\\ngiven a certain loop order of \\xce\\x93N\\r\\n                               gg .\\r\\n\\r\\n\\r\\n\\r\\n         (f f )abcd = f abe f cde ,                       (f f f )abcde = f abm f mcn f nde ,\\r\\n            dabcde\\r\\n             4f    = dabcm\\r\\n                      4    f mde ,                              dabcd    abmn mce edn\\r\\n                                                                 4f f = d4   f   f    ,                  (3.20)\\r\\n                              1\\r\\n             dabcd\\r\\n              d    = dabcd\\r\\n                      4f f \\xe2\\x88\\x92 CA d4\\r\\n                                   abcd\\r\\n                                        ,\\r\\n              4f f            3\\r\\nand the symmetrised trace is defined by\\r\\n                               1\\r\\n                    dabcd\\r\\n                     4    =       [Tr(TAa TAb TAc TAd ) + symmetric permutations] ,                      (3.21)\\r\\n                               4!\\r\\nwhere (TA )bac = if abc . Going beyond four loops would not only require further operators,\\r\\n       (N ),k>4\\r\\ni.e. OEOM , but also further color structures in the definitions of Cijk and Cijkl . In fact to\\r\\narbitrary loop-order, there are arbitrarily many independent color structures contributing\\r\\nto Cijk and Cijkl . If one was to work in a fixed gauge group this task would be far\\r\\nsimpler. For instance in SU(Nc ) we know that the complete basis at 4 and 5 points is\\r\\nexpressible in terms of single and double traces of permutations of the generators in the\\r\\nfundamental representation. The penalty for working in an arbitrary gauge group thus\\r\\nbecomes increasingly higher at higher loops, but is still mild at the four-loop level.\\r\\n     Let us now come to the definition of the sums appearing in eqs. (3.13)-(3.16). These\\r\\nare defined such that we sum over all non-negative integer values of the indices i, j, k, l,\\r\\nappearing in the sum which satisfy the respective constraint, e.g. i + j + k = N \\xe2\\x88\\x92 4. While\\r\\nthis sum notation leads to reasonably compact definitions of the EOM operators, it does also\\r\\nlead to overcounting. For instance in the order g term we sum over all indices i+j = N \\xe2\\x88\\x923,\\r\\nbut since the associated color tensor, f abc , is asymmetric under exchange of b and c the\\r\\noperators appearing in the sum for j > i are related to those with j < i. To compensate\\r\\nthis over-counting of independent operators we impose relations on the \\xce\\xba-coefficients. In\\r\\ngeneral there exists a lot of freedom in how to choose these relations. A particularly\\r\\nconvenient choice of constraints is obtained by demanding the \\xce\\xba-coefficients to satisfy\\r\\nthe same relations as their respective color factors. That this works can be understood as\\r\\nfollows. If we were to use up all the color identities, we would clearly land in an independent\\r\\nbasis of operators. By imposing the same identities on the \\xce\\xba-coefficients, we effectively\\r\\nensure that solving these identities would lead to the right degrees of freedom - that is the\\r\\nright number of independent \\xce\\xba-coefficients. This choice is in spirit not dissimilar to the\\r\\nBCJ-choice of numerators [59] for Feynman diagrams where the numerators of Feynman\\r\\ndiagrams are chosen such that they satisfy the same constraints as the corresponding color\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                  \\xe2\\x80\\x93 11 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\nfactors. Here however the motivation is solely to make manipulations with these operators\\r\\nmore manageable.\\r\\n     These considerations then finally lead us to impose the following relations on the \\xce\\xba-\\r\\ncoefficients:\\r\\n\\r\\n        \\xce\\xbaij + \\xce\\xbaji = 0,                                          (antisymmetry of f )      (3.22)\\r\\n         (1)     (1)\\r\\n        \\xce\\xbaijk + \\xce\\xbaikj = 0,                                        (antisymmetry of f )      (3.23)\\r\\n         (1)      (1)\\r\\n        \\xce\\xbaijkl + \\xce\\xbaijlk = 0,                                      (antisymmetry of f )      (3.24)\\r\\n         (1)     (1)    (1)\\r\\n        \\xce\\xbaijk + \\xce\\xbajki + \\xce\\xbakij = 0,                                                (Jacobi)   (3.25)\\r\\n         (1)      (1)     (1)\\r\\n        \\xce\\xbaijkl + \\xce\\xbaiklj + \\xce\\xbailjk = 0,                                             (Jacobi)   (3.26)\\r\\n         (1)      (1)     (1)    (1)\\r\\n        \\xce\\xbaijkl + \\xce\\xbajilk + \\xce\\xbalkji + \\xce\\xbaklij = 0,                            (double Jacobi)     (3.27)\\r\\n         (2)     (2)     (2)    (2)     (2) (2)\\r\\n        \\xce\\xbaijk = \\xce\\xbajik = \\xce\\xbaikj = \\xce\\xbakji = \\xce\\xbajki = \\xce\\xbakij ,                   (symmetry of d4 )     (3.28)\\r\\n         (3)     (3)\\r\\n        \\xce\\xbaijk = \\xce\\xbaikj ,                                           (antisymmetry of f )      (3.29)\\r\\n         (3)     (3)    (3)\\r\\n        \\xce\\xbaijk + \\xce\\xbajki + \\xce\\xbakij = 0,                                 (generalised Jacobi )     (3.30)\\r\\n         (2)      (2)\\r\\n        \\xce\\xbaijkl + \\xce\\xbaijlk = 0,                                      (antisymmetry of f )      (3.31)\\r\\n         (2)      (2)\\r\\n        \\xce\\xbaijkl = \\xce\\xbajikl ,                                             (symmetry of d4 )     (3.32)\\r\\n\\r\\nAn independent set of operators is then found for any given N by solving these relations.\\r\\nFixing N this is in principle a straight forward exercise, but is somewat difficult to do\\r\\nkeeping N general.\\r\\n     We now give the color identities which lead to eqs. (3.22)-(3.32). The Jacobi relation\\r\\nis as usual,\\r\\n                           (f f )abcd + (f f )acdb + (f f )adbc = 0 .                (3.33)\\r\\nBy the double Jacobi relation we refer to the identity\\r\\n\\r\\n                  (f f f )abcde + (f f f )acbed + (f f f )adebc + (f f f )aedcb = 0 ,     (3.34)\\r\\n\\r\\nwhich itself can be derived by repeated use of the Jacobi relation. Another consequence of\\r\\nthe Jacobi relation is what is sometimes refered to as a generalised Jacobi relation [60]:\\r\\n\\r\\n                             dabcde\\r\\n                              4f    + dbcdae\\r\\n                                       4f    + dcdabe\\r\\n                                                4f    + ddabce\\r\\n                                                         4f    = 0.                       (3.35)\\r\\n                                                                                 (2)\\r\\nThis identity does not lead to any relations among the coefficients \\xce\\xbaijkl , since we fix the\\r\\nposition of the index a, which contracts the EOM, to be in the d4 . The relation would\\r\\nconnect it to operators where the a would be attached to the corresponding f . However\\r\\ncontracting this relation with f deg leads to\\r\\n\\r\\n                             CA dabcd\\r\\n                                 4    + dabcd    bcad    cabd\\r\\n                                         4f f + d4f f + d4f f = 0 .                       (3.36)\\r\\n\\r\\nCombining this equation with its permutations, and using the symmetry properties,\\r\\n\\r\\n                                 dabcd    abdc    bacd    badc\\r\\n                                  4f f = d4f f = d4f f = d4f f ,                          (3.37)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                               \\xe2\\x80\\x93 12 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\nwhich follow directly from the definition in eq. (3.20), one can further derive the less obvious\\r\\nrelation\\r\\n                                        dabcd      cdab\\r\\n                                         4f f = d4f f .                                   (3.38)\\r\\nCombining eqs. (3.36)-(3.38) we then derive\\r\\n\\r\\n                                         CA dabcd  abcd    acdb    adbc\\r\\n                                             4f + d4f f + d4f f + d4f f = 0 .                                                            (3.39)\\r\\n\\r\\nThis relation implies that operators with color structures d4 and d4f f are linearly depen-\\r\\ndent. To avoid this undesirable feature we introduced the modified color factor d4f\\r\\n                                                                                  d f\\r\\n                                                                                      which\\r\\nsatisfies\\r\\n                                 dabcd\\r\\n                                  d\\r\\n                                  4f f\\r\\n                                       + dacdb\\r\\n                                          d\\r\\n                                          4f f\\r\\n                                               + dadbc\\r\\n                                                  d\\r\\n                                                  4f f\\r\\n                                                       = 0,                           (3.40)\\r\\n\\r\\nand is therefore independent of dabcd\\r\\n                                    4f . Having discussed an explicit basis of the EOM\\r\\noperators and their color structures we can now consider the generalised gauge invariance\\r\\ndiscussed in section 3.2. To 4-loop order the \\xe2\\x88\\x86-dependent part of the transformation reads\\r\\n                    \"\\r\\n                                 \\x01     X a;a a            \\x01             \\x01\\r\\n      \\xce\\xb4w A\\xc2\\xb5 = \\xe2\\x88\\x92\\xe2\\x88\\x86\\xc2\\xb5 \\xce\\xb7 \\xe2\\x88\\x82 N \\xe2\\x88\\x921 \\xcf\\x89 a + g\\r\\n       \\xe2\\x88\\x86 a                                  e 1 2 \\xe2\\x88\\x82 i1 Aa1 \\xe2\\x88\\x82 i2 +1 \\xcf\\x89 a2\\r\\n                                            C                         i1 i2\\r\\n                                                       i1 +i2\\r\\n                                                       =N \\xe2\\x88\\x923\\r\\n                           X                                  \\x01                  \\x01                   \\x01\\r\\n               + g2                   e a;a1 a2 a3 \\xe2\\x88\\x82 i1 Aa1\\r\\n                                      C                               \\xe2\\x88\\x82 i2 Aa2        \\xe2\\x88\\x82 i3 +1 \\xcf\\x89 a3                                       (3.41)\\r\\n                                        i1 i2 i3\\r\\n                         i1 +i2 +i3\\r\\n                          =N \\xe2\\x88\\x924\\r\\n                                                                                                                                     #\\r\\n                           X                                      \\x01                   \\x01             \\x01                   \\x01\\r\\n               +g    3               e a;a1 a2 a3 a4\\r\\n                                     C                  i1\\r\\n                                                       \\xe2\\x88\\x82 A   a1\\r\\n                                                                        \\xe2\\x88\\x82 A i2   a2       i3\\r\\n                                                                                          \\xe2\\x88\\x82 A  a3\\r\\n                                                                                                         \\xe2\\x88\\x82   i4 +1 a4\\r\\n                                                                                                                   \\xcf\\x89             4\\r\\n                                                                                                                            + O(g ) ,\\r\\n                                       i1 i2 i3 i4\\r\\n                         i1 +..+i4\\r\\n                          =N \\xe2\\x88\\x925\\r\\n\\r\\n      e a;a1 ..an involve the same colour structures which appear in eqs. (3.13)-(3.16)\\r\\nwhere C i1 ..in\\r\\n\\r\\n        e a;a1 a2 = \\xce\\xb7 (1) f a;a1 a2 ,\\r\\n        C                                                                                                                                (3.42)\\r\\n          i1 i2      i1 i2\\r\\n      e a;a1 a2 a3 = \\xce\\xb7 (1) (f f )aa1 a2 a3 + \\xce\\xb7 (2) daa1 a2 a3 + \\xce\\xb7 (3) daa1 a2 a3 ,\\r\\n      C                                                                                                                                  (3.43)\\r\\n        i1 i2 i3      i1 i2 i3                i1 i2 i3           i1 i2 i3 4f\\r\\n                                                                          d  f\\r\\n    e a;a1 a2 a3 a4 = \\xce\\xb7 (1)\\r\\n    C                                      aa1 a2 a3 a4    (2a)\\r\\n                                                        + \\xce\\xb7i1 i2 i3 i4 daa 1 a2 a3 a4    (2b)\\r\\n                                                                                      + \\xce\\xb7i1 i2 i3 i4 daa 4 a1 a2 a3\\r\\n      i1 i2 i3 i4      i1 i2 i3 i4 (f f f )                             4f                            4f            .                    (3.44)\\r\\n\\r\\n                         (k)                                                                                 (k)\\r\\nThe coefficients \\xce\\xb7i1 ..in are then fixed in terms of the coefficients \\xce\\xbai1 ..in of eqs. (3.13)-(3.16)\\r\\nby means of eq. (3.10) and eq. (3.11). We obtain the following relations\\r\\n                                                       \\x10i +i +1\\x11\\r\\n                               (1)                      1         2\\r\\n                           \\xce\\xb7i1 i2 = 2 \\xce\\xbai1 i2 + \\xce\\xb7                                 ,                                                       (3.45)\\r\\n                                                                 i1\\r\\n                                                        \\x10i +i +1\\x11                        h                     i\\r\\n                           (1)                               2          3                  (1)         (1)\\r\\n                         \\xce\\xb7i1 i2 i3 = 2\\xce\\xbai1 (i2 +i3 +1)                                 + 2 \\xce\\xbai1 i2 i3 + \\xce\\xbai3 i2 i1 ,                        (3.46)\\r\\n                                                                      i2\\r\\n                           (2)             (2)\\r\\n                         \\xce\\xb7i1 i2 i3 = 3\\xce\\xbai1 i2 i3 ,                                                                                        (3.47)\\r\\n                                      h                     i\\r\\n                          (3)             (3)       (3)\\r\\n                         \\xce\\xb7i1 i2 i3 = 2 \\xce\\xbai1 i2 i3 \\xe2\\x88\\x92 \\xce\\xbai3 i2 i1 ,                                                                           (3.48)\\r\\n                                     h                                       i\\x10i +i +1\\x11\\r\\n                      (1)              (1)                  (1)                 3  4\\r\\n                     \\xce\\xb7i1 i2 i3 i4 = 2 \\xce\\xbai1 i2 (i3 +i4 +1) + \\xce\\xba(i3 +i4 +1)i2 i1\\r\\n                                                                                  i3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 \\xe2\\x80\\x93 13 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                               h                                                         i\\r\\n                                 (1)            (1)            (1)            (1)\\r\\n                            + 2 \\xce\\xbai1 i2 i3 i4 + \\xce\\xbai1 i4 i3 i2 + \\xce\\xbai4 i1 i3 i2 + \\xce\\xbai4 i3 i1 i2 ,             (3.49)\\r\\n                                                      \\x10i +i +1\\x11\\r\\n                 (2a)              (2)                   3     4              (2)\\r\\n                \\xce\\xb7i1 i2 i3 i4 = 3\\xce\\xbai1 i2 (i3 +i4 +1)                     + 2 \\xce\\xbai1 i2 i3 i4 ,               (3.50)\\r\\n                                                             i3\\r\\n                 (2b)               (2)\\r\\n                \\xce\\xb7i1 i2 i3 i4 = 2 \\xce\\xbai4 i1 i2 i3 .                                                         (3.51)\\r\\n\\r\\nThe use and power of these relations will become clear in the next section, where we discuss\\r\\nhow the generalised gauge symmetry is promoted to a generalised BRST symmetry.\\r\\n\\r\\n4     Ghost operators and generalised BRST symmetry\\r\\n\\r\\n4.1    Generalised BRST symmetry\\r\\nThe main virtue of the generalised gauge transformation, \\xce\\xb4\\xcf\\x89 + \\xce\\xb4\\xcf\\x89\\xe2\\x88\\x86 , which we established in\\r\\nsection 3.2, is that we can promote it to a generalised BRST (gBRST) transformation:\\r\\n                                  \\xe2\\x80\\xb2\\r\\n                                 \\xce\\xb4BRST (\\xe2\\x80\\xa2) \\xe2\\x89\\xa1 \\xce\\xb8 s\\xe2\\x80\\xb2 (\\xe2\\x80\\xa2) ,             s\\xe2\\x80\\xb2 = s + s\\xe2\\x88\\x86 .                        (4.1)\\r\\n\\r\\nHere s is the action of the usual BRST transformation and s\\xe2\\x88\\x86 is the new \\xe2\\x88\\x86-dependent\\r\\npart. To define the action of this symmetry on the fields we follow [15, 42]. The only non-\\r\\nvanishing action is the variation of the gauge field. It is constructed simply by replacing\\r\\nthe gauge parameter \\xcf\\x89(x)a in eq. (3.41) with the ghost field c(x)a . We thus obtain\\r\\n\\r\\n                               s \\xe2\\x88\\x86 ba = 0 ,            s\\xe2\\x88\\x86 ca = 0,       s\\xe2\\x88\\x86 ca = 0 ,                      (4.2)\\r\\n\\r\\nand\\r\\n                     \\xe2\\x88\\x9e\\r\\n                     X               X                                 \\x01               \\x01           \\x01\\r\\n      s\\xe2\\x88\\x86 Aa\\xc2\\xb5 = \\xe2\\x88\\x92\\xe2\\x88\\x86\\xc2\\xb5         gk\\xe2\\x88\\x921                   e a;a1 .. a4 \\xe2\\x88\\x82 i1 Aa1 .. \\xe2\\x88\\x82 ik\\xe2\\x88\\x921 Aak\\xe2\\x88\\x921 \\xe2\\x88\\x82 ik +1 cak .\\r\\n                                                  C                                                      (4.3)\\r\\n                                                    i1 .. i4\\r\\n                     k=0          i1 +..+ik\\r\\n                                  =N \\xe2\\x88\\x92k+1\\r\\n\\r\\nFurthermore eq. (3.9) can be promoted to an idenitity for the corresponding BRST varia-\\r\\ntions:\\r\\n                           s\\xe2\\x88\\x86 A\\xc2\\xb5a (x) + sGa\\xc2\\xb5 \\xe2\\x88\\x92 g fabc Gb\\xc2\\xb5 cc = 0 ,                (4.4)\\r\\nThis relation is very useful. For instance it allows us to show that s\\xe2\\x80\\xb2 is nilpotent. In the\\r\\nfollowing we prove this up to terms of order \\xe2\\x88\\x862 , which is all we require for the renor-\\r\\nmalisation of Green\\xe2\\x80\\x99s functions with single insertions of twist-2 operators. First we note\\r\\nthat\\r\\n                           2\\r\\n                         s\\xe2\\x80\\xb2 = s2 + ss\\xe2\\x88\\x86 + s\\xe2\\x88\\x86 s + O(\\xe2\\x88\\x862 ) = O(\\xe2\\x88\\x862 ) .                      (4.5)\\r\\nGiven s2 = 0 we therefore require\\r\\n\\r\\n                                                   ss\\xe2\\x88\\x86 + s\\xe2\\x88\\x86 s = 0 .                                      (4.6)\\r\\n\\r\\nTo prove this identity it is sufficient to show that it holds for the gauge field. We start\\r\\nwith\\r\\n\\r\\n                        ss\\xe2\\x88\\x86 Aa\\xc2\\xb5 = \\xe2\\x88\\x92s(sG\\xc2\\xb5a \\xe2\\x88\\x92 g f abc G\\xc2\\xb5b cc ) = g f abc s(G\\xc2\\xb5b cc )\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         \\xe2\\x80\\x93 14 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                                                   g2 b abc cde d e\\r\\n                               = g f abc sG\\xc2\\xb5b cc \\xe2\\x88\\x92    G f f c c                           (4.7)\\r\\n                                                    2 \\xc2\\xb5\\r\\n                               = g f abc sG\\xc2\\xb5b cc \\xe2\\x88\\x92 g2 f abc f cde G\\xc2\\xb5d cb ce\\r\\n\\r\\nwhere we used eq. (4.4) and nilpotence of s in the first line and the Jacobi identity and\\r\\nindex relabeling to get to the last line. Next we now consider\\r\\n\\r\\n                   s\\xe2\\x88\\x86 sAa\\xc2\\xb5 = s\\xe2\\x88\\x86 (D\\xc2\\xb5ac cc ) = gf abc s\\xe2\\x88\\x86 Ab\\xc2\\xb5 cc\\r\\n                           = \\xe2\\x88\\x92gf abc sG\\xc2\\xb5b cc + g2 f abc f bde G\\xc2\\xb5d ce cc                   (4.8)\\r\\n                           = \\xe2\\x88\\x92gf abc sG\\xc2\\xb5b cc + g2 f abc f cde G\\xc2\\xb5d cb ce = \\xe2\\x88\\x92ss\\xe2\\x88\\x86 Aa\\xc2\\xb5\\r\\n\\r\\nThis proves eq. (4.5). The nilpotence is thus a direct consequence of the generalised gauge\\r\\ninvariance.\\r\\n     Let us now come to the general form of the gauge-fixing+ghost Lagrangian required\\r\\n                                                                      (N )\\r\\nto renormalise arbitrary Green\\xe2\\x80\\x99s functions with single insertions of O1 . We propose that\\r\\nit can be represented as follows:\\r\\n\\r\\n                                        L\\xe2\\x80\\xb2GF +G = s\\xe2\\x80\\xb2 Oancestor ,                          (4.9)\\r\\n\\r\\nwhere the ancestor is the same one which appears in the usual gauge-fixing and ghost term\\r\\nrequired for the Yang-Mills Lagrangian; that is the one we defined in eq. (2.12). Expanding\\r\\nout s\\xe2\\x80\\xb2 we thus obtain\\r\\n                                                        (N )\\r\\n                                L\\xe2\\x80\\xb2GF +G = LGF +G + OG                                 (4.10)\\r\\nwith\\r\\n                                (N )\\r\\n                              OG       = s\\xe2\\x88\\x86 Oancestor = \\xe2\\x88\\x92ca \\xe2\\x88\\x82 \\xc2\\xb5 s\\xe2\\x88\\x86 Aa\\xc2\\xb5 .                 (4.11)\\r\\nWe can then rewrite the complete Lagrangian, introduced in eq. (2.21), as\\r\\n\\r\\n                                         L\\xcc\\x83 = LEGI + L\\xe2\\x80\\xb2GF +G ,                           (4.12)\\r\\n\\r\\nwith\\r\\n                                                       (N )      (N )\\r\\n                                 LEGI = LYM + O1              + OEOM .                   (4.13)\\r\\nIn this formulation the Lagrangian is then manifestly invariant under the generalised BRST\\r\\n                   \\xe2\\x80\\xb2\\r\\ntransformation \\xce\\xb4BRST    . For LEGI this follows immediately from its invariance under gener-\\r\\nalised gauge transformations. And given the nilpotence of s\\xe2\\x80\\xb2 it also follows that L\\xe2\\x80\\xb2GF +G is\\r\\ninvariant under the symmetry, as it lies in the image of s\\xe2\\x80\\xb2 . Instead LEGI lies in the kernel of\\r\\ns\\xe2\\x80\\xb2 . The cohomology of the generalised BRST transformation, defined as the kernel modulo\\r\\nthe image of s\\xe2\\x80\\xb2 , is thus unaffected of the details of the gauge fixing function - an important\\r\\nfeature which underlies also the usual BRST symmetry.\\r\\n\\r\\n4.2    Compatibility with the Theorems of Joglekar and Lee\\r\\nLet us now come to an important issue concerning the mixing between gauge invariant\\r\\nand gauge variant operators. For physics to be independent of the gauge variant operators\\r\\n                             (N )\\r\\nthe renormalisation matrix Zij should be block triangular. This theorem was proven by\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 \\xe2\\x80\\x93 15 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\nJoglekar and Lee [42] and states, essentially, that the block triangular structure is present\\r\\nas long as the unphysical operators belong to two different operator classes2 :\\r\\n\\r\\n       \\xe2\\x80\\xa2 Class I operators:\\r\\n                                              \\xce\\xb4S \\xce\\xb4F [A, c, c\\xcc\\x84]\\r\\n                                    OI =                       + sF [A, c, c\\xcc\\x84]                    (4.14)\\r\\n                                             \\xce\\xb4A\\xc2\\xb5a \\xce\\xb4(\\xe2\\x88\\x82 \\xc2\\xb5 c\\xcc\\x84a )\\r\\n       \\xe2\\x80\\xa2 Class II operators:\\r\\n                                                      \\xce\\xb4S a\\r\\n                                              OII =       X [A, c, c\\xcc\\x84]                            (4.15)\\r\\n                                                      \\xce\\xb4ca\\r\\nwhere X and F are local (polynomial) functionals of the fields. For our construction this\\r\\n                           (N )\\r\\nwould then imply that Zj1 = 0. However it is not obvious that the ghost and EOM\\r\\noperators presented here fall into these classes. We will now show that they do. Using\\r\\neq. (4.4) we can write eq. (4.11) as follows:\\r\\n                                 (N )\\r\\n                               OG       = ca \\xe2\\x88\\x82 \\xc2\\xb5 sG\\xc2\\xb5a \\xe2\\x88\\x92 gf abc ca \\xe2\\x88\\x82 \\xc2\\xb5 G\\xc2\\xb5b cc .                    (4.16)\\r\\n\\r\\nUsing now that s(c\\xcc\\x84a \\xe2\\x88\\x82 \\xc2\\xb5 G\\xc2\\xb5a ) = \\xe2\\x88\\x82\\xc2\\xb5 ba G \\xc2\\xb5;a + \\xe2\\x88\\x82\\xc2\\xb5 c\\xcc\\x84a sGa\\xc2\\xb5 and the EOM of the ba -field, eq. (4.16)\\r\\nbecomes\\r\\n                     (N )                     \\x021                         \\x03\\r\\n                  OG = \\xe2\\x88\\x92s(c\\xcc\\x84a \\xe2\\x88\\x82G a ) + (\\xe2\\x88\\x82\\xe2\\x88\\x82\\xce\\xbd Ab;\\xce\\xbd ) + gf abc (\\xe2\\x88\\x82ca )cc G b .                   (4.17)\\r\\n                                               \\xce\\xbe\\r\\nCombining this expression with eq. (3.3) and eq. (2.8) we thus obtain\\r\\n                                 (N )       (N )                     \\xce\\xb4S a\\r\\n                               OG + OEOM = s(\\xe2\\x88\\x82c\\xcc\\x84a G a ) +               G .                       (4.18)\\r\\n                                                                    \\xce\\xb4Aa\\r\\nIt is thus apparent that our expressions for the ghost and EOM operators are just Class\\r\\nI operators, and therefore comply with the theorems of Joglekar and Lee, if we identify\\r\\nF = \\xe2\\x88\\x82c\\xcc\\x84a G a in eq. (4.14). In our case the class II operators can not actually contribute due\\r\\n                                                                  \\xce\\xb4S\\r\\nto the leading twist nature. This follows as the ghost EOM \\xce\\xb4c      a is already twist 3. For a\\r\\n                  a\\r\\nsimilar reason G can also not depend on ghost and anti-ghost fields at twist two.\\r\\n                                                         (N )\\r\\n     The structure of the renormalisation matrix Zi j is therefore, by the theorem of\\r\\nJoglekar and Lee, expected to be of the form\\r\\n                                        \\xef\\xa3\\xab                       \\xef\\xa3\\xb6\\r\\n                                            (N )   (N )    (N )\\r\\n                                          Z1 1 Z1 2 .. Z1 n\\r\\n                                        \\xef\\xa3\\xac                       \\xef\\xa3\\xb7\\r\\n                                        \\xef\\xa3\\xac          (N )\\r\\n                                                                \\xef\\xa3\\xb7\\r\\n                                                           (N ) \\xef\\xa3\\xb7\\r\\n                                  (N )  \\xef\\xa3\\xac 0      Z2 2 .. Z2 n \\xef\\xa3\\xb7 .\\r\\n                                Z      =\\xef\\xa3\\xac                                                (4.19)\\r\\n                                        \\xef\\xa3\\xac                       \\xef\\xa3\\xb7\\r\\n                                        \\xef\\xa3\\xad ..       ...     .. \\xef\\xa3\\xb8\\r\\n                                                   (N )    (N )\\r\\n                                            0    Zn 2 .. Zn n\\r\\n              (N )                                                    (N )\\r\\nSo while O1 may mix into the unphysical operators Oi>1 , the unphysical operators can\\r\\nonly mix among themselves. For calculations of physical quantities, such as S-matrix ele-\\r\\n                                              (N )\\r\\nments, it is thus fully sufficient to know Z1 1 . We require Z1 i>1 only when renormalising\\r\\n                                         (N )               (N )\\r\\nGreen\\xe2\\x80\\x99s functions with insertions of O1 . Instead the Zi>1 j>1 are only required for cal-\\r\\nculations of Green\\xe2\\x80\\x99s functions with insertions of the unphysical operators.\\r\\n   2\\r\\n     Note that this is slightly different from the classification into EOM and BRST-exact operators which\\r\\nis often stated and which can be found for instance in [39].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                   \\xe2\\x80\\x93 16 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n4.3     Ghost operators up to four loops\\r\\nWe will now work out the structure of the ghost operator, given in eq. (4.11) through\\r\\nfour-loop order. This requires the BRST variation of the gauge field up to 4 loops, which\\r\\nis given by\\r\\n                 \"\\r\\n                            \\x01      X a;a a            \\x01          \\x01\\r\\n   s\\xe2\\x88\\x86 Aa\\xc2\\xb5 = \\xe2\\x88\\x92\\xe2\\x88\\x86\\xc2\\xb5 \\xce\\xb7 \\xe2\\x88\\x82 N \\xe2\\x88\\x921 ca + g       e 1 2 \\xe2\\x88\\x82 i1 Aa1 \\xe2\\x88\\x82 i2 +1 ca2\\r\\n                                      C                       i1 i2\\r\\n                                                 i1 +i2\\r\\n                                                 =N \\xe2\\x88\\x923\\r\\n                     X                                    \\x01               \\x01                  \\x01\\r\\n            + g2                e a;a1 a2 a3 \\xe2\\x88\\x82 i1 Aa1\\r\\n                                C                             \\xe2\\x88\\x82 i2 Aa2        \\xe2\\x88\\x82 i3 +1 ca3\\r\\n                                  i1 i2 i3\\r\\n                   i1 +i2 +i3\\r\\n                    =N \\xe2\\x88\\x924\\r\\n                                                                                                 #\\r\\n                     X                                   \\x01        \\x01        \\x01           \\x01\\r\\n            + g3               e a;a1 a2 a3 a4\\r\\n                               C                 \\xe2\\x88\\x82 i1 Aa1 \\xe2\\x88\\x82 i2 Aa2 \\xe2\\x88\\x82 i3 Aa3 \\xe2\\x88\\x82 i4 +1 ca4 + O(g 4 ) ,                             (4.20)\\r\\n                                 i1 i2 i3 i4\\r\\n                   i1 +..+i4\\r\\n                    =N \\xe2\\x88\\x925\\r\\n\\r\\n                      e a;a1 a2 .. defined in eqs. (3.42)-(3.44) in terms of a range of \\xce\\xb7-coefficients,\\r\\nwith the coefficients C i1 i2 ..\\r\\nwhich in turn are related to the \\xce\\xba-coefficients, defined in eqs. (3.45)-(3.51) and which are\\r\\nattributed to the EOM operators. The ghost operator for arbitrary N as required for\\r\\ncalculations up to the four loop level is thus determined to be\\r\\n                                            (N )\\r\\n                                                    X (N ),k\\r\\n                                          OG =         OG ,                                     (4.21)\\r\\n                                                                      k\\r\\n\\r\\nwith\\r\\n       (N ),1                      \\x01\\r\\n      OG      = \\xe2\\x88\\x92\\xce\\xb7 (\\xe2\\x88\\x82ca ) \\xe2\\x88\\x82 N \\xe2\\x88\\x921 ca ,                                                                                           (4.22)\\r\\n       (N ),2\\r\\n                    X a;a a                   \\x01           \\x01\\r\\n      OG      = \\xe2\\x88\\x92g       e 1 2 (\\xe2\\x88\\x82ca ) \\xe2\\x88\\x82 i1 Aa1 \\xe2\\x88\\x82 i2 +1 ca2 ,\\r\\n                         C                                                                                                      (4.23)\\r\\n                           i1 i2\\r\\n                      i1 +i2\\r\\n                      =N \\xe2\\x88\\x923\\r\\n       (N ),3\\r\\n                          X                                               \\x01              \\x01              \\x01\\r\\n      OG        = \\xe2\\x88\\x92g2                e a;a1 a2 a3 (\\xe2\\x88\\x82ca ) \\xe2\\x88\\x82 i1 Aa1\\r\\n                                     C                                        \\xe2\\x88\\x82 i2 Aa2       \\xe2\\x88\\x82 i3 +1 ca3 ,                      (4.24)\\r\\n                                       i1 i2 i3\\r\\n                        i1 +i2 +i3\\r\\n                         =N \\xe2\\x88\\x924\\r\\n       (N ),4\\r\\n                          X                                                   \\x01              \\x01              \\x01              \\x01\\r\\n      OG        = \\xe2\\x88\\x92g3                e a;a1 a2 a3 a4 (\\xe2\\x88\\x82ca ) \\xe2\\x88\\x82 i1 Aa1\\r\\n                                     C                                            \\xe2\\x88\\x82 i2 Aa2       \\xe2\\x88\\x82 i3 Aa3       \\xe2\\x88\\x82 i4 +1 ca4 .   (4.25)\\r\\n                                       i1 i2 i3 i4\\r\\n                        i1 +..+i4\\r\\n                         =N \\xe2\\x88\\x925\\r\\n\\r\\nThe ghost operator is therefore completely determined by the generalised BRST symme-\\r\\ntry, or, equivalently, by the generalised gauge invariance and the particular form of the\\r\\ngauge-fixing term. Since the couplings appearing in eq. (4.21) are determined as linear\\r\\ncombinations of an independent set of the \\xce\\xba-couplings of the EOM operators we can effec-\\r\\ntively combine the independent parts of the ghost operator with those of the different EOM\\r\\noperators, collecting terms together which share common \\xce\\xba-coupling coefficients, into what\\r\\nJoglekar and Lee called Class I operators.\\r\\n     One welcome result is thus that the generalised BRST symmetry vastly reduces the\\r\\nindependent set of operators one needs to consider. This was of course already oberserved\\r\\nin [37] and [13] although it was accounted for in slightly different ways. In [37] the relations\\r\\namong the couplings were derived by enforcing the Lie algebra structure on the generalised\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                               \\xe2\\x80\\x93 17 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\ngauge invariance. Instead in [13] they followed from the generalised BRST invariance of\\r\\nthe complete Lagrangian. We like to stress here that in both these references the basis was\\r\\nonly considered to two-loop level, and that no connection to EOM operators and BRST\\r\\nexact operators was made. The explicit form of the gauge variant operators and their\\r\\nconnection to the ghost operators was thus rather non-trivial and somewhat mysterious.\\r\\nWe hope that our presentation finally sheds some light into this long-standing puzzle.\\r\\n    Another advantage of the formalism is that in order to compute the full anomalous\\r\\n                                                                  (N )\\r\\ndimension mixing matrix we only need to consider the mixing of O1 into ghost operators,\\r\\nwhich depending on the method of computation may also require the mixing of the ghost\\r\\noperators among themselves. This is of course much easier to compute then the mixing\\r\\n     (N )\\r\\nof O1 into the EOM operators, whose renormalisation would naively require the com-\\r\\nputation of multi-gluon correlators. Instead the anomalous dimensions of ghost operators\\r\\ncan be extracted from multi-gluon correlators with a ghost anti-ghost pair; which yields a\\r\\nwelcome reduction of complexity. This point will be discussed in more detail in section 7\\r\\nwith reference to specific examples.\\r\\n\\r\\n4.4   Generalised anti-BRST symmetry\\r\\nIn the following we will discuss a rather remarkable fact: there exists a second formulation of\\r\\nthe generalised gauge-fixing and ghost lagrangian L\\xe2\\x80\\xb2GF +G introduced in eq. (4.10). Rather\\r\\nthan writing it as a gBRST-exact operator we can write it as an anti-gBRST exact operator\\r\\nwith the anti-ancestor operator defined in eq. (2.17):\\r\\n                                                                                          (N )\\r\\n                                      L\\xe2\\x80\\xb2GF +G = s\\xcc\\x84\\xe2\\x80\\xb2 O\\xcc\\x84ancestor = LGF +G + OG .                                                    (4.26)\\r\\n\\r\\nwhere s\\xcc\\x84\\xe2\\x80\\xb2 = s\\xcc\\x84 + s\\xcc\\x84\\xe2\\x88\\x86 and the anti-gBRST transformation is defined as a generalised gauge\\r\\ntransformation with \\xcf\\x89(x) \\xe2\\x86\\x92 c\\xcc\\x84(x), that is\\r\\n\\r\\n       s\\xe2\\x88\\x86 ca = 0,              s\\xe2\\x88\\x86 c\\xcc\\x84a = 0,         s\\xe2\\x88\\x86 b\\xcc\\x84a = 0,\\r\\n                           \"\\r\\n                                          \\x01    X a;a a         \\x01            \\x01\\r\\n      s\\xe2\\x88\\x86 Aa\\xc2\\xb5 = \\xe2\\x88\\x92\\xe2\\x88\\x86\\xc2\\xb5             \\xce\\xb7 \\xe2\\x88\\x82 N \\xe2\\x88\\x921 ca + g  Ce 1 2 \\xe2\\x88\\x82 i1 Aa1 \\xe2\\x88\\x82 i2 +1 ca2\\r\\n                                                  i1 i2\\r\\n                                                    i1 +i2\\r\\n                                                    =N \\xe2\\x88\\x923\\r\\n                          X                                  \\x01              \\x01                 \\x01\\r\\n               + g2                  e a;a1 a2 a3 \\xe2\\x88\\x82 i1 Aa1\\r\\n                                     C                           \\xe2\\x88\\x82 i2 Aa2       \\xe2\\x88\\x82 i3 +1 ca3                                       (4.27)\\r\\n                                       i1 i2 i3\\r\\n                        i1 +i2 +i3\\r\\n                         =N \\xe2\\x88\\x924\\r\\n                                                                                                                              #\\r\\n                          X                                      \\x01              \\x01             \\x01                  \\x01\\r\\n               +g   3                e a;a1 a2 a3 a4 \\xe2\\x88\\x82 i1 Aa1\\r\\n                                     C                               i2\\r\\n                                                                     \\xe2\\x88\\x82 A  a2        i3\\r\\n                                                                                    \\xe2\\x88\\x82 A  a3\\r\\n                                                                                                  \\xe2\\x88\\x82   i4 +1 a4\\r\\n                                                                                                          c               4\\r\\n                                                                                                                     + O(g ) ,\\r\\n                                       i1 i2 i3 i4\\r\\n                        i1 +..+i4\\r\\n                         =N \\xe2\\x88\\x925\\r\\n\\r\\nThe fact that it is possible to define an anti-gBRST transformation and use it to construct\\r\\nthe ghost operator may not be too surprising given that this was also possible for the usual\\r\\nrenormalisable gauge-fixing+ghost Lagrangian. What is more surprising is that the ghost\\r\\noperator generated by the anti-gBRST exact operator,\\r\\n        (N )\\r\\n      OG       = \\xe2\\x88\\x92ca \\xe2\\x88\\x82 \\xc2\\xb5 s\\xe2\\x88\\x86 Aa\\xc2\\xb5 ,                                                                                                 (4.28)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                             \\xe2\\x80\\x93 18 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                                \"\\r\\n                                      \\x01    X a;a a         \\x01            \\x01\\r\\n                   = (\\xe2\\x88\\x82c ) \\xce\\xb7 \\xe2\\x88\\x82 N \\xe2\\x88\\x921 ca + g\\r\\n                            a\\r\\n                                            Ce 1 2 \\xe2\\x88\\x82 i1 Aa1 \\xe2\\x88\\x82 i2 +1 ca2\\r\\n                                              i1 i2\\r\\n                                                                i1 +i2\\r\\n                                                                =N \\xe2\\x88\\x923\\r\\n                                X                                     \\x01               \\x01                    \\x01\\r\\n                  + g2                  e a;a1 a2 a3 \\xe2\\x88\\x82 i1 Aa1\\r\\n                                        C                                  \\xe2\\x88\\x82 i2 Aa2        \\xe2\\x88\\x82 i3 +1 ca3\\r\\n                                          i1 i2 i3\\r\\n                           i1 +i2 +i3\\r\\n                            =N \\xe2\\x88\\x924\\r\\n                                                                                                                                           #\\r\\n                             X                                             \\x01              \\x01                \\x01                  \\x01\\r\\n                  +g   3               e a;a1 a2 a3 a4\\r\\n                                       C                    \\xe2\\x88\\x82 A  i1   a1       i2\\r\\n                                                                               \\xe2\\x88\\x82 A   a2         i3\\r\\n                                                                                                \\xe2\\x88\\x82 A  a3\\r\\n                                                                                                               \\xe2\\x88\\x82   i4 +1 a4\\r\\n                                                                                                                       c               4\\r\\n                                                                                                                                  + O(g ) ,\\r\\n                                         i1 i2 i3 i4\\r\\n                           i1 +..+i4\\r\\n                            =N \\xe2\\x88\\x925\\r\\n\\r\\nis at first sight not equivalent to its gBRST generated cousin. Equating the two with each\\r\\nother,\\r\\n                                   ca \\xe2\\x88\\x82 \\xc2\\xb5 s\\xe2\\x88\\x86 Aa\\xc2\\xb5 = ca \\xe2\\x88\\x82 \\xc2\\xb5 s\\xe2\\x88\\x86 Aa\\xc2\\xb5 ,                  (4.29)\\r\\ntherefore generates non-trivial identities among the various \\xce\\xb7-coefficients. Identifying the\\r\\nRHS of eqs. (4.21) and (4.28) we then find, after using integration by parts and the product\\r\\nrule, the following relation:\\r\\n                                                             (\\r\\n        X                     \\x01               \\x01            \\x01\\r\\n 0=            (\\xe2\\x88\\x82ca ) \\xe2\\x88\\x82 i1 Aa1 .. \\xe2\\x88\\x82 in\\xe2\\x88\\x921 Aan\\xe2\\x88\\x921 \\xe2\\x88\\x82 in +1 can     e a;a1 ..an \\xe2\\x88\\x92\\r\\n                                                               C                       (4.30)    i1 ..in\\r\\n      i1 +..+in\\r\\n      =N \\xe2\\x88\\x92n\\xe2\\x88\\x921\\r\\n i1         in\\xe2\\x88\\x921                                                                                     )\\r\\n X          X (s1 + .. + sn\\xe2\\x88\\x921 + in )!\\r\\n                                            s1 +..sn\\xe2\\x88\\x921 +in e an ;a1 ..an\\xe2\\x88\\x921 a\\r\\n         ..                           \\xc3\\x97 (\\xe2\\x88\\x921)              C(i1 \\xe2\\x88\\x92s1 )..(in\\xe2\\x88\\x921 \\xe2\\x88\\x92sn\\xe2\\x88\\x921 )(in +s1 +..+sn\\xe2\\x88\\x921 ) .\\r\\n                   s1 !..sn\\xe2\\x88\\x921 !in !\\r\\n s1 =0    sn\\xe2\\x88\\x921 =0\\r\\n\\r\\nLet us remark that the summand does not necessarily vanish independently here. It does\\r\\nonly as long as the field contents and its derivatives are independent in each term in the\\r\\nsum. One therefore has to be careful when applying this identity. Using the definitions\\r\\nin eqs. (3.42)-(3.44) we then derive the following set of constraints on the couplings of the\\r\\nghost operators:\\r\\n                           i1\\r\\n                           X                     \\x10s +i \\x11\\r\\n            (1)                                       1          2       (1)\\r\\n           \\xce\\xb7i1 i2 = \\xe2\\x88\\x92            (\\xe2\\x88\\x921)s1 +i2                           \\xce\\xb7(i1 \\xe2\\x88\\x92s1 )(i2 +s1 ) ,                                                    (4.31)\\r\\n                           s1 =0\\r\\n                                                           s1\\r\\n                          i2\\r\\n                       i1 X\\r\\n                       X\\r\\n           (1)                          (s1 + s2 + i3 )!                 (1)\\r\\n         \\xce\\xb7i1 i2 i3 =                                     (\\xe2\\x88\\x921)s1 +s2 +i3 \\xce\\xb7(i2 \\xe2\\x88\\x92s2 )(i1 \\xe2\\x88\\x92s1 )(i3 +s1 +s2 ) ,                                     (4.32)\\r\\n                                           s1 !s2 !i3 !\\r\\n                       s1 =0 s2 =0\\r\\n                          i2\\r\\n                       i1 X\\r\\n                       X\\r\\n          (2)                (s1 + s2 + i3 )!                                             (2)\\r\\n         \\xce\\xb7i1 i2 i3 =                                             (\\xe2\\x88\\x921)s1 +s2 +i3 \\xce\\xb7(i1 \\xe2\\x88\\x92s1 )(i2 \\xe2\\x88\\x92s2 )(i3 +s1 +s2 ) ,                             (4.33)\\r\\n                                            s1 !s2 !i3 !\\r\\n                       s1 =0 s2 =0\\r\\n                       i1 X\\r\\n                       X  i2\\r\\n          (3)                (s1 + s2 + i3 )!                                             (3)\\r\\n         \\xce\\xb7i1 i2 i3 =                                             (\\xe2\\x88\\x921)s1 +s2 +i3 \\xce\\xb7(i2 \\xe2\\x88\\x92s2 )(i1 \\xe2\\x88\\x92s1 )(i3 +s1 +s2 ) ,                             (4.34)\\r\\n                       s1 =0 s2 =0\\r\\n                                            s1 !s2 !i3 !\\r\\n                           i1 X\\r\\n                           X  i2 X\\r\\n                                 i3\\r\\n      (1)                           (s1 + s2 + s3 + i4 )!\\r\\n     \\xce\\xb7i1 i2 i3 i4 = \\xe2\\x88\\x92\\r\\n                           s1 =0 s2 =0 s3 =0\\r\\n                                                       s1 !s2 !s3 !i4 !\\r\\n\\r\\n                                                                               (3)\\r\\n                                            \\xc3\\x97 (\\xe2\\x88\\x921)s1 +s2 +s3 +i4 \\xce\\xb7(i3 \\xe2\\x88\\x92s3 )(i2 \\xe2\\x88\\x92s2 )(i1 \\xe2\\x88\\x92s1 )(i4 +s1 +s2 +s3 ) ,                               (4.35)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                      \\xe2\\x80\\x93 19 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                       i1 X\\r\\n                       X  i2 X\\r\\n                             i3\\r\\n     (2a)                       (s1 + s2 + s3 + i4 )!\\r\\n    \\xce\\xb7i1 i2 i3 i4 = \\xe2\\x88\\x92\\r\\n                       s1 =0 s2 =0 s3 =0\\r\\n                                                      s1 !s2 !s3 !i4 !\\r\\n\\r\\n                                                                         (2a)\\r\\n                                          \\xc3\\x97 (\\xe2\\x88\\x921)s1 +s2 +s3 +i4 \\xce\\xb7(i1 \\xe2\\x88\\x92s1 )(i2 \\xe2\\x88\\x92s2 )(i3 \\xe2\\x88\\x92s3 )(i4 +s1 +s2 +s3 ) ,         (4.36)\\r\\n\\r\\n                                                          i2 X\\r\\n                                                       i1 X\\r\\n                                                       X     i3\\r\\n      (2b)            (2a)               (2a)                   (s1 + s2 + s3 + i4 )!\\r\\n    \\xce\\xb7i1 i2 i3 i4 = \\xce\\xb7i1 i3 i2 i4 \\xe2\\x88\\x92 \\xce\\xb7i1 i2 i3 i4 +\\r\\n                                                                                  s1 !s2 !s3 !i4 !\\r\\n                                                       s1 =0 s2 =0 s3 =0\\r\\n                                                                           (2b)\\r\\n                                                \\xc3\\x97 (\\xe2\\x88\\x921)s1 +s2 +s3 +i4 \\xce\\xb7(i1 \\xe2\\x88\\x92s1 )(i2 \\xe2\\x88\\x92s2 )(i3 \\xe2\\x88\\x92s3 )(i4 +s1 +s2 +s4 ) .   (4.37)\\r\\n\\r\\nTo the best of our knowledge the existence of these kind of identities was not known by\\r\\nthe authors of the previous works [13, 37]. But we can use their one-loop all-N results for\\r\\n (1)\\r\\n\\xce\\xb7ij , which in their work was named \\xce\\xb7i to check eq. (4.31) at this order. Their one-loop\\r\\nresult in our notation is given by\\r\\n\\r\\n                                             1      h          \\x10 N \\xe2\\x88\\x92 2 \\x11 \\x10 N \\xe2\\x88\\x92 2 \\x11i\\r\\n                             (1),1\\r\\n                       \\xc7\\xab\\xce\\xb7ij          =                (\\xe2\\x88\\x921)i \\xe2\\x88\\x92 3         \\xe2\\x88\\x92           .                                  (4.38)\\r\\n                                         2N (N \\xe2\\x88\\x92 1)                i       i+1\\r\\n\\r\\nSubstituting this result into eq. (4.31) we then find:\\r\\n                              i\\r\\n                              X                    \\x10s+j \\x11                            3 (\\xe2\\x88\\x921)N \\xe2\\x88\\x92 1 \\x10 N \\xe2\\x88\\x92 2 \\x11\\r\\n                  (1),1                                          (1),1\\r\\n                \\xce\\xb7ij       +          (\\xe2\\x88\\x921)s+j                    \\xce\\xb7(i\\xe2\\x88\\x92s)(j+s) =                                          (4.39)\\r\\n                                                        s                            2\\xc7\\xab N (N \\xe2\\x88\\x92 1) 1 + i\\r\\n                              s=0\\r\\n\\r\\nThe right hand side thus indeed vanishes for all positive even values of N , as required.\\r\\n     We initially found these identities after inspecting the results of explicit calculations.\\r\\nWe could explain the extra relations by imposing a ghost-antighost exchange symmetry;\\r\\nwhose origin we then finally derived as a consequence of the anti-gBRST symmetry. Since\\r\\nthe \\xce\\xb7-coefficients can be written in terms of the \\xce\\xba-coefficients it then follows that the set\\r\\nof \\xce\\xba-coefficients associated to the different EOM operators is not actually independent.\\r\\nThat is there are nontrivial relations among the EOM-operators. To solve these relations\\r\\nin closed form is in general difficult but it is not too hard to solve them for fixed N on\\r\\na case-by-case basis. We will give examples and demonstrate the use of these relations in\\r\\nsection 5 where we construct minimal bases of operators for the lowest values of N , and\\r\\nstudy the size of the basis for higher N .\\r\\n\\r\\n5   Operator Bases Construction for fixed N\\r\\n\\r\\nIn this section we will construct explicit bases of the unphysical (EOM and ghost) operators\\r\\n                                                       (N )\\r\\nwhich can mix with the gauge-invariant operator O1 for fixed N , valid up to the four-loop\\r\\n                                                (N ),k\\r\\nlevel. The structure of the EOM operator OEOM up to four loops was discussed in section\\r\\n3.3. As explained there, we only require k \\xe2\\x89\\xa4 4 for general N when working up to four\\r\\nloops. The corresponding EOM operators were presented in eqs. (3.13)- (3.16). Another\\r\\nconstraint on k arises for fixed N since we only have a total budget of N \\xe2\\x88\\x92 1 \\xe2\\x88\\x82s and As to\\r\\n                                                                      (k)\\r\\nspend in the G-function multiplying the EOM in eq. (3.4). Since OEOM requires at least k\\r\\nAs, this leads to k \\xe2\\x89\\xa4 N \\xe2\\x88\\x92 1.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                                 \\xe2\\x80\\x93 20 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n    Being determined by the gBRST symmetry explicit expressions for all ghost operators\\r\\nrequired up to four loops are given in eqs. (4.22)-(4.25). Their color decompositions are\\r\\ngiven in eqs.(3.42)-(3.44), with the Wilson coefficients being related to those of the EOM\\r\\noperators via eqs.(3.45)-(3.51). Finally, the generalised anti-BRST symmetry imposes fur-\\r\\nther constraints given in eqs.(4.31)-(4.37), reducing unphysical operators to a yet smaller\\r\\nbasis. Having all these definitions at our deposal we are now in a position to construct\\r\\nexplicit and minimal bases. In the remaining part of this section we provide explicitly the\\r\\n                                                      (N )\\r\\nbases that are relevant for the renormalisation of O1 , with N = 2, 4 and N = 6, and we\\r\\ndescribe the space of independent operators for higher N .\\r\\n\\r\\n5.1   N = 2 operators\\r\\n                                                                                     (2)\\r\\nThe construction of a basis of unphysical operators mixing with O1 , which has dimension\\r\\n                                                          (2),1\\r\\n4, is straightforward. There is a single EOM operator, OEOM , defined in eq. (3.13). The\\r\\n                                 (2),1\\r\\ncorresponding ghost operator, OG , is given in eq. (4.22). They read\\r\\n                           (2),1                                 (2),1\\r\\n                       OEOM = \\xce\\xb7 (D.F )a Aa ,                   OG        = \\xe2\\x88\\x92\\xce\\xb7 \\xe2\\x88\\x82c\\xcc\\x84a \\xe2\\x88\\x82ca .       (5.1)\\r\\n\\r\\nWe note that both operators feature the same coupling constant, \\xce\\xb7, which follows from\\r\\nthe generalised BRST symmetry. In practice, this fact has important consequences for\\r\\n                                                (2)         (2)         (N )\\r\\nrenormalisation, because it implies that the OEOM and OG mix with O1 with the same\\r\\ncounterterm. In other words, we find only one unphysical operator mixing with the gauge\\r\\ninvariant operator of N = 2. Following the vector notation introduced in sec.2, the twist-2\\r\\noperators of dimension 4 are written as O~ (2) = (O(2) , O(2) ) with\\r\\n                                                    1     2\\r\\n\\r\\n                                         (2) 1 \\x02        \\x03\\r\\n                                        O1 =   Tr F\\xce\\xbd F \\xce\\xbd ,                                     (5.2)\\r\\n                                             2\\r\\n                                         (2)\\r\\n                                        O2 = (D.F )a Aa + ca \\xe2\\x88\\x82 2 ca .                          (5.3)\\r\\n\\r\\n5.2   N = 4 operators\\r\\n                                            (4)\\r\\nThe mass dimension-6 operator O1 undergoes a less trivial mixing pattern. All EOM\\r\\n                (4),k\\r\\noperators in OEOM with k \\xe2\\x89\\xa4 3 are relevant and each sector generates associated ghost\\r\\n                                                                                (N ),1\\r\\noperators. As for the case N = 2, we can readily write down the EOM operator OEOM\\r\\nand its associated ghost operator\\r\\n                                           (4),1\\r\\n                                         OEOM = \\xce\\xb7 (D.F )a \\xe2\\x88\\x82 2 Aa ,                             (5.4)\\r\\n                                            (4),1\\r\\n                                          OG        = \\xe2\\x88\\x92\\xce\\xb7 \\xe2\\x88\\x82c\\xcc\\x84a \\xe2\\x88\\x82 3 ca .                         (5.5)\\r\\n                   (4),2\\r\\nNext we consider OEOM , eq. (3.14), which involves only one operator, due to the antisym-\\r\\nmetry of the coefficients \\xce\\xbaij , eq. (3.22). It reads\\r\\n                                (4),2\\r\\n                             OEOM = 2 g\\xce\\xba01 f aa1 a2 (D.F )a Aa1 \\xe2\\x88\\x82Aa2 .                         (5.6)\\r\\n                        (4),2\\r\\nThe ghost operator, OG          , is defined in eq. (4.23) in terms of the coefficients C\\xcc\\x83iaa 1 a2\\r\\n                                                                                           1 i2\\r\\n                                                                                                   of\\r\\neq. (3.42) as\\r\\n                                             h                               i\\r\\n                    (4),2                      (1)               (1)\\r\\n                  OG        = \\xe2\\x88\\x92gf aa1 a2 \\xe2\\x88\\x82c\\xcc\\x84a \\xce\\xb701 Aa1 \\xe2\\x88\\x82 2 ca2 + \\xce\\xb710 \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82ca2 .                (5.7)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      \\xe2\\x80\\x93 21 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                                                                                   (1)       (1)\\r\\nThe generalised BRST symmetry imposes that the coefficients \\xce\\xb701 and \\xce\\xb710 are related to\\r\\n                     (4),1   (4),2\\r\\nthe parameters in OEOM and OEOM , respectively \\xce\\xb7 and \\xce\\xba01 . These relations are given in\\r\\neq. (3.45) and lead to\\r\\n                    h      \\x10                     \\x11           \\x10                       \\x11i\\r\\n    (4),2\\r\\n  OG = \\xe2\\x88\\x92gf aa1 a2 \\xce\\xb7 \\xe2\\x88\\x82c\\xcc\\x84a 2\\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82ca2 + Aa1 \\xe2\\x88\\x82 2 ca2 + 2\\xce\\xba01 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 \\xe2\\x88\\x82 2 ca2 \\xe2\\x88\\x92 \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82ca2 .\\r\\n                                                                                                           (5.8)\\r\\n\\r\\nThe unphysical operators in eqs. (5.4), (5.5), (5.6) and (5.8) contribute to the renormali-\\r\\n           (4)\\r\\nsation of O1 starting from two loops [13, 37]. From three loops onwards, we must take\\r\\n                                       (4),3\\r\\ninto account also the EOM operator OEOM , eq. (3.15), which reads\\r\\n                              (4),3           (2)\\r\\n                            OEOM = g2 \\xce\\xba000 daa1 a2 a3 (D.F )a Aa1 Aa2 Aa3 ,                                (5.9)\\r\\n\\r\\nwhere we applied eqs. (3.23), (3.25), (3.28), (3.29) and (3.30) to restrict the independent\\r\\ncouplings to a single operator at mass dimension 6. Due to the fully symmetric nature\\r\\nof the colour structure daa1 a2 a3 of eq. (5.9), we find that two-point correlators with an\\r\\n                (4),3                                                                                   (4),3\\r\\ninsertion of OEOM vanish automatically at one and at two loops. This implies that OEOM\\r\\n                                 (N )                                                                   (4),3\\r\\nenters the renormalisation of O1 only at four loops. We derive the ghost operator OG ,\\r\\neq. (4.24), by computing the coefficients (3.46)-(3.48), which enter C        e aa1 a2 a3 in eq. (3.43).\\r\\n                                                                                i1 i2 i3\\r\\nWe get\\r\\n                                         h                                                          i\\r\\n          (4),3                            (1)                (2)                 (3)\\r\\n        OG = \\xe2\\x88\\x92g2 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82ca3 \\xce\\xb7000 (f f )aa1 a2 a3 + \\xce\\xb7000 daa1 a2 a3 + \\xce\\xb7000 daa     d\\r\\n                                                                                            1 a2 a3\\r\\n                                                                                         4f f\\r\\n                                         h                                      i\\r\\n                      2 a a1 a2       a3          aa1 a2 a3       (2) aa1 a2 a3\\r\\n                = \\xe2\\x88\\x92g \\xe2\\x88\\x82c\\xcc\\x84 A A \\xe2\\x88\\x82c 2\\xce\\xba01 (f f )                 + 3\\xce\\xba000 d             .                   (5.10)\\r\\n\\r\\nBy taking into account only the relations deriving from the generalised BRST symmetry,\\r\\nwe obtained a set of three unphysical operators, each of them corresponding to the terms\\r\\nin eqs. (5.4)-(5.6) and (5.8)-(5.10) that are proportional to the coefficients \\xce\\xb7, \\xce\\xba01 and\\r\\n  (2)\\r\\n\\xce\\xba000 , respectively. However, the generalised anti-BRST symmetry introduces an additional\\r\\nconstraint on these coefficients and reduces the set of independent operators further. By\\r\\nspecialising i1 , i2 = 0, 1 in eq. (4.31) we find\\r\\n                                        (1)         (1)\\r\\n                                      2\\xce\\xb710 = \\xce\\xb701 \\xe2\\x87\\x90\\xe2\\x87\\x92 2\\xce\\xba01 = \\xce\\xb7.                                             (5.11)\\r\\n\\r\\nThis identity is surprising, because it relates the couplings of different EOM operators,\\r\\nwhich are free a priori. Therefore the EOM and ghost Lagrangian feature only two in-\\r\\n                                        (2)\\r\\ndependent parameters, e.g. \\xce\\xb7 and \\xce\\xba000 , which are chosen as coupling constants of two\\r\\nindependent unphysical operators. In conclusion, we obtain a basis of operators with spin\\r\\n4 (and dimension 6) O~ (4) = (O(4) , O(4) , O(4) ) with\\r\\n                               1      2      3\\r\\n\\r\\n (4) 1 \\x02          \\x03\\r\\nO1 = Tr F\\xce\\xbd D 2 F \\xce\\xbd ,                                                                                      (5.12)\\r\\n     2    h                                         i                               h                           i\\r\\n  (4)\\r\\nO2 = (D.F )a \\xe2\\x88\\x82 2 Aa + gf aa1 a2 Aa1 \\xe2\\x88\\x82Aa2                \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a \\xe2\\x88\\x82 3 ca \\xe2\\x88\\x92 gf aa1 a2 \\xe2\\x88\\x82c\\xcc\\x84a 2Aa1 \\xe2\\x88\\x82 2 ca2 + \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82ca2\\r\\n     \\xe2\\x88\\x92 g2 (f f )aa1 a2 a3 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82ca3 ,                                                             (5.13)\\r\\n                 h                                       i\\r\\n (4)\\r\\nO3 = daa1 a2 a3 (D.F )a Aa1 Aa2 Aa3 \\xe2\\x88\\x92 3 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82ca3 .                                               (5.14)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         \\xe2\\x80\\x93 22 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n5.3    N = 6 operators\\r\\nThe basis of operators at mass dimension 8, which includes the gauge invariant operator\\r\\n  (6)                                           (6),k\\r\\nO1 , is generated by the full set of operators OEOM , with k \\xe2\\x89\\xa4 4. For k = 1 we get\\r\\nimmediately the EOM and ghost operators\\r\\n                   (6),1                                         (6),1\\r\\n                 OEOM = \\xce\\xb7 (D.F )a \\xe2\\x88\\x82 4 Aa ,                     OG        = \\xe2\\x88\\x92\\xce\\xb7 \\xe2\\x88\\x82c\\xcc\\x84a \\xe2\\x88\\x82 5 ca .       (5.15)\\r\\n\\r\\nThe definition in eq. (3.14) and antisymmetry of the coefficents \\xce\\xbaij , eq. (3.22), imply that\\r\\n (6),2\\r\\nOEOM includes two independent terms\\r\\n                                       h                                      i\\r\\n                 (6),2\\r\\n               OEOM = gf aa1 a2 (D.F )a 2\\xce\\xba03 Aa1 \\xe2\\x88\\x82 3 Aa2 + 2\\xce\\xba12 (\\xe2\\x88\\x82Aa1 )\\xe2\\x88\\x82 2 Aa2 .       (5.16)\\r\\n\\r\\n                                  (6),2\\r\\nTo get the ghost sector OG , we expand out eq. (4.23) with i1 , i2 = 0, .., 3 and we use the\\r\\ndefinitions in eqs. (3.42) and (3.45), to get\\r\\n                            n h                                                               i\\r\\n      (6),2\\r\\n    OG = \\xe2\\x88\\x92 gf aa1 a2 \\xe2\\x88\\x82c\\xcc\\x84a \\xce\\xb7 Aa1 \\xe2\\x88\\x82 4 ca2 + 4 \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82 3 ca2 + 6 \\xe2\\x88\\x82 2 Aa1 \\xe2\\x88\\x82 2 ca2 + 4 \\xe2\\x88\\x82 3 Aa1 \\xe2\\x88\\x82ca2\\r\\n                     h                         i      h                               io\\r\\n              + 2\\xce\\xba03 Aa1 \\xe2\\x88\\x82 4 ca2 \\xe2\\x88\\x92 \\xe2\\x88\\x82 3 Aa1 \\xe2\\x88\\x82ca2 + 2\\xce\\xba12 \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82 3 ca2 \\xe2\\x88\\x92 \\xe2\\x88\\x82 2 Aa1 \\xe2\\x88\\x82 2 ca2 .       (5.17)\\r\\n\\r\\n                                               (6),3\\r\\nSimilarly, we write down the operator OEOM , following the definition in eq. (3.15) and the\\r\\nrelations eq. (3.23), (3.25) and (3.28)-(3.30) on the coefficients, to obtain\\r\\n                                                 h                                          i\\r\\n          (6),3                                      (1)                    (1)\\r\\n        OEOM = + 2g 2 (f f )aa1 a2 a3 (D.F )a \\xce\\xba002 Aa1 Aa2 \\xe2\\x88\\x82 2 Aa3 + \\xce\\xba101 \\xe2\\x88\\x82Aa1 Aa2 \\xe2\\x88\\x82Aa3\\r\\n                                             h                                            i\\r\\n                                                (2)                    (2)\\r\\n                  + 3g 2 daa1 a2 a3 (D.F )a \\xce\\xba002 Aa1 Aa2 \\xe2\\x88\\x82 2 Aa3 + \\xce\\xba011 Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82Aa3\\r\\n                                             h\\r\\n                                                (3)\\r\\n                  + 2g 2 daa\\r\\n                          d\\r\\n                             1 a2 a3\\r\\n                                     (D.F )a\\r\\n                                               \\xce\\xba002 (Aa1 Aa2 \\xe2\\x88\\x82 2 Aa3 \\xe2\\x88\\x92 \\xe2\\x88\\x82 2 Aa1 Aa2 Aa3 )\\r\\n                          4f f\\r\\n                                                                                         i\\r\\n                                                 (3)\\r\\n                                            + \\xce\\xba101 (\\xe2\\x88\\x82Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x92 Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82Aa3 ) .         (5.18)\\r\\n\\r\\nBy expanding out eq. (4.24) for i1 , i2 , i3 = 0..2 and by using the definitions in eqs. (3.43),\\r\\n(3.46), (3.47) and (3.48) we obtain the related ghost operator\\r\\n                                            h                                                       i\\r\\n    (6),3          (1)\\r\\n  OG = \\xe2\\x88\\x92 2g2 \\xce\\xba03 (f f )aa1 a2 a3 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82 3 ca3 + 3Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82 2 ca3 + 3Aa1 \\xe2\\x88\\x82 2 Aa2 \\xe2\\x88\\x82ca3\\r\\n                                            h                                                         i\\r\\n                   (1)\\r\\n            \\xe2\\x88\\x92 2g2 \\xce\\xba12 (f f )aa1 a2 a3 \\xe2\\x88\\x82c\\xcc\\x84a \\xe2\\x88\\x82Aa1 Aa2 \\xe2\\x88\\x82 2 ca3 \\xe2\\x88\\x92 \\xe2\\x88\\x82 2 Aa1 Aa2 \\xe2\\x88\\x82ca3 + 2 \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82ca3\\r\\n                                            h                                                      i\\r\\n                   (1)\\r\\n            \\xe2\\x88\\x92 2g2 \\xce\\xba002 (f f )aa1 a2 a3 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82 3 ca3 + \\xe2\\x88\\x82 2 Aa1 Aa2 \\xe2\\x88\\x82ca3 \\xe2\\x88\\x92 2Aa1 \\xe2\\x88\\x82 2 Aa2 \\xe2\\x88\\x82ca3\\r\\n                                            h                                                         i\\r\\n                   (1)\\r\\n            \\xe2\\x88\\x92 2g2 \\xce\\xba101 (f f )aa1 a2 a3 \\xe2\\x88\\x82c\\xcc\\x84a 2 \\xe2\\x88\\x82Aa1 Aa2 \\xe2\\x88\\x82 2 ca3 \\xe2\\x88\\x92 Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82 2 ca3 \\xe2\\x88\\x92 \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82ca3\\r\\n                                        h                                    i\\r\\n                   (2)\\r\\n            \\xe2\\x88\\x92 3g2 \\xce\\xba002 daa1 a2 a3 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82 3 ca3 + 2Aa1 \\xe2\\x88\\x82 2 Aa2 \\xe2\\x88\\x82ca3\\r\\n                                        h                                        i\\r\\n                   (2)\\r\\n            \\xe2\\x88\\x92 3g2 \\xce\\xba011 daa1 a2 a3 \\xe2\\x88\\x82c\\xcc\\x84a \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82ca3 + 2Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82 2 ca3\\r\\n                                        h                                  i\\r\\n                   (3)\\r\\n            \\xe2\\x88\\x92 6g2 \\xce\\xba002 daa\\r\\n                        d\\r\\n                           1 a2 a3\\r\\n                                   \\xe2\\x88\\x82c\\xcc\\x84a\\r\\n                                          Aa1 a2 3 a3\\r\\n                                               A  \\xe2\\x88\\x82 c   \\xe2\\x88\\x92  \\xe2\\x88\\x82 2 a1 a2\\r\\n                                                              A    A  \\xe2\\x88\\x82ca3\\r\\n                        4f f\\r\\n                                        h                                      i\\r\\n                   (3)\\r\\n            \\xe2\\x88\\x92 6g2 \\xce\\xba101 daa\\r\\n                        d\\r\\n                           1 a2 a3\\r\\n                                   \\xe2\\x88\\x82c\\xcc\\x84a\\r\\n                                          \\xe2\\x88\\x82A  a1\\r\\n                                                 \\xe2\\x88\\x82Aa2\\r\\n                                                      \\xe2\\x88\\x82ca3\\r\\n                                                           \\xe2\\x88\\x92  A a1\\r\\n                                                                   \\xe2\\x88\\x82Aa2 2 a3\\r\\n                                                                       \\xe2\\x88\\x82  c      .                 (5.19)\\r\\n                           4f f\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                 \\xe2\\x80\\x93 23 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                      (6),4       (6),4\\r\\nWe construct OEOM and OG , by expanding out eqs. (3.16) and (4.25) with i1 ..i4 = 0, 1.\\r\\nAfter imposing the relations in eqs.(3.24), (3.26), (3.27), (3.31) and (3.32), which constrain\\r\\n                                                                     (1)       (2)\\r\\nthe coefficients of Cia;a 1 ..a4\\r\\n                      1 ..i4\\r\\n                                 , defined in eq. (3.19), we choose \\xce\\xba0001 and \\xce\\xba0001 as independent\\r\\n                      (6),4                     (6),4\\r\\nparameters in OEOM . At this point, OG is written in terms of the coefficients appearing\\r\\n    (6),4      (6),3\\r\\nin OEOM and OEOM , by means of eqs.(3.44), (3.49), (3.50) and (3.51), which give\\r\\n                        (6),4             (1)\\r\\n                      OEOM = + 2g3 \\xce\\xba0001 (f f f )aa1 a2 a3 a4 (D.F )a Aa1 Aa2 Aa3 \\xe2\\x88\\x82Aa4\\r\\n                                          (2)\\r\\n                                + 2g3 \\xce\\xba0001 daa\\r\\n                                             4f\\r\\n                                                1 a2 a3 a4\\r\\n                                                           (D.F )a Aa1 Aa2 Aa3 \\xe2\\x88\\x82Aa4 ,                   (5.20)\\r\\n\\r\\n                                                        h                                           i\\r\\n      (6),4             (1)\\r\\n   OG         = \\xe2\\x88\\x92 2g 3 \\xce\\xba002 (f f f )aa1 a2 a3 a4 a5 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 Aa3 \\xe2\\x88\\x82 2 ca4 + 2Aa1 Aa2 (\\xe2\\x88\\x82Aa3 )\\xe2\\x88\\x82ca4\\r\\n                                                        h                                        i\\r\\n                        (1)\\r\\n                + 2g3 \\xce\\xba101 (f f f )aa1 a2 a3 a4 a5 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 \\xe2\\x88\\x82Aa2 Aa3 \\xe2\\x88\\x82ca4 \\xe2\\x88\\x92 \\xe2\\x88\\x82Aa1 Aa2 Aa3 \\xe2\\x88\\x82ca4\\r\\n                                                h                                           i\\r\\n                        (2)\\r\\n                \\xe2\\x88\\x92 3g3 \\xce\\xba002 daa\\r\\n                             4f\\r\\n                                1 a2 a3 a4\\r\\n                                           \\xe2\\x88\\x82c\\xcc\\x84a\\r\\n                                                  Aa1 a2 a3 2 a4\\r\\n                                                      A   A  \\xe2\\x88\\x82  c  +   2A  a1 a2\\r\\n                                                                             A   \\xe2\\x88\\x82A a3\\r\\n                                                                                       \\xe2\\x88\\x82ca4\\r\\n\\r\\n                          (2)\\r\\n                \\xe2\\x88\\x92 6g3 \\xce\\xba011 daa\\r\\n                            4f\\r\\n                               1 a2 a3 a4\\r\\n                                          \\xe2\\x88\\x82c\\xcc\\x84a \\xe2\\x88\\x82Aa1 Aa2 Aa3 \\xe2\\x88\\x82ca4\\r\\n                                                        h\\r\\n                       (1)\\r\\n                \\xe2\\x88\\x92 2g3 \\xce\\xba0001 (f f f )aa1 a2 a3 a4 a5 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 Aa3 \\xe2\\x88\\x82 2 ca4 + 3Aa1 \\xe2\\x88\\x82Aa2 Aa3 \\xe2\\x88\\x82ca4\\r\\n                                                                                                         i\\r\\n                                                        \\xe2\\x88\\x92 3Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x82ca4 \\xe2\\x88\\x92 \\xe2\\x88\\x82Aa1 Aa2 Aa3 \\xe2\\x88\\x82ca4\\r\\n                                h\\r\\n                       (2)\\r\\n                \\xe2\\x88\\x92 2g3 \\xce\\xba0001 \\xe2\\x88\\x82c\\xcc\\x84a daa\\r\\n                                  4f\\r\\n                                     1 a2 a3 a4\\r\\n                                                (Aa1 Aa2 Aa3 \\xe2\\x88\\x82 2 ca4 \\xe2\\x88\\x92 Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x82ca4 )\\r\\n                                                                        i\\r\\n                                 + 2daa4f\\r\\n                                          4 a1 a2 a3\\r\\n                                                     \\xe2\\x88\\x82ca4 a1 a3\\r\\n                                                         A  A     \\xe2\\x88\\x82A a3\\r\\n                                                                          .                             (5.21)\\r\\n\\r\\nAt mass dimension 8, we found a total of eleven unphysical operators, parameterised by\\r\\n                                                                                         (6)\\r\\nan equal number of free coefficients \\xce\\xb7, \\xce\\xba03 , \\xce\\xba12 .. , that are required to renormalise O1 up\\r\\nto four loops. This picture simplifies significantly by taking into account the anti-BRST\\r\\nrelations. For instance, by evaluating eq. (4.31) for i1 , i2 = 0..3, we obtain\\r\\n                              (\\r\\n                                    (1)      (1)\\r\\n                                 2\\xce\\xb712 \\xe2\\x88\\x92 3\\xce\\xb703 = 0\\r\\n                                  (1)    (1)     (1)      (1)                            (5.22)\\r\\n                                 \\xce\\xb703 + \\xce\\xb712 \\xe2\\x88\\x92 \\xce\\xb721 + 2\\xce\\xb730 = 0\\r\\n                (1)\\r\\nwhere the \\xce\\xb7ij depend on \\xce\\xb7, \\xce\\xba12 and \\xce\\xba03 , as in eq. (3.45). The equations above are both\\r\\nsolved simultanously by imposing\\r\\n\\r\\n                                            5\\xce\\xb7 + 4\\xce\\xba12 \\xe2\\x88\\x92 6\\xce\\xba03 = 0.                                       (5.23)\\r\\n\\r\\nSimilarly, we derive further constraints by expanding eqs. (4.32) - (4.37), which lead to\\r\\n\\r\\n                                      (1)      (1)  5  5\\r\\n                                     \\xce\\xba101 \\xe2\\x88\\x92 2\\xce\\xba002 + \\xce\\xb7 + \\xce\\xba12 = 0,                                        (5.24)\\r\\n                                                    6  3\\r\\n                                      (2)    (2)\\r\\n                                     \\xce\\xba011 \\xe2\\x88\\x92 \\xce\\xba002 = 0,                                                   (5.25)\\r\\n                                      (3)     (3)\\r\\n                                     \\xce\\xba101 + 2\\xce\\xba002 = 0,                                                  (5.26)\\r\\n                                        (1)                (1)\\r\\n                                     3\\xce\\xba0001 + \\xce\\xb7 + 2\\xce\\xba12 \\xe2\\x88\\x92 3\\xce\\xba002 = 0,                                     (5.27)\\r\\n                                        (2)      (2)\\r\\n                                     2\\xce\\xba0001 \\xe2\\x88\\x92 3\\xce\\xba002 = 0.                                                (5.28)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                    \\xe2\\x80\\x93 24 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\nIn conclusion, by imposing the relations on the coefficients of eqs. (5.15)-(5.21), which are\\r\\ngiven in eqs.(5.23)-(5.28), we obtain a minimal basis of only five independent unphysical\\r\\noperators at dimension 8. For instance, we might solve eqs.(5.23)-(5.28) in terms of \\xce\\xb7, \\xce\\xba12 ,\\r\\n (1)    (2)    (3)\\r\\n\\xce\\xba002 , \\xce\\xba002 , \\xce\\xba002 and pick the following basis of independent operators\\r\\n\\r\\n                                       (6)  1 h           i\\r\\n                                     O1 = Tr F\\xce\\xbd D 4 F \\xce\\xbd ,                              (5.29)\\r\\n                                            2\\r\\n\\r\\n                                                  h5                                 \\x108\\r\\n  (6)\\r\\nO2 = (D.F )a \\xe2\\x88\\x82 4 Aa \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a \\xe2\\x88\\x82 5 ca + gf a1 a2 a3       (D.F )a1 Aa2 \\xe2\\x88\\x82 3 Aa3 \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a1 Aa2 \\xe2\\x88\\x82 4 ca3\\r\\n                                                    3                                 3\\r\\n             a2 3 a2        2 a2 2 a3     7 3 a2 a3 \\x11i          2\\r\\n                                                                                 h\\r\\n                                                                        aa1 a2 a3 5\\r\\n        + 4\\xe2\\x88\\x82A \\xe2\\x88\\x82 c + 6\\xe2\\x88\\x82 A \\xe2\\x88\\x82 c + \\xe2\\x88\\x82 A \\xe2\\x88\\x82c                        + g (f f )              (D.F )a \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82Aa2 Aa3\\r\\n                                          3                                        3\\r\\n          5   \\x10\\r\\n        \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82 3 ca3 + 4Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82 2 ca3 + 3Aa1 \\xe2\\x88\\x82 2 Aa2 \\xe2\\x88\\x82ca3 + \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82ca3\\r\\n          3\\r\\n                           \\x11i                        h2\\r\\n        \\xe2\\x88\\x92 2\\xe2\\x88\\x82Aa1 Aa2 \\xe2\\x88\\x82 2 ca3 \\xe2\\x88\\x92 g3 (f f f )aa1 a2 a3 a4 (D.F )a Aa1 Aa2 Aa3 \\xe2\\x88\\x82Aa4\\r\\n                                                       3\\r\\n          1 a \\x10 a1 a2 a3 2 a4\\r\\n        + \\xe2\\x88\\x82c\\xcc\\x84 2A A A \\xe2\\x88\\x82 c + 6Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x82ca4 \\xe2\\x88\\x92 Aa1 \\xe2\\x88\\x82Aa2 Aa3 \\xe2\\x88\\x82ca4\\r\\n          3                   \\x11i\\r\\n        \\xe2\\x88\\x92 8\\xe2\\x88\\x82Aa1 Aa2 Aa3 \\xe2\\x88\\x82ca4                                                                        (5.30)\\r\\n\\r\\n\\r\\n               h        \\x10                    4              \\x11 4         \\x10                3\\r\\n (6)\\r\\nO3 = gf aa1 a2 (D.F )a 2\\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82 2 Aa2 + Aa1 \\xe2\\x88\\x82 3 Aa2 \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a aa1 \\xe2\\x88\\x82 4 ca2 + \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82 2 ca2\\r\\n                                             3                   3                       2\\r\\n       3 2 a1 2 a2                     \\x11i                     h 10\\r\\n                           3 a1     a2       2      aa1 a2 a3\\r\\n     \\xe2\\x88\\x92 \\xe2\\x88\\x82 A \\xe2\\x88\\x82 c \\xe2\\x88\\x92 \\xe2\\x88\\x82 A \\xe2\\x88\\x82c                   + g (f f )                           a\\r\\n                                                               \\xe2\\x88\\x92 (D.F ) \\xe2\\x88\\x82A A \\xe2\\x88\\x82Aa3   a1 a2\\r\\n       2                                                          3\\r\\n           \\x104                    22                                             14\\r\\n     \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82 3 ca3 + Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82 2 ca3 + Aa1 \\xe2\\x88\\x82 2 Aa2 \\xe2\\x88\\x82ca3 \\xe2\\x88\\x92 \\xe2\\x88\\x82Aa1 Aa2 \\xe2\\x88\\x82 2 ca3\\r\\n             3                    3                                              3\\r\\n          2 a1 a2    a3     22 a1 a2 a3 \\x11i 4 3                     aa1 a2 a3 a4\\r\\n                                                                                h\\r\\n     \\xe2\\x88\\x92 2\\xe2\\x88\\x82 A A \\xe2\\x88\\x82c + \\xe2\\x88\\x82A \\xe2\\x88\\x82A \\xe2\\x88\\x82c                        \\xe2\\x88\\x92 g (f f f )                   (D.F )a Aa1 Aa2 Aa3 \\xe2\\x88\\x82Aa4\\r\\n                             3                        3\\r\\n           \\x10                                                Aa1 \\xe2\\x88\\x82Aa2 Aa3 \\xe2\\x88\\x82ca4                            \\x11i\\r\\n     \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 Aa3 \\xe2\\x88\\x82 2 ca4 \\xe2\\x88\\x92 3Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x82ca4 +                             + 4\\xe2\\x88\\x82Aa1 Aa2 Aa3 \\xe2\\x88\\x82ca4\\r\\n                                                                     2\\r\\n                                                                                                     (5.31)\\r\\n\\r\\n                         h       \\x10                                \\x11      \\x10\\r\\n (6)\\r\\nO4 = 2g 2 (f f )aa1 a2 a3 (D.F )a Aa1 Aa2 \\xe2\\x88\\x82 2 Aa3 + 2\\xe2\\x88\\x82Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82 3 ca3\\r\\n        \\xe2\\x88\\x92 2Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82 2 ca3 \\xe2\\x88\\x92 2Aa1 \\xe2\\x88\\x82 2 Aa2 \\xe2\\x88\\x82ca3 + 4\\xe2\\x88\\x82Aa1 Aa2 \\xe2\\x88\\x82 2 ca3 + \\xe2\\x88\\x82 2 Aa1 Aa2 \\xe2\\x88\\x82ca3\\r\\n                            \\x11i                      h\\r\\n        \\xe2\\x88\\x92 2\\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82ca3 + 2g3 (f f f )aa1 a2 a3 a4 (D.F )a Aa1 Aa2 Aa3 \\xe2\\x88\\x82Aa4\\r\\n              \\x10                                                                                  \\x11i\\r\\n        \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a 2Aa1 Aa2 Aa3 \\xe2\\x88\\x82 2 ca4 \\xe2\\x88\\x92 Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x82ca4 + Aa1 \\xe2\\x88\\x82Aa2 Aa3 \\xe2\\x88\\x82ca4 + 3\\xe2\\x88\\x82Aa1 Aa2 Aa3 \\xe2\\x88\\x82ca4\\r\\n                                                                                                    (5.32)\\r\\n\\r\\n                        h         \\x10                              \\x11       \\x10\\r\\n (6)\\r\\nO5 = 3g 2 daa1 a2 a3 (D.F )a Aa1 Aa2 \\xe2\\x88\\x82 2 Aa3 + Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82 3 ca3\\r\\n                                                                \\x11i\\r\\n     + 2Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82 2 ca3 + 2Aa1 \\xe2\\x88\\x82 2 Aa2 \\xe2\\x88\\x82ca3 + \\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82ca3\\r\\n                           h                              \\x10\\r\\n     + 3g 3 daa\\r\\n             4f\\r\\n                1 a2 a3 a4\\r\\n                             (D.F )a a1 a2 a3\\r\\n                                    A  A  A   \\xe2\\x88\\x82Aa4\\r\\n                                                   \\xe2\\x88\\x92 \\xe2\\x88\\x82c\\xcc\\x84a\\r\\n                                                           2AA1 Aa2 Aa3 \\xe2\\x88\\x82 2 ca4 + Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x82ca4\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                  \\xe2\\x80\\x93 25 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                                 \\x11i\\r\\n          + 2Aa1 \\xe2\\x88\\x82Aa2 Aa3 \\xe2\\x88\\x82ca4        \\xe2\\x88\\x92 6g3 daa\\r\\n                                             4f\\r\\n                                                4 a1 a2 a3\\r\\n                                                           \\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x82ca4                       (5.33)\\r\\n\\r\\n\\r\\n                               h         \\x10\\r\\n           (6)       aa1 a2 a3\\r\\n          O6 = 2g2 d4f f         (D.F )a\\r\\n                                           Aa1 Aa2 \\xe2\\x88\\x82 2 Aa3 + 2Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x92 \\xe2\\x88\\x82 2 Aa1 Aa2 Aa3\\r\\n                                   \\x11         \\x10\\r\\n               \\xe2\\x88\\x92 2\\xe2\\x88\\x82Aa1 Aa2 \\xe2\\x88\\x82Aa3 \\xe2\\x88\\x92 6\\xe2\\x88\\x82c\\xcc\\x84a Aa1 Aa2 \\xe2\\x88\\x82 3 ca3 + 2Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82 2 ca3 \\xe2\\x88\\x92 \\xe2\\x88\\x82 2 Aa1 Aa2 \\xe2\\x88\\x82ca3\\r\\n                                   \\x11i\\r\\n               \\xe2\\x88\\x92 2\\xe2\\x88\\x82Aa1 \\xe2\\x88\\x82Aa2 \\xe2\\x88\\x82ca3                                                              (5.34)\\r\\n\\r\\n5.4        Operators of higher N\\r\\nThe construction of an operator basis to renormalise twist-2 operators of higher spin N is\\r\\nsummarised by the following steps.\\r\\n                                                 (N ),k\\r\\n        1 List all the EOM operators, OEOM , defined in eq. (3.4). Up to four loops, only\\r\\n          the terms with k \\xe2\\x89\\xa4 4, given in eqs. (3.13)-(3.16) are relevant. All these operators\\r\\n          have been written in terms of the colour structures in eqs.(3.17)-(3.19) and associ-\\r\\n          ated parameters. The latter obey the relations in eqs.(3.22)-(3.32), which define an\\r\\n          independent set of EOM operators, considering Bose symmetry only.\\r\\n\\r\\n        2 The structure of ghost operators is dictated by the generalised BRST symmetry,\\r\\n                                                        (N )\\r\\n          eq. (4.11). The operators that mix with O1 up to four loops are given, for ev-\\r\\n          ery value of N , in eqs. (4.22)-(4.25). They involve the colour structures given in\\r\\n          eqs.(3.42)-(3.44). Eqs. (3.45)-(3.51) uniquely determine all parameters of the ghost\\r\\n          Lagrangian, in terms of the parameters of the EOM operators.\\r\\n\\r\\n        3 Impose the anti-BRST symmetry, eq. (4.28). The latter implies relations among the\\r\\n          coefficients of the EOM operators via eqs. (4.31)-(4.37). These reduce the number of\\r\\n          independent operators to a minimal set.\\r\\n\\r\\nThe steps above allow to automate easily the construction of the operators, e.g. in FORM [61].\\r\\nFinding independent operators boils down to finding a set of coefficients which solves the\\r\\nlinear relations3 in eqs.(3.22)-(3.32) and (4.31)-(4.36), using the definitions in eqs.(3.45)-\\r\\n(3.51). By solving these, we determine the number of independent unphysical operators of\\r\\nhigher spin N . For up to N = 16 the size of the basis is given in table 3. The second line\\r\\nin table 3 gives the size of the basis without using anti-BRST relations, while the first line\\r\\nincludes them. While the basis grows significantly with the spin N , we find that most of\\r\\n                                                        (N ),4                (N ),4\\r\\nthe free parameters are associated to the operators OEOM . For instance, OEOM generates\\r\\n                                                                                 (N ),4\\r\\n112 out of the 140 unphysical operators at N = 16. Since mixing with OEOM is only\\r\\nrelevant at one loop, see table 2, these operators do not introduce prohibitive obstacles.\\r\\n    3\\r\\n    We notice that both eqs. (4.36) and (4.37) originate from the structure associated to the coefficients\\r\\n (2)          (N),4\\r\\n         in OEOM . We checked explicitly up to N = 10 that eq. (4.37) is automatically satisfied by the\\r\\n\\xce\\xbai1 i2 i3 i4\\r\\nsolutions of eq. (4.36), which rely also on eq. (4.33), and therefore it doesn\\xe2\\x80\\x99t provide further simplifications\\r\\nof the basis.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      \\xe2\\x80\\x93 26 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                   Spin N            2   4     6       8     10     12       14      16\\r\\n\\r\\n                   w aBRST           1   2      5      12    25     50      87      140\\r\\n                   w/o aBRST         1   3     11      30    66     126     215     339\\r\\n\\r\\n\\r\\nTable 3. Table showing the number of independent operators with and without the use of anti-\\r\\nBRST (aBRST) relations.\\r\\n\\r\\n\\r\\n6   Background-field formulation\\r\\n\\r\\nA powerful trick to simplify calculations of anomalous dimensions is to use the background\\r\\nfield method. The basic idea is to split the gauge field into a classical (non-propagating)\\r\\nbackground field component B and a purely Quantum field component Q as follows:\\r\\n\\r\\n                                   A\\xc2\\xb5a (x) = Ba\\xc2\\xb5 (x) + Q\\xc2\\xb5a (x) .                             (6.1)\\r\\n\\r\\nOne can then consider Green\\xe2\\x80\\x99s functions with external background fields. By using a clever\\r\\ngauge fixing and ghost ghost Lagrangian, for the Quantum field [50\\xe2\\x80\\x9353]\\r\\n                                                    1\\r\\n                  LBGF+BG (Q, B, c\\xcc\\x84, c) = \\xe2\\x88\\x92            (D\\xcc\\x84 \\xc2\\xb5 A\\xc2\\xb5 )2 \\xe2\\x88\\x92 c\\xcc\\x84a D\\xcc\\x84\\xc2\\xb5ab D \\xc2\\xb5;bc cc ,   (6.2)\\r\\n                                                    2\\xce\\xbe\\r\\nwhere the background- and background+quantum-field covariant derivatives are defined as\\r\\n\\r\\n               D\\xcc\\x84\\xc2\\xb5ac = \\xe2\\x88\\x82\\xc2\\xb5 \\xce\\xb4ac + gf abc B\\xc2\\xb5b ,        D\\xc2\\xb5ac = \\xe2\\x88\\x82\\xc2\\xb5 \\xce\\xb4ac + gf abc (B + Q)b\\xc2\\xb5 ,       (6.3)\\r\\n\\r\\nit then follows that the quantum gauge-fixed Lagrangian,\\r\\n\\r\\n                   LB (Q, B, c\\xcc\\x84, c) = L0 (Q + B) + LBGF+BG (Q, B, c\\xcc\\x84, c) ,                   (6.4)\\r\\n\\r\\nstays invariant under background-field gauge transformations\\r\\n                                    B a\\r\\n                                   \\xce\\xb4w B\\xc2\\xb5 (x) = D\\xcc\\x84\\xc2\\xb5ac \\xcf\\x89(x)c ,\\r\\n                                    B a\\r\\n                                   \\xce\\xb4w Q\\xc2\\xb5 (x) = gf abc Qb\\xc2\\xb5 \\xcf\\x89(x)c .                            (6.5)\\r\\n\\r\\nWe now wish to discuss the form of the complete Lagrangian L\\xcc\\x83(A, c\\xcc\\x84, c), introduced in\\r\\neq. (2.21), which contains besides the Yang-Mills, gauge fixing and ghost terms also the\\r\\n                                                                             (N )\\r\\ntwist-2 gauge invariant gluonic operator ON (A), the EOM operator OEOM (A) and the\\r\\n                  (N )\\r\\nghost operator OG (A). Here we have purposefully included a dependence on A, although\\r\\nwe will not write out explicitly the dependence on its derivatives.\\r\\n     The lifting of L\\xcc\\x83(A, c\\xcc\\x84, c) into the background field formalism is straight forward for\\r\\nthe gauge invariant part but requires some minor modifications to the EOM and ghost\\r\\n                                                                        (N )\\r\\noperator. We therefore introduce their background field versions OBEOM (Q, B, c\\xcc\\x84, c) and\\r\\n  (N )\\r\\nOBG (Q, B). Before giving a detailed derivation of the form of the Lagrangian we will state\\r\\ntheir form below. The complete Lagrangian then reads\\r\\n                                                                                   (N )\\r\\n            L\\xcc\\x83B (A, B, c\\xcc\\x84, c) =L0 (Q + B) + LBGF+BG (Q, B, c\\xcc\\x84, c) + O1 (Q + B)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                               \\xe2\\x80\\x93 27 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                                              (N )                        (N )\\r\\n                                       + OBEOM (Q, B, c\\xcc\\x84, c) + OBG (Q, B) ,                                  (6.6)\\r\\n\\r\\nwhere                                    X                                   X\\r\\n                               (N )               (N ),k           (N )               (N ),k\\r\\n                             OBG =             OBG ,             OBEOM =             OBEOM ,                 (6.7)\\r\\n                                          k                                      k\\r\\n\\r\\n\\r\\n           (N ),k\\r\\n                                                           X                             \\x01            \\x01\\r\\n         OBEOM = g k\\xe2\\x88\\x921 (D \\xc2\\xb7 F (B + Q))a                            Cia;a 1 ..ak\\r\\n                                                                     1 ..ik\\r\\n                                                                                D\\xcc\\x84 i1 Qa1 .. D\\xcc\\x84 ik Qak ,     (6.8)\\r\\n                                                      i1 +..+ik\\r\\n                                                      =N \\xe2\\x88\\x92k\\xe2\\x88\\x921\\r\\n\\r\\n\\r\\n        (N ),k\\r\\n                             X                              \\x01            \\x01                \\x01            \\x01\\r\\n    OBG          = \\xe2\\x88\\x92gk\\xe2\\x88\\x921               e a;a1 .. a4 D\\xcc\\x84ca\\r\\n                                       C                        D\\xcc\\x84 i1 Qa1 .. D\\xcc\\x84 ik\\xe2\\x88\\x921 Qak\\xe2\\x88\\x921 D\\xcc\\x84 ik +1 cak .    (6.9)\\r\\n                                         i1 .. i4\\r\\n                           i1 +..+ik\\r\\n                           =N \\xe2\\x88\\x92k+1\\r\\n\\r\\n                                                       e abc.. are identical in their definitions\\r\\n                                             abc.. and C\\r\\nNote in particular that the coefficients Cijk..          ijk..\\r\\nto those defined respectively in eqs. (3.17)-(3.19) and (3.42)-(3.44). The set of EOM and\\r\\nghost operators in the background gauge formalism is thus directly related to those in the\\r\\nstandard formulation.\\r\\n    To understand the structure of the EOM operator note that it should be generated from\\r\\nan infinitesimal field transformation of the kind Q \\xe2\\x86\\x92 Q + G B (Q, B, \\xe2\\x88\\x82Q, \\xe2\\x88\\x82B, ...), since the\\r\\nQuantum effective action contains a path integral only over the field Q being a functional\\r\\nof B. This fixes the form of the EOM operator as follows:\\r\\n                             Z\\r\\n                    (N )            \\xce\\xb4S0 (A + Q) B;a \\xc2\\xb5 \\xc2\\xb5 \\xc2\\xb5\\r\\n                 OBEOM = dD x                      G\\xc2\\xb5 (Q , B , \\xe2\\x88\\x82 Q, \\xe2\\x88\\x82 \\xc2\\xb5 B, ...)\\r\\n                                      \\xce\\xb4Q\\xc2\\xb5a (x)\\r\\n                                           \\x01a\\r\\n                           = D.F (Q + B) G B;a (Q, B, \\xe2\\x88\\x82Q, \\xe2\\x88\\x82B, ...)                         (6.10)\\r\\n\\r\\nwhere we have used also our earlier considerations about the mass dimension and counting\\r\\nof \\xe2\\x88\\x86-contractions. Finally we make the assertion that\\r\\n\\r\\n                                G\\xc2\\xb5B;a (Q, B, \\xe2\\x88\\x82Q, \\xe2\\x88\\x82B, ...) = G\\xc2\\xb5a (Q, D\\xcc\\x84Q, ...) .                             (6.11)\\r\\n\\r\\nwith G\\xc2\\xb5a defined in eqs. (3.2) and (3.3). There are a number of considerations which fix\\r\\nthis relation. First we require G B;a to be background-field gauge covariant - thus it can\\r\\nonly depend on Q or on its background-field covariant derivatives. However this fixes only\\r\\nits dependence on Q and B but not its functional form, G B = G. To fix this form we\\r\\nset B = 0, Q = A in the complete Lagrangian, i.e. we consider L\\xcc\\x83B (A, 0, c\\xcc\\x84, c). For this\\r\\nLagrangian to generate the same Green\\xe2\\x80\\x99s functions as L\\xcc\\x83(A, c\\xcc\\x84, c) (note their gauge-invariant\\r\\nparts are now identical) we therefore require:\\r\\n\\r\\n                                              L\\xcc\\x83B (A, 0, c\\xcc\\x84, c) = L\\xcc\\x83(A, c\\xcc\\x84, c)                              (6.12)\\r\\n\\r\\nFrom this it immediately follows that\\r\\n\\r\\n                    G\\xc2\\xb5B;a (Q, D\\xcc\\x84Q, ...)                    = G\\xc2\\xb5B;a (A, \\xe2\\x88\\x82A, ...) = G\\xc2\\xb5a (A, \\xe2\\x88\\x82A, ...)          (6.13)\\r\\n                                           B=0,Q=A\\r\\n\\r\\nand we see that eq. (6.11) satisfies these constraints uniquely.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                            \\xe2\\x80\\x93 28 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n     Let us now turn our attention to the ghost operator in the background formalism.\\r\\n                                                                        (N )\\r\\nAgain we need to satisfy the constraints that it should coincide with OG when we set\\r\\nB = 0, Q = A and that it should be background gauge invariant. A simple recipe which\\r\\n                                                                                  (N )\\r\\nsatisfies all these constraints is to to make the replacements A \\xe2\\x86\\x92 Q, \\xe2\\x88\\x82 \\xe2\\x86\\x92 D\\xcc\\x84 in OG . A\\r\\nmore thorough path to arrive at the same answer would involve working out the generalised\\r\\ngauge invariance and its associated generalised BRST symmetry. In turn one could write\\r\\nthe ghost operator in gBRST exact form, in the background field formalism.\\r\\n\\r\\n6.1   Bases of operators up to four loops\\r\\n                                                                                                (N )\\r\\nIn the background field method, we determine the renormalisation constants of O1 by\\r\\ncomputing the counterterms of correlators of the background field\\r\\n   \\x10         \\x11a1 a2             Z                              h                            i\\r\\n      (N )                                                                             (N )\\r\\n     \\xce\\x93Oi ;BB                \\xc2\\xb5\\r\\n                    (g, \\xce\\xbe; p ) = dd x1 dd x2 eip\\xc2\\xb7(x1 \\xe2\\x88\\x92x2 ) h0|T B\\xce\\xbda11 (x1 )B\\xce\\xbda22 (x2 )Oi (0) |0i1PI ,\\r\\n                 \\xce\\xbd1 \\xce\\xbd2\\r\\n                                                                                     (6.14)\\r\\nwhere the subscript 1PI indicates one-particle-irreducibe, amputated Green\\xe2\\x80\\x99s functions. In\\r\\n                                      (N )   (N )\\r\\nthe equation above, the operator Oi = Oi (B + Q) is inserted with zero momentum.\\r\\n                                 (N )        (N )\\r\\nCounterterms proportional to OBEOM and OBG are required in order to cancel divergences\\r\\nof the diagrams that contribute to eq. (6.14). Notably, these unphysical operators always\\r\\ninvolve at least one quantum gluon or a ghost-antighost pair, as it follows from the def-\\r\\ninitions in eqs. (6.8) and (6.9). Therefore, EOM and ghost operators are only required\\r\\nfrom the two-loop level onwards, in order to cancel the subdivergences of the correlator in\\r\\neq. (6.14); and no unphysical counterterm can arise at one loop [54, 62]. In table 4 we re-\\r\\n                                                     (N )\\r\\nport example diagrams showing subdivergences of \\xce\\x93Oi ;BB , which are renormalised by each\\r\\n        (N ),k                                                                                    (N ),k\\r\\nterm OEOM . Table 5 summarises the maximal loop order at which each operator OEOM\\r\\nenters the renormalisation of eq. (6.14). By comparing the last line of tables 2 and 5 we\\r\\nfind that there is an advantage in renormalising correlators of background fields, in that\\r\\nunphysical counterterms are needed only up to 3 loops. In contrast without background-\\r\\n                                    (N ),1\\r\\nfield invariance the counterterm OEOM would be required up to 4 loops, as in table 2.\\r\\n\\r\\n     In the next section we will compute the counterterms required to renormalise these\\r\\nsubdivergences. To this end, it is convenient to reduce to a basis of independent operators.\\r\\nIn the background-field method a basis for a given fixed value of N is obtained by modifying\\r\\nthe corresponding basis obtained without background field, according to the replacements:\\r\\n\\r\\n                   (D.F )a \\xe2\\x88\\x92\\xe2\\x86\\x92 (D.F (Q + B))a ,                      (\\xe2\\x88\\x82 i Aa ) \\xe2\\x88\\x92\\xe2\\x86\\x92 (D\\xcc\\x84 i Q)a ,    (6.15)\\r\\n                         (\\xe2\\x88\\x82c\\xcc\\x84a ) \\xe2\\x88\\x92\\xe2\\x86\\x92 (D\\xcc\\x84c\\xcc\\x84)a ,                        (\\xe2\\x88\\x82 i ca ) \\xe2\\x88\\x92\\xe2\\x86\\x92 (D\\xcc\\x84 i c)a .   (6.16)\\r\\n\\r\\nFor instance, the basis for N = 2 can be directly read off eq. (5.3), giving\\r\\n                                   (2)\\r\\n                                O1 = F\\xce\\xbda (Q + B)F \\xce\\xbd;a (Q + B) ,                                 (6.17)\\r\\n                                   (2)\\r\\n                                O2 = (D.F (Q + B))a Qa + ca D\\xcc\\x84 aa1 D\\xcc\\x84 a1 a2 ca2 .               (6.18)\\r\\n\\r\\nSimilarly, bases for N = 4 and N = 6 are obtained by applying eqs. (6.15) and (6.16) to\\r\\neqs.(5.12)-(5.14) and to eqs.(5.29)-(5.34), respectively.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                     \\xe2\\x80\\x93 29 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                  (N ),\\xe2\\x89\\xa41                    (N ),\\xe2\\x89\\xa42                         (N ),\\xe2\\x89\\xa43                     (N ),\\xe2\\x89\\xa44\\r\\n    L        OBEOM                      OBEOM                             OBEOM                      OBEOM\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    4\\r\\n\\r\\n\\r\\n\\r\\nTable 4. In the Lth row the table gives examples of diagrams contributing to the L-loop contri-\\r\\n            (N )                                                                          (N ),k\\r\\nbution to \\xce\\x93O1 ;BB . Subgraphs whose UV-counterterms require the various EOM operators OEOM\\r\\nare highlighted with dashed boxes.\\r\\n\\r\\n                              (N )     (N ),1          (N ),2          (N ),3          (N ),4\\r\\n                            \\xce\\x93O1 ;BB   OBEOM        OBEOM          OBEOM            OBEOM\\r\\n                                1        0               0               0               0\\r\\n                                2        1               1               0               0\\r\\n                                3        2               2               1               0\\r\\n                                4        3               3               2               1\\r\\n\\r\\n                                                                                          (N )           (N ),k\\r\\nTable 5. The table summarizes the loop orders for which the mixing of O1                         into OEOM is required,\\r\\n                               (N )\\r\\ngiven a certain loop order of \\xce\\x93O1 ;BB .\\r\\n\\r\\n\\r\\n7   Calculations and results\\r\\n\\r\\nIn this section we renormalise gauge invariant operators of spin N = 2, 4 and 6, using the\\r\\nbases in eqs. (5.2)-(5.3), (5.12)-(5.14) and (5.29)-(5.34), respectively. In these bases, we\\r\\n                                                                   (N )\\r\\nproceed to calculate the associated renormalisation constants Zi,j , defined in eq. (2.23),\\r\\nwhich in the MS scheme can be expanded as follows,\\r\\n                                                                          \\xe2\\x88\\x9e\\r\\n                                                                          X\\r\\n                       (N )             (N )                    (N )        1          (N ),r\\r\\n                     Zi j = \\xce\\xb4i j + \\xce\\xb4Zi j ,        with \\xce\\xb4Zi j =                     Zi j         (\\xce\\xb1s ).             (7.1)\\r\\n                                                                          r=1\\r\\n                                                                              \\xc7\\xabr\\r\\n\\r\\n                                                                         (N )\\r\\nThe renormalisation matrix is block triangular with Zj>1 1 = 0, as described in eq. (4.19),\\r\\n           (N )\\r\\nand only Z1 1 is required to describe the scale evolution of the gauge invariant operator O1N\\r\\nin physical matrix elements. In particular, from the definition of the anomalous dimension\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                       \\xe2\\x80\\x93 30 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                  (N ),\\xe2\\x89\\xa41                        (N ),\\xe2\\x89\\xa42                   (N ),\\xe2\\x89\\xa43     (N ),\\xe2\\x89\\xa44\\r\\n      L         OEOM+G                      OEOM+G                       OEOM+G      OEOM+G\\r\\n\\r\\n\\r\\n\\r\\n      2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      4\\r\\n\\r\\n\\r\\n\\r\\nTable 6. In the Lth row the table gives examples of diagrams containing a ghost-anti-ghost pair\\r\\n                                                               (N )\\r\\nand gluons whose UV-counterterms determine the mixing of O1 into both the EOM - and ghost\\r\\n                                                                           (N )\\r\\n- operators as required for the computation of the L-loop contribution to \\xce\\x931;BB .\\r\\n\\r\\n\\r\\nmatrix,\\r\\n                                    (N )               d2 (N ) \\xe2\\x88\\x921\\r\\n                                  \\xce\\xb3ij      = \\xe2\\x88\\x92\\xc2\\xb52         Z    (Z )kj ,                           (7.2)\\r\\n                                                      d\\xc2\\xb52 ik\\r\\none can obtain\\r\\n                                               (N )         \\xe2\\x88\\x82   (N ),1\\r\\n                                \\xce\\xb3 (N ) \\xe2\\x89\\xa1 \\xce\\xb31 1 = \\xce\\xb1s            Z        (\\xce\\xb1s ) .                   (7.3)\\r\\n                                                           \\xe2\\x88\\x82\\xce\\xb1s 1 1\\r\\nOff-diagonal elements of the renormalisation matrix do not contribute to the anomalous\\r\\ndimension of the physical operators. However, the computational method that we adopt\\r\\n                (N )                                                       (N )\\r\\nto determine Z1 1 requires the knowledge of a set of mixing contributions Z1 i>1 . Below\\r\\nwe describe the calculation of these renormalisation constants and that of the physical\\r\\nanomalous dimension.\\r\\n\\r\\n7.1       Mixing with EOM and Ghost Operators\\r\\n                                        (N )\\r\\nThe renormalisation constants Z1 i , with i > 1, are determined by the counterterms\\r\\n                                                                                             (N )\\r\\nof one-particle-irreducible, amputated Green functions, with one insertion of O1 and\\r\\nexternal ghost and gluon fields. We list examples of diagrams contributing to such Green\\xe2\\x80\\x99s\\r\\nfunctions in table 6 for general N . In practice, if we work at fixed values of N , not all\\r\\nthese contributions enter. In table 7 we show the structure of the relevant counterterms\\r\\nfor N = 2, 4 and 6. Specifically, we consider the following correlators with an operator\\r\\ninsertion at zero momentum,\\r\\n                             Z\\r\\n         (N )                                               \\x02                 (N ) \\x03\\r\\n       (\\xce\\x93i;cc )ab (g, \\xce\\xbe, p) = dd x1 dd x2 eip\\xc2\\xb7(x1 \\xe2\\x88\\x92x2 ) h0|T ca (x1 )cb (x2 )Oi (0) |0i1PI .      (7.4)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                      \\xe2\\x80\\x93 31 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\nExamples of Feynman diagrams contributing to eq. (7.4) are depicted in the first column\\r\\n                                       (N )                                   (N )    (N )\\r\\nof table 6. For every value of N , \\xce\\x93i;cc vanishes at tree level, unless Oi         = O2 , as it\\r\\ncan be seen by inspecting the operators bases in eqs. (5.3), (5.12)-(5.14) and (5.29)-(5.34).\\r\\nTherefore we write\\r\\n                                  \\xef\\xa3\\xb1     h                             i\\r\\n                                  \\xef\\xa3\\xb4  ab \\xce\\x93(N ),0 (p) + \\xce\\xb4\\xce\\x93(N ) (g, \\xce\\xbe, p)\\r\\n                                  \\xef\\xa3\\xb2\\xce\\xb4\\r\\n                                  \\xef\\xa3\\xb4         i;cc\\xcc\\x84        i;cc            if i = 2\\r\\n               (N ) ab\\r\\n             (\\xce\\x93i;cc ) (g, \\xce\\xbe, p) =                                                          (7.5)\\r\\n                                  \\xef\\xa3\\xb4\\r\\n                                  \\xef\\xa3\\xb4\\r\\n                                  \\xef\\xa3\\xb3 \\xce\\xb4ab \\xce\\xb4\\xce\\x93(N ) (g, \\xce\\xbe, p)                if i 6= 2\\r\\n                                                 i;cc\\r\\n\\r\\n                                                                                       (N )\\r\\nin order to separate the tree level contribution from the term \\xce\\xb4\\xce\\x93i;cc\\xcc\\x84 , which represents the\\r\\nsum of loop corrections to all orders, namely\\r\\n                                                     \\xe2\\x88\\x9e\\r\\n                                                     X                    \\x10 \\xce\\xb1 \\x11r\\r\\n                                  (N )                      (N ),r          s\\r\\n                                \\xce\\xb4\\xce\\x93i;cc (g, \\xce\\xbe, p) =         \\xce\\x93i;cc (\\xce\\xbe, p)            ,                 (7.6)\\r\\n                                                     r=1\\r\\n                                                                           4\\xcf\\x80\\r\\n\\r\\n          g    2                         (N )                                                 (N )\\r\\nwith \\xce\\xb1s = 4\\xcf\\x80 . Counterterms of \\xce\\xb4\\xce\\x93i;cc\\xcc\\x84 must therefore be proportional to O2 . In particular,\\r\\n               (N )\\r\\ninserting O1          into eq. (7.4), we get\\r\\n                                     h       i\\r\\n                                        (N )       (N ) (N ),0\\r\\n                                  Z \\xce\\xb4\\xce\\x931;cc\\xcc\\x84 = Zc \\xce\\xb4Z1 2 \\xce\\x932,cc\\xcc\\x84 ,             \\xe2\\x88\\x80N                       (7.7)\\r\\n\\r\\nwhere Z extracts the local counterterm of each Feynman diagram contributing to eq. (7.4).\\r\\nTo this end, we apply the R\\xe2\\x88\\x97 operation [63\\xe2\\x80\\x9366], using a formulation that is valid for a\\r\\ngeneral Feynman rule of the inserted operator [67\\xe2\\x80\\x9370]\\r\\n                            h       i         h                 i\\r\\n                               (N )                   (N )\\r\\n                         Z \\xce\\xb4\\xce\\x931;cc\\xcc\\x84 = \\xe2\\x88\\x92K\\xc7\\xab R\\xcc\\x84\\xe2\\x88\\x97 Tp(N ) \\xce\\xb4\\xce\\x931;cc\\xcc\\x84 |p=0 .                  (7.8)\\r\\n\\r\\n        (N )\\r\\nHere Tp   denotes a Taylor expansion operator which extracts the term of order pN . The\\r\\noperation K\\xc7\\xab extracts the singular terms of Laurent series in \\xc7\\xab\\r\\n                               \" \\xe2\\x88\\x9e           #   \\xe2\\x88\\x921\\r\\n                                 X               X\\r\\n                            K\\xc7\\xab        f(k) \\xc7\\xabk =       f(k) \\xc7\\xabk ,                    (7.9)\\r\\n                                         k=\\xe2\\x88\\x92n                  k=\\xe2\\x88\\x92n\\r\\n\\r\\nand the operation R\\xcc\\x84\\xe2\\x88\\x97 isolates the local counterterm by subtracting all UV subdivergences\\r\\n                                                                                  (N )\\r\\nand IR divergences. In addition to eq. (7.7), we determined the elements Z1 2 of the mixing\\r\\nmatrix using an alternative approach, described in appendix A. In this way we obtain\\r\\n                                   \\x14                         \\x15 \\x10 \\x11\\r\\n    (2)     \\xce\\xb1s CA \\x10 \\xce\\xb1s \\x112 2 19                  5 \\xce\\xbeF     35        \\xce\\xb1s 3 3 h       779\\r\\n \\xce\\xb4Z1 2 = \\xe2\\x88\\x92           +        CA         2\\r\\n                                            +         \\xe2\\x88\\x92       +          CA \\xe2\\x88\\x92\\r\\n            4\\xcf\\x80 2\\xc7\\xab        4\\xcf\\x80          24\\xc7\\xab       48 \\xc7\\xab      48\\xc7\\xab       4\\xcf\\x80            432\\xc7\\xab3\\r\\n            \\x10\\r\\n          1 2807 35\\xce\\xbeF          5\\xce\\xbe 2 \\x11     1  \\x10    16759 11\\xce\\xb63        377\\xce\\xbeF     5\\xce\\xb63 \\xce\\xbeF     65\\xce\\xbeF2 \\x11i\\r\\n        + 2          \\xe2\\x88\\x92       + F +             \\xe2\\x88\\x92         \\xe2\\x88\\x92       +         +           \\xe2\\x88\\x92\\r\\n          \\xc7\\xab    864      216    288         \\xc7\\xab       7776      72      1728       72        1728\\r\\n              4\\r\\n        + O(\\xce\\xb1s ),                                                                              (7.10)\\r\\n                       \\x10    \\x11      \\x14                              \\x15 \\x10 \\x11           h\\r\\n    (4)     \\xce\\xb1s CA        \\xce\\xb1s 2 2        97          \\xce\\xbeF      8641         \\xce\\xb1s 3 3 9437\\r\\n \\xce\\xb4Z1 2 = \\xe2\\x88\\x92           \\xe2\\x88\\x92        CA             2\\r\\n                                               \\xe2\\x88\\x92        +           +         CA\\r\\n            4\\xcf\\x80 12\\xc7\\xab       4\\xcf\\x80          1440\\xc7\\xab        320\\xc7\\xab 86400\\xc7\\xab           4\\xcf\\x80          86400\\xc7\\xab3\\r\\n          1 \\x10      1520341    853\\xce\\xbeF   \\x11      1 \\x10    166178237       \\xce\\xb63     37199\\xce\\xbeF       37\\xce\\xb63 \\xce\\xbeF \\x11i\\r\\n        + 2 \\xe2\\x88\\x92               +           +        \\xe2\\x88\\x92              \\xe2\\x88\\x92       +            +\\r\\n          \\xc7\\xab       15552000    86400          \\xc7\\xab      466560000 2400          648000         9600\\r\\n\\r\\n\\r\\n\\r\\n                                                     \\xe2\\x80\\x93 32 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                        (N \\xe2\\x89\\xa52)                   (4)                 (6)                        (6)\\r\\n     L             O2                          O3                 O3                      Oi\\xe2\\x88\\x88{3,4,5,6}\\r\\n\\r\\n\\r\\n\\r\\n     2\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n     3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n     4\\r\\n\\r\\n\\r\\n\\r\\nTable 7. In the Lth row the table gives examples of diagrams containing a ghost-anti-ghost pair and\\r\\n                                                                                           (N \\xe2\\x88\\x88{2,4,6})\\r\\ngluons whose UV-counterterms determine the mixing of O(N ) into unphysical operators Oi\\r\\n                                                               (N \\xe2\\x88\\x88{2,4,6}),L\\r\\nas required for the computation of the L-loop contribution to Z11             in the background field\\r\\nmethod.\\r\\n\\r\\n\\r\\n           + O(\\xce\\xb14s ),                                                                                    (7.11)\\r\\n                             \\x10 \\xce\\xb1 \\x112        \\x14                                   \\x15\\r\\n     (6)        \\xce\\xb1s CA     s            2         653       19\\xce\\xbeF   185093\\r\\n \\xce\\xb4Z1 2 = \\xe2\\x88\\x92             \\xe2\\x88\\x92              CA              2\\r\\n                                                        +       +         + O(\\xce\\xb13s ).                     (7.12)\\r\\n                4\\xcf\\x80 30\\xc7\\xab   4\\xcf\\x80                    10080\\xc7\\xab     20160\\xc7\\xab 4233600\\xc7\\xab\\r\\n\\r\\nHere \\xce\\xbeF = 1 \\xe2\\x88\\x92 \\xce\\xbe is the gauge fixing parameter, such that \\xce\\xbeF = 0 recovers the result in\\r\\nFeynman gauge.\\r\\n                                           (N >2)\\r\\n    In order to extract the terms Z1 i>2 , we compute the counterterms of three- and four-\\r\\npoint correlators, depicted in the second and in the third columns of table 6, respectively.\\r\\nFor this purpose we consider the three-point Green\\xe2\\x80\\x99s function\\r\\n                                  Z\\r\\n      (N )\\r\\n   (\\xce\\x93i;ccg )abc\\r\\n            \\xc2\\xb5   (g, \\xce\\xbe, p  ,\\r\\n                         1 2p ) =   dd x1 dd x2 dd x3 eip1 \\xc2\\xb7(x1 \\xe2\\x88\\x92x3 ) eip2 \\xc2\\xb7(x2 \\xe2\\x88\\x92x3 )\\r\\n                                                            h                              i\\r\\n                                                              a         b        c    (N )\\r\\n                                                  \\xc3\\x97 h0|T c (x1 )c (x2 )A\\xc2\\xb5 (x3 )Oi (0) |0i, (7.13)\\r\\n\\r\\nwhich is expanded as follows:\\r\\n    \\x10           \\x11                      h\\x10       \\x11                 X\\xe2\\x88\\x9e \\x10        \\x11                 \\x10 \\xce\\xb1 \\x11r i\\r\\n         (N ) abc                         (N ),0 abc                   (N ),r abc                  s\\r\\n        \\xce\\x93i;cc\\xcc\\x84g    (g, \\xce\\xbe, p1 , p2 ) = g \\xce\\x93i;cc\\xcc\\x84g      (p1 , p2 ) +     \\xce\\x93i;cc\\xcc\\x84g     (\\xce\\xbe, p1 , p2 )          ,\\r\\n                 \\xc2\\xb5                               \\xc2\\xb5                              \\xc2\\xb5                 4\\xcf\\x80\\r\\n                                                                  r=1\\r\\n                                                                 |                   {z                  }\\r\\n                                                                              \\x10            \\x11\\r\\n                                                                                    (N) abc\\r\\n                                                                                  \\xce\\xb4\\xce\\x93i;cc\\xcc\\x84g\\r\\n                                                                                            \\xc2\\xb5\\r\\n                                                                                (7.14)\\r\\nwhere we separated the tree-level contribution from the loop corrections, similarly to\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                       \\xe2\\x80\\x93 33 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                                                                                                      (N )\\r\\neqs. (7.5) and (7.6). The counterterm of eq. (7.13), with an insertion of O1 , reads\\r\\n                     \\x14\\x10       \\x11 \\x15          p X (N ) \\x10 (N ),0 \\x11abc\\r\\n                          (N ) abc\\r\\n                   Z \\xce\\xb4\\xce\\x931;cc\\xcc\\x84g      = gZg Zc Z3        \\xce\\xb4Z1 k \\xce\\x93k,cc\\xcc\\x84g      .           (7.15)\\r\\n                                           \\xc2\\xb5                                                 \\xc2\\xb5\\r\\n                                                                    k>1\\r\\n\\r\\n                  (N ),0                                                                                         (N )\\r\\nThe terms g(\\xce\\x93k,cc\\xcc\\x84g )abc\\r\\n                      \\xc2\\xb5 are ghost-antighost-gluon vertices generated by each operator Ok ,\\r\\nwith k > 1. Notably, there is no such counterterm for N = 2, as it can be seen by inspecting\\r\\n  (2)                                         (4)\\r\\nO2 in eq. (5.3). For N = 4, the operator O2 , given in eq. (5.13), generates both the ghost-\\r\\nantighost vertex and the ghost-antighost-gluon vertex. Therefore, the same counterterm\\r\\n   (4)\\r\\n\\xce\\xb4Z1 2 will suffice to renormalise both eqs. (7.7) and (7.15). For consistency, we verified that\\r\\n   (4)\\r\\n\\xce\\xb4Z1 2 extracted from eq. (7.15) agrees with the result in eq. (7.11). For N = 6 we find\\r\\n          \\x14\\x10         \\x11abc \\x15         p \\x14 (6) \\x10 (6),0 \\x11abc             \\x10      \\x11 \\x15\\r\\n                (6)                                              (6)   (6),0 abc\\r\\n       Z \\xce\\xb4\\xce\\x931;cc\\xcc\\x84g           = gZg Zc Z3 \\xce\\xb4Z1 2 \\xce\\x932,cc\\xcc\\x84g       + \\xce\\xb4Z1 3 \\xce\\x933,cc\\xcc\\x84g      ,       (7.16)\\r\\n                           \\xc2\\xb5                                              \\xc2\\xb5                       \\xc2\\xb5\\r\\n\\r\\n                                     (6)\\r\\nwhich can be solved for \\xce\\xb4Z1 3 , upon computing the left hand-side, by means of the R\\xe2\\x88\\x97\\r\\n                                              (6)\\r\\noperation, and by replacing the result for \\xce\\xb4Z1 2 , given in eq. (7.12). We get\\r\\n                                     \\x14                                 \\x15\\r\\n        (6)     \\xce\\xb1s CA \\x10 \\xce\\xb1s \\x112 2          2021        235813\\r\\n      \\xce\\xb4Z1 3 = \\xe2\\x88\\x92        \\xe2\\x88\\x92         CA             +             + O(\\xce\\xbeF ) + O(\\xce\\xb13s ), (7.17)\\r\\n                4\\xcf\\x80 48\\xc7\\xab     4\\xcf\\x80          40320\\xc7\\xab2      8467200\\xc7\\xab\\r\\nwhere we performed the calculation in Feynman gauge, dropping terms proportional to \\xce\\xbeF .\\r\\n                                                                                                                   (4)\\r\\n    Finally, we determine the remaining elements of the mixing matrices for operators O1\\r\\n      (6)\\r\\nand O1 , by computing the counterterms of the four-point functions\\r\\n                                        Z\\r\\n     (N )\\r\\n   (\\xce\\x93i;ccgg )abcd\\r\\n             \\xc2\\xb5\\xce\\xbd   (g, \\xce\\xbe, p  , p\\r\\n                           1 2 3, p ) =    dd x1 dd x2 dd x3 dd x4 eip1 \\xc2\\xb7(x1 \\xe2\\x88\\x92x4 ) eip2 \\xc2\\xb7(x2 \\xe2\\x88\\x92x4 ) eip3 \\xc2\\xb7(x3 \\xe2\\x88\\x92x4 )\\r\\n                                                h                                               i\\r\\n                                                                                         (N )\\r\\n                                        \\xc3\\x97 h0|T ca (x1 )cb (x2 )Ac\\xc2\\xb5 (x3 )Ad\\xce\\xbd (x4 )Oi (0) |0i,                   (7.18)\\r\\n                           h\\x10         \\x11                     X\\xe2\\x88\\x9e \\x10         \\x11                       \\x10 \\xce\\xb1 \\x11r i\\r\\n                              (N ),0 abcd                        (N ),r abcd                        s\\r\\n                    \\xe2\\x89\\xa1 g2     \\xce\\x93i;cc\\xcc\\x84gg     (p1 , p2 , p3 ) +     \\xce\\x93i;cc\\xcc\\x84gg      (\\xce\\xbe, p1 , p2 , p3 )          .\\r\\n                                       \\xc2\\xb5\\xce\\xbd                                  \\xc2\\xb5\\xce\\xbd                      4\\xcf\\x80\\r\\n                                                            r=1\\r\\n                                                            |                  {z                     }\\r\\n                                                                                  (N)\\r\\n                                                                               (\\xce\\xb4\\xce\\x93i;cc\\xcc\\x84gg )abcd\\r\\n                                                                                           \\xc2\\xb5\\xce\\xbd\\r\\n\\r\\n                                                                                                              (7.19)\\r\\n\\r\\nThe counterterms of eq. (7.18) are given by\\r\\n                  \\x14\\x10         \\x11abcd \\x15                X (N ) \\x10 (N ),0 \\x11abcd\\r\\n                       (N )\\r\\n               Z \\xce\\xb4\\xce\\x931;cc\\xcc\\x84gg           = g2 Zg2 Zc Z3  \\xce\\xb4Z1 k \\xce\\x93k,cc\\xcc\\x84gg       .                                   (7.20)\\r\\n                                           \\xc2\\xb5\\xce\\xbd                                                \\xc2\\xb5\\xce\\xbd\\r\\n                                                                    k>1\\r\\n\\r\\nBy specialising the equation above to N = 4, we find that it receives only one contribution\\r\\n                                                (4)\\r\\nfrom the vertex associated to the operator O3 , written in eq. (5.14). We get\\r\\n                      \\x14\\x10        \\x11abcd \\x15                       \\x10        \\x11abcd\\r\\n                           (4)                            (4)   (4),0\\r\\n                    Z \\xce\\xb4\\xce\\x931;cc\\xcc\\x84gg         = g 2 Zg2 Zc Z3 \\xce\\xb4Z1 3 \\xce\\x933,cc\\xcc\\x84gg       ,        (7.21)\\r\\n                                                \\xc2\\xb5\\xce\\xbd                                      \\xc2\\xb5\\xce\\xbd\\r\\n\\r\\nwhich leads to\\r\\n                                                     (4)   \\xce\\xb1s CA\\r\\n                                                \\xce\\xb4Z1 3 =           + O(\\xce\\xb12s ).                                  (7.22)\\r\\n                                                           4\\xcf\\x80 24\\xc7\\xab\\r\\n\\r\\n\\r\\n\\r\\n                                                           \\xe2\\x80\\x93 34 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                                   (6)\\r\\nFor N = 6, all operators Oi>1 contribute to eq. (7.20). By plugging the known results for\\r\\n   (6)             (6)\\r\\n\\xce\\xb4Z1 2 and \\xce\\xb4Z1 3 , given in eqs. (7.12) and (7.17) respectively, into eq. (7.20), we get\\r\\n\\r\\n         (6)        \\xce\\xb1s CA         \\x01                (6)      \\xce\\xb1s CA         \\x01    (6)        \\x01\\r\\n       \\xce\\xb4Z1 4 = \\xe2\\x88\\x92           + O \\xce\\xb12s ,            \\xce\\xb4Z1 5 =            + O \\xce\\xb12s , \\xce\\xb4Z1 6 = O \\xce\\xb12s .                        (7.23)\\r\\n                    4\\xcf\\x80 32\\xc7\\xab                                  4\\xcf\\x80 24\\xc7\\xab\\r\\n                                                                  (6)\\r\\nThe terms of O(\\xce\\xb12s ) contribute to renormalise O1 only at four loops, because they arise\\r\\nfrom divergent four-point subdiagrams at two loops, such as the one depicted in the botton\\r\\nright entry of table 6. In this work we renormalise the gauge invariant operator of spin\\r\\nN = 6 up to three loops and therefore we don\\xe2\\x80\\x99t need to compute such contributions.\\r\\n    Eqs. (7.10)-(7.12), (7.17), (7.22) and (7.23) include all off-diagonal terms of the mixing\\r\\n           (N )                                                                         (N )\\r\\nmatrix \\xce\\xb4Z1 i , which are required to renormalise the gauge invariant operators O1 at\\r\\nN = 2 and 4 up to 4 loops and N = 6 up to three loops. The calculation of the physical\\r\\n                (N )\\r\\ncontribution Z1 1 is described in the remaining part of this section.\\r\\n\\r\\n7.2      Renormalisation of physical operators\\r\\n                                            (N )\\r\\nThe renormalisation constants Z1 1 , which determine the anomalous dimension of the\\r\\ngauge invariant operator via eq. (7.3), are best extracted from correlators of the background\\r\\nfield B. Using the definition in eq. (6.14) and the definition of the gauge invariant operators\\r\\nin eq. (2.18) we have\\r\\n                \\x10       \\x11ab              \\x10        \\x11ab\\r\\n                   (N )                      (N )\\r\\n                  \\xce\\x931;BB       (g, \\xce\\xbe, p) \\xe2\\x89\\xa1 \\xce\\x931;BB              (g, \\xce\\xbe, p)\\xe2\\x88\\x86\\xc2\\xb51 ..\\xe2\\x88\\x86\\xc2\\xb5N .        (7.24)\\r\\n                              \\xce\\xbd1 \\xce\\xbd2                            \\xce\\xbd1 \\xce\\xbd2 ;\\xc2\\xb51 ..\\xc2\\xb5N\\r\\n\\r\\nThe renormalisation of eq. (7.24) requires a single counterterm\\r\\n                          \\x14\\x10       \\x11ab \\x15               \\x10       \\x11\\r\\n                              (N )                (N )   (N ),0 ab\\r\\n                       Z \\xce\\x931;BB            = ZB Z1 1 \\xce\\x931;BB          ,                                                (7.25)\\r\\n                                              \\xce\\xbd1 \\xce\\xbd2                               \\xce\\xbd1 \\xce\\xbd2\\r\\n\\r\\n            (N )\\r\\nwhere (\\xce\\x931;BB )ab\\r\\n               \\xce\\xbd1 \\xce\\xbd2 is the tree-level contribution to eq. (7.24). In practice, applying the R\\r\\n                                                                                               \\xe2\\x88\\x97\\r\\n\\r\\noperation becomes computationally challenging at higher loop orders or higher N -values .     4\\r\\n\\r\\nInstead, we renormalise the bare Green\\xe2\\x80\\x99s functions, which are defined by using bare fields\\r\\n             (N ),b\\r\\n(including Oi       ) and bare parameters in eq. (6.14). We compute the scalar quantities\\r\\n\\r\\n                                   \\xce\\xb4a1 a2 g\\xce\\xbd1 \\xce\\xbd2 HN1 N (p) \\x10 (N ) \\x11a1 a2\\r\\n                                                   \\xc2\\xb5 ..\\xc2\\xb5\\r\\n          (N )\\r\\n         \\xce\\x93i;BB (gB , \\xce\\xbeB , p2 ) =                            \\xce\\x93i;BB                 (gB , \\xce\\xbeB , p),                    (7.26)\\r\\n                                    NA          (d \\xe2\\x88\\x92 1)            \\xce\\xbd1 \\xce\\xbd2 ;\\xc2\\xb51 ..\\xc2\\xb5N\\r\\n\\r\\nwhere d = 4 \\xe2\\x88\\x92 2\\xc7\\xab is the dimension of spacetime and NA the dimension of the adjoint\\r\\n                                      \\xc2\\xb51 ..\\xc2\\xb5N\\r\\nrepresentation of the gauge group. HN         (p) are the harmonic tensors introduced in refs.\\r\\n[20, 71, 72], which project the Green\\xe2\\x80\\x99s function on its symmetric and traceless component.\\r\\nThe harmonic projectors are defined to satisfy\\r\\n           \\xc2\\xb51 ..\\xc2\\xb5N                                       \\xc2\\xb5 ..\\xc2\\xb5i ..\\xc2\\xb5j ..\\xc2\\xb5N          \\xc2\\xb5 ..\\xc2\\xb5j ..\\xc2\\xb5i ..\\xc2\\xb5N\\r\\n          HN       (p) g\\xc2\\xb5i \\xc2\\xb5j = 0         and         HN1                   (p) = HN1                 (p)   \\xe2\\x88\\x80i, j\\r\\n             \\xc2\\xb51 ..\\xc2\\xb5N           \\xc2\\xb5 ..\\xc2\\xb5\\r\\n            HN       (p)p\\xc2\\xb5N = HN1\\xe2\\x88\\x921 N\\xe2\\x88\\x921 (p) p2 ,                                                                    (7.27)\\r\\n   4\\r\\n    The mass dimension of the operator increases with the spin, as d = N + 2, and therefore also the degree\\r\\nof divergence of the Feynman diagrams of eq. (7.24). This requires to compute high order terms in the\\r\\nTaylor expansion of the diagrams, see eq. (7.8), which can generate large numbers of terms.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         \\xe2\\x80\\x93 35 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\nand they are explicitly constructed in [20]. We generated all the Feynman diagrams that\\r\\ncontribute to eq. (7.26) with QGRAF [73], we performed the color and Lorentz algebra with\\r\\ninhouse code, which is written in FORM [61] and makes use of the package COLOR [60]. All\\r\\nthe Feynman integrals that contribute to eq. (7.26) are massless two-point functions, also\\r\\ncalled p-integrals [74\\xe2\\x80\\x9377], which we computed with the code Forcer [78]. In order to\\r\\nrenormalise eq. (7.26), we separate the tree-level from loop contributions\\r\\n                                     \\xef\\xa3\\xb1 (N ),0       (N )\\r\\n                                     \\xef\\xa3\\xb4\\r\\n                                     \\xef\\xa3\\xb2 \\xce\\x93i;BB + \\xce\\xb4\\xce\\x93i;BB (gB , \\xce\\xbeB ), for i = 1\\r\\n                   (N )\\r\\n                  \\xce\\x93i;BB (gB , \\xce\\xbeB ) =                                               (7.28)\\r\\n                                     \\xef\\xa3\\xb4\\r\\n                                     \\xef\\xa3\\xb3 (N )\\r\\n                                       \\xce\\xb4\\xce\\x93i;BB (gB , \\xce\\xbeB ),         for i 6= 1.\\r\\n\\r\\nwhere we omit the dependence on p2 , which can be reconstructed via dimensional analysis,\\r\\nand with\\r\\n                                          \\xe2\\x88\\x9e\\r\\n                                          X             \\x10 \\xce\\xb1 \\x11r\\r\\n                         (N )                 (N ),r        s,B\\r\\n                      \\xce\\xb4\\xce\\x93i;BB (gB , \\xce\\xbeB ) =   \\xce\\x93i;BB (\\xce\\xbeB )         ,                 (7.29)\\r\\n                                                           4\\xcf\\x80\\r\\n                                               r=1\\r\\n                2 /(4\\xcf\\x80). Upon considering \\xce\\x93       (N )\\r\\nwhere \\xce\\xb1s,B = gB                             1;BB (gB , \\xce\\xbeB ) in eq. (7.26), we find the renor-\\r\\nmalised correlator to obey\\r\\n                  \"                                                       #\\r\\n                      h                     X                           i\\r\\n                         (N ) (N )                 (N )    (N )\\r\\n              K\\xc7\\xab ZB Z1 1 \\xce\\x931;BB (gB , \\xce\\xbeB ) +     \\xce\\xb4Z1 i \\xce\\xb4\\xce\\x93i;BB (gB , \\xce\\xbeB ) = 0,           (7.30)\\r\\n                                                  i>1\\r\\n\\r\\nwhere ZB = Z12 is the renormalisation constant of the background field [52, 53]. Eq. (7.30)\\r\\n              g\\r\\n                                                                   (N )\\r\\ncan be solved in terms of the renormalisation constant Z1 1 of the gauge invariant operator.\\r\\nUsing identities eqs. (7.1) and (7.28), we then get\\r\\n                                           \\xef\\xa3\\xae                              \\xef\\xa3\\xb9\\r\\n                     (N ) (N ),0     1          X    (N )  (N )\\r\\n                   \\xce\\xb4Z1 1 \\xce\\x931;BB = \\xe2\\x88\\x92      K\\xc7\\xab \\xef\\xa3\\xb0ZB      Z1 i \\xce\\xb4\\xce\\x93i;BB (gB , \\xce\\xbeB )\\xef\\xa3\\xbb .         (7.31)\\r\\n                                    ZB\\r\\n                                                     i\\xe2\\x89\\xa51\\r\\n\\r\\n                                                                                                    (N )\\r\\nThe equation above holds to all loop orders. The renormalisation constants Z1 i>1 , on the\\r\\n                                                                                                    (N )\\r\\nright hand-side of eq. (7.31), are required to renormalise sub-divergences of \\xce\\x931;BB , which\\r\\ninvolve quantum gluons and/or a ghost-antighost pair. Each sub-divergence is proportional\\r\\n                                      (N )\\r\\nto one of the unphysical operators Oi>1 . This determines the maximal loop order at which\\r\\n (N )\\r\\nZ1 i has been computed, as shown in table 7. The diagonal renormalisation constant,\\r\\n   (N )                                                                       (N )\\r\\n\\xce\\xb4Z1 1 , appears on both sides of eq. (7.31). However, we notice that the Z1 1 appearing\\r\\n                                          (N )\\r\\non the right hand-side is multiplied by \\xce\\xb4\\xce\\x931;BB (gB , \\xce\\xbeB ), which starts at O(\\xce\\xb1s ). Therefore,\\r\\n                                  (N )                                                      (N )\\r\\neq. (7.31) allows us to compute Z1 1 at L-loops, given the knowledge of Z1 i at l < L loops\\r\\n                                                       (N )\\r\\nas discussed before. We plug the l-loop values of Z1 i , given in eqs. (7.10)-(7.12), (7.17),\\r\\n(7.22) and (7.23) respectively, into eq. (7.31). After computing the relevant correlators\\r\\n   (N )\\r\\n\\xce\\xb4\\xce\\x93i;BB (gB , \\xce\\xbeB ) at the required L \\xe2\\x88\\x92 l loop order, we find\\r\\n  (2)            \\x01\\r\\n Z1 1 = 1 + O \\xce\\xb15s ,                                                                                        (7.32)\\r\\n                         \\x10 \\xce\\xb1 \\x112          \\x12                 \\x13       \\x10 \\xce\\xb1 \\x113          \\x12\\r\\n   (4)        \\xce\\xb1s 21CA    s         2          28   7121                   s    3            1316\\r\\n Z1 1 = 1 +           +           CA             +             +              CA       \\xe2\\x88\\x92\\r\\n              4\\xcf\\x80 5\\xc7\\xab     4\\xcf\\x80                   25\\xc7\\xab2 1000\\xc7\\xab              4\\xcf\\x80                    1125\\xc7\\xab3\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                \\xe2\\x80\\x93 36 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n                                  \\x13 \\x10 \\x11 \\x1a        \\x14\\r\\n             151441     103309639      \\xce\\xb1s 4    4   11186    1512989    5437269017\\r\\n           \\xe2\\x88\\x92          +             +         CA          +          \\xe2\\x88\\x92\\r\\n             45000\\xc7\\xab2     4050000\\xc7\\xab      4\\xcf\\x80          5625\\xc7\\xab4   450000\\xc7\\xab3   162000000\\xc7\\xab2\\r\\n                                                 \\x15        \\x12                          \\x13\\x1b\\r\\n             1 \\x10 1502628149 1146397\\xce\\xb63     126\\xce\\xb65 \\x11     dAA 21623 3899 \\xce\\xb63      1512 \\xce\\xb65\\r\\n           +                +           \\xe2\\x88\\x92          +              +        \\xe2\\x88\\x92\\r\\n             \\xc7\\xab 13500000          45000      5         NA     600\\xc7\\xab     15\\xc7\\xab      5\\xc7\\xab\\r\\n           + O(\\xce\\xb15s ),                                                                             (7.33)\\r\\n                            \\x10 \\xce\\xb1 \\x112     \\x12                     \\x13       \\x10 \\xce\\xb1 \\x113        \\x12\\r\\n     (6)         \\xce\\xb1s 83 CA       s    2    7885     1506899             s       3            465215\\r\\n    Z1 1 = 1 +             +       CA          2\\r\\n                                                 +               +            CA       \\xe2\\x88\\x92\\r\\n                 4\\xcf\\x80 14\\xc7\\xab        4\\xcf\\x80        1176\\xc7\\xab     148176\\xc7\\xab            4\\xcf\\x80                   148176\\xc7\\xab3\\r\\n                                       \\x13\\r\\n             243375989     96390174479           \\x01\\r\\n           +           2\\r\\n                         +               + O \\xce\\xb14s ,                                                (7.34)\\r\\n             18670176\\xc7\\xab     2613824640\\xc7\\xab\\r\\n\\r\\nwhere dAA = dabcd4  dabcd\\r\\n                     4    , with dabcd\\r\\n                                  4    defined in eq. (3.21). As a check on our calculation, we\\r\\nverified that all non-local divergences of the form 1/\\xc7\\xabp logq (\\xc2\\xb52 /p2 ), which appear in the bare\\r\\ncorrelators, cancel upon combining the required counterterms. Furthermore, we verified\\r\\nthat the dependence on the gauge parameter \\xce\\xbe cancels up to three loops in eqs. (7.32)\\r\\nand (7.33). The O(\\xce\\xb14s )-terms in those equations were computed only in Feynman gauge.\\r\\nSimilarly, the calculation of the O(\\xce\\xb13s )-terms in eq. (7.34) was performed in Feynman\\r\\n                                                                            (2)\\r\\ngauge and the cancellation of \\xce\\xbe was verified to two loops. The result Z1 1 = 1, in eq. (7.32),\\r\\n                                                                  (2)\\r\\nagrees with the findings of refs. [79, 80], which imply that O1 does not renormalise to all\\r\\norders. Finally, by extracting the anomalous dimension \\xce\\xb3 (N ) , as written in eq. (7.3), we\\r\\nfind agreement with the results at three and at four loops given in refs. [20] and [34].\\r\\n\\r\\n8     Conclusions\\r\\n\\r\\nIn this paper we generalised a method, originally by Dixon and Taylor [37], for the con-\\r\\nstruction of unphysical operators which are required for the renormalisation of Green\\xe2\\x80\\x99s\\r\\nfunctions with insertions of twist-two gluonic operators. As one increases the loop order\\r\\nof the Green\\xe2\\x80\\x99s function more unphysical operators are in general required for its renormal-\\r\\nisation. The previously known basis was restricted to two-loop calculations, and it was\\r\\nunclear how to systematically extend it to higher loop order, thereby preventing the OPE\\r\\nmethod to be used for calculations of the singlet splitting functions. We have uncovered\\r\\na general and systematic formalism for extending the basis to arbitrary loop order. Using\\r\\nthis formalism we then worked out the explicit basis for calculations up to four-loop order\\r\\nand used it to perform calculations of the N = 2, 4 Mellin moments at four loops and the\\r\\nN = 6 Mellin moment at three loops, obtaining the correct known results.\\r\\n     The formalism we developed can essentially be broken down to a few key concepts.\\r\\nThe first is that we identified the gluonic gauge-variant operators in the Dixon-Taylor\\r\\nbasis with EOM operators, these are not EOM operators of the gauge-fixed Lagrangian,\\r\\nbut EOM operators of the gauge invariant part of the Lagrangian. With this identification\\r\\nwe could easily write down the all-loop structure of the EOM operator. The second concept\\r\\nis that of a generalised gauge transformation which leaves invariant the Lagrangian made\\r\\nup of the gauge invariant and EOM operators. Following the works of Hamberg and Van\\r\\nNeerven [13] and Joglekar and Lee [42] this generalised gauge invariance is promoted to a\\r\\ngeneralised BRST symmetry. We then propose that the most general ghost operator can\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             \\xe2\\x80\\x93 37 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\nbe written as the generalised BRST action acting on a single BRST ancestor operator. The\\r\\nghost operator is therefore identified as an BRST-exact operator in the BRST generalised\\r\\nsense. This proposition not only reproduces the previously known ghost operators required\\r\\nat two loops, but we also confirmed that it complies with the theorems of Joglekar and\\r\\nLee [42]. Indeed we show that the operators generated with our procedure can be always\\r\\nwritten as a sum of a BRST-exact term (in the sense of the original, not generalised, BRST\\r\\ntransformations) and a term that vanishes on the equation of motion of the complete Yang-\\r\\nMills lagrangian, as required by [42].\\r\\n      We explored two further symmetry principles to simplify calculations of unphysical\\r\\ncounterterms. The first is the anti-BRST symmetry which can be used to derive a ghost\\r\\nanti-ghost exchange symmetry of the ghost operators. This symmetry allows one to drasti-\\r\\ncally reduce the number of independent unphysical operators. Another symmetry principle\\r\\nis background field gauge invariance, which we employed in our calculations. Background\\r\\nfield invariance allows to do calculations without unphysical operators at the one-loop level,\\r\\nbeyond one-loop counterterms a number of unphysical operators are however still required\\r\\nto perform calculations.\\r\\n      The task of computing unphysical counterterms requires the extraction of local renor-\\r\\nmalisation counterterms of Green\\xe2\\x80\\x99s functions containing a ghost anti-ghost pair and mul-\\r\\ntiple gluons. For instance to determine the anomalous dimension of the gauge invariant\\r\\noperator at the four-loop level generally requires, among others, the counterterms associ-\\r\\nated to Green\\xe2\\x80\\x99s functions containing a ghost anti-ghost pair with two gluons at two loops\\r\\nand with one gluon at three loops. These quantities can thus not be extracted through\\r\\na naive calculation of a self energy diagram. In this work we employed a fully auto-\\r\\nmated implementation of the local R\\xe2\\x88\\x97 -operation, an operation which allows to extract the\\r\\ncounterterms of Greens\\xe2\\x80\\x99s functions of arbitrary many external particles from self energy\\r\\ndiagrams, via the technique of IR rearrangement and IR subtractions. However the R\\xe2\\x88\\x97 -\\r\\noperation becomes very expensive for higher moments, due to the many derivatives and\\r\\nmany counterterms one requires. Already at N = 6 we found that the calculations were\\r\\nbecoming prohibitively time-consuming even with substantial computing resources. It may\\r\\nbe possible with further optimisation to push the R\\xe2\\x88\\x97 -approach to higher N , however we\\r\\nbelieve that a more streamlined approach could be more promising. We leave further\\r\\nimprovements of this task to the future.\\r\\n      Assuming that the problem of calculating these UV counterterms can be solved ef-\\r\\nficiently one can expect that the methods presented here should allow for a much more\\r\\nefficient approach to computing Mellin moments of gluonic splitting functions at N3LO than\\r\\nthe brute force approach which was currently used [34]. To extend the methods presented\\r\\nhere to singlet splitting functions containing also quarks will require further extensions of\\r\\nthe formalism. We do not believe these to give major complications.\\r\\n\\r\\nAcknowledgements\\r\\n\\r\\nWe would like to thank Sven Moch, Jos Vermaseren and Andreas Vogt for many insightful\\r\\ndiscussions and their continuous encouragement. G.F. would like to thank Arnd Behring\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                           \\xe2\\x80\\x93 38 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\nand Mattia Dalla Brida for numerous discussions on related topics. F.H. is supported by\\r\\nthe NWO Vidi grant 680-47-551 and the UKRI FLF Mr/S03479x/1. G.F. is supported\\r\\nby the ERC Starting Grant 715049 \\xe2\\x80\\x98QCDforfuture\\xe2\\x80\\x99 with Principal Investigator Jennifer\\r\\nSmillie and by the STFC Consolidated Grant \\xe2\\x80\\x98Particle Physics at the Higgs Centre\\xe2\\x80\\x99.\\r\\n\\r\\nA    Computing anomalous dimension in QCD\\r\\n\\r\\nIt is convenient to spell out also a procedure to compute anomalous dimensions which does\\r\\nnot rely on the background field method, but involves instead only the calculation of bare\\r\\nGreen\\xe2\\x80\\x99s functions with external gluons or ghosts. These were defined in eqs. (3.12) and\\r\\n(7.4), respectively, and they read\\r\\n                          \\x10       \\x11                 \\x10       \\x11\\r\\n                            (N ) a1 a2                (N ) a1 a2\\r\\n                           \\xce\\x93i;gg         \\xe2\\x89\\xa1 \\xe2\\x88\\x86\\xc2\\xb51 ..\\xe2\\x88\\x86\\xc2\\xb5N \\xce\\x93i;gg                  ,        (A.1)\\r\\n                                   \\xce\\xbd1 \\xce\\xbd2                     \\xce\\xbd1 \\xce\\xbd2 ;\\xc2\\xb51 ..\\xc2\\xb5N\\r\\n                          \\x10       \\x11                 \\x10       \\x11\\r\\n                            (N ) a1 a2                (N ) a1 a2\\r\\n                           \\xce\\x93i;cc\\xcc\\x84        \\xe2\\x89\\xa1 \\xe2\\x88\\x86\\xc2\\xb51 ..\\xe2\\x88\\x86\\xc2\\xb5N \\xce\\x93i;cc\\xcc\\x84            .             (A.2)\\r\\n                                                                        \\xc2\\xb51 ..\\xc2\\xb5N\\r\\n\\r\\nWe compute these correlators with the help of FORCER, after applying harmonic and colour\\r\\nprojectors to reduce eqs. (A.1) and (A.2), as described below eq. (7.26)\\r\\n\\r\\n                 (N )                    \\xce\\xb4a1 a2 g\\xce\\xbd1 \\xce\\xbd2 H \\xc2\\xb51 ..\\xc2\\xb5N (p) \\x10 (N ) \\x11a1 a2\\r\\n                \\xce\\x93i;gg (gB , \\xce\\xbeB , p2 ) =                                 \\xce\\x93i;gg                  ,         (A.3)\\r\\n                                          NA          (d \\xe2\\x88\\x92 1)                   \\xce\\xbd1 \\xce\\xbd2 ;\\xc2\\xb51 ..\\xc2\\xb5N\\r\\n                                         \\xce\\xb4a1 a2 \\xc2\\xb51 ..\\xc2\\xb5N        \\x10       \\x11\\r\\n                 (N )                                              (N ) a1 a2\\r\\n                \\xce\\x93i;cc\\xcc\\x84 (gB , \\xce\\xbeB , p2 ) =        H        (p) \\xce\\x93i;cc\\xcc\\x84              .                       (A.4)\\r\\n                                          NA                            \\xc2\\xb51 ..\\xc2\\xb5N\\r\\n\\r\\nBy definition, the ghost correlator \\xce\\x93i;cc\\xcc\\x84 doesn\\xe2\\x80\\x99t vanish at tree level, only if we consider\\r\\n                           (N )                                           (N ),1\\r\\ninsertion of the operator O2 , which is chosen to contain the term OG            eq. (4.22), as\\r\\nwe have done in the construction of operator bases for N = 2, 4 and 6 in eqs. (5.3), (5.13)\\r\\n                                    (N )\\r\\nand (5.30). The gluon correlator \\xce\\x93i;gg receives contributions at tree level from both the\\r\\n                               (N )                   (N )                                     (N ),1\\r\\ngauge invariant operator O1 and from O2 , which includes the term OEOM , eq. (3.13),\\r\\n              (N ),1\\r\\nrelated to OG        by (generalised) BRST symmetry. We get\\r\\n                                   \\xef\\xa3\\xb1 (N ),0              (N )\\r\\n                                   \\xef\\xa3\\xb4           2                       2\\r\\n                                   \\xef\\xa3\\xb2 \\xce\\x93i;gg (p ) + \\xce\\xb4\\xce\\x93i;gg (gB , \\xce\\xbeB , p ) for i = 1, 2\\r\\n           (N )\\r\\n          \\xce\\x93i;gg (gB , \\xce\\xbeB , p2 ) =                                                     (A.5a)\\r\\n                                   \\xef\\xa3\\xb4\\r\\n                                   \\xef\\xa3\\xb3 (N )                2\\r\\n                                     \\xce\\xb4\\xce\\x93i;gg (gB , \\xce\\xbeB , p )                  for i > 2\\r\\n                                   \\xef\\xa3\\xb1 (N ),0              (N )\\r\\n                                   \\xef\\xa3\\xb4           2                       2\\r\\n                                   \\xef\\xa3\\xb2 \\xce\\x93i;cc\\xcc\\x84 (p ) + \\xce\\xb4\\xce\\x93i;cc\\xcc\\x84 (gB , \\xce\\xbeB , p ) for i = 2\\r\\n           (N )\\r\\n          \\xce\\x93i;cc\\xcc\\x84 (gB , \\xce\\xbeB , p2 ) =                                                    (A.5b)\\r\\n                                   \\xef\\xa3\\xb4\\r\\n                                   \\xef\\xa3\\xb3 (N )                2\\r\\n                                     \\xce\\xb4\\xce\\x93i;cc\\xcc\\x84 (gB , \\xce\\xbeB , p )               for i 6= 2\\r\\n                                                                 (N )\\r\\nIn order to compute the renormalisation constant Z1 1 , we renormalise the bare correlators\\r\\n (N )       (N )                                                        (N ),b\\r\\n\\xce\\x931;gg and \\xce\\x931;cc\\xcc\\x84 , where we inserted the gauge invariant operator O1\\r\\n                                h X                               i\\r\\n                                              (N )\\r\\n                             K\\xc7\\xab Z3      Z1 i \\xce\\x93i;gg (gB , \\xce\\xbeB , p2 ) = 0,             (A.6a)\\r\\n                                          i\\xe2\\x89\\xa51\\r\\n                                   h      X                                i\\r\\n                                                      (N )\\r\\n                               K\\xc7\\xab Zc            Z1 i \\xce\\x93i;cc\\xcc\\x84 (gB , \\xce\\xbeB , p2 ) = 0.                        (A.6b)\\r\\n                                          i\\xe2\\x89\\xa51\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                     \\xe2\\x80\\x93 39 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\nWe separate the contributions of the tree-level terms and of the loop corrections to the\\r\\nGreen\\xe2\\x80\\x99s functions, according to eqs. (A.5a) and (A.5b) and we solve the equations above\\r\\n     (N )      (N )\\r\\nfor Z1 1 and Z1 2 . We find\\r\\n\\r\\n                                            1    h X                                 i\\r\\n                   (N ),0          (N ),0                       (N )\\r\\n             Z1 1 \\xce\\x931;gg + \\xce\\xb4Z1 2 \\xce\\x932;gg = \\xe2\\x88\\x92      K\\xc7\\xab Z3     Z1 i \\xce\\xb4\\xce\\x93i;gg (gB , \\xce\\xbeB , p2 ) ,      (A.7)\\r\\n                                            Z3\\r\\n                                                     i\\xe2\\x89\\xa51\\r\\n                                            1    h   X                               i\\r\\n                                   (N ),0                       (N )\\r\\n                            \\xce\\xb4Z1 2 \\xce\\x932;cc\\xcc\\x84 = \\xe2\\x88\\x92 K\\xc7\\xab Zc       Z1 i \\xce\\xb4\\xce\\x93i;cc\\xcc\\x84 (gB , \\xce\\xbeB , p2 ) .     (A.8)\\r\\n                                            Zc\\r\\n                                                         i\\xe2\\x89\\xa51\\r\\n\\r\\nWe solve the equations above order-by-order is \\xce\\xb1s . Provided we have knowledge of the\\r\\n                            (N )\\r\\nrenormalisation constants Z1 i up to L \\xe2\\x88\\x92 1 loops, which enter the right hand-side of both\\r\\n                                       (N )\\r\\neqs. (A.7) and (A.8), we determine \\xce\\xb4Z1 2 to L loops by means of eq. (A.8). We applied\\r\\n                          (N )\\r\\nthis method to compute Z1 2 in eqs.(7.10)-(7.12) with complete dependence on the gauge\\r\\n                                      (N )\\r\\nparameter \\xce\\xbe. Finally, by replacing \\xce\\xb4Z1 2 at L-loop in the left hand-side of eq. (A.7), we\\r\\n                                 (N )\\r\\ndetermine the renormalisation Z1 1 to L loops.\\r\\n\\r\\nReferences\\r\\n [1] C. Anastasiou, C. Duhr, F. Dulat, F. Herzog and B. Mistlberger, Higgs Boson Gluon-Fusion\\r\\n     Production in QCD at Three Loops, Phys. Rev. Lett. 114 (2015) 212001\\r\\n     [arXiv:1503.06056].\\r\\n [2] C. Anastasiou, C. Duhr, F. Dulat, E. Furlan, T. Gehrmann, F. Herzog et al., High precision\\r\\n     determination of the gluon fusion Higgs boson cross-section at the LHC,\\r\\n     JHEP 05 (2016) 058 [arXiv:1602.00695].\\r\\n [3] B. Mistlberger, Higgs boson production at hadron colliders at N3 LO in QCD,\\r\\n     JHEP 05 (2018) 028 [arXiv:1802.00833].\\r\\n [4] C. Duhr, F. Dulat and B. Mistlberger, Higgs Boson Production in Bottom-Quark Fusion to\\r\\n     Third Order in the Strong Coupling, Phys. Rev. Lett. 125 (2020) 051804\\r\\n     [arXiv:1904.09990].\\r\\n [5] C. Duhr, F. Dulat and B. Mistlberger, Drell-Yan Cross Section to Third Order in the Strong\\r\\n     Coupling Constant, Phys. Rev. Lett. 125 (2020) 172001 [arXiv:2001.07717].\\r\\n [6] X. Chen, T. Gehrmann, E.W.N. Glover, A. Huss, B. Mistlberger and A. Pelloni, Fully\\r\\n     Differential Higgs Boson Production to Third Order in QCD,\\r\\n     Phys. Rev. Lett. 127 (2021) 072002 [arXiv:2102.07607].\\r\\n [7] C. Duhr, F. Dulat and B. Mistlberger, Charged current Drell-Yan production at N3 LO,\\r\\n     JHEP 11 (2020) 143 [arXiv:2007.13313].\\r\\n [8] P. Bargiela, F. Caola, A. von Manteuffel and L. Tancredi, Three-loop helicity amplitudes for\\r\\n     diphoton production in gluon fusion, arXiv:2111.13595.\\r\\n [9] F. Caola, A. Chakraborty, G. Gambuti, A. von Manteuffel and L. Tancredi, Three-loop gluon\\r\\n     scattering in QCD and the gluon Regge trajectory, arXiv:2112.11097.\\r\\n[10] E.G. Floratos, D.A. Ross and C.T. Sachrajda, Higher Order Effects in Asymptotically Free\\r\\n     Gauge Theories. 2. Flavor Singlet Wilson Operators and Coefficient Functions,\\r\\n     Nucl. Phys. B 152 (1979) 493.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              \\xe2\\x80\\x93 40 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n[11] A. Gonzalez-Arroyo and C. Lopez, Second Order Contributions to the Structure Functions in\\r\\n     Deep Inelastic Scattering. 3. The Singlet Case, Nucl. Phys. B 166 (1980) 429.\\r\\n[12] W. Furmanski and R. Petronzio, Singlet Parton Densities Beyond Leading Order,\\r\\n     Phys. Lett. B 97 (1980) 437.\\r\\n[13] R. Hamberg and W.L. van Neerven, The Correct renormalization of the gluon operator in a\\r\\n     covariant gauge, Nucl. Phys. B 379 (1992) 143.\\r\\n[14] W. Vogelsang, A Rederivation of the spin dependent next-to-leading order splitting functions,\\r\\n     Phys. Rev. D 54 (1996) 2023 [hep-ph/9512218].\\r\\n[15] R. Mertig and W.L. van Neerven, The Calculation of the two loop spin splitting functions\\r\\n     P(ij)(1)(x), Z. Phys. C 70 (1996) 637 [hep-ph/9506451].\\r\\n[16] R.K. Ellis and W. Vogelsang, The Evolution of parton distributions beyond leading order:\\r\\n     The Singlet case, hep-ph/9602356.\\r\\n[17] Y. Matiounine, J. Smith and W.L. van Neerven, Two loop operator matrix elements\\r\\n     calculated up to finite terms, Phys. Rev. D 57 (1998) 6701 [hep-ph/9801224].\\r\\n[18] Y. Matiounine, J. Smith and W.L. van Neerven, Two loop operator matrix elements\\r\\n     calculated up to finite terms for polarized deep inelastic lepton - hadron scattering,\\r\\n     Phys. Rev. D 58 (1998) 076002 [hep-ph/9803439].\\r\\n[19] S.A. Larin, T. van Ritbergen and J.A.M. Vermaseren, The Next next-to-leading QCD\\r\\n     approximation for nonsinglet moments of deep inelastic structure functions,\\r\\n     Nucl. Phys. B 427 (1994) 41.\\r\\n[20] S.A. Larin, P. Nogueira, T. van Ritbergen and J.A.M. Vermaseren, The Three loop QCD\\r\\n     calculation of the moments of deep inelastic structure functions,\\r\\n     Nucl. Phys. B 492 (1997) 338 [hep-ph/9605317].\\r\\n[21] S. Moch, J.A.M. Vermaseren and A. Vogt, The Three loop splitting functions in QCD: The\\r\\n     Nonsinglet case, Nucl. Phys. B 688 (2004) 101 [hep-ph/0403192].\\r\\n[22] A. Vogt, S. Moch and J.A.M. Vermaseren, The Three-loop splitting functions in QCD: The\\r\\n     Singlet case, Nucl. Phys. B 691 (2004) 129 [hep-ph/0404111].\\r\\n[23] J. Ablinger, A. Behring, J. Blu\\xcc\\x88mlein, A. De Freitas, A. von Manteuffel and C. Schneider, The\\r\\n     3-loop pure singlet heavy flavor contributions to the structure function F2 (x, Q2 ) and the\\r\\n     anomalous dimension, Nucl. Phys. B 890 (2014) 48 [arXiv:1409.1135].\\r\\n[24] J. Ablinger, A. Behring, J. Blu\\xcc\\x88mlein, A. De Freitas, A. von Manteuffel and C. Schneider, The\\r\\n                                     (2)       (2,N )\\r\\n     three-loop splitting functions Pqg and Pgg F , Nucl. Phys. B 922 (2017) 1\\r\\n     [arXiv:1705.01508].\\r\\n[25] A. Behring, J. Blu\\xcc\\x88mlein, A. De Freitas, A. Goedicke, S. Klein, A. von Manteuffel et al., The\\r\\n     Polarized Three-Loop Anomalous Dimensions from On-Shell Massive Operator Matrix\\r\\n     Elements, Nucl. Phys. B 948 (2019) 114753 [arXiv:1908.03779].\\r\\n[26] J. Ablinger, A. Behring, J. Blu\\xcc\\x88mlein, A. De Freitas, A. von Manteuffel, C. Schneider et al.,\\r\\n     The three-loop single mass polarized pure singlet operator matrix element,\\r\\n     Nucl. Phys. B 953 (2020) 114945 [arXiv:1912.02536].\\r\\n[27] J. Blu\\xcc\\x88mlein, P. Marquard, C. Schneider and K. Scho\\xcc\\x88nwald, The three-loop unpolarized and\\r\\n     polarized non-singlet anomalous dimensions from off shell operator matrix elements,\\r\\n     Nucl. Phys. B 971 (2021) 115542 [arXiv:2107.06267].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                              \\xe2\\x80\\x93 41 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n[28] J. Blu\\xcc\\x88mlein, P. Marquard, C. Schneider and K. Scho\\xcc\\x88nwald, The three-loop polarized singlet\\r\\n     anomalous dimensions from off-shell operator matrix elements, JHEP 01 (2022) 193\\r\\n     [arXiv:2111.12401].\\r\\n[29] V.N. Velizhanin, Four-loop anomalous dimension of the third and fourth moments of the\\r\\n     nonsinglet twist-2 operator in QCD, Int. J. Mod. Phys. A 35 (2020) 2050199\\r\\n     [arXiv:1411.1331].\\r\\n[30] S. Moch, B. Ruijl, T. Ueda, J.A.M. Vermaseren and A. Vogt, Four-Loop Non-Singlet Splitting\\r\\n     Functions in the Planar Limit and Beyond, JHEP 10 (2017) 041 [arXiv:1707.08315].\\r\\n[31] J. Davies, A. Vogt, B. Ruijl, T. Ueda and J.A.M. Vermaseren, Large-nf contributions to the\\r\\n     four-loop splitting functions in QCD, Nucl. Phys. B 915 (2017) 335 [arXiv:1610.07477].\\r\\n[32] J.A. Gracey, Anomalous dimensions of operators in polarized deep inelastic scattering at\\r\\n     O(1/N(f )), Nucl. Phys. B 480 (1996) 73 [hep-ph/9609301].\\r\\n[33] F. Herzog, S. Moch, B. Ruijl, T. Ueda, J.A.M. Vermaseren and A. Vogt, Five-loop\\r\\n     contributions to low-N non-singlet anomalous dimensions in QCD,\\r\\n     Phys. Lett. B 790 (2019) 436 [arXiv:1812.11818].\\r\\n[34] S. Moch, B. Ruijl, T. Ueda, J.A.M. Vermaseren and A. Vogt, Low moments of the four-loop\\r\\n     splitting functions in QCD, Phys. Lett. B 825 (2022) 136853 [arXiv:2111.15561].\\r\\n[35] D.J. Gross and F. Wilczek, Asymptotically free gauge theories. 2., Phys. Rev. D 9 (1974) 980.\\r\\n[36] H. Georgi and H.D. Politzer, Electroproduction scaling in an asymptotically free theory of\\r\\n     strong interactions, Phys. Rev. D 9 (1974) 416.\\r\\n[37] J.A. Dixon and J.C. Taylor, Renormalization of wilson operators in gauge theories,\\r\\n     Nucl. Phys. B 78 (1974) 552.\\r\\n[38] J. Blu\\xcc\\x88mlein, P. Marquard, C. Schneider and K. Scho\\xcc\\x88nwald, The Two-Loop Massless Off-Shell\\r\\n     QCD Operator Matrix Elements to Finite Terms, arXiv:2202.03216.\\r\\n[39] J.C. Collins and R.J. Scalise, The Renormalization of composite operators in Yang-Mills\\r\\n     theories using general covariant gauge, Phys. Rev. D 50 (1994) 4117 [hep-ph/9403231].\\r\\n[40] H. Kluberg-Stern and J.B. Zuber, Renormalization of Nonabelian Gauge Theories in a\\r\\n     Background Field Gauge. 1. Green Functions, Phys. Rev. D 12 (1975) 482.\\r\\n[41] H. Kluberg-Stern and J.B. Zuber, Renormalization of Nonabelian Gauge Theories in a\\r\\n     Background Field Gauge. 2. Gauge Invariant Operators, Phys. Rev. D 12 (1975) 3159.\\r\\n[42] S.D. Joglekar and B.W. Lee, General Theory of Renormalization of Gauge Invariant\\r\\n     Operators, Annals Phys. 97 (1976) 160.\\r\\n[43] S.D. Joglekar, Local Operator Products in Gauge Theories. 1., Annals Phys. 108 (1977) 233.\\r\\n[44] S.D. Joglekar, Local Operator Products in Gauge Theories. 2., Annals Phys. 109 (1977) 210.\\r\\n[45] M. Henneaux, Remarks on the renormalization of gauge invariant operators in Yang-Mills\\r\\n     theory, Phys. Lett. B 313 (1993) 35 [hep-th/9306101].\\r\\n[46] G. Curci and R. Ferrari, On a Class of Lagrangian Models for Massive and Massless\\r\\n     Yang-Mills Fields, Nuovo Cim. A 32 (1976) 151.\\r\\n[47] I. Ojima, Another BRS Transformation, Prog. Theor. Phys. 64 (1980) 625.\\r\\n[48] L. Baulieu and J. Thierry-Mieg, The Principle of BRS Symmetry: An Alternative Approach\\r\\n     to Yang-Mills Theories, Nucl. Phys. B 197 (1982) 477.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                             \\xe2\\x80\\x93 42 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n[49] D. Binosi and A. Quadri, Anti-BRST symmetry and background field method,\\r\\n     Phys. Rev. D 88 (2013) 085036 [arXiv:1309.1021].\\r\\n[50] B.S. DeWitt, Quantum Theory of Gravity. 2. The Manifestly Covariant Theory,\\r\\n     Phys. Rev. 162 (1967) 1195.\\r\\n[51] G. \\xe2\\x80\\x99t Hooft, The Background Field Method in Gauge Field Theories, in 12th Annual Winter\\r\\n     School of Theoretical Physics, 1975.\\r\\n[52] L.F. Abbott, The Background Field Method Beyond One Loop,\\r\\n     Nucl. Phys. B 185 (1981) 189.\\r\\n[53] L.F. Abbott, Introduction to the Background Field Method, Acta Phys. Polon. B 13 (1982)\\r\\n     33.\\r\\n[54] S. Sarkar and H. Strubbe, Anomalous Dimensions in Background Field Gauges,\\r\\n     Nucl. Phys. B 90 (1975) 45.\\r\\n[55] N. Nakanishi, Covariant Quantization of the Electromagnetic Field in the Landau Gauge,\\r\\n     Prog. Theor. Phys. 35 (1966) 1111.\\r\\n[56] B. Lautrup, Canonical Quantum Electrodynamics in covariant Gauges, .\\r\\n[57] C. Becchi, A. Rouet and R. Stora, Renormalization of Gauge Theories,\\r\\n     Annals Phys. 98 (1976) 287.\\r\\n[58] I.V. Tyutin, Gauge Invariance in Field Theory and Statistical Physics in Operator\\r\\n     Formalism, arXiv:0812.0580.\\r\\n[59] Z. Bern, J.J.M. Carrasco and H. Johansson, New Relations for Gauge-Theory Amplitudes,\\r\\n     Phys. Rev. D 78 (2008) 085011 [arXiv:0805.3993].\\r\\n[60] T. van Ritbergen, A.N. Schellekens and J.A.M. Vermaseren, Group theory factors for\\r\\n     Feynman diagrams, Int. J. Mod. Phys. A 14 (1999) 41 [hep-ph/9802376].\\r\\n[61] B. Ruijl, T. Ueda and J. Vermaseren, FORM version 4.2, arXiv:1707.06453.\\r\\n[62] P. Pascual and R. Tarrach, QCD: Renormalization for the Practitioner, vol. 194 (1984).\\r\\n[63] K.G. Chetyrkin and F.V. Tkachov, Infrared R Operation and ultraviolet Counterterms in the\\r\\n     MS scheme, Phys. Lett. 114B (1982) 340.\\r\\n[64] K.G. Chetyrkin and V.A. Smirnov, R* operation corrected, Phys. Lett. 144B (1984) 419.\\r\\n[65] V.A. Smirnov and K.G. Chetyrkin, R* Operation in the Minimal Subtraction Scheme,\\r\\n     Theor. Math. Phys. 63 (1985) 462.\\r\\n[66] K.G. Chetyrkin, Combinatorics of R-, R\\xe2\\x88\\x921 -, and R\\xe2\\x88\\x97 -operations and asymptotic expansions\\r\\n     of feynman integrals in the limit of large momenta and masses, arXiv:1701.08627.\\r\\n[67] F. Herzog and B. Ruijl, The R\\xe2\\x88\\x97 -operation for Feynman graphs with generic numerators,\\r\\n     JHEP 05 (2017) 037 [arXiv:1703.03776].\\r\\n[68] J. de Vries, G. Falcioni, F. Herzog and B. Ruijl, Two- and three-loop anomalous dimensions\\r\\n     of Weinberg\\xe2\\x80\\x99s dimension-six CP-odd gluonic operator, Phys. Rev. D 102 (2020) 016010\\r\\n     [arXiv:1907.04923].\\r\\n[69] R. Beekveldt, M. Borinsky and F. Herzog, The Hopf algebra structure of the R*-operation,\\r\\n     JHEP 07 (2020) 061 [arXiv:2003.04301].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            \\xe2\\x80\\x93 43 \\xe2\\x80\\x93\\r\\n\\x0c\\n\\n[70] W. Cao, F. Herzog, T. Melia and J.R. Nepveu, Renormalization and non-renormalization of\\r\\n     scalar EFTs at higher orders, JHEP 09 (2021) 014 [arXiv:2105.12742].\\r\\n[71] S.G. Gorishnii, S.A. Larin and F.V. Tkachov, The Algorithm for OPE Coefficient Functions\\r\\n     in the MS scheme, Phys. Lett. B 124 (1983) 217.\\r\\n[72] S.G. Gorishnii and S.A. Larin, Coefficient Functions of Asymptotic Operator Expansions in\\r\\n     Minimal Subtraction Scheme, Nucl. Phys. B 283 (1987) 452.\\r\\n[73] P. Nogueira, Automatic Feynman graph generation, J. Comput. Phys. 105 (1993) 279.\\r\\n[74] P.A. Baikov and K.G. Chetyrkin, Four Loop Massless Propagators: An Algebraic Evaluation\\r\\n     of All Master Integrals, Nucl. Phys. B837 (2010) 186 [arXiv:1004.1153].\\r\\n[75] R.N. Lee, A.V. Smirnov and V.A. Smirnov, Master Integrals for Four-Loop Massless\\r\\n     Propagators up to Transcendentality Weight Twelve, Nucl. Phys. B 856 (2012) 95\\r\\n     [arXiv:1108.0732].\\r\\n[76] A. Georgoudis, V. Goncalves, E. Panzer and R. Pereira, Five-loop massless propagator\\r\\n     integrals, arXiv:1802.00803.\\r\\n[77] A. Georgoudis, V. Gonc\\xcc\\xa7alves, E. Panzer, R. Pereira, A.V. Smirnov and V.A. Smirnov,\\r\\n     Glue-and-cut at five loops, JHEP 09 (2021) 098 [arXiv:2104.08272].\\r\\n[78] B. Ruijl, T. Ueda and J.A.M. Vermaseren, Forcer, a FORM program for the parametric\\r\\n     reduction of four-loop massless propagator diagrams, arXiv:1704.06650.\\r\\n[79] D.Z. Freedman, I.J. Muzinich and E.J. Weinberg, On the Energy-Momentum Tensor in\\r\\n     Gauge Field Theories, Annals Phys. 87 (1974) 95.\\r\\n[80] D.Z. Freedman and E.J. Weinberg, The Energy-Momentum Tensor in Scalar and Gauge\\r\\n     Field Theories, Annals Phys. 87 (1974) 354.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                            \\xe2\\x80\\x93 44 \\xe2\\x80\\x93\\r\\n\\x0c', b'     Physics-driven Synthetic Data Learning for Biomedical Magnetic Resonance\\r\\n\\r\\n              Qinqin Yang1, Zi Wang1, Kunyuan Guo1, Congbo Cai1, and Xiaobo Qu1,*\\r\\n\\r\\n                                                   Abstract\\r\\n\\r\\n       Deep learning has innovated the field of computational imaging. One of its bottlenecks is\\r\\n\\r\\nunavailable or insufficient training data. This article reviews an emerging paradigm, imaging\\r\\n\\r\\nphysics-based data synthesis (IPADS), that can provide huge training data in biomedical\\r\\n\\r\\nmagnetic resonance without or with few real data. Following the physical law of magnetic\\r\\n\\r\\nresonance, IPADS generates signals from differential equations or analytical solution models,\\r\\n\\r\\nmaking the learning more scalable, explainable, and better protecting privacy. Key components\\r\\n\\r\\nof IPADS learning, including signal generation models, basic deep learning network structures,\\r\\n\\r\\nenhanced data generation and learning methods are discussed. Great potentials of IPADS have\\r\\n\\r\\nbeen demonstrated by representative applications in fast imaging, ultrafast signal reconstruction\\r\\n\\r\\nand accurate parameter quantification. Finally, open questions and future work have been\\r\\n\\r\\ndiscussed.\\r\\n\\r\\n                                                 Index Terms\\r\\n\\r\\n       Data synthesis, Physical model, Biomedical magnetic resonance, Deep learning\\r\\n\\r\\n                                            I. INTRODUCTION\\r\\n\\r\\n       Data learning has empowered the computational imaging with fast sampling, ultrafast\\r\\n\\r\\nsignal reconstruction and straightforward parameter quantification [1, 2]. In the age of deep\\r\\n\\r\\nlearning, a large amount of high-quality data is essentially important to achieve excellent\\r\\n\\r\\nperformance. However, in biomedical imaging, these data may be hard to be acquired in the\\r\\n\\r\\nchallenging applications, e.g., the blurred images of moving organs, the lengthy measuring of\\r\\n\\r\\nquantitative physical parameters, or the irreversible acquisition of physiological processes.\\r\\n\\r\\nThus, new data generation and learning schemes are highly desired to boost biomedical imaging\\r\\n\\r\\napplications.\\r\\n\\r\\n\\r\\n1\\r\\n    Department of Electronic Science, Biomedical Intelligent Cloud Research and Development Center, Fujian\\r\\n  Provincial Key Laboratory of Plasma and Magnetic Resonance, National Institute for Data Science in Health and\\r\\n  Medicine, Xiamen University, Xiamen, China, 361005\\r\\n* Corresponding author (quxiaobo@xmu.edu.cn)\\r\\n\\x0c\\n\\n     Recently, synthetic data starts to attract attention in computational imaging [3, 4]. Learning\\r\\n\\r\\nin synthetic data could reduce the dependence on paired real-world data, quickly generate\\r\\n\\r\\nmassive data, overcome the difficulties or even the impossibility to collect real data, and protect\\r\\n\\r\\nprivacy in biomedicine. Here, we focus on the imaging physics-based data synthesis (IPADS)\\r\\n\\r\\nbecause it follows plausible physical model and enables good interpretability [4]. To make the\\r\\n\\r\\ndiscussions compact, we limit the content to the biomedical Magnetic Resonance (MR) since\\r\\n\\r\\nIPADS learning (Fig. 1) has become frontiers in this area, such as fast quantitative imaging [5-\\r\\n\\r\\n15], signal reconstruction [16-21] and pulse sequence optimization [22, 23]. The concept of\\r\\n\\r\\nIPADS learning could be generalized to other computational imaging modalities as long as an\\r\\n\\r\\nappropriate physical model and learning network are included.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 1. Two lines of imaging physics-based data synthesis (IPADS) in biomedical magnetic resonance. Various\\r\\nsynthetic images can be generated from numerical tissue parameters through physical evolution or analytical models.\\r\\nNote: PD is short for proton density.\\r\\n\\r\\n\\r\\n     This article will give an overview of IPADS for deep learning magnetic resonance. Based\\r\\n\\r\\non the physical signal evolution or not, we first divide the IPADS into two lines, including the\\r\\n\\r\\nphysical signal evolution and analytical modeling (Fig. 1), and then discuss the enhanced\\r\\n\\r\\nlearning with realistic data adaption and advanced network structures. Representative\\r\\n\\r\\napplications and future works will be provided.\\r\\n\\r\\n                              II. MAGNETIC RESONANCE PHYSICS\\r\\n\\r\\n     The physics of MR governs signal formation, which involves spin dynamics, quantum\\r\\n\\r\\nmechanics and electromagnetism. For example, MR imaging (MRI) presents spatially structural\\r\\n\\x0c\\n\\ninformation with different contrasts, while MR spectroscopy (MRS) focuses on the spectral\\r\\n\\r\\nsignals of multiple molecules. The MR signal evolution may have analytical solutions under\\r\\n\\r\\nsome conditions or may not have them in general. Accordingly, IPADS methods are divided\\r\\n\\r\\ninto three categories and some representative works are summarized in Table 1.\\r\\n\\r\\n                      Table 1. A summary of IPADS in biomedical magnetic resonance\\r\\n  Category             Data generation                          Learning issues              Method\\r\\n                         Bloch simulation                   MR fingerprinting             DRONE [5]\\r\\n                         Bloch simulation                      T2 mapping                  OLED [6]\\r\\n   Physical              Bloch simulation                  Water-fat separation         SPEN-WFS [21]\\r\\n   evolution             Bloch simulation                Sequence optimization            MRzero [22]\\r\\n                         Bloch simulation             Cardiac motion tag tracking      SyntheticCMR [24]\\r\\n                     Bloch simulation + FDTD        Electrical properties tomography      DL-EPT [10]\\r\\n                     Dipole convolution model      Quantitative susceptibility mapping   DeepQSM [11]\\r\\n  Analytical       Multi-pool exponential model     Myelin water fraction estimation    ANN-MWF [14]\\r\\n    model           Bloch-McConnell equation         CEST z-spectra quantification      ANNCEST [15]\\r\\n                   Lorentzian line shape model            Spectra reconstruction         DL-NMR [16]\\r\\n                 Quantum mechanical simulation\\r\\n                                                           MRSI reconstruction           DL-MRSI [19]\\r\\n    Hybrid          + Exponential signal model\\r\\n   approach              Bloch simulation\\r\\n                                                              SPIO mapping               DeepSPIO [13]\\r\\n                    + Dipole convolution model\\r\\nFDTD: finite difference time domain method; DL: deep learning; MRSI: MR spectroscopic imaging; CEST:\\r\\nchemical exchange saturation transfer; RF: radio frequency; QSM: quantitative susceptibility mapping; EPT:\\r\\nelectrical properties tomography; SPIO: super paramagnetic iron oxide.\\r\\n\\r\\n\\r\\nA. Physical Evolution\\r\\n\\r\\n     Bloch equation [25] describes the time-dependent evolution of magnetization vector\\r\\n\\r\\nM(t)\\xe2\\x88\\x88R3 under an external magnetic field B(t)\\xe2\\x88\\x88R3. Two key phenomena in magnetized spin,\\r\\n\\r\\ni.e., precession and relaxation, can be formulated in the differential equations:\\r\\n\\r\\n                                   dM (t )\\r\\n                                           \\xef\\x80\\xbd \\xef\\x81\\xa7 B (t ) \\xef\\x82\\xb4 M (t ) \\xef\\x80\\xab R (M (t )) ,                         (1)\\r\\n                                    dt       \\xef\\x80\\xb1\\xef\\x80\\xb4  \\xef\\x80\\xb4\\xef\\x80\\xb2\\xef\\x80\\xb4\\xef\\x80\\xb4       \\xef\\x80\\xb3 \\xef\\x80\\xb1   \\xef\\x80\\xb4\\xef\\x80\\xb2\\xef\\x80\\xb4     \\xef\\x80\\xb3\\r\\n                                                 Precession       Relaxation\\r\\n\\r\\n\\r\\nwhere \\xce\\xb3 is the gyromagnetic ratio (e.g., 42.6 MHz/T for hydrogen nuclei). For the precession\\r\\n\\r\\nterm, the motion of spins is determined by many factors, including the main static field B0, the\\r\\n\\r\\nradio-frequency (RF) pulse field B1(t), the chemical shift (frequency) \\xe2\\x88\\x86\\xcf\\x890 and the magnetic\\r\\n\\r\\nfield gradients G(t), at location r =(x\\xcb\\x86 , y\\xcb\\x86 , z\\xcb\\x86 ) as:\\r\\n\\r\\n                                                    \\xef\\x81\\x84\\xef\\x81\\xb70\\r\\n                                   B(t ) \\xef\\x80\\xbd ( B0 -         ) z\\xcb\\x86 \\xef\\x80\\xab B1 (t ) x\\xcb\\x86 \\xef\\x80\\xab r \\xef\\x83\\x97 G (t ) ,            (2)\\r\\n                                                    \\xef\\x81\\xa7\\r\\n\\r\\nwhere the thermal equilibrium magnetization vector is tipped from the B0 direction (z) into the\\r\\n\\r\\ntransverse plane (x-y) under the effect of B1(t). The G(t) is usually used to modulate phase or\\r\\n\\x0c\\n\\nencode different spatial locations. In contrast, the relaxation term describes the process of\\r\\n\\r\\nmagnetization vector returning to its equilibrium state. Two relaxation time constants, T1 and\\r\\n\\r\\nT2, are used to characterize the regrowth of longitudinal magnetization (Mz) and the decay of\\r\\n\\r\\nthe transverse magnetization (Mx,y), respectively. These relaxation parameters are very valuable\\r\\n\\r\\nin clinics, such as lesion diagnosis [9].\\r\\n\\r\\n     MR pulse sequence describes a series of physical radio frequency pulses applied to the\\r\\n\\r\\nobjects, resulting in a particular image or spectrum appearance. It usually consists of a series of\\r\\n\\r\\nvarying B1(t) and G(t) in the form of timing diagram. Typically, the signal model for a simple\\r\\n\\r\\nMR pulse sequence has an analytical solution if steady-state is assumed. However, as the\\r\\n\\r\\ncomplexity of the pulse sequence increases, analytical solutions are hard to obtain due to spin\\r\\n\\r\\nhistory effects at unsteady-states and system imperfections. For example, in MR fingerprinting\\r\\n\\r\\n[26], various sequence components are varied in a pseudorandom pattern and MR signals are\\r\\n\\r\\nnot analyzed using the analytic expression but the dictionary matching.\\r\\n\\r\\nB. Analytical Model\\r\\n\\r\\n     Analytical model provides a clear closed form solution of MR signals and could\\r\\n\\r\\napproximate the physical evolution of MR signal under some assumptions or simplifications.\\r\\n\\r\\n     In MRI, consider a most common pulse sequence, the spin-echo sequence, Eq. (3) provides\\r\\n\\r\\nan analytical solution for image contrast as [27]:\\r\\n\\r\\n                          S (r, TE , TR ) \\xef\\x80\\xbd M 0 (r ) \\xef\\x83\\x97 (1 \\xef\\x80\\xad e \\xef\\x80\\xadTR /T1 (r ) ) \\xef\\x83\\x97 e \\xef\\x80\\xadTE /T2 ( r ) ,   (3)\\r\\n\\r\\nassuming that the initial magnetization vector undergoes the action of 90\\xc2\\xb0 and 180\\xc2\\xb0 radio\\r\\n\\r\\nfrequency pulses, and the repetition time is much larger than echo time. M 0 (r ) represents\\r\\n\\r\\nequilibrium longitudinal magnetization. By adjusting the repetition and echo time, image\\r\\n\\r\\ncontrasts can be generated if the tissue parameters M 0 (r ) , T1 (r ) and T2 (r) are provided.\\r\\n\\r\\n     In addition to contrasts, magnetic susceptibility \\xcf\\x87 (r ) represents the ability of a\\r\\n\\r\\nsubstance to become magnetized under an applied magnetic field. Tissue-specific magnetic\\r\\n\\r\\nsusceptibility variations within the MRI scanner can cause inhomogeneity of the static magnetic\\r\\n\\r\\nfield B0 . To describe the field variations caused by \\xcf\\x87 (r ) , a dipole convolution model is\\r\\n\\r\\ntypically used as [11-13]:\\r\\n\\x0c\\n\\n                                     \\xef\\x81\\x84B0 (r) \\xef\\x80\\xbd B0 \\xef\\x83\\x97 \\xcf\\x87 (r ) \\xef\\x80\\xaa D(r) ,                              (4)\\r\\n\\r\\nin which, the induced field inhomogeneity \\xef\\x81\\x84B0 (r) is expressed as the convolution between\\r\\n\\r\\nthe spatial distribution of susceptibility \\xcf\\x87 (r ) and a unit dipole response D (r ) .\\r\\n\\r\\n     Not limited to the image, in MRS, the spectral signal of each individual voxel comes from\\r\\n\\r\\nmultiple molecules. This signal is commonly modeled as [20, 28]:\\r\\n                                                  M\\r\\n                                     S (r, t ) \\xef\\x80\\xbd \\xef\\x83\\xa5cm (r)vm (t )e\\r\\n                                                                       \\xef\\x80\\xadt T2,m (r )\\r\\n                                                                                        ,        (5)\\r\\n                                                 m\\xef\\x80\\xbd1\\r\\n\\r\\n\\r\\nwhere the m denotes the mth molecule, cm (r ) and vm (t ) are its concentration and basis\\r\\n\\r\\nfunction, respectively. For the ex vivo biological MRS, which are used to determine the\\r\\n\\r\\nconcentrations of metabolites or structures of proteins, the objects (usually in liquid or solid)\\r\\n\\r\\nto be acquired are placed in a tube and treated as a whole. Thus, the spatial location r is\\r\\n\\r\\ncommonly ignored and only the S (t ) is acquired from scanners. In biological MRS, the\\r\\n\\r\\nbasis function could be expressed as a linear combination of multiple exponentials (Jm) as [28]:\\r\\n                                                 Jm\\r\\n                                      vm (t ) \\xef\\x80\\xbd \\xef\\x83\\xa5 (a j ,me j ,m )e\\r\\n                                                           i\\xef\\x81\\xa6        i 2\\xef\\x81\\xb0 f j ,mt\\r\\n                                                                                    ,            (6)\\r\\n                                                 j \\xef\\x80\\xbd1\\r\\n\\r\\n\\r\\nwhere i is the imaginary unit, a j , m , f j , m and \\xef\\x81\\xa6 j ,m are the amplitude, frequency, and phase\\r\\n\\r\\n        th\\r\\nof the j spectral peak, respectively. By performing the Fourier transform on S (t ) , a\\r\\n\\r\\nspectrum will be obtained and the spectral peaks follow the Lorentzian line shape [16, 17, 28].\\r\\n\\r\\n     For in vivo MRS, Eq. (5) is sub-optimal since it does not consider imperfect but real\\r\\n\\r\\nimaging conditions, e.g., field inhomogeneity and motions. More practically, the signal of\\r\\n\\r\\nMRS could be modeled as [19]:\\r\\n                                      M\\r\\n                          S (r, t ) \\xef\\x80\\xbd \\xef\\x83\\xa5 cm (r )vm (t )em (t ;\\xef\\x81\\xb1 m (r )) \\xef\\x80\\xab b(r, t ) ,              (7)\\r\\n                                     m \\xef\\x80\\xbd1\\r\\nwhere b(r, t ) is a baseline signal mainly contributed by macromolecules and commonly\\r\\n\\r\\nfollows the Gaussian line shape, em (t;\\xef\\x81\\xb1 m (r )) captures molecule-dependent time-domain\\r\\n\\r\\nmodulation functions that can be described by some experimental and physiological parameters\\r\\n\\r\\nin \\xef\\x81\\xb1m (r) .\\r\\n\\x0c\\n\\n                                  III. SYNTHETIC DATA LEARNING\\r\\n\\r\\nA. Physical Evolution\\r\\n\\r\\n       Physical evolution-based IPADS (PE-IPADS) and learning relies on simulating discrete\\r\\n\\r\\nspin motion at small time interval using the Bloch equation. This process can be described with\\r\\n\\r\\nsuccessive operators according to a specific MR pulse sequence. The signal formation, however,\\r\\n\\r\\nis computationally expensive, which involves large-scale matrix operations, integration and\\r\\n\\r\\ndifferentiation. Fortunately, several MR physical simulation tools have been developed, e.g.,\\r\\n\\r\\nSPROM [29], JEMRIS 1 , MRiLab 2 [30]. With these tools, one has to set proper physical\\r\\n\\r\\nparameters to make data generation in IPADS as realistic as possible.\\r\\n\\r\\n       Physical parameters include object-specific and experiment-specific parameters. The\\r\\n\\r\\nformer indicates the nature of the scanned objects, such as the proton density, relaxations (T1,\\r\\n\\r\\nT2 and T2*), diffusion and electromagnetic properties at a particular spatial location in MRI, or\\r\\n\\r\\namplitudes, resonance frequencies and metabolite concentrations in MRS. The latter takes the\\r\\n\\r\\npulse sequence and real imaging conditions into account, such as the repetition time, echo time,\\r\\n\\r\\nflip angle, the strength of the main field magnetic field (B0) and its inhomogeneity (\\xe2\\x88\\x86B0), radio\\r\\n\\r\\nfrequency field inhomogeneity (B1), and eddy currents, etc.\\r\\n\\r\\n       Up to now, most PE-IPADS learning is on imaging [5-10, 21, 24], especially quantitative\\r\\n\\r\\nparametric imaging [5-10]. A representative application of PE-IPADS learning is the T2\\r\\n\\r\\nmapping with overlapping-echo detachment (OLED) [6-9]. OLED is an ultrafast imaging\\r\\n\\r\\nsequence that encodes information of multiple image into one image, allowing parametric\\r\\n\\r\\nimaging within very short time (within 10 s for a whole brain). How to estimate reliable\\r\\n\\r\\nquantitative parameters within the same imaging time is a main problem. Deep learning has\\r\\n\\r\\nbeen evidenced powerful but the lack of real-world pairs of quantitative parameters and images\\r\\n\\r\\nshould be addressed.\\r\\n\\r\\n       A typical flow of data generation for OLED is illustrated in Fig. 2. First, object-shape\\r\\n\\r\\ntemplates should be provided. They are common created by randomly filling blank templates\\r\\n\\r\\nwith hundreds of different 2D/3D basic geometric shapes [6-8, 11, 13, 21, 24]. Second, tissue-\\r\\n\\r\\n\\r\\n\\r\\n  1\\r\\n      https://www.jemris.org\\r\\n  2\\r\\n      http://mrilab.sourceforge.net\\r\\n\\x0c\\n\\nspecific parameters, such as the proton density and T2 relaxation at particular spatial locations\\r\\n\\r\\nare set in the ranges of [0, 1] and [20, 700] ms, respectively. Then, pulse sequence needs to be\\r\\n\\r\\nprogrammed and imaging parameters (echo times= 22, 52. 82, 110 ms, flip angle=30\\xc2\\xb0) are set\\r\\n\\r\\nto be consistent with real imaging experiments. Last, an imperfection imaging condition, the\\r\\n\\r\\nradio frequency field inhomogeneity (B1), is generated by random polynomial functions.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 2. An illustration of physical evolution-based data generation. (a) Parametric templates that have geometric\\r\\nshapes and object-parameters, (b) Magnetic resonance pulse sequence needs to be programmed according to the real-\\r\\nworld experiment, (c) A software (here is the MRiLab [30]) that enables physical signal evolution, (d) Massive\\r\\nsynthetic data.\\r\\n\\r\\n\\r\\n      Once the data is generated with physical simulation tools, a network structure should be\\r\\n\\r\\ndesigned to conduct the learning with PE-IPADS data. A direct and common chosen approach\\r\\n\\r\\nis to learn the mapping from the signal to quantitative parameters in the end-to-end way [5, 6,\\r\\n\\r\\n10]. For example, the OLED images were mapped to T2 parameters in Fig. 3. In general, for a\\r\\n\\r\\ndeep learning network \\xef\\x81\\x8e\\xef\\x81\\xb2with trainable network parameter \\xce\\xb8 , an optimal learning is to\\r\\n\\r\\nminimize the loss          L \\xef\\x80\\xa8 \\xef\\x83\\x97\\xef\\x80\\xa9                                               p\\xcb\\x86 n \\xef\\x80\\xbd \\xef\\x81\\x8e (sn ; \\xce\\xb8)\\r\\n                                    of estimated quantitative object-parameters \\xef\\x81\\xb2\\r\\n\\r\\naccording to:\\r\\n                                                   N\\r\\n                                    \\xce\\xb8\\xcb\\x86 \\xef\\x80\\xbd arg min \\xef\\x83\\xa5 L \\xef\\x80\\xa8 p n \\xef\\x80\\xad \\xef\\x81\\x8e (s n ; \\xce\\xb8) \\xef\\x80\\xa9 \\xef\\xbc\\x8c                                 (8)\\r\\n                                               \\xce\\xb8\\r\\n                                                   n \\xef\\x80\\xbd1\\r\\n\\r\\n\\r\\nwhere pn is the ground-truth but simulated quantitative parameters, sn is the generated images,\\r\\n\\r\\nand n denotes the nth sample and its total number is N. Thus, PE-IPADS learning tries to\\r\\n\\r\\napproximate the inverse process from physical quantitative parameters of scanned objects to\\r\\n\\r\\nthe output signal.\\r\\n\\x0c\\n\\n     Once the network is trained with sufficient samples, estimating quantitative parameters\\r\\n\\r\\np \\xef\\x80\\xbd \\xef\\x81\\x8e (s; \\xce\\xb8\\xcb\\x86 ) from a target image s becomes a forward and fast process. For the PE-IPADS\\r\\n\\r\\nlearning for OLED imaging, 800 images were trained in around 22 hours and less than 1 second\\r\\n\\r\\nwas consumed to obtain faithful T2 maps (Figs. 3(b) and (c)). High fidelity was achieved on the\\r\\n\\r\\ncorrelation coefficients (0.999 and 0.997 in phantom and human brain data) to the T2 maps of\\r\\n\\r\\nconventional imaging pulse sequence, but the data acquisition time is reduced\\r\\n\\r\\nfrom 17 minutes to 10 seconds [7]. Besides, OLED with PE-IPADS avoids challenging motion\\r\\n\\r\\nartifacts in an epilepsy patient (Fig. 3(d)) [9].\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 3. Residual dense network structure for T2 mapping with OLED imaging sequence [7, 9]. (a) The network\\r\\narchitecture learns the end-to-end mapping from the images to T2 parameters, (b)-(c) Reconstructed T2 maps of\\r\\nOLED and the corresponding reference of conventional acquisition in phantom and in vivo brain data, respectively.\\r\\n(d) An image of clinical sequence and T2 map of OLED from an epilepsy patient.\\r\\n\\r\\n\\r\\n     The PE-IPADS has been applied to many other imaging scenarios due to its high flexibility\\r\\n\\r\\n[10, 21, 22, 24]. At the current early stage research, the network structure seems not the primary\\r\\n\\r\\nfocus. Most approaches adopted mainstream network structures, e.g., fully connected network\\r\\n\\r\\nin MR fingerprinting [5] and residual network in cardiac motion tag tracking [24]. Through\\r\\n\\r\\nsimulating the imaging process of water and fat, the SPEN-WFS [21] trained a U-Net on the\\r\\n\\r\\ngenerated data and fast separated water and fat in 0.46 s on a personal computer (tradition\\r\\n\\r\\nmethod costs 30.31 s). Additionally, with a 3D convolution neural network, the common Bloch\\r\\n\\r\\nequation evolution could be combined with electromagnetic simulation to obtain coil-specific\\r\\n\\r\\nradio frequency profiles and phases [10]. This approach leverages PE-IPADS into electrical\\r\\n\\r\\nproperty tomography, extending traditional MRI to new imaging modalities.\\r\\n\\x0c\\n\\nB. Analytical Model\\r\\n\\r\\n    Analytical model-based IPADS (AM-IPADS) skips the complex physical evolution\\r\\n\\r\\nprocess. It directly and quickly generates a large amount of training data by adjusting the\\r\\n\\r\\nphysical parameters in the closed form solution expression [11, 12, 14, 16-18]. It is interesting\\r\\n\\r\\nto see that most AM-IPADS learning is on spectra in MRS [15-20] and few applications on\\r\\n\\r\\nMRI [11, 12, 14]. Thus, in the following, we mainly discuss the spectra applications.\\r\\n\\r\\n    In ex vivo biological MRS, the spectral peaks are usually in Lorentzian lineshapes and the\\r\\n\\r\\neffect of system imperfection is negligible in the ideal scenario, i.e., only object-specific\\r\\n\\r\\nparameters need to be considered. Specifically, synthetic data are generated using general\\r\\n\\r\\nexponential functions[16-18].\\r\\n\\r\\n    The AM-IPADS are commonly used in the spectra reconstruction [16-18] that does not\\r\\n\\r\\ninvolve the physical evolution process. Reconstruction aims at estimating a high-quality\\r\\n\\r\\nspectrum from the undersampled or low signal-to-noise data. Although deep learning has shown\\r\\n\\r\\nastonishing performance in image reconstruction of MRI, the methodology was developed\\r\\n\\r\\nrelatively later in MRS due to the lack of paired realistic data. IPADS relaxes this requirement\\r\\n\\r\\nthrough data generation according to Eqs. (5)-(7). As the physical parameters, such as\\r\\n\\r\\namplitudes, resonance frequencies and metabolite concentrations can be simulated, IPADS\\r\\n\\r\\nstrongly increases the flexibility of the deep learning. In general, the network \\xef\\x81\\x8e learns the\\r\\n\\r\\ntrainable parameters \\xce\\xb8 to minimize the total difference between the fully-sampled (or noise-\\r\\n\\r\\nfree) label signal s\\xef\\x80\\xa5 and the output of network \\xef\\x81\\x8e (d n ; \\xce\\xb8) as follows:\\r\\n\\r\\n                                              N\\r\\n                                \\xce\\xb8\\xcb\\x86 \\xef\\x80\\xbd argmin \\xef\\x83\\xa5 L \\xef\\x80\\xa8 s\\xef\\x80\\xa5n \\xef\\x80\\xad \\xef\\x81\\x8e (dn ; \\xce\\xb8)\\xef\\x80\\xa9 ,                        (9)\\r\\n                                          \\xce\\xb8\\r\\n                                              n\\xef\\x80\\xbd1\\r\\n\\r\\n\\r\\nwhere N is the number of training samples, L is the loss function such as the l2 norm loss.\\r\\n\\r\\nAfter obtaining the optimal network parameters \\xce\\xb8\\xcc\\x82 , a target signal s is reconstructed via\\r\\n\\r\\ns \\xef\\x80\\xbd \\xef\\x81\\x8e (d; \\xce\\xb8\\xcb\\x86 ) for a given undersampled (or noisy) realistic input d . The reconstruction is\\r\\n\\r\\nalways ultrafast in realistic experiments, e.g., only 0.04/2.75 seconds for reconstructing 2D/3D\\r\\n\\r\\nprotein spectra and is about 30 times faster than the conventional compressed sensing methods\\r\\n\\r\\n[18]. The basic network architectures are convolutional neural networks (CNN) [18] or its\\r\\n\\r\\ndensely connected version [16, 17].\\r\\n\\x0c\\n\\n    Without any real data involved in training, a representative application of the AM-IPADS\\r\\n\\r\\nlearning is ultrafast MRS reconstruction [16-18]. Firstly, based on the exponential model in Eq.\\r\\n\\r\\n(5), we vary spectral parameters according to the uniform distribution, such as discretely\\r\\n\\r\\nrandomizing the number of peaks from 1 to 10, normalized amplitude from 0.05 to 1, and the\\r\\n\\r\\nnormalized frequency from 0.01 to 0.99 Hz [16-18]. In total, 40000 pairs of synthetic data\\r\\n\\r\\n(inputs are undersampled time-domain free induction decay, FID, signals and output labels are\\r\\n\\r\\nfully sampled spectra) are generated within several seconds. Then, a deep learning magnetic\\r\\n\\r\\nresonance network, DLNMR [16], is trained with the synthetic data in 5~31 hours. The\\r\\n\\r\\nflowchart of DLNMR (Fig. 4(a)) shows that the spectrum artifacts introduced by undersampling\\r\\n\\r\\nare first removed with dense CNN and then the intermediate spectra are further refined to\\r\\n\\r\\nmaintain the data consistency to the sampled FID. With the increase of the network phase,\\r\\n\\r\\nartifacts are gradually removed, and finally a clean spectrum can be reconstructed. The\\r\\n\\r\\nDLNMR is good at restoring high-intensity peaks (the 2nd row of Fig. 4(c)). It has been applied\\r\\n\\r\\nto reconstruct many real spectra of proteins, achieving the peak correlation up to 0.9996 in 2D\\r\\n\\r\\nspectra and the acceleration factor of sampling up to 10 in 3D spectra. Even though, small peaks\\r\\n\\r\\n(the 2nd row of Fig. 4(c)) may be comprised and even becomes worse (the 2nd row of Fig. 4(d))\\r\\n\\r\\nif the mismatch is existed between the training and target data. These observations imply that,\\r\\n\\r\\nwithout any real data in training, AM-IPADS has great protentional to boost deep learning but\\r\\n\\r\\nstill needs further improvements.\\r\\n\\x0c\\n\\nFig. 4. Exponential signal reconstructions with mismatched training data [16, 17]. (a) Recursive DLNMR framework\\r\\nthat alternates between the dense CNN and the data consistency. (b) A uniformly distribution I of peak intensities for\\r\\ntraining or reconstruction and a non-uniform distribution II that is only for reconstruction, (c) and (d) are\\r\\nreconstructed target signals that satisfy uniform and non-uniform distributions, respectively. Note: The sampling rate\\r\\nis 25%.\\r\\n\\r\\n\\r\\n     In addition to signal reconstruction, AM-IPADS deep learning has been explored in\\r\\n\\r\\nphysical parameter quantification, such as quantitative susceptibility mapping [11, 12] and\\r\\n\\r\\nwater fraction estimation [14]. As the loss function in Eq. (8) is defined on generated data but\\r\\n\\r\\nnot the physical evolution, this model can also be applied here. AM-IPADS enables the inverse\\r\\n\\r\\nlearning from the image to physical parameters. But this learning does not mean that a\\r\\n\\r\\nsufficiently faithful results can be obtained. A possible way is to use the forward analytical\\r\\n\\r\\nmodel \\xef\\x81\\x86         to bridge the missed connection from physical parameters p                               to image\\r\\n\\r\\ns \\xef\\x80\\xbd \\xef\\x81\\x86 \\xef\\x80\\xa8 p \\xef\\x80\\xa9 , and then regularize the solution to has small errors of both physical parameters and\\r\\n\\r\\nimages as:\\r\\n                                    N\\r\\n                   \\xce\\xb8\\xcb\\x86 \\xef\\x80\\xbd arg min \\xef\\x83\\xa5 L \\xef\\x80\\xa8 p n \\xef\\x80\\xad \\xef\\x81\\x8e (s n ; \\xce\\xb8) \\xef\\x80\\xa9 \\xef\\x80\\xab Lmodel \\xef\\x81\\x9b \\xef\\x81\\x86 (p n ) \\xef\\x80\\xad \\xef\\x81\\x86 ( \\xef\\x81\\x8e (s n ; \\xce\\xb8)) \\xef\\x81\\x9d ,            (10)\\r\\n                               \\xce\\xb8\\r\\n                                   n \\xef\\x80\\xbd1\\r\\n\\r\\n\\r\\n\\r\\nwhere Lmodel is the additional loss with the analytical model. For example, in QSMnet+ [12],\\r\\n\\x0c\\n\\np is a susceptibility map, s is a local B0 inhomogeneity image and \\xef\\x81\\x86 is an analytical\\r\\n\\r\\ndifferentiable dipole model defined in Eq. (4).\\r\\n\\r\\nC. Hybrid Approach\\r\\n\\r\\n       Hybrid approach (HB-IPADS) means that IPADS learning integrates physical evolution\\r\\n\\r\\nand analytical model together. The former could be used for some parts that involve Bloch\\r\\n\\r\\nequation, specific MR pulse equations, and resonance structures. The latter is used to directly\\r\\n\\r\\ngenerate a large amount of training data from the analytical model with randomized parameters.\\r\\n\\r\\n       The HB-IPADS has been successfully applied to in vivo MRS spectra reconstruction [19,\\r\\n\\r\\n20, 31], which considers the complex non-ideal acquisition conditions. The basis functions,\\r\\n\\r\\nvm (t ) in Eq. (7), which are invariant to different subjects, are first generated using physical\\r\\n\\r\\nquantum mechanical simulations tools, such as jMRUI1 and FID-A2. Then, all other physical\\r\\n\\r\\nsignals, which are related to metabolite concentrations of objects, unexpected macromolecule\\r\\n\\r\\nsignals and system imperfections, are directly generated according to the analytical expressions\\r\\n\\r\\nin Eq. (7), by varying parameters from an empirical range [19, 20, 31]. After minimizing the\\r\\n\\r\\nsame loss function in Eq. (9), the trained network can be applied to MRS denoising [19, 31]\\r\\n\\r\\nand separation [20]. With an auto-encoder network, the mapping from the low signal-to-noise-\\r\\n\\r\\nratio (SNR) spectra to the high SNR one is learnt on the IPADS data, and then applied to SNR\\r\\n\\r\\nimprovement of realistic MRS [19]. Compared with the conventional spatial smoothness and\\r\\n\\r\\nsubspace methods, auto-encoder reduces the mean square error of denoised 31P spectra on a\\r\\n\\r\\nnumerical phantom by 90% and 58%, respectively [19].\\r\\n\\r\\n       Moreover, the HB-IPADS is also extended to MRI deep learning quantification, such as\\r\\n\\r\\nsuper paramagnetic iron oxide (SPIO) particle quantification [13]. This method firstly uses the\\r\\n\\r\\nanalytical model in Eq. (4) to obtain the inhomogeneous local B0 field of SPIO concentration,\\r\\n\\r\\nand then generates slice-modulated MRI image through Bloch simulation. The network learns\\r\\n\\r\\nthe mapping from the generated images to the wavelet coefficients of spatial concentration\\r\\n\\r\\ndistribution, which is finally obtained by performing the inverse wavelet transform. The\\r\\n\\r\\nnetwork consists of encoder, bottleneck and four decoder sub-networks. This HB-IPADS\\r\\n\\r\\n\\r\\n  1\\r\\n      https://github.com/isi-nmr/jMRUI\\r\\n  2\\r\\n      https://github.com/CIC-methods/FID-A\\r\\n\\x0c\\n\\nmethod significantly outperforms traditional algorithms which are unreliable when the\\r\\n\\r\\nhigh concentration of SPIO has a large effect on the inhomogeneous field and phase.\\r\\n\\r\\n                                  IV. DATA ENHANCEMENT\\r\\n\\r\\n     Enhancing the IPADS data to fit for real applications is discussed here.\\r\\n\\r\\n     The experiment-specific characteristics, especially system imperfections, should be\\r\\n\\r\\nmatched to real applications. For example, in CEST imaging, the inhomogeneity of B0 has been\\r\\n\\r\\ncarefully chosen for phantom (-0.4~0.4 ppm) and human skeletal muscle (-0.25~0.25 ppm),\\r\\n\\r\\nrespectively [15]. Besides, a same range of imperfect parameters in training and testing data\\r\\n\\r\\ncould greatly improve the deep learning performance. In real-time spiral MRI [32], a major\\r\\n\\r\\nlimitation is image blurring introduced by off-resonance. A neural network trained with the\\r\\n\\r\\nsame off-resonance range significantly increases the peak signal-to-noise ratio of deblurred\\r\\n\\r\\nimages from 11.04 dB to 24.57 dB [32]. Furthermore, MRI images are usually acquired from\\r\\n\\r\\nmulti-coils and the correlation of them can be reflected by the coil sensitivity maps. To adapt\\r\\n\\r\\nfor real parallel imaging, these maps could be obtained on few real experimental data (5 subjects)\\r\\n\\r\\nfrom one scanner and then multiplied them onto single-channel images from other scanners or\\r\\n\\r\\npublic databases [9].\\r\\n\\r\\n     The object-specific parameter should pay attention to the non-uniform distribution in real\\r\\n\\r\\ndata. Many physical parameters of IPADS data are chosen from a given range with equal\\r\\n\\r\\nprobability, resulting in the uniform distributions of these parameters in the training data [16].\\r\\n\\r\\nThis type of data generation provides unbiased representation when no other prior knowledge\\r\\n\\r\\nis provided. For example, uniform distributions of amplitudes and frequencies were made for\\r\\n\\r\\nIPADS spectrum training and have been successfully applied to many real protein spectra [16-\\r\\n\\r\\n18]. However, the reconstruction performance will be compromised if the true distribution is\\r\\n\\r\\nnon-uniform. For example, many low-intensity peaks (the 2nd row of Fig. 4(c)) are lost in the\\r\\n\\r\\nreconstruction if they do not have a high ratio in the training data. Thus, a good estimation of\\r\\n\\r\\nthe distribution for real data is valuable.\\r\\n\\x0c\\n\\nFig. 5. Enhance the susceptibility distribution to a wider range [12]. (a) the original susceptibility distribution trained\\r\\nfrom healthy controls and the enhanced one. (b) Reconstructed QSM from hemorrhagic patients without and with\\r\\ndata enhancement. The artifacts were marked by yellow arrow. ppm: parts per million.\\r\\n\\r\\n\\r\\n      In imaging, spatial and temporal distributions of physical parameters should be carefully\\r\\n\\r\\ntuned. In quantitative susceptibility mapping, the hemorrhagic lesions have higher\\r\\n\\r\\nsusceptibility than healthy tissues. To better fit patient data, the susceptibility values were\\r\\n\\r\\nwidened through multiplying different factors at different image regions of estimated maps,\\r\\n\\r\\nresulting in the much better artifacts removal [12] (Fig. 5). The random synthetic shapes\\r\\n\\r\\nassigned with restricted parametric values have been used for network training in OLED T2\\r\\n\\r\\nmapping [6-8] and water-fat imaging [21]. Instead, the spatial distribution with texture\\r\\n\\r\\ninformation from real data could be introduced into enhance IPADS learning. More realistic\\r\\n\\r\\ntexture was achieved than the result of deep network trained with arbitrary random synthetic\\r\\n\\r\\nshapes [9, 21] (Fig. 6). In dynamic cardiac imaging, more motions were set at the beginning of\\r\\n\\r\\nthe cycle than at the end, making the synthetic motion paths more realistic [24].\\r\\n\\x0c\\n\\nFig. 6. Representative examples of spatial distribution enhanced IPADS learning. Upper row: Rich texture\\r\\ninformation from natural images were added into random shape-based templates for better water-fat imaging [21].\\r\\nLower row: More realistic textures were captured when real spatial distribution was introduced in T2 mapping [9].\\r\\n\\r\\n\\r\\n                            V. ENHANCED LEARNING WITH IPADS\\r\\n\\r\\n     Narrowing the gap between IPADS and real data is a main task of the network design.\\r\\n\\r\\n     Regularizing the solution with general signal priors can generalize the IPADS network to\\r\\n\\r\\npractical data. These priors could be the smoothness [20] and low-rankness [18], that were\\r\\n\\r\\nwidely adopted in traditional model-based reconstruction. A direct scheme is taking the output\\r\\n\\r\\nof a pre-trained network as an intermediate solution and then improving it with traditional\\r\\n\\r\\noptimization model. For example, the smoothness of image in MRSI is enforced with a finite-\\r\\n\\r\\ndifference operator [20]. This scheme allows convergence characterization thus makes the\\r\\n\\r\\nalgorithm more traceable. Another scheme is to design a network structure by imitating the\\r\\n\\r\\niterative process in the model-based reconstruction, e.g., the low-rank MRS reconstruction (Fig.\\r\\n\\r\\n6) [18]. This approach better preserves the low-intensity peaks and may handle the shifted\\r\\n\\r\\ndistributions of spectra intensities in the training IPADS and target data. Besides, interpretable\\r\\n\\r\\nbehavior of network, such as the progressive low-rank approximation, is also provided. Thus,\\r\\n\\r\\nthe general signal prior is still powerful to improve IPADS network.\\r\\n\\r\\n     Refining the network parameters may repair knowledge missed in IPADS learning. Under\\r\\n\\r\\nthe principle of meta learning, introducing a sub-network to adjust hyper-parameters of the\\r\\n\\r\\noriginal network can adapt well to real data, which goes beyond the IPADS training set [18].\\r\\n\\r\\nFor instance (see Fig. 7), threshold in the backbone of sparse learning network can be adjusted\\r\\n\\x0c\\n\\nso that each input had its own threshold to eliminate undersampling artifacts, resulting in the\\r\\n\\r\\nmitigation of reconstruction performance loss under unseen sampling rates in IPADS [18].\\r\\n\\r\\nAnother way is introducing few real data to adjust network parameters by transfer learning.\\r\\n\\r\\nThis approach has been explored in conventional network training without IPADS [33]. For\\r\\n\\r\\nexample, thousands of natural images were assembled with phase and coil information and then\\r\\n\\r\\nused to train MRI reconstruction network. By transfer learning from tens of real MRI images,\\r\\n\\r\\nnearly identical reconstruction performance is achieved as training thousands of real images\\r\\n\\r\\n[33]. Since IPADS learning has approximated physical laws in biomedical magnetic resonance,\\r\\n\\r\\nmuch less uncertainty needs to be addressed in the transfer learning. Thus, fewer real data may\\r\\n\\r\\nbe required by IPADS learning than those of conventional network.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFig. 7. Hyper-parameter adjustment with meta-learning in spectrum reconstruction [18]. (a) is the backbone network\\r\\nwith soft-thresholding and the hyper-network that adjusts the hyper-parameter (threshold), (b) robust reconstruction\\r\\nto unseen sampling rates in IPADS, (c) a region of reconstructed spectra. Note: Higher peak intensity correlation\\r\\nmeans better reconstruction of spectral peaks.\\r\\n\\r\\n\\r\\n     Directly incorporating physical models into network structure may relax the requirement\\r\\n\\r\\nof paired dataset. A good example is the recently proposed physics-informed neural networks\\r\\n\\r\\nfor myocardial perfusion MRI quantification. By modeling the evolution of the concentration\\r\\n\\r\\nof contrast agent as differential equations, a physics-informed network is proposed to\\r\\n\\r\\nsimultaneously estimate concentrations of contrast agent and quantify kinetic parameters [34].\\r\\n\\r\\nThis strategy does not need a large database of paired samples, but costs relatively long running\\r\\n\\r\\ntime (1 hour per imaging slice) under a shallow network of 4 layers [34]. Since the solutions\\r\\n\\r\\nsatisfy the underlying physical laws, the degrees of freedom to describe the imaging physics\\r\\n\\r\\nwould be reduced. Thus, the requirement of paired data may be relaxed. Besides, the IPADS\\r\\n\\r\\ndata could be used to train the initial network parameters offline. This would be very helpful to\\r\\n\\x0c\\n\\nreduce the computational time and allows using deeper networks to improve parameter\\r\\n\\r\\nquantification performance.\\r\\n\\r\\n                                  VI. SUMMARY AND OUTLOOK\\r\\n\\r\\n       The deep learning has innovated the field of computational imaging. One of its bottlenecks\\r\\n\\r\\nis lacking of labels in challenging applications. This article reviews the recent progress of\\r\\n\\r\\nbiomedical magnetic resonance on deep learning with imaging physics-based data synthesis\\r\\n\\r\\n(IPADS). Non or few real data is required since IPADS generate signals from partial equations\\r\\n\\r\\nor analytical solution models following physical laws, making the learning more scalable,\\r\\n\\r\\nexplainable and better protecting privacy. Great potentials of IPADS have been evidenced in\\r\\n\\r\\nfast imaging, ultrafast signal reconstruction and accurate parameter quantification.\\r\\n\\r\\n       Mitigating the difference between synthetic with real data is still at the early stage. Many\\r\\n\\r\\ncurrent methods highly depend on the accuracy of physical modeling and the selection of\\r\\n\\r\\nparameter ranges. Although good performance has been achieved on unseen real data,\\r\\n\\r\\nunpredictability in clinical situations remains the biggest risk of synthetic databased approaches.\\r\\n\\r\\nTo generalize synthetic-data learning to real biomedicine and clinic, more effort should be made\\r\\n\\r\\nto mine the data/parameter distribution of real data, design robust networks and characterize\\r\\n\\r\\nthe performance. Few subjective evaluations from radiologists have been found in the existed\\r\\n\\r\\nIPADS learning and needs to strengthen.\\r\\n\\r\\n       Besides, a powerful, open and any time accessible computing platform of IPADS is highly\\r\\n\\r\\nexpected. As programming for physical signal evolution is non-trivial, an online platform that\\r\\n\\r\\ncan fast and accurately generate data with user-defined parameters will allow more researchers\\r\\n\\r\\nto obtain huge IPADS datasets. These data can be used as training set for learning network or\\r\\n\\r\\nstress test data for developed deep learning imaging approaches. Scalable programming, such\\r\\n\\r\\nas graphic pulse sequence design for MRI, and high-precision simulation, e.g., non-rigid motion,\\r\\n\\r\\nproton diffusion and intravoxel multi-spin dephasing, will enable challenging imaging\\r\\n\\r\\napplications. It is worth noting that we have developed the prototype of such IPADS cloud\\r\\n\\r\\nplatform (Fig. 8), called CloudBrain1, and will continue to improve it with online physical\\r\\n\\r\\nevolution tools to better serve the community.\\r\\n\\r\\n\\r\\n  1\\r\\n      https://csrc.xmu.edu.cn/XCloudAiImaging.html\\r\\n\\x0c\\n\\nFig. 8. A vision map of CloudBrain.\\r\\n\\r\\n\\r\\n                                          ACKNOWLEDGMENTS\\r\\n\\r\\n     This work was supported in part by the National Natural Science Foundation of China\\r\\n\\r\\n(62122064, 61871341, 82071913), the Xiamen University Nanqiang Outstanding Talents\\r\\n\\r\\nProgram. The authors are grateful to China Mobile for cloud computing resources. The authors\\r\\n\\r\\nthank Yirong Zhou, Jun Liu, Haoming Fang, Jiayu Li, Bangjun Chen for building a prototype\\r\\n\\r\\nof the CloudBrain; Haitao Huang, Xinran Chen, Chen Qian for providing experimental results\\r\\n\\r\\nand valuable discussions and guest editors for providing valuable suggestions.\\r\\n\\r\\n                                                REFERENCES\\r\\n\\r\\n[1] J. S. Duncan, M. F. Insana, and N. Ayache, \\xe2\\x80\\x9cBiomedical imaging and analysis in the age of big data and deep\\r\\nlearning,\\xe2\\x80\\x9d Proc. IEEE, vol. 108, no. 1, pp. 3-10, Jan, 2020. doi: 10.1109/jproc.2019.2956422.\\r\\n[2] M. Jacob, J. C. Ye, L. Ying, and M. Doneva, \\xe2\\x80\\x9cComputational MRI: Compressive sensing and beyond,\\xe2\\x80\\x9d IEEE\\r\\nSignal Process. Mag., vol. 37, no. 1, pp. 21-23, Jan, 2020. doi: 10.1109/msp.2019.2953993.\\r\\n[3] A. F. Frangi, S. A. Tsaftaris, and J. L. Prince, \\xe2\\x80\\x9cSimulation and synthesis in medical imaging,\\xe2\\x80\\x9d IEEE Trans. Med.\\r\\nImag., vol. 37, no. 3, pp. 673-679, Mar, 2018. doi: 10.1109/tmi.2018.2800298.\\r\\n[4] R. J. Chen, M. Y. Lu, T. Y. Chen, D. F. K. Williamson, and F. Mahmood, \\xe2\\x80\\x9cSynthetic data in machine learning for\\r\\nmedicine and healthcare,\\xe2\\x80\\x9d Nat. Biomed. Eng., vol. 5, no. 6, pp. 493-497, Jun, 2021. doi: 10.1038/s41551-021-00751-\\r\\n8.\\r\\n[5] O. Cohen, B. Zhu, and M. S. Rosen, \\xe2\\x80\\x9cMR fingerprinting Deep RecOnstruction NEtwork (DRONE),\\xe2\\x80\\x9d Magn.\\r\\nReson. Med., vol. 80, no. 3, pp. 885-894, Sep, 2018. doi: 10.1002/mrm.27198.\\r\\n[6] C. Cai, C. Wang, Y. Zeng, S. Cai, D. Liang, Y. Wu, Z. Chen, X. Ding, and J. Zhong, \\xe2\\x80\\x9cSingle-shot T-2 mapping\\r\\nusing overlapping-echo detachment planar imaging and a deep convolutional neural network,\\xe2\\x80\\x9d Magn. Reson. Med.,\\r\\nvol. 80, no. 5, pp. 2202-2214, Nov, 2018. doi: 10.1002/mrm.27205.\\r\\n[7] J. Zhang, J. Wu, S. Chen, Z. Zhang, S. Cai, C. Cai, and Z. Chen, \\xe2\\x80\\x9cRobust single-shot T2 mapping via multiple\\r\\noverlapping-echo acquisition and deep neural network,\\xe2\\x80\\x9d IEEE Trans. Med. Imag., vol. 38, no. 8, pp. 1801-1811, Aug,\\r\\n2019. doi: 10.1109/tmi.2019.2896085.\\r\\n[8] S. M. Li, J. Wu, L. C. Ma, S. H. Cai, and C. B. Cai, \\xe2\\x80\\x9cA simultaneous multi-slice T2 mapping framework based\\r\\n\\x0c\\n\\non overlapping-echo detachment planar imaging and deep learning reconstruction,\\xe2\\x80\\x9d Magn. Reson. Med., vol. 87, no.\\r\\n5, pp. 2239-2253, May, 2022. doi: 10.1002/mrm.29128.\\r\\n[9] Q. Yang, J. Wang, J. Bao, X. Wang, L. Ma, Q. Yang, S. Cai, H. He, C. Cai, and J. Dong, \\xe2\\x80\\x9cModel-based synthetic\\r\\ndata-driven learning (MOST-DL): Application in single-shot T2 mapping with severe head motion using\\r\\noverlapping-echo acquisition,\\xe2\\x80\\x9d 2021. [Online]. Available: https://arxiv.org/abs/2107.14521.\\r\\n[10] S. Gavazzi, C. A. T. van den Berg, M. H. F. Savenije, H. P. Kok, P. de Boer, L. J. A. Stalpers, J. J. W. Lagendijk,\\r\\nH. Crezee, and A. van Lier, \\xe2\\x80\\x9cDeep learning-based reconstruction of in vivo pelvis conductivity with a 3D patch-\\r\\nbased convolutional neural network trained on simulated MR data,\\xe2\\x80\\x9d Magn. Reson. Med., vol. 84, no. 5, pp. 2772-\\r\\n2787, Nov, 2020. doi: 10.1002/mrm.28285.\\r\\n[11] S. Bollmann, K. G. B. Rasmussen, M. Kristensen, R. G. Blendal, L. R. Ostergaard, M. Plocharski, K. O\\'Brien,\\r\\nC. Langkammer, A. Janke, and M. Barth, \\xe2\\x80\\x9cDeepQSM - using deep learning to solve the dipole inversion for\\r\\nquantitative   susceptibility    mapping,\\xe2\\x80\\x9d     Neuroimage,      vol.   195,    pp.    373-383,     Jul,   2019.    doi:\\r\\n10.1016/j.neuroimage.2019.03.060.\\r\\n[12] W. Jung, J. Yoon, J. Y. Choi, J. M. Kim, Y. Nam, E. Y. Kim, J. Lee, and S. Ji, \\xe2\\x80\\x9cExploring linearity of deep neural\\r\\nnetwork trained QSM: QSMnet+,\\xe2\\x80\\x9d Neuroimage, vol. 211, May, 2020. doi: 10.1016/j.neuroimage.2020.116619.\\r\\n[13] G. Della Maggiora, C. Castillo-Passi, W. Qiu, S. Liu, C. Milovic, M. Sekino, C. Tejos, S. Uribe, and P.\\r\\nIrarrazaval, \\xe2\\x80\\x9cDeepSPIO: Super paramagnetic iron oxide particle quantification using deep learning in magnetic\\r\\nresonance imaging,\\xe2\\x80\\x9d IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 1, pp. 143-153, Jan, 2022. doi:\\r\\n10.1109/tpami.2020.3012103.\\r\\n[14] S. Jung, H. Lee, K. Ryu, J. E. Song, M. Park, W. J. Moon, and D. H. Kim, \\xe2\\x80\\x9cArtificial neural network for multi-\\r\\necho gradient echo-based myelin water fraction estimation,\\xe2\\x80\\x9d Magn. Reson. Med., vol. 85, no. 1, pp. 394-403, Jan,\\r\\n2021. doi: 10.1002/mrm.28407.\\r\\n[15] L. Chen, M. Schar, K. W. Y. Chan, J. Huang, Z. Wei, H. Lu, Q. Qin, R. G. Weiss, P. C. M. van Zijl, and J. Xu,\\r\\n\\xe2\\x80\\x9cIn vivo imaging of phosphocreatine with artificial neural networks,\\xe2\\x80\\x9d Nature Communications, vol. 11, no. 1, Feb,\\r\\n2020. doi: 10.1038/s41467-020-14874-0.\\r\\n[16] X. Qu, Y. Huang, H. Lu, T. Qiu, D. Guo, T. Agback, V. Orekhov, and Z. Chen, \\xe2\\x80\\x9cAccelerated nuclear magnetic\\r\\nresonance spectroscopy with deep learning,\\xe2\\x80\\x9d Angew. Chem. Int. Ed., vol. 59, no. 26, pp. 10297-10300, Jun, 2020.\\r\\ndoi: 10.1002/anie.201908162.\\r\\n[17] Y. Huang, J. Zhao, Z. Wang, V. Orekhov, D. Guo, and X. Qu, \\xe2\\x80\\x9cExponential signal reconstruction with deep\\r\\nhankel matrix factorization,\\xe2\\x80\\x9d IEEE Trans. Neural Netw. Learn. Syst., (Epub 2021 Dec 23), 2021. doi:\\r\\n10.1109/tnnls.2021.3134717.\\r\\n[18] Z. Wang, D. Guo, Z. Tu, Y. Huang, Y. Zhou, J. Wang, L. Feng, D. Lin, Y. You, T. Agback, V. Orekhov, and X.\\r\\nQu, \\xe2\\x80\\x9cA sparse model-inspired deep thresholding network for exponential signal reconstruction--Application in fast\\r\\nbiological spectroscopy,\\xe2\\x80\\x9d IEEE Trans. Neural Netw. Learn. Syst., (Epub 2022 Feb 04), 2022. doi:\\r\\n10.1109/tnnls.2022.3144580.\\r\\n[19] F. Lam, Y. Li, and X. Peng, \\xe2\\x80\\x9cConstrained magnetic resonance spectroscopic imaging by learning nonlinear low-\\r\\ndimensional models,\\xe2\\x80\\x9d IEEE Trans. Med. Imag., vol. 39, no. 3, pp. 545-555, Mar, 2020. doi:\\r\\n10.1109/tmi.2019.2930586.\\r\\n[20] Y. Li, Z. Wang, R. Sun, and F. Lam, \\xe2\\x80\\x9cSeparation of metabolites and macromolecules for short-TE H-1-MRSI\\r\\nusing learned component-specific representations,\\xe2\\x80\\x9d IEEE Trans. Med. Imag., vol. 40, no. 4, pp. 1157-1167, Apr,\\r\\n2021. doi: 10.1109/tmi.2020.3048933.\\r\\n[21] X. Chen, W. Wang, J. Huang, J. Wu, L. Chen, C. Cai, S. Cai, and Z. Chen, \\xe2\\x80\\x9cUltrafast water-fat separation using\\r\\ndeep learning-based single-shot MRI,\\xe2\\x80\\x9d Magn. Reson. Med., (Epub 2022 Jam 31), 2022. doi: 10.1002/mrm.29172.\\r\\n[22] A. Loktyushin, K. Herz, N. Dang, F. Glang, A. Deshmane, S. Weinmuller, A. Doerfler, B. Scholkopf, K.\\r\\n\\x0c\\n\\nScheffler, and M. Zaiss, \\xe2\\x80\\x9cMRzero - Automated discovery of MRI sequences using supervised learning,\\xe2\\x80\\x9d Magn.\\r\\nReson. Med., vol. 86, no. 2, pp. 709-724, Aug, 2021. doi: 10.1002/mrm.28727.\\r\\n[23] Y. Zhang, K. Jiang, W. Jiang, N. Wang, A. J. Wright, A. Liu, and J. Wang, \\xe2\\x80\\x9cMulti-task convolutional neural\\r\\nnetwork-based design of radio frequency pulse and the accompanying gradients for magnetic resonance imaging,\\xe2\\x80\\x9d\\r\\nNMR Biomed., vol. 34, no. 2, Feb, 2021. doi: 10.1002/nbm.4443.\\r\\n[24] M. Loecher, L. E. Perotti, and D. B. Ennis, \\xe2\\x80\\x9cUsing synthetic data generation to train a cardiac motion tag\\r\\ntracking neural network,\\xe2\\x80\\x9d Med. Image Anal., vol. 74, Dec, 2021. doi: 10.1016/j.media.2021.102223.\\r\\n[25] F. Bloch, W. W. Hansen, and M. Packard, \\xe2\\x80\\x9cNuclear induction,\\xe2\\x80\\x9d Phys. Rev., vol. 69, no. 3-4, pp. 127-127, 1946.\\r\\n[26] D. Ma, V. Gulani, N. Seiberlich, K. C. Liu, J. L. Sunshine, J. L. Duerk, and M. A. Griswold, \\xe2\\x80\\x9cMagnetic resonance\\r\\nfingerprinting,\\xe2\\x80\\x9d Nature, vol. 495, no. 7440, pp. 187-192, Mar, 2013. doi: 10.1038/nature11971.\\r\\n[27] M. A. Bernstein, K. F. King, and X. J. Zhou, \"Chapter 14 \\xe2\\x80\\x93 Basic Pulse Sequences,\" Handbook of MRI Pulse\\r\\nSequences, M. A. Bernstein, K. F. King and X. J. Zhou, eds., pp. 579-647, Burlington: Academic Press, 2004.\\r\\n[28] X. Qu, M. Mayzel, J.-F. Cai, Z. Chen, and V. Orekhov, \\xe2\\x80\\x9cAccelerated NMR spectroscopy with low-rank\\r\\nreconstruction,\\xe2\\x80\\x9d Angew. Chem. Int. Ed., vol. 54, no. 3, pp. 852-854, Jan, 2015. doi: 10.1002/anie.201409291.\\r\\n[29] C. B. Cai, M. J. Lin, Z. Chen, X. Chen, S. H. Cai, and J. H. Zhong, \\xe2\\x80\\x9cSPROM - an efficient program for\\r\\nNMR/MRI simulations of inter- and intra-molecular multiple quantum coherences,\\xe2\\x80\\x9d C. R. Phys., vol. 9, no. 1, pp.\\r\\n119-126, Jan, 2008. doi: 10.1016/j.crhy.2007.11.007.\\r\\n[30] F. Liu, J. V. Velikina, W. F. Block, R. Kijowski, and A. A. Samsonov, \\xe2\\x80\\x9cFast realistic MRI simulations based on\\r\\ngeneralized multi-pool exchange tissue model,\\xe2\\x80\\x9d IEEE Trans. Med. Imag., vol. 36, no. 2, pp. 527-537, Feb, 2017. doi:\\r\\n10.1109/tmi.2016.2620961.\\r\\n[31] H. H. Lee, and H. Kim, \\xe2\\x80\\x9cIntact metabolite spectrum mining by deep learning in proton magnetic resonance\\r\\nspectroscopy of the brain,\\xe2\\x80\\x9d Magn. Reson. Med., vol. 82, no. 1, pp. 33-48, Jul, 2019. doi: 10.1002/mrm.27727.\\r\\n[32] Y. Lim, Y. Bliesener, S. Narayanan, and K. S. Nayak, \\xe2\\x80\\x9cDeblurring for spiral real-time MRI using convolutional\\r\\nneural networks,\\xe2\\x80\\x9d Magn. Reson. Med., vol. 84, no. 6, pp. 3438-3452, 2020. doi: 10.1002/mrm.28393.\\r\\n[33] S. U. Dar, M. Ozbey, A. B. Catli, and T. Cukur, \\xe2\\x80\\x9cA transfer-learning approach for accelerated MRI using deep\\r\\nneural networks,\\xe2\\x80\\x9d Magn. Reson. Med., vol. 84, no. 2, pp. 663-685, Aug, 2020. doi: 10.1002/mrm.28148.\\r\\n[34] R. L. M. van Herten, A. Chiribiri, M. Breeuwer, M. Veta, and C. M. Scannell, \\xe2\\x80\\x9cPhysics-informed neural\\r\\nnetworks for myocardial perfusion MRI quantification,\\xe2\\x80\\x9d Med. Image Anal., vol. 78, pp. 102399, May, 2022. doi:\\r\\n10.1016/j.media.2022.102399.\\r\\n\\x0c', b'                                              DBSOP: An Efficient Heuristic for Speedy\\r\\n                                                  MCMC Sampling on Polytopes\\r\\n                                                                              A preprint\\r\\n\\r\\n\\r\\n                                                           Christos Karras1       and Aristeidis Karras1\\r\\narXiv:2203.10916v1 [cs.CG] 21 Mar 2022\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                         Computer Engineering and Informatics Department,\\r\\n                                                               University of Patras, Patras, Hellas\\r\\n                                                             {c.karras, akarras}@ceid.upatras.gr\\r\\n\\r\\n\\r\\n\\r\\n                                               Abstract. Markov Chain Monte Carlo (MCMC) techniques have long\\r\\n                                               been studied in computational geometry subjects whereabouts the prob-\\r\\n                                               lems to be studied are complex geometric objects which by their nature\\r\\n                                               require optimized techniques to be deployed or to gain useful insights\\r\\n                                               by them. MCMC approaches are directly answering to geometric prob-\\r\\n                                               lems we are attempting to answer, and how these problems could be de-\\r\\n                                               ployed from theory to practice. Polytope which is a limited volume in n-\\r\\n                                               dimensional space specified by a collection of linear inequality constraints\\r\\n                                               require specific approximation. Therefore, sampling across density based\\r\\n                                               polytopes can not be performed without the use of such methods in which\\r\\n                                               the amount of repetition required is defined as a property of error mar-\\r\\n                                               gin. In this work we propose a simple accurate sampling approach based\\r\\n                                               on the triangulation (tessellation) of a polytope. Moreover, we propose\\r\\n                                               an efficient algorithm named Density Based Sampling on Polytopes (DB-\\r\\n                                               SOP) for speedy MCMC sampling where the time required to perform\\r\\n                                               sampling is significantly lower compared    to existing approaches in low\\r\\n                                               dimensions with complexity O\\xe2\\x88\\x97 n3 . Ultimately, we highlight possible\\r\\n                                                                                    \\x01\\r\\n\\r\\n                                               future aspects and how the proposed scheme can be further improved\\r\\n                                               with the integration of reservoir-sampling based methods resulting in\\r\\n                                               more speedy and efficient solution.\\r\\n\\r\\n                                               Keywords: Polytopes \\xc2\\xb7 Uniform Sampling \\xc2\\xb7 Data Engineering \\xc2\\xb7 Convex\\r\\n                                               Bodies \\xc2\\xb7 Markov Chain \\xc2\\xb7 Monte Carlo \\xc2\\xb7 Hit-and-Run Methods\\r\\n\\r\\n\\r\\n                                         1   Introduction\\r\\n\\r\\n                                         The sampling process across different distributions is a major topic in statistics,\\r\\n                                         probability, systems engineering, as well as other disciplines that use stochastic\\r\\n                                         models ([6],[11],[12],[25],[26]). Before Monte-Carlo techniques may be used to\\r\\n                                         estimate anticipated values and other integrals, sampling algorithms must first\\r\\n                                         be developed and implemented. In recent decades, Markov Chain Monte Carlo\\r\\n                                         (MCMC) algorithms have gained remarkable success; for example, the book [7]\\r\\n                                         and the references therein discuss this issue in great detail. These tactics are\\r\\n                                         predicated on the creation of a Markov model with a density function that\\r\\n\\x0c\\n\\n2        C. Karras et al.\\r\\n\\r\\nmatches the goal distribution in which the chain is simulated for a set number\\r\\nof steps to generate samples. MCMC algorithms offer the benefit of requiring\\r\\njust wisdom of the desired density up to a ratio constant, significantly reducing\\r\\nthe quantity of data required. On the other hand, theoretical knowledge of the\\r\\nMCMC methods that are employed in practice is far from adequate. It is critical\\r\\nto control the decomposition rate of a MCMC operation, which can be defined as\\r\\nthe amount of repetitions required as a property of error margin, issue element\\r\\nn, and other variables for the chain to land on a distribution that is well within\\r\\na specific range from the objective.\\r\\n\\r\\n1.1    Problem definition\\r\\nWe are concerned with the issue of sampling from a uniform density across a\\r\\nconvex polytope1 , which is a limited volume in n-dimensional space specified by a\\r\\ncollection of linear inequality constraints, and we are interested in sampling from\\r\\na convex polytope. There are several applications for this sampling issue, but we\\r\\nare particularly interested in its application to the sampling of weight vectors\\r\\nfor multi-class discriminant analysis (MCDA). Previous research has shown that\\r\\nthe method of Hit-n-Run may be often employed to this particular use case\\r\\n[23]. Hit-n-Run has the drawback of being a MCMC method, which necessitates\\r\\nthat use of convergence is required checking or oversampling to confirm that\\r\\nconvergence has been achieved. In this work, we investigate a straightforward\\r\\nprecise sampling procedure based on the triangulation (tesselation) of a polytope.\\r\\nTechnical abbreviations are defined the very first time they appear in the text.\\r\\nUltimately, the notation used in this work is given in table 1.\\r\\n\\r\\n\\r\\n                              Table 1. Notation of this work.\\r\\n\\r\\n             Symbol         Meaning                              First in\\r\\n             4\\r\\n             =              Definition or equality by definition Eq. (1)\\r\\n             |\\xc2\\xb7|            Absolute value                       Eq. (4)\\r\\n             det(\\xc2\\xb7)         Determinant                          Eq. (4)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2     Related Work\\r\\nWith a number of applications and methodologies, the challenge of equally sam-\\r\\npling from a polytope is crucial to the success of the process. A good exam-\\r\\nple is the basis for a number of ways of estimating randomised approxima-\\r\\ntions to polytope volumes, such as the one described here. A lengthy history\\r\\nof study on sampling strategies for generating randomised estimates to the di-\\r\\nmensions of polytopes and other convex structures can be found in works such\\r\\n1\\r\\n    Not to be confused with Polytropes.\\r\\n\\x0c\\n\\n                         Efficient and speedy MCMC sampling on polytopes          3\\r\\n\\r\\nas [21],[18],[3],[20],[8]. Aspects of polytope sampling that are particularly advan-\\r\\ntageous include the development of fast randomised algorithms for multiobjec-\\r\\ntive problems [4] and sampling situational tables [15], Additionally, randomised\\r\\nstrategies for approximated solving mixed - integer linear convex programmes\\r\\nare being studied and developed [13]. Polytope sampling, as indicated in [16], is\\r\\nalso associated with hard-disk model simulators in statistical physics, along with\\r\\nestimations of erroneous incidences for linear programming in communication [9].\\r\\n    In order to sample across a uniform distribution encompassing a targeted\\r\\npolytope, one approach follows the assumption to gain useful samples from a ho-\\r\\nmogeneous proposal density which is covering the targeted polytope, for instance,\\r\\na homogeneous density centred on a square hyperbox, or a Dirichlet distribution,\\r\\nboth of which are examples of uniform proposal densities. As demonstrated in\\r\\n[14], the Dirichlet population is uniform across the simplex when the density\\r\\nfactor is assigned to 1. This attribute was employed to establish homogeneity\\r\\nthroughout the simplex in the multi-class discriminant analysis (MCDA) sce-\\r\\nnario [19]. In order to avoid a situation where the proposal density is close to\\r\\nthe desired density, such techniques must include a rejection phase. Generally,\\r\\nthe rate of rejection grows in a exponential way proportionally with the size of\\r\\nthe sample space, making this strategy ineffective for large sample spaces [22].\\r\\nAdditionally, weights may be simulated using a variety of MCMC techniques,\\r\\nwhich are detailed below. Typically, a trade-off arises among the frequency of\\r\\nmixing and the rate of acceptance by the sampler. While dealing with homoge-\\r\\nneous joints and dependent distributions, a solitary-state sampler such as Gibbs\\r\\nis the ideal approach [10]. In this circumstance, the rate of rejection is zero by\\r\\ndefault, and the weights can be repeatedly replicated while adhering to the linear\\r\\nlimits and ratio limitations set out. It has been shown that using a systematic\\r\\nstrategy of repeated sampling, there are strong connections between drawings\\r\\nand delayed mixing [1],[5]. Improved mixing characteristics may be achieved\\r\\nby modelling the weights together using random walk methods, as opposed to\\r\\nsimulating them separately.\\r\\n    Numerous MCMC approaches have widely been investigated for sample pro-\\r\\ncesses through polytopes schemas and, more broadly, for convex bodies sampling\\r\\nprocesses. There are several preliminary observations of algorithms that perform\\r\\nsampling derived from broad convex bodies, including the Ball Walk shown in\\r\\n[21] and the hit-n-run approach proposed in [3],[20]. Despite the fact that these\\r\\napproaches are applicable to polytopes, they do not take use of the particular\\r\\nstructure presented by the issue. In contrast, the Dikin walk was introduced in\\r\\n[15], which is tailored for polytopes and so achieves greater convergence rates\\r\\nthan generic techniques. With its connection to methodologies for solving lin-\\r\\near programmes using interior point approaches, the Dikin walk was the first\\r\\nsampling process found globally. Additionally, as stated in greater detail later\\r\\nin this section, it generates proposal distributions beginning with the typical\\r\\nlogarithmic barrier for a polytope. As inducted in [24], it was shown that the\\r\\nDikin walk may be extended to generic curves with subconscious barriers, which\\r\\nwas proven in a further study.\\r\\n\\x0c\\n\\n4       C. Karras et al.\\r\\n\\r\\n3     Methodology\\r\\n3.1   Definitions and requisites\\r\\nDefinition 1 (Polytope). A bounded convex n-polytope or polytope is the group\\r\\nof points\\r\\n                             4\\r\\n                          P = { p : Ap \\xe2\\x89\\xa4 b }                              (1)\\r\\nin Rn , where A is a r \\xc3\\x97 n real matrix of coefficients, b is a r-vector, and the\\r\\nrelation \\xe2\\x89\\xa4 is meant elementwise. A polytope can be determined by its vertices or\\r\\nextreme points V (V represenation of polytopes).\\r\\n\\r\\n\\r\\n\\r\\nDefinition 2 (Simplex). Let v0 , . . . , vn be points in general position in Rn .\\r\\nThe set\\r\\n            n                                     n\\r\\n                                                  X                           o\\r\\n          4\\r\\n        S = p : p = a0 v0 + . . . an vn , ai \\xe2\\x89\\xa5 0,   ai = 1 \\xe2\\x88\\x80 i = 0, . . . , n   (2)\\r\\n                                                   i=0\\r\\n                                  n\\r\\nis a n-simplex or simplex in R . S is an n-dimensional polytope.\\r\\nMore compactly, write V0 = (v1 \\xe2\\x88\\x92 v0 , . . . , vn \\xe2\\x88\\x92 v0 ) and a = (a1 , . . . , an )T , with\\r\\n(T ) denoting transpose. Then\\r\\n                           p=    a0 v0 + a1 v1 + . . . an vn\\r\\n                                   Pn              Pn\\r\\n                            = (1 \\xe2\\x88\\x92 i=1 ai )v0 + i=1 ai vi\\r\\n                                       Pn\\r\\n                            =    v0 + i=1 ai (vi \\xe2\\x88\\x92 v0 )\\r\\n                            =            v0 + V0 a,\\r\\nand (2) becomes\\r\\n\\r\\n                     S = {p : p = v0 + V0 a, a \\xe2\\x89\\xa5 0, a1T \\xe2\\x89\\xa4 1},                         (3)\\r\\nwhere \\xe2\\x89\\xa5, \\xe2\\x89\\xa4 are meant elementwise.\\r\\nThe volume of S is\\r\\n                                         | det(V0 )|\\r\\n                                 Vol(S) =            ,                           (4)\\r\\n                                             n!\\r\\nwhere |\\xc2\\xb7| means absolute value and det(\\xc2\\xb7) means determinant. Because v0 , . . . , vn\\r\\nare in general position, rank(V0 ) = n and det(V0 ) 6= 0.\\r\\n\\r\\n\\r\\n\\r\\nLemma 1. Simplicial decomposition of polytopes\\r\\nAn n-polytope P can be decomposed into n-dimensional simplices Sk , k = 1, . . . K\\r\\nsuch that P = S1 \\xe2\\x88\\xaa \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 \\xe2\\x88\\xaa SK and, for k 6= l, Sk \\xe2\\x88\\xa9 Sl = \\xe2\\x88\\x85 or Sk \\xe2\\x88\\xa9 Sl = T , where\\r\\nT is a lower-dimensional simplex.\\r\\nCorollary 1. If S1 , . . . , SK is a simplicial decomposition of P, then Vol(P) =\\r\\nPK\\r\\n k=1 Vol(Sk ).\\r\\n\\x0c\\n\\n                            Efficient and speedy MCMC sampling on polytopes     5\\r\\n\\r\\n3.2   Construction of a uniform density over a simplex S\\r\\nTheorem 1. Assume w = [wi ], i = 1, . . . , n to be a vector created at random\\r\\nwith wi \\xe2\\x89\\xa5 0, w1 +\\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7+wn \\xe2\\x89\\xa4 1, and density h(w). Then the vector p = v0 +V0 w \\xe2\\x88\\x88\\r\\nS has density\\r\\n                            g(p) = h(w)| det(V0 )|\\xe2\\x88\\x921 ,                       (5)\\r\\nProof.\\r\\nThis preceding proof utilizes the multivariate principle of Change of Variables:\\r\\n                                   \\x10 dw \\x11\\r\\n                 g(p) = h(w) det           = h(w)| det(V0 )|\\xe2\\x88\\x921 ,\\r\\n                                     dp\\r\\n       \\x10 \\x11\\r\\n                           \\xe2\\x88\\x921\\r\\nwhere dwdp    = \\xe2\\x88\\x82w\\r\\n                 \\xe2\\x88\\x82pj = (V0 )ij .\\r\\n                   i\\r\\n\\r\\n             ij\\r\\n\\r\\n   To construct a uniform probability density in S define\\r\\n                                                       P a random variable w\\r\\nwith uniform density on a regular simplex W = {wi \\xe2\\x89\\xa5 0, wi \\xe2\\x89\\xa4 1, i = 0, . . . , n}.\\r\\nSuch is a n-dimensional Dirichlet distribution\\r\\n                            h(w) = Dirichlet(1) = (n!)\\xe2\\x88\\x921 .                     (6)\\r\\nFrom Theorem 1 the choice (6) results in a random vector p with uniform density\\r\\ng(p) = (| det(V0 )|n!)\\xe2\\x88\\x921 over S.\\r\\n\\r\\n3.3   Construction of a uniform density over a polytope P\\r\\nWe denote f (p) for the following cases as:\\r\\n                  (\\r\\n                    gk (p)P (p \\xe2\\x88\\x88 Sk ) , if p \\xe2\\x88\\x88 Sk \\xe2\\x8a\\x86 P \\xe2\\x88\\x80 k = 1, . . . , K\\r\\n         f (p) =                                                               (7)\\r\\n                    0                        /P\\r\\n                                      , if p \\xe2\\x88\\x88\\r\\nwhere P (p \\xe2\\x88\\x88 Sk ) the probability that p belongs to the k-th simplex of a decom-\\r\\nposition of P as per Lemma 1. Choose P (p \\xe2\\x88\\x88 Sk ) = Vol(Sk )/Vol(P) for all k.\\r\\nThen for the k-th simplex we obtain:\\r\\n                                        \\x10 Vol(S ) \\x11\\r\\n                                                k\\r\\n             gk (p)P (w \\xe2\\x88\\x88 Sk ) = gk (p)\\r\\n                                          Vol(P)\\r\\n                                        1       \\x10 | det(V )|/n! \\x11\\r\\n                                                            0k\\r\\n                               =                  PK\\r\\n                                 | det(V0k )|n!     j=1 | det(V0j )|/n!\\r\\n                                                 1\\r\\n                                   =        PK\\r\\n                                       n!    j=1 | det(V0j )|\\r\\n\\r\\nsubsequently (7) becomes\\r\\n                      (            PK                  \\x01\\xe2\\x88\\x921\\r\\n                              n!       j=1 | det(V0j )|         , if p \\xe2\\x88\\x88 P\\r\\n                  f (p) =                                                  .   (8)\\r\\n                             0                                         /P\\r\\n                                                                , if p \\xe2\\x88\\x88\\r\\n\\r\\nAs per (8), the construction results in a uniform density distribution over the\\r\\npolytope P.\\r\\n\\x0c\\n\\n6       C. Karras et al.\\r\\n\\r\\n3.4   Proposed Algorithm for Sampling\\r\\nGiven the results derived above, we can sample uniformly from a convex polytope\\r\\nP as defined in algorithm 1.\\r\\n\\r\\n\\r\\nAlgorithm 1 Density Based Sampling On Polytope P (DBSOP)\\r\\nRequire: Vertices v0 , . . . , vn of a polytope P\\r\\nEnsure: Uniform sampling from a convex polytope P\\r\\n 1: Find the vertices v0 , . . . , vn of the polytope.\\r\\n    This can be achieved using the Avis-Fukuda pivoting algorithm as in [2].\\r\\n 2: Decompose P in n-simplices S1 , . . . , SK using, e.g., Delaunay triangulation\\r\\n    (any triangulation satifying Lemma 1 is appropriate).\\r\\n 3: Return the vertices V(Sk ) and content Vol(Sk ) of each simplex Sk from the\\r\\n    triangulation process.\\r\\n 4: Set q = (q1 , . . . , qK ) with qk = Vol(Sk )/Vol(P).\\r\\n 5: for the i-th of N samples do\\r\\n 6:    Draw a random vector wi form a regular n-simplex:\\r\\n       wi \\xe2\\x88\\xbc Dirichlet(1).\\r\\n 7:    Decide which simplex is sampled from ji \\xe2\\x88\\xbc Categorical(q).\\r\\n 8:    Compute the point pi as: pi = vji 0 + Vji 0 wi\\r\\n 9: end for\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n3.5   Implementation\\r\\nThe implementation of the proposed scheme is in RStudio where several libraries\\r\\nwere used for each step. To find the number of vertices the findVertices func-\\r\\ntion is used derived from the hitandrun package which in turn uses the rcdd\\r\\npackage. To perform the tessellation the delaunayn function is used obtained\\r\\nfrom the geometry package. The function simplex.sample to sample from the\\r\\ndegenerate Dirichlet is used acquired from the hitandrun package and the func-\\r\\ntion sample obtained from the base package is used to sample from the Cate-\\r\\ngorical.\\r\\n\\r\\n3.6   Complexity\\r\\nDue to the fact that the number of simplices created may scale up to n!, triangu-\\r\\nlation is by far the most dominant term for the complexity. Hence, the algorithm\\r\\nis not feasible in high-dimensional space. The overall complexity is O\\xe2\\x88\\x97 n3 .\\r\\n                                                                            \\x01\\r\\n\\r\\n\\r\\n4     Results\\r\\nThe running times are shown in table 2 and they were obtained using a 5.2 GHz\\r\\nIntel Core i9-10850k CPU and 32 GB of RAM for a fairly simple polytope. We\\r\\nrefer to n as the number of n dimensions of polytopes and to k as interactions.\\r\\n\\x0c\\n\\n                                                                Efficient and speedy MCMC sampling on polytopes                                                                                                7\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      0.4\\r\\n                                             + ++++++\\r\\n                                             +++           ++++++\\r\\n                                                          ++           ++++\\r\\n                                                                  ++++++       +++++++++     ++    +++\\r\\n                                                                                               +++++    +++++      +++ ++\\r\\n                                                                                                              ++++++      +++++++\\r\\n                                                                                                                                   ++++++++           ++++++++++    ++ +++  ++\\r\\n                                                 +++ +++++\\r\\n                                                    ++    ++    +        +   +\\r\\n                                                                            ++++++++\\r\\n                                                                             ++       +++   ++      +++   ++++++       +++                     ++++\\r\\n                                                                                                                                                   +++ +++\\r\\n                                                                                                                                                   +++++   +++++  +++ +  +++++\\r\\n                                                ++++ ++++ +++++++++\\r\\n                                                                  +++++++ +++ ++++++++\\r\\n                                                                                     +++  ++\\r\\n                                                                                          ++++++++++++         +++\\r\\n                                                                                                                  +++\\r\\n                                                                                                                    +++ ++  +++++++    +++++\\r\\n                                                                                                                                           ++++           +++\\r\\n                                                                                                                                                          +  +++\\r\\n                                                                                                                                                               ++   +++++   ++++\\r\\n                                                   +++\\r\\n                                                      ++++++  +\\r\\n                                                               +\\r\\n                                                               +++\\r\\n                                                               +   + +  ++\\r\\n                                                                        ++ ++++   + + +++  +  +\\r\\n                                                                                              ++\\r\\n                                                                                               ++   +++++++++++\\r\\n                                                                                                              +       ++++ +  ++\\r\\n                                                                                                                               +   ++++    +     + +\\r\\n                                                                                                                                                   +\\r\\n                                                                                                                                                     ++\\r\\n                                                                                                                                                     ++++    + +\\r\\n                                                                                                                                                               +\\r\\n                                                                                                                                                                 ++\\r\\n                                                                                                                                                                 + +      ++\\r\\n                                                                                                                                                                           + +++\\r\\n                                                               ++        +            ++ ++  +      ++++                                   ++               +\\r\\n                                                     ++++++\\r\\n                                                            ++++++++\\r\\n                                                                 ++++\\r\\n                                                                     +++\\r\\n                                                                    +++  ++++++ ++\\r\\n                                                                                +\\r\\n                                                                                    ++\\r\\n                                                                                      +++ + ++ +++\\r\\n                                                                                               +   +\\r\\n                                                                                                   +\\r\\n                                                                                                   ++++++ ++++++++++++++++++       ++++\\r\\n                                                                                                                             ++++++++    +++\\r\\n                                                                                                                                                   +++++\\r\\n                                                                                                                                                 +++ +++++\\r\\n                                                                                                                                                                ++\\r\\n                                                                                                                                                           +++++++++\\r\\n                                                                                                                                                                        ++\\r\\n                                                                                                                                                                       ++\\r\\n                                                                                                                                                                        +\\r\\n                                                                                                                                                                          +++\\r\\n                                                                                                                                                                           +\\r\\n                                                                                                                                                                           ++\\r\\n                                                                                                                                                                               +\\r\\n                                                         +++   +\\r\\n                                                              +++ + ++ +\\r\\n                                                                + ++++++++\\r\\n                                                                         +++ +  +  ++ +  + ++++ +\\r\\n                                                                                                ++ +     ++ +    +  +  ++  ++  +  +         +  +     + + +   +   ++ +++ + + + +\\r\\n                                                                                 ++++ +++++++++     +      ++++++++++              ++         +++++++++++++++\\r\\n                                                              +++++              +                  +++++\\r\\n                                                                                                      +            +++++   +++++ +++ +++    +\\r\\n                                                                                                                                           ++++++                             ++\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                      0.2\\r\\n                                                             ++       +++ +++++             ++++++++    ++ ++++++         +                            ++++++++      ++++\\r\\n                                                                +        ++++ ++ ++ +++++++                +        +++            +++           ++  ++ ++    ++++      +++++\\r\\n                                                                  ++++\\r\\n                                                                 ++   ++++   +   +++++++    ++++\\r\\n                                                                                              +++ ++   +     ++\\r\\n                                                                                                          ++++++++    ++++\\r\\n                                                                                                                     ++   +++++\\r\\n                                                                                                                            +  +++\\r\\n                                                                                                                                 ++\\r\\n                                                                                                                                   + ++++      +++ ++    +++        ++++++ +++ +\\r\\n                                                                    +++ +++  + +\\r\\n                                                                               +++  ++ +\\r\\n                                                                                        +\\r\\n                                                                                        +++\\r\\n                                                                                         +\\r\\n                                                                                          ++\\r\\n                                                                                           +++\\r\\n                                                                                             +\\r\\n                                                                                                  +\\r\\n                                                                                                  +\\r\\n                                                                                                     +++\\r\\n                                                                                                       + +    ++++\\r\\n                                                                                                                 +\\r\\n                                                                                                                  ++\\r\\n                                                                                                                   +\\r\\n                                                                                                                    +\\r\\n                                                                                                                    ++\\r\\n                                                                                                                     +\\r\\n                                                                                                                      ++++  +\\r\\n                                                                                                                             ++\\r\\n                                                                                                                             + ++  +++++++     +\\r\\n                                                                                                                                                 + +++\\r\\n                                                                                                                                                  ++  ++++  +++\\r\\n                                                                                                                                                            +\\r\\n                                                                                                                                                                +++++ + +\\r\\n                                                                                                                                                                        ++  ++\\r\\n                                                                        +++  +++ +++\\r\\n                                                                                           +\\r\\n                                                                                      ++++++++\\r\\n                                                                                    +++       ++ + +     ++\\r\\n                                                                                                     ++++++++          +\\r\\n                                                                                                                   ++++++++     ++++    +++     +++       +  +\\r\\n                                                                                                                                                      ++++++++++          +++++\\r\\n                                                                           ++++++ + +\\r\\n                                                                                    + ++   +\\r\\n                                                                                                ++\\r\\n                                                                                           ++++++   +++++++\\r\\n                                                                                                          +   ++\\r\\n                                                                                                              +  ++++ +++  ++\\r\\n                                                                                                                           + +\\r\\n                                                                                                                              ++++\\r\\n                                                                                                                               +   +  +  + +  +\\r\\n                                                                                                                                               ++\\r\\n                                                                                                                                                + + +++++++++++++\\r\\n                                                                                                                                                         + +              + ++++\\r\\n                                                                             +     +++++++++++      +  +    +  +++                 + +\\r\\n                                                                                                                        +++++++++++++++++           +  + +   +\\r\\n                                                                                                                                                      + ++++++   +++++++   + +\\r\\n                                                                              +++++  +++++  +        ++++++++++    +++++\\r\\n                                                                                                                       +++\\r\\n                                                                                                                                  ++  +++++ ++  ++++   ++ +++            +++++\\r\\n                                                                                                                                ++++                  ++++++++\\r\\n                                      0.0\\r\\n                                                                                 ++++++\\r\\n                                                                                    +++\\r\\n                                                                                         +++ +++  +++++            ++     +++                 ++\\r\\n                                                                                                                                         ++++++     +++              ++++\\r\\n                                                                                       +++\\r\\n                                                                                       +\\r\\n                                                                                         +\\r\\n                                                                                        +++   +++++\\r\\n                                                                                           ++++        ++ +\\r\\n                                                                                                         ++\\r\\n                                                                                                           ++\\r\\n                                                                                                            ++++++++++    +\\r\\n                                                                                                                        +++\\r\\n                                                                                                                        +  +++\\r\\n                                                                                                                             ++\\r\\n                                                                                                                               +\\r\\n                                                                                                                               ++++++       ++++\\r\\n                                                                                                                                                  ++ +++++++++++\\r\\n                                                                                       +      ++   + ++ +    +      +\\r\\n                                                                                                                    + ++\\r\\n                                                                                                                       +\\r\\n                                                                                                                       +  +   +    + ++       +    + +\\r\\n                                                                                           +              +     +\\r\\n                                                                                                           +++++++++        +   +       +\\r\\n                                                                                          ++ +++++\\r\\n                                                                                               +++  ++   ++            ++\\r\\n                                                                                                                    +++++ +\\r\\n                                                                                                                            +++    +++++\\r\\n                       simplex[, 2]\\r\\n\\r\\n\\r\\n                                                                                                      ++   ++++++\\r\\n                                                                                               +++    ++++        ++++++++\\r\\n                                                                                                                   ++\\r\\n                                                                                                   ++\\r\\n                                                                                                  ++    ++++++\\r\\n                                      \\xe2\\x88\\x920.2\\r\\n\\r\\n                                                                                                     ++++\\r\\n                                      \\xe2\\x88\\x920.4\\r\\n                                      \\xe2\\x88\\x920.6\\r\\n                                      \\xe2\\x88\\x920.8\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                                        \\xe2\\x88\\x920.6                                \\xe2\\x88\\x920.4                                 \\xe2\\x88\\x920.2                                   0.0        0.2   0.4      0.6\\r\\n\\r\\n                                                                                                                                                         simplex[, 1]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                                Fig. 1. Polytope sampling using the proposed method\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                      10\\r\\n                      9\\r\\n                      8\\r\\n                      7\\r\\n        Samples (n)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                      6\\r\\n                      5\\r\\n                      4\\r\\n                      3\\r\\n                      2\\r\\n                                       0                              1000                                                           2000      3000                                            4000     5000\\r\\n                                                                                                                                        Time (sec)\\r\\n\\r\\n                                                                                            Fig. 2. Samples vs Time\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n   As depicted in figure 2, the algorithm achieves fast sampling up to n = 8 and\\r\\nshows a steady performance across n = 9, . . . n = 10.\\r\\n\\x0c\\n\\n8      C. Karras et al.\\r\\n\\r\\n\\r\\n                       500\\r\\n\\r\\n                       400\\r\\n        Vertices (v)   300\\r\\n\\r\\n                       200\\r\\n\\r\\n                       100\\r\\n\\r\\n                        0\\r\\n                             0       1000       2000      3000      4000       5000\\r\\n                                                   Time (sec)\\r\\n\\r\\n                                       Fig. 3. Vertices vs Time\\r\\n\\r\\n\\r\\n\\r\\n    As depicted in figure 3, the vertices found by the algorithm are \\xe2\\x89\\x88 200 in a\\r\\nrelative short time interval while for \\xe2\\x89\\xa5 250 the process of finding vertices occurs\\r\\nwith a stable performance.\\r\\n\\r\\n\\r\\n\\r\\n                       500\\r\\n\\r\\n                       400\\r\\n\\r\\n                       300\\r\\n        Vertices (v)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                       200\\r\\n\\r\\n                       100\\r\\n\\r\\n                        0\\r\\n                             2   3          4   5       6       7   8      9     10\\r\\n                                                    Samples (n)\\r\\n\\r\\n                                      Fig. 4. Vertices vs Samples\\r\\n\\r\\n\\r\\n\\r\\n   As depicted in figure 4 the sampling of n-polytopes vs the vertices found can\\r\\nbe expressed in a f (x) = 2x \\xe2\\x88\\x92 1 way.\\r\\n\\x0c\\n\\n                         Efficient and speedy MCMC sampling on polytopes         9\\r\\n\\r\\n                             Table 2. Actual Results.\\r\\n\\r\\n                                   Actual Results\\r\\n                    n        t (s)      v             K\\r\\n                    2        0.142       3             1\\r\\n                    3        0.151       6             5\\r\\n                    4        0.154      12            44\\r\\n                    5        0.159      18           210\\r\\n                    6        0.218      39          2.486\\r\\n                    7        0.731      62         19.763\\r\\n                    8       10.218     127        359.214\\r\\n                    9       202.97     243       4.481.667\\r\\n                    10     5124.75     498      62.743.338\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n    For n = 10, the triangulation started swapping out of RAM (31GB used out\\r\\nof 32 total), and therefore this is the last actual measurement taken. Noteworthy,\\r\\nthe running time is almost three minutes for n = 9. Moreover, the triangulation\\r\\nonly becomes a dominant cost at n = 7, and in lower dimensions the running\\r\\ntime could be reduced by about 45% through more efficient implementation of\\r\\nstep 3 of the algorithm.\\r\\n   Rejection sampling is similarly only feasible up to about n = 8 [3] (note that\\r\\ntheir n is our n + 1). However, our algorithm is significantly faster for n = 7 and\\r\\nn = 8, for example rejection sampling takes over 10 seconds for n = 8 and about\\r\\n200 seconds or 3 minutes and 20 seconds for n = 9.\\r\\n\\r\\n\\r\\n                           Table 3. Predicted Results.\\r\\n\\r\\n                                  Predicted Results\\r\\n   n            t (s)      t (days)             v                   K\\r\\n   11          13572          0.15            1.082            125.486.676\\r\\n   12          34638          0.40            3.246            376.460.028\\r\\n   13         271484            3            12.984           1.505.840.112\\r\\n   14         678605           7.8           64.920           7.529.200.564\\r\\n   15         1678609         19.4          389.520          45.175.203.247\\r\\n   16         4763672         55.1         2.726.640        316.226.423.531\\r\\n   17         8163851         94.4        21.813.120       2.529.811.388.174\\r\\n   18        21263149        246.1       196.318.080      22.768.302.493.218\\r\\n   19        72163554        835.2      2.159.498.880    227.683.024.934.355\\r\\n\\x0c\\n\\n10     C. Karras et al.\\r\\n\\r\\n    Because of lack of more RAM we used a machine learning model trained\\r\\non the results of table 2 to predict the results for n = 11, 12 . . . n = 19. Table\\r\\n3 depicts the results obtained by the machine learning model where the model\\r\\nshows that as with the actual results, the number of vertices as well as the K and\\r\\nthe time t(s) grows exponentially. Note that for n = 19, the prediction shows\\r\\nthat the time required to calculate the polytope will be approximately 835 days\\r\\nor almost 2 years. Hence, it is crucial to readjust the algorithm in step 3 rather\\r\\nthan trying to calculate higher dimensions or using more RAM.\\r\\n\\r\\n5    Evaluation\\r\\nIn this section we evaluate the proposed method to existing techniques such\\r\\nas bench and har [27]. Figure 5 depicts the average time required to perform\\r\\nsampling for n = 1, 2 . . . n = 10. As shown in the figure, the proposed method\\r\\noutperforms the other two existing approaches by \\xe2\\x89\\x88 35%. The evaluation metrics\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n       Fig. 5. Comparison of the proposed method in terms of average time\\r\\n\\r\\n\\r\\nare shown in table 4. The proposed method outperforms the other two existing\\r\\nmethods across all three metrics (values shown are average) and the overall\\r\\nperformance achieved was higher. We moreover define shape compactness (SC)\\r\\nas the number of samples divided by each sampled dimension.\\r\\n\\r\\n\\r\\n                   Table 4. Evaluation of the proposed method.\\r\\n\\r\\n                     Evaluation Metric bench har DBSOP\\r\\n                     Z-value            2.1 4.9    6.4\\r\\n                     SCE                0.04 0.10 0.27\\r\\n                     SC                 6.4 8.4 11.2\\r\\n\\x0c\\n\\n                         Efficient and speedy MCMC sampling on polytopes           11\\r\\n\\r\\n6   Conclusions and Future Work\\r\\n\\r\\nIn the context of this work, a solution to the problem of sampling from a uni-\\r\\nform density over a convex polytope is presented, where polytope is a finite\\r\\nvolume in n-dimensional space characterized by a combination of linear inequal-\\r\\nity constraints. This sampling problem has a variety of applications, but we are\\r\\nespecially interested in how it might be used to the sampling of weight vectors\\r\\nfor multi-class discriminant analysis (MCDA). The outcome of the proposed al-\\r\\ngorithm resulted in a efficient and fast sampling scheme whereabouts the time\\r\\nrequired was significantly lower than existing methods in low dimensions. How-\\r\\never, for high dimensions we may require futher investigation of the rejection\\r\\nrate. Future directions of this work include the readjustment of step 3 of the\\r\\nalgorithm to decrease the time required to perform sampling. An efficient vari-\\r\\nation of this step could decrease the cost significantly resulting in a \\xe2\\x89\\x88 45%\\r\\nreduction. Moreover, another future aspect is to transform the problem of sam-\\r\\npling in a CPU-based approach rather than using RAM memory, which will\\r\\nresult in a parallel execution of all steps without requiring significant amount of\\r\\nI/Os. Ultimately, a potential path for this work in the future is the integration of\\r\\nreservoir-based sampling techniques as in [17] where the selection of k elements\\r\\nrepresentative of the whole distribution will occur to enhance the overall per-\\r\\nformance and to further reduce the time as well as the cost required to perform\\r\\nsampling.\\r\\n\\r\\n\\r\\nReferences\\r\\n 1. Amit, Y., Grenander, U.: Comparing sweep strategies for stochastic relaxation.\\r\\n    Journal of multivariate analysis 37(2), 197\\xe2\\x80\\x93222 (1991)\\r\\n 2. Avis, D., Fukuda, K.: A pivoting algorithm for convex hulls and vertex enumeration\\r\\n    of arrangements and polyhedra. Discrete & Computational Geometry 8(1), 295\\xe2\\x80\\x93\\r\\n    313 (1992). https://doi.org/10.1007/BF02293050\\r\\n 3. Be\\xcc\\x81lisle, C.J., Romeijn, H.E., Smith, R.L.: Hit-and-run algorithms for generating\\r\\n    multivariate distributions. Mathematics of Operations Research 18(2), 255\\xe2\\x80\\x93266\\r\\n    (1993)\\r\\n 4. Bertsimas, D., Vempala, S.: Solving convex programs by random walks. Journal of\\r\\n    the ACM (JACM) 51(4), 540\\xe2\\x80\\x93556 (2004)\\r\\n 5. Besag, J., Green, P., Higdon, D., Mengersen, K.: Bayesian computation and\\r\\n    stochastic systems. Statistical science pp. 3\\xe2\\x80\\x9341 (1995)\\r\\n 6. Bre\\xcc\\x81maud, P.: Markov chains: Gibbs fields, Monte Carlo simulation, and queues,\\r\\n    vol. 31. Springer Science & Business Media (2013)\\r\\n 7. Brooks, S., Gelman, A., Jones, G., Meng, X.L.: Handbook of markov chain monte\\r\\n    carlo. CRC press (2011)\\r\\n 8. Cousins, B., Vempala, S.: A cubic algorithm for computing gaussian volume. In:\\r\\n    Proceedings of the twenty-fifth annual ACM-SIAM symposium on discrete algo-\\r\\n    rithms. pp. 1215\\xe2\\x80\\x931228. SIAM (2014)\\r\\n 9. Feldman, J., Wainwright, M.J., Karger, D.R.: Using linear programming to decode\\r\\n    binary linear codes. IEEE Transactions on Information Theory 51(3), 954\\xe2\\x80\\x93972\\r\\n    (2005)\\r\\n\\x0c\\n\\n12      C. Karras et al.\\r\\n\\r\\n10. Gelfand, A.E.: Gibbs sampling. Journal of the American statistical Association\\r\\n    95(452), 1300\\xe2\\x80\\x931304 (2000)\\r\\n11. Geman, S., Geman, D.: Stochastic relaxation, gibbs distributions, and the bayesian\\r\\n    restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelli-\\r\\n    gence PAMI-6(6), 721\\xe2\\x80\\x93741 (1984). https://doi.org/10.1109/TPAMI.1984.4767596\\r\\n12. Hastings, W.K.: Monte carlo sampling methods using markov chains and their\\r\\n    applications. Biometrika 57(1), 97\\xe2\\x80\\x93109 (1970)\\r\\n13. Huang, K.L., Mehrotra, S.: An empirical evaluation of walk-and-round heuristics\\r\\n    for mixed integer linear programs. Computational optimization and applications\\r\\n    55(3), 545\\xe2\\x80\\x93570 (2013)\\r\\n14. Jia, J., Fischer, G.W., Dyer, J.S.: Attribute weighting methods and decision quality\\r\\n    in the presence of response error: a simulation study. Journal of Behavioral Decision\\r\\n    Making 11(2), 85\\xe2\\x80\\x93105 (1998)\\r\\n15. Kannan, R., Narayanan, H.: Random walks on polytopes and an affine interior\\r\\n    point method for linear programming. Mathematics of Operations Research 37(1),\\r\\n    1\\xe2\\x80\\x9320 (2012)\\r\\n16. Kapfer, S.C., Krauth, W.: Sampling from a polytope and hard-disk monte carlo. In:\\r\\n    Journal of Physics: Conference Series. vol. 454, p. 012031. IOP Publishing (2013)\\r\\n17. Karras, C., Karras, A., Sioutas, S.: Pattern Recognition and Event\\r\\n    Detection on IoT Data-streams. arXiv preprint arXiv:2203.01114 (2022).\\r\\n    https://doi.org/10.48550/arXiv.2203.01114\\r\\n18. Lawrence, J.: Polytope volume computation. Mathematics of computation 57(195),\\r\\n    259\\xe2\\x80\\x93271 (1991)\\r\\n19. Li, T., Zhu, S., Ogihara, M.: Using discriminant analysis for multi-class classifi-\\r\\n    cation: an experimental investigation. Knowledge and information systems 10(4),\\r\\n    453\\xe2\\x80\\x93472 (2006)\\r\\n20. Lova\\xcc\\x81sz, L.: Hit-and-run mixes fast. Mathematical programming 86(3), 443\\xe2\\x80\\x93461\\r\\n    (1999)\\r\\n21. Lova\\xcc\\x81sz, L., Simonovits, M.: The mixing rate of markov chains, an isoperimetric in-\\r\\n    equality, and computing the volume. In: Proceedings [1990] 31st annual symposium\\r\\n    on foundations of computer science. pp. 346\\xe2\\x80\\x93354. IEEE (1990)\\r\\n22. Mackay, D.J.C.: Introduction to monte carlo methods. In: Learning in graphical\\r\\n    models, pp. 175\\xe2\\x80\\x93204. Springer (1998)\\r\\n23. Mete, H., Zabinsky, Z.: Pattern hit-and-run for sampling efficiently on polytopes.\\r\\n    Oper. Res. Lett. 40, 6\\xe2\\x80\\x9311 (01 2012). https://doi.org/10.1016/j.orl.2011.11.002\\r\\n24. Narayanan, H.: Randomized interior point methods for sampling and optimization.\\r\\n    The Annals of Applied Probability 26(1), 597\\xe2\\x80\\x93641 (2016)\\r\\n25. Revuz, D.: Markov chains. Elsevier (2008)\\r\\n26. Ripley, B.D.: Stochastic simulation. John Wiley & Sons (2009)\\r\\n27. Smith, R.L.: Efficient monte carlo procedures for generating points uniformly dis-\\r\\n    tributed over bounded regions. Operations Research 32(6), 1296\\xe2\\x80\\x931308 (1984)\\r\\n\\x0c']\n"
     ]
    }
   ],
   "source": [
    "#Encoding all the pdf text in UTF-8\n",
    "elements=[]\n",
    "for i in array_pdf_text:\n",
    "    elements.append(i.encode(\"utf-8\"))\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a4129a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Barbu_Mayo_Alverio_Luo_Wang_Gutfreund',\n",
       "  'Xe_Dec',\n",
       "  'Monet',\n",
       "  'Kirillov_Zagoruyko',\n",
       "  'Xe_Springer',\n",
       "  'Chang_Funkhouser',\n",
       "  'Guibas_Hanrahan',\n",
       "  'Huang_Li_Savarese_Savva_Song_Su_Xiao_Yi_Yu_Shapenet',\n",
       "  'Huang_Li_Savarese_Savva_Song_Su',\n",
       "  'Chen_Tagliasacchi_Zhang_Bsp',\n",
       "  'Xe_Chen_Yin_Fisher',\n",
       "  'Chaudhuri_Zhang_Bae',\n",
       "  'Cho_Van_Merri',\n",
       "  'Kim_Russell',\n",
       "  'Wu_Unsupervised',\n",
       "  'Ellis_Wong_Nye_Sable_Meyer_Cary',\n",
       "  'Morales_Hewitt',\n",
       "  'Dreamcoder_Grow',\n",
       "  'Engelcke_Kosiorek_Jones_Posner_Genesis',\n",
       "  'Eslami_Heess_Weber',\n",
       "  'Hinton_Attend',\n",
       "  'Gao_Yang',\n",
       "  'Wu_Yuan',\n",
       "  'Wichmann_Brendel_Imagenet',\n",
       "  'Goyal_Lamb_Hoffmann',\n",
       "  'Levine_Bengio',\n",
       "  'Greff_Kaufman',\n",
       "  'Lerchner_Multi',\n",
       "  'Greff_Van_Steenkiste_Schmidhuber',\n",
       "  'Hubert_Arabie_Comparing',\n",
       "  'Jo_Bengio',\n",
       "  'Johnson_Hariharan_Van_Der_Maaten_Fei_Fei_Lawrence_Zitnick_Girshick_Clevr',\n",
       "  'Xe_Johnson',\n",
       "  'Stark_Li',\n",
       "  'Shamma_Bernstein_Fei_Fei_Image',\n",
       "  'Kipf_Elsayed_Mahendran',\n",
       "  'Heigold_Jonschkowski',\n",
       "  'Kosiorek_Kim',\n",
       "  'Teh_Posner',\n",
       "  'Lal_Prabhudesai',\n",
       "  'Coconets_Continuous',\n",
       "  'Li_Mao_Zhang',\n",
       "  'Wu_Multi',\n",
       "  'Locatello_Weissenborn_Unterthiner',\n",
       "  'Luo_Mo_Huang_Xu',\n",
       "  'Mildenhall_Srinivasan',\n",
       "  'Barron_Ramamoorthi_Ng_Nerf',\n",
       "  'Xe_Mo_Zhu',\n",
       "  'Chang_Yi_Tripathi',\n",
       "  'Guibas_Su_Partnet',\n",
       "  'Niu_Li_Xu_Imstruct_Recover',\n",
       "  'Paschalidou_Gool_Geiger_Learning',\n",
       "  'Katharopoulos_Geiger_Fidler_Neural',\n",
       "  'Goyal_Gondal',\n",
       "  'Sharma_Bengio',\n",
       "  'Srm_Spatially',\n",
       "  'Sitzmann_Chan_Tucker',\n",
       "  'Wetzstein_Metasdf_Meta',\n",
       "  'Sun_Wang_Liu_Miller',\n",
       "  'Efros_Hardt',\n",
       "  'Wu_Learning',\n",
       "  'Tsai_Srivastava',\n",
       "  'Tulsiani_Su_Guibas',\n",
       "  'Xe_Van_Steenkiste',\n",
       "  'Wu_Zhuang_Xu',\n",
       "  'Zhang_Chen_Pq',\n",
       "  'Zhang_Yang',\n",
       "  'Xue_Zhou',\n",
       "  'Yang_Gan',\n",
       "  'Yao_Hung_Jampani_Yang_Discovering',\n",
       "  'Xe_Yu',\n",
       "  'Wu',\n",
       "  'Zablotskaia_Dominici_Sigal_Lehrmann_Unsupervised',\n",
       "  'Zhang_Donahue',\n",
       "  'Girshick_Darrell',\n",
       "  'Zhao_Jiang',\n",
       "  'Jia_Torr_Koltun_Point',\n",
       "  'Zhou_Girdhar',\n",
       "  'Joulin_Kr',\n",
       "  'Xahenb',\n",
       "  'Zhu_Kr',\n",
       "  'Shechtman_Efros',\n",
       "  'Xe_Zuffi',\n",
       "  'Chairs_Tables',\n",
       "  'Cubes_Ncuboids',\n",
       "  'Additionaly',\n",
       "  'Chairs',\n",
       "  'Xce_Xb_Xce',\n",
       "  'Npoint_Cloud',\n",
       "  'Ci',\n",
       "  'Xdinputs',\n",
       "  'Xe_Xbc_Xc',\n",
       "  'Xdslots_Layer',\n",
       "  'Softmax_Xe',\n",
       "  'Avg',\n",
       "  'Nstruct_Implicit',\n",
       "  'Fig',\n",
       "  'Chair',\n",
       "  'Tables_Method_Fast_Infer_Slow_Infer_Fast_Infer_Slow_Infer'],\n",
       " ['Martin_Danelljan_Goutam',\n",
       "  'Shahbaz_Khan',\n",
       "  'Goutam_Bhat',\n",
       "  'Martin_Danelljan',\n",
       "  'Michael_Felsberg',\n",
       "  'Shahbaz_Khan_Michael',\n",
       "  'Learning_Goutam_Bhat_Joakim_Johnander',\n",
       "  'Pro_Shahbaz_Khan_Michael_Felsberg_Unveil',\n",
       "  'Proceedings_Goutam',\n",
       "  'Lawin_Martin',\n",
       "  'Jia_Deng',\n",
       "  'Wei_Dong',\n",
       "  'Richard_Socher',\n",
       "  'Li_Jia',\n",
       "  'Li_Kai_Li',\n",
       "  'Com_Li_Fei_Fei_Imagenet',\n",
       "  'David_Bolme_Ross_Beveridge_Bruce_Draper_Computer_Vision_Pattern',\n",
       "  'Yui_Man_Lui_Visual',\n",
       "  'Dong_Jianbing_Shen_Ling',\n",
       "  'Kirillov_Sergey_Zagoruyko_End_Computer_Vision_Eccv',\n",
       "  'Peng_Chu_Ge',\n",
       "  'Deng_Sijia_Yu',\n",
       "  'Mingzhen_Huang_Juehuan_Liu',\n",
       "  'Yong_Xu',\n",
       "  'Al_Lasot',\n",
       "  'Chen_Bin_Yan_Jiawen_Zhu_Dong_Wang_Xiaoyun',\n",
       "  'Huchuan_Lu',\n",
       "  'June_Yu',\n",
       "  'Bai_Yong',\n",
       "  'Chunyuan_Liao_Haibin_Ling_Lasot',\n",
       "  'Yiwei_Chen',\n",
       "  'Jingtao_Xu',\n",
       "  'Jiaqian_Yu',\n",
       "  'Qiang_Wang_Byungin',\n",
       "  'Zedu_Chen_Bineng',\n",
       "  'Zhong_Guorong_Li_Shengping_Zhang',\n",
       "  'Rongrong_Ji_Siamese',\n",
       "  'Zhihong_Fu_Qingjie_Liu_Zehua_Fu_Yunhong_Wang',\n",
       "  'Janghoon_Choi',\n",
       "  'Kwon_Kyoung_Mu_Lee_Visual',\n",
       "  'Ashton_Fagg',\n",
       "  'Chen_Huang_Deva',\n",
       "  'Kenan_Dai',\n",
       "  'Yunhua_Zhang',\n",
       "  'Wang_Jianhua',\n",
       "  'Li_Huchuan_Lu',\n",
       "  'Xiaoyun_Yang',\n",
       "  'Simon_Lucey',\n",
       "  'Proceedings_Learning',\n",
       "  'Martin_Danelljan_Goutam_Bhat_Pytracking',\n",
       "  'Junyu_Gao',\n",
       "  'Tianzhu_Zhang',\n",
       "  'Changsheng_Xu_Graph',\n",
       "  'Dongyan_Guo',\n",
       "  'Shao_Ying_Cui_Zhenhua_Wang_Abdelrahman',\n",
       "  'Jani_Ka',\n",
       "  'Liyan_Zhang_Chunhua',\n",
       "  'Jun_Wang_Ying_Cui_Zhenhua_Wang',\n",
       "  'Shengyong_Chen_Siamcar_Siamese',\n",
       "  'Matej_Kristan',\n",
       "  'Leonardis_Michael_Felsberg',\n",
       "  'Xinen_Hyung_Jin',\n",
       "  'Alan_Lukez',\n",
       "  'Drbohlav_Jani_Ka',\n",
       "  'Gustav',\n",
       "  'Song_Yan_Jinyu_Kaiming',\n",
       "  'Jian_Sun_Yang',\n",
       "  'Zhongqun_Zhang_Gustavo_Ferna',\n",
       "  'Jorge_Batista',\n",
       "  'Bo_Li',\n",
       "  'Wei_Wu_Qiang_Wang_Fangyi_Zhang',\n",
       "  'Junliang_Xing',\n",
       "  'Junjie_Yan_Siamrpn_Evolution',\n",
       "  'Lianghua_Huang_Xin_Zhao_Kaiqi_Huang_Globaltrack_Ieee',\n",
       "  'Siyuan_Li',\n",
       "  'Ziyu_Liu',\n",
       "  'Anna_Wang_Linglong',\n",
       "  'Feng_Du_Tlpg',\n",
       "  'Joint',\n",
       "  'Lianghua_Huang_Xin_Zhao_Kaiqi_Huang',\n",
       "  'Li_Changhong_Fu_Fangqiang',\n",
       "  'Ding_Ziyuan',\n",
       "  'Huang_Angelos',\n",
       "  'Katharopoulos_Apoorv_Vyas',\n",
       "  'Geng_Lu',\n",
       "  'Autotrack_Towards',\n",
       "  'Fleuret_Transformers',\n",
       "  'Bingyan_Liao',\n",
       "  'Chenye_Wang',\n",
       "  'Yayun_Wang',\n",
       "  'Yaonong',\n",
       "  'Wang_Dai',\n",
       "  'Wang_Dong_Lu',\n",
       "  'Sun_Chong',\n",
       "  'Li_Jun',\n",
       "  'Tsung_Yi_Lin',\n",
       "  'Michael_Maire_Serge_Belongie',\n",
       "  'Lubomir_Bourdev_Ross_Girshick',\n",
       "  'James_Hays',\n",
       "  'Deva_Nikita_Kitaev_Lukasz_Kaiser',\n",
       "  'Ramanan_Piotr_Dolla',\n",
       "  'Lawrence_Zitnick_Microsoft',\n",
       "  'Liu_Ruoteng',\n",
       "  'Li_Yu',\n",
       "  'Cheng_Robby',\n",
       "  'Tan_Xi_Matej_Kristan',\n",
       "  'Leonardis_Jir',\n",
       "  'Michael_Fels',\n",
       "  'Sui_Object',\n",
       "  'Roman_Pflugfelder',\n",
       "  'Joni_Kristian_Ka',\n",
       "  'Xinen_Martin',\n",
       "  'Yushan_Zhang',\n",
       "  'Song_Yan_Jinyu_Yang_Gustavo_Ferna',\n",
       "  'Al',\n",
       "  'Alan_Lukezic_Toma',\n",
       "  'Luka_Cehovin',\n",
       "  'Michael_Fels_Matej_Kristan',\n",
       "  'Roman_Pfugfelder',\n",
       "  'Tomas_Vojir',\n",
       "  'Alan_Lukezic_Abdelrahman',\n",
       "  'Ziang_Ma',\n",
       "  'Linyuan_Wang_Haitao_Zhang',\n",
       "  'Wei_Lu',\n",
       "  'Leonardis_Michael',\n",
       "  'Felsberg',\n",
       "  'Mayer_Martin',\n",
       "  'Luka_Ce_Luc_Van_Gool_Learning',\n",
       "  'Alan_Lukezic',\n",
       "  'Amanda_Berg',\n",
       "  'Hsuan_Yang',\n",
       "  'Neil_Smith',\n",
       "  'Bernard_Ghanem_Machine',\n",
       "  'Yinda_Xu',\n",
       "  'Wang_Zuoxin',\n",
       "  'Gang_Yu',\n",
       "  'Adel_Bibi_Silvio',\n",
       "  'Salman_Al',\n",
       "  'Proceedings_Confer',\n",
       "  'Ghanem_Trackingnet',\n",
       "  'Vi_Bin_Yan_Houwen',\n",
       "  'Peng_Jianlong',\n",
       "  'Huchuan_Lu_Learning',\n",
       "  'Amir_Bin_Yan_Xinyu_Zhang',\n",
       "  'Ian_Reid_Silvio',\n",
       "  'Bin_Yan_Haojie',\n",
       "  'Zhao_Dong',\n",
       "  'Lu_Xi_Zhuoran',\n",
       "  'Mingyuan_Zhang',\n",
       "  'Zhao_Shuai_Yi',\n",
       "  'Yang_Xe_Xskimming_Perusal',\n",
       "  'Xe',\n",
       "  'Hongsheng_Li',\n",
       "  'Tianyu_Yang',\n",
       "  'Pengfei_Xu',\n",
       "  'Runbo_Hu_Hua_Chai',\n",
       "  'Chan_Roam_Recurrently',\n",
       "  'Bin_Yu_Ming',\n",
       "  'Tang_Linyu',\n",
       "  'Zheng_Guibo',\n",
       "  'Zhu_Jinqiao',\n",
       "  'Hao_Feng',\n",
       "  'Feng_Hanqing_Lu',\n",
       "  'Zhi_Tian',\n",
       "  'Shen_Hao_Chen_Tong_He_Fco',\n",
       "  'Parmar_Jakob',\n",
       "  'Yuechen_Yu_Yilei',\n",
       "  'Huang_Matthew',\n",
       "  'Aidan_Gomez',\n",
       "  'Paul_Voigtlaender_Jonathon',\n",
       "  'Zhipeng_Zhang',\n",
       "  'Yihao_Liu_Xiao',\n",
       "  'Wang_Bing_Li_Weim',\n",
       "  'Hu_Learn',\n",
       "  'Wang_Chong',\n",
       "  'Luo_Xiaoyan_Sun',\n",
       "  'Wenjun_Zeng',\n",
       "  'Meta_Xe',\n",
       "  'Confer_Zhipeng',\n",
       "  'Peng_Jianlong_Fu',\n",
       "  'Weiming_Hu_Ocean_Object',\n",
       "  'Wang_Wengang',\n",
       "  'Zhou_Jie',\n",
       "  'Wang_Houqiang_Li',\n",
       "  'Zikai_Zhang_Bineng',\n",
       "  'Zhong_Shengping',\n",
       "  'Zhang_Zhenjun',\n",
       "  'Tang_Xin_Liu_Zhaoxiang_Zhang',\n",
       "  'Li_Zhang_Luca',\n",
       "  'Philip_Torr_Fast',\n",
       "  'Linyu_Zheng_Ming_Tang',\n",
       "  'Yingying_Chen',\n",
       "  'Jinqiao_Wang',\n",
       "  'Hanqe_Lu_Learn',\n",
       "  'Zikun_Zhou',\n",
       "  'Wenjie_Pei',\n",
       "  'Xin_Li',\n",
       "  'Hongpeng_Wang_Feng',\n",
       "  'Zheng_Zhenyu',\n",
       "  'Zheng_Zhu_Qiang_Wang_Li',\n",
       "  'Wu_Junjie',\n",
       "  'Yan_Weiming_Hu_Distractor',\n",
       "  'Dimp',\n",
       "  'Su',\n",
       "  'Table_Comparison',\n",
       "  'Tab_Nd_Th_Row',\n",
       "  'Tab',\n",
       "  'Fur',\n",
       "  'Fig',\n",
       "  'St_Rd_Row',\n",
       "  'Overlap',\n",
       "  'Uav_Otb',\n",
       "  'Pr_Siam_Stm_Siam_Retina',\n",
       "  'Xe_Xe_Otb',\n",
       "  'Xe_Xe',\n",
       "  'Siam_Siam_Siam_Retina',\n",
       "  'Mask_Roam',\n",
       "  'Stark',\n",
       "  'Keeptrack_Keeptrack',\n",
       "  'Nc_Uav_Otb',\n",
       "  'Nfs_Fig',\n",
       "  'Fig_Fig',\n",
       "  'Lasot_Lasotextsub',\n",
       "  'Threshold_Xe',\n",
       "  'Figs_We',\n",
       "  'Al_Nc_Attributes'],\n",
       " ['Berman_Triki_Blaschko',\n",
       "  'Bertinetto_Valmadre',\n",
       "  'Vedaldi_Torr_Fully',\n",
       "  'Chen_Yan_Zhu',\n",
       "  'Wang_Yang',\n",
       "  'Montes_Van_Gool_Blazingly',\n",
       "  'Xe_Dai_Zhang',\n",
       "  'Wang_Li',\n",
       "  'Lu_Yang_High',\n",
       "  'Khan_Felsberg_Atom',\n",
       "  'Shahbaz_Khan_Felsberg_Eco',\n",
       "  'Robinson_Shahbaz_Khan_Felsberg',\n",
       "  'Lin_Yang_Chu_Deng_Yu_Bai',\n",
       "  'Xu_Liao_Ling_Lasot',\n",
       "  'Fu_Liu_Fu_Wang_Stmtrack_Template',\n",
       "  'Guo_Feng',\n",
       "  'Huang_Wan_Wang_Learning',\n",
       "  'Henriques_Caseiro_Martins',\n",
       "  'Xe_Hu',\n",
       "  'Huang_Zhao_Huang',\n",
       "  'Tpami_Khoreva',\n",
       "  'Benenson_Ilg_Brox_Schiele_Lucid',\n",
       "  'Bengio_Lecun_Ed',\n",
       "  'Felsberg_Pflugfelder',\n",
       "  'Lukez',\n",
       "  'Drbohlav',\n",
       "  'Zhang_Yan',\n",
       "  'Al',\n",
       "  'Li_Wu',\n",
       "  'Wang_Zhang_Xing_Yan',\n",
       "  'June_Li',\n",
       "  'Wu_Zhu_Hu',\n",
       "  'June_Li_Change_Loy_Video',\n",
       "  'Kristan',\n",
       "  'Lukezic_Voj',\n",
       "  'Kristan_Discriminative',\n",
       "  'Maninis_Caelles_Chen_Pont',\n",
       "  'Mueller_Smith',\n",
       "  'Ghanem',\n",
       "  'Bibi_Giancola',\n",
       "  'Ghanem_Trackingnet',\n",
       "  'Lee_Xu_Kim_Video',\n",
       "  'Sorkine_Hornung',\n",
       "  'Khoreva_Benenson',\n",
       "  'Caelles_Arbela',\n",
       "  'Sorkine_Hornung_Van_Gool',\n",
       "  'Son_Jung_Park_Han',\n",
       "  'Valmadre_Bertinetto',\n",
       "  'Vedaldi_Torr_End',\n",
       "  'Chai_Schroff',\n",
       "  'Adam_Leibe',\n",
       "  'Chen_Feelvos',\n",
       "  'Xe_Voigtlaender_Leibe',\n",
       "  'Torr_Leibe_Siam',\n",
       "  'Paul_Danelljan_Mayer_Van_Gool',\n",
       "  'Wang_Zhou',\n",
       "  'Wang_Teng',\n",
       "  'Xing_Gao_Hu',\n",
       "  'Wang_Zhang',\n",
       "  'Bertinetto_Hu_Torr_Fast',\n",
       "  'Wug_Oh',\n",
       "  'Lee_Sunkavalli',\n",
       "  'Kim_Fast',\n",
       "  'Xe_Xu',\n",
       "  'Liang_Yang_Huang_Youtube',\n",
       "  'Yan_Wang_Lu',\n",
       "  'Yang_Alpha',\n",
       "  'Peng_Fu',\n",
       "  'Wang_Lu_Learning',\n",
       "  'October_Yu_Tang',\n",
       "  'Zheng_Zhu',\n",
       "  'Feng_Feng_Lu',\n",
       "  'Yu_Xiong',\n",
       "  'June_Zhang',\n",
       "  'Peng_Fu_Li_Hu',\n",
       "  'Zhong_Zhang',\n",
       "  'Tang_Liu_Zhang_Distractor',\n",
       "  'Zheng_Tang',\n",
       "  'Chen_Wang_Lu_Learn',\n",
       "  'Zhu_Wang',\n",
       "  'Wu_Yan_Hu',\n",
       "  'Xce_Xb',\n",
       "  'Xm_Xf',\n",
       "  'Xm',\n",
       "  'Nsegmentation_Branch',\n",
       "  'Nf_Xce',\n",
       "  'Xce_Xb_Shot_Learner',\n",
       "  'Lwl_Hence',\n",
       "  'Xce_Xba',\n",
       "  'Paul_Danelljan_Mayer_Van_Gool_Conv',\n",
       "  'Kernel_Size',\n",
       "  'Resblock_Conv',\n",
       "  'Size_Stride_Conv',\n",
       "  'Size_Stride_Kernel',\n",
       "  'Fseen_Funseen',\n",
       "  'Ltot_Ls',\n",
       "  'Nfinetuning_Youtube',\n",
       "  'Paul_Danelljan_Mayer_Van_Gool_Success',\n",
       "  'Lwl_Dmtrack',\n",
       "  'Alpharefine_Siam',\n",
       "  'Youtube',\n",
       "  'Table_Youtube',\n",
       "  'Davis',\n",
       "  'Overlap',\n",
       "  'Fig_Attributes',\n",
       "  'Keeptrack'],\n",
       " ['Xe_Xcisotropy_Log_Concave',\n",
       "  'Xe_Ald',\n",
       "  'Ali_Yeganeh',\n",
       "  'Kirankumar_Shiragur',\n",
       "  'Duong_Vuong',\n",
       "  'Kuikui_Liu',\n",
       "  'Gharan_Cynthia',\n",
       "  'Nathan_Hu',\n",
       "  'Aaron_Schild',\n",
       "  'Elizabeth_Yang',\n",
       "  'Jain_Frederic',\n",
       "  'Log_Concave',\n",
       "  'Gharan_Alireza',\n",
       "  'Xcmonte_Carlo_Markov',\n",
       "  'Xe_Bar_Alexander_Barvinok',\n",
       "  'Bbl_Julius_Borcea',\n",
       "  'Thomas_Liggett',\n",
       "  'Bro_Andrei',\n",
       "  'Xe_Xcgenerating',\n",
       "  'Problem_Xe',\n",
       "  'Systems_Ed_Bengio',\n",
       "  'Cesa_Bianchi_Garnett_Vol_Curran',\n",
       "  'Cel_Elisa_Celis_Amit',\n",
       "  'Cel_Elisa_Celis_Amit_Deshpande_Tarun',\n",
       "  'Chen_Kuikui',\n",
       "  'Eric_Vigoda',\n",
       "  'Csa_Laszlo_Csanky',\n",
       "  'Derezinski_Michael_Mahoney',\n",
       "  'Xe_Xcdeterminantal',\n",
       "  'Mohamed_Elhoseiny',\n",
       "  'Learn',\n",
       "  'Fan_Li',\n",
       "  'Xe_Issn_Doi_Http',\n",
       "  'Hayes_Yitong',\n",
       "  'Metropolis',\n",
       "  'Elvis_Dohmatob',\n",
       "  'Syrine_Krichene',\n",
       "  'Mike_Gartrell_Insu',\n",
       "  'Jennifer_Gillenwater',\n",
       "  'Lg_Gon_Boqe_Gong_Wei',\n",
       "  'Chao_Kristen_Grauman',\n",
       "  'Fei_Sha_Large_Margin_Determi',\n",
       "  'Mike_Gartrell_Ulrich',\n",
       "  'Noam_Koenigstein',\n",
       "  'Jonathan_Hermon_Justin',\n",
       "  'Jvv_Mark_Jerrum',\n",
       "  'Leslie_Valiant_Vijay_Vazirani',\n",
       "  'Alex_Kulesza',\n",
       "  'Ben_Taskar',\n",
       "  'Lin_Jeff_Bilmes',\n",
       "  'Xe',\n",
       "  'Suvrit_Sra',\n",
       "  'Ly_Hongyang',\n",
       "  'Liu_Yitong',\n",
       "  'Robin_Pemantle',\n",
       "  'Lipschitz',\n",
       "  'Guus_Regts',\n",
       "  'Shang_Hua_Teng',\n",
       "  'Science',\n",
       "  'Wil_Mark_Wilhelm_Ajith',\n",
       "  'Alexander_Bonomo',\n",
       "  'Jain_Ed_Chi_Jennifer_Gillenwater',\n",
       "  'Youtube_De'],\n",
       " ['Xbm_Dipanjan_Das_Jakob',\n",
       "  'Nakari_Asai_Hannaneh',\n",
       "  'Compu_Austin',\n",
       "  'Computational_Linguistics',\n",
       "  'Tandon_Peter_Clark_Bha_Nj',\n",
       "  'Wei_Chang_Kenton_Lee_Kristina',\n",
       "  'Eduard_Hovy',\n",
       "  'Bert_Pre',\n",
       "  'Wei_Chang_Kenton_Lee',\n",
       "  'Linguistics_Kristina',\n",
       "  'Maarten_Sap_Ronan_Le_Bras_Emily_Allaway_Chan',\n",
       "  'Nicholas_Lourie',\n",
       "  'Hannah_Rashkin_North_American_Chapter_Association',\n",
       "  'Brendan_Roof_Noah',\n",
       "  'Michael_Schlichtkrull',\n",
       "  'Thomas_Kipf_Peter_Bloem',\n",
       "  'Rianne_Van_Den_Berg',\n",
       "  'Ivan_Titov',\n",
       "  'Max_Welling',\n",
       "  'Nyinya_Huang',\n",
       "  'Meng_Fang_Xunlin_Zhan_Qingxing_Cao',\n",
       "  'Xiaodan_Liang',\n",
       "  'Liang_Lin_Rem',\n",
       "  'Ali_Farhadi',\n",
       "  'Jimmy_Ba_Adam',\n",
       "  'Joshua_Chin_Catherine_Havasi_Conceptnet',\n",
       "  'Lin_Xinyue',\n",
       "  'Chen_Jamin_Chen_Xi',\n",
       "  'Michael_Luke_Zettlemoyer_Natural_Language_Processing_Th',\n",
       "  'Com_Nyinhan',\n",
       "  'Liu_Myle_Ott',\n",
       "  'Jingfei_Du_Man',\n",
       "  'Linguistics_Dar',\n",
       "  'Chen_Omer_Levy',\n",
       "  'Mike_Lewis',\n",
       "  'Luke_Zettlemoyer',\n",
       "  'Lourie_Roberta',\n",
       "  'Jonathan_Berant',\n",
       "  'Yang_Ab',\n",
       "  'Zheng_Quan_Guo_Parisa',\n",
       "  'Linguistics',\n",
       "  'Zheng_Parisa'],\n",
       " ['Batten_Clarke',\n",
       "  'Lambert_Causon',\n",
       "  'Riemann',\n",
       "  'Giovangigli_De_Gassowski_Impact',\n",
       "  'Xe_Xhydrogen_Bubble',\n",
       "  'Navier_Xe_Xstokes',\n",
       "  'Xb_Castro_De_Luna_Pare',\n",
       "  'Handbook_Numer_Anal',\n",
       "  'Castro_Fjordholm',\n",
       "  'Castro',\n",
       "  'Gallardo_Pare',\n",
       "  'Manuel_Jesu',\n",
       "  'Nieto_Enrique',\n",
       "  'Morales_De_Luna_Toma',\n",
       "  'Reina_Gladys_Pare',\n",
       "  'Carlos',\n",
       "  'Xe_Chen_Shu_Entropy',\n",
       "  'Galerkin',\n",
       "  'Cheng_Zhang_Liu',\n",
       "  'Coquel_Marmignon',\n",
       "  'Baer_Nunziato',\n",
       "  'Xe_De_Frahan',\n",
       "  'Varadan_Johnsen',\n",
       "  'Derigs_Winters',\n",
       "  'Gassner_Walch',\n",
       "  'Basel',\n",
       "  'Dumbser_Balsara',\n",
       "  'Fjordholm_Mishra',\n",
       "  'Franquet_Perrier_Runge',\n",
       "  'Xe_Gassner',\n",
       "  'Gassner_Winter',\n",
       "  'Kopriva_Split',\n",
       "  'Xe_Giordano',\n",
       "  'Burtschell_Richtmyer',\n",
       "  'Fluid_Mech',\n",
       "  'Harten_Lax_Leer',\n",
       "  'Godunov',\n",
       "  'Mishra_Pare',\n",
       "  'Xe_Hu',\n",
       "  'Khoo_Adams_Huang',\n",
       "  'Jiang_Shu',\n",
       "  'Xe_Kennedy',\n",
       "  'Kopriva_Gassner',\n",
       "  'Xe_Liu_Khoo_Yeo_Ghost',\n",
       "  'Menikoff_Plohr',\n",
       "  'Rev_Mod',\n",
       "  'Quirk_Karni',\n",
       "  'Xe_Renac',\n",
       "  'Commun_Math_Sci',\n",
       "  'Couaillier_Aghora_High_Order_Dg_Solver_Turbulent_Flow',\n",
       "  'Xe_Shu_Osher_Efficient',\n",
       "  'Xe_Shyue_An',\n",
       "  'Xe_Sjo',\n",
       "  'Xe_Tadmor',\n",
       "  'Baer_Xe_Xnunziato',\n",
       "  'Toro_Mu',\n",
       "  'Siviglia_Bounds',\n",
       "  'Toro_Riemann',\n",
       "  'Xe_Riemann',\n",
       "  'Hll_Riemann',\n",
       "  'Volpert',\n",
       "  'Wang_Zhang',\n",
       "  'Gibson_Giraldo',\n",
       "  'Xe_Zhang_Shu',\n",
       "  'Zhang_Shu',\n",
       "  'Xe_Xc'],\n",
       " ['Achituve_Maron',\n",
       "  'Xe_Bao_Dong_Wei_Beit_Bert',\n",
       "  'Kirillov_Zagoruyko',\n",
       "  'Xe_Springer',\n",
       "  'Chang_Funkhouser',\n",
       "  'Guibas_Hanrahan',\n",
       "  'Huang_Li_Savarese_Savva_Song_Su',\n",
       "  'Chen_Kornblith_Norouzi_Hinton',\n",
       "  'Iii_Singh_Ed_Proceed',\n",
       "  'Chen_Duan_Houthooft',\n",
       "  'Chen_Ding_Wang_Xin_Mo_Wang',\n",
       "  'Luo_Zeng_Wang_Context',\n",
       "  'Chen_He_Explore',\n",
       "  'Dai_Chang_Savva_Halber',\n",
       "  'Nie',\n",
       "  'Scannet_Richly',\n",
       "  'Danila_Rukhovich',\n",
       "  'Deng_Dong_Socher',\n",
       "  'Li_Li_Fei_Fei_Imagenet',\n",
       "  'Devlin_Chang_Lee_Toutanova_Bert_Pre',\n",
       "  'Jun',\n",
       "  'Liu',\n",
       "  'Zhai_Unterthiner_Dehghani',\n",
       "  'Learning_Xe_Xt',\n",
       "  'Lin_Le',\n",
       "  'Gidaris_Singh',\n",
       "  'Svnl_Guo_Cai',\n",
       "  'Liu_Mu_Martin',\n",
       "  'Chen_Xie',\n",
       "  'Wu_Xie',\n",
       "  'Huang_Sun_Liu',\n",
       "  'Huang_Xie_Zhu_Zhu_Spatio',\n",
       "  'Jiang_Chang_Wang_Transgan',\n",
       "  'Joshi_Chen_Liu',\n",
       "  'Zettlemoyer_Levy_Spanbert',\n",
       "  'Lan_Chen_Goodman',\n",
       "  'Gimpel_Sharma_Soricut',\n",
       "  'Albert',\n",
       "  'Bert_Self',\n",
       "  'Heaaetvs_Li_Bu_Sun',\n",
       "  'Wu_Di_Chen',\n",
       "  'Lin_Goyal_Girshick',\n",
       "  'Meng_Lu',\n",
       "  'Xiang_Pan_Densepoint_Learning',\n",
       "  'Xe_Liu',\n",
       "  'Xiang_Pan_Relation',\n",
       "  'Xe_Liu_Hu',\n",
       "  'Cao_Zhang_Tong',\n",
       "  'Misra_Girdhar',\n",
       "  'Xe_Springer_Poursaeed',\n",
       "  'Jiang_Qiao',\n",
       "  'Kim_Self',\n",
       "  'Xe_Qi_Yi_Su_Guibas',\n",
       "  'Radford_Kim_Hallacy_Ramesh_Goh_Agarwal_Sastry',\n",
       "  'Askell_Mishkin_Clark',\n",
       "  'Rao_Liu',\n",
       "  'Wei_Lu',\n",
       "  'Hsieh_Zhou',\n",
       "  'Srivastava',\n",
       "  'Krizhevsky_Sutskever',\n",
       "  'Steiner_Kolesnikov',\n",
       "  'Wightman_Uszkoreit_Beyer',\n",
       "  'Thabet_Alwassel',\n",
       "  'Thomas_Qi',\n",
       "  'Marcotegui_Goulette_Guibas',\n",
       "  'Kolesnikov_Beyer',\n",
       "  'Zhai_Unterthiner_Yung_Steiner_Keysers_Uszkoreit',\n",
       "  'Al_Uy',\n",
       "  'Pham_Hua_Nguyen_Yeung',\n",
       "  'Parmar_Uszkoreit_Jones_Gomez_Kaiser',\n",
       "  'Luxburg_Bengio_Wallach',\n",
       "  'Vishwanathan_Garnett',\n",
       "  'Wang_Liu_Yue_Lasenby_Kusner',\n",
       "  'Wang_Zhu',\n",
       "  'Yuille_Chen_Max',\n",
       "  'Wang_Sun',\n",
       "  'Liu_Sarma_Bronstein_Solomon',\n",
       "  'Acm_Transactions',\n",
       "  'Wei_Fan_Xie',\n",
       "  'Wu_Yuille',\n",
       "  'Wu_Qi',\n",
       "  'Fuxin_Pointconv',\n",
       "  'Wu_Song_Khosla',\n",
       "  'Tang_Xiao',\n",
       "  'Xu_Zeng',\n",
       "  'Qiao_Spidercnn_Deep',\n",
       "  'Xe_Yan_Yang_Li',\n",
       "  'Kang_Hua_Huang_Implicit',\n",
       "  'Yang_Dai_Yang_Carbonell',\n",
       "  'Yi_Kim',\n",
       "  'Shen_Yan_Su_Lu',\n",
       "  'Huang_Sheffer_Guibas',\n",
       "  'Xe_Yu_Tang_Rao_Huang_Zhou_Lu_Point',\n",
       "  'Yun_Han',\n",
       "  'Chun_Choe_Yoo',\n",
       "  'Sun_Yang_Huang_Hdnet',\n",
       "  'Zhao_Jiang_Fu',\n",
       "  'Xe_Zhao',\n",
       "  'Jia_Torr_Koltun_Point',\n",
       "  'Xe_Zhong',\n",
       "  'Zheng_Kang',\n",
       "  'Li_Yang_Random'],\n",
       " ['Rev_Lett',\n",
       "  'Lhc_Jhep',\n",
       "  'Mistlberger_Higgs',\n",
       "  'Lo_Qcd',\n",
       "  'Chen_Gehrmann',\n",
       "  'Huss_Mistlberger',\n",
       "  'Pelloni_Fully_Differential_Higgs',\n",
       "  'Drell_Yan',\n",
       "  'Bargiela_Caola',\n",
       "  'Von_Manteuffel_Tancredi',\n",
       "  'Floratos_Ross_Sachrajda_High_Order_Effects_Asymptotically_Free_Gauge_Theories',\n",
       "  'Xe',\n",
       "  'Gonzalez',\n",
       "  'Lett_Hamberg',\n",
       "  'Ellis_Vogelsang',\n",
       "  'Singlet',\n",
       "  'Matiounine_Smith_Van_Neerven',\n",
       "  'Lepton_Hadron',\n",
       "  'Phys_Rev',\n",
       "  'Larin_Van_Ritbergen',\n",
       "  'Larin_Nogueira',\n",
       "  'Moch_Vermaseren_Vogt',\n",
       "  'Nonsinglet',\n",
       "  'Ablinger_Behring_Blu',\n",
       "  'De_Freitas',\n",
       "  'Von_Manteuffel_Schneider',\n",
       "  'Behring_Blu',\n",
       "  'Goedicke_Klein',\n",
       "  'Von_Manteuffel_Et',\n",
       "  'Marquard_Schneider',\n",
       "  'Xe_Xe',\n",
       "  'Velizhanin_Four',\n",
       "  'Qcd_Int_Mod_Phys',\n",
       "  'Moch_Ruijl_Ueda_Vermaseren_Vogt_Four_Loop_Non_Singlet_Splitting',\n",
       "  'Gracey_Anomalous',\n",
       "  'Lett',\n",
       "  'Moch_Ruijl',\n",
       "  'Gross_Wilczek',\n",
       "  'Rev_Georgi',\n",
       "  'Rev_Dixon',\n",
       "  'Taylor_Renormalization',\n",
       "  'Yang_Mills',\n",
       "  'Kluberg_Stern_Zuber',\n",
       "  'Rev_Kluberg',\n",
       "  'Rev_Joglekar_Lee',\n",
       "  'Joglekar_Local_Operator',\n",
       "  'Phys_Lett',\n",
       "  'Curci',\n",
       "  'Thierry_Mieg',\n",
       "  'Rev_Xe',\n",
       "  'Xt_Hooft',\n",
       "  'Polon_Sarkar_Strubbe_Anomalous_Dimensions_Background',\n",
       "  'Gauges_Becchi',\n",
       "  'Tyutin_Gauge_Invariance_Field_Theory_Statistical_Physics',\n",
       "  'Int_Mod_Phys',\n",
       "  'Chetyrkin_Tkachov_Infrared_Operation',\n",
       "  'Lett_Chetyrkin',\n",
       "  'Lett_Smirnov_Chetyrkin_Operation_Minimal_Subtraction',\n",
       "  'Xe_Xe_Operation_Asymptotic',\n",
       "  'Herzog_Ruijl',\n",
       "  'Jhep',\n",
       "  'Weinberg_Xe_X_Dimension',\n",
       "  'Cao_Herzog_Melia',\n",
       "  'Gorishnii_Larin_Tkachov',\n",
       "  'Lett_Gorishnii_Larin_Coefficient_Functions_Asymptotic',\n",
       "  'Nogueira_Automatic',\n",
       "  'Baikov_Chetyrkin',\n",
       "  'Lee_Smirnov',\n",
       "  'Georgoudis_Goncalves_Panzer',\n",
       "  'Georgoudis_Gonc',\n",
       "  'Freedman_Muzinich_Weinberg',\n",
       "  'Freedman_Weinberg'],\n",
       " ['Duncan_Insana_Ayache',\n",
       "  'Jacob_Ye_Ying',\n",
       "  'Jan',\n",
       "  'Chen_Lu',\n",
       "  'Chen_Williamson_Mahmood',\n",
       "  'Nat_Biomed_Eng',\n",
       "  'Jun',\n",
       "  'Zhu_Rosen',\n",
       "  'Xe_Xcmr',\n",
       "  'Magn_Nreson_Med',\n",
       "  'Cai',\n",
       "  'Wang_Zeng',\n",
       "  'Liang_Wu',\n",
       "  'Chen_Ding_Zhong',\n",
       "  'Magn_Reson_Med',\n",
       "  'Mrm_Zhang',\n",
       "  'Chen_Zhang_Cai_Cai',\n",
       "  'Tmi_Li',\n",
       "  'Wu_Ma',\n",
       "  'Cai_Cai',\n",
       "  'Mrm_Yang_Wang_Bao_Wang_Ma',\n",
       "  'Cai_Dong',\n",
       "  'Gavazzi',\n",
       "  'Xe',\n",
       "  'Mrm_Bollmann_Rasmussen_Kristensen',\n",
       "  'Plocharski_Brien',\n",
       "  'Nc_Langkammer',\n",
       "  'Janke_Barth',\n",
       "  'Jung_Yoon_Choi',\n",
       "  'Nam_Kim_Lee_Ji',\n",
       "  'Della_Maggiora_Castillo',\n",
       "  'Qiu_Liu',\n",
       "  'Tejos_Uribe',\n",
       "  'Xcdeepspio_Super',\n",
       "  'Jung_Lee_Ryu_Song',\n",
       "  'Moon_Kim',\n",
       "  'Mrm_Chen_Schar_Chan',\n",
       "  'Huang_Wei_Lu',\n",
       "  'Qin',\n",
       "  'Zijl_Xu',\n",
       "  'Huang_Lu',\n",
       "  'Qiu_Guo_Agback',\n",
       "  'Chen_Xe',\n",
       "  'Angew_Chem_Int_Ed',\n",
       "  'Huang_Zhao_Wang_Orekhov_Guo',\n",
       "  'Wang_Guo_Tu_Huang_Zhou',\n",
       "  'Wang_Feng',\n",
       "  'Lam_Li',\n",
       "  'Tmi_Chen_Wang_Huang',\n",
       "  'Chen_Cai_Cai_Chen',\n",
       "  'Magn_Reson_Med_Epub',\n",
       "  'Loktyushin_Herz_Dang',\n",
       "  'Glang_Deshmane',\n",
       "  'Nscheffler_Zaiss',\n",
       "  'Jiang_Jiang',\n",
       "  'Wang_Wright',\n",
       "  'Liu_Wang',\n",
       "  'Xe_Xcmulti',\n",
       "  'Dec',\n",
       "  'Bloch_Hansen_Packard',\n",
       "  'Ma_Gulani',\n",
       "  'Liu_Sunshine_Duerk',\n",
       "  'Bernstein_King',\n",
       "  'Chen_Orekhov',\n",
       "  'Cai_Lin',\n",
       "  'Chen_Chen_Cai_Zhong',\n",
       "  'Crhy_Liu_Velikina_Block',\n",
       "  'Kijowski',\n",
       "  'Lee_Kim',\n",
       "  'Lim_Bliesener',\n",
       "  'Narayanan_Nayak',\n",
       "  'Mrm_Dar_Ozbey',\n",
       "  'Mrm_Van_Herten_Chiribiri'],\n",
       " ['Avis_Fukuda',\n",
       "  'Romeijn_Smith',\n",
       "  'Bertsimas_Vempala_Solving',\n",
       "  'Besag_Green_Higdon',\n",
       "  'Bre',\n",
       "  'Markov',\n",
       "  'Gibbs',\n",
       "  'Monte_Carlo',\n",
       "  'Meng_Handbook',\n",
       "  'Karger_Use',\n",
       "  'Geman_Geman_Stochastic',\n",
       "  'Pami_Xe_Http_Doi',\n",
       "  'Tpami_Hastings_Monte',\n",
       "  'Biometrika',\n",
       "  'Huang_Mehrotra',\n",
       "  'Jia_Fischer_Dyer_Attribute',\n",
       "  'Xe_Kapfer_Krauth_Sample',\n",
       "  'Lawrence_Polytope',\n",
       "  'Li_Zhu_Ogihara',\n",
       "  'Xe_Lova',\n",
       "  'Narayanan_Randomize',\n",
       "  'Revuz_Markov',\n",
       "  'John_Wiley_Sons_Smith']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All function in one\n",
    "\n",
    "entities=[]\n",
    "def main():\n",
    "    for text in elements:\n",
    "        temp=after_references(str(text))\n",
    "        temp=preprocess_text(temp)\n",
    "        temp=extract(temp)\n",
    "        entities.append(temp)\n",
    "    return entities\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a35d1365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897bff1",
   "metadata": {},
   "source": [
    "## Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "164621c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready import *\n",
    "\n",
    "onto_path.append(\"owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8470daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Creating new ontology onto <http://test.org/onto.owl>.\n"
     ]
    }
   ],
   "source": [
    "onto = Ontology(\"http://test.org/onto.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5aad8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class References(Thing):\n",
    "    ontology = onto\n",
    "    \n",
    "class quoted_by(Property):\n",
    "    ontolgy = onto\n",
    "    domain = [References]\n",
    "    range = [Author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd4f26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.sax.saxutils import escape\n",
    "\n",
    "def invalid_xml_remove(char):\n",
    "    \"\"\"Tracks illegal unicode characters\"\"\"\n",
    "    #http://stackoverflow.com/questions/1707890\n",
    "    # /fast-way-to-filter-illegal-xml-unicode-chars-in-python\n",
    "    illegal_unichrs = [ (0x00, 0x08), (0x0B, 0x1F), (0x7F, 0x84), (0x86, 0x9F),\n",
    "                    (0xD800, 0xDFFF), (0xFDD0, 0xFDDF), (0xFFFE, 0xFFFF),\n",
    "                    (0x1FFFE, 0x1FFFF), (0x2FFFE, 0x2FFFF), (0x3FFFE, 0x3FFFF),\n",
    "                    (0x4FFFE, 0x4FFFF), (0x5FFFE, 0x5FFFF), (0x6FFFE, 0x6FFFF),\n",
    "                    (0x7FFFE, 0x7FFFF), (0x8FFFE, 0x8FFFF), (0x9FFFE, 0x9FFFF),\n",
    "                    (0xAFFFE, 0xAFFFF), (0xBFFFE, 0xBFFFF), (0xCFFFE, 0xCFFFF),\n",
    "                    (0xDFFFE, 0xDFFFF), (0xEFFFE, 0xEFFFF), (0xFFFFE, 0xFFFFF),\n",
    "                    (0x10FFFE, 0x10FFFF) ]\n",
    "\n",
    "    illegal_ranges = [f\"{chr(low)}-{chr(high)}\"\n",
    "                  for (low, high) in illegal_unichrs\n",
    "                  if low < sys.maxunicode]\n",
    "\n",
    "    illegal_xml_re = re.compile(f'[{\"\".join(illegal_ranges)}]')\n",
    "    if illegal_xml_re.search(char) is not None:\n",
    "        #Replace with space\n",
    "        return ''\n",
    "    else:\n",
    "        return char\n",
    "\n",
    "def clean_char(char):\n",
    "    \"\"\"\n",
    "    Function for remove invalid XML characters from\n",
    "    incoming data.\n",
    "    \"\"\"\n",
    "    #Get rid of the ctrl characters first.\n",
    "    #http://stackoverflow.com/questions/1833873/python-regex-escape-characters\n",
    "    char = re.sub('\\x1b[^m]*m', '', char)\n",
    "    #Clean up invalid xml\n",
    "    char = invalid_xml_remove(char)\n",
    "    replacements = [\n",
    "        ('\\u201c', '\\\"'),\n",
    "        ('\\u0141', '\\\"'),\n",
    "        ('\\u201d', '\\\"'),\n",
    "        (\"\\u001B\", ''), #http://www.fileformat.info/info/unicode/char/1b/index.htm\n",
    "        (\"\\u0019\", ''), #http://www.fileformat.info/info/unicode/char/19/index.htm\n",
    "        (\"\\u0016\", ''), #http://www.fileformat.info/info/unicode/char/16/index.htm\n",
    "        (\"\\u001C\", ''), #http://www.fileformat.info/info/unicode/char/1c/index.htm\n",
    "        (\"\\u0003\", ''), #http://www.utf8-chartable.de/unicode-utf8-table.pl?utf8=0x\n",
    "        (\"\\u000C\", ''),\n",
    "        (\"\\u03b1\", ''),\n",
    "        (\"u\\u039C\", ''),\n",
    "        (\"\\u03C3\", ''),\n",
    "        (\"\\u0141\", ''),\n",
    "        (\"\\u0308\", ''),\n",
    "        (\"\\u2032\", ''),\n",
    "        (\"\\u03b8\", '')\n",
    "\n",
    "    \n",
    "    ]\n",
    "    for rep, new_char in replacements:\n",
    "        if char == rep:\n",
    "            return new_char\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4794f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_value(text: str):\n",
    "        \"\"\"Escape the illegal characters for an ontology property\"\"\"\n",
    "        if text is None:\n",
    "            return None\n",
    "        # function to escape XML character data\n",
    "        text = escape(text)\n",
    "        text = text.replace('\\n', '')\n",
    "        text = text.replace('\\r', '')\n",
    "        text = text.replace('\\f', '')\n",
    "        text = text.replace('\\b', '')\n",
    "        text = text.replace('\"', '')\n",
    "        text = text.replace('[', '')\n",
    "        text = text.replace(']', '')\n",
    "        text = text.replace('{', '')\n",
    "        text = text.replace('}', '')\n",
    "        text = text.replace('#', '')\n",
    "        text = text.replace('|', '')\n",
    "        text = text.replace(' ', '_')\n",
    "        text = text.replace('σ', '')\n",
    "        text = text.replace('ℓ', '')\n",
    "        text = text.replace('Σ', '')\n",
    "        text = text.replace('ı', '')\n",
    "        text = text.replace('ē', '')\n",
    "        text = text.replace('μ', '')\n",
    "        text = text.replace('Μ', '')\n",
    "        text = text.replace('Ł', '')\n",
    "        text = text.replace('ł', '')\n",
    "        text = text.replace('θ', '')\n",
    "        text = text.replace('φ', '')\n",
    "        text = text.replace('Φ', '')\n",
    "        text = text.replace('̈', '')\n",
    "        text = text.replace('́', '')\n",
    "        text = text.replace('′', '')\n",
    "        text = text.replace('∈', '')\n",
    "        text = text.replace('ć', '')\n",
    "        text = text.replace('ź', '')\n",
    "        text = clean_char(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2126e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Mihir.Prabhudesai]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Anirudh.Goyal]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Deepak.Pathak]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Katerina.Fragkiadaki]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Christoph.Mayer]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Martin.Danelljan]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Goutam.Bhat]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Matthieu.Paul]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Danda.Pani.Paudel]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Fisher.Yu]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Luc.Van.Gool]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Matthieu.Paul_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Martin.Danelljan_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Christoph.Mayer_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Luc.Van.Gool_2]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Nima.Anari]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Callum.Burgess]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Kevin.Tian]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Thuy.Duong.Vuong]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Chen.Zheng]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.Parisa.Kordjamshidi]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.R.mi.Abgrall]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Pratik.Rai]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Florent.Renac]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Haotian.Liu]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Mu.Cai]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Yong.Jae.Lee]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Giulio.Falcioni]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Franz.Herzog]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Qinqin.Yang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Zi.Wang]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Kunyuan.Guo]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Congbo.Cai]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Xiaobo.Qu]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Christos.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n",
      "[onto.Aristeidis.Karras]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(array_authors)):\n",
    "    for j in range(len(array_authors[i])):\n",
    "        aut_num = Author(escape_value(array_authors[i][j]))\n",
    "        for z in range(len(entities[i])):\n",
    "            ref_num = References(escape_value(entities[i][z]))\n",
    "            ref_num.quoted_by.append(aut_num)\n",
    "            print(ref_num.quoted_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e27e8710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready * Saving ontology onto to owl\\onto.owl...\n"
     ]
    }
   ],
   "source": [
    "onto.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472064be",
   "metadata": {},
   "source": [
    "### To conlude, we will change the xml encoding to UTF-8 otherwise, Protégé may not read the file ont.owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# iconv -f ISO-8859-1 -t UTF-8 owl/onto.owl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
